{
  "cacheItemsMap": {
    "case-studies/content/chipper-cash.mdx": {
      "document": {
        "title": "Chipper Cash cuts testing time from ~24 hours to about 5 min",
        "logo": "/case-study-images/f8772922b3983f5b3216f0163e47a845.svg",
        "description": "Chipper Cash brings in Release to cut developer downtime and increase efficiencies and collaboration.",
        "publishDate": "2024-02-29T22:13:29.000Z",
        "thumbnail": "/case-study-images/0e8d782eca236713ec2b35ac0dc01d20.svg",
        "body": {
          "raw": "\n#### About the Company\n\nChipper Cash is a financial technology company serving more than five million people across the African continent. In 2018, Chipper Cash revolutionized intra-Africa money transfers with the introduction of fee-free personal payments—providing a frictionless way to send and receive money cross-border—immediately offering financial inclusivity to millions.\n\nSince then, Chipper Cash has increased its product suite, introducing services across personal investments and digital business transactions, and expanded its reach into the UK and US.\n\nLed by Ugandan and Ghanian co-founders Ham Serunjogi and Maijid Moujaled, Chipper Cash is focused on its mission to unlock global opportunities and connect Africa.\n\n#### Key Release Technologies Used\n\nEphemeral Environments\n\nApp Imports \n\n#### Challenge\n\nChipper Cash has a remote-first philosophy, meaning the company has a global cohort of team members based across the world. For Chipper Cash’s population of engineers, it was critical to have quick and easy access to tools and systems that help combat complexities around collaboration and learning is critical. \n\nTo aid this, Chipper Cash wanted to set up local (or isolated environments) builds that would emulate the backend, so developers could test changes. Think about it this way- if you’re developing a feature or trying to test a bug, it can be difficult and time intensive, as there are many moving pieces to the backend of an application. ChipperCash found the engineers only had two options: ship a massive image at once (which can cause major outages), or make changes quickly, which introduces risk.\n\nNeither option was proving suitable. Based on demands within the fintech industry, Chipper Cash needs to make changes quickly without risk of breaking other developers’ work. Additionally, Chipper Cash needed a responsive tool that was also natively integrated with their source code.\n\n#### Pre-Release\n\nBefore Release, Chipper Cash hosted their staging environment, or as they refer to it, their sandbox environment, in Heroku. However, a single staging environment meant dozens of developers were waiting in line to test changes. This became increasingly problematic as they added more engineers to their team, resulting in more pull requests, and unmanageable operational overhead.\n\n‍  \nThey found teams still had to diligently follow a set of rules and test changes in the staging environment, which led to a backlog of test requirements. This bottleneck caused multi day delays of feature releases, while their engineering team continued to scale. They realized engineers were being slowed down as they waited in line to test changes for solutions that ultimately didn’t meet their needs. If they had the ability to test earlier in the development process, they recognized they would have saved dozens of development hours building ill-fitting solutions.\n\n#### A New Solution\n\nRelease allowed Chipper Cash to alleviate the difficulties—the undifferentiated lift and time required to build and maintain these environments. As a mobile app that uses 90% Javascript, each engineer now has their own isolated test environment to work asynchronously, hosted in their own AWS account. It has become fast and easy to do a pull request, go to the environment, test some features, and push code through continuous integration pipelines leveraging Release. With Release, Chipper Cash has cut testing time from **_~24 hours to about 5 minutes._** The team also has the ability within their own dashboards to view pull requests from other engineers and get a visual snapshot of the code that is being pushed out daily. This has increased teamwork and collaboration.  \n\n#### What’s Next\n\nChipper Cash has been most excited about Release’s cross-functional collaboration and continues to expand. There has been a steady flow of teams leveraging these environments to view changes before they are pushed to production. The short-term goal is to increase collaboration as well as migrate all sandbox environments to Release. As the integration continues, developers are no longer limited by bottlenecks and Chipper Cash can release new features to customers faster.\n\n#### A word from the client\n\n> The communication between our own departments using the software and the Integration Engineers at Release has created a collaborative environment for us to be successful in our testing. Release has helped cut our testing time from days to minutes and provided insight to how we can optimize internally. It has enabled us to involve more teams to review feature updates and increase synergy within our company.\n\n###### Wendy Whitsett\n\n###### Software Engineer • Chipper Cash\n\n<CaseStudyCTA />\n",
          "code": "var Component=(()=>{var d=Object.create;var s=Object.defineProperty;var p=Object.getOwnPropertyDescriptor;var m=Object.getOwnPropertyNames;var u=Object.getPrototypeOf,g=Object.prototype.hasOwnProperty;var f=(a,e)=>()=>(e||a((e={exports:{}}).exports,e),e.exports),b=(a,e)=>{for(var t in e)s(a,t,{get:e[t],enumerable:!0})},r=(a,e,t,o)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let i of m(e))!g.call(a,i)&&i!==t&&s(a,i,{get:()=>e[i],enumerable:!(o=p(e,i))||o.enumerable});return a};var w=(a,e,t)=>(t=a!=null?d(u(a)):{},r(e||!a||!a.__esModule?s(t,\"default\",{value:a,enumerable:!0}):t,a)),y=a=>r(s({},\"__esModule\",{value:!0}),a);var l=f((A,h)=>{h.exports=_jsx_runtime});var x={};b(x,{default:()=>k,frontmatter:()=>v});var n=w(l()),v={title:\"Chipper Cash cuts testing time from ~24 hours to about 5 min\",description:\"Chipper Cash brings in Release to cut developer downtime and increase efficiencies and collaboration.\",publishDate:\"2024-02-29T22:13:29.000Z\",logo:\"/case-study-images/f8772922b3983f5b3216f0163e47a845.svg\",thumbnail:\"/case-study-images/0e8d782eca236713ec2b35ac0dc01d20.svg\"};function c(a){let e=Object.assign({h4:\"h4\",a:\"a\",span:\"span\",p:\"p\",br:\"br\",strong:\"strong\",em:\"em\",blockquote:\"blockquote\",h6:\"h6\"},a.components),{CaseStudyCTA:t}=e;return t||N(\"CaseStudyCTA\",!0),(0,n.jsxs)(n.Fragment,{children:[(0,n.jsxs)(e.h4,{id:\"about-the-company\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#about-the-company\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"About the Company\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Chipper Cash is a financial technology company serving more than five million people across the African continent. In 2018, Chipper Cash revolutionized intra-Africa money transfers with the introduction of fee-free personal payments\\u2014providing a frictionless way to send and receive money cross-border\\u2014immediately offering financial inclusivity to millions.\"}),`\n`,(0,n.jsx)(e.p,{children:\"Since then, Chipper Cash has increased its product suite, introducing services across personal investments and digital business transactions, and expanded its reach into the UK and US.\"}),`\n`,(0,n.jsx)(e.p,{children:\"Led by Ugandan and Ghanian co-founders Ham Serunjogi and Maijid Moujaled, Chipper Cash is focused on its mission to unlock global opportunities and connect Africa.\"}),`\n`,(0,n.jsxs)(e.h4,{id:\"key-release-technologies-used\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#key-release-technologies-used\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Key Release Technologies Used\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Ephemeral Environments\"}),`\n`,(0,n.jsx)(e.p,{children:\"App Imports\\xA0\"}),`\n`,(0,n.jsxs)(e.h4,{id:\"challenge\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#challenge\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Challenge\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Chipper Cash has a remote-first philosophy, meaning the company has a global cohort of team members based across the world. For Chipper Cash\\u2019s population of engineers, it was critical to have quick and easy access to tools and systems that help combat complexities around collaboration and learning is critical.\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:\"To aid this, Chipper Cash wanted to set up local (or isolated environments) builds that would emulate the backend, so developers could test changes. Think about it this way- if you\\u2019re developing a feature or trying to test a bug, it can be difficult and time intensive, as there are many moving pieces to the backend of an application. ChipperCash found the engineers only had two options: ship a massive image at once (which can cause major outages), or make changes quickly, which introduces risk.\"}),`\n`,(0,n.jsx)(e.p,{children:\"Neither option was proving suitable. Based on demands within the fintech industry, Chipper Cash needs to make changes quickly without risk of breaking other developers\\u2019 work. Additionally, Chipper Cash needed a responsive tool that was also natively integrated with their source code.\"}),`\n`,(0,n.jsxs)(e.h4,{id:\"pre-release\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#pre-release\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Pre-Release\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Before Release, Chipper Cash hosted their staging environment, or as they refer to it, their sandbox environment, in Heroku. However, a single staging environment meant dozens of developers were waiting in line to test changes. This became increasingly problematic as they added more engineers to their team, resulting in more pull requests, and unmanageable operational overhead.\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"\\u200D\",(0,n.jsx)(e.br,{}),`\n`,\"They found teams still had to diligently follow a set of rules and test changes in the staging environment, which led to a backlog of test requirements. This bottleneck caused multi day delays of feature releases, while their engineering team continued to scale. They realized engineers were being slowed down as they waited in line to test changes for solutions that ultimately didn\\u2019t meet their needs. If they had the ability to test earlier in the development process, they recognized they would have saved dozens of development hours building ill-fitting solutions.\"]}),`\n`,(0,n.jsxs)(e.h4,{id:\"a-new-solution\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#a-new-solution\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"A New Solution\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"Release allowed Chipper Cash to alleviate the difficulties\\u2014the undifferentiated lift and time required to build and maintain these environments. As a mobile app that uses 90% Javascript, each engineer now has their own isolated test environment to work asynchronously, hosted in their own AWS account. It has become fast and easy to do a pull request, go to the environment, test some features, and push code through continuous integration pipelines leveraging Release. With Release, Chipper Cash has cut testing time from \",(0,n.jsx)(e.strong,{children:(0,n.jsx)(e.em,{children:\"~24 hours to about 5 minutes.\"})}),\" The team also has the ability within their own dashboards to view pull requests from other engineers and get a visual snapshot of the code that is being pushed out daily. This has increased teamwork and collaboration.\\xA0\\xA0\"]}),`\n`,(0,n.jsxs)(e.h4,{id:\"whats-next\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#whats-next\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"What\\u2019s Next\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Chipper Cash has been most excited about Release\\u2019s cross-functional collaboration and continues to expand. There has been a steady flow of teams leveraging these environments to view changes before they are pushed to production. The short-term goal is to increase collaboration as well as migrate all sandbox environments to Release. As the integration continues, developers are no longer limited by bottlenecks and Chipper Cash can release new features to customers faster.\"}),`\n`,(0,n.jsxs)(e.h4,{id:\"a-word-from-the-client\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#a-word-from-the-client\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"A word from the client\"]}),`\n`,(0,n.jsxs)(e.blockquote,{children:[`\n`,(0,n.jsx)(e.p,{children:\"The communication between our own departments using the software and the Integration Engineers at Release has created a collaborative environment for us to be successful in our testing. Release has helped cut our testing time from days to minutes and provided insight to how we can optimize internally. It has enabled us to involve more teams to review feature updates and increase synergy within our company.\"}),`\n`]}),`\n`,(0,n.jsxs)(e.h6,{id:\"wendy-whitsett\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#wendy-whitsett\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Wendy Whitsett\"]}),`\n`,(0,n.jsxs)(e.h6,{id:\"software-engineer--chipper-cash\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#software-engineer--chipper-cash\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Software Engineer \\u2022 Chipper Cash\"]}),`\n`,(0,n.jsx)(t,{})]})}function C(a={}){let{wrapper:e}=a.components||{};return e?(0,n.jsx)(e,Object.assign({},a,{children:(0,n.jsx)(c,a)})):c(a)}var k=C;function N(a,e){throw new Error(\"Expected \"+(e?\"component\":\"object\")+\" `\"+a+\"` to be defined: you likely forgot to import, pass, or provide it.\")}return y(x);})();\n;return Component;"
        },
        "_id": "case-studies/content/chipper-cash.mdx",
        "_raw": {
          "sourceFilePath": "case-studies/content/chipper-cash.mdx",
          "sourceFileName": "chipper-cash.mdx",
          "sourceFileDir": "case-studies/content",
          "contentType": "mdx",
          "flattenedPath": "case-studies/content/chipper-cash"
        },
        "type": "CaseStudy",
        "computedSlug": "chipper-cash"
      },
      "documentHash": "1739393595033",
      "hasWarnings": false,
      "documentTypeName": "CaseStudy"
    },
    "case-studies/content/datasaurai.mdx": {
      "document": {
        "title": "How Release was able to easily replicate the complex environments of Datasaur.ai",
        "logo": "/case-study-images/1f668a1eca8e16461fe596b61ece5158.svg",
        "description": "Datasaur is making data labeling simple.",
        "publishDate": "2024-03-01T16:29:50.000Z",
        "thumbnail": "/case-study-images/8a406138434670fb4294918ed6cc6af7.png",
        "body": {
          "raw": "\n#### About the client\n\nDatasaur is making data labeling simple. They set the standard for best practices in both data labeling and extracting valuable insights from raw data. By labeling data with Datasaur, companies are able to create great machine learning solutions.\n\n#### Problem\n\nWhen Datasaur came to Release, they were running their application on multiple EC2 nodes in AWS. The engineering team was small but growing, and the VP of Engineering was responsible for their Architecture, implementation, DevOps and SRE functions. They were a seed-round funded startup at the time but didn’t have a budget to grow headcount, especially in the area of DevOps, so the VP of Engineering spent many late nights and weekends keeping the systems running. Like many startups they were making it work, but as their team grew they knew they needed to improve their deployment and environment ecosystem.\n\nInitially, they needed help building out their pre-production staging ecosystem. They first needed the traditional environments for staging and QA that were automatically deployed when code changed. They also liked the idea of having environments for every Pull Request to enable better code reviews and higher quality deploys.\n\nAfter adopting Release for pre-production and staging environments, Datasaur experienced 4x growth in their business and their production ecosystem became a challenge. Scaling to meet the demand of their users was an issue. There were production outages and the existing production environment didn’t afford them the ability to scale up and down with the demands of their users.\n\n#### How was this solved before\n\nIn the past, the typical solution to building out a pre-product environment ecosystem for a startup like Datasaur would be to just simply try and re-create the production environment manually. You’d end up with a single shared environment that all the developers used but soon you’d quickly realize that it, being a shared resource, became a bottleneck.\n\nBecause creating the second or third staging environment was time consuming, most startups at the stage of Datasaur wouldn’t invest in automatically created environments with each PR.\n\nYou’d also need to begin building out automation around CI/CD which would require more resources to create an automated workflow.\n\nSolving spikes in production usage would require manual intervention and constant monitoring to ensure you were keeping up with the demand. You may have used ECR with auto-scaling, but ensuring your application was architected to take advantage of auto-scaling is a time and resource heavy project.\n\nTypically all of these projects would require hiring more engineers, or more late nights from the most senior engineer or person responsible.\n\nA typical solution to migrate to a better production system might involve bringing in a DevOps person (or team) to create and manage the workflows that you’d need in order to utilize Kubernetes, but this is an expensive and very time consuming solution. This also won’t solve the issue of urgency for companies that need a solution now without a DevOps team, and without extensive Kubernetes knowledge.  \n\nAnother approach may have been to move to something like Heroku, but because Datasaur had a complex application, Heroku was not an option.\n\n#### A new solution\n\nDatasaur needed a new solution that could solve their pre-production environment problems in a fast, cost effective, and streamlined manner. Their application was multi service and complex, which is typically difficult to work with in other platforms such as Heroku. Because Release supports complex, multi-service applications we were able to easily replicate their complex environments.\n\nAs Datasaur grew, their lack of ample environments was causing them issues. They needed a consistent set of environments they wouldn’t need to worry about so they could focus on delivering value to their customers.\n\nHere at Release we were able to provide Datasaur with an Environments as a Service solution. They used Release to create permanent staging environments that tracked their mainline branch. Whenever a push to their mainline branch occured, the main staging environment was automatically updated. Datasaur created automated tests around this process to increase the quality of code deployed to production.\n\nBecause deployment automation from source control to environment is part of the Release platform, Datasaur didn’t have to spend time building out automation for CI/CD.\n\nEphemeral Environments come standard with Release, so they also started utilizing environments with every Pull Request that track feature branches to allow developers and stakeholders to preview changes before they were merged into the mainline branch.\n\nLastly, as Datasaur started experiencing rapid growth, we began speaking with them about running production environments in Kubernetes as a way to manage their growth. Because the deployment process and pre-production environment templates were already being run through Release and deployed into Kubernetes, they began seeing how Release could solve their production scaling issues.\n\nWe created a production EKS cluster for Datasaur in preparation for the move to Release powered production when Datasaur had scaling issues over one weekend. The VP of Engineering flipped the switch on DNS and Release began powering production in the middle of an outage to get them the capacity they needed to handle their growth. The lights came back on and they’ve been powered by Release ever since.\n\n#### Retrospective\n\nDatasaur came to us to get reproducible pre-production environments, but quickly saw the path to Kubernetes production as being the right choice. Release migrated Datasaur to Kubernetes seamlessly and without the typical company resources required to make that happen. Instead of hiring a complete DevOps team, and taking up to a year to be fully migrated to Kubernetes, we had them up and running in days, all with a DevOps team of one. They are now an enterprise customer using all of Release’s features.\n\n#### What’s next\n\nAs Datasaur continues to grow, they can more fully lean into the features of Release in production so they can scale their operation without having to bring on a full-time DevOps Team. As new engineers join the team, they will gain the benefits of an automated developer experience with environments on-demand. Datasaur won’t have to invest heavily in operational resources for their platform and, instead, will focus those hires on core product work to drive value for their business. They came to us for staging, but grew with us when they realized we could provide an easy, cost-effective, and time-saving way to use Kubernetes in all their environments.\n\n#### Liked most about Release\n\nIn addition to relying on Release as their Kubernetes-in-a-box solution, Datasaur also relies on us as their solutions architects. For example, when a random issue arises, we can help them decipher the problem and create a solution to fix it. We fuel their rocketship and have a hand in their mission to make sure they reach their end goal.\n\nHaving an extra set of eyes on their operation lets them know we’ve got their back. We monitor their system and catch things they need to know. When you’ve got a small DevOps team, it’s nice to know you’ve got an extra DevOps team paying attention without the cost typically associated with it.\n\n#### A word from the client\n\n> Release has enabled us to move quickly as a startup and deliver higher quality and better solutions to our customers. _We have an environment platform that would have taken us years to build, but instead we’ve been able to focus on our application. We’ve been able to stay lean with our budget, but we haven’t had to sacrifice capabilities. We can add new services and test them immediately. They’ve been a fantastic partner and we couldn’t have scaled to meet our rapid growth without them._\n\n###### Ivan Lee\n\n###### CEO • Datasaur\n\n![Photo of Ivan Lee, CEO of Datasaur](https://assets-global.website-files.com/603dd147c5b0a480611bd348/65e1e2903abc6243ce8efa37_Ivan%20Lee.png)\n\nIvan Lee\n\nCEO • Datasaur\n\n<CaseStudyCTA />\n",
          "code": "var Component=(()=>{var d=Object.create;var i=Object.defineProperty;var u=Object.getOwnPropertyDescriptor;var m=Object.getOwnPropertyNames;var p=Object.getPrototypeOf,g=Object.prototype.hasOwnProperty;var w=(n,e)=>()=>(e||n((e={exports:{}}).exports,e),e.exports),b=(n,e)=>{for(var a in e)i(n,a,{get:e[a],enumerable:!0})},s=(n,e,a,r)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let o of m(e))!g.call(n,o)&&o!==a&&i(n,o,{get:()=>e[o],enumerable:!(r=u(e,o))||r.enumerable});return n};var v=(n,e,a)=>(a=n!=null?d(p(n)):{},s(e||!n||!n.__esModule?i(a,\"default\",{value:n,enumerable:!0}):a,n)),y=n=>s(i({},\"__esModule\",{value:!0}),n);var h=w((T,l)=>{l.exports=_jsx_runtime});var R={};b(R,{default:()=>D,frontmatter:()=>f});var t=v(h()),f={title:\"How Release was able to easily replicate the complex environments of Datasaur.ai\",description:\"Datasaur is making data labeling simple.\",publishDate:\"2024-03-01T16:29:50.000Z\",logo:\"/case-study-images/1f668a1eca8e16461fe596b61ece5158.svg\",thumbnail:\"/case-study-images/8a406138434670fb4294918ed6cc6af7.png\"};function c(n){let e=Object.assign({h4:\"h4\",a:\"a\",span:\"span\",p:\"p\",blockquote:\"blockquote\",em:\"em\",h6:\"h6\",img:\"img\"},n.components),{CaseStudyCTA:a}=e;return a||x(\"CaseStudyCTA\",!0),(0,t.jsxs)(t.Fragment,{children:[(0,t.jsxs)(e.h4,{id:\"about-the-client\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#about-the-client\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"About the client\"]}),`\n`,(0,t.jsx)(e.p,{children:\"Datasaur is making data labeling simple. They set the standard for best practices in both data labeling and extracting valuable insights from raw data. By labeling data with Datasaur, companies are able to create great machine learning solutions.\"}),`\n`,(0,t.jsxs)(e.h4,{id:\"problem\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#problem\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Problem\"]}),`\n`,(0,t.jsx)(e.p,{children:\"When Datasaur came to Release, they were running their application on multiple EC2 nodes in AWS. The engineering team was small but growing, and the VP of Engineering was responsible for their Architecture, implementation, DevOps and SRE functions. They were a seed-round funded startup at the time but didn\\u2019t have a budget to grow headcount, especially in the area of DevOps, so the VP of Engineering spent many late nights and weekends keeping the systems running. Like many startups they were making it work, but as their team grew they knew they needed to improve their deployment and environment ecosystem.\"}),`\n`,(0,t.jsx)(e.p,{children:\"Initially, they needed help building out their pre-production staging ecosystem. They first needed the traditional environments for staging and QA that were automatically deployed when code changed. They also liked the idea of having environments for every Pull Request to enable better code reviews and higher quality deploys.\"}),`\n`,(0,t.jsx)(e.p,{children:\"After adopting Release for pre-production and staging environments, Datasaur experienced 4x growth in their business and their production ecosystem became a challenge. Scaling to meet the demand of their users was an issue. There were production outages and the existing production environment didn\\u2019t afford them the ability to scale up and down with the demands of their users.\"}),`\n`,(0,t.jsxs)(e.h4,{id:\"how-was-this-solved-before\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#how-was-this-solved-before\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"How was this solved before\"]}),`\n`,(0,t.jsx)(e.p,{children:\"In the past, the typical solution to building out a pre-product environment ecosystem for a startup like Datasaur would be to just simply try and re-create the production environment manually. You\\u2019d end up with a single shared environment that all the developers used but soon you\\u2019d quickly realize that it, being a shared resource, became a bottleneck.\"}),`\n`,(0,t.jsx)(e.p,{children:\"Because creating the second or third staging environment was time consuming, most startups at the stage of Datasaur wouldn\\u2019t invest in automatically created environments with each PR.\"}),`\n`,(0,t.jsx)(e.p,{children:\"You\\u2019d also need to begin building out automation around CI/CD which would require more resources to create an automated workflow.\"}),`\n`,(0,t.jsx)(e.p,{children:\"Solving spikes in production usage would require manual intervention and constant monitoring to ensure you were keeping up with the demand. You may have used ECR with auto-scaling, but ensuring your application was architected to take advantage of auto-scaling is a time and resource heavy project.\"}),`\n`,(0,t.jsx)(e.p,{children:\"Typically all of these projects would require hiring more engineers, or more late nights from the most senior engineer or person responsible.\"}),`\n`,(0,t.jsx)(e.p,{children:\"A typical solution to migrate to a better production system might involve bringing in a DevOps person (or team) to create and manage the workflows that you\\u2019d need in order to utilize Kubernetes, but this is an expensive and very time consuming solution. This also won\\u2019t solve the issue of urgency for companies that need a solution now without a DevOps team, and without extensive Kubernetes knowledge. \\xA0\"}),`\n`,(0,t.jsx)(e.p,{children:\"Another approach may have been to move to something like Heroku, but because Datasaur had a complex application, Heroku was not an option.\"}),`\n`,(0,t.jsxs)(e.h4,{id:\"a-new-solution\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#a-new-solution\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"A new solution\"]}),`\n`,(0,t.jsx)(e.p,{children:\"Datasaur needed a new solution that could solve their pre-production environment problems in a fast, cost effective, and streamlined manner. Their application was multi service and complex, which is typically difficult to work with in other platforms such as Heroku. Because Release supports complex, multi-service applications we were able to easily replicate their complex environments.\"}),`\n`,(0,t.jsx)(e.p,{children:\"As Datasaur grew, their lack of ample environments was causing them issues. They needed a consistent set of environments they wouldn\\u2019t need to worry about so they could focus on delivering value to their customers.\"}),`\n`,(0,t.jsx)(e.p,{children:\"Here at Release we were able to provide Datasaur with an Environments as a Service solution. They used Release to create permanent staging environments that tracked their mainline branch. Whenever a push to their mainline branch occured, the main staging environment was automatically updated. Datasaur created automated tests around this process to increase the quality of code deployed to production.\"}),`\n`,(0,t.jsx)(e.p,{children:\"Because deployment automation from source control to environment is part of the Release platform, Datasaur didn\\u2019t have to spend time building out automation for CI/CD.\"}),`\n`,(0,t.jsx)(e.p,{children:\"Ephemeral Environments come standard with Release, so they also started utilizing environments with every Pull Request that track feature branches to allow developers and stakeholders to preview changes before they were merged into the mainline branch.\"}),`\n`,(0,t.jsx)(e.p,{children:\"Lastly, as Datasaur started experiencing rapid growth, we began speaking with them about running production environments in Kubernetes as a way to manage their growth. Because the deployment process and pre-production environment templates were already being run through Release and deployed into Kubernetes, they began seeing how Release could solve their production scaling issues.\"}),`\n`,(0,t.jsx)(e.p,{children:\"We created a production EKS cluster for Datasaur in preparation for the move to Release powered production when Datasaur had scaling issues over one weekend. The VP of Engineering flipped the switch on DNS and Release began powering production in the middle of an outage to get them the capacity they needed to handle their growth. The lights came back on and they\\u2019ve been powered by Release ever since.\"}),`\n`,(0,t.jsxs)(e.h4,{id:\"retrospective\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#retrospective\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Retrospective\"]}),`\n`,(0,t.jsx)(e.p,{children:\"Datasaur came to us to get reproducible pre-production environments, but quickly saw the path to Kubernetes production as being the right choice. Release migrated Datasaur to Kubernetes seamlessly and without the typical company resources required to make that happen. Instead of hiring a complete DevOps team, and taking up to a year to be fully migrated to Kubernetes, we had them up and running in days, all with a DevOps team of one. They are now an enterprise customer using all of Release\\u2019s features.\"}),`\n`,(0,t.jsxs)(e.h4,{id:\"whats-next\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#whats-next\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"What\\u2019s next\"]}),`\n`,(0,t.jsx)(e.p,{children:\"As Datasaur continues to grow, they can more fully lean into the features of Release in production so they can scale their operation without having to bring on a full-time DevOps Team. As new engineers join the team, they will gain the benefits of an automated developer experience with environments on-demand. Datasaur won\\u2019t have to invest heavily in operational resources for their platform and, instead, will focus those hires on core product work to drive value for their business. They came to us for staging, but grew with us when they realized we could provide an easy, cost-effective, and time-saving way to use Kubernetes in all their environments.\"}),`\n`,(0,t.jsxs)(e.h4,{id:\"liked-most-about-release\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#liked-most-about-release\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Liked most about Release\"]}),`\n`,(0,t.jsx)(e.p,{children:\"In addition to relying on Release as their Kubernetes-in-a-box solution, Datasaur also relies on us as their solutions architects. For example, when a random issue arises, we can help them decipher the problem and create a solution to fix it. We fuel their rocketship and have a hand in their mission to make sure they reach their end goal.\"}),`\n`,(0,t.jsx)(e.p,{children:\"Having an extra set of eyes on their operation lets them know we\\u2019ve got their back. We monitor their system and catch things they need to know. When you\\u2019ve got a small DevOps team, it\\u2019s nice to know you\\u2019ve got an extra DevOps team paying attention without the cost typically associated with it.\"}),`\n`,(0,t.jsxs)(e.h4,{id:\"a-word-from-the-client\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#a-word-from-the-client\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"A word from the client\"]}),`\n`,(0,t.jsxs)(e.blockquote,{children:[`\n`,(0,t.jsxs)(e.p,{children:[\"Release has enabled us to move quickly as a startup and deliver higher quality and better solutions to our customers. \",(0,t.jsx)(e.em,{children:\"We have an environment platform that would have taken us years to build, but instead we\\u2019ve been able to focus on our application. We\\u2019ve been able to stay lean with our budget, but we haven\\u2019t had to sacrifice capabilities. We can add new services and test them immediately. They\\u2019ve been a fantastic partner and we couldn\\u2019t have scaled to meet our rapid growth without them.\"})]}),`\n`]}),`\n`,(0,t.jsxs)(e.h6,{id:\"ivan-lee\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#ivan-lee\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Ivan Lee\"]}),`\n`,(0,t.jsxs)(e.h6,{id:\"ceo--datasaur\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#ceo--datasaur\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"CEO \\u2022 Datasaur\"]}),`\n`,(0,t.jsx)(e.p,{children:(0,t.jsx)(e.img,{src:\"https://assets-global.website-files.com/603dd147c5b0a480611bd348/65e1e2903abc6243ce8efa37_Ivan%20Lee.png\",alt:\"Photo of Ivan Lee, CEO of Datasaur\"})}),`\n`,(0,t.jsx)(e.p,{children:\"Ivan Lee\"}),`\n`,(0,t.jsx)(e.p,{children:\"CEO \\u2022 Datasaur\"}),`\n`,(0,t.jsx)(a,{})]})}function k(n={}){let{wrapper:e}=n.components||{};return e?(0,t.jsx)(e,Object.assign({},n,{children:(0,t.jsx)(c,n)})):c(n)}var D=k;function x(n,e){throw new Error(\"Expected \"+(e?\"component\":\"object\")+\" `\"+n+\"` to be defined: you likely forgot to import, pass, or provide it.\")}return y(R);})();\n;return Component;"
        },
        "_id": "case-studies/content/datasaurai.mdx",
        "_raw": {
          "sourceFilePath": "case-studies/content/datasaurai.mdx",
          "sourceFileName": "datasaurai.mdx",
          "sourceFileDir": "case-studies/content",
          "contentType": "mdx",
          "flattenedPath": "case-studies/content/datasaurai"
        },
        "type": "CaseStudy",
        "computedSlug": "datasaurai"
      },
      "documentHash": "1739393595033",
      "hasWarnings": false,
      "documentTypeName": "CaseStudy"
    },
    "case-studies/content/debtbook.mdx": {
      "document": {
        "title": "How DebtBook Ships 6x Faster with Release",
        "logo": "/case-study-images/8d8d80bfcbfdc28a8edbc153e06b3aa5.svg",
        "description": "",
        "publishDate": "2024-03-04T16:54:20.000Z",
        "thumbnail": "/case-study-images/ab47393e1f11c0b99fdd85e76f9fdd4e.svg",
        "companySize": "200+",
        "industry": "Financial Services",
        "location": "North Carolina, USA",
        "developmentVelocity": "<p id=\"\">6x improvement in development velocity</p>",
        "developerExperience": "<p id=\"\">100% adoption of ephemeral environments</p>",
        "leanOperations": "<p>Focusing engineering efforts on the core product</p>",
        "body": {
          "raw": "\n**Shipping features and gathering feedback took DebtBook too long, making engineers context switch, instead of focusing on timely issues customers cared about. By implementing Release, DebtBook accelerated development velocity six times, shortened feedback loops, and brought customers closer to the product-building cycle, while improving the overall developer experience.**\n\nDebtBook tackles complex accounting and reporting challenges for public sector finance teams. They handle debt, lease, subscription management, and compliance reporting to ensure taxpayers’ funds are spent wisely and transparently. With just over 200 employees distributed throughout the United States, DebtBook serves more than 2,000 customers in government, higher education, and healthcare sectors.\n\n#### Slowed Down by Complexity\n\nDebtBook’s customers operate in a highly regulated, constantly evolving environment that demands not just compliance but efficiency and transparency in managing taxpayer funds. This drives DebtBook to deliver a safe, secure, and robust set of tools with little room for error. To meet the complex and high standard, DebtBook implemented a rigorous software development process that inevitably became long and burdensome.\n\nComplex shared environments forced developers to move their individual code changes through multiple stages and multiple shared environments for validation and confidence building. The process was slow, frustrating, and prone to error, which caused re-work and additional toil. On average it took 40 days to release changes and new features, well over the desired timeline.\n\nThe feedback also trickled slowly. Developers got relevant product and customer feedback weeks or months after they finished working on a specific feature. This made them context-switch to an old issue and lose momentum on current tasks.\n\n**“Our shared environments setup was too slow and cumbersome to accommodate timely feedback. We knew we could ease these frustrations, shorten the feedback loop and speed up our deployment process, with the right tools,”** said Michael Gorsuch, Director of Infrastructure for DebtBook.\n\nDebtBook needed an ephemeral environments platform to remove the shared environments bottlenecks, shorten the feedback loops and improve developer experience.\n\n#### Find the Right Fit\n\nDebtBook maintains leverage and delivers a differentiated product in the market, by remaining lean, and focusing their engineering efforts on the core product. Their infrastructure team helps engineering win, by delivering a set of capabilities for moving faster, more securely, and with less friction. So when deciding on ephemeral environments solutions, they quickly rejected the build option as it would take too long, cost too much, and would divert attention from more important tasks.\n\n**“On the infrastructure side, the more we have to build ourselves, the more it weighs us down. So you have to be careful about what you build vs. what you buy,”** shared Gorsuch.\n\nThe team evaluated potential partners with a goal of building a working prototype within a week, finding Release to be the most flexible, customizable, and extendable among the options.\n\n**“Release proof of concept worked end-to-end on the first day, giving us another boost of confidence in the product,”** said Gorsuch.\n\n#### Accelerate Software Delivery with Release\n\nEngineers quickly adopted Release and continue to provide positive feedback on improved focus and velocity. Within a month Release was incorporated into all major development workflows, speeding up processes and removing frustration.\n\n**“Onboarding with Release was a truly collaborative experience, allowing us to fine tune the product and improve our process, making both work together smoothly,”** said Gorsuch.\n\nCommit-to-customer cycle that used to take well over a month is now completed in under a week. Now developers spin up a near-production environment in minutes, instantly share their work-in-progress with product and peers, get relevant feedback, and release features quicker than ever before. Velocity improved six times and is continuing to improve with ongoing refinement.\n\nAn unexpected aspect of implementing Release was a culture shift that followed. Reducing friction and context-switching allowed engineers to focus more intentionally on the features they were building. So instead of perfecting the code early on they now drill in into the “why” behind each line and iterate on feedback early and often. This creates features that truly align with the needs of the customer and gives engineers the momentum to take the product to the next level.\n\n**“Release allows the right feedback to come early, so everyone s getting excited, they start to interact with the feature, they can t wait for it to get out there, to get it right for the customer. And then the team helps get the code right. That s how we work now.”** said Gorsuch.\n\nRelease became an integral part of the development workflow at DebtBook, accelerating new feature development, shortening feedback loops, and improving developers' experience. All while maintaining a lean infrastructure team and optimizing internal resources to focus on the core product.\n\n#### Faster, Leaner, and more Collaborative Releases\n\nDebtBook, who delivers sophisticated digital products to internal and external customers in the cloud, is on a mission to improve operations in three key areas: Development Velocity, Developer Experience, and Lean Operations.\n\nImplementing Release helped them achieved the following outcomes:\n\n**Development Velocity -> 6x improvement in development velocity.** Went from shipping new features less than once a month to releasing new functionality multiple times a week, continuously improving to move faster.\n\n**Developer Experience -> 100% adoption of ephemeral environments.** Shifted feedback earlier in the process, allowing engineers to collaborate and share ideas freely, and get features in the hands of customers faster.\n\n**Lean operations -> Focusing engineering efforts on the core product.** Leveraging resources of a small infrastructure team to deliver key functionality that help the engineering team win.\n\n**“With Release we're getting a scalable and sustainable environments platform with a far better experience than we had before. It changes everything for us.”** said the Director of Infrastructure\n\n#### Consistent Improvement\n\nImplementing the Release ephemeral environments platform is just the beginning of the productive partnership between Release and DebtBook. As old roadblocks are removed, and the quality of collaboration increases, new opportunities for improvement come up, and Release is ready for it.\n\n**“Working with Release is like working with a trusted partner and an extension of our team. I know we’re in it together and will continue to build great products together.”** summed up Gorsuch\n\n<CaseStudyCTA />\n",
          "code": "var Component=(()=>{var h=Object.create;var r=Object.defineProperty;var p=Object.getOwnPropertyDescriptor;var m=Object.getOwnPropertyNames;var u=Object.getPrototypeOf,g=Object.prototype.hasOwnProperty;var f=(n,e)=>()=>(e||n((e={exports:{}}).exports,e),e.exports),v=(n,e)=>{for(var o in e)r(n,o,{get:e[o],enumerable:!0})},s=(n,e,o,a)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let i of m(e))!g.call(n,i)&&i!==o&&r(n,i,{get:()=>e[i],enumerable:!(a=p(e,i))||a.enumerable});return n};var w=(n,e,o)=>(o=n!=null?h(u(n)):{},s(e||!n||!n.__esModule?r(o,\"default\",{value:n,enumerable:!0}):o,n)),b=n=>s(r({},\"__esModule\",{value:!0}),n);var c=f((S,l)=>{l.exports=_jsx_runtime});var R={};v(R,{default:()=>x,frontmatter:()=>y});var t=w(c()),y={title:\"How DebtBook Ships 6x Faster with Release\",description:\"\",publishDate:\"2024-03-04T16:54:20.000Z\",logo:\"/case-study-images/8d8d80bfcbfdc28a8edbc153e06b3aa5.svg\",thumbnail:\"/case-study-images/ab47393e1f11c0b99fdd85e76f9fdd4e.svg\",companySize:\"200+\",industry:\"Financial Services\",location:\"North Carolina, USA\",developmentVelocity:'<p id=\"\">6x improvement in development velocity</p>',developerExperience:'<p id=\"\">100% adoption of ephemeral environments</p>',leanOperations:\"<p>Focusing engineering efforts on the core product</p>\"};function d(n){let e=Object.assign({p:\"p\",strong:\"strong\",h4:\"h4\",a:\"a\",span:\"span\"},n.components),{CaseStudyCTA:o}=e;return o||D(\"CaseStudyCTA\",!0),(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(e.p,{children:(0,t.jsx)(e.strong,{children:\"Shipping features and gathering feedback took DebtBook too long, making engineers context switch, instead of focusing on timely issues customers cared about. By implementing Release, DebtBook accelerated development velocity six times, shortened feedback loops, and brought customers closer to the product-building cycle, while improving the overall developer experience.\"})}),`\n`,(0,t.jsx)(e.p,{children:\"DebtBook tackles complex accounting and reporting challenges for public sector finance teams. They handle debt, lease, subscription management, and compliance reporting to ensure taxpayers\\u2019 funds are spent wisely and transparently. With just over 200 employees distributed throughout the United States, DebtBook serves more than 2,000 customers in government, higher education, and healthcare sectors.\"}),`\n`,(0,t.jsxs)(e.h4,{id:\"slowed-down-by-complexity\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#slowed-down-by-complexity\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Slowed Down by Complexity\"]}),`\n`,(0,t.jsx)(e.p,{children:\"DebtBook\\u2019s customers operate in a highly regulated, constantly evolving environment that demands not just compliance but efficiency and transparency in managing taxpayer funds. This drives DebtBook to deliver a safe, secure, and robust set of tools with little room for error. To meet the complex and high standard, DebtBook implemented a rigorous software development process that inevitably became long and burdensome.\"}),`\n`,(0,t.jsx)(e.p,{children:\"Complex shared environments forced developers to move their individual code changes through multiple stages and multiple shared environments for validation and confidence building. The process was slow, frustrating, and prone to error, which caused re-work and additional toil. On average it took 40 days to release changes and new features, well over the desired timeline.\"}),`\n`,(0,t.jsx)(e.p,{children:\"The feedback also trickled slowly. Developers got relevant product and customer feedback weeks or months after they finished working on a specific feature. This made them context-switch to an old issue and lose momentum on current tasks.\"}),`\n`,(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:\"\\u201COur shared environments setup was too slow and cumbersome to accommodate timely feedback. We knew we could ease these frustrations, shorten the feedback loop and speed up our deployment process, with the right tools,\\u201D\"}),\" said Michael Gorsuch, Director of Infrastructure for DebtBook.\"]}),`\n`,(0,t.jsx)(e.p,{children:\"DebtBook needed an ephemeral environments platform to remove the shared environments bottlenecks, shorten the feedback loops and improve developer experience.\"}),`\n`,(0,t.jsxs)(e.h4,{id:\"find-the-right-fit\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#find-the-right-fit\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Find the Right Fit\"]}),`\n`,(0,t.jsx)(e.p,{children:\"DebtBook maintains leverage and delivers a differentiated product in the market, by remaining lean, and focusing their engineering efforts on the core product. Their infrastructure team helps engineering win, by delivering a set of capabilities for moving faster, more securely, and with less friction. So when deciding on ephemeral environments solutions, they quickly rejected the build option as it would take too long, cost too much, and would divert attention from more important tasks.\"}),`\n`,(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:\"\\u201COn the infrastructure side, the more we have to build ourselves, the more it weighs us down. So you have to be careful about what you build vs. what you buy,\\u201D\"}),\" shared Gorsuch.\"]}),`\n`,(0,t.jsx)(e.p,{children:\"The team evaluated potential partners with a goal of building a working prototype within a week, finding Release to be the most flexible, customizable, and extendable among the options.\"}),`\n`,(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:\"\\u201CRelease proof of concept worked end-to-end on the first day, giving us another boost of confidence in the product,\\u201D\"}),\" said Gorsuch.\"]}),`\n`,(0,t.jsxs)(e.h4,{id:\"accelerate-software-delivery-with-release\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#accelerate-software-delivery-with-release\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Accelerate Software Delivery with Release\"]}),`\n`,(0,t.jsx)(e.p,{children:\"Engineers quickly adopted Release and continue to provide positive feedback on improved focus and velocity. Within a month Release was incorporated into all major development workflows, speeding up processes and removing frustration.\"}),`\n`,(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:\"\\u201COnboarding with Release was a truly collaborative experience, allowing us to fine tune the product and improve our process, making both work together smoothly,\\u201D\"}),\" said Gorsuch.\"]}),`\n`,(0,t.jsx)(e.p,{children:\"Commit-to-customer cycle that used to take well over a month is now completed in under a week. Now developers spin up a near-production environment in minutes, instantly share their work-in-progress with product and peers, get relevant feedback, and release features quicker than ever before. Velocity improved six times and is continuing to improve with ongoing refinement.\"}),`\n`,(0,t.jsx)(e.p,{children:\"An unexpected aspect of implementing Release was a culture shift that followed. Reducing friction and context-switching allowed engineers to focus more intentionally on the features they were building. So instead of perfecting the code early on they now drill in into the \\u201Cwhy\\u201D behind each line and iterate on feedback early and often. This creates features that truly align with the needs of the customer and gives engineers the momentum to take the product to the next level.\"}),`\n`,(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:\"\\u201CRelease allows the right feedback to come early, so everyone s getting excited, they start to interact with the feature, they can t wait for it to get out there, to get it right for the customer. And then the team helps get the code right. That s how we work now.\\u201D\"}),\" said Gorsuch.\"]}),`\n`,(0,t.jsx)(e.p,{children:\"Release became an integral part of the development workflow at DebtBook, accelerating new feature development, shortening feedback loops, and improving developers' experience. All while maintaining a lean infrastructure team and optimizing internal resources to focus on the core product.\"}),`\n`,(0,t.jsxs)(e.h4,{id:\"faster-leaner-and-more-collaborative-releases\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#faster-leaner-and-more-collaborative-releases\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Faster, Leaner, and more Collaborative Releases\"]}),`\n`,(0,t.jsx)(e.p,{children:\"DebtBook, who delivers sophisticated digital products to internal and external customers in the cloud, is on a mission to improve operations in three key areas: Development Velocity, Developer Experience, and Lean Operations.\"}),`\n`,(0,t.jsx)(e.p,{children:\"Implementing Release helped them achieved the following outcomes:\"}),`\n`,(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:\"Development Velocity -> 6x improvement in development velocity.\"}),\" Went from shipping new features less than once a month to releasing new functionality multiple times a week, continuously improving to move faster.\"]}),`\n`,(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:\"Developer Experience -> 100% adoption of ephemeral environments.\"}),\" Shifted feedback earlier in the process, allowing engineers to collaborate and share ideas freely, and get features in the hands of customers faster.\"]}),`\n`,(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:\"Lean operations -> Focusing engineering efforts on the core product.\"}),\" Leveraging resources of a small infrastructure team to deliver key functionality that help the engineering team win.\"]}),`\n`,(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:\"\\u201CWith Release we're getting a scalable and sustainable environments platform with a far better experience than we had before. It changes everything for us.\\u201D\"}),\" said the Director of Infrastructure\"]}),`\n`,(0,t.jsxs)(e.h4,{id:\"consistent-improvement\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#consistent-improvement\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Consistent Improvement\"]}),`\n`,(0,t.jsx)(e.p,{children:\"Implementing the Release ephemeral environments platform is just the beginning of the productive partnership between Release and DebtBook. As old roadblocks are removed, and the quality of collaboration increases, new opportunities for improvement come up, and Release is ready for it.\"}),`\n`,(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:\"\\u201CWorking with Release is like working with a trusted partner and an extension of our team. I know we\\u2019re in it together and will continue to build great products together.\\u201D\"}),\" summed up Gorsuch\"]}),`\n`,(0,t.jsx)(o,{})]})}function k(n={}){let{wrapper:e}=n.components||{};return e?(0,t.jsx)(e,Object.assign({},n,{children:(0,t.jsx)(d,n)})):d(n)}var x=k;function D(n,e){throw new Error(\"Expected \"+(e?\"component\":\"object\")+\" `\"+n+\"` to be defined: you likely forgot to import, pass, or provide it.\")}return b(R);})();\n;return Component;"
        },
        "_id": "case-studies/content/debtbook.mdx",
        "_raw": {
          "sourceFilePath": "case-studies/content/debtbook.mdx",
          "sourceFileName": "debtbook.mdx",
          "sourceFileDir": "case-studies/content",
          "contentType": "mdx",
          "flattenedPath": "case-studies/content/debtbook"
        },
        "type": "CaseStudy",
        "computedSlug": "debtbook"
      },
      "documentHash": "1739393595033",
      "hasWarnings": false,
      "documentTypeName": "CaseStudy"
    },
    "case-studies/content/monad.mdx": {
      "document": {
        "title": "B2B SaaS On-premise/VPC environments made easy - Monad case study",
        "logo": "/case-study-images/a4e636f0dabe5d84067ba1ad49581fbb.svg",
        "description": "Monad is a security company that collects and aggregates data from the information security tools used by an enterprise and generates insights for a more holistic view of their security posture. ",
        "publishDate": "2024-02-29T22:08:15.000Z",
        "thumbnail": "/case-study-images/a8772fde334097dc407dd694c3165473.png",
        "body": {
          "raw": "\n#### About the client\n\nMonad is a security company that collects and aggregates data from the information security tools used by an enterprise and generates insights for a more holistic view of their security posture. They deliver their software as an “on-cloud” solution into their customers’ VPCs. \n\n#### Release Technologies used:**‍**\n\n- Preview Environments with every Pull Request for User Acceptance Testing\n- Permanent Staging Environments\n- On-Demand Sales Demo Environments for reproducible and consistent sales demos across industry verticals.\n- Instant Datasets for instantly loading appropriate data into sales and preview environments.\n- On-cloud distribution for delivering Monad “on-cloud” into their customer VPCs.\n- Production Environments for delivery of the Monad hosted SaaS solution\n\n#### Problem\n\nMonad came to Release early in their evolution as a startup before they had invested in building out a DevOps team. They initially approached Release for help getting their software distributed into one of their early customer’s cloud accounts running in AWS. They had a looming deadline and had yet to begin the work to design and implement a way to deliver their software into their customer’s cloud/VPC.\n\nWith this looming deadline and a need to move quickly, Monad needed a solution that could easily define their application so it could be reproduced and delivered within a few weeks. Their application was a combination of open-source software, connections to datasets like Snowflake, and custom in-house software. The complexity of the application required more than just containerizing their solution and offering it as a docker image. There were many interdependent services and the architecture resembled a micro-services ecosystem. \n\nThe challenge they faced was ‘How are we going to take this architecture and deliver it into our customer’s cloud, while also having the ability to upgrade and monitor the software over time?’\n\nWith only a few weeks to implement and a team that was overworked without deep DevOps skills, they approached Release as they knew we had a system that could easily reproduce environments and deploy into AWS.\n\n#### How this would have been solved before\n\nIn some cases, when the application is just one service, a company could build a container and allow their customers to deploy the container on their own. In other situations where the application was more complex with many services, a company would have no other option than to build out a custom solution built internally. Usually this would involve a DevOps engineer (or many) to write custom scripts that would essentially define a rigid definition of their application, a mechanism to connect to their customer’s AWS account with appropriate permissions and some sort of ability to upgrade the software in a controlled way when new releases were available. \n\nThe upgrade mechanism is especially difficult as it needs to allow for the customer to test the upgrade and deploy when they are comfortable doing so. Monitoring is also a tricky problem to solve as the solution would need some way to send alerts and events out of the customer’s VPC for monitoring and analysis.\n\nIn the end, all of this is custom and difficult to implement and could easily take a team of highly skilled DevOps engineers many months or years to implement. This is also not a core product value offering and conflicts with resources that could be devoted to security product features.\n\n#### A new solution\n\nBecause Release uses a standard application definition, Monad was able to define their multi-service application in a Release Application Template. Additionally, since Release is designed to deploy into a Kubernetes cluster running within a customer’s AWS account, Monad was able to leverage this capability to easily deploy their solution into their customer’s AWS account and gain the benefits of Kubernetes for their application. Release provided Monad with a custom landing page which allowed their customers to connect their AWS account with Monad for a seamless installation process. \n\nThe ability to add monitoring within the Kubernetes cluster with sidecar containers is built into the Release solution. Release provided core infrastructure monitoring while Monad was able to install their own agent to monitor application level logs and events.\n\nLastly, because Release easily allows for creation of new environments, when a new version of Monad was available to their customers, a preview version of the upgrade could be distributed into a “staging” environment within their customers AWS account for testing and preview. When the customer was ready to upgrade, they simply approved the upgrade and the new environment became primary and the old version was automatically deprecated.\n\nOne of the other benefits of Release that Monad is leveraging is using Release for their own internal development. Because their development/pre-production ecosystem and their customer deployment solution is on the same platform, Monad is able to quickly iterate, test, and deploy ephemeral environments. This gives them the ability to deliver quickly without worrying about managing their pre-production environment and infrastructure. Monad uses the same tool to deliver seamlessly all the way from development to testing to user acceptance testing to production.\n\n#### What’s next\n\nMonad will be using Release to deploy and manage their own production environment for hosted customers. This will bring their entire pre-production, production, and SaaS on-cloud environment ecosystem completely under one roof. This greatly simplifies their entire product development lifecycle as all environments from dev to prod to on-cloud delivery is identical. \n\n#### A word from the client\n\n> Building out a team to manage infrastructure is a huge expense and would have taken us many months to build an on-premise/on-cloud delivery platform. We would have been on the hook to maintain it and improve it and that would have been at the expense of building our core product. Release has enabled us to focus on building our product without worrying about how we’re going to develop, deploy, and deliver it to our customers. It’s fantastic.\n\n###### Jacolon Walker\n\n###### CTO • Monad\n\n<CaseStudyCTA />\n",
          "code": "var Component=(()=>{var h=Object.create;var a=Object.defineProperty;var u=Object.getOwnPropertyDescriptor;var m=Object.getOwnPropertyNames;var p=Object.getPrototypeOf,f=Object.prototype.hasOwnProperty;var w=(o,e)=>()=>(e||o((e={exports:{}}).exports,e),e.exports),y=(o,e)=>{for(var t in e)a(o,t,{get:e[t],enumerable:!0})},r=(o,e,t,s)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let i of m(e))!f.call(o,i)&&i!==t&&a(o,i,{get:()=>e[i],enumerable:!(s=u(e,i))||s.enumerable});return o};var g=(o,e,t)=>(t=o!=null?h(p(o)):{},r(e||!o||!o.__esModule?a(t,\"default\",{value:o,enumerable:!0}):t,o)),v=o=>r(a({},\"__esModule\",{value:!0}),o);var c=w((S,l)=>{l.exports=_jsx_runtime});var x={};y(x,{default:()=>M,frontmatter:()=>b});var n=g(c()),b={title:\"B2B SaaS On-premise/VPC environments made easy - Monad case study\",description:\"Monad is a security company that collects and aggregates data from the information security tools used by an enterprise and generates insights for a more holistic view of their security posture. \",publishDate:\"2024-02-29T22:08:15.000Z\",logo:\"/case-study-images/a4e636f0dabe5d84067ba1ad49581fbb.svg\",thumbnail:\"/case-study-images/a8772fde334097dc407dd694c3165473.png\"};function d(o){let e=Object.assign({h4:\"h4\",a:\"a\",span:\"span\",p:\"p\",strong:\"strong\",ul:\"ul\",li:\"li\",blockquote:\"blockquote\",h6:\"h6\"},o.components),{CaseStudyCTA:t}=e;return t||T(\"CaseStudyCTA\",!0),(0,n.jsxs)(n.Fragment,{children:[(0,n.jsxs)(e.h4,{id:\"about-the-client\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#about-the-client\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"About the client\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Monad is a security company that collects and aggregates data from the information security tools used by an enterprise and generates insights for a more holistic view of their security posture. They deliver their software as an \\u201Con-cloud\\u201D solution into their customers\\u2019 VPCs.\\xA0\"}),`\n`,(0,n.jsxs)(e.h4,{id:\"release-technologies-used\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#release-technologies-used\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Release Technologies used:\",(0,n.jsx)(e.strong,{children:\"\\u200D\"})]}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsx)(e.li,{children:\"Preview Environments with every Pull Request for User Acceptance Testing\"}),`\n`,(0,n.jsx)(e.li,{children:\"Permanent Staging Environments\"}),`\n`,(0,n.jsx)(e.li,{children:\"On-Demand Sales Demo Environments for reproducible and consistent sales demos across industry verticals.\"}),`\n`,(0,n.jsx)(e.li,{children:\"Instant Datasets for instantly loading appropriate data into sales and preview environments.\"}),`\n`,(0,n.jsx)(e.li,{children:\"On-cloud distribution for delivering Monad \\u201Con-cloud\\u201D into their customer VPCs.\"}),`\n`,(0,n.jsx)(e.li,{children:\"Production Environments for delivery of the Monad hosted SaaS solution\"}),`\n`]}),`\n`,(0,n.jsxs)(e.h4,{id:\"problem\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#problem\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Problem\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Monad came to Release early in their evolution as a startup before they had invested in building out a DevOps team. They initially approached Release for help getting their software distributed into one of their early customer\\u2019s cloud accounts running in AWS. They had a looming deadline and had yet to begin the work to design and implement a way to deliver their software into their customer\\u2019s cloud/VPC.\"}),`\n`,(0,n.jsx)(e.p,{children:\"With this looming deadline and a need to move quickly, Monad needed a solution that could easily define their application so it could be reproduced and delivered within a few weeks. Their application was a combination of open-source software, connections to datasets like Snowflake, and custom in-house software. The complexity of the application required more than just containerizing their solution and offering it as a docker image. There were many interdependent services and the architecture resembled a micro-services ecosystem.\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:\"The challenge they faced was \\u2018How are we going to take this architecture and deliver it into our customer\\u2019s cloud, while also having the ability to upgrade and monitor the software over time?\\u2019\"}),`\n`,(0,n.jsx)(e.p,{children:\"With only a few weeks to implement and a team that was overworked without deep DevOps skills, they approached Release as they knew we had a system that could easily reproduce environments and deploy into AWS.\"}),`\n`,(0,n.jsxs)(e.h4,{id:\"how-this-would-have-been-solved-before\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#how-this-would-have-been-solved-before\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"How this would have been solved before\"]}),`\n`,(0,n.jsx)(e.p,{children:\"In some cases, when the application is just one service, a company could build a container and allow their customers to deploy the container on their own. In other situations where the application was more complex with many services, a company would have no other option than to build out a custom solution built internally. Usually this would involve a DevOps engineer (or many) to write custom scripts that would essentially define a rigid definition of their application, a mechanism to connect to their customer\\u2019s AWS account with appropriate permissions and some sort of ability to upgrade the software in a controlled way when new releases were available.\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:\"The upgrade mechanism is especially difficult as it needs to allow for the customer to test the upgrade and deploy when they are comfortable doing so. Monitoring is also a tricky problem to solve as the solution would need some way to send alerts and events out of the customer\\u2019s VPC for monitoring and analysis.\"}),`\n`,(0,n.jsx)(e.p,{children:\"In the end, all of this is custom and difficult to implement and could easily take a team of highly skilled DevOps engineers many months or years to implement. This is also not a core product value offering and conflicts with resources that could be devoted to security product features.\"}),`\n`,(0,n.jsxs)(e.h4,{id:\"a-new-solution\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#a-new-solution\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"A new solution\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Because Release uses a standard application definition, Monad was able to define their multi-service application in a Release Application Template. Additionally, since Release is designed to deploy into a Kubernetes cluster running within a customer\\u2019s AWS account, Monad was able to leverage this capability to easily deploy their solution into their customer\\u2019s AWS account and gain the benefits of Kubernetes for their application. Release provided Monad with a custom landing page which allowed their customers to connect their AWS account with Monad for a seamless installation process.\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:\"The ability to add monitoring within the Kubernetes cluster with sidecar containers is built into the Release solution. Release provided core infrastructure monitoring while Monad was able to install their own agent to monitor application level logs and events.\"}),`\n`,(0,n.jsx)(e.p,{children:\"Lastly, because Release easily allows for creation of new environments, when a new version of Monad was available to their customers, a preview version of the upgrade could be distributed into a \\u201Cstaging\\u201D environment within their customers AWS account for testing and preview. When the customer was ready to upgrade, they simply approved the upgrade and the new environment became primary and the old version was automatically deprecated.\"}),`\n`,(0,n.jsx)(e.p,{children:\"One of the other benefits of Release that Monad is leveraging is using Release for their own internal development. Because their development/pre-production ecosystem and their customer deployment solution is on the same platform, Monad is able to quickly iterate, test, and deploy ephemeral environments. This gives them the ability to deliver quickly without worrying about managing their pre-production environment and infrastructure. Monad uses the same tool to deliver seamlessly all the way from development to testing to user acceptance testing to production.\"}),`\n`,(0,n.jsxs)(e.h4,{id:\"whats-next\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#whats-next\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"What\\u2019s next\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Monad will be using Release to deploy and manage their own production environment for hosted customers. This will bring their entire pre-production, production, and SaaS on-cloud environment ecosystem completely under one roof. This greatly simplifies their entire product development lifecycle as all environments from dev to prod to on-cloud delivery is identical.\\xA0\"}),`\n`,(0,n.jsxs)(e.h4,{id:\"a-word-from-the-client\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#a-word-from-the-client\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"A word from the client\"]}),`\n`,(0,n.jsxs)(e.blockquote,{children:[`\n`,(0,n.jsx)(e.p,{children:\"Building out a team to manage infrastructure is a huge expense and would have taken us many months to build an on-premise/on-cloud delivery platform. We would have been on the hook to maintain it and improve it and that would have been at the expense of building our core product. Release has enabled us to focus on building our product without worrying about how we\\u2019re going to develop, deploy, and deliver it to our customers. It\\u2019s fantastic.\"}),`\n`]}),`\n`,(0,n.jsxs)(e.h6,{id:\"jacolon-walker\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#jacolon-walker\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Jacolon Walker\"]}),`\n`,(0,n.jsxs)(e.h6,{id:\"cto--monad\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#cto--monad\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"CTO \\u2022 Monad\"]}),`\n`,(0,n.jsx)(t,{})]})}function k(o={}){let{wrapper:e}=o.components||{};return e?(0,n.jsx)(e,Object.assign({},o,{children:(0,n.jsx)(d,o)})):d(o)}var M=k;function T(o,e){throw new Error(\"Expected \"+(e?\"component\":\"object\")+\" `\"+o+\"` to be defined: you likely forgot to import, pass, or provide it.\")}return v(x);})();\n;return Component;"
        },
        "_id": "case-studies/content/monad.mdx",
        "_raw": {
          "sourceFilePath": "case-studies/content/monad.mdx",
          "sourceFileName": "monad.mdx",
          "sourceFileDir": "case-studies/content",
          "contentType": "mdx",
          "flattenedPath": "case-studies/content/monad"
        },
        "type": "CaseStudy",
        "computedSlug": "monad"
      },
      "documentHash": "1739393595033",
      "hasWarnings": false,
      "documentTypeName": "CaseStudy"
    },
    "case-studies/content/mosaic.mdx": {
      "document": {
        "title": "Building the Right Solution for Residential Developers for Mosaic",
        "logo": "/case-study-images/d7dc0ce8bef4cbf1471555186b5abbdc.svg",
        "description": "Mosaic.us is a technology-powered general contractor.",
        "publishDate": "2024-03-01T13:52:39.000Z",
        "thumbnail": "/case-study-images/93f9ffb4b6907969c2ff64a853cc57b1.png",
        "body": {
          "raw": "\nMosaic.us is a technology-powered general contractor. They work with residential real estate developers and homebuilders to construct homes after initial design and planning work is complete. Mosaic’s internal tech platform increases communication and collaboration between the dozens of teams that have to work together to complete construction projects.\n\n#### Release Technologies Used\n\n- Instant Datasets for instantly loading appropriate data into sales and preview environments.\n\n#### Challenge\n\nHomebuilding is among the most complex businesses out there. Between design, permitting and construction any number of things can go wrong, leading to costly delays. There are often 40 to 50 different crews of subcontractors to manage in every home construction project, and supply scarcity is always a challenge.\n\nMuch of this work has historically been done with pen and paper, or by spreadsheet at best.\n\nMosaic uses technology to digitize every aspect of the homebuilding process. That means dozens of solutions covering disparate workflows, including data reports about timing and budgets, onboarding for subcontractors, bidding management and day-to-day site operations.\n\nMosaic has an engineering team of roughly 15, split across three product teams. In creating a wide-ranging platform, they ran into a common problem that comes with quickly iterating on new products. As the team grew, and as they built out the service, they ran into a ceiling of what they could do.\n\nWith just a few environments, and a significant time component for creating new ones, the logistics of testing new features proved challenging. That put a ceiling on how quickly Mosaic could round out its platform to capture every piece of the exhaustive process that is homebuilding.\n\n#### What they were doing before\n\nFor a long time, Mosaic used Terraform on top of Heroku for creating environments (prior to migrating to AWS), without any kind of CI/CD pipeline. That was fine for a while when the engineering team was small, but over time, as the team grew and the complexity compounded, bottlenecks emerged. Limits on the number of apps that could be integrated with Heroku – they had over 150 integrated because of the sheer volume of different pull requests – held them back.\n\nHeroku was not particularly reliable. Frequent memory errors and difficulty connecting to the API shut down the ability to continue working on new and updated features.\n\nExtensive downtime meant teams of high-priced engineers sometimes spent several hours in a day just waiting to be able to work in their environments. With these wait times, a few hours here and a few hours there, engineer downtime quickly added up to weeks of delay for updates and new products.\n\nBecause there were limits on the number of apps, when developers wanted to create a new environment, they had to take inventory of their environments. And sometimes that meant deleting old environments to make room for new ones.\n\nSpinning up new environments was doable, but their solutions didn’t preload the necessary data for accurate testing environments. Because of this, each new environment took about half an hour to create; collectively DevOps teams spent days just creating new environments.\n\n#### A New Solution\n\nMosaic initially considered building an in-house solution, which would have required a significant time investment from their most highly skilled technical teams. Instead, they turned to Release’s Instant Datasets, which allowed Mosaic to pre-populate environments with the data needed for a full picture of how the software would perform. This was important for Mosaic, as executives, sales teams and others frequently wanted new environments to show individual features and updates to customers, or to play with a new idea.\n\nThe previous setup of manually creating and populating made these quick environments a painstaking endeavor. With Release, Mosaic can now spin up or shut down new environments with just a couple clicks.\n\nMosaic quickly switched over to Release, digging deep into their code in GitHub and setting up containers. This integration could have taken weeks, but with Release, Mosaic was able to quickly increase speed and reliability of their environments.\n\n#### What’s Next\n\nMosaic is looking into Release’s Docker Compose feature, as it continues to improve upon its environments. This allows Mosaic to run their environments locally before shipping them to GitHub.\n\nAnd going forward Mosaic will continue to integrate Docker tech into their operations. This is key to becoming more cloud independent and utilize a variety of different services to satisfy their needs.\n\n#### A word from the client\n\n> The Release platform has been great for our developers. It's more stable and much faster than our previous solution. No longer are they losing hours of productivity at a time due to frequent issues of downtime.\n\n###### Rodrigo Reis\n\n###### Director Of Engineering  •  Mosaic\n\n<CaseStudyCTA />\n",
          "code": "var Component=(()=>{var d=Object.create;var a=Object.defineProperty;var u=Object.getOwnPropertyDescriptor;var m=Object.getOwnPropertyNames;var p=Object.getPrototypeOf,g=Object.prototype.hasOwnProperty;var f=(t,e)=>()=>(e||t((e={exports:{}}).exports,e),e.exports),w=(t,e)=>{for(var o in e)a(t,o,{get:e[o],enumerable:!0})},r=(t,e,o,s)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let i of m(e))!g.call(t,i)&&i!==o&&a(t,i,{get:()=>e[i],enumerable:!(s=u(e,i))||s.enumerable});return t};var y=(t,e,o)=>(o=t!=null?d(p(t)):{},r(e||!t||!t.__esModule?a(o,\"default\",{value:t,enumerable:!0}):o,t)),b=t=>r(a({},\"__esModule\",{value:!0}),t);var l=f((j,c)=>{c.exports=_jsx_runtime});var N={};w(N,{default:()=>M,frontmatter:()=>v});var n=y(l()),v={title:\"Building the Right Solution for Residential Developers for Mosaic\",description:\"Mosaic.us is a technology-powered general contractor.\",publishDate:\"2024-03-01T13:52:39.000Z\",logo:\"/case-study-images/d7dc0ce8bef4cbf1471555186b5abbdc.svg\",thumbnail:\"/case-study-images/93f9ffb4b6907969c2ff64a853cc57b1.png\"};function h(t){let e=Object.assign({p:\"p\",h4:\"h4\",a:\"a\",span:\"span\",ul:\"ul\",li:\"li\",blockquote:\"blockquote\",h6:\"h6\"},t.components),{CaseStudyCTA:o}=e;return o||x(\"CaseStudyCTA\",!0),(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(e.p,{children:\"Mosaic.us is a technology-powered general contractor. They work with residential real estate developers and homebuilders to construct homes after initial design and planning work is complete. Mosaic\\u2019s internal tech platform increases communication and collaboration between the dozens of teams that have to work together to complete construction projects.\"}),`\n`,(0,n.jsxs)(e.h4,{id:\"release-technologies-used\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#release-technologies-used\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Release Technologies Used\"]}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsx)(e.li,{children:\"Instant Datasets for instantly loading appropriate data into sales and preview environments.\"}),`\n`]}),`\n`,(0,n.jsxs)(e.h4,{id:\"challenge\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#challenge\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Challenge\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Homebuilding is among the most complex businesses out there. Between design, permitting and construction any number of things can go wrong, leading to costly delays. There are often 40 to 50 different crews of subcontractors to manage in every home construction project, and supply scarcity is always a challenge.\"}),`\n`,(0,n.jsx)(e.p,{children:\"Much of this work has historically been done with pen and paper, or by spreadsheet at best.\"}),`\n`,(0,n.jsx)(e.p,{children:\"Mosaic uses technology to digitize every aspect of the homebuilding process. That means dozens of solutions covering disparate workflows, including data reports about timing and budgets, onboarding for subcontractors, bidding management and day-to-day site operations.\"}),`\n`,(0,n.jsx)(e.p,{children:\"Mosaic has an engineering team of roughly 15, split across three product teams. In creating a wide-ranging platform, they ran into a common problem that comes with quickly iterating on new products. As the team grew, and as they built out the service, they ran into a ceiling of what they could do.\"}),`\n`,(0,n.jsx)(e.p,{children:\"With just a few environments, and a significant time component for creating new ones, the logistics of testing new features proved challenging. That put a ceiling on how quickly Mosaic could round out its platform to capture every piece of the exhaustive process that is homebuilding.\"}),`\n`,(0,n.jsxs)(e.h4,{id:\"what-they-were-doing-before\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#what-they-were-doing-before\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"What they were doing before\"]}),`\n`,(0,n.jsx)(e.p,{children:\"For a long time, Mosaic used Terraform on top of Heroku for creating environments (prior to migrating to AWS), without any kind of CI/CD pipeline. That was fine for a while when the engineering team was small, but over time, as the team grew and the complexity compounded, bottlenecks emerged. Limits on the number of apps that could be integrated with Heroku \\u2013 they had over 150 integrated because of the sheer volume of different pull requests \\u2013 held them back.\"}),`\n`,(0,n.jsx)(e.p,{children:\"Heroku was not particularly reliable. Frequent memory errors and difficulty connecting to the API shut down the ability to continue working on new and updated features.\"}),`\n`,(0,n.jsx)(e.p,{children:\"Extensive downtime meant teams of high-priced engineers sometimes spent several hours in a day just waiting to be able to work in their environments. With these wait times, a few hours here and a few hours there, engineer downtime quickly added up to weeks of delay for updates and new products.\"}),`\n`,(0,n.jsx)(e.p,{children:\"Because there were limits on the number of apps, when developers wanted to create a new environment, they had to take inventory of their environments. And sometimes that meant deleting old environments to make room for new ones.\"}),`\n`,(0,n.jsx)(e.p,{children:\"Spinning up new environments was doable, but their solutions didn\\u2019t preload the necessary data for accurate testing environments. Because of this, each new environment took about half an hour to create; collectively DevOps teams spent days just creating new environments.\"}),`\n`,(0,n.jsxs)(e.h4,{id:\"a-new-solution\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#a-new-solution\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"A New Solution\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Mosaic initially considered building an in-house solution, which would have required a significant time investment from their most highly skilled technical teams. Instead, they turned to Release\\u2019s Instant Datasets, which allowed Mosaic to pre-populate environments with the data needed for a full picture of how the software would perform. This was important for Mosaic, as executives, sales teams and others frequently wanted new environments to show individual features and updates to customers, or to play with a new idea.\"}),`\n`,(0,n.jsx)(e.p,{children:\"The previous setup of manually creating and populating made these quick environments a painstaking endeavor. With Release, Mosaic can now spin up or shut down new environments with just a couple clicks.\"}),`\n`,(0,n.jsx)(e.p,{children:\"Mosaic quickly switched over to Release, digging deep into their code in GitHub and setting up containers. This integration could have taken weeks, but with Release, Mosaic was able to quickly increase speed and reliability of their environments.\"}),`\n`,(0,n.jsxs)(e.h4,{id:\"whats-next\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#whats-next\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"What\\u2019s Next\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Mosaic is looking into Release\\u2019s Docker Compose feature, as it continues to improve upon its environments. This allows Mosaic to run their environments locally before shipping them to GitHub.\"}),`\n`,(0,n.jsx)(e.p,{children:\"And going forward Mosaic will continue to integrate Docker tech into their operations. This is key to becoming more cloud independent and utilize a variety of different services to satisfy their needs.\"}),`\n`,(0,n.jsxs)(e.h4,{id:\"a-word-from-the-client\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#a-word-from-the-client\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"A word from the client\"]}),`\n`,(0,n.jsxs)(e.blockquote,{children:[`\n`,(0,n.jsx)(e.p,{children:\"The Release platform has been great for our developers. It's more stable and much faster than our previous solution. No longer are they losing hours of productivity at a time due to frequent issues of downtime.\"}),`\n`]}),`\n`,(0,n.jsxs)(e.h6,{id:\"rodrigo-reis\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#rodrigo-reis\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Rodrigo Reis\"]}),`\n`,(0,n.jsxs)(e.h6,{id:\"director-of-engineering--mosaic\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#director-of-engineering--mosaic\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Director Of Engineering \\xA0\\u2022 \\xA0Mosaic\"]}),`\n`,(0,n.jsx)(o,{})]})}function k(t={}){let{wrapper:e}=t.components||{};return e?(0,n.jsx)(e,Object.assign({},t,{children:(0,n.jsx)(h,t)})):h(t)}var M=k;function x(t,e){throw new Error(\"Expected \"+(e?\"component\":\"object\")+\" `\"+t+\"` to be defined: you likely forgot to import, pass, or provide it.\")}return b(N);})();\n;return Component;"
        },
        "_id": "case-studies/content/mosaic.mdx",
        "_raw": {
          "sourceFilePath": "case-studies/content/mosaic.mdx",
          "sourceFileName": "mosaic.mdx",
          "sourceFileDir": "case-studies/content",
          "contentType": "mdx",
          "flattenedPath": "case-studies/content/mosaic"
        },
        "type": "CaseStudy",
        "computedSlug": "mosaic"
      },
      "documentHash": "1739393595033",
      "hasWarnings": false,
      "documentTypeName": "CaseStudy"
    },
    "case-studies/content/noteable.mdx": {
      "document": {
        "title": "With Release, Noteable Increases Collaboration and Velocity While Cutting Downtime by 50%",
        "logo": "/case-study-images/c7e0fe6523d616fedbf4aa86deedb856.svg",
        "description": "Noteable is a collaborative notebook platform that enables teams to use and visualize data, together.",
        "publishDate": "2024-02-29T22:13:29.000Z",
        "thumbnail": "/case-study-images/3604ac35f8024f1f78daf543ef9be000.png",
        "body": {
          "raw": "\n#### About the client\n\nNoteable is a collaborative notebook platform that enables teams to use and visualize data, together. Its cloud-based and secure deployment options, no-code visualizations, and collaborative environment make it easy for teams to work with data in a single platform. Users can comment on, export, and share data points through a modern UI with native SQL support. Historically, different teams had different skill sets and used specialized tools that weren’t tightly integrated with one another. Too many handoffs led to unverifiable or unreproducible work, and the workflow process became prone to failure. Now, with Noteable, teams remain fully integrated in existing programming environments, while creating rich visualizations to share. All without code.\n\n#### Technologies used\n\n- Preview environments to empower non-traditional users to work with data in new ways.\n\n#### Problem\n\nNoteable is focused on improving collaboration and data visualization techniques by adding features to virtual notebook environments such as versioning, commenting, real-time updates and role-based access control. Advanced visualization capabilities enable Noteable’s customers to automatically visualize their data in sophisticated new ways, and to explore their data before writing any code.\n\nThis is crucial, as IDC found that [data experts worldwide spend 44 percent of their workdays on unsuccessful data activities](https://www.alteryx.com/resources/report/idc-state-of-data-science-and-analytics). A staggering [90 percent](https://venturebeat.com/2021/11/16/noteable-expands-analytics-tools-for-programming-with-new-21m/) of time is spent searching for, preparing, and analyzing data – essential housekeeping tasks, which ultimately don’t add any meaningful value at all.\n\nSo Noteable developed a suite of integrated collaboration tools to simplify communication and encourage exploration across teams, including commenting, version control and auto save. Additional features – including managed cloud infrastructure, integrated single sign-on, and role-based access control – act as a secure foundation.\n\nDesigning and augmenting these types of rich feature sets, however, required Noteable developers to rapidly test their own code changes. Pull requests were taking too long, and they needed to streamline their entire code review and developer collaboration processes.  Noteable needed faster feedback loops, and environments where they could quickly preview their code changes. Ephemeral environments through Release fit their requirements perfectly.\n\n#### How was this solved before\n\nDiego Rodriguez, staff devOps engineer tech lead at Noteable, had tried building custom environments during his time at prior companies. He knew the DIY approach relied heavily on pull requests and interconnected tools to browse and manage code. This required a lot of cobbled together infrastructure, as well as writing customized scripts. It resulted in a situation where different teams were working on different aspects of the platform simultaneously. Developers made changes in isolated environments and couldn’t easily determine the full impact of code changes until everything was merged together. And with many code changes in flight at the same time, it became increasingly difficult to identify and fix errors.\n\nIn his words: “At Noteable, we didn't have anything before Release - but because of my previous experience, we set forth to look for a vendor and not build something by ourselves.”\n\n#### A new solution\n\nBy turning to Release, the developers at Noteable now review their full stack and all the supporting infrastructure at the click of a button. Noteable also has immediate access to on-demand ephemeral environments, where they can spin up and delete environments on the fly, easily preview all code changes and run end-to-end tests in a fully fleshed out environment – before they get merged into the main branch on the source code repository.\n\n‍  \nThe resulting impact Noteable has experienced is significant. Noteable estimates the time saved with Release compared to building custom environments is at least a full quarter worth of development work across two devops engineers, not accounting for the operational burden of maintaining a custom solution. With Release, they’re able to push a change to GitHub and instantly visualize the code change, which accelerates development and product releases. Additionally, running end-to-end tests in a fully fleshed out environment allows faster iteration, and collaboration is greatly improved as developers can access environments from a central, shared location. Finally, developers at Noteable don’t need to worry about maintaining any tooling or added infrastructure.\n\n#### What’s next\n\nNoteable currently uses Release ephemeral environments with the frontend stack, and they  plan to also deploy ephemeral environments for the backend stack. Doing so will alleviate local development complexities due to the varied supporting infrastructure systems that are in use and give the backend team more time to focus on development.\n\n#### A word from the client\n\n> All pull requests require review, which became a substantial time investment and ultimately a blocking step for us. During code review, even the best developers may not understand what they’re looking at initially. Having ephemeral environments to bring up on-demand, and play with while looking at the code – rather than having to stop work, pull everything down, integrate new code changes into the main project repository, and fire it up again – reduces our time by 50 percent.\n\n###### Krishna Rajendran\n\n<CaseStudyCTA />\n",
          "code": "var Component=(()=>{var h=Object.create;var i=Object.defineProperty;var u=Object.getOwnPropertyDescriptor;var m=Object.getOwnPropertyNames;var p=Object.getPrototypeOf,f=Object.prototype.hasOwnProperty;var g=(t,e)=>()=>(e||t((e={exports:{}}).exports,e),e.exports),b=(t,e)=>{for(var a in e)i(t,a,{get:e[a],enumerable:!0})},s=(t,e,a,r)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let o of m(e))!f.call(t,o)&&o!==a&&i(t,o,{get:()=>e[o],enumerable:!(r=u(e,o))||r.enumerable});return t};var w=(t,e,a)=>(a=t!=null?h(p(t)):{},s(e||!t||!t.__esModule?i(a,\"default\",{value:t,enumerable:!0}):a,t)),v=t=>s(i({},\"__esModule\",{value:!0}),t);var d=g((j,l)=>{l.exports=_jsx_runtime});var q={};b(q,{default:()=>N,frontmatter:()=>y});var n=w(d()),y={title:\"With Release, Noteable Increases Collaboration and Velocity While Cutting Downtime by 50%\",description:\"Noteable is a collaborative notebook platform that enables teams to use and visualize data, together.\",publishDate:\"2024-02-29T22:13:29.000Z\",logo:\"/case-study-images/c7e0fe6523d616fedbf4aa86deedb856.svg\",thumbnail:\"/case-study-images/3604ac35f8024f1f78daf543ef9be000.png\"};function c(t){let e=Object.assign({h4:\"h4\",a:\"a\",span:\"span\",p:\"p\",ul:\"ul\",li:\"li\",br:\"br\",blockquote:\"blockquote\",h6:\"h6\"},t.components),{CaseStudyCTA:a}=e;return a||x(\"CaseStudyCTA\",!0),(0,n.jsxs)(n.Fragment,{children:[(0,n.jsxs)(e.h4,{id:\"about-the-client\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#about-the-client\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"About the client\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Noteable is a collaborative notebook platform that enables teams to use and visualize data, together. Its cloud-based and secure deployment options, no-code visualizations, and collaborative environment make it easy for teams to work with data in a single platform. Users can comment on, export, and share data points through a modern UI with native SQL support. Historically, different teams had different skill sets and used specialized tools that weren\\u2019t tightly integrated with one another. Too many handoffs led to unverifiable or unreproducible work, and the workflow process became prone to failure. Now, with Noteable, teams remain fully integrated in existing programming environments, while creating rich visualizations to share. All without code.\"}),`\n`,(0,n.jsxs)(e.h4,{id:\"technologies-used\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#technologies-used\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Technologies used\"]}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsx)(e.li,{children:\"Preview environments to empower non-traditional users to work with data in new ways.\"}),`\n`]}),`\n`,(0,n.jsxs)(e.h4,{id:\"problem\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#problem\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Problem\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Noteable is focused on improving collaboration and data visualization techniques by adding features to virtual notebook environments such as versioning, commenting, real-time updates and role-based access control. Advanced visualization capabilities enable Noteable\\u2019s customers to automatically visualize their data in sophisticated new ways, and to explore their data before writing any code.\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"This is crucial, as IDC found that \",(0,n.jsx)(e.a,{href:\"https://www.alteryx.com/resources/report/idc-state-of-data-science-and-analytics\",children:\"data experts worldwide spend 44 percent of their workdays on unsuccessful data activities\"}),\". A staggering \",(0,n.jsx)(e.a,{href:\"https://venturebeat.com/2021/11/16/noteable-expands-analytics-tools-for-programming-with-new-21m/\",children:\"90 percent\"}),\" of time is spent searching for, preparing, and analyzing data \\u2013 essential housekeeping tasks, which ultimately don\\u2019t add any meaningful value at all.\"]}),`\n`,(0,n.jsx)(e.p,{children:\"So Noteable developed a suite of integrated collaboration tools to simplify communication and encourage exploration across teams, including commenting, version control and auto save. Additional features \\u2013 including managed cloud infrastructure, integrated single sign-on, and role-based access control \\u2013 act as a secure foundation.\"}),`\n`,(0,n.jsx)(e.p,{children:\"Designing and augmenting these types of rich feature sets, however, required Noteable developers to rapidly test their own code changes. Pull requests were taking too long, and they needed to streamline their entire code review and developer collaboration processes. \\xA0Noteable needed faster feedback loops, and environments where they could quickly preview their code changes. Ephemeral environments through Release fit their requirements perfectly.\"}),`\n`,(0,n.jsxs)(e.h4,{id:\"how-was-this-solved-before\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#how-was-this-solved-before\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"How was this solved before\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Diego Rodriguez, staff devOps engineer tech lead at Noteable, had tried building custom environments during his time at prior companies. He knew the DIY approach relied heavily on pull requests and interconnected tools to browse and manage code. This required a lot of cobbled together infrastructure, as well as writing customized scripts. It resulted in a situation where different teams were working on different aspects of the platform simultaneously. Developers made changes in isolated environments and couldn\\u2019t easily determine the full impact of code changes until everything was merged together. And with many code changes in flight at the same time, it became increasingly difficult to identify and fix errors.\"}),`\n`,(0,n.jsx)(e.p,{children:\"In his words: \\u201CAt Noteable, we didn't have anything before Release - but because of my previous experience, we set forth to look for a vendor and not build something by ourselves.\\u201D\"}),`\n`,(0,n.jsxs)(e.h4,{id:\"a-new-solution\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#a-new-solution\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"A new solution\"]}),`\n`,(0,n.jsx)(e.p,{children:\"By turning to Release, the developers at Noteable now review their full stack and all the supporting infrastructure at the click of a button. Noteable also has immediate access to on-demand ephemeral environments, where they can spin up and delete environments on the fly, easily preview all code changes and run end-to-end tests in a fully fleshed out environment \\u2013 before they get merged into the main branch on the source code repository.\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"\\u200D\",(0,n.jsx)(e.br,{}),`\n`,\"The resulting impact Noteable has experienced is significant. Noteable estimates the time saved with Release compared to building custom environments is at least a full quarter worth of development work across two devops engineers, not accounting for the operational burden of maintaining a custom solution. With Release, they\\u2019re able to push a change to GitHub and instantly visualize the code change, which accelerates development and product releases. Additionally, running end-to-end tests in a fully fleshed out environment allows faster iteration, and collaboration is greatly improved as developers can access environments from a central, shared location. Finally, developers at Noteable don\\u2019t need to worry about maintaining any tooling or added infrastructure.\"]}),`\n`,(0,n.jsxs)(e.h4,{id:\"whats-next\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#whats-next\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"What\\u2019s next\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Noteable currently uses Release ephemeral environments with the frontend stack, and they \\xA0plan to also deploy ephemeral environments for the backend stack. Doing so will alleviate local development complexities due to the varied supporting infrastructure systems that are in use and give the backend team more time to focus on development.\"}),`\n`,(0,n.jsxs)(e.h4,{id:\"a-word-from-the-client\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#a-word-from-the-client\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"A word from the client\"]}),`\n`,(0,n.jsxs)(e.blockquote,{children:[`\n`,(0,n.jsx)(e.p,{children:\"All pull requests require review, which became a substantial time investment and ultimately a blocking step for us. During code review, even the best developers may not understand what they\\u2019re looking at initially. Having ephemeral environments to bring up on-demand, and play with while looking at the code \\u2013 rather than having to stop work, pull everything down, integrate new code changes into the main project repository, and fire it up again \\u2013 reduces our time by 50 percent.\"}),`\n`]}),`\n`,(0,n.jsxs)(e.h6,{id:\"krishna-rajendran\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#krishna-rajendran\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Krishna Rajendran\"]}),`\n`,(0,n.jsx)(a,{})]})}function k(t={}){let{wrapper:e}=t.components||{};return e?(0,n.jsx)(e,Object.assign({},t,{children:(0,n.jsx)(c,t)})):c(t)}var N=k;function x(t,e){throw new Error(\"Expected \"+(e?\"component\":\"object\")+\" `\"+t+\"` to be defined: you likely forgot to import, pass, or provide it.\")}return v(q);})();\n;return Component;"
        },
        "_id": "case-studies/content/noteable.mdx",
        "_raw": {
          "sourceFilePath": "case-studies/content/noteable.mdx",
          "sourceFileName": "noteable.mdx",
          "sourceFileDir": "case-studies/content",
          "contentType": "mdx",
          "flattenedPath": "case-studies/content/noteable"
        },
        "type": "CaseStudy",
        "computedSlug": "noteable"
      },
      "documentHash": "1739393595034",
      "hasWarnings": false,
      "documentTypeName": "CaseStudy"
    },
    "case-studies/content/simon.mdx": {
      "document": {
        "title": "Pre-production ephemeral environments and sales demo environments with Simon Data",
        "logo": "/case-study-images/e1ef7dd360ebb603b27b35968823d9f0.svg",
        "description": "Simon Data is the enterprise customer data platform that empowers brands to deliver data-driven, personalized customer experiences everywhere.",
        "publishDate": "2024-02-29T22:13:29.000Z",
        "thumbnail": "/case-study-images/aa0280aeda0fddfd89bb866da41852cc.png",
        "body": {
          "raw": "\n#### About the client\n\nSimon Data is the enterprise customer data platform that empowers brands to deliver data-driven, personalized customer experiences everywhere. Simon is a marketing platform with a diverse technology ecosystem that spans many technologies within AWS.\n\n#### Release Technologies used:\n\n- Preview Environments with every Pull Request for User Acceptance Testing\n- On-Demand Sales Demo Environments for reproducible and consistent sales demos across industry verticals.\n- Instant Datasets for instantly loading appropriate data into sales and preview environments.\n\n#### Problem\n\nSimon Data was an early partner of Release with a handful of unique problems that needed to be solved in the environment ecosystem. First, Simon needed a better way to create environments for code reviews and product acceptance tests for their frontend web development team. In addition, Simon’s product is tricky to demo because each customer has specific data needs that required a special setup in a standalone environment such that the appropriate data could be loaded into the environment for the appropriate industry vertical (healthcare, automotive, etc…). \n\nProduct acceptance testing and code reviews were done in one of two ways prior to Release. Simon started by adopting the ‘test in production’ philosophy by releasing changes behind feature flags. They had a shared staging environment as well, but it was never kept up to date and maintained, so it consistently fell behind and wasn’t used frequently. As the Simon team grew and product and design teams became a part of the development process for their B2B product, ‘test in production’ became less desirable. Features that weren’t ready for customers and tested comprehensively were being released, which caused a lot of frustration and rework.\n\nSales demos were also a problem for Simon as there was only a single demo environment that had to be reloaded for each demo with the appropriate data for the specific industry of the target customer. This process was error prone and tedious to manage.\n\nSimon’s technology ecosystem was incredibly diverse across many of the technologies with AWS and their DevOps team managing a ton of complexity. Building out a better way to manage environments was a project they wanted to do but they were spending most of their time keeping up with the demands of the organization and couldn’t spare the cycles to fix their environment ecosystem.\n\n#### How was this solved before\n\nPrior to Release, Simon had an in-house team build a staging environment where code reviews and product acceptance testing could be performed. but they only had one available to the growing development team. This staging environment was manually created and maintained which meant it regularly fell behind the most up-to-date technologies being used in their production environment. \n\nThere was only one sales demo environment which also suffered from lagging behind the most current features and technologies. In addition, this environment had to be refreshed for each customer with the appropriate data. A single employee in sales operations was capable of loading the appropriate data and the process was manual.\n\nIn the past, the typical solution to building out a pre-product environment ecosystem for a company like Simon Data would be to simply try and re-create the production environment manually. You’d end up with another shared environment that all the developers used but you’d quickly realize that it, being a shared resource, became a bottleneck. Now they would face having two bottlenecks that had to be maintained.\n\nOther solutions would be to spend 6-12 months building out a more automated environment ecosystem in house with a dedicated DevOps team. However, this is a large investment and requires continual maintenance on a system that doesn’t add direct value to the business. Ongoing monitoring, maintenance, and support for new features would be a recurring cost for this in-house solution.\n\n#### A new solution\n\nSimon Data needed a better solution that could solve all their pre-production environment problems in a fast, cost effective, and streamlined manner. Because Release supports complex, multi-service applications within AWS we were able to replicate their complex environments. In addition, Simon Data is a data business, so environments needed the appropriate data to accurately test features and demonstrate value in their demo environments.\n\nWe were able to provide Simon Data with an Environments as a Service solution. They used Release to create ephemeral staging environments for every Pull Request so code reviews and product acceptance testing could be done prior to releasing to production. This has resulted in higher quality solutions delivered to customers because Product Managers and stakeholders can see solutions much earlier in the development process than with testing in prod or shared staging environments. Simon has happier customers with better solutions and less “oops” moments or products that miss the mark.\n\nThey are also creating on-demand Sales Demo environments with Release Instant Datasets. With Instant Datasets, Simon automatically loads industry specific data into demo environments to provide high quality and relevant demos for their potential customers. For each demo, they can choose the type of data they’d like to share with the customer and click a button to create a demo environment with relevant data for the demo. Over time they will be able to adapt their demos to any industry with a workflow for the sales teams which can easily add relevant data to the Instant Dataset library. This has resulted in much better and more relevant demos that have directly led to new Simon Data customers.\n\n#### What’s next\n\nAs Simon’s business is rapidly evolving so are their needs to streamline development and deployments. More and more services are being developed at Simon and their needs for quickly deploying services grows. In the future, Simon will leverage Release for production deployments of appropriate services that are best delivered via Kubernetes.  \n\nWe are also working towards expanding the types of data available in Release Instant Datasets to include services such as Redshift, a highly critical data element for Simon’s products and environments.\n\nThe streamlining of deployments across a common environment management platform will reduce costs of operations and improve quality by reducing errors experienced by Simon’s customers. \n\nAn additional area of focus we’re exploring are on-premise (on-cloud) deployments of our software into Simon’s customer accounts where security and privacy are crucial.\n\nLastly, we’re also working towards helping Simon build out a joyful developer experience that makes developing against a complex technology ecosystem simple and easy for Simon’s many developers. \n\n#### A word from the client\n\n> Release has been a fantastic partner that’s solved problems for our developers, product managers and sales operations teams that we just simply didn’t have the time or money to address. We’ve been able to focus on our core, value added code while Release has helped us deliver higher quality software, faster, even with our unique and complex technology ecosystem.\n\n###### Jason Davis\n\n###### CEO • Simon Data\n\n<CaseStudyCTA />\n",
          "code": "var Component=(()=>{var h=Object.create;var s=Object.defineProperty;var m=Object.getOwnPropertyDescriptor;var p=Object.getOwnPropertyNames;var u=Object.getPrototypeOf,v=Object.prototype.hasOwnProperty;var f=(t,e)=>()=>(e||t((e={exports:{}}).exports,e),e.exports),w=(t,e)=>{for(var a in e)s(t,a,{get:e[a],enumerable:!0})},r=(t,e,a,i)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let o of p(e))!v.call(t,o)&&o!==a&&s(t,o,{get:()=>e[o],enumerable:!(i=m(e,o))||i.enumerable});return t};var y=(t,e,a)=>(a=t!=null?h(u(t)):{},r(e||!t||!t.__esModule?s(a,\"default\",{value:t,enumerable:!0}):a,t)),g=t=>r(s({},\"__esModule\",{value:!0}),t);var l=f((q,d)=>{d.exports=_jsx_runtime});var D={};w(D,{default:()=>k,frontmatter:()=>b});var n=y(l()),b={title:\"Pre-production ephemeral environments and sales demo environments with Simon Data\",description:\"Simon Data is the enterprise customer data platform that empowers brands to deliver data-driven, personalized customer experiences everywhere.\",publishDate:\"2024-02-29T22:13:29.000Z\",logo:\"/case-study-images/e1ef7dd360ebb603b27b35968823d9f0.svg\",thumbnail:\"/case-study-images/aa0280aeda0fddfd89bb866da41852cc.png\"};function c(t){let e=Object.assign({h4:\"h4\",a:\"a\",span:\"span\",p:\"p\",ul:\"ul\",li:\"li\",blockquote:\"blockquote\",h6:\"h6\"},t.components),{CaseStudyCTA:a}=e;return a||x(\"CaseStudyCTA\",!0),(0,n.jsxs)(n.Fragment,{children:[(0,n.jsxs)(e.h4,{id:\"about-the-client\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#about-the-client\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"About the client\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Simon Data is the enterprise customer data platform that empowers brands to deliver data-driven, personalized customer experiences everywhere. Simon is a marketing platform with a diverse technology ecosystem that spans many technologies within AWS.\"}),`\n`,(0,n.jsxs)(e.h4,{id:\"release-technologies-used\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#release-technologies-used\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Release Technologies used:\"]}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsx)(e.li,{children:\"Preview Environments with every Pull Request for User Acceptance Testing\"}),`\n`,(0,n.jsx)(e.li,{children:\"On-Demand Sales Demo Environments for reproducible and consistent sales demos across industry verticals.\"}),`\n`,(0,n.jsx)(e.li,{children:\"Instant Datasets for instantly loading appropriate data into sales and preview environments.\"}),`\n`]}),`\n`,(0,n.jsxs)(e.h4,{id:\"problem\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#problem\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Problem\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Simon Data was an early partner of Release with a handful of unique problems that needed to be solved in the environment ecosystem. First, Simon needed a better way to create environments for code reviews and product acceptance tests for their frontend web development team. In addition, Simon\\u2019s product is tricky to demo because each customer has specific data needs that required a special setup in a standalone environment such that the appropriate data could be loaded into the environment for the appropriate industry vertical (healthcare, automotive, etc\\u2026).\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:\"Product acceptance testing and code reviews were done in one of two ways prior to Release. Simon started by adopting the \\u2018test in production\\u2019 philosophy by releasing changes behind feature flags. They had a shared staging environment as well, but it was never kept up to date and maintained, so it consistently fell behind and wasn\\u2019t used frequently. As the Simon team grew and product and design teams became a part of the development process for their B2B product, \\u2018test in production\\u2019 became less desirable. Features that weren\\u2019t ready for customers and tested comprehensively were being released, which caused a lot of frustration and rework.\"}),`\n`,(0,n.jsx)(e.p,{children:\"Sales demos were also a problem for Simon as there was only a single demo environment that had to be reloaded for each demo with the appropriate data for the specific industry of the target customer. This process was error prone and tedious to manage.\"}),`\n`,(0,n.jsx)(e.p,{children:\"Simon\\u2019s technology ecosystem was incredibly diverse across many of the technologies with AWS and their DevOps team managing a ton of complexity. Building out a better way to manage environments was a project they wanted to do but they were spending most of their time keeping up with the demands of the organization and couldn\\u2019t spare the cycles to fix their environment ecosystem.\"}),`\n`,(0,n.jsxs)(e.h4,{id:\"how-was-this-solved-before\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#how-was-this-solved-before\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"How was this solved before\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Prior to Release, Simon had an in-house team build a staging environment where code reviews and product acceptance testing could be performed. but they only had one available to the growing development team. This staging environment was manually created and maintained which meant it regularly fell behind the most up-to-date technologies being used in their production environment.\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:\"There was only one sales demo environment which also suffered from lagging behind the most current features and technologies. In addition, this environment had to be refreshed for each customer with the appropriate data. A single employee in sales operations was capable of loading the appropriate data and the process was manual.\"}),`\n`,(0,n.jsx)(e.p,{children:\"In the past, the typical solution to building out a pre-product environment ecosystem for a company like Simon Data would be to simply try and re-create the production environment manually. You\\u2019d end up with another shared environment that all the developers used but you\\u2019d quickly realize that it, being a shared resource, became a bottleneck. Now they would face having two bottlenecks that had to be maintained.\"}),`\n`,(0,n.jsx)(e.p,{children:\"Other solutions would be to spend 6-12 months building out a more automated environment ecosystem in house with a dedicated DevOps team. However, this is a large investment and requires continual maintenance on a system that doesn\\u2019t add direct value to the business. Ongoing monitoring, maintenance, and support for new features would be a recurring cost for this in-house solution.\"}),`\n`,(0,n.jsxs)(e.h4,{id:\"a-new-solution\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#a-new-solution\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"A new solution\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Simon Data needed a better solution that could solve all their pre-production environment problems in a fast, cost effective, and streamlined manner. Because Release supports complex, multi-service applications within AWS we were able to replicate their complex environments. In addition, Simon Data is a data business, so environments needed the appropriate data to accurately test features and demonstrate value in their demo environments.\"}),`\n`,(0,n.jsx)(e.p,{children:\"We were able to provide Simon Data with an Environments as a Service solution. They used Release to create ephemeral staging environments for every Pull Request so code reviews and product acceptance testing could be done prior to releasing to production. This has resulted in higher quality solutions delivered to customers because Product Managers and stakeholders can see solutions much earlier in the development process than with testing in prod or shared staging environments. Simon has happier customers with better solutions and less \\u201Coops\\u201D moments or products that miss the mark.\"}),`\n`,(0,n.jsx)(e.p,{children:\"They are also creating on-demand Sales Demo environments with Release Instant Datasets. With Instant Datasets, Simon automatically loads industry specific data into demo environments to provide high quality and relevant demos for their potential customers. For each demo, they can choose the type of data they\\u2019d like to share with the customer and click a button to create a demo environment with relevant data for the demo. Over time they will be able to adapt their demos to any industry with a workflow for the sales teams which can easily add relevant data to the Instant Dataset library. This has resulted in much better and more relevant demos that have directly led to new Simon Data customers.\"}),`\n`,(0,n.jsxs)(e.h4,{id:\"whats-next\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#whats-next\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"What\\u2019s next\"]}),`\n`,(0,n.jsx)(e.p,{children:\"As Simon\\u2019s business is rapidly evolving so are their needs to streamline development and deployments. More and more services are being developed at Simon and their needs for quickly deploying services grows. In the future, Simon will leverage Release for production deployments of appropriate services that are best delivered via Kubernetes.\\xA0\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:\"We are also working towards expanding the types of data available in Release Instant Datasets to include services such as Redshift, a highly critical data element for Simon\\u2019s products and environments.\"}),`\n`,(0,n.jsx)(e.p,{children:\"The streamlining of deployments across a common environment management platform will reduce costs of operations and improve quality by reducing errors experienced by Simon\\u2019s customers.\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:\"An additional area of focus we\\u2019re exploring are on-premise (on-cloud) deployments of our software into Simon\\u2019s customer accounts where security and privacy are crucial.\"}),`\n`,(0,n.jsx)(e.p,{children:\"Lastly, we\\u2019re also working towards helping Simon build out a joyful developer experience that makes developing against a complex technology ecosystem simple and easy for Simon\\u2019s many developers.\\xA0\"}),`\n`,(0,n.jsxs)(e.h4,{id:\"a-word-from-the-client\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#a-word-from-the-client\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"A word from the client\"]}),`\n`,(0,n.jsxs)(e.blockquote,{children:[`\n`,(0,n.jsx)(e.p,{children:\"Release has been a fantastic partner that\\u2019s solved problems for our developers, product managers and sales operations teams that we just simply didn\\u2019t have the time or money to address. We\\u2019ve been able to focus on our core, value added code while Release has helped us deliver higher quality software, faster, even with our unique and complex technology ecosystem.\"}),`\n`]}),`\n`,(0,n.jsxs)(e.h6,{id:\"jason-davis\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#jason-davis\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Jason Davis\"]}),`\n`,(0,n.jsxs)(e.h6,{id:\"ceo--simon-data\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#ceo--simon-data\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"CEO \\u2022 Simon Data\"]}),`\n`,(0,n.jsx)(a,{})]})}function S(t={}){let{wrapper:e}=t.components||{};return e?(0,n.jsx)(e,Object.assign({},t,{children:(0,n.jsx)(c,t)})):c(t)}var k=S;function x(t,e){throw new Error(\"Expected \"+(e?\"component\":\"object\")+\" `\"+t+\"` to be defined: you likely forgot to import, pass, or provide it.\")}return g(D);})();\n;return Component;"
        },
        "_id": "case-studies/content/simon.mdx",
        "_raw": {
          "sourceFilePath": "case-studies/content/simon.mdx",
          "sourceFileName": "simon.mdx",
          "sourceFileDir": "case-studies/content",
          "contentType": "mdx",
          "flattenedPath": "case-studies/content/simon"
        },
        "type": "CaseStudy",
        "computedSlug": "simon"
      },
      "documentHash": "1739393595034",
      "hasWarnings": false,
      "documentTypeName": "CaseStudy"
    },
    "blog/posts/10-best-practices-when-using-feature-flags.mdx": {
      "document": {
        "title": "10 Best Practices When Using Feature Flags",
        "summary": "This post will explore what feature flags are, answer common questions, and discuss the best practices for using them.",
        "publishDate": "Mon Jan 09 2023 08:06:23 GMT+0000 (Coordinated Universal Time)",
        "author": "israel-oyetunji",
        "readingTime": 9,
        "categories": [
          "platform-engineering",
          "product"
        ],
        "mainImage": "/blog-images/35d3a8c5eb5d8692679503d011fee3da.svg",
        "imageAlt": "10 Best Practices When Using Feature Flags",
        "showCTA": true,
        "ctaCopy": "Accelerate feature testing and deployment with Release's on-demand environments, aligning with best practices for feature flags.",
        "ctaLink": "https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=10-best-practices-when-using-feature-flags",
        "relatedPosts": [
          ""
        ],
        "ogImage": "/blog-images/35d3a8c5eb5d8692679503d011fee3da.svg",
        "excerpt": "This post will explore what feature flags are, answer common questions, and discuss the best practices for using them.",
        "tags": [
          "platform-engineering",
          "product"
        ],
        "ctaButton": "Try Release for Free",
        "body": {
          "raw": "\nFeature flags are a powerful tool for software development teams to control the release of new features. By using feature flags, teams can easily turn features on or off in real time. This allows for the quick and safe release of new functionality to users. \n\nIn this post, we'll explore what feature flags are. We'll also cover best practices for using them and discuss common challenges and questions surrounding their use. \n\n![](/blog-images/8315fbc5578c48ba18e7f5df3ef99ffa.png)\n\n### What are Feature Flags?\n\nFeature flags, also known as [feature toggle](https://en.wikipedia.org/wiki/Feature_toggle) or feature switch, are a software development technique. They allow developers to enable or disable certain features or functionality in their application without deploying new code. This allows for more flexible and efficient software development. Using feature flags, teams can quickly and easily test and roll out new features. They can also manage these features without impacting the stability and reliability of the application. \n\nFeature flags provide a number of benefits, including the ability to: \n\n- test new features with a small group of users before rolling them out to the entire user base;\n- roll out new features gradually, rather than all at once;\n- quickly disable or roll back features that are causing problems or are not performing as expected; and\n- customize user experience for different groups of users or for specific regions or markets.\n\n### Best Practices When Using Feature Flags\n\nTo fully realize the benefits of feature flags, it's important to follow best practices when using them. \n\n#### 1\\. Clean up unused feature flags\n\nYou should regularly remove temporary feature flags, such as release flags and experiment flags, when they are no longer needed. This helps prevent the accumulation of technical debt in the code. It also keeps the feature flag management system organized and easy to understand. Each flag should have a specific, independent purpose, and the code should be modular enough to allow different features to be turned on in any combination. \n\nHowever, if multiple flags are required or may conflict with each other, using flags can become confusing and may have a negative impact on user experience. To avoid these problems, you must carefully plan and manage the use of feature flags. \n\n#### 2\\. Use a feature flag management platform\n\nIt's important to choose a feature flag management system that's easy to understand and use. These characteristics are essential whether it's a specialized tool, a config file, or a database table. When introducing feature flags to a team, take the time to carefully consider the best system for your needs. That way, the solution you pick can be used long term. \n\nA few popular feature flag management platforms to consider are Harness and LaunchDarkly. \n\n#### 3\\. Establish naming conventions\n\nMake your feature flags easy to understand and use by establishing a naming convention. Good, descriptive naming conventions and clear documentation can help ensure that all software engineers understand the purpose and use of each flag. \n\nWhen creating a naming convention, consider including a prefix with the project or team name, indicating whether the flag is temporary or permanent, and including a creation date. It may also be helpful to include the word \"flag\" in the name if using a homegrown solution, as this can clarify the purpose of the code. \n\nIn general, it's a good idea to follow a style guide for code that includes conventions for things like camelCase and indentation, as this can make it easier to read and understand the code. By establishing clear naming conventions and style guidelines for feature flags, you can improve the maintainability and readability of your codebase. \n\n#### 4\\. Use feature flags for small test releases\n\nBig, new feature releases can be stressful and risky, as they can potentially impact the stability and reliability of the application. However, you can reduce this risk by using feature flags to expose your new feature to a small audience first, monitor the effects, and roll back if necessary. \n\nOne of the key benefits of feature flags is the ability to perform canary releases, or the gradual rollout of a new feature to a small group of users before making it available to the entire user base. This allows you to test the functionality of the new feature and gather feedback from a small group of users before exposing it to the entire user base. \n\n![](/blog-images/8c1409159ad8d20252accb782f2240ec.jpeg)\n\n#### 5\\. Avoid dependencies between flags\n\nEnsure that each feature flag serves a specific, independent purpose. When multiple flags are required for a single release or the state of the flag conflicts with another flag, it can lead to confusion and make it difficult to maintain the code. This can also have a negative impact on user experience. To avoid these issues, you must carefully plan and manage the use of feature flags, ensuring that they're used effectively and efficiently. \n\n#### 6\\. Use targeted feature flags\n\nUse feature flags to customize the user experience. Feature flags can be used to customize user experience for different groups of users or for specific regions or markets. By using targeted feature flags, developers can enable or disable features for specific users or groups based on various criteria such as location, language, or user role. \n\nOne popular use of feature flags is to manage styling, themes, and personalized content. For example, a feature flag could be used to enable or disable dark mode for a website or app. This allows developers to test the new feature with a subset of users before rolling it out to the entire user base. \n\n#### 7\\. Use feature flags to enable feature branches\n\nFeature branches allow developers to work on new features in a separate branch of the codebase without impacting the stability and reliability of the main branch. This can be useful for developing and testing new features without disrupting the main development process. \n\n#### 8\\. Track changes with audit logs\n\nAn audit log can be a useful tool for tracking and managing changes to feature flags. This log can provide a record of all changes made to each flag, including the identity of the person making the change and the date and time of the change. \n\nAn audit log can help ensure transparency and visibility in the implementation of feature flag changes. Changes to feature flags can be particularly important in regulated industries such as finance and healthcare. By restricting access to the audit log to a limited number of authorized individuals, it's also possible to enhance the security of the process and protect sensitive flags from unauthorized changes. \n\n#### 9\\. Control access based on policies\n\nYou may want to control access to feature flags based on policies. One of the benefits of feature flags is that they can be accessed by nontechnical team members, such as the product team, who can use them to assist with A/B testing. Another way to limit access is to allow only administrators to toggle a feature flag in a production-related environment. \n\nHowever, it's important to carefully control access to feature flags and to track changes made to them. This can involve locking changes in the production environment or maintaining a log of who's modified which flags. By taking these precautions, you can ensure that feature flags are used effectively and that changes are made by authorized personnel. \n\n#### 10\\. Plan ahead for feature flags\n\nProper planning is key to the successful implementation of feature flags. Rather than treating feature flags as an afterthought, it's important to consider them during the design process. This will help you determine whether a flag should be temporary or permanent. You'll then be able to plan accordingly for things like naming conventions, configuration settings, review and removal processes, and access control and safety checks. By planning carefully for all flags upfront, you can increase the chances of success and ensure that your feature flags are implemented effectively. \n\n### Challenges of Using Feature Flags\n\nFeature flags can be a useful tool in software development, but they also come with their own set of challenges. Some common challenges of using feature flags include: \n\n- **Technical debt:** Using feature flags can lead to technical debt if the flags are not maintained and updated over time. This can result in increased maintenance costs and may require dedicated resources to address.\n- **Feature creep:** Feature flags can also lead to feature creep, where more and more features are added over time without a clear plan or goal. This can result in a cluttered and confusing user experience.\n- **Complexity:** Managing multiple feature flags can be complex, especially as the number of flags increases. Difficulty in keeping track of enabled and disabled flags can lead to potential issues with code changes and deployments.\n- **Overuse:** It can be tempting to use feature flags as a catch-all solution, but this can lead to code complexity and maintenance issues. It's important to carefully consider whether a feature flag is the appropriate solution for a given situation, rather than relying on them as a default.\n- **Lack of documentation:** If a feature flag management solution does not track data such as the owner and purpose of a specific flag, it can be challenging to identify and document this information. When employees change or time passes, teams may need to rediscover the original purpose of the flag in order to determine whether it is still needed or risk leaving it in the code without understanding its purpose. This can lead to confusion and the potential for unintended consequences.\n\n![](/blog-images/aa7092ae4b95a8a14c5e73cc2b8367a1.jpeg)\n\n### **Frequently Asked Questions (FAQs)**\n\n#### When should you use feature flags?\n\nUse feature flags when you want to release new features to a subset of users or when you need to quickly turn off a feature that's causing issues. They can also be useful for A/B testing or for gradually rolling out features. \n\n#### Where do you store feature flags?\n\nYour team and development process will determine where you store feature flags, which can include configuration files, databases, or a feature flag platform. \n\n#### Should feature flags be removed?\n\nOnce you've tested a feature and it's ready for widespread release, it's generally a good idea to remove the feature flag and clean up any related code. This can help to reduce code complexity and improve maintainability. \n\n### Conclusion\n\nOverall, feature flags are a useful tool for modern software development teams to control the release of new features. By following best practices and carefully considering the challenges surrounding their use, teams can effectively leverage feature flags to enable more agile and responsive development, as well as to better control the user experience. \n\nNow you've learned about feature flags and best practices for using them to control the release of new features in software development. You also got answers to common questions and discovered how to effectively leverage feature flags to improve your team's workflow. Check out [the complete guide to automated software environments](https://release.com/ebook/the-complete-guide-to-automated-software-environments) by Release to help speed up your workflow and get your apps and projects running smoothly. \n\n_This post was written by Israel Oyetunji._ [_Israel_](https://twitter.com/israelmitolu) _is a frontend developer with a knack for creating engaging UI and interactive experiences. He has proven experience developing consumer-focused websites using HTML, CSS, JavaScript, React JS, SASS, and relevant technologies. He loves writing about tech and creating how-to tutorials for developers._\n",
          "code": "var Component=(()=>{var h=Object.create;var r=Object.defineProperty;var f=Object.getOwnPropertyDescriptor;var d=Object.getOwnPropertyNames;var g=Object.getPrototypeOf,m=Object.prototype.hasOwnProperty;var p=(n,e)=>()=>(e||n((e={exports:{}}).exports,e),e.exports),y=(n,e)=>{for(var t in e)r(n,t,{get:e[t],enumerable:!0})},i=(n,e,t,o)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let s of d(e))!m.call(n,s)&&s!==t&&r(n,s,{get:()=>e[s],enumerable:!(o=f(e,s))||o.enumerable});return n};var b=(n,e,t)=>(t=n!=null?h(g(n)):{},i(e||!n||!n.__esModule?r(t,\"default\",{value:n,enumerable:!0}):t,n)),w=n=>i(r({},\"__esModule\",{value:!0}),n);var c=p((F,l)=>{l.exports=_jsx_runtime});var x={};y(x,{default:()=>N,frontmatter:()=>v});var a=b(c()),v={title:\"10 Best Practices When Using Feature Flags\",summary:\"This post will explore what feature flags are, answer common questions, and discuss the best practices for using them.\",publishDate:\"Mon Jan 09 2023 08:06:23 GMT+0000 (Coordinated Universal Time)\",author:\"israel-oyetunji\",readingTime:9,categories:[\"platform-engineering\",\"product\"],mainImage:\"/blog-images/35d3a8c5eb5d8692679503d011fee3da.svg\",imageAlt:\"10 Best Practices When Using Feature Flags\",showCTA:!0,ctaCopy:\"Accelerate feature testing and deployment with Release's on-demand environments, aligning with best practices for feature flags.\",ctaLink:\"https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=10-best-practices-when-using-feature-flags\",relatedPosts:[\"\"],ogImage:\"/blog-images/35d3a8c5eb5d8692679503d011fee3da.svg\",excerpt:\"This post will explore what feature flags are, answer common questions, and discuss the best practices for using them.\",tags:[\"platform-engineering\",\"product\"],ctaButton:\"Try Release for Free\"};function u(n){let e=Object.assign({p:\"p\",img:\"img\",h3:\"h3\",a:\"a\",span:\"span\",ul:\"ul\",li:\"li\",h4:\"h4\",strong:\"strong\",em:\"em\"},n.components);return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(e.p,{children:\"Feature flags are a powerful tool for software development teams to control the release of new features. By using feature flags, teams can easily turn features on or off in real time. This allows for the quick and safe release of new functionality to users.\\xA0\"}),`\n`,(0,a.jsx)(e.p,{children:\"In this post, we'll explore what feature flags are. We'll also cover best practices for using them and discuss common challenges and questions surrounding their use.\\xA0\"}),`\n`,(0,a.jsx)(e.p,{children:(0,a.jsx)(e.img,{src:\"/blog-images/8315fbc5578c48ba18e7f5df3ef99ffa.png\",alt:\"\"})}),`\n`,(0,a.jsxs)(e.h3,{id:\"what-are-feature-flags\",children:[(0,a.jsx)(e.a,{className:\"anchor\",href:\"#what-are-feature-flags\",children:(0,a.jsx)(e.span,{className:\"icon icon-link\"})}),\"What are Feature Flags?\"]}),`\n`,(0,a.jsxs)(e.p,{children:[\"Feature flags, also known as \",(0,a.jsx)(e.a,{href:\"https://en.wikipedia.org/wiki/Feature_toggle\",children:\"feature toggle\"}),\" or feature switch, are a software development technique. They allow developers to enable or disable certain features or functionality in their application without deploying new code. This allows for more flexible and efficient software development. Using feature flags, teams can quickly and easily test and roll out new features. They can also manage these features without impacting the stability and reliability of the application.\\xA0\"]}),`\n`,(0,a.jsx)(e.p,{children:\"Feature flags provide a number of benefits, including the ability to:\\xA0\"}),`\n`,(0,a.jsxs)(e.ul,{children:[`\n`,(0,a.jsx)(e.li,{children:\"test new features with a small group of users before rolling them out to the entire user base;\"}),`\n`,(0,a.jsx)(e.li,{children:\"roll out new features gradually, rather than all at once;\"}),`\n`,(0,a.jsx)(e.li,{children:\"quickly disable or roll back features that are causing problems or are not performing as expected; and\"}),`\n`,(0,a.jsx)(e.li,{children:\"customize user experience for different groups of users or for specific regions or markets.\"}),`\n`]}),`\n`,(0,a.jsxs)(e.h3,{id:\"best-practices-when-using-feature-flags\",children:[(0,a.jsx)(e.a,{className:\"anchor\",href:\"#best-practices-when-using-feature-flags\",children:(0,a.jsx)(e.span,{className:\"icon icon-link\"})}),\"Best Practices When Using Feature Flags\"]}),`\n`,(0,a.jsx)(e.p,{children:\"To fully realize the benefits of feature flags, it's important to follow best practices when using them.\\xA0\"}),`\n`,(0,a.jsxs)(e.h4,{id:\"1-clean-up-unused-feature-flags\",children:[(0,a.jsx)(e.a,{className:\"anchor\",href:\"#1-clean-up-unused-feature-flags\",children:(0,a.jsx)(e.span,{className:\"icon icon-link\"})}),\"1. Clean up unused feature flags\"]}),`\n`,(0,a.jsx)(e.p,{children:\"You should regularly remove temporary feature flags, such as release flags and experiment flags, when they are no longer needed. This helps prevent the accumulation of technical debt in the code. It also keeps the feature flag management system organized and easy to understand. Each flag should have a specific, independent purpose, and the code should be modular enough to allow different features to be turned on in any combination.\\xA0\"}),`\n`,(0,a.jsx)(e.p,{children:\"However, if multiple flags are required or may conflict with each other, using flags can become confusing and may have a negative impact on user experience. To avoid these problems, you must carefully plan and manage the use of feature flags.\\xA0\"}),`\n`,(0,a.jsxs)(e.h4,{id:\"2-use-a-feature-flag-management-platform\",children:[(0,a.jsx)(e.a,{className:\"anchor\",href:\"#2-use-a-feature-flag-management-platform\",children:(0,a.jsx)(e.span,{className:\"icon icon-link\"})}),\"2. Use a feature flag management platform\"]}),`\n`,(0,a.jsx)(e.p,{children:\"It's important to choose a feature flag management system that's easy to understand and use. These characteristics are essential whether it's a specialized tool, a config file, or a database table. When introducing feature flags to a team, take the time to carefully consider the best system for your needs. That way, the solution you pick can be used long term.\\xA0\"}),`\n`,(0,a.jsx)(e.p,{children:\"A few popular feature flag management platforms to consider are Harness and LaunchDarkly.\\xA0\"}),`\n`,(0,a.jsxs)(e.h4,{id:\"3-establish-naming-conventions\",children:[(0,a.jsx)(e.a,{className:\"anchor\",href:\"#3-establish-naming-conventions\",children:(0,a.jsx)(e.span,{className:\"icon icon-link\"})}),\"3. Establish naming conventions\"]}),`\n`,(0,a.jsx)(e.p,{children:\"Make your feature flags easy to understand and use by establishing a naming convention. Good, descriptive naming conventions and clear documentation can help ensure that all software engineers understand the purpose and use of each flag.\\xA0\"}),`\n`,(0,a.jsx)(e.p,{children:'When creating a naming convention, consider including a prefix with the project or team name, indicating whether the flag is temporary or permanent, and including a creation date. It may also be helpful to include the word \"flag\" in the name if using a homegrown solution, as this can clarify the purpose of the code.\\xA0'}),`\n`,(0,a.jsx)(e.p,{children:\"In general, it's a good idea to follow a style guide for code that includes conventions for things like camelCase and indentation, as this can make it easier to read and understand the code. By establishing clear naming conventions and style guidelines for feature flags, you can improve the maintainability and readability of your codebase.\\xA0\"}),`\n`,(0,a.jsxs)(e.h4,{id:\"4-use-feature-flags-for-small-test-releases\",children:[(0,a.jsx)(e.a,{className:\"anchor\",href:\"#4-use-feature-flags-for-small-test-releases\",children:(0,a.jsx)(e.span,{className:\"icon icon-link\"})}),\"4. Use feature flags for small test releases\"]}),`\n`,(0,a.jsx)(e.p,{children:\"Big, new feature releases can be stressful and risky, as they can potentially impact the stability and reliability of the application. However, you can reduce this risk by using feature flags to expose your new feature to a small audience first, monitor the effects, and roll back if necessary.\\xA0\"}),`\n`,(0,a.jsx)(e.p,{children:\"One of the key benefits of feature flags is the ability to perform canary releases, or the gradual rollout of a new feature to a small group of users before making it available to the entire user base. This allows you to test the functionality of the new feature and gather feedback from a small group of users before exposing it to the entire user base.\\xA0\"}),`\n`,(0,a.jsx)(e.p,{children:(0,a.jsx)(e.img,{src:\"/blog-images/8c1409159ad8d20252accb782f2240ec.jpeg\",alt:\"\"})}),`\n`,(0,a.jsxs)(e.h4,{id:\"5-avoid-dependencies-between-flags\",children:[(0,a.jsx)(e.a,{className:\"anchor\",href:\"#5-avoid-dependencies-between-flags\",children:(0,a.jsx)(e.span,{className:\"icon icon-link\"})}),\"5. Avoid dependencies between flags\"]}),`\n`,(0,a.jsx)(e.p,{children:\"Ensure that each feature flag serves a specific, independent purpose. When multiple flags are required for a single release or the state of the flag conflicts with another flag, it can lead to confusion and make it difficult to maintain the code. This can also have a negative impact on user experience. To avoid these issues, you must carefully plan and manage the use of feature flags, ensuring that they're used effectively and efficiently.\\xA0\"}),`\n`,(0,a.jsxs)(e.h4,{id:\"6-use-targeted-feature-flags\",children:[(0,a.jsx)(e.a,{className:\"anchor\",href:\"#6-use-targeted-feature-flags\",children:(0,a.jsx)(e.span,{className:\"icon icon-link\"})}),\"6. Use targeted feature flags\"]}),`\n`,(0,a.jsx)(e.p,{children:\"Use feature flags to customize the user experience. Feature flags can be used to customize user experience for different groups of users or for specific regions or markets. By using targeted feature flags, developers can enable or disable features for specific users or groups based on various criteria such as location, language, or user role.\\xA0\"}),`\n`,(0,a.jsx)(e.p,{children:\"One popular use of feature flags is to manage styling, themes, and personalized content. For example, a feature flag could be used to enable or disable dark mode for a website or app. This allows developers to test the new feature with a subset of users before rolling it out to the entire user base.\\xA0\"}),`\n`,(0,a.jsxs)(e.h4,{id:\"7-use-feature-flags-to-enable-feature-branches\",children:[(0,a.jsx)(e.a,{className:\"anchor\",href:\"#7-use-feature-flags-to-enable-feature-branches\",children:(0,a.jsx)(e.span,{className:\"icon icon-link\"})}),\"7. Use feature flags to enable feature branches\"]}),`\n`,(0,a.jsx)(e.p,{children:\"Feature branches allow developers to work on new features in a separate branch of the codebase without impacting the stability and reliability of the main branch. This can be useful for developing and testing new features without disrupting the main development process.\\xA0\"}),`\n`,(0,a.jsxs)(e.h4,{id:\"8-track-changes-with-audit-logs\",children:[(0,a.jsx)(e.a,{className:\"anchor\",href:\"#8-track-changes-with-audit-logs\",children:(0,a.jsx)(e.span,{className:\"icon icon-link\"})}),\"8. Track changes with audit logs\"]}),`\n`,(0,a.jsx)(e.p,{children:\"An audit log can be a useful tool for tracking and managing changes to feature flags. This log can provide a record of all changes made to each flag, including the identity of the person making the change and the date and time of the change.\\xA0\"}),`\n`,(0,a.jsx)(e.p,{children:\"An audit log can help ensure transparency and visibility in the implementation of feature flag changes. Changes to feature flags can be particularly important in regulated industries such as finance and healthcare. By restricting access to the audit log to a limited number of authorized individuals, it's also possible to enhance the security of the process and protect sensitive flags from unauthorized changes.\\xA0\"}),`\n`,(0,a.jsxs)(e.h4,{id:\"9-control-access-based-on-policies\",children:[(0,a.jsx)(e.a,{className:\"anchor\",href:\"#9-control-access-based-on-policies\",children:(0,a.jsx)(e.span,{className:\"icon icon-link\"})}),\"9. Control access based on policies\"]}),`\n`,(0,a.jsx)(e.p,{children:\"You may want to control access to feature flags based on policies. One of the benefits of feature flags is that they can be accessed by nontechnical team members, such as the product team, who can use them to assist with A/B testing. Another way to limit access is to allow only administrators to toggle a feature flag in a production-related environment.\\xA0\"}),`\n`,(0,a.jsx)(e.p,{children:\"However, it's important to carefully control access to feature flags and to track changes made to them. This can involve locking changes in the production environment or maintaining a log of who's modified which flags. By taking these precautions, you can ensure that feature flags are used effectively and that changes are made by authorized personnel.\\xA0\"}),`\n`,(0,a.jsxs)(e.h4,{id:\"10-plan-ahead-for-feature-flags\",children:[(0,a.jsx)(e.a,{className:\"anchor\",href:\"#10-plan-ahead-for-feature-flags\",children:(0,a.jsx)(e.span,{className:\"icon icon-link\"})}),\"10. Plan ahead for feature flags\"]}),`\n`,(0,a.jsx)(e.p,{children:\"Proper planning is key to the successful implementation of feature flags. Rather than treating feature flags as an afterthought, it's important to consider them during the design process. This will help you determine whether a flag should be temporary or permanent. You'll then be able to plan accordingly for things like naming conventions, configuration settings, review and removal processes, and access control and safety checks. By planning carefully for all flags upfront, you can increase the chances of success and ensure that your feature flags are implemented effectively.\\xA0\"}),`\n`,(0,a.jsxs)(e.h3,{id:\"challenges-of-using-feature-flags\",children:[(0,a.jsx)(e.a,{className:\"anchor\",href:\"#challenges-of-using-feature-flags\",children:(0,a.jsx)(e.span,{className:\"icon icon-link\"})}),\"Challenges of Using Feature Flags\"]}),`\n`,(0,a.jsx)(e.p,{children:\"Feature flags can be a useful tool in software development, but they also come with their own set of challenges. Some common challenges of using feature flags include:\\xA0\"}),`\n`,(0,a.jsxs)(e.ul,{children:[`\n`,(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:\"Technical debt:\"}),\" Using feature flags can lead to technical debt if the flags are not maintained and updated over time. This can result in increased maintenance costs and may require dedicated resources to address.\"]}),`\n`,(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:\"Feature creep:\"}),\" Feature flags can also lead to feature creep, where more and more features are added over time without a clear plan or goal. This can result in a cluttered and confusing user experience.\"]}),`\n`,(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:\"Complexity:\"}),\" Managing multiple feature flags can be complex, especially as the number of flags increases. Difficulty in keeping track of enabled and disabled flags can lead to potential issues with code changes and deployments.\"]}),`\n`,(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:\"Overuse:\"}),\" It can be tempting to use feature flags as a catch-all solution, but this can lead to code complexity and maintenance issues. It's important to carefully consider whether a feature flag is the appropriate solution for a given situation, rather than relying on them as a default.\"]}),`\n`,(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:\"Lack of documentation:\"}),\" If a feature flag management solution does not track data such as the owner and purpose of a specific flag, it can be challenging to identify and document this information. When employees change or time passes, teams may need to rediscover the original purpose of the flag in order to determine whether it is still needed or risk leaving it in the code without understanding its purpose. This can lead to confusion and the potential for unintended consequences.\"]}),`\n`]}),`\n`,(0,a.jsx)(e.p,{children:(0,a.jsx)(e.img,{src:\"/blog-images/aa7092ae4b95a8a14c5e73cc2b8367a1.jpeg\",alt:\"\"})}),`\n`,(0,a.jsxs)(e.h3,{id:\"frequently-asked-questions-faqs\",children:[(0,a.jsx)(e.a,{className:\"anchor\",href:\"#frequently-asked-questions-faqs\",children:(0,a.jsx)(e.span,{className:\"icon icon-link\"})}),(0,a.jsx)(e.strong,{children:\"Frequently Asked Questions (FAQs)\"})]}),`\n`,(0,a.jsxs)(e.h4,{id:\"when-should-you-use-feature-flags\",children:[(0,a.jsx)(e.a,{className:\"anchor\",href:\"#when-should-you-use-feature-flags\",children:(0,a.jsx)(e.span,{className:\"icon icon-link\"})}),\"When should you use feature flags?\"]}),`\n`,(0,a.jsx)(e.p,{children:\"Use feature flags when you want to release new features to a subset of users or when you need to quickly turn off a feature that's causing issues. They can also be useful for A/B testing or for gradually rolling out features.\\xA0\"}),`\n`,(0,a.jsxs)(e.h4,{id:\"where-do-you-store-feature-flags\",children:[(0,a.jsx)(e.a,{className:\"anchor\",href:\"#where-do-you-store-feature-flags\",children:(0,a.jsx)(e.span,{className:\"icon icon-link\"})}),\"Where do you store feature flags?\"]}),`\n`,(0,a.jsx)(e.p,{children:\"Your team and development process will determine where you store feature flags, which can include configuration files, databases, or a feature flag platform.\\xA0\"}),`\n`,(0,a.jsxs)(e.h4,{id:\"should-feature-flags-be-removed\",children:[(0,a.jsx)(e.a,{className:\"anchor\",href:\"#should-feature-flags-be-removed\",children:(0,a.jsx)(e.span,{className:\"icon icon-link\"})}),\"Should feature flags be removed?\"]}),`\n`,(0,a.jsx)(e.p,{children:\"Once you've tested a feature and it's ready for widespread release, it's generally a good idea to remove the feature flag and clean up any related code. This can help to reduce code complexity and improve maintainability.\\xA0\"}),`\n`,(0,a.jsxs)(e.h3,{id:\"conclusion\",children:[(0,a.jsx)(e.a,{className:\"anchor\",href:\"#conclusion\",children:(0,a.jsx)(e.span,{className:\"icon icon-link\"})}),\"Conclusion\"]}),`\n`,(0,a.jsx)(e.p,{children:\"Overall, feature flags are a useful tool for modern software development teams to control the release of new features. By following best practices and carefully considering the challenges surrounding their use, teams can effectively leverage feature flags to enable more agile and responsive development, as well as to better control the user experience.\\xA0\"}),`\n`,(0,a.jsxs)(e.p,{children:[\"Now you've learned about feature flags and best practices for using them to control the release of new features in software development. You also got answers to common questions and discovered how to effectively leverage feature flags to improve your team's workflow. Check out \",(0,a.jsx)(e.a,{href:\"https://release.com/ebook/the-complete-guide-to-automated-software-environments\",children:\"the complete guide to automated software environments\"}),\" by Release to help speed up your workflow and get your apps and projects running smoothly.\\xA0\"]}),`\n`,(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.em,{children:\"This post was written by Israel Oyetunji.\"}),\" \",(0,a.jsx)(e.a,{href:\"https://twitter.com/israelmitolu\",children:(0,a.jsx)(e.em,{children:\"Israel\"})}),\" \",(0,a.jsx)(e.em,{children:\"is a frontend developer with a knack for creating engaging UI and interactive experiences. He has proven experience developing consumer-focused websites using HTML, CSS, JavaScript, React JS, SASS, and relevant technologies. He loves writing about tech and creating how-to tutorials for developers.\"})]})]})}function k(n={}){let{wrapper:e}=n.components||{};return e?(0,a.jsx)(e,Object.assign({},n,{children:(0,a.jsx)(u,n)})):u(n)}var N=k;return w(x);})();\n;return Component;"
        },
        "_id": "blog/posts/10-best-practices-when-using-feature-flags.mdx",
        "_raw": {
          "sourceFilePath": "blog/posts/10-best-practices-when-using-feature-flags.mdx",
          "sourceFileName": "10-best-practices-when-using-feature-flags.mdx",
          "sourceFileDir": "blog/posts",
          "contentType": "mdx",
          "flattenedPath": "blog/posts/10-best-practices-when-using-feature-flags"
        },
        "type": "BlogPost",
        "computedSlug": "10-best-practices-when-using-feature-flags"
      },
      "documentHash": "1739393595014",
      "hasWarnings": false,
      "documentTypeName": "BlogPost"
    },
    "blog/posts/10-kubernetes-namespace-best-practices-to-start-following.mdx": {
      "document": {
        "title": "10 Kubernetes Namespace Best Practices to Start Following",
        "summary": "This post will discuss how you can use kubernetes namespace to achieve even more efficiency by following best practices.",
        "publishDate": "Mon Sep 19 2022 18:53:42 GMT+0000 (Coordinated Universal Time)",
        "author": "marion-newman",
        "readingTime": 5,
        "categories": [
          "kubernetes",
          "platform-engineering"
        ],
        "mainImage": "/blog-images/b90c46ea0d9d01c1c904f3758308d591.jpg",
        "imageAlt": "a person writing on a book",
        "showCTA": true,
        "ctaCopy": "Improve Kubernetes efficiency with Release's on-demand environments, following best practices for resource segregation and allocation.",
        "ctaLink": "https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=10-kubernetes-namespace-best-practices-to-start-following",
        "relatedPosts": [
          ""
        ],
        "ogImage": "/blog-images/b90c46ea0d9d01c1c904f3758308d591.jpg",
        "excerpt": "This post will discuss how you can use kubernetes namespace to achieve even more efficiency by following best practices.",
        "tags": [
          "kubernetes",
          "platform-engineering"
        ],
        "ctaButton": "Try Release for Free",
        "body": {
          "raw": "\nNamespaces are the most efficient resource segregation and allocation method in the Kubernetes cluster.\n\nThis post will discuss how you can use namespaces to improve efficiency by following best practices.\n\n![](/blog-images/0960ec04f74ef2c8f6a971e6d156cbf7.png)\n\n### What are Namespaces in Kubernetes, and Why are They Important?\n\nBefore moving to best practices, let’s briefly look into what namespaces are and why you should use them.\n\nSuppose you want to use a single cluster as both your performance and dev clusters in order to save cost on computational resources. To do this in Kubernetes, you can use namespaces to segregate [pod](https://release.com/blog/kubernetes-pod-a-beginners-guide-to-an-essential-resource) services while running them on a single cluster.\n\nThe namespace system is not new to computing; almost all programming languages use namespaces. Wherever you have encountered namespaces, the fundamental purpose is the same: They are used for logical grouping.\n\nNamespaces are a feature of the Linux kernel, and containers use namespaces extensively. Each container has its own storage namespace and network namespace for the segregation and allocation of resources.\n\nThe [Kubernetes namespace](https://release.com/blog/kubernetes-namespaces-the-ultimate-guide) refers to virtual clusters that are backed by the same physical cluster. This option is designed for use in environments with multiple users spread across multiple work teams or projects.\n\n### Types of Kubernetes Namespaces\n\nThese are the four initial namespaces that Kubernetes starts with:\n\n- **\"default\"—**The default namespace set by the system. It's intended for objects that don't specify any of the namespaces.\n- **\"kube-system\"—**This namespace is assigned to resources that are created by the Kubernetes system.\n- **\"kube-public\"—**This namespace is created by the system and is visible to all users, even users that aren't authenticated. Usually, this namespace is focused on the internal use of the platform cluster in situations where some of the resources need to be publicly visible and readable for the entire cluster.\n- ‍**\"kube-node-lease \"—**This namespace holds lease objects associated with each node. These leases allow the kubelet to send heartbeats so that you can determine node availability.\n\nIn addition to these four namespaces, you can create custom namespaces.\n\n### Naming Convention of Kubernetes Namespaces\n\nNamespaces in Kubernetes follow the same naming convention as other objects created in Kubernetes.\n\nYou can create a name with a maximum length of 253 characters using only alphanumeric characters and hyphens. Names cannot start with a hyphen and the alpha characters can only be lowercase.\n\n### 10 Kubernetes Namespace Best Practices\n\nLet's take a look at 10 Kubernetes namespace best practices so that you can get the most out of this feature.\n\nIt's important to note that the actual utility of these practices depends on your particular needs and the nature of the project.\n\n#### 1\\. Use Convenient and Scalable Names\n\nNaming is at the root of programming and is one of its basic building blocks. Names should be meaningful and provide context. Therefore, it’s recommended to use names that are expressive and scalable.\n\nFor example, if you're working on a streaming application, you can name the namespace \"stream\". For the different development environments, you can scale this name by adding a suffix, for example, \"stream-dev\" for the development environment, \"stream-test\" for testing, and \"stream-prod\" for production.\n\n#### 2\\. Attach Labels to Namespaces\n\nLabels in Kubernetes are not just a way to distinguish resources, but they're also a major source of metadata that can be used to log, analyze, and audit resources.\n\nThough it’s considered a best practice to use labels throughout Kubernetes, using them in a namespace is essential when you have a large team. Here is an example:\n\n```yaml\nkubectl create namespace namespace_name\nkubectl label namespaces namespace_name labelname=value --overwrite=true\n```\n\n#### 3\\. Use RBAC to Allocate Resources\n\nUsing role-based access control (RBAC), you can authorize and limit users’ access to certain resources. You can manage access locally within a cluster and globally to the entire cluster.\n\nTo use RBAC for a specific namespace, you can use the _Role_ resource type while the _ClusterRole_ resource type can be used globally.\n\nUsing RBAC helps you to secure clusters and manage resources by defining permissions based on roles.\n\n#### 4\\. Use _ResourceQuota_ and _LimitRange_\n\nThe namespaces in a cluster don't all need the same resources. Giving all namespaces equal resources can compromise the performance of key namespaces. Use a resource quota to limit the resource usage of particular namespaces.\n\nUse Kubernetes _ResourceQuota_ to control the number of resources that can be created in a namespace and _LimitRange_ to restrict the consumption of resources by pods.\n\nHere's an example of how to use _ResourceQuota_:\n\n```yaml\n\napiVersion: v1\n      kind: ResourceQuota\n      name: default-resourcequota\n      synchronize: true\n      namespace: stream\n      data:\n        spec:\n          hard:\n            requests.cpu: '4'\n            requests.memory: ‘16Gi’\n            limits.cpu: '4'\n            limits.memory: ‘16Gi’\n\n```\n\nAnd here's an example of how to use _LimitRange_:\n\n```yaml\n\napiVersion: v1\n      kind: LimitRange\n      name: default-limitrange\n      synchronize: true\n      namespace: stream\n      data:\n        spec:\n          limits:\n          - default:\n              cpu: 500m\n              memory: 1Gi\n            defaultRequest:\n              cpu: 200m\n              memory: 256Mi\n\n```\n\n![](/blog-images/8129568b7e7e6ef40c342ba8bd48df8a.png)\n\n#### 5\\. Use a NetworkPolicy\n\nKubernetes allows different pods across clusters to communicate. To secure the pods and only allow the desired traffic to pods from selected sources, it's necessary to use a NetworkPolicy for each namespace along with a CNI plugin to restrict communications.\n\nUsing a NetworkPolicy will allow you to deny ingress, egress, or any unwanted traffic coming into pods through the namespace.  \n\nHere's an example of how to use a NetworkPolicy:\n\n```yaml\n\napiVersion: networking.k8s.io/v1\n      kind: NetworkPolicy\n      name: default-deny\n      namespace: stream\n      synchronize: true\n      data:\n        spec:\n          # select all pods in the Namespace\n          podSelector: {}\n          # deny selected or all traffic\n          policyTypes:\n          - Ingress\n          - Egress\n\n```\n\n#### 6\\. Don’t Create Too Many Namespaces\n\nEven though there's no restriction on how many namespaces you can create and how many namespaces Kubernetes can handle, it's best to avoid creating too many namespaces.\n\nCreating namespaces without any definite function can become difficult to manage and too many namespaces can affect the efficient consumption of resources.\n\n#### 7\\. Don’t Shy Away From Creating a Cluster\n\nNamespaces are used to create virtual clusters to segregate resources and reduce costs. However, it's important to understand that as your team grows, the better FinOps approach is to create additional clusters rather than creating namespaces so that you don't to compromise on performance.\n\n#### 8\\. Don’t Use the Default Namespace\n\nAll objects created without a specified namespace are placed in the Kubernetes \"default\" namespace. If you use the \"default\" namespace, it can become difficult to segregate objects in it or implement RBAC and NetworkPolicies.\n\n#### 9\\. Have an Idea of What’s Inside\n\nFor better management of the Kubernetes cluster, it's important to understand which objects and resources are located in namespaces. This includes objects such as pods, replication controllers managed by the Kubernetes controller manager, and others.\n\nHowever, some elements are responsible for representing these resources are found outside Kubernetes namespaces. Additionally, low-level resources, such as persistent volumes and nodes, aren't found within namespaces. Services like [Release](https://release.com/) use dynamic provisioning in Kubernetes to provide on-demand environments that reduce the management overhead required to create persistent volumes.\n\n![](/blog-images/3a54975116f4f4b2a7d02a6c569e61f4.png)\n\n#### 10\\. Sync Secrets\n\nSecrets in Kubernetes often need to exist in multiple namespaces in a cluster so pods can access them. Registry credentials, for example, need to exist in all namespaces in a cluster. If you have many namespaces, managing registry credentials manually can be tricky.\n\nSyncing secrets allows you to copy \"regcred\" to all new namespaces when they are created and pushes updates to the copied secrets.\n\n### Conclusion\n\nThe use of namespaces in Kubernetes is a convenient way to organize and manage resources. If you know how to use namespaces effectively, it can make a significant difference in the performance of your Kubernetes workflows and help you enhance the operability of your clusters while saving on cost.\n\n‍\n",
          "code": "var Component=(()=>{var m=Object.create;var c=Object.defineProperty;var d=Object.getOwnPropertyDescriptor;var u=Object.getOwnPropertyNames;var p=Object.getPrototypeOf,f=Object.prototype.hasOwnProperty;var g=(a,e)=>()=>(e||a((e={exports:{}}).exports,e),e.exports),b=(a,e)=>{for(var s in e)c(a,s,{get:e[s],enumerable:!0})},o=(a,e,s,r)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let t of u(e))!f.call(a,t)&&t!==s&&c(a,t,{get:()=>e[t],enumerable:!(r=d(e,t))||r.enumerable});return a};var y=(a,e,s)=>(s=a!=null?m(p(a)):{},o(e||!a||!a.__esModule?c(s,\"default\",{value:a,enumerable:!0}):s,a)),w=a=>o(c({},\"__esModule\",{value:!0}),a);var l=g((x,i)=>{i.exports=_jsx_runtime});var T={};b(T,{default:()=>N,frontmatter:()=>k});var n=y(l()),k={title:\"10 Kubernetes Namespace Best Practices to Start Following\",summary:\"This post will discuss how you can use kubernetes namespace to achieve even more efficiency by following best practices.\",publishDate:\"Mon Sep 19 2022 18:53:42 GMT+0000 (Coordinated Universal Time)\",author:\"marion-newman\",readingTime:5,categories:[\"kubernetes\",\"platform-engineering\"],mainImage:\"/blog-images/b90c46ea0d9d01c1c904f3758308d591.jpg\",imageAlt:\"a person writing on a book\",showCTA:!0,ctaCopy:\"Improve Kubernetes efficiency with Release's on-demand environments, following best practices for resource segregation and allocation.\",ctaLink:\"https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=10-kubernetes-namespace-best-practices-to-start-following\",relatedPosts:[\"\"],ogImage:\"/blog-images/b90c46ea0d9d01c1c904f3758308d591.jpg\",excerpt:\"This post will discuss how you can use kubernetes namespace to achieve even more efficiency by following best practices.\",tags:[\"kubernetes\",\"platform-engineering\"],ctaButton:\"Try Release for Free\"};function h(a){let e=Object.assign({p:\"p\",img:\"img\",h3:\"h3\",a:\"a\",span:\"span\",ul:\"ul\",li:\"li\",h4:\"h4\",pre:\"pre\",code:\"code\",em:\"em\"},a.components);return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(e.p,{children:\"Namespaces are the most efficient resource segregation and allocation method in the Kubernetes cluster.\"}),`\n`,(0,n.jsx)(e.p,{children:\"This post will discuss how you can use namespaces to improve efficiency by following best practices.\"}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/0960ec04f74ef2c8f6a971e6d156cbf7.png\",alt:\"\"})}),`\n`,(0,n.jsxs)(e.h3,{id:\"what-are-namespaces-in-kubernetes-and-why-are-they-important\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#what-are-namespaces-in-kubernetes-and-why-are-they-important\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"What are Namespaces in Kubernetes, and Why are They Important?\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Before moving to best practices, let\\u2019s briefly look into what namespaces are and why you should use them.\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"Suppose you want to use a single cluster as both your performance and dev clusters in order to save cost on computational resources. To do this in Kubernetes, you can use namespaces to segregate \",(0,n.jsx)(e.a,{href:\"https://release.com/blog/kubernetes-pod-a-beginners-guide-to-an-essential-resource\",children:\"pod\"}),\" services while running them on a single cluster.\"]}),`\n`,(0,n.jsx)(e.p,{children:\"The namespace system is not new to computing; almost all programming languages use namespaces. Wherever you have encountered namespaces, the fundamental purpose is the same: They are used for logical grouping.\"}),`\n`,(0,n.jsx)(e.p,{children:\"Namespaces are a feature of the Linux kernel, and containers use namespaces extensively. Each container has its own storage namespace and network namespace for the segregation and allocation of resources.\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"The \",(0,n.jsx)(e.a,{href:\"https://release.com/blog/kubernetes-namespaces-the-ultimate-guide\",children:\"Kubernetes namespace\"}),\" refers to virtual clusters that are backed by the same physical cluster. This option is designed for use in environments with multiple users spread across multiple work teams or projects.\"]}),`\n`,(0,n.jsxs)(e.h3,{id:\"types-of-kubernetes-namespaces\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#types-of-kubernetes-namespaces\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Types of Kubernetes Namespaces\"]}),`\n`,(0,n.jsx)(e.p,{children:\"These are the four initial namespaces that Kubernetes starts with:\"}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsx)(e.li,{children:`**\"default\"\\u2014**The default namespace set by the system. It's intended for objects that don't specify any of the namespaces.`}),`\n`,(0,n.jsx)(e.li,{children:'**\"kube-system\"\\u2014**This namespace is assigned to resources that are created by the Kubernetes system.'}),`\n`,(0,n.jsx)(e.li,{children:`**\"kube-public\"\\u2014**This namespace is created by the system and is visible to all users, even users that aren't authenticated. Usually, this namespace is focused on the internal use of the platform cluster in situations where some of the resources need to be publicly visible and readable for the entire cluster.`}),`\n`,(0,n.jsx)(e.li,{children:'\\u200D**\"kube-node-lease \"\\u2014**This namespace holds lease objects associated with each node. These leases allow the kubelet to send heartbeats so that you can determine node availability.'}),`\n`]}),`\n`,(0,n.jsx)(e.p,{children:\"In addition to these four namespaces, you can create custom namespaces.\"}),`\n`,(0,n.jsxs)(e.h3,{id:\"naming-convention-of-kubernetes-namespaces\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#naming-convention-of-kubernetes-namespaces\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Naming Convention of Kubernetes Namespaces\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Namespaces in Kubernetes follow the same naming convention as other objects created in Kubernetes.\"}),`\n`,(0,n.jsx)(e.p,{children:\"You can create a name with a maximum length of 253 characters using only alphanumeric characters and hyphens. Names cannot start with a hyphen and the alpha characters can only be lowercase.\"}),`\n`,(0,n.jsxs)(e.h3,{id:\"10-kubernetes-namespace-best-practices\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#10-kubernetes-namespace-best-practices\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"10 Kubernetes Namespace Best Practices\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Let's take a look at 10 Kubernetes namespace best practices so that you can get the most out of this feature.\"}),`\n`,(0,n.jsx)(e.p,{children:\"It's important to note that the actual utility of these practices depends on your particular needs and the nature of the project.\"}),`\n`,(0,n.jsxs)(e.h4,{id:\"1-use-convenient-and-scalable-names\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#1-use-convenient-and-scalable-names\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"1. Use Convenient and Scalable Names\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Naming is at the root of programming and is one of its basic building blocks. Names should be meaningful and provide context. Therefore, it\\u2019s recommended to use names that are expressive and scalable.\"}),`\n`,(0,n.jsx)(e.p,{children:`For example, if you're working on a streaming application, you can name the namespace \"stream\". For the different development environments, you can scale this name by adding a suffix, for example, \"stream-dev\" for the development environment, \"stream-test\" for testing, and \"stream-prod\" for production.`}),`\n`,(0,n.jsxs)(e.h4,{id:\"2-attach-labels-to-namespaces\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#2-attach-labels-to-namespaces\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"2. Attach Labels to Namespaces\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Labels in Kubernetes are not just a way to distinguish resources, but they're also a major source of metadata that can be used to log, analyze, and audit resources.\"}),`\n`,(0,n.jsx)(e.p,{children:\"Though it\\u2019s considered a best practice to use labels throughout Kubernetes, using them in a namespace is essential when you have a large team. Here is an example:\"}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-yaml\",children:`kubectl create namespace namespace_name\nkubectl label namespaces namespace_name labelname=value --overwrite=true\n`})}),`\n`,(0,n.jsxs)(e.h4,{id:\"3-use-rbac-to-allocate-resources\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#3-use-rbac-to-allocate-resources\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"3. Use RBAC to Allocate Resources\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Using role-based access control (RBAC), you can authorize and limit users\\u2019 access to certain resources. You can manage access locally within a cluster and globally to the entire cluster.\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"To use RBAC for a specific namespace, you can use the \",(0,n.jsx)(e.em,{children:\"Role\"}),\" resource type while the \",(0,n.jsx)(e.em,{children:\"ClusterRole\"}),\" resource type can be used globally.\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Using RBAC helps you to secure clusters and manage resources by defining permissions based on roles.\"}),`\n`,(0,n.jsxs)(e.h4,{id:\"4-use-resourcequota-and-limitrange\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#4-use-resourcequota-and-limitrange\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"4. Use \",(0,n.jsx)(e.em,{children:\"ResourceQuota\"}),\" and \",(0,n.jsx)(e.em,{children:\"LimitRange\"})]}),`\n`,(0,n.jsx)(e.p,{children:\"The namespaces in a cluster don't all need the same resources. Giving all namespaces equal resources can compromise the performance of key namespaces. Use a resource quota to limit the resource usage of particular namespaces.\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"Use Kubernetes \",(0,n.jsx)(e.em,{children:\"ResourceQuota\"}),\" to control the number of resources that can be created in a namespace and \",(0,n.jsx)(e.em,{children:\"LimitRange\"}),\" to restrict the consumption of resources by pods.\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"Here's an example of how to use \",(0,n.jsx)(e.em,{children:\"ResourceQuota\"}),\":\"]}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-yaml\",children:`\napiVersion: v1\n \\xA0 \\xA0 \\xA0kind: ResourceQuota\n \\xA0 \\xA0 \\xA0name: default-resourcequota\n \\xA0 \\xA0 \\xA0synchronize: true\n \\xA0 \\xA0 \\xA0namespace: stream\n \\xA0 \\xA0 \\xA0data:\n \\xA0 \\xA0 \\xA0 \\xA0spec:\n \\xA0 \\xA0 \\xA0 \\xA0 \\xA0hard:\n \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0requests.cpu: '4'\n \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0requests.memory: \\u201816Gi\\u2019\n \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0limits.cpu: '4'\n \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0limits.memory: \\u201816Gi\\u2019\n\n`})}),`\n`,(0,n.jsxs)(e.p,{children:[\"And here's an example of how to use \",(0,n.jsx)(e.em,{children:\"LimitRange\"}),\":\"]}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-yaml\",children:`\napiVersion: v1\n \\xA0 \\xA0 \\xA0kind: LimitRange\n \\xA0 \\xA0 \\xA0name: default-limitrange\n \\xA0 \\xA0 \\xA0synchronize: true\n \\xA0 \\xA0 \\xA0namespace: stream\n \\xA0 \\xA0 \\xA0data:\n \\xA0 \\xA0 \\xA0 \\xA0spec:\n \\xA0 \\xA0 \\xA0 \\xA0 \\xA0limits:\n \\xA0 \\xA0 \\xA0 \\xA0 \\xA0- default:\n \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0cpu: 500m\n \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0memory: 1Gi\n \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0defaultRequest:\n \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0cpu: 200m\n \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0memory: 256Mi\n\n`})}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/8129568b7e7e6ef40c342ba8bd48df8a.png\",alt:\"\"})}),`\n`,(0,n.jsxs)(e.h4,{id:\"5-use-a-networkpolicy\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#5-use-a-networkpolicy\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"5. Use a NetworkPolicy\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Kubernetes allows different pods across clusters to communicate. To secure the pods and only allow the desired traffic to pods from selected sources, it's necessary to use a NetworkPolicy for each namespace along with a CNI plugin to restrict communications.\"}),`\n`,(0,n.jsx)(e.p,{children:\"Using a NetworkPolicy will allow you to deny ingress, egress, or any unwanted traffic coming into pods through the namespace. \\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:\"Here's an example of how to use a NetworkPolicy:\"}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-yaml\",children:`\napiVersion: networking.k8s.io/v1\n \\xA0 \\xA0 \\xA0kind: NetworkPolicy\n \\xA0 \\xA0 \\xA0name: default-deny\n \\xA0 \\xA0 \\xA0namespace: stream\n \\xA0 \\xA0 \\xA0synchronize: true\n \\xA0 \\xA0 \\xA0data:\n \\xA0 \\xA0 \\xA0 \\xA0spec:\n \\xA0 \\xA0 \\xA0 \\xA0 \\xA0# select all pods in the Namespace\n \\xA0 \\xA0 \\xA0 \\xA0 \\xA0podSelector: {}\n \\xA0 \\xA0 \\xA0 \\xA0 \\xA0# deny selected or all traffic\n \\xA0 \\xA0 \\xA0 \\xA0 \\xA0policyTypes:\n \\xA0 \\xA0 \\xA0 \\xA0 \\xA0- Ingress\n \\xA0 \\xA0 \\xA0 \\xA0 \\xA0- Egress\n\n`})}),`\n`,(0,n.jsxs)(e.h4,{id:\"6-dont-create-too-many-namespaces\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#6-dont-create-too-many-namespaces\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"6. Don\\u2019t Create Too Many Namespaces\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Even though there's no restriction on how many namespaces you can create and how many namespaces Kubernetes can handle, it's best to avoid creating too many namespaces.\"}),`\n`,(0,n.jsx)(e.p,{children:\"Creating namespaces without any definite function can become difficult to manage and too many namespaces can affect the efficient consumption of resources.\"}),`\n`,(0,n.jsxs)(e.h4,{id:\"7-dont-shy-away-from-creating-a-cluster\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#7-dont-shy-away-from-creating-a-cluster\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"7. Don\\u2019t Shy Away From Creating a Cluster\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Namespaces are used to create virtual clusters to segregate resources and reduce costs. However, it's important to understand that as your team grows, the better FinOps approach is to create additional clusters rather than creating namespaces so that you don't to compromise on performance.\"}),`\n`,(0,n.jsxs)(e.h4,{id:\"8-dont-use-the-default-namespace\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#8-dont-use-the-default-namespace\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"8. Don\\u2019t Use the Default Namespace\"]}),`\n`,(0,n.jsx)(e.p,{children:'All objects created without a specified namespace are placed in the Kubernetes \"default\" namespace. If you use the \"default\" namespace, it can become difficult to segregate objects in it or implement RBAC and NetworkPolicies.'}),`\n`,(0,n.jsxs)(e.h4,{id:\"9-have-an-idea-of-whats-inside\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#9-have-an-idea-of-whats-inside\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"9. Have an Idea of What\\u2019s Inside\"]}),`\n`,(0,n.jsx)(e.p,{children:\"For better management of the Kubernetes cluster, it's important to understand which objects and resources are located in namespaces. This includes objects such as pods, replication controllers managed by the Kubernetes controller manager, and others.\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"However, some elements are responsible for representing these resources are found outside Kubernetes namespaces. Additionally, low-level resources, such as persistent volumes and nodes, aren't found within namespaces. Services like \",(0,n.jsx)(e.a,{href:\"https://release.com/\",children:\"Release\"}),\" use dynamic provisioning in Kubernetes to provide on-demand environments that reduce the management overhead required to create persistent volumes.\"]}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/3a54975116f4f4b2a7d02a6c569e61f4.png\",alt:\"\"})}),`\n`,(0,n.jsxs)(e.h4,{id:\"10-sync-secrets\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#10-sync-secrets\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"10. Sync Secrets\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Secrets in Kubernetes often need to exist in multiple namespaces in a cluster so pods can access them. Registry credentials, for example, need to exist in all namespaces in a cluster. If you have many namespaces, managing registry credentials manually can be tricky.\"}),`\n`,(0,n.jsx)(e.p,{children:'Syncing secrets allows you to copy \"regcred\" to all new namespaces when they are created and pushes updates to the copied secrets.'}),`\n`,(0,n.jsxs)(e.h3,{id:\"conclusion\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#conclusion\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Conclusion\"]}),`\n`,(0,n.jsx)(e.p,{children:\"The use of namespaces in Kubernetes is a convenient way to organize and manage resources. If you know how to use namespaces effectively, it can make a significant difference in the performance of your Kubernetes workflows and help you enhance the operability of your clusters while saving on cost.\"}),`\n`,(0,n.jsx)(e.p,{children:\"\\u200D\"})]})}function v(a={}){let{wrapper:e}=a.components||{};return e?(0,n.jsx)(e,Object.assign({},a,{children:(0,n.jsx)(h,a)})):h(a)}var N=v;return w(T);})();\n;return Component;"
        },
        "_id": "blog/posts/10-kubernetes-namespace-best-practices-to-start-following.mdx",
        "_raw": {
          "sourceFilePath": "blog/posts/10-kubernetes-namespace-best-practices-to-start-following.mdx",
          "sourceFileName": "10-kubernetes-namespace-best-practices-to-start-following.mdx",
          "sourceFileDir": "blog/posts",
          "contentType": "mdx",
          "flattenedPath": "blog/posts/10-kubernetes-namespace-best-practices-to-start-following"
        },
        "type": "BlogPost",
        "computedSlug": "10-kubernetes-namespace-best-practices-to-start-following"
      },
      "documentHash": "1739393595015",
      "hasWarnings": false,
      "documentTypeName": "BlogPost"
    },
    "blog/posts/11-continuous-deployment-tools-and-how-to-choose-one.mdx": {
      "document": {
        "title": "11 Continuous Deployment Tools and How to Choose One",
        "summary": "Create a reliable application with little downtime with the right continuous deployment tools for the DevOps pipeline",
        "publishDate": "Tue Sep 13 2022 19:27:56 GMT+0000 (Coordinated Universal Time)",
        "author": "kevin-luu",
        "readingTime": 6,
        "categories": [
          "platform-engineering",
          "product"
        ],
        "mainImage": "/blog-images/b736c54718aa57a1a9e027fd79651291.jpg",
        "imageAlt": "A bunch of tools",
        "showCTA": true,
        "ctaCopy": "Improve CI/CD workflows with Release's ephemeral environments for faster testing and deployment cycles.",
        "ctaLink": "https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=11-continuous-deployment-tools-and-how-to-choose-one",
        "relatedPosts": [
          ""
        ],
        "ogImage": "/blog-images/b736c54718aa57a1a9e027fd79651291.jpg",
        "excerpt": "Create a reliable application with little downtime with the right continuous deployment tools for the DevOps pipeline",
        "tags": [
          "platform-engineering",
          "product"
        ],
        "ctaButton": "Try Release for Free",
        "body": {
          "raw": "\nThe popularity of agile methodologies among businesses has escalated and resulted in the mass adoption of continuous software development processes. This has given rise to many tools that make implementing your DevOps pipeline and application development easier. Choosing which tool to use to produce a robust system is difficult because of the market surge in continuous deployment tools. \n\nContinuous deployment (CD) is the procedure whereby updates to software code are pipelined, automated, tested, and then made available in the production environment. It is a crucial stage in the DevOps life cycle and assists companies in quickly and efficiently deploying changes, upgrades, and updates at the app level. \n\nIn this article, I'll outline 11 continuous deployment tools that you should consider for deployment processes in your DevOps pipeline. We’ll look at the features and highlight the advantages and disadvantages of each tool. I'll talk about how these technologies may be utilized in DevOps, and finally, we’ll look at a comparison of these tools in detail. \n\n![](/blog-images/656221e0dc7a62d308d1cd6debb33380.jpeg)\n\n### Top 11 Continuous Deployment Tools for Software Development\n\nTools for DevOps automation make it easier, simpler, and faster for teams to manage these operations at scale. Using CI/CD tools, DevOps teams and engineers can offer continuous software upgrades of any size, platform, and environment. The top eleven CI/CD tools covered in this list all provide essential options. \n\n- AWS CodeDeploy\n- Buddy\n- Bamboo\n- CircleCI\n- DeployBot\n- CodeShip\n- GitLab CI/CD\n- Jenkins\n- Octopus Deploy\n- TeamCity\n- Travis CI\n\n#### AWS CodeDeploy\n\nA deployment solution called [AWS CodeDeploy](https://aws.amazon.com/codedeploy/) automates the deployment of applications computing services like Amazon EC2 instances, on-premises instances, serverless Lambda functions, or Amazon Web Services. \n\n- **Features:**\n- Can be deployed automatically\n- Provides centralized control and surveillance\n- Provides QA tracking for your deployment\n- Is compatible with different architectures and languages\n- Has an effective notification system\n- **Strengths:**\n- Offers a command-line interface (CLI) or an online administration console, which can be viewed or edited in any environment\n- Offers code as configuration features\n- Integrates with other AWS services\n- Can repeat the application deployment process to other instance groups\n- **Weakness:** \n- AWS CodeDeploy does not integrate with GitHub\n- **Pricing:** \n- This tool does not require deployment costs. However, you are required to pay $0.02 per update for on-premises instances.\n- **Suitable for:** Small or large-scale businesses\n- **Website:** [aws.amazon.com/codedeploy](https://aws.amazon.com/codedeploy/?nc=sn&loc=0)\n\n#### Buddy\n\nBuddy is an automation platform that makes DevOps easy for developers. It's a commercial CI/CD platform that enables rapid development, testing, and deployment of websites and applications. \n\n- **Features:** \n- Support for all widely used deployment protocols, IaaS/PaaS services, custom tools, and scripts\n- One-click rollbacks that restore the server to its prior condition\n- Parallel test execution and simultaneous deployment to several servers\n- Multithreaded transfers for FTP/SFTP, AWS, Google Cloud, DigitalOcean, Azure, and others\n- An exclusive list of Docker/Kubernetes activities\n- **Strengths:** \n- Ease of setup and use\n- Quality support\n- Accessible for on-premises and cloud deployment\n- Free commercial tool\n- **Weakness:** \n- A little daunting learning curve for first-time users of CI/CD tools\n- **Pricing:** \n- Buddy offers four price tiers, ranging from $0 to $200. Its plans include a free account ($0/month for five projects), Pro ($75/month for 20 projects), Hyper ($200/month unlimited), and On-premises ($35/month per user).\n- **Suitable for:** Small or large-scale businesses\n- **Website:** [https://buddy.works/](https://buddy.works/)\n\n![](/blog-images/96c57e483b0065a987d27e4e0bde7a7a.png)\n\n#### Bamboo\n\nBamboo is a CI/CD tool from Atlassian that integrates and sets up automated builds, tests, and releases in a single DevOps flow. It seamlessly integrates with multiple projects like Bitbucket and JIRA. \n\n- **Features:** \n- Flexibility with languages and technologies such as CodeDeploy, Mercurial, Docker, AWS, Git, SVN, and Amazon S3 buckets\n- Available both in hosted and on-premises versions\n- Git workflows and branching built in, so Bamboo automatically merges branches\n- Simple management for enterprise CI scaling\n- **Strengths:**\n- Has a user-friendly graphical user interface\n- Has built-in disaster recovery features that deliver high availability\n- **Weaknesses:** \n- Is high priced\n- Doesn't have a cloud version\n- **Suitable for:** Small or large-scale businesses\n- **Pricing:** \n- In Bamboo, pricing changes according to agents rather than consumers. It provides two packages for small and growing teams, from $10 to $1,100.\n- **Website:** [https://www.atlassian.com/software/bamboo](https://www.atlassian.com/software/bamboo)\n\n#### CircleCI\n\nCircleCI is a CI tool for optimizing software development procedures. It combines with other third-party tools to automate the end-to-end deployment process in several languages.  \n\n- **Features:**\n- Runs on any environment, on-premises, public and private cloud\n- Functions well with Docker and lets you configure a customized environment\n- Automates parallelization, branch-specific and continuous deployments for quick speed\n- Supports multiple languages like C++, JavaScript, .NET, PHP, Python, and Ruby\n- Splits, shares, and reuses builds across multiple containers to reduce overall build time\n- Supports all of the popular repositories, including GitHub, Bitbucket, and GitHub Enterprise\n- **Strengths:**\n- Is a compact CI/CD platform\n- Provides unrivaled security\n- **Weaknesses:** \n- It doesn't consolidate all workflows into a single platform\n- It doesn't have an intuitive dashboard and lacks a comprehensive picture of the statistics for builds across the organization\n- When the CI process is complicated, it can occasionally be exceedingly slow\n- **Suitable for:** Small or large-scale businesses\n- **Pricing:** \n- CircleCI offers four paid plans, including a free plan of $0/month, $15/month, $2,000/month for enterprise-level deployments, and custom pricing for self-hosted.\n- **Website:** [https://circleci.com/](https://circleci.com/)\n\n#### DeployBot\n\nDeployBot is a solution for cloud-based code distribution to assist businesses in creating and deploying code through a single interface**.** \n\n- **Features:** \n- Allows you to roll back a release\n- Provides real-time deployment tracking\n- Allows for easy deployment in an open interface protocol or integration\n- Can run shell scripts on your server before, after, or during deployment\n- **Strengths:** \n- Allows code deployment without requiring server connection\n- Handles permission management for clients and teams\n- Is easy to use and integrate\n- **Weakness:** \n- UI is not intuitive and often has too much information on one screen\n- **Suitable for:** Start-ups and scale-up businesses\n- **Pricing:** \n- It offers three payment tiers: the free plan ($0/month), Plus ($25/month), and Premium ($50/month).\n- **Website:** [https://deploybot.com/](https://deploybot.com/)\n\n#### CodeShip\n\nCodeShip is a cloud-based application development platform for continuous integration and delivery. It resides between your source code repository and the hosting environment and automatically tests and delivers any modification to your platform. \n\n- **Features:** \n- Includes access regulations and permits\n- Has a builds log\n- Offers automated and parallel testing\n- Has configuration management for deployment pipelines\n- Offers native Docker support\n- **Strengths:** \n- Gives users the ability to choose containers for their production environment\n- Integrates with any tool\n- Has an easy-to-use web interface\n- **Weaknesses:** \n- It lacks integrations\n- Upgrades can cause certain plugins and integrations to stop working\n- **Suitable for:** Any team or project\n- **Pricing:** \n- CodeShip offers pricing tiers: Starter ($49/month), Essential ($99/month), and Power ($399/month).\n- **Website:** [https://www.cloudbees.com/codeship/features-pricing](https://www.cloudbees.com/codeship/features-pricing)\n\n#### GitLab CI/CD\n\nGitLab is a popular DevOps/CI/CD tool that offers continuous integration, delivery, and deployment all within a single interface that integrates with the Git source control system. It enables you to implement planning, source code management, tracking, and security into your development life cycle. \n\n- **Features:** \n- Handles performance evaluation for your server and applications life cycle\n- Can be used for producing, displaying, and managing branches of code and project data\n- Stores Docker images securely in the GitLab Container Registry\n- Allows you to automatically create, test, and publish software releases with the aid of GitLab Auto DevOps\n- Offers more features, like support for Docker, parallel builds, and real-time logging\n- **Strengths:** \n- Is an open-source solution that is simple to use and scalable and will help you get results more quickly\n- Offers self-hosted capability or GitLab’s SaaS option\n- Supports multiple languages.\n- **Weaknesses:** \n- GitLab's UI and UX are quite overwhelming and complex for a first-time user\n- For larger projects, it can get pricy\n- Integration with third parties can be confusing\n- **Suitable for:** Small or large-scale businesses\n- **Pricing:** \n- GitLab offers three payment options that include free ($0), Premium ($19/month), and Ultimate ($99/month).\n- **Website:** [https://docs.gitlab.com/ee/ci/introduction/index.html#continuous-deployment](https://docs.gitlab.com/ee/ci/introduction/index.html#continuous-deployment)\n\n#### Jenkins\n\nJenkins is a popular open-source automation server that provides programmers with a reliable way to build, test, and deploy applications. \n\n- **Features:** \n- Supports multiple OSs like Mac, Windows, and others\n- Distributes workloads across multiple machines\n- Offers over 1,000 plugins\n- Enables easy installation and configuration with web interface\n- **Strengths:** \n- Has extensible plugin architecture\n- Integrates with every language tool\n- Is open source and free\n- **Weaknesses:** \n- UI is not intuitive and has a learning curve\n- Sudden failure can occur from updating processes\n- Configuration and integration process for some plugins is not properly documented\n- **Suitable for:** Small or large-scale businesses\n- **Pricing:** \n- Available to all users for free.\n- **Website:** [https://www.jenkins.io/](https://www.jenkins.io/)\n\n#### Octopus Deploy\n\nOctopus Deploy is a cloud-based and on-premises DevOps automation server that enables developers to manage, audit, and automate deployments and operational runbooks from a single platform. \n\n- **Features:** \n- Allows you to monitor releases and deployments\n- Supports custom scripts and controls critical data\n- Helps you to build and manage your CI/CD pipeline\n- Allows teams to manage runbooks from a single location while planning, monitoring, inspecting, and scheduling them\n- **Strengths:** \n- Has built-in features that offer flexibility for automating the software deployment pipeline\n- Offers multi-tenancy support\n- Aids with certificate management\n- Has support for the same deployment steps across all environments\n- **Weaknesses:** \n- Lacks relevant reports or metrics for tracking active deployments or historical data\n- Can sometimes have complicated integration\n- Requires a bit of a learning curve\n- **Suitable for:** Small or large-scale businesses\n- **Pricing:** \n- Octopus Deploy is free for up to 10 deployment targets but offers two payment options, which include Cloud ($50/month) and Server ($600/annually).\n- **Website:** [https://octopus.com/](https://octopus.com/)\n\n#### TeamCity\n\nDevOps teams use TeamCity, a continuous integration tool, to deploy apps, packages, and containers and run automated tests. \n\n- **Features:** \n- Integrates with well-known version control programs\n- Improves the quality of the code\n- Offers cloud connections with Kubernetes clusters, Microsoft Azure, or Amazon EC2\n- Allows you to set up builds using DSL\n- Has detailed VCS integration\n- Remotely executes and tests commits\n- Keeps track of builds for rollbacks\n- **Strengths:** \n- Runs on all OSs\n- Integrates with .NET technologies\n- Supports multiple clouds, multiple platforms, and multiple languages\n- **Weaknesses:** \n- Has a steep learning curve\n- Requires manual upgrading\n- **Suitable for:** Small or large-scale businesses\n- **Pricing:** \n- TeamCity is available across three pricing tiers. Small teams can use a free Professional Server License, the Build Agent License costs $299, and Enterprise Server Licenses start at $1,999.\n- **Website:** [https://www.jetbrains.com/teamcity/](https://www.jetbrains.com/teamcity/)\n\n#### Travis CI\n\nTravis CI is a cloud-hosted open-source continuous integration and deployment system. It's used to create and test projects on Bitbucket and GitHub. \n\n- **Features:** \n- Is available to both open-source and private projects\n- Includes authentication, testing management, change management\n- Issues permissions based on roles\n- Handles data synchronization, continuous deployment, and bespoke development\n- **Strengths:** \n- Is easy to use and set up\n- Is open source\n- Takes less time to configure and can be hosted without a hosting server\n- **Weaknesses:** \n- Can occasionally be slow\n- Lacks scalability\n- Has no flexibility in customization\n- Has limited integration with third-party tools\n- Lacks plugins\n- **Suitable for:** Small- to medium-scale projects\n- **Pricing:** \n- Travis CI offers two pricing tiers, Core and Enterprise, starting at $69/month. It also offers a 30-day free trial plan.\n- **Website:** [https://www.travis-ci.com/](https://www.travis-ci.com/)\n\n#### Which Tools Can Be Used for Continuous Deployment in DevOps?\n\nThe final phase in a DevOps pipeline is deployment, and to accomplish CD (continuous deployment), the whole pipeline must have been correctly automated (build, testing, and staging). Deciding a preferred CD tool for the system depends on the scale and sensitivity of the project, as well as what features you look to achieve in your DevOps pipeline. \n\nHere is a comparison of these top 11 continuous deployment tools discussed in this article. \n\n![](/blog-images/9f9483eca33a80fa5cab356867d982f7.png)\n\n### What to Consider When Choosing a CD Tool\n\nDespite the inclination, you may have to utilize specific tools because of their popularity. Before choosing a continuous deployment technology, consider the following essential points. Your continuous deployment tool should meet these needs. \n\n- It should be user-friendly with little or no substantial technical obstacles or a high learning curve due to the device.\n- The DevOps teams should have a seamless experience with the configuration.\n- Pricing needs to be fair for both small and large teams.\n- It should be compatible with your existing integrations and technologies.\n- The tool should accommodate multiple languages and work on different OSs.\n- It should offer flexibility based on the environment.\n- The tool should give you a robust software development pipeline.\n- And it should have support for multiple integrations and plugins.\n\n### Conclusion\n\nThis post has thoroughly compared and reviewed the best continuous deployment tools currently available on the market. As was said above, automating software development processes is the key to enhancing team communication and ensuring that code and features are delivered and deployed efficiently with little to no human intervention. In conclusion, you can create a reliable application with little downtime with the right continuous deployment tool.\n",
          "code": "var Component=(()=>{var h=Object.create;var o=Object.defineProperty;var p=Object.getOwnPropertyDescriptor;var u=Object.getOwnPropertyNames;var m=Object.getPrototypeOf,g=Object.prototype.hasOwnProperty;var f=(i,e)=>()=>(e||i((e={exports:{}}).exports,e),e.exports),y=(i,e)=>{for(var l in e)o(i,l,{get:e[l],enumerable:!0})},s=(i,e,l,r)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let t of u(e))!g.call(i,t)&&t!==l&&o(i,t,{get:()=>e[t],enumerable:!(r=p(e,t))||r.enumerable});return i};var b=(i,e,l)=>(l=i!=null?h(m(i)):{},s(e||!i||!i.__esModule?o(l,\"default\",{value:i,enumerable:!0}):l,i)),v=i=>s(o({},\"__esModule\",{value:!0}),i);var c=f((D,a)=>{a.exports=_jsx_runtime});var S={};y(S,{default:()=>C,frontmatter:()=>w});var n=b(c()),w={title:\"11 Continuous Deployment Tools and How to Choose One\",summary:\"Create a reliable application with little downtime with the right continuous deployment tools for the DevOps pipeline\",publishDate:\"Tue Sep 13 2022 19:27:56 GMT+0000 (Coordinated Universal Time)\",author:\"kevin-luu\",readingTime:6,categories:[\"platform-engineering\",\"product\"],mainImage:\"/blog-images/b736c54718aa57a1a9e027fd79651291.jpg\",imageAlt:\"A bunch of tools\",showCTA:!0,ctaCopy:\"Improve CI/CD workflows with Release's ephemeral environments for faster testing and deployment cycles.\",ctaLink:\"https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=11-continuous-deployment-tools-and-how-to-choose-one\",relatedPosts:[\"\"],ogImage:\"/blog-images/b736c54718aa57a1a9e027fd79651291.jpg\",excerpt:\"Create a reliable application with little downtime with the right continuous deployment tools for the DevOps pipeline\",tags:[\"platform-engineering\",\"product\"],ctaButton:\"Try Release for Free\"};function d(i){let e=Object.assign({p:\"p\",img:\"img\",h3:\"h3\",a:\"a\",span:\"span\",ul:\"ul\",li:\"li\",h4:\"h4\",strong:\"strong\"},i.components);return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(e.p,{children:\"The popularity of agile methodologies among businesses has escalated and resulted in the mass adoption of continuous software development processes. This has given rise to many tools that make implementing your DevOps pipeline and application development easier. Choosing which tool to use to produce a robust system is difficult because of the market surge in continuous deployment tools.\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:\"Continuous deployment (CD) is the procedure whereby updates to software code are pipelined, automated, tested, and then made available in the production environment. It is a crucial stage in the DevOps life cycle and assists companies in quickly and efficiently deploying changes, upgrades, and updates at the app level.\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:\"In this article, I'll outline 11 continuous deployment tools that you should consider for deployment processes in your DevOps pipeline. We\\u2019ll look at the features and highlight the advantages and disadvantages of each tool. I'll talk about how these technologies may be utilized in DevOps, and finally, we\\u2019ll look at a comparison of these tools in detail.\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/656221e0dc7a62d308d1cd6debb33380.jpeg\",alt:\"\"})}),`\n`,(0,n.jsxs)(e.h3,{id:\"top-11-continuous-deployment-tools-for-software-development\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#top-11-continuous-deployment-tools-for-software-development\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Top 11 Continuous Deployment Tools for Software Development\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Tools for DevOps automation make it easier, simpler, and faster for teams to manage these operations at scale. Using CI/CD tools, DevOps teams and engineers can offer continuous software upgrades of any size, platform, and environment. The top eleven CI/CD tools covered in this list all provide essential options.\\xA0\"}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsx)(e.li,{children:\"AWS CodeDeploy\"}),`\n`,(0,n.jsx)(e.li,{children:\"Buddy\"}),`\n`,(0,n.jsx)(e.li,{children:\"Bamboo\"}),`\n`,(0,n.jsx)(e.li,{children:\"CircleCI\"}),`\n`,(0,n.jsx)(e.li,{children:\"DeployBot\"}),`\n`,(0,n.jsx)(e.li,{children:\"CodeShip\"}),`\n`,(0,n.jsx)(e.li,{children:\"GitLab CI/CD\"}),`\n`,(0,n.jsx)(e.li,{children:\"Jenkins\"}),`\n`,(0,n.jsx)(e.li,{children:\"Octopus Deploy\"}),`\n`,(0,n.jsx)(e.li,{children:\"TeamCity\"}),`\n`,(0,n.jsx)(e.li,{children:\"Travis CI\"}),`\n`]}),`\n`,(0,n.jsxs)(e.h4,{id:\"aws-codedeploy\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#aws-codedeploy\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"AWS CodeDeploy\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"A deployment solution called \",(0,n.jsx)(e.a,{href:\"https://aws.amazon.com/codedeploy/\",children:\"AWS CodeDeploy\"}),\" automates the deployment of applications computing services like Amazon EC2 instances, on-premises instances, serverless Lambda functions, or Amazon Web Services.\\xA0\"]}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsx)(e.li,{children:(0,n.jsx)(e.strong,{children:\"Features:\"})}),`\n`,(0,n.jsx)(e.li,{children:\"Can be deployed automatically\"}),`\n`,(0,n.jsx)(e.li,{children:\"Provides centralized control and surveillance\"}),`\n`,(0,n.jsx)(e.li,{children:\"Provides QA tracking for your deployment\"}),`\n`,(0,n.jsx)(e.li,{children:\"Is compatible with different architectures and languages\"}),`\n`,(0,n.jsx)(e.li,{children:\"Has an effective notification system\"}),`\n`,(0,n.jsx)(e.li,{children:(0,n.jsx)(e.strong,{children:\"Strengths:\"})}),`\n`,(0,n.jsx)(e.li,{children:\"Offers a command-line interface (CLI) or an online administration console, which can be viewed or edited in any environment\"}),`\n`,(0,n.jsx)(e.li,{children:\"Offers code as configuration features\"}),`\n`,(0,n.jsx)(e.li,{children:\"Integrates with other AWS services\"}),`\n`,(0,n.jsx)(e.li,{children:\"Can repeat the application deployment process to other instance groups\"}),`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"Weakness:\"}),\"\\xA0\"]}),`\n`,(0,n.jsx)(e.li,{children:\"AWS CodeDeploy does not integrate with GitHub\"}),`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"Pricing:\"}),\"\\xA0\"]}),`\n`,(0,n.jsx)(e.li,{children:\"This tool does not require deployment costs. However, you are required to pay $0.02 per update for on-premises instances.\"}),`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"Suitable for:\"}),\" Small or large-scale businesses\"]}),`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"Website:\"}),\" \",(0,n.jsx)(e.a,{href:\"https://aws.amazon.com/codedeploy/?nc=sn&loc=0\",children:\"aws.amazon.com/codedeploy\"})]}),`\n`]}),`\n`,(0,n.jsxs)(e.h4,{id:\"buddy\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#buddy\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Buddy\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Buddy is an automation platform that makes DevOps easy for developers. It's a commercial CI/CD platform that enables rapid development, testing, and deployment of websites and applications.\\xA0\"}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"Features:\"}),\"\\xA0\"]}),`\n`,(0,n.jsx)(e.li,{children:\"Support for all widely used deployment protocols, IaaS/PaaS services, custom tools, and scripts\"}),`\n`,(0,n.jsx)(e.li,{children:\"One-click rollbacks that restore the server to its prior condition\"}),`\n`,(0,n.jsx)(e.li,{children:\"Parallel test execution and simultaneous deployment to several servers\"}),`\n`,(0,n.jsx)(e.li,{children:\"Multithreaded transfers for FTP/SFTP, AWS, Google Cloud, DigitalOcean, Azure, and others\"}),`\n`,(0,n.jsx)(e.li,{children:\"An exclusive list of Docker/Kubernetes activities\"}),`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"Strengths:\"}),\"\\xA0\"]}),`\n`,(0,n.jsx)(e.li,{children:\"Ease of setup and use\"}),`\n`,(0,n.jsx)(e.li,{children:\"Quality support\"}),`\n`,(0,n.jsx)(e.li,{children:\"Accessible for on-premises and cloud deployment\"}),`\n`,(0,n.jsx)(e.li,{children:\"Free commercial tool\"}),`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"Weakness:\"}),\"\\xA0\"]}),`\n`,(0,n.jsx)(e.li,{children:\"A little daunting learning curve for first-time users of CI/CD tools\"}),`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"Pricing:\"}),\"\\xA0\"]}),`\n`,(0,n.jsx)(e.li,{children:\"Buddy offers four price tiers, ranging from $0 to $200. Its plans include a free account ($0/month for five projects), Pro ($75/month for 20 projects), Hyper ($200/month unlimited), and On-premises ($35/month per user).\"}),`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"Suitable for:\"}),\" Small or large-scale businesses\"]}),`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"Website:\"}),\" \",(0,n.jsx)(e.a,{href:\"https://buddy.works/\",children:\"https://buddy.works/\"})]}),`\n`]}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/96c57e483b0065a987d27e4e0bde7a7a.png\",alt:\"\"})}),`\n`,(0,n.jsxs)(e.h4,{id:\"bamboo\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#bamboo\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Bamboo\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Bamboo is a CI/CD tool from Atlassian that integrates and sets up automated builds, tests, and releases in a single DevOps flow. It seamlessly integrates with multiple projects like Bitbucket and JIRA.\\xA0\"}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"Features:\"}),\"\\xA0\"]}),`\n`,(0,n.jsx)(e.li,{children:\"Flexibility with languages and technologies such as CodeDeploy, Mercurial, Docker, AWS, Git, SVN, and Amazon S3 buckets\"}),`\n`,(0,n.jsx)(e.li,{children:\"Available both in hosted and on-premises versions\"}),`\n`,(0,n.jsx)(e.li,{children:\"Git workflows and branching built in, so Bamboo automatically merges branches\"}),`\n`,(0,n.jsx)(e.li,{children:\"Simple management for enterprise CI scaling\"}),`\n`,(0,n.jsx)(e.li,{children:(0,n.jsx)(e.strong,{children:\"Strengths:\"})}),`\n`,(0,n.jsx)(e.li,{children:\"Has a user-friendly graphical user interface\"}),`\n`,(0,n.jsx)(e.li,{children:\"Has built-in disaster recovery features that deliver high availability\"}),`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"Weaknesses:\"}),\"\\xA0\"]}),`\n`,(0,n.jsx)(e.li,{children:\"Is high priced\"}),`\n`,(0,n.jsx)(e.li,{children:\"Doesn't have a cloud version\"}),`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"Suitable for:\"}),\" Small or large-scale businesses\"]}),`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"Pricing:\"}),\"\\xA0\"]}),`\n`,(0,n.jsx)(e.li,{children:\"In Bamboo, pricing changes according to agents rather than consumers. It provides two packages for small and growing teams, from $10 to $1,100.\"}),`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"Website:\"}),\" \",(0,n.jsx)(e.a,{href:\"https://www.atlassian.com/software/bamboo\",children:\"https://www.atlassian.com/software/bamboo\"})]}),`\n`]}),`\n`,(0,n.jsxs)(e.h4,{id:\"circleci\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#circleci\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"CircleCI\"]}),`\n`,(0,n.jsx)(e.p,{children:\"CircleCI is a CI tool for optimizing software development procedures. It combines with other third-party tools to automate the end-to-end deployment process in several languages. \\xA0\"}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsx)(e.li,{children:(0,n.jsx)(e.strong,{children:\"Features:\"})}),`\n`,(0,n.jsx)(e.li,{children:\"Runs on any environment, on-premises, public and private cloud\"}),`\n`,(0,n.jsx)(e.li,{children:\"Functions well with Docker and lets you configure a customized environment\"}),`\n`,(0,n.jsx)(e.li,{children:\"Automates parallelization, branch-specific and continuous deployments for quick speed\"}),`\n`,(0,n.jsx)(e.li,{children:\"Supports multiple languages like C++, JavaScript, .NET, PHP, Python, and Ruby\"}),`\n`,(0,n.jsx)(e.li,{children:\"Splits, shares, and reuses builds across multiple containers to reduce overall build time\"}),`\n`,(0,n.jsx)(e.li,{children:\"Supports all of the popular repositories, including GitHub, Bitbucket, and GitHub Enterprise\"}),`\n`,(0,n.jsx)(e.li,{children:(0,n.jsx)(e.strong,{children:\"Strengths:\"})}),`\n`,(0,n.jsx)(e.li,{children:\"Is a compact CI/CD platform\"}),`\n`,(0,n.jsx)(e.li,{children:\"Provides unrivaled security\"}),`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"Weaknesses:\"}),\"\\xA0\"]}),`\n`,(0,n.jsx)(e.li,{children:\"It doesn't consolidate all workflows into a single platform\"}),`\n`,(0,n.jsx)(e.li,{children:\"It doesn't have an intuitive dashboard and lacks a comprehensive picture of the statistics for builds across the organization\"}),`\n`,(0,n.jsx)(e.li,{children:\"When the CI process is complicated, it can occasionally be exceedingly slow\"}),`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"Suitable for:\"}),\" Small or large-scale businesses\"]}),`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"Pricing:\"}),\"\\xA0\"]}),`\n`,(0,n.jsx)(e.li,{children:\"CircleCI offers four paid plans, including a free plan of $0/month, $15/month, $2,000/month for enterprise-level deployments, and custom pricing for self-hosted.\"}),`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"Website:\"}),\" \",(0,n.jsx)(e.a,{href:\"https://circleci.com/\",children:\"https://circleci.com/\"})]}),`\n`]}),`\n`,(0,n.jsxs)(e.h4,{id:\"deploybot\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#deploybot\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"DeployBot\"]}),`\n`,(0,n.jsx)(e.p,{children:\"DeployBot is a solution for cloud-based code distribution to assist businesses in creating and deploying code through a single interface**.**\\xA0\"}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"Features:\"}),\"\\xA0\"]}),`\n`,(0,n.jsx)(e.li,{children:\"Allows you to roll back a release\"}),`\n`,(0,n.jsx)(e.li,{children:\"Provides real-time deployment tracking\"}),`\n`,(0,n.jsx)(e.li,{children:\"Allows for easy deployment in an open interface protocol or integration\"}),`\n`,(0,n.jsx)(e.li,{children:\"Can run shell scripts on your server before, after, or during deployment\"}),`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"Strengths:\"}),\"\\xA0\"]}),`\n`,(0,n.jsx)(e.li,{children:\"Allows code deployment without requiring server connection\"}),`\n`,(0,n.jsx)(e.li,{children:\"Handles permission management for clients and teams\"}),`\n`,(0,n.jsx)(e.li,{children:\"Is easy to use and integrate\"}),`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"Weakness:\"}),\"\\xA0\"]}),`\n`,(0,n.jsx)(e.li,{children:\"UI is not intuitive and often has too much information on one screen\"}),`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"Suitable for:\"}),\" Start-ups and scale-up businesses\"]}),`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"Pricing:\"}),\"\\xA0\"]}),`\n`,(0,n.jsx)(e.li,{children:\"It offers three payment tiers: the free plan ($0/month), Plus ($25/month), and Premium ($50/month).\"}),`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"Website:\"}),\" \",(0,n.jsx)(e.a,{href:\"https://deploybot.com/\",children:\"https://deploybot.com/\"})]}),`\n`]}),`\n`,(0,n.jsxs)(e.h4,{id:\"codeship\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#codeship\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"CodeShip\"]}),`\n`,(0,n.jsx)(e.p,{children:\"CodeShip is a cloud-based application development platform for continuous integration and delivery. It resides between your source code repository and the hosting environment and automatically tests and delivers any modification to your platform.\\xA0\"}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"Features:\"}),\"\\xA0\"]}),`\n`,(0,n.jsx)(e.li,{children:\"Includes access regulations and permits\"}),`\n`,(0,n.jsx)(e.li,{children:\"Has a builds log\"}),`\n`,(0,n.jsx)(e.li,{children:\"Offers automated and parallel testing\"}),`\n`,(0,n.jsx)(e.li,{children:\"Has configuration management for deployment pipelines\"}),`\n`,(0,n.jsx)(e.li,{children:\"Offers native Docker support\"}),`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"Strengths:\"}),\"\\xA0\"]}),`\n`,(0,n.jsx)(e.li,{children:\"Gives users the ability to choose containers for their production environment\"}),`\n`,(0,n.jsx)(e.li,{children:\"Integrates with any tool\"}),`\n`,(0,n.jsx)(e.li,{children:\"Has an easy-to-use web interface\"}),`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"Weaknesses:\"}),\"\\xA0\"]}),`\n`,(0,n.jsx)(e.li,{children:\"It lacks integrations\"}),`\n`,(0,n.jsx)(e.li,{children:\"Upgrades can cause certain plugins and integrations to stop working\"}),`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"Suitable for:\"}),\" Any team or project\"]}),`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"Pricing:\"}),\"\\xA0\"]}),`\n`,(0,n.jsx)(e.li,{children:\"CodeShip offers pricing tiers: Starter ($49/month), Essential ($99/month), and Power ($399/month).\"}),`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"Website:\"}),\" \",(0,n.jsx)(e.a,{href:\"https://www.cloudbees.com/codeship/features-pricing\",children:\"https://www.cloudbees.com/codeship/features-pricing\"})]}),`\n`]}),`\n`,(0,n.jsxs)(e.h4,{id:\"gitlab-cicd\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#gitlab-cicd\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"GitLab CI/CD\"]}),`\n`,(0,n.jsx)(e.p,{children:\"GitLab is a popular DevOps/CI/CD tool that offers continuous integration, delivery, and deployment all within a single interface that integrates with the Git source control system. It enables you to implement planning, source code management, tracking, and security into your development life cycle.\\xA0\"}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"Features:\"}),\"\\xA0\"]}),`\n`,(0,n.jsx)(e.li,{children:\"Handles performance evaluation for your server and applications life cycle\"}),`\n`,(0,n.jsx)(e.li,{children:\"Can be used for producing, displaying, and managing branches of code and project data\"}),`\n`,(0,n.jsx)(e.li,{children:\"Stores Docker images securely in the GitLab Container Registry\"}),`\n`,(0,n.jsx)(e.li,{children:\"Allows you to automatically create, test, and publish software releases with the aid of GitLab Auto DevOps\"}),`\n`,(0,n.jsx)(e.li,{children:\"Offers more features, like support for Docker, parallel builds, and real-time logging\"}),`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"Strengths:\"}),\"\\xA0\"]}),`\n`,(0,n.jsx)(e.li,{children:\"Is an open-source solution that is simple to use and scalable and will help you get results more quickly\"}),`\n`,(0,n.jsx)(e.li,{children:\"Offers self-hosted capability or GitLab\\u2019s SaaS option\"}),`\n`,(0,n.jsx)(e.li,{children:\"Supports multiple languages.\"}),`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"Weaknesses:\"}),\"\\xA0\"]}),`\n`,(0,n.jsx)(e.li,{children:\"GitLab's UI and UX are quite overwhelming and complex for a first-time user\"}),`\n`,(0,n.jsx)(e.li,{children:\"For larger projects, it can get pricy\"}),`\n`,(0,n.jsx)(e.li,{children:\"Integration with third parties can be confusing\"}),`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"Suitable for:\"}),\" Small or large-scale businesses\"]}),`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"Pricing:\"}),\"\\xA0\"]}),`\n`,(0,n.jsx)(e.li,{children:\"GitLab offers three payment options that include free ($0), Premium ($19/month), and Ultimate ($99/month).\"}),`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"Website:\"}),\" \",(0,n.jsx)(e.a,{href:\"https://docs.gitlab.com/ee/ci/introduction/index.html#continuous-deployment\",children:\"https://docs.gitlab.com/ee/ci/introduction/index.html#continuous-deployment\"})]}),`\n`]}),`\n`,(0,n.jsxs)(e.h4,{id:\"jenkins\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#jenkins\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Jenkins\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Jenkins is a popular open-source automation server that provides programmers with a reliable way to build, test, and deploy applications.\\xA0\"}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"Features:\"}),\"\\xA0\"]}),`\n`,(0,n.jsx)(e.li,{children:\"Supports multiple OSs like Mac, Windows, and others\"}),`\n`,(0,n.jsx)(e.li,{children:\"Distributes workloads across multiple machines\"}),`\n`,(0,n.jsx)(e.li,{children:\"Offers over 1,000 plugins\"}),`\n`,(0,n.jsx)(e.li,{children:\"Enables easy installation and configuration with web interface\"}),`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"Strengths:\"}),\"\\xA0\"]}),`\n`,(0,n.jsx)(e.li,{children:\"Has extensible plugin architecture\"}),`\n`,(0,n.jsx)(e.li,{children:\"Integrates with every language tool\"}),`\n`,(0,n.jsx)(e.li,{children:\"Is open source and free\"}),`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"Weaknesses:\"}),\"\\xA0\"]}),`\n`,(0,n.jsx)(e.li,{children:\"UI is not intuitive and has a learning curve\"}),`\n`,(0,n.jsx)(e.li,{children:\"Sudden failure can occur from updating processes\"}),`\n`,(0,n.jsx)(e.li,{children:\"Configuration and integration process for some plugins is not properly documented\"}),`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"Suitable for:\"}),\" Small or large-scale businesses\"]}),`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"Pricing:\"}),\"\\xA0\"]}),`\n`,(0,n.jsx)(e.li,{children:\"Available to all users for free.\"}),`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"Website:\"}),\" \",(0,n.jsx)(e.a,{href:\"https://www.jenkins.io/\",children:\"https://www.jenkins.io/\"})]}),`\n`]}),`\n`,(0,n.jsxs)(e.h4,{id:\"octopus-deploy\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#octopus-deploy\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Octopus Deploy\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Octopus Deploy is a cloud-based and on-premises DevOps automation server that enables developers to manage, audit, and automate deployments and operational runbooks from a single platform.\\xA0\"}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"Features:\"}),\"\\xA0\"]}),`\n`,(0,n.jsx)(e.li,{children:\"Allows you to monitor releases and deployments\"}),`\n`,(0,n.jsx)(e.li,{children:\"Supports custom scripts and controls critical data\"}),`\n`,(0,n.jsx)(e.li,{children:\"Helps you to build and manage your CI/CD pipeline\"}),`\n`,(0,n.jsx)(e.li,{children:\"Allows teams to manage runbooks from a single location while planning, monitoring, inspecting, and scheduling them\"}),`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"Strengths:\"}),\"\\xA0\"]}),`\n`,(0,n.jsx)(e.li,{children:\"Has built-in features that offer flexibility for automating the software deployment pipeline\"}),`\n`,(0,n.jsx)(e.li,{children:\"Offers multi-tenancy support\"}),`\n`,(0,n.jsx)(e.li,{children:\"Aids with certificate management\"}),`\n`,(0,n.jsx)(e.li,{children:\"Has support for the same deployment steps across all environments\"}),`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"Weaknesses:\"}),\"\\xA0\"]}),`\n`,(0,n.jsx)(e.li,{children:\"Lacks relevant reports or metrics for tracking active deployments or historical data\"}),`\n`,(0,n.jsx)(e.li,{children:\"Can sometimes have complicated integration\"}),`\n`,(0,n.jsx)(e.li,{children:\"Requires a bit of a learning curve\"}),`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"Suitable for:\"}),\" Small or large-scale businesses\"]}),`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"Pricing:\"}),\"\\xA0\"]}),`\n`,(0,n.jsx)(e.li,{children:\"Octopus Deploy is free for up to 10 deployment targets but offers two payment options, which include Cloud ($50/month) and Server ($600/annually).\"}),`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"Website:\"}),\" \",(0,n.jsx)(e.a,{href:\"https://octopus.com/\",children:\"https://octopus.com/\"})]}),`\n`]}),`\n`,(0,n.jsxs)(e.h4,{id:\"teamcity\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#teamcity\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"TeamCity\"]}),`\n`,(0,n.jsx)(e.p,{children:\"DevOps teams use TeamCity, a continuous integration tool, to deploy apps, packages, and containers and run automated tests.\\xA0\"}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"Features:\"}),\"\\xA0\"]}),`\n`,(0,n.jsx)(e.li,{children:\"Integrates with well-known version control programs\"}),`\n`,(0,n.jsx)(e.li,{children:\"Improves the quality of the code\"}),`\n`,(0,n.jsx)(e.li,{children:\"Offers cloud connections with Kubernetes clusters, Microsoft Azure, or Amazon EC2\"}),`\n`,(0,n.jsx)(e.li,{children:\"Allows you to set up builds using DSL\"}),`\n`,(0,n.jsx)(e.li,{children:\"Has detailed VCS integration\"}),`\n`,(0,n.jsx)(e.li,{children:\"Remotely executes and tests commits\"}),`\n`,(0,n.jsx)(e.li,{children:\"Keeps track of builds for rollbacks\"}),`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"Strengths:\"}),\"\\xA0\"]}),`\n`,(0,n.jsx)(e.li,{children:\"Runs on all OSs\"}),`\n`,(0,n.jsx)(e.li,{children:\"Integrates with .NET technologies\"}),`\n`,(0,n.jsx)(e.li,{children:\"Supports multiple clouds, multiple platforms, and multiple languages\"}),`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"Weaknesses:\"}),\"\\xA0\"]}),`\n`,(0,n.jsx)(e.li,{children:\"Has a steep learning curve\"}),`\n`,(0,n.jsx)(e.li,{children:\"Requires manual upgrading\"}),`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"Suitable for:\"}),\" Small or large-scale businesses\"]}),`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"Pricing:\"}),\"\\xA0\"]}),`\n`,(0,n.jsx)(e.li,{children:\"TeamCity is available across three pricing tiers. Small teams can use a free Professional Server License, the Build Agent License costs $299, and Enterprise Server Licenses start at $1,999.\"}),`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"Website:\"}),\" \",(0,n.jsx)(e.a,{href:\"https://www.jetbrains.com/teamcity/\",children:\"https://www.jetbrains.com/teamcity/\"})]}),`\n`]}),`\n`,(0,n.jsxs)(e.h4,{id:\"travis-ci\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#travis-ci\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Travis CI\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Travis CI is a cloud-hosted open-source continuous integration and deployment system. It's used to create and test projects on Bitbucket and GitHub.\\xA0\"}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"Features:\"}),\"\\xA0\"]}),`\n`,(0,n.jsx)(e.li,{children:\"Is available to both open-source and private projects\"}),`\n`,(0,n.jsx)(e.li,{children:\"Includes authentication, testing management, change management\"}),`\n`,(0,n.jsx)(e.li,{children:\"Issues permissions based on roles\"}),`\n`,(0,n.jsx)(e.li,{children:\"Handles data synchronization, continuous deployment, and bespoke development\"}),`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"Strengths:\"}),\"\\xA0\"]}),`\n`,(0,n.jsx)(e.li,{children:\"Is easy to use and set up\"}),`\n`,(0,n.jsx)(e.li,{children:\"Is open source\"}),`\n`,(0,n.jsx)(e.li,{children:\"Takes less time to configure and can be hosted without a hosting server\"}),`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"Weaknesses:\"}),\"\\xA0\"]}),`\n`,(0,n.jsx)(e.li,{children:\"Can occasionally be slow\"}),`\n`,(0,n.jsx)(e.li,{children:\"Lacks scalability\"}),`\n`,(0,n.jsx)(e.li,{children:\"Has no flexibility in customization\"}),`\n`,(0,n.jsx)(e.li,{children:\"Has limited integration with third-party tools\"}),`\n`,(0,n.jsx)(e.li,{children:\"Lacks plugins\"}),`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"Suitable for:\"}),\" Small- to medium-scale projects\"]}),`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"Pricing:\"}),\"\\xA0\"]}),`\n`,(0,n.jsx)(e.li,{children:\"Travis CI offers two pricing tiers, Core and Enterprise, starting at $69/month. It also offers a 30-day free trial plan.\"}),`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"Website:\"}),\" \",(0,n.jsx)(e.a,{href:\"https://www.travis-ci.com/\",children:\"https://www.travis-ci.com/\"})]}),`\n`]}),`\n`,(0,n.jsxs)(e.h4,{id:\"which-tools-can-be-used-for-continuous-deployment-in-devops\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#which-tools-can-be-used-for-continuous-deployment-in-devops\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Which Tools Can Be Used for Continuous Deployment in DevOps?\"]}),`\n`,(0,n.jsx)(e.p,{children:\"The final phase in a DevOps pipeline is deployment, and to accomplish CD (continuous deployment), the whole pipeline must have been correctly automated (build, testing, and staging). Deciding a preferred CD tool for the system depends on the scale and sensitivity of the project, as well as what features you look to achieve in your DevOps pipeline.\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:\"Here is a comparison of these top 11 continuous deployment tools discussed in this article.\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/9f9483eca33a80fa5cab356867d982f7.png\",alt:\"\"})}),`\n`,(0,n.jsxs)(e.h3,{id:\"what-to-consider-when-choosing-a-cd-tool\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#what-to-consider-when-choosing-a-cd-tool\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"What to Consider When Choosing a CD Tool\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Despite the inclination, you may have to utilize specific tools because of their popularity. Before choosing a continuous deployment technology, consider the following essential points. Your continuous deployment tool should meet these needs.\\xA0\"}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsx)(e.li,{children:\"It should be user-friendly with little or no substantial technical obstacles or a high learning curve due to the device.\"}),`\n`,(0,n.jsx)(e.li,{children:\"The DevOps teams should have a seamless experience with the configuration.\"}),`\n`,(0,n.jsx)(e.li,{children:\"Pricing needs to be fair for both small and large teams.\"}),`\n`,(0,n.jsx)(e.li,{children:\"It should be compatible with your existing integrations and technologies.\"}),`\n`,(0,n.jsx)(e.li,{children:\"The tool should accommodate multiple languages and work on different OSs.\"}),`\n`,(0,n.jsx)(e.li,{children:\"It should offer flexibility based on the environment.\"}),`\n`,(0,n.jsx)(e.li,{children:\"The tool should give you a robust software development pipeline.\"}),`\n`,(0,n.jsx)(e.li,{children:\"And it should have support for multiple integrations and plugins.\"}),`\n`]}),`\n`,(0,n.jsxs)(e.h3,{id:\"conclusion\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#conclusion\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Conclusion\"]}),`\n`,(0,n.jsx)(e.p,{children:\"This post has thoroughly compared and reviewed the best continuous deployment tools currently available on the market. As was said above, automating software development processes is the key to enhancing team communication and ensuring that code and features are delivered and deployed efficiently with little to no human intervention. In conclusion, you can create a reliable application with little downtime with the right continuous deployment tool.\"})]})}function k(i={}){let{wrapper:e}=i.components||{};return e?(0,n.jsx)(e,Object.assign({},i,{children:(0,n.jsx)(d,i)})):d(i)}var C=k;return v(S);})();\n;return Component;"
        },
        "_id": "blog/posts/11-continuous-deployment-tools-and-how-to-choose-one.mdx",
        "_raw": {
          "sourceFilePath": "blog/posts/11-continuous-deployment-tools-and-how-to-choose-one.mdx",
          "sourceFileName": "11-continuous-deployment-tools-and-how-to-choose-one.mdx",
          "sourceFileDir": "blog/posts",
          "contentType": "mdx",
          "flattenedPath": "blog/posts/11-continuous-deployment-tools-and-how-to-choose-one"
        },
        "type": "BlogPost",
        "computedSlug": "11-continuous-deployment-tools-and-how-to-choose-one"
      },
      "documentHash": "1739393595015",
      "hasWarnings": false,
      "documentTypeName": "BlogPost"
    },
    "blog/posts/12-things-you-didnt-know-you-could-do-with-release-part-1.mdx": {
      "document": {
        "title": "12 Things You Didn’t Know You Could Do With Release (Part 1)",
        "summary": "Tips and tricks to elevate your experience with Release (Part 1)",
        "publishDate": "Tue May 16 2023 22:15:42 GMT+0000 (Coordinated Universal Time)",
        "author": "regis-wilson",
        "readingTime": 8,
        "categories": [
          "kubernetes",
          "platform-engineering",
          "product"
        ],
        "mainImage": "/blog-images/9933868091bf15adcbfbdbaf9ca23492.jpg",
        "imageAlt": "multiple hot air balloons ",
        "showCTA": true,
        "ctaCopy": "Unlock seamless testing of Helm charts, Terraform, and serverless code with Release's isolated environments. Test confidently, deploy effortlessly.",
        "ctaLink": "https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=12-things-you-didnt-know-you-could-do-with-release-part-1",
        "relatedPosts": [
          ""
        ],
        "ogImage": "/blog-images/9933868091bf15adcbfbdbaf9ca23492.jpg",
        "excerpt": "Tips and tricks to elevate your experience with Release (Part 1)",
        "tags": [
          "kubernetes",
          "platform-engineering",
          "product"
        ],
        "ctaButton": "Try Release for Free",
        "body": {
          "raw": "\n‍*This is Part 1 of a 2 part series on things you didn't know about using Release. Stay tuned for* [_Part 2_](https://release.com/blog/12-things-you-didnt-know-you-could-do-with-release-part-2) _next week._\n\nYou may know that Release makes environments easy by allowing anyone to create full stack, isolated environments for development, testing, quality assurance, user acceptance testing, or even production. But did you know that there are many other things you could use Release for that you may not have thought about? This guide will help you find some really cool hidden tips and tricks for maximising your enjoyment and delight using Release!\n\n### #1: Install and Manage Your Kubernetes Helm Charts\n\nRelease natively supports Helm charts that can be deployed to your clusters. Deploying and managing Helm charts can be difficult, error prone, and tedious. For example, setting up a GitOps workflow with Argo or Flux can be quite daunting to get started. And once installed, how safe is it to test new features and changes in existing deployments?\n\nWith Release, you can create an application that contains a Helm chart and deploy it to a namespace. If you want to test a new version of your Helm chart(s), simply create a branch on your repository and create a new environment. You will now be able to test a full version of the Helm chart in complete isolation without breaking any existing installations. When you are finished testing your changes, merge the new branch into the main branch to roll out the tested changes to your existing environment(s) automatically – or on demand, however you prefer.\n\nDanger: Do not try this with [CRDs](https://hackmd.io/@carvel/rJKraqlDD).\n\n### #2: Test and Update Advanced Terraform and Serverless Code\n\nRunning [Terraform](https://www.terraform.io/), [Pulumi](https://www.pulumi.com/), or [Serverless code](https://en.wikipedia.org/wiki/Serverless_computing) is amazing but the development and testing for the cloud-native services they touch can be extremely challenging, to the point where testing in production is a real activity people resort to all the time. In some cases, you have no choice but to test in production and pray. If you are a DevOps engineer with full access to cloud resources, testing accounts, an/or infrastructure, the ability to develop and test automation scripts and code is a little easier. But what if you are a developer with little-to-no access to cloud credentials or environments to test in? And how closely similar are these environments to the real production environment?\n\nWith Release, you can create a new environment based on a template that can deploy a new copy of your infrastructure code (or your serverless function code – usually you’ll need both) to a new environment (and even a separate testing cluster and/or cloud account as needed) to be able to deploy and test changes in isolation. Each commit and push to your branch will be deployed and executed in this safe environment so you can test and verify the functionality of the infrastructure you are deploying as code. You can be sure your changes are being tested in a high-fidelity environment that closely mimics your target environment because Release manages the templating and deployments to keep you safe and secure.\n\nAll of this without requiring extra authentication, access to resources, and credentials, while still operating with the safety and guidelines set up and enforced by your policies.\n\n### #3: Save on AWS Costs Using Instant Dataset Pause Schedules\n\nWith the Release [Instant Datasets](https://docs.release.com/reference-documentation/instant-datasets-aws), you can instantly access a full version of your database snapshot in your own sandbox environment to test against. It’s a popular functionality, since it allows developers to test more realistic scenarios, that using fake data. But did you know that you can save money by pausing idle Instant Dataset instances on a schedule? This adds up to significant cost savings in database and cloud bills, depending on the dataset size and number of instances. Currently, this feature only applies to AWS, where RDS and Aurora support the concept of pausing database instances and clusters, and only charged for storage during the paused state.\n\nThe simplest and easiest way to save money with Instant Datasets is to pause the datasets during the weekend when most of our customers are not actively building, testing, and running their development cycles. However, you can also expand the schedule to include hours when your teams may not be active, for example during off hours of the week. For example, your team may only be actively developing and utilising environments during the morning and afternoon hours. You can set up a schedule to pause your Instant Datasets (and save a lot of money!) during the off hours of the week and also all day on the weekends.\n\nEven if you have teams working in multiple locations and time zones, most of our customers are able to find 8 or 12 hours per weekday when their environment datasets are not needed and can be paused, which adds up overtime.\n\n### #4: Manipulate and Test Data Safely in Isolation\n\nWith the Release [Instant Datasets](https://docs.release.com/reference-documentation/instant-datasets-aws), you can get a full version of your database snapshot from recent copies of data into your own sandbox environment. From there, you can perform any number of tasks and administrative commands on the database. For example, you could take a full production snapshot with sensitive data in it and run “sanitising” or pruning scripts to create snapshots that would be suitable for testing purposes. You could even script this to automatically create and update test database snapshots for other environments to use as an Instant Dataset.\n\nAnother use case is to perform potentially dangerous operations on your database in the safety of an isolated environment. You could perform dangerous operations like updating security patches, upgrading versions, or changing configuration values without altering the source dataset. You could also test less invasive, but potentially blocking operations, like changing various configuration settings on the database to see how performance is affected. For example, you could take an Instant Dataset and change the instance type or size and run load testing from the safety and comfort of an isolated environment to test how the application performs under the new configuration. With Release, environments aren’t just code!\n\n### #5: Move Your Applications Across Cloud Providers\n\nYou probably already understand the power and advantages of deploying code in testing environments and then promoting changes to production environments. You may already know that Release makes this fantastically easy and if you have a production environment already hosted in Release, how comfortable you are that your environments are high-fidelity versions of the actual production environment. You may also know that you can now deploy your applications and environments between clusters (for example, a testing cluster and a production cluster, or a primary region and a backup or alternate region). Promoting changes between testing, QA, staging, and production environments, or from primary to secondary is as simple as merging a pull request!\n\nBut did you also know that your clusters need not merely be separated by region or type (like preproduction and production), but also across cloud providers AWS and GCP? Because we keep our deployment features in parity (as much as humanly possible) with cloud providers, you can almost certainly move an application template between two clusters in two different cloud providers and have a nearly identical copy running in both! If you are using one of our supported databases via [Instant Datasets](https://docs.release.com/reference-documentation/instant-datasets-aws), you will be able to take snapshots from each cloud provider and check in a full set of data no matter whether you are using AWS or GCP. Combined with your code and application template, it is eminently possible to move an application across cloud providers, or even to run in a multi-cloud scenario.\n\nOf course, if you use cloud-native resources outside of Release constructs with Terraform or other IAAC, you will need to adjust your code and probably need to be extremely clever with your infrastructure. But this is easy to do on our platform and you now have as close a shot as ever in the history of cloud computing to pull this stunt off and make it a reality.\n\n### #6: Connect to Full-Stack Cloud Environments Remotely\n\nThere are entire companies whose only product is to allow you to access a cloud environment and work seamlessly with your local environment to test and develop code in the cloud as easily and quickly as you would if everything were completely local. At Release, this dream is just one side feature of our product around environments. Because your environments based on a branch, feature, or pull request are available securely in your own cloud, we can easily flip your environment into a “developer mode” where you can have complete access to containers running in the cloud environment on local ports and local filesystems.\n\nThis means it really is as easy as editing a local file, hitting refresh in your browser’s “localhost” and seeing the results live from the cloud environment. You also don’t need to lose remote access to your cloud environment because you can still share the links and environments publicly with customers and end users or privately with colleagues and coworkers while you still update and test changes, fully live, and nearly instantly without waiting for builds and deployments! You can even connect to remote resources in your environment, like databases and services that are deployed in the cloud that might be too large or too complicated to run locally.\n\nWhen you are done, simply turn off “developer mode” and your environment will go back to the latest commit on the branch or pull request you are tracking to continue where you started.\n\n### Conclusion\n\nRead the other six things you may not have known about using Release in [Part 2](https://release.com/blog/12-things-you-didnt-know-you-could-do-with-release-part-2) next week. We hope you have enjoyed these tips and hope they inspire you to try a few on your own, or contact us to get a demonstration of how these features work. If you found other creative ways to use Release or have ideas for new features, drop us a line at [hello@release.com](mailto:hello@release.com) We’d love to hear from you!\n",
          "code": "var Component=(()=>{var u=Object.create;var s=Object.defineProperty;var h=Object.getOwnPropertyDescriptor;var p=Object.getOwnPropertyNames;var m=Object.getPrototypeOf,y=Object.prototype.hasOwnProperty;var f=(n,e)=>()=>(e||n((e={exports:{}}).exports,e),e.exports),g=(n,e)=>{for(var a in e)s(n,a,{get:e[a],enumerable:!0})},i=(n,e,a,r)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let o of p(e))!y.call(n,o)&&o!==a&&s(n,o,{get:()=>e[o],enumerable:!(r=h(e,o))||r.enumerable});return n};var v=(n,e,a)=>(a=n!=null?u(m(n)):{},i(e||!n||!n.__esModule?s(a,\"default\",{value:n,enumerable:!0}):a,n)),w=n=>i(s({},\"__esModule\",{value:!0}),n);var c=f((T,l)=>{l.exports=_jsx_runtime});var R={};g(R,{default:()=>x,frontmatter:()=>b});var t=v(c()),b={title:\"12 Things You Didn\\u2019t Know You Could Do With Release (Part 1)\",summary:\"Tips and tricks to elevate your experience with Release (Part 1)\",publishDate:\"Tue May 16 2023 22:15:42 GMT+0000 (Coordinated Universal Time)\",author:\"regis-wilson\",readingTime:8,categories:[\"kubernetes\",\"platform-engineering\",\"product\"],mainImage:\"/blog-images/9933868091bf15adcbfbdbaf9ca23492.jpg\",imageAlt:\"multiple hot air balloons \",showCTA:!0,ctaCopy:\"Unlock seamless testing of Helm charts, Terraform, and serverless code with Release's isolated environments. Test confidently, deploy effortlessly.\",ctaLink:\"https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=12-things-you-didnt-know-you-could-do-with-release-part-1\",relatedPosts:[\"\"],ogImage:\"/blog-images/9933868091bf15adcbfbdbaf9ca23492.jpg\",excerpt:\"Tips and tricks to elevate your experience with Release (Part 1)\",tags:[\"kubernetes\",\"platform-engineering\",\"product\"],ctaButton:\"Try Release for Free\"};function d(n){let e=Object.assign({p:\"p\",em:\"em\",a:\"a\",h3:\"h3\",span:\"span\"},n.components);return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsxs)(e.p,{children:[\"\\u200D\",(0,t.jsx)(e.em,{children:\"This is Part 1 of a 2 part series on things you didn't know about using Release. Stay tuned for\"}),\" \",(0,t.jsx)(e.a,{href:\"https://release.com/blog/12-things-you-didnt-know-you-could-do-with-release-part-2\",children:(0,t.jsx)(e.em,{children:\"Part 2\"})}),\" \",(0,t.jsx)(e.em,{children:\"next week.\"})]}),`\n`,(0,t.jsx)(e.p,{children:\"You may know that Release makes environments easy by allowing anyone to create full stack, isolated environments for development, testing, quality assurance, user acceptance testing, or even production. But did you know that there are many other things you could use Release for that you may not have thought about? This guide will help you find some really cool hidden tips and tricks for maximising your enjoyment and delight using Release!\"}),`\n`,(0,t.jsxs)(e.h3,{id:\"1-install-and-manage-your-kubernetes-helm-charts\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#1-install-and-manage-your-kubernetes-helm-charts\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"#1: Install and Manage Your Kubernetes Helm Charts\"]}),`\n`,(0,t.jsx)(e.p,{children:\"Release natively supports Helm charts that can be deployed to your clusters. Deploying and managing Helm charts can be difficult, error prone, and tedious. For example, setting up a GitOps workflow with Argo or Flux can be quite daunting to get started. And once installed, how safe is it to test new features and changes in existing deployments?\"}),`\n`,(0,t.jsx)(e.p,{children:\"With Release, you can create an application that contains a Helm chart and deploy it to a namespace. If you want to test a new version of your Helm chart(s), simply create a branch on your repository and create a new environment. You will now be able to test a full version of the Helm chart in complete isolation without breaking any existing installations. When you are finished testing your changes, merge the new branch into the main branch to roll out the tested changes to your existing environment(s) automatically \\u2013 or on demand, however you prefer.\"}),`\n`,(0,t.jsxs)(e.p,{children:[\"Danger: Do not try this with \",(0,t.jsx)(e.a,{href:\"https://hackmd.io/@carvel/rJKraqlDD\",children:\"CRDs\"}),\".\"]}),`\n`,(0,t.jsxs)(e.h3,{id:\"2-test-and-update-advanced-terraform-and-serverless-code\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#2-test-and-update-advanced-terraform-and-serverless-code\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"#2: Test and Update Advanced Terraform and Serverless Code\"]}),`\n`,(0,t.jsxs)(e.p,{children:[\"Running \",(0,t.jsx)(e.a,{href:\"https://www.terraform.io/\",children:\"Terraform\"}),\", \",(0,t.jsx)(e.a,{href:\"https://www.pulumi.com/\",children:\"Pulumi\"}),\", or \",(0,t.jsx)(e.a,{href:\"https://en.wikipedia.org/wiki/Serverless_computing\",children:\"Serverless code\"}),\" is amazing but the development and testing for the cloud-native services they touch can be extremely challenging, to the point where testing in production is a real activity people resort to all the time. In some cases, you have no choice but to test in production and pray. If you are a DevOps engineer with full access to cloud resources, testing accounts, an/or infrastructure, the ability to develop and test automation scripts and code is a little easier. But what if you are a developer with little-to-no access to cloud credentials or environments to test in? And how closely similar are these environments to the real production environment?\"]}),`\n`,(0,t.jsx)(e.p,{children:\"With Release, you can create a new environment based on a template that can deploy a new copy of your infrastructure code (or your serverless function code \\u2013 usually you\\u2019ll need both) to a new environment (and even a separate testing cluster and/or cloud account as needed) to be able to deploy and test changes in isolation. Each commit and push to your branch will be deployed and executed in this safe environment so you can test and verify the functionality of the infrastructure you are deploying as code. You can be sure your changes are being tested in a high-fidelity environment that closely mimics your target environment because Release manages the templating and deployments to keep you safe and secure.\"}),`\n`,(0,t.jsx)(e.p,{children:\"All of this without requiring extra authentication, access to resources, and credentials, while still operating with the safety and guidelines set up and enforced by your policies.\"}),`\n`,(0,t.jsxs)(e.h3,{id:\"3-save-on-aws-costs-using-instant-dataset-pause-schedules\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#3-save-on-aws-costs-using-instant-dataset-pause-schedules\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"#3: Save on AWS Costs Using Instant Dataset Pause Schedules\"]}),`\n`,(0,t.jsxs)(e.p,{children:[\"With the Release \",(0,t.jsx)(e.a,{href:\"https://docs.release.com/reference-documentation/instant-datasets-aws\",children:\"Instant Datasets\"}),\", you can instantly access a full version of your database snapshot in your own sandbox environment to test against. It\\u2019s a popular functionality, since it allows developers to test more realistic scenarios, that using fake data. But did you know that you can save money by pausing idle Instant Dataset instances on a schedule? This adds up to significant cost savings in database and cloud bills, depending on the dataset size and number of instances. Currently, this feature only applies to AWS, where RDS and Aurora support the concept of pausing database instances and clusters, and only charged for storage during the paused state.\"]}),`\n`,(0,t.jsx)(e.p,{children:\"The simplest and easiest way to save money with Instant Datasets is to pause the datasets during the weekend when most of our customers are not actively building, testing, and running their development cycles. However, you can also expand the schedule to include hours when your teams may not be active, for example during off hours of the week. For example, your team may only be actively developing and utilising environments during the morning and afternoon hours. You can set up a schedule to pause your Instant Datasets (and save a lot of money!) during the off hours of the week and also all day on the weekends.\"}),`\n`,(0,t.jsx)(e.p,{children:\"Even if you have teams working in multiple locations and time zones, most of our customers are able to find 8 or 12 hours per weekday when their environment datasets are not needed and can be paused, which adds up overtime.\"}),`\n`,(0,t.jsxs)(e.h3,{id:\"4-manipulate-and-test-data-safely-in-isolation\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#4-manipulate-and-test-data-safely-in-isolation\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"#4: Manipulate and Test Data Safely in Isolation\"]}),`\n`,(0,t.jsxs)(e.p,{children:[\"With the Release \",(0,t.jsx)(e.a,{href:\"https://docs.release.com/reference-documentation/instant-datasets-aws\",children:\"Instant Datasets\"}),\", you can get a full version of your database snapshot from recent copies of data into your own sandbox environment. From there, you can perform any number of tasks and administrative commands on the database. For example, you could take a full production snapshot with sensitive data in it and run \\u201Csanitising\\u201D or pruning scripts to create snapshots that would be suitable for testing purposes. You could even script this to automatically create and update test database snapshots for other environments to use as an Instant Dataset.\"]}),`\n`,(0,t.jsx)(e.p,{children:\"Another use case is to perform potentially dangerous operations on your database in the safety of an isolated environment. You could perform dangerous operations like updating security patches, upgrading versions, or changing configuration values without altering the source dataset. You could also test less invasive, but potentially blocking operations, like changing various configuration settings on the database to see how performance is affected. For example, you could take an Instant Dataset and change the instance type or size and run load testing from the safety and comfort of an isolated environment to test how the application performs under the new configuration. With Release, environments aren\\u2019t just code!\"}),`\n`,(0,t.jsxs)(e.h3,{id:\"5-move-your-applications-across-cloud-providers\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#5-move-your-applications-across-cloud-providers\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"#5: Move Your Applications Across Cloud Providers\"]}),`\n`,(0,t.jsx)(e.p,{children:\"You probably already understand the power and advantages of deploying code in testing environments and then promoting changes to production environments. You may already know that Release makes this fantastically easy and if you have a production environment already hosted in Release, how comfortable you are that your environments are high-fidelity versions of the actual production environment. You may also know that you can now deploy your applications and environments between clusters (for example, a testing cluster and a production cluster, or a primary region and a backup or alternate region). Promoting changes between testing, QA, staging, and production environments, or from primary to secondary is as simple as merging a pull request!\"}),`\n`,(0,t.jsxs)(e.p,{children:[\"But did you also know that your clusters need not merely be separated by region or type (like preproduction and production), but also across cloud providers AWS and GCP? Because we keep our deployment features in parity (as much as humanly possible) with cloud providers, you can almost certainly move an application template between two clusters in two different cloud providers and have a nearly identical copy running in both! If you are using one of our supported databases via \",(0,t.jsx)(e.a,{href:\"https://docs.release.com/reference-documentation/instant-datasets-aws\",children:\"Instant Datasets\"}),\", you will be able to take snapshots from each cloud provider and check in a full set of data no matter whether you are using AWS or GCP. Combined with your code and application template, it is eminently possible to move an application across cloud providers, or even to run in a multi-cloud scenario.\"]}),`\n`,(0,t.jsx)(e.p,{children:\"Of course, if you use cloud-native resources outside of Release constructs with Terraform or other IAAC, you will need to adjust your code and probably need to be extremely clever with your infrastructure. But this is easy to do on our platform and you now have as close a shot as ever in the history of cloud computing to pull this stunt off and make it a reality.\"}),`\n`,(0,t.jsxs)(e.h3,{id:\"6-connect-to-full-stack-cloud-environments-remotely\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#6-connect-to-full-stack-cloud-environments-remotely\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"#6: Connect to Full-Stack Cloud Environments Remotely\"]}),`\n`,(0,t.jsx)(e.p,{children:\"There are entire companies whose only product is to allow you to access a cloud environment and work seamlessly with your local environment to test and develop code in the cloud as easily and quickly as you would if everything were completely local. At Release, this dream is just one side feature of our product around environments. Because your environments based on a branch, feature, or pull request are available securely in your own cloud, we can easily flip your environment into a \\u201Cdeveloper mode\\u201D where you can have complete access to containers running in the cloud environment on local ports and local filesystems.\"}),`\n`,(0,t.jsx)(e.p,{children:\"This means it really is as easy as editing a local file, hitting refresh in your browser\\u2019s \\u201Clocalhost\\u201D and seeing the results live from the cloud environment. You also don\\u2019t need to lose remote access to your cloud environment because you can still share the links and environments publicly with customers and end users or privately with colleagues and coworkers while you still update and test changes, fully live, and nearly instantly without waiting for builds and deployments! You can even connect to remote resources in your environment, like databases and services that are deployed in the cloud that might be too large or too complicated to run locally.\"}),`\n`,(0,t.jsx)(e.p,{children:\"When you are done, simply turn off \\u201Cdeveloper mode\\u201D and your environment will go back to the latest commit on the branch or pull request you are tracking to continue where you started.\"}),`\n`,(0,t.jsxs)(e.h3,{id:\"conclusion\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#conclusion\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Conclusion\"]}),`\n`,(0,t.jsxs)(e.p,{children:[\"Read the other six things you may not have known about using Release in \",(0,t.jsx)(e.a,{href:\"https://release.com/blog/12-things-you-didnt-know-you-could-do-with-release-part-2\",children:\"Part 2\"}),\" next week. We hope you have enjoyed these tips and hope they inspire you to try a few on your own, or contact us to get a demonstration of how these features work. If you found other creative ways to use Release or have ideas for new features, drop us a line at \",(0,t.jsx)(e.a,{href:\"mailto:hello@release.com\",children:\"hello@release.com\"}),\" We\\u2019d love to hear from you!\"]})]})}function k(n={}){let{wrapper:e}=n.components||{};return e?(0,t.jsx)(e,Object.assign({},n,{children:(0,t.jsx)(d,n)})):d(n)}var x=k;return w(R);})();\n;return Component;"
        },
        "_id": "blog/posts/12-things-you-didnt-know-you-could-do-with-release-part-1.mdx",
        "_raw": {
          "sourceFilePath": "blog/posts/12-things-you-didnt-know-you-could-do-with-release-part-1.mdx",
          "sourceFileName": "12-things-you-didnt-know-you-could-do-with-release-part-1.mdx",
          "sourceFileDir": "blog/posts",
          "contentType": "mdx",
          "flattenedPath": "blog/posts/12-things-you-didnt-know-you-could-do-with-release-part-1"
        },
        "type": "BlogPost",
        "computedSlug": "12-things-you-didnt-know-you-could-do-with-release-part-1"
      },
      "documentHash": "1739393595015",
      "hasWarnings": false,
      "documentTypeName": "BlogPost"
    },
    "blog/posts/12-things-you-didnt-know-you-could-do-with-release-part-2.mdx": {
      "document": {
        "title": "12 Things You Didn’t Know You Could Do With Release (Part 2)",
        "summary": "Tips and tricks to elevate your experience with Release (Part 2)",
        "publishDate": "Tue May 16 2023 22:16:49 GMT+0000 (Coordinated Universal Time)",
        "author": "regis-wilson",
        "readingTime": 8,
        "categories": [
          "platform-engineering",
          "product"
        ],
        "mainImage": "/blog-images/5b54e7eba82afae695059f62adbc1bb1.jpg",
        "imageAlt": "hot air balloons",
        "showCTA": true,
        "ctaCopy": "Unlock rapid environment provisioning for demos, hackathons, and security testing with Release's ephemeral environments.",
        "ctaLink": "https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=12-things-you-didnt-know-you-could-do-with-release-part-2",
        "relatedPosts": [
          ""
        ],
        "ogImage": "/blog-images/5b54e7eba82afae695059f62adbc1bb1.jpg",
        "excerpt": "Tips and tricks to elevate your experience with Release (Part 2)",
        "tags": [
          "platform-engineering",
          "product"
        ],
        "ctaButton": "Try Release for Free",
        "body": {
          "raw": "\n‍*This is part 2 of a 2 part series on things you didn't know about using Release. Read* [_Part 1_](https://release.com/blog/12-things-you-didnt-know-you-could-do-with-release-part-1) _to get started._\n\nYou may know that Release makes environments easy by allowing anyone to create full stack, isolated environments for development, testing, quality assurance, user acceptance testing, or even production. But did you know that there are a lot of other things you could use Release for that you may not have thought about? This guide will help you find some really cool hidden tips and tricks for maximising your enjoyment and delight using Release!\n\n### #7: Enable Your Sales Teams and Customers to Quickly Demo or Proof-of-Concept Test Your Product\n\nWith Release, it’s fantastically easy to spin up environments for any purpose. You can share your latest user acceptance tests with customers or your product team. But environments can be used by almost anyone in your company: for example, you can let your sales team spin up the latest preview version of your product and demonstrate it to a customer live: not a mock up or demonstration in a shared account, but a fully-fledged production-capable environment. This environment could be ahead of or behind the full production release cycle. The demonstration environment could even be customised for each customer and given full access to the potential or existing customer to “play with” a live demonstration for a limited period of time. Once the demonstration or proof of concept period is over, the environment can be turned down, deleted, or recycled without any intervention from DevOps or other technical teams.\n\n### #8: Host a Hackathon\n\nOne of my favourite things to participate in at work that I wouldn’t even consider work is joining a Hackathon and banging out a demonstration or proof of concept of a minimal viable product idea. Joining with my team to deliver an idea starting from nothing to tangible demonstration, spending all night, or several days and nights in a row if needed, is an amazing thing to do and I recommend this experience to every engineer, regardless of field or industry.\n\nIn terms of software development however, the truly special fruit of this endeavour is being able to see the results of your efforts running live within minutes of each change. Simply have all your teams start with a “hello world” template (even better, start off with your whole product stack as the initial “hello world” demonstration!) and turn them loose in their own pull requests and branches to write amazing code and unleash their ideas instantly.\n\nThe judges can easily see the results of the code by scanning the environments and testing the code live. May the best team win! :heart_emoji: :starry_eyes_emoji:\n\n### #9: Stay Secure and Up-to-Date With Latest Code Dependencies\n\nAt Release, we are very keen to keep our code secure and constantly up to date with all the latest updates and dependencies. Every time a Dependabot or Renovate issue is opened, we know all too well the sinking feeling of having to test and verify every update. Simply merging in all the changes can potentially break your application in the most insidiously small or sometimes the biggest, most horrendous ways possible. Testing every single change is a daunting task to say the least. The easiest path is to avoid these notifications and keep ploughing ahead. But you create a real risk in ignoring updates and patches that might make your code vulnerable to attacks or exposure of important data.\n\nWith Release, you can create a full-stack high-fidelity environment for every pull request or via labels when Dependabot or Renovate scans your repository for updates. Testing each application stack is straightforward because the links to the environment are ready and available in the comments or the Release UI. You can also run automated tests as well or in lieu of manual testing to verify functionality of your application. You can then simply approve and merge the PR to immediately promote the latest (hopefully up-to-date and secure) version out to production with a minimal amount of toil and labour for each proposed change.\n\n### #10: Control Public or Private Access to Your Services\n\nDid you know that you can hide your services behind a VPN or internal tunnelling tool to your cloud provider? For AWS customers, we support Transit Gateway out of the box to handle routing, but you can also use VPC peering or Client VPN to connect to internal services. GCP customers can also use Cloud VPN to reach internal and private services securely and safely. You can even mix public services (like a frontend and backend API) and private services (like an internal administrative interface) in one environment. You merely need to specify the `visibility` parameter for each service as shown [in our docs](https://docs.release.com/reference-documentation/application-settings/application-template/schema-definition#rules-1) and below:\n\n```yaml\n\nrules:\n  - service: admin\n    hostnames:\n      - admin-${env_id}.internal.example.com\n    path: \"/\"\n    visibility: private\n  - service: backend\n    hostnames:\n      - backend-${env_id}.${domain}\n    path: \"/auth/\"\n    visibility: public-direct\n  - service: frontend\n    hostnames:\n      - frontend-${env_id}.${domain}\n    path: \"/\"\n    visibility: public\n\n```\n\n### #11: Run Your Own Application Infrastructure and Supporting Services\n\nYou may need supporting “infrastructure” in your application stack that are actually shared services, like ElasticSearch, Kafka, RabbitMQ, Prometheus, PostHog, Solr, and on and on the list goes. You can easily create an application based on any open-source Helm chart (or your own, obviously) and deploy shared permanent environments (like QA, staging, etc.) or ephemeral environments only used for a short period of time for testing and integration. You can also use the Release [App Imports](https://docs.release.com/reference-documentation/application-settings/application-template/schema-definition#app-imports) feature to include an application into your environment stack with one additional line of configuration.\n\nSimilarly, almost any open-source application that has a `docker-compose` file (or one that you make) can be imported in literally minutes to create a full application stack that can be either used as a stand-alone shared environment or imported into your own application stack environment. In this way, virtually any application repository in GitHub, GitLab, or Bitbucket can become part of your own internal hosted application stack in minutes rather than hours or days of trying to build and install software in your application stacks. Plus, you can create as many environments as you like, both permanent and ephemeral.\n\nWe have several customers who were able to build these supporting infrastructure applications live in their own accounts faster than they could deploy their own code due to how easy off-the-shelf open-source applications can be installed via Release.\n\n### #12: Reference Secrets in Your Services, Jobs, and Build Arguments\n\nEveryone loves secrets and if you believe the band U2, a secret is something you tell someone else. Well, not on our watch at Release! We currently have support for several secrets sources: own own internal encrypted store (where all our built-in secrets and environment variables are stored) which is published to your Kubernetes cluster as a secret and encrypted there too, AWS Parameter Store (SSM), and AWS Secrets Manager (support for GCP Secrets Manager is coming soon, let us know if you are interested).\n\nIf you have an API token or an application ID you need to keep covered up but not necessarily secret, you can use our built-in secret store and reference it in either build arguments or environment variables like this:\n\n```yaml\nservice:\n   backend:\n       - key: API_TOKEN\n         value: 123456\n         secret: true\n```\n\nOnce you save the value it will be swallowed up in our vault and never see the light of day again until your application accesses it at runtime.\n\nAnother option is to pull from either SSM or Secrets Manager (AWS, today) by reference:\n\n```yaml\n\nservice:\n  backend:\n      - key: API_TOKEN\n        value: $secrets.ssm./path/to/some_token\n      - key: DATABASE_PASSWORD\n        value: $secrets.secretsmanager./prod/db/main_password\n\n```\n\nIn this way, Release gives you access to your own secrets in your own account so a secret is truly something you never have to tell someone else. You can read our top-secret [documentation about secrets here](https://docs.release.com/reference-documentation/environment-settings/environment-specific-environment-variables/secrets).\n\n### Conclusion\n\nThis completes our series on 12 things you may not have known about using Release, see [Part 1 here](https://release.com/blog/12-things-you-didnt-know-you-could-do-with-release-part-1). We hope you have enjoyed these tips and they inspire you to try a few on your own, or contact us to get a demonstration of how these tips work. If you have any ideas or ways that you have implemented something similar, drop us a line at [hello@release.com](mailto:hello@release.com), we’d love to hear from you!\n",
          "code": "var Component=(()=>{var u=Object.create;var i=Object.defineProperty;var h=Object.getOwnPropertyDescriptor;var p=Object.getOwnPropertyNames;var m=Object.getPrototypeOf,y=Object.prototype.hasOwnProperty;var f=(t,e)=>()=>(e||t((e={exports:{}}).exports,e),e.exports),v=(t,e)=>{for(var o in e)i(t,o,{get:e[o],enumerable:!0})},s=(t,e,o,r)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let a of p(e))!y.call(t,a)&&a!==o&&i(t,a,{get:()=>e[a],enumerable:!(r=h(e,a))||r.enumerable});return t};var g=(t,e,o)=>(o=t!=null?u(m(t)):{},s(e||!t||!t.__esModule?i(o,\"default\",{value:t,enumerable:!0}):o,t)),b=t=>s(i({},\"__esModule\",{value:!0}),t);var l=f((T,c)=>{c.exports=_jsx_runtime});var P={};v(P,{default:()=>S,frontmatter:()=>w});var n=g(l()),w={title:\"12 Things You Didn\\u2019t Know You Could Do With Release (Part 2)\",summary:\"Tips and tricks to elevate your experience with Release (Part 2)\",publishDate:\"Tue May 16 2023 22:16:49 GMT+0000 (Coordinated Universal Time)\",author:\"regis-wilson\",readingTime:8,categories:[\"platform-engineering\",\"product\"],mainImage:\"/blog-images/5b54e7eba82afae695059f62adbc1bb1.jpg\",imageAlt:\"hot air balloons\",showCTA:!0,ctaCopy:\"Unlock rapid environment provisioning for demos, hackathons, and security testing with Release's ephemeral environments.\",ctaLink:\"https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=12-things-you-didnt-know-you-could-do-with-release-part-2\",relatedPosts:[\"\"],ogImage:\"/blog-images/5b54e7eba82afae695059f62adbc1bb1.jpg\",excerpt:\"Tips and tricks to elevate your experience with Release (Part 2)\",tags:[\"platform-engineering\",\"product\"],ctaButton:\"Try Release for Free\"};function d(t){let e=Object.assign({p:\"p\",em:\"em\",a:\"a\",h3:\"h3\",span:\"span\",code:\"code\",pre:\"pre\"},t.components);return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsxs)(e.p,{children:[\"\\u200D\",(0,n.jsx)(e.em,{children:\"This is part 2 of a 2 part series on things you didn't know about using Release. Read\"}),\" \",(0,n.jsx)(e.a,{href:\"https://release.com/blog/12-things-you-didnt-know-you-could-do-with-release-part-1\",children:(0,n.jsx)(e.em,{children:\"Part 1\"})}),\" \",(0,n.jsx)(e.em,{children:\"to get started.\"})]}),`\n`,(0,n.jsx)(e.p,{children:\"You may know that Release makes environments easy by allowing anyone to create full stack, isolated environments for development, testing, quality assurance, user acceptance testing, or even production. But did you know that there are a lot of other things you could use Release for that you may not have thought about? This guide will help you find some really cool hidden tips and tricks for maximising your enjoyment and delight using Release!\"}),`\n`,(0,n.jsxs)(e.h3,{id:\"7-enable-your-sales-teams-and-customers-to-quickly-demo-or-proof-of-concept-test-your-product\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#7-enable-your-sales-teams-and-customers-to-quickly-demo-or-proof-of-concept-test-your-product\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"#7: Enable Your Sales Teams and Customers to Quickly Demo or Proof-of-Concept Test Your Product\"]}),`\n`,(0,n.jsx)(e.p,{children:\"With Release, it\\u2019s fantastically easy to spin up environments for any purpose. You can share your latest user acceptance tests with customers or your product team. But environments can be used by almost anyone in your company: for example, you can let your sales team spin up the latest preview version of your product and demonstrate it to a customer live: not a mock up or demonstration in a shared account, but a fully-fledged production-capable environment. This environment could be ahead of or behind the full production release cycle. The demonstration environment could even be customised for each customer and given full access to the potential or existing customer to \\u201Cplay with\\u201D a live demonstration for a limited period of time. Once the demonstration or proof of concept period is over, the environment can be turned down, deleted, or recycled without any intervention from DevOps or other technical teams.\"}),`\n`,(0,n.jsxs)(e.h3,{id:\"8-host-a-hackathon\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#8-host-a-hackathon\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"#8: Host a Hackathon\"]}),`\n`,(0,n.jsx)(e.p,{children:\"One of my favourite things to participate in at work that I wouldn\\u2019t even consider work is joining a Hackathon and banging out a demonstration or proof of concept of a minimal viable product idea. Joining with my team to deliver an idea starting from nothing to tangible demonstration, spending all night, or several days and nights in a row if needed, is an amazing thing to do and I recommend this experience to every engineer, regardless of field or industry.\"}),`\n`,(0,n.jsx)(e.p,{children:\"In terms of software development however, the truly special fruit of this endeavour is being able to see the results of your efforts running live within minutes of each change. Simply have all your teams start with a \\u201Chello world\\u201D template (even better, start off with your whole product stack as the initial \\u201Chello world\\u201D demonstration!) and turn them loose in their own pull requests and branches to write amazing code and unleash their ideas instantly.\"}),`\n`,(0,n.jsx)(e.p,{children:\"The judges can easily see the results of the code by scanning the environments and testing the code live. May the best team win! :heart_emoji: :starry_eyes_emoji:\"}),`\n`,(0,n.jsxs)(e.h3,{id:\"9-stay-secure-and-up-to-date-with-latest-code-dependencies\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#9-stay-secure-and-up-to-date-with-latest-code-dependencies\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"#9: Stay Secure and Up-to-Date With Latest Code Dependencies\"]}),`\n`,(0,n.jsx)(e.p,{children:\"At Release, we are very keen to keep our code secure and constantly up to date with all the latest updates and dependencies. Every time a Dependabot or Renovate issue is opened, we know all too well the sinking feeling of having to test and verify every update. Simply merging in all the changes can potentially break your application in the most insidiously small or sometimes the biggest, most horrendous ways possible. Testing every single change is a daunting task to say the least. The easiest path is to avoid these notifications and keep ploughing ahead. But you create a real risk in ignoring updates and patches that might make your code vulnerable to attacks or exposure of important data.\"}),`\n`,(0,n.jsx)(e.p,{children:\"With Release, you can create a full-stack high-fidelity environment for every pull request or via labels when Dependabot or Renovate scans your repository for updates. Testing each application stack is straightforward because the links to the environment are ready and available in the comments or the Release UI. You can also run automated tests as well or in lieu of manual testing to verify functionality of your application. You can then simply approve and merge the PR to immediately promote the latest (hopefully up-to-date and secure) version out to production with a minimal amount of toil and labour for each proposed change.\"}),`\n`,(0,n.jsxs)(e.h3,{id:\"10-control-public-or-private-access-to-your-services\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#10-control-public-or-private-access-to-your-services\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"#10: Control Public or Private Access to Your Services\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"Did you know that you can hide your services behind a VPN or internal tunnelling tool to your cloud provider? For AWS customers, we support Transit Gateway out of the box to handle routing, but you can also use VPC peering or Client VPN to connect to internal services. GCP customers can also use Cloud VPN to reach internal and private services securely and safely. You can even mix public services (like a frontend and backend API) and private services (like an internal administrative interface) in one environment. You merely need to specify the \",(0,n.jsx)(e.code,{children:\"visibility\"}),\" parameter for each service as shown \",(0,n.jsx)(e.a,{href:\"https://docs.release.com/reference-documentation/application-settings/application-template/schema-definition#rules-1\",children:\"in our docs\"}),\" and below:\"]}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-yaml\",children:`\nrules:\n \\xA0- service: admin\n \\xA0 \\xA0hostnames:\n \\xA0 \\xA0 \\xA0- admin-\\${env_id}.internal.example.com\n \\xA0 \\xA0path: \"/\"\n \\xA0 \\xA0visibility: private\n \\xA0- service: backend\n \\xA0 \\xA0hostnames:\n \\xA0 \\xA0 \\xA0- backend-\\${env_id}.\\${domain}\n \\xA0 \\xA0path: \"/auth/\"\n \\xA0 \\xA0visibility: public-direct\n \\xA0- service: frontend\n \\xA0 \\xA0hostnames:\n \\xA0 \\xA0 \\xA0- frontend-\\${env_id}.\\${domain}\n \\xA0 \\xA0path: \"/\"\n \\xA0 \\xA0visibility: public\n\n`})}),`\n`,(0,n.jsxs)(e.h3,{id:\"11-run-your-own-application-infrastructure-and-supporting-services\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#11-run-your-own-application-infrastructure-and-supporting-services\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"#11: Run Your Own Application Infrastructure and Supporting Services\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"You may need supporting \\u201Cinfrastructure\\u201D in your application stack that are actually shared services, like ElasticSearch, Kafka, RabbitMQ, Prometheus, PostHog, Solr, and on and on the list goes. You can easily create an application based on any open-source Helm chart (or your own, obviously) and deploy shared permanent environments (like QA, staging, etc.) or ephemeral environments only used for a short period of time for testing and integration. You can also use the Release \",(0,n.jsx)(e.a,{href:\"https://docs.release.com/reference-documentation/application-settings/application-template/schema-definition#app-imports\",children:\"App Imports\"}),\" feature to include an application into your environment stack with one additional line of configuration.\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"Similarly, almost any open-source application that has a \",(0,n.jsx)(e.code,{children:\"docker-compose\"}),\" file (or one that you make) can be imported in literally minutes to create a full application stack that can be either used as a stand-alone shared environment or imported into your own application stack environment. In this way, virtually any application repository in GitHub, GitLab, or Bitbucket can become part of your own internal hosted application stack in minutes rather than hours or days of trying to build and install software in your application stacks. Plus, you can create as many environments as you like, both permanent and ephemeral.\"]}),`\n`,(0,n.jsx)(e.p,{children:\"We have several customers who were able to build these supporting infrastructure applications live in their own accounts faster than they could deploy their own code due to how easy off-the-shelf open-source applications can be installed via Release.\"}),`\n`,(0,n.jsxs)(e.h3,{id:\"12-reference-secrets-in-your-services-jobs-and-build-arguments\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#12-reference-secrets-in-your-services-jobs-and-build-arguments\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"#12: Reference Secrets in Your Services, Jobs, and Build Arguments\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Everyone loves secrets and if you believe the band U2, a secret is something you tell someone else. Well, not on our watch at Release! We currently have support for several secrets sources: own own internal encrypted store (where all our built-in secrets and environment variables are stored) which is published to your Kubernetes cluster as a secret and encrypted there too, AWS Parameter Store (SSM), and AWS Secrets Manager (support for GCP Secrets Manager is coming soon, let us know if you are interested).\"}),`\n`,(0,n.jsx)(e.p,{children:\"If you have an API token or an application ID you need to keep covered up but not necessarily secret, you can use our built-in secret store and reference it in either build arguments or environment variables like this:\"}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-yaml\",children:`service:\n  \\xA0backend:\n  \\xA0 \\xA0 \\xA0- key: API_TOKEN\n  \\xA0 \\xA0 \\xA0 \\xA0value: 123456\n  \\xA0 \\xA0 \\xA0 \\xA0secret: true\n`})}),`\n`,(0,n.jsx)(e.p,{children:\"Once you save the value it will be swallowed up in our vault and never see the light of day again until your application accesses it at runtime.\"}),`\n`,(0,n.jsx)(e.p,{children:\"Another option is to pull from either SSM or Secrets Manager (AWS, today) by reference:\"}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-yaml\",children:`\nservice:\n \\xA0backend:\n \\xA0 \\xA0 \\xA0- key: API_TOKEN\n \\xA0 \\xA0 \\xA0 \\xA0value: $secrets.ssm./path/to/some_token\n \\xA0 \\xA0 \\xA0- key: DATABASE_PASSWORD\n \\xA0 \\xA0 \\xA0 \\xA0value: $secrets.secretsmanager./prod/db/main_password\n\n`})}),`\n`,(0,n.jsxs)(e.p,{children:[\"In this way, Release gives you access to your own secrets in your own account so a secret is truly something you never have to tell someone else. You can read our top-secret \",(0,n.jsx)(e.a,{href:\"https://docs.release.com/reference-documentation/environment-settings/environment-specific-environment-variables/secrets\",children:\"documentation about secrets here\"}),\".\"]}),`\n`,(0,n.jsxs)(e.h3,{id:\"conclusion\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#conclusion\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Conclusion\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"This completes our series on 12 things you may not have known about using Release, see \",(0,n.jsx)(e.a,{href:\"https://release.com/blog/12-things-you-didnt-know-you-could-do-with-release-part-1\",children:\"Part 1 here\"}),\". We hope you have enjoyed these tips and they inspire you to try a few on your own, or contact us to get a demonstration of how these tips work. If you have any ideas or ways that you have implemented something similar, drop us a line at \",(0,n.jsx)(e.a,{href:\"mailto:hello@release.com\",children:\"hello@release.com\"}),\", we\\u2019d love to hear from you!\"]})]})}function k(t={}){let{wrapper:e}=t.components||{};return e?(0,n.jsx)(e,Object.assign({},t,{children:(0,n.jsx)(d,t)})):d(t)}var S=k;return b(P);})();\n;return Component;"
        },
        "_id": "blog/posts/12-things-you-didnt-know-you-could-do-with-release-part-2.mdx",
        "_raw": {
          "sourceFilePath": "blog/posts/12-things-you-didnt-know-you-could-do-with-release-part-2.mdx",
          "sourceFileName": "12-things-you-didnt-know-you-could-do-with-release-part-2.mdx",
          "sourceFileDir": "blog/posts",
          "contentType": "mdx",
          "flattenedPath": "blog/posts/12-things-you-didnt-know-you-could-do-with-release-part-2"
        },
        "type": "BlogPost",
        "computedSlug": "12-things-you-didnt-know-you-could-do-with-release-part-2"
      },
      "documentHash": "1739393595015",
      "hasWarnings": false,
      "documentTypeName": "BlogPost"
    },
    "blog/posts/5-best-practices-in-a-staging-environment.mdx": {
      "document": {
        "title": "5 Best Practices in a Staging Environment",
        "summary": "In this post we will discuss best practices for staging environments and how to effectively manage staging environments.",
        "publishDate": "Tue Jan 17 2023 09:53:13 GMT+0000 (Coordinated Universal Time)",
        "author": "",
        "readingTime": 8,
        "categories": [
          "platform-engineering",
          "product"
        ],
        "mainImage": "/blog-images/e14d0f8a855275df16eb2ebe6080c16d.jpeg",
        "imageAlt": "5 Best Practices in a Staging Environment",
        "showCTA": true,
        "ctaCopy": "Automate staging environment setup like a production mirror with Release for efficient testing and seamless collaboration.",
        "ctaLink": "https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=5-best-practices-in-a-staging-environment",
        "relatedPosts": [
          ""
        ],
        "ogImage": "/blog-images/e14d0f8a855275df16eb2ebe6080c16d.jpeg",
        "excerpt": "In this post we will discuss best practices for staging environments and how to effectively manage staging environments.",
        "tags": [
          "platform-engineering",
          "product"
        ],
        "ctaButton": "Try Release for Free",
        "body": {
          "raw": "\n![](/blog-images/986d150eb23fd838555289282a42de42.jpeg)\n\n\"Staging\" is a term used to describe the process of preparing a web page or application for public viewing. This usually involves creating a copy of a live site, testing it out, and then deploying it to a production server. The goal is to ensure that the final version looks good and performs well. Staging is necessary because it allows you to test new features before they go live. If something goes wrong during the deployment process, you can fix it before it affects visitors.\n\n\"Staging environments are essential for any developer who wants to deploy their code into production.\" - Paul Irish\n\nIf you've ever worked on a project where you had to wait until the last minute to get something working, you'll appreciate the benefits of a staging environment. It helps identify bugs early and prevent downtime.\n\n## **What's a Staging Environment?**\n\nA staging environment replicates the production environment, and it's a place where new code changes are first deployed and tested before being pushed to production. This allows for a safer and more controlled testing process. Any issues that arise can be fixed before they impact your live site or web application.\n\nDevelopers can also use the staging environment to test new features or changes before they are released to the public, ensuring they're ready for production. There are several ways to implement a staging environment, such as using a local server or a cloud service.\n\n![](/blog-images/c207e26f398cd9648130dc1673d0ecae.png)\n\n## **What is the Difference Between UAT and a Staging Environment?**\n\nUser Acceptance Testing (UAT) is the process of testing a software application with real users to ensure that it meets their needs and expectations. This testing is essential to ensure that the software is fit for purpose and user-friendly before it's released to the public. It typically includes a range of tests, such as functional testing, usability testing, and performance testing.\n\nOn the other hand, a staging environment allows developers and engineers to test their new code for breaking changes. Thus, it solves a different problem than UAT does. UAT testers aim for usability, whereas developers aim for a bug-free release. These can be done in parallel or independently of each other, and the cumulative outcome of both these crucial stages impacts the overall performance of your application and users' experience of it.\n\n## **Best Practices for Staging Environments**\n\nHere are some best practices for implementing and using a staging environment.\n\n### **Staging Should be a Copy of the Production Environment**\n\nAs staging is to serve as a copy of production, keep your staging environment as similar to your live environment as possible. This will ensure that any issues arising in the staging environment represent what would happen on the site. \n\nSmall changes can sometimes have unexpected consequences and testing them in the staging environment can help you avoid them. An identical replica of production will ensure accurate error reports.\n\nFor example, suppose you're planning to add chat functionality to your product, for which availability and the ability to handle concurrent connections simultaneously are of the utmost importance. A production environment would be able to handle it, as organizations spend a lot of resources on ensuring the scalability of their production applications. \n\nIt's generally assumed that if production can handle the requirements of new functionality, staging can too. But the practical advice is to look at it the other way around: If staging can handle it, production can. This ensures that teams focus their efforts on making staging an equivalent environment to production to achieve this.\n\n### **Use Version Control for Configurations and Databases**\n\nUsing version control will make it easier to keep track of changes and roll back changes if necessary. You might consider pairing version control with automatic backups of your website's configurations and database. This way, you have a backup if something goes wrong and you can further investigate the commit history to pinpoint the issue while the production site doesn't suffer from the error.\n\nFor example, let's say a team of developers is working on a module and you're using a database to store your data. With version control, you can each make changes to the database independently and then use the version control system to merge your changes. By doing this, developers can avoid conflicts and ensure that everyone is working with the most up-to-date version of the database. Additionally, if you need to revert to a previous database version for any reason, you can easily do so using the version control system.\n\n### **Don't use the Staging Site for Production Work**\n\nThe staging site should be used solely for testing and making changes, not publishing content or conducting business. \n\nRegularly update the staging site with the latest content and data from the live site. You can automate updates with some DevOps effort. Automated processes ensure that the staging environment is synced to production, but not the other way around. This will help ensure that the staging site accurately represents the live site.\n\nFor example, let's say you're working on a web application and you've set up a staging environment to test new features and ensure everything is working correctly before deploying to production. However, instead of deploying the latest application version to the production environment, you accidentally deploy it to the staging environment. \n\nSince the staging environment is not intended for production use, the application may not be stable or reliable, so you should never direct users to your staging environment. This could lead to dissatisfaction among your users and damage to your reputation. Any changes intended for user consumption should always be made in production. So in the example above, you'd need to redeploy the application to the correct location.\n\n![](/blog-images/d428bfd633b0e24bfbda54106c31e7e9.png)\n\n### **Keep a Separate Domain for Staging**\n\nA separate domain or subdomain, instead of your production domain, is recommended for your staging environment. If things go south, those changes don't negatively impact the SEO of the domain you use for your production site. \n\nKeeping the staging domain separate from production will also prevent users from accidentally accessing the staging version of your application. In cases of prolonged testing or experimentations with SEO-related experiments, it'll also avoid being crawled and indexed by search engines. Additionally, using a separate domain for the staging environment makes it easier to manage and maintain the two environments independently.\n\nFor example, suppose you've set up a staging environment to test new features before deploying them to production and the staging environment uses the same domain as the production environment. Users may accidentally access the staging version of the site instead of the production version without even knowing, due to the same domain. This could lead to confusion and dissatisfaction among your users, as their accounts wouldn't be on the staging database. Thus, they wouldn't be able to access their work. \n\nUsing a separate subdomain for the staging environment (such as \"staging.example.com\") is a better idea. Users will know that they're accessing the staging version of the site, and they can easily identify it and switch to the production version. This can help avoid confusion and ensure that users always access the correct version of your site.\n\n### **Test, Test, Test**\n\nTest all changes thoroughly in the staging environment before deploying them to the live site. Thorough testing will help you catch any potential issues before they affect your live site, even if the changes seem minor. The scope of this testing usually tests the changes a particular release will introduce into the application. However, to utilize a staging environment effectively, you should set up automated testing with the widest possible coverage. That way, the possibility of new changes breaking existing functionality can be invalidated and ruled out.\n\nAdditionally, thorough testing on a staging environment can help improve the overall quality of the application. Furthermore, it can provide confidence that it'll function as expected in the production environment. You can even mitigate some costs and wasted time through automating your testing.\n\nFor example, let's say you're working on an e-commerce website. In the staging environment, you test the website's checkout process, payment processing, and other critical features to ensure they work correctly. You also simulate high-traffic scenarios to ensure that the website can handle many users without crashing or experiencing other problems. \n\nThis allows you to identify and fix any potential issues or bugs before they affect users in the production environment. As a result, when you deploy the website to production, it's stable and reliable, and users can use it to make purchases without encountering any problems. This can help improve the overall user experience and satisfaction with your website. And if you've automated this testing, you can often save time, money, and effort on the development side.\n\n## **Managing Environments**\n\nThere are several ways to create and manage staging environments, and the best option for you depends on the infrastructure already in place at your company. Some companies choose to set up multiple on-premises servers to get both staging and production environments. Others use virtual machines (VMs) or cloud instances. Whatever you choose, be sure to replicate all aspects of your production environment in staging. This means servers, databases, networking, storage, configurations, and so on.\n\nTo do that, you may want to consider a tool to help manage your environments. [Release](https://releasehub.com/) is one company offering [easy environment management](https://releasehub.com/whitepaper/easy-environments-management), enabling developers to test in a sandboxed environment, where they can unleash their creativity without worrying about breaking things.\n\n![](/blog-images/2ccfc6a034c07d9cf4b4b75e110526db.png)\n\n## **Summary**\n\nA staging environment replicates the production environment, but it's used to deploy and test new code changes before they're pushed to production. Staging is necessary because it allows you to test new features before they go live. This offers more control and a safe testing process, as any issues that arise can be fixed before they impact your live site or application.\n\nWe discussed best practices for staging environments, as well as how to effectively manage staging environments. These tips can help you derive the most value from your staging environment.\n\nUltimately, it's essential to test everything before deploying to production. That way, you don't waste time and money when something goes wrong.\n",
          "code": "var Component=(()=>{var d=Object.create;var a=Object.defineProperty;var g=Object.getOwnPropertyDescriptor;var u=Object.getOwnPropertyNames;var p=Object.getPrototypeOf,m=Object.prototype.hasOwnProperty;var f=(t,e)=>()=>(e||t((e={exports:{}}).exports,e),e.exports),y=(t,e)=>{for(var o in e)a(t,o,{get:e[o],enumerable:!0})},r=(t,e,o,s)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let i of u(e))!m.call(t,i)&&i!==o&&a(t,i,{get:()=>e[i],enumerable:!(s=g(e,i))||s.enumerable});return t};var v=(t,e,o)=>(o=t!=null?d(p(t)):{},r(e||!t||!t.__esModule?a(o,\"default\",{value:t,enumerable:!0}):o,t)),b=t=>r(a({},\"__esModule\",{value:!0}),t);var l=f((S,c)=>{c.exports=_jsx_runtime});var x={};y(x,{default:()=>T,frontmatter:()=>w});var n=v(l()),w={title:\"5 Best Practices in a Staging Environment\",summary:\"In this post we will discuss best practices for staging environments and how to effectively manage staging environments.\",publishDate:\"Tue Jan 17 2023 09:53:13 GMT+0000 (Coordinated Universal Time)\",author:\"\",readingTime:8,categories:[\"platform-engineering\",\"product\"],mainImage:\"/blog-images/e14d0f8a855275df16eb2ebe6080c16d.jpeg\",imageAlt:\"5 Best Practices in a Staging Environment\",showCTA:!0,ctaCopy:\"Automate staging environment setup like a production mirror with Release for efficient testing and seamless collaboration.\",ctaLink:\"https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=5-best-practices-in-a-staging-environment\",relatedPosts:[\"\"],ogImage:\"/blog-images/e14d0f8a855275df16eb2ebe6080c16d.jpeg\",excerpt:\"In this post we will discuss best practices for staging environments and how to effectively manage staging environments.\",tags:[\"platform-engineering\",\"product\"],ctaButton:\"Try Release for Free\"};function h(t){let e=Object.assign({p:\"p\",img:\"img\",h2:\"h2\",a:\"a\",span:\"span\",strong:\"strong\",h3:\"h3\"},t.components);return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/986d150eb23fd838555289282a42de42.jpeg\",alt:\"\"})}),`\n`,(0,n.jsx)(e.p,{children:'\"Staging\" is a term used to describe the process of preparing a web page or application for public viewing. This usually involves creating a copy of a live site, testing it out, and then deploying it to a production server. The goal is to ensure that the final version looks good and performs well. Staging is necessary because it allows you to test new features before they go live. If something goes wrong during the deployment process, you can fix it before it affects visitors.'}),`\n`,(0,n.jsx)(e.p,{children:'\"Staging environments are essential for any developer who wants to deploy their code into production.\" - Paul Irish'}),`\n`,(0,n.jsx)(e.p,{children:\"If you've ever worked on a project where you had to wait until the last minute to get something working, you'll appreciate the benefits of a staging environment. It helps identify bugs early and prevent downtime.\"}),`\n`,(0,n.jsxs)(e.h2,{id:\"whats-a-staging-environment\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#whats-a-staging-environment\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.strong,{children:\"What's a Staging Environment?\"})]}),`\n`,(0,n.jsx)(e.p,{children:\"A staging environment replicates the production environment, and it's a place where new code changes are first deployed and tested before being pushed to production. This allows for a safer and more controlled testing process. Any issues that arise can be fixed before they impact your live site or web application.\"}),`\n`,(0,n.jsx)(e.p,{children:\"Developers can also use the staging environment to test new features or changes before they are released to the public, ensuring they're ready for production. There are several ways to implement a staging environment, such as using a local server or a cloud service.\"}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/c207e26f398cd9648130dc1673d0ecae.png\",alt:\"\"})}),`\n`,(0,n.jsxs)(e.h2,{id:\"what-is-the-difference-between-uat-and-a-staging-environment\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#what-is-the-difference-between-uat-and-a-staging-environment\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.strong,{children:\"What is the Difference Between UAT and a Staging Environment?\"})]}),`\n`,(0,n.jsx)(e.p,{children:\"User Acceptance Testing (UAT) is the process of testing a software application with real users to ensure that it meets their needs and expectations. This testing is essential to ensure that the software is fit for purpose and user-friendly before it's released to the public. It typically includes a range of tests, such as functional testing, usability testing, and performance testing.\"}),`\n`,(0,n.jsx)(e.p,{children:\"On the other hand, a staging environment allows developers and engineers to test their new code for breaking changes. Thus, it solves a different problem than UAT does. UAT testers aim for usability, whereas developers aim for a bug-free release. These can be done in parallel or independently of each other, and the cumulative outcome of both these crucial stages impacts the overall performance of your application and users' experience of it.\"}),`\n`,(0,n.jsxs)(e.h2,{id:\"best-practices-for-staging-environments\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#best-practices-for-staging-environments\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.strong,{children:\"Best Practices for Staging Environments\"})]}),`\n`,(0,n.jsx)(e.p,{children:\"Here are some best practices for implementing and using a staging environment.\"}),`\n`,(0,n.jsxs)(e.h3,{id:\"staging-should-be-a-copy-of-the-production-environment\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#staging-should-be-a-copy-of-the-production-environment\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.strong,{children:\"Staging Should be a Copy of the Production Environment\"})]}),`\n`,(0,n.jsx)(e.p,{children:\"As staging is to serve as a copy of production, keep your staging environment as similar to your live environment as possible. This will ensure that any issues arising in the staging environment represent what would happen on the site.\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:\"Small changes can sometimes have unexpected consequences and testing them in the staging environment can help you avoid them. An identical replica of production will ensure accurate error reports.\"}),`\n`,(0,n.jsx)(e.p,{children:\"For example, suppose you're planning to add chat functionality to your product, for which availability and the ability to handle concurrent connections simultaneously are of the utmost importance. A production environment would be able to handle it, as organizations spend a lot of resources on ensuring the scalability of their production applications.\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:\"It's generally assumed that if production can handle the requirements of new functionality, staging can too. But the practical advice is to look at it the other way around: If staging can handle it, production can. This ensures that teams focus their efforts on making staging an equivalent environment to production to achieve this.\"}),`\n`,(0,n.jsxs)(e.h3,{id:\"use-version-control-for-configurations-and-databases\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#use-version-control-for-configurations-and-databases\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.strong,{children:\"Use Version Control for Configurations and Databases\"})]}),`\n`,(0,n.jsx)(e.p,{children:\"Using version control will make it easier to keep track of changes and roll back changes if necessary. You might consider pairing version control with automatic backups of your website's configurations and database. This way, you have a backup if something goes wrong and you can further investigate the commit history to pinpoint the issue while the production site doesn't suffer from the error.\"}),`\n`,(0,n.jsx)(e.p,{children:\"For example, let's say a team of developers is working on a module and you're using a database to store your data. With version control, you can each make changes to the database independently and then use the version control system to merge your changes. By doing this, developers can avoid conflicts and ensure that everyone is working with the most up-to-date version of the database. Additionally, if you need to revert to a previous database version for any reason, you can easily do so using the version control system.\"}),`\n`,(0,n.jsxs)(e.h3,{id:\"dont-use-the-staging-site-for-production-work\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#dont-use-the-staging-site-for-production-work\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.strong,{children:\"Don't use the Staging Site for Production Work\"})]}),`\n`,(0,n.jsx)(e.p,{children:\"The staging site should be used solely for testing and making changes, not publishing content or conducting business.\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:\"Regularly update the staging site with the latest content and data from the live site. You can automate updates with some DevOps effort. Automated processes ensure that the staging environment is synced to production, but not the other way around. This will help ensure that the staging site accurately represents the live site.\"}),`\n`,(0,n.jsx)(e.p,{children:\"For example, let's say you're working on a web application and you've set up a staging environment to test new features and ensure everything is working correctly before deploying to production. However, instead of deploying the latest application version to the production environment, you accidentally deploy it to the staging environment.\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:\"Since the staging environment is not intended for production use, the application may not be stable or reliable, so you should never direct users to your staging environment. This could lead to dissatisfaction among your users and damage to your reputation. Any changes intended for user consumption should always be made in production. So in the example above, you'd need to redeploy the application to the correct location.\"}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/d428bfd633b0e24bfbda54106c31e7e9.png\",alt:\"\"})}),`\n`,(0,n.jsxs)(e.h3,{id:\"keep-a-separate-domain-for-staging\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#keep-a-separate-domain-for-staging\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.strong,{children:\"Keep a Separate Domain for Staging\"})]}),`\n`,(0,n.jsx)(e.p,{children:\"A separate domain or subdomain, instead of your production domain, is recommended for your staging environment. If things go south, those changes don't negatively impact the SEO of the domain you use for your production site.\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:\"Keeping the staging domain separate from production will also prevent users from accidentally accessing the staging version of your application. In cases of prolonged testing or experimentations with SEO-related experiments, it'll also avoid being crawled and indexed by search engines. Additionally, using a separate domain for the staging environment makes it easier to manage and maintain the two environments independently.\"}),`\n`,(0,n.jsx)(e.p,{children:\"For example, suppose you've set up a staging environment to test new features before deploying them to production and the staging environment uses the same domain as the production environment. Users may accidentally access the staging version of the site instead of the production version without even knowing, due to the same domain. This could lead to confusion and dissatisfaction among your users, as their accounts wouldn't be on the staging database. Thus, they wouldn't be able to access their work.\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:`Using a separate subdomain for the staging environment (such as \"staging.example.com\") is a better idea. Users will know that they're accessing the staging version of the site, and they can easily identify it and switch to the production version. This can help avoid confusion and ensure that users always access the correct version of your site.`}),`\n`,(0,n.jsxs)(e.h3,{id:\"test-test-test\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#test-test-test\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.strong,{children:\"Test, Test, Test\"})]}),`\n`,(0,n.jsx)(e.p,{children:\"Test all changes thoroughly in the staging environment before deploying them to the live site. Thorough testing will help you catch any potential issues before they affect your live site, even if the changes seem minor. The scope of this testing usually tests the changes a particular release will introduce into the application. However, to utilize a staging environment effectively, you should set up automated testing with the widest possible coverage. That way, the possibility of new changes breaking existing functionality can be invalidated and ruled out.\"}),`\n`,(0,n.jsx)(e.p,{children:\"Additionally, thorough testing on a staging environment can help improve the overall quality of the application. Furthermore, it can provide confidence that it'll function as expected in the production environment. You can even mitigate some costs and wasted time through automating your testing.\"}),`\n`,(0,n.jsx)(e.p,{children:\"For example, let's say you're working on an e-commerce website. In the staging environment, you test the website's checkout process, payment processing, and other critical features to ensure they work correctly. You also simulate high-traffic scenarios to ensure that the website can handle many users without crashing or experiencing other problems.\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:\"This allows you to identify and fix any potential issues or bugs before they affect users in the production environment. As a result, when you deploy the website to production, it's stable and reliable, and users can use it to make purchases without encountering any problems. This can help improve the overall user experience and satisfaction with your website. And if you've automated this testing, you can often save time, money, and effort on the development side.\"}),`\n`,(0,n.jsxs)(e.h2,{id:\"managing-environments\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#managing-environments\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.strong,{children:\"Managing Environments\"})]}),`\n`,(0,n.jsx)(e.p,{children:\"There are several ways to create and manage staging environments, and the best option for you depends on the infrastructure already in place at your company. Some companies choose to set up multiple on-premises servers to get both staging and production environments. Others use virtual machines (VMs) or cloud instances. Whatever you choose, be sure to replicate all aspects of your production environment in staging. This means servers, databases, networking, storage, configurations, and so on.\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"To do that, you may want to consider a tool to help manage your environments. \",(0,n.jsx)(e.a,{href:\"https://releasehub.com/\",children:\"Release\"}),\" is one company offering \",(0,n.jsx)(e.a,{href:\"https://releasehub.com/whitepaper/easy-environments-management\",children:\"easy environment management\"}),\", enabling developers to test in a sandboxed environment, where they can unleash their creativity without worrying about breaking things.\"]}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/2ccfc6a034c07d9cf4b4b75e110526db.png\",alt:\"\"})}),`\n`,(0,n.jsxs)(e.h2,{id:\"summary\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#summary\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.strong,{children:\"Summary\"})]}),`\n`,(0,n.jsx)(e.p,{children:\"A staging environment replicates the production environment, but it's used to deploy and test new code changes before they're pushed to production. Staging is necessary because it allows you to test new features before they go live. This offers more control and a safe testing process, as any issues that arise can be fixed before they impact your live site or application.\"}),`\n`,(0,n.jsx)(e.p,{children:\"We discussed best practices for staging environments, as well as how to effectively manage staging environments. These tips can help you derive the most value from your staging environment.\"}),`\n`,(0,n.jsx)(e.p,{children:\"Ultimately, it's essential to test everything before deploying to production. That way, you don't waste time and money when something goes wrong.\"})]})}function k(t={}){let{wrapper:e}=t.components||{};return e?(0,n.jsx)(e,Object.assign({},t,{children:(0,n.jsx)(h,t)})):h(t)}var T=k;return b(x);})();\n;return Component;"
        },
        "_id": "blog/posts/5-best-practices-in-a-staging-environment.mdx",
        "_raw": {
          "sourceFilePath": "blog/posts/5-best-practices-in-a-staging-environment.mdx",
          "sourceFileName": "5-best-practices-in-a-staging-environment.mdx",
          "sourceFileDir": "blog/posts",
          "contentType": "mdx",
          "flattenedPath": "blog/posts/5-best-practices-in-a-staging-environment"
        },
        "type": "BlogPost",
        "computedSlug": "5-best-practices-in-a-staging-environment"
      },
      "documentHash": "1739393595015",
      "hasWarnings": false,
      "documentTypeName": "BlogPost"
    },
    "blog/posts/6-best-eaas-product-alternatives-to-gitpod.mdx": {
      "document": {
        "title": "6 Best EaaS Product Alternatives to Gitpod",
        "summary": "Learn about the top alternatives to Gitpod in this article and find the best tool to meet your needs and preferences.",
        "publishDate": "Mon Jan 23 2023 14:08:21 GMT+0000 (Coordinated Universal Time)",
        "author": "",
        "readingTime": 6,
        "categories": [
          "platform-engineering",
          "product"
        ],
        "mainImage": "/blog-images/164b4639a5457485fe5d5df2f93429ad.jpeg",
        "imageAlt": "6 Best EaaS Product Alternatives to Gitpod",
        "showCTA": true,
        "ctaCopy": "Simplify cloud-based IDE setup with Release's ephemeral environments for efficient collaboration and faster deployment cycles.",
        "ctaLink": "https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=6-best-eaas-product-alternatives-to-gitpod",
        "relatedPosts": [
          ""
        ],
        "ogImage": "/blog-images/164b4639a5457485fe5d5df2f93429ad.jpeg",
        "excerpt": "Learn about the top alternatives to Gitpod in this article and find the best tool to meet your needs and preferences.",
        "tags": [
          "platform-engineering",
          "product"
        ],
        "ctaButton": "Try Release for Free",
        "body": {
          "raw": "\nAre you looking for an alternative to Gitpod for your code? Many options offer similar functionality as cloud-based integrated development environments (IDEs). These alternatives can help you develop, collaborate on, and manage your projects more efficiently. By exploring some of the top alternatives to Gitpod, you can find the best tool to meet your specific needs and preferences. In this post, we'll introduce you to some of the top Gitpod alternatives and help you decide which one is right for you. \n\n## **What is EaaS?**\n\nEnvironment as a service (EaaS) is a cloud computing model that provides users access to a fully configured and managed computing environment over the internet. With EaaS, users can quickly and easily create, deploy, and manage applications and services without purchasing, configuring, and maintaining hardware and software infrastructure. \n\nEaaS providers offer a range of preconfigured environments that users can choose from, including different operating systems, programming languages, and frameworks. Users can customize their environments to meet their specific needs. \n\nEaaS provides many benefits over traditional IT infrastructure, including lower up-front costs, faster deployment times, and greater scalability and flexibility. It allows users to focus on developing and deploying their applications and services without worrying about managing and maintaining the underlying infrastructure. \n\n![](/blog-images/40b6aaae5e2083a9a91d3ebbc8d8c8e6.png)\n\n## **What Is Gitpod?**\n\n[Gitpod](https://www.gitpod.io/) is a cloud-based IDE that allows developers to work on code from any device with an internet connection. It enables collaboration on code projects with features like real-time code sharing and version control. \n\nOne of Gitpod's critical features is its seamless integration with Git, a popular version control system. This allows developers to use Gitpod to manage their code repositories and collaborate with others on code projects hosted on Git. \n\nGitpod includes several other tools and features helpful for developers. These features are code completion, debugging, testing, and deployment. Gitpod is available as a cloud service or can be installed on-premises. \n\n## **What are the Gitpod Alternatives?**\n\nMany alternatives to Gitpod offer similar functionality for cloud-based IDEs. Some of the top alternatives include the following: \n\n- Cloud9\n- Eclipse Che\n- CodeAnywhere\n- JupyterLab\n- Code Sandbox\n- Replit\n\n![ApplicationDescription automatically generated with medium confidence](/blog-images/5e37c49273081ddc5637fc9e17fbf3ba.png)\n\n## **Cloud9**\n\n[Cloud9](https://aws.amazon.com/cloud9/) is an online development environment by Amazon Web Services (AWS) that enables developers to write, run, and debug code from any web browser. Its features include collaboration, version control, and integration with popular programming languages and frameworks. \n\n### **Pros**\n\n- Cloud9 allows developers to work from anywhere with an internet connection, enabling teams to collaborate in real time.\n- Cloud9 provides a quick and easy set-up process, allowing developers to spin up a workspace in minutes without needing local installations.\n- Workspaces are protected with built-in security measures, such as identity management and access control.\n- Workspaces can be easily scaled up or down to meet the demands of a project.\n\n### **Cons**\n\n- Cloud9 doesn't offer extensive customer support, so developers may have difficulty getting help if they encounter problems.\n- Cloud9 lacks features such as built-in code completion, a feature that can make coding more efficient.\n\n### **Best Practices**\n\nLeverage the Cloud9 features. Cloud9 IDE offers a wide range of features designed to help you work more efficiently, such as quick file search, code completion, and autocompletion of code snippets. \n\nIntegrate with external services. Cloud9 IDE can be integrated with external services, such as code repositories and databases, to help streamline the development process. \n\nUse the Cloud9 Marketplace. The Cloud9 Marketplace offers a variety of tools and services that can be integrated into Cloud9 IDE, providing you with access to additional features and capabilities. \n\nStay up to date on security patches. This will help ensure that your environment remains secure. \n\n## **Eclipse Che**\n\n[Eclipse Che](https://www.eclipse.org/che/) is an open-source cloud IDE and developer workspace server that runs on Kubernetes. It offers a revolutionary developer workspace server that enables developers to create, edit, collaborate on, and debug applications in the cloud. It provides a single workspace server with a powerful IDE that can be accessed anywhere, anytime. \n\n### **Pros**\n\n- Eclipse Che enables collaboration between developers with an integrated development environment, version control system, task tracking, and more.\n- Developers can scale their applications from small teams to large enterprise projects.\n- Developers can customize the platform to their needs.\n\n### **Cons**\n\n- Eclipse Che only offers limited plugins and integrations with other tools.\n- Eclipse Che requires a complex set-up process and can be difficult for beginners.\n\n### **Best Practices**\n\nMake sure to use strong authentication and access control to prevent unauthorized access. Use the built-in version control system to keep track of changes in your code. Test your applications in a staging environment before deploying. \n\n‍\n\n![A picture containing outdoor, nature, cloudsDescription automatically generated](/blog-images/949ca2a6258cd25cc1a6510330c864e5.jpeg)\n\n## **Codeanywhere**\n\nCodeanywhere is a cloud-based IDE that allows developers to write, edit, and collaborate on code from anywhere. It has an intuitive interface that offers extensive features, including version control, debugging, and integration with popular cloud-based services. \n\n### **Pros**\n\n- Codeanywhere is easy to use with an intuitive interface.\n- It's portable and accessible from any device.\n- Supports a wide range of languages.\n- Includes debugging tools.\n- Codeanywhere supports version control.\n\n### **Cons**\n\n- There is a potential lag when working on large projects.\n- Codeanywhere's customization options are limited.\n- There is limited support for third-party integrations.\n\n### **Best Practices**\n\nUse version control to track changes and keep a backup of your code. Monitor and manage resources to optimize performance. Use linting and debuggers to identify and fix errors. \n\n## **JupyterLab**\n\n[JupyterLab](https://jupyter.org/try-jupyter/lab/) is a web-based interactive computing environment for programming, data exploration, and visualization. It provides a set of tools for data science and machine learning. \n\nA data scientist working on a machine learning project could use JupyterLab to quickly explore different algorithms, visualize data, and debug code. \n\n### **Pros**\n\n- JupyterLab gives you access to powerful tools and libraries for data analysis.\n- It supports multiple programming languages like Python, Julia, and R.\n- JupyterLab is flexible, allowing users to create and share their notebooks.\n- JupyterLab allows for big data integration and manipulation.\n- It supports interactive widgets and provides visualization capabilities.\n\n### **Cons**\n\n- JupyterLab has limited memory and CPU resources.\n- It's slower than other IDEs.\n- Error reporting with JupyterLab is poor.\n\n### **Best Practices**\n\nUse version control to keep track of changes to your code. \n\nMonitor the performance and stability of your code. \n\nSet up your environment with the necessary libraries and packages. \n\nBreak your code into smaller, more manageable chunks. \n\n## **CodeSandbox**\n\n[CodeSandbox](https://codesandbox.io/) is an online development environment that allows developers to write and test code without installing software. You can use it for server-side scripting. \n\n### **Pros**\n\n- CodeSandbox is easy to access.\n- There's no need to install any software.\n- It allows developers to collaborate and share code easily.\n- CodeSandbox can be used to test code without having to deploy it.\n\n### **Cons**\n\n- CodeSandbox has limited access to system resources.\n- There is limited support for libraries and frameworks.\n- Security is a concern if the code is shared publicly.\n\n### **Best Practices**\n\nUtilize the built-in tools and resources to maximize development efficiency. \n\n![](/blog-images/236d6cd83c825b90ef1be34aa06969a2.jpeg)\n\n## **Replit**\n\nRepl stands for Read-Eval-Print-Loop, a type of interactive programming environment. [Replit](https://replit.com/) allows developers to quickly execute code snippets and instantly view the results, allowing them to quickly iterate and improve their code. \n\n### **Pros**\n\n- Replit is a flexible environment for experimentation.\n- It's a useful tool for teaching and learning programming languages.\n\n### **Cons**\n\n- Replit is not suitable for complex projects.\n- It can be slow for large datasets.\n\n### **Best Practices**\n\nMake sure you're familiar with the language you're using in the Repl. \n\nDocument your code and keep notes on what you’ve tested and debugged. \n\nUse Repls as a teaching tool to help others learn to program. \n\n## **Is Gitpod VS Code?**\n\nGitpod is not Visual Studio Code (VS Code), though both are development environments. Gitpod is a cloud-based IDE that's integrated with GitHub, while VS Code is a desktop-based IDE. Both provide features like syntax highlighting, autocomplete, linting, and debugging. However, Gitpod also includes features like a built-in terminal, Docker support, and a built-in CI/CD pipeline. \n\n## **Is Gitpod Part of GitHub?**\n\nGitpod is not directly affiliated with GitHub. It allows developers to use GitHub repositories and collaborate with others on code projects hosted on GitHub using its cloud-based IDE. Gitpod is available as a cloud service. \n\n## **Conclusion**\n\nMany alternatives to Gitpod are available for those looking for a cloud-based IDE. These alternatives offer a range of features and tools to help developers collaborate on, develop, and manage code projects efficiently. Some popular Gitpod alternatives include Cloud9, CodeAnywhere, Eclipse Che, and CodeSandbox, each of which has its strengths and capabilities. \n\nTry out the Release [automated software development environment](https://releasehub.com/ebook/the-complete-guide-to-automated-software-environments) today to see how quickly and easily you can move your development projects forward. Automate your software development environment and eliminate manual steps to accelerate development timelines and reduce costs.\n",
          "code": "var Component=(()=>{var h=Object.create;var r=Object.defineProperty;var p=Object.getOwnPropertyDescriptor;var u=Object.getOwnPropertyNames;var m=Object.getPrototypeOf,g=Object.prototype.hasOwnProperty;var f=(i,e)=>()=>(e||i((e={exports:{}}).exports,e),e.exports),b=(i,e)=>{for(var a in e)r(i,a,{get:e[a],enumerable:!0})},s=(i,e,a,t)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let o of u(e))!g.call(i,o)&&o!==a&&r(i,o,{get:()=>e[o],enumerable:!(t=p(e,o))||t.enumerable});return i};var v=(i,e,a)=>(a=i!=null?h(m(i)):{},s(e||!i||!i.__esModule?r(a,\"default\",{value:i,enumerable:!0}):a,i)),y=i=>s(r({},\"__esModule\",{value:!0}),i);var c=f((x,l)=>{l.exports=_jsx_runtime});var C={};b(C,{default:()=>N,frontmatter:()=>w});var n=v(c()),w={title:\"6 Best EaaS Product Alternatives to Gitpod\",summary:\"Learn about the top alternatives to Gitpod in this article and find the best tool to meet your needs and preferences.\",publishDate:\"Mon Jan 23 2023 14:08:21 GMT+0000 (Coordinated Universal Time)\",author:\"\",readingTime:6,categories:[\"platform-engineering\",\"product\"],mainImage:\"/blog-images/164b4639a5457485fe5d5df2f93429ad.jpeg\",imageAlt:\"6 Best EaaS Product Alternatives to Gitpod\",showCTA:!0,ctaCopy:\"Simplify cloud-based IDE setup with Release's ephemeral environments for efficient collaboration and faster deployment cycles.\",ctaLink:\"https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=6-best-eaas-product-alternatives-to-gitpod\",relatedPosts:[\"\"],ogImage:\"/blog-images/164b4639a5457485fe5d5df2f93429ad.jpeg\",excerpt:\"Learn about the top alternatives to Gitpod in this article and find the best tool to meet your needs and preferences.\",tags:[\"platform-engineering\",\"product\"],ctaButton:\"Try Release for Free\"};function d(i){let e=Object.assign({p:\"p\",h2:\"h2\",a:\"a\",span:\"span\",strong:\"strong\",img:\"img\",ul:\"ul\",li:\"li\",h3:\"h3\"},i.components);return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(e.p,{children:\"Are you looking for an alternative to Gitpod for your code? Many options offer similar functionality as cloud-based integrated development environments (IDEs). These alternatives can help you develop, collaborate on, and manage your projects more efficiently. By exploring some of the top alternatives to Gitpod, you can find the best tool to meet your specific needs and preferences. In this post, we'll introduce you to some of the top Gitpod alternatives and help you decide which one is right for you.\\xA0\"}),`\n`,(0,n.jsxs)(e.h2,{id:\"what-is-eaas\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#what-is-eaas\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.strong,{children:\"What is EaaS?\"})]}),`\n`,(0,n.jsx)(e.p,{children:\"Environment as a service (EaaS) is a cloud computing model that provides users access to a fully configured and managed computing environment over the internet. With EaaS, users can quickly and easily create, deploy, and manage applications and services without purchasing, configuring, and maintaining hardware and software infrastructure.\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:\"EaaS providers offer a range of preconfigured environments that users can choose from, including different operating systems, programming languages, and frameworks. Users can customize their environments to meet their specific needs.\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:\"EaaS provides many benefits over traditional IT infrastructure, including lower up-front costs, faster deployment times, and greater scalability and flexibility. It allows users to focus on developing and deploying their applications and services without worrying about managing and maintaining the underlying infrastructure.\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/40b6aaae5e2083a9a91d3ebbc8d8c8e6.png\",alt:\"\"})}),`\n`,(0,n.jsxs)(e.h2,{id:\"what-is-gitpod\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#what-is-gitpod\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.strong,{children:\"What Is Gitpod?\"})]}),`\n`,(0,n.jsxs)(e.p,{children:[(0,n.jsx)(e.a,{href:\"https://www.gitpod.io/\",children:\"Gitpod\"}),\" is a cloud-based IDE that allows developers to work on code from any device with an internet connection. It enables collaboration on code projects with features like real-time code sharing and version control.\\xA0\"]}),`\n`,(0,n.jsx)(e.p,{children:\"One of Gitpod's critical features is its seamless integration with Git, a popular version control system. This allows developers to use Gitpod to manage their code repositories and collaborate with others on code projects hosted on Git.\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:\"Gitpod includes several other tools and features helpful for developers. These features are code completion, debugging, testing, and deployment. Gitpod is available as a cloud service or can be installed on-premises.\\xA0\"}),`\n`,(0,n.jsxs)(e.h2,{id:\"what-are-the-gitpod-alternatives\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#what-are-the-gitpod-alternatives\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.strong,{children:\"What are the Gitpod Alternatives?\"})]}),`\n`,(0,n.jsx)(e.p,{children:\"Many alternatives to Gitpod offer similar functionality for cloud-based IDEs. Some of the top alternatives include the following:\\xA0\"}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsx)(e.li,{children:\"Cloud9\"}),`\n`,(0,n.jsx)(e.li,{children:\"Eclipse Che\"}),`\n`,(0,n.jsx)(e.li,{children:\"CodeAnywhere\"}),`\n`,(0,n.jsx)(e.li,{children:\"JupyterLab\"}),`\n`,(0,n.jsx)(e.li,{children:\"Code Sandbox\"}),`\n`,(0,n.jsx)(e.li,{children:\"Replit\"}),`\n`]}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/5e37c49273081ddc5637fc9e17fbf3ba.png\",alt:\"ApplicationDescription automatically generated with medium confidence\"})}),`\n`,(0,n.jsxs)(e.h2,{id:\"cloud9\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#cloud9\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.strong,{children:\"Cloud9\"})]}),`\n`,(0,n.jsxs)(e.p,{children:[(0,n.jsx)(e.a,{href:\"https://aws.amazon.com/cloud9/\",children:\"Cloud9\"}),\" is an online development environment by Amazon Web Services (AWS) that enables developers to write, run, and debug code from any web browser. Its features include collaboration, version control, and integration with popular programming languages and frameworks.\\xA0\"]}),`\n`,(0,n.jsxs)(e.h3,{id:\"pros\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#pros\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.strong,{children:\"Pros\"})]}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsx)(e.li,{children:\"Cloud9 allows developers to work from anywhere with an internet connection, enabling teams to collaborate in real time.\"}),`\n`,(0,n.jsx)(e.li,{children:\"Cloud9 provides a quick and easy set-up process, allowing developers to spin up a workspace in minutes without needing local installations.\"}),`\n`,(0,n.jsx)(e.li,{children:\"Workspaces are protected with built-in security measures, such as identity management and access control.\"}),`\n`,(0,n.jsx)(e.li,{children:\"Workspaces can be easily scaled up or down to meet the demands of a project.\"}),`\n`]}),`\n`,(0,n.jsxs)(e.h3,{id:\"cons\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#cons\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.strong,{children:\"Cons\"})]}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsx)(e.li,{children:\"Cloud9 doesn't offer extensive customer support, so developers may have difficulty getting help if they encounter problems.\"}),`\n`,(0,n.jsx)(e.li,{children:\"Cloud9 lacks features such as built-in code completion, a feature that can make coding more efficient.\"}),`\n`]}),`\n`,(0,n.jsxs)(e.h3,{id:\"best-practices\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#best-practices\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.strong,{children:\"Best Practices\"})]}),`\n`,(0,n.jsx)(e.p,{children:\"Leverage the Cloud9 features. Cloud9 IDE offers a wide range of features designed to help you work more efficiently, such as quick file search, code completion, and autocompletion of code snippets.\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:\"Integrate with external services. Cloud9 IDE can be integrated with external services, such as code repositories and databases, to help streamline the development process.\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:\"Use the Cloud9 Marketplace. The Cloud9 Marketplace offers a variety of tools and services that can be integrated into Cloud9 IDE, providing you with access to additional features and capabilities.\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:\"Stay up to date on security patches. This will help ensure that your environment remains secure.\\xA0\"}),`\n`,(0,n.jsxs)(e.h2,{id:\"eclipse-che\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#eclipse-che\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.strong,{children:\"Eclipse Che\"})]}),`\n`,(0,n.jsxs)(e.p,{children:[(0,n.jsx)(e.a,{href:\"https://www.eclipse.org/che/\",children:\"Eclipse Che\"}),\" is an open-source cloud IDE and developer workspace server that runs on Kubernetes. It offers a revolutionary developer workspace server that enables developers to create, edit, collaborate on, and debug applications in the cloud. It provides a single workspace server with a powerful IDE that can be accessed anywhere, anytime.\\xA0\"]}),`\n`,(0,n.jsxs)(e.h3,{id:\"pros-1\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#pros-1\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.strong,{children:\"Pros\"})]}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsx)(e.li,{children:\"Eclipse Che enables collaboration between developers with an integrated development environment, version control system, task tracking, and more.\"}),`\n`,(0,n.jsx)(e.li,{children:\"Developers can scale their applications from small teams to large enterprise projects.\"}),`\n`,(0,n.jsx)(e.li,{children:\"Developers can customize the platform to their needs.\"}),`\n`]}),`\n`,(0,n.jsxs)(e.h3,{id:\"cons-1\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#cons-1\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.strong,{children:\"Cons\"})]}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsx)(e.li,{children:\"Eclipse Che only offers limited plugins and integrations with other tools.\"}),`\n`,(0,n.jsx)(e.li,{children:\"Eclipse Che requires a complex set-up process and can be difficult for beginners.\"}),`\n`]}),`\n`,(0,n.jsxs)(e.h3,{id:\"best-practices-1\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#best-practices-1\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.strong,{children:\"Best Practices\"})]}),`\n`,(0,n.jsx)(e.p,{children:\"Make sure to use strong authentication and access control to prevent unauthorized access. Use the built-in version control system to keep track of changes in your code. Test your applications in a staging environment before deploying.\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:\"\\u200D\"}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/949ca2a6258cd25cc1a6510330c864e5.jpeg\",alt:\"A picture containing outdoor, nature, cloudsDescription automatically generated\"})}),`\n`,(0,n.jsxs)(e.h2,{id:\"codeanywhere\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#codeanywhere\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.strong,{children:\"Codeanywhere\"})]}),`\n`,(0,n.jsx)(e.p,{children:\"Codeanywhere is a cloud-based IDE that allows developers to write, edit, and collaborate on code from anywhere. It has an intuitive interface that offers extensive features, including version control, debugging, and integration with popular cloud-based services.\\xA0\"}),`\n`,(0,n.jsxs)(e.h3,{id:\"pros-2\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#pros-2\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.strong,{children:\"Pros\"})]}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsx)(e.li,{children:\"Codeanywhere is easy to use with an intuitive interface.\"}),`\n`,(0,n.jsx)(e.li,{children:\"It's portable and accessible from any device.\"}),`\n`,(0,n.jsx)(e.li,{children:\"Supports a wide range of languages.\"}),`\n`,(0,n.jsx)(e.li,{children:\"Includes debugging tools.\"}),`\n`,(0,n.jsx)(e.li,{children:\"Codeanywhere supports version control.\"}),`\n`]}),`\n`,(0,n.jsxs)(e.h3,{id:\"cons-2\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#cons-2\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.strong,{children:\"Cons\"})]}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsx)(e.li,{children:\"There is a potential lag when working on large projects.\"}),`\n`,(0,n.jsx)(e.li,{children:\"Codeanywhere's customization options are limited.\"}),`\n`,(0,n.jsx)(e.li,{children:\"There is limited support for third-party integrations.\"}),`\n`]}),`\n`,(0,n.jsxs)(e.h3,{id:\"best-practices-2\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#best-practices-2\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.strong,{children:\"Best Practices\"})]}),`\n`,(0,n.jsx)(e.p,{children:\"Use version control to track changes and keep a backup of your code. Monitor and manage resources to optimize performance. Use linting and debuggers to identify and fix errors.\\xA0\"}),`\n`,(0,n.jsxs)(e.h2,{id:\"jupyterlab\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#jupyterlab\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.strong,{children:\"JupyterLab\"})]}),`\n`,(0,n.jsxs)(e.p,{children:[(0,n.jsx)(e.a,{href:\"https://jupyter.org/try-jupyter/lab/\",children:\"JupyterLab\"}),\" is a web-based interactive computing environment for programming, data exploration, and visualization. It provides a set of tools for data science and machine learning.\\xA0\"]}),`\n`,(0,n.jsx)(e.p,{children:\"A data scientist working on a machine learning project could use JupyterLab to quickly explore different algorithms, visualize data, and debug code.\\xA0\"}),`\n`,(0,n.jsxs)(e.h3,{id:\"pros-3\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#pros-3\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.strong,{children:\"Pros\"})]}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsx)(e.li,{children:\"JupyterLab gives you access to powerful tools and libraries for data analysis.\"}),`\n`,(0,n.jsx)(e.li,{children:\"It supports multiple programming languages like Python, Julia, and R.\"}),`\n`,(0,n.jsx)(e.li,{children:\"JupyterLab is flexible, allowing users to create and share their notebooks.\"}),`\n`,(0,n.jsx)(e.li,{children:\"JupyterLab allows for big data integration and manipulation.\"}),`\n`,(0,n.jsx)(e.li,{children:\"It supports interactive widgets and provides visualization capabilities.\"}),`\n`]}),`\n`,(0,n.jsxs)(e.h3,{id:\"cons-3\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#cons-3\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.strong,{children:\"Cons\"})]}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsx)(e.li,{children:\"JupyterLab has limited memory and CPU resources.\"}),`\n`,(0,n.jsx)(e.li,{children:\"It's slower than other IDEs.\"}),`\n`,(0,n.jsx)(e.li,{children:\"Error reporting with JupyterLab is poor.\"}),`\n`]}),`\n`,(0,n.jsxs)(e.h3,{id:\"best-practices-3\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#best-practices-3\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.strong,{children:\"Best Practices\"})]}),`\n`,(0,n.jsx)(e.p,{children:\"Use version control to keep track of changes to your code.\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:\"Monitor the performance and stability of your code.\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:\"Set up your environment with the necessary libraries and packages.\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:\"Break your code into smaller, more manageable chunks.\\xA0\"}),`\n`,(0,n.jsxs)(e.h2,{id:\"codesandbox\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#codesandbox\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.strong,{children:\"CodeSandbox\"})]}),`\n`,(0,n.jsxs)(e.p,{children:[(0,n.jsx)(e.a,{href:\"https://codesandbox.io/\",children:\"CodeSandbox\"}),\" is an online development environment that allows developers to write and test code without installing software. You can use it for server-side scripting.\\xA0\"]}),`\n`,(0,n.jsxs)(e.h3,{id:\"pros-4\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#pros-4\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.strong,{children:\"Pros\"})]}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsx)(e.li,{children:\"CodeSandbox is easy to access.\"}),`\n`,(0,n.jsx)(e.li,{children:\"There's no need to install any software.\"}),`\n`,(0,n.jsx)(e.li,{children:\"It allows developers to collaborate and share code easily.\"}),`\n`,(0,n.jsx)(e.li,{children:\"CodeSandbox can be used to test code without having to deploy it.\"}),`\n`]}),`\n`,(0,n.jsxs)(e.h3,{id:\"cons-4\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#cons-4\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.strong,{children:\"Cons\"})]}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsx)(e.li,{children:\"CodeSandbox has limited access to system resources.\"}),`\n`,(0,n.jsx)(e.li,{children:\"There is limited support for libraries and frameworks.\"}),`\n`,(0,n.jsx)(e.li,{children:\"Security is a concern if the code is shared publicly.\"}),`\n`]}),`\n`,(0,n.jsxs)(e.h3,{id:\"best-practices-4\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#best-practices-4\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.strong,{children:\"Best Practices\"})]}),`\n`,(0,n.jsx)(e.p,{children:\"Utilize the built-in tools and resources to maximize development efficiency.\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/236d6cd83c825b90ef1be34aa06969a2.jpeg\",alt:\"\"})}),`\n`,(0,n.jsxs)(e.h2,{id:\"replit\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#replit\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.strong,{children:\"Replit\"})]}),`\n`,(0,n.jsxs)(e.p,{children:[\"Repl stands for Read-Eval-Print-Loop, a type of interactive programming environment. \",(0,n.jsx)(e.a,{href:\"https://replit.com/\",children:\"Replit\"}),\" allows developers to quickly execute code snippets and instantly view the results, allowing them to quickly iterate and improve their code.\\xA0\"]}),`\n`,(0,n.jsxs)(e.h3,{id:\"pros-5\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#pros-5\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.strong,{children:\"Pros\"})]}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsx)(e.li,{children:\"Replit is a flexible environment for experimentation.\"}),`\n`,(0,n.jsx)(e.li,{children:\"It's a useful tool for teaching and learning programming languages.\"}),`\n`]}),`\n`,(0,n.jsxs)(e.h3,{id:\"cons-5\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#cons-5\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.strong,{children:\"Cons\"})]}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsx)(e.li,{children:\"Replit is not suitable for complex projects.\"}),`\n`,(0,n.jsx)(e.li,{children:\"It can be slow for large datasets.\"}),`\n`]}),`\n`,(0,n.jsxs)(e.h3,{id:\"best-practices-5\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#best-practices-5\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.strong,{children:\"Best Practices\"})]}),`\n`,(0,n.jsx)(e.p,{children:\"Make sure you're familiar with the language you're using in the Repl.\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:\"Document your code and keep notes on what you\\u2019ve tested and debugged.\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:\"Use Repls as a teaching tool to help others learn to program.\\xA0\"}),`\n`,(0,n.jsxs)(e.h2,{id:\"is-gitpod-vs-code\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#is-gitpod-vs-code\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.strong,{children:\"Is Gitpod VS Code?\"})]}),`\n`,(0,n.jsx)(e.p,{children:\"Gitpod is not Visual Studio Code (VS Code), though both are development environments. Gitpod is a cloud-based IDE that's integrated with GitHub, while VS Code is a desktop-based IDE. Both provide features like syntax highlighting, autocomplete, linting, and debugging. However, Gitpod also includes features like a built-in terminal, Docker support, and a built-in CI/CD pipeline.\\xA0\"}),`\n`,(0,n.jsxs)(e.h2,{id:\"is-gitpod-part-of-github\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#is-gitpod-part-of-github\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.strong,{children:\"Is Gitpod Part of GitHub?\"})]}),`\n`,(0,n.jsx)(e.p,{children:\"Gitpod is not directly affiliated with GitHub. It allows developers to use GitHub repositories and collaborate with others on code projects hosted on GitHub using its cloud-based IDE. Gitpod is available as a cloud service.\\xA0\"}),`\n`,(0,n.jsxs)(e.h2,{id:\"conclusion\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#conclusion\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.strong,{children:\"Conclusion\"})]}),`\n`,(0,n.jsx)(e.p,{children:\"Many alternatives to Gitpod are available for those looking for a cloud-based IDE. These alternatives offer a range of features and tools to help developers collaborate on, develop, and manage code projects efficiently. Some popular Gitpod alternatives include Cloud9, CodeAnywhere, Eclipse Che, and CodeSandbox, each of which has its strengths and capabilities.\\xA0\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"Try out the Release \",(0,n.jsx)(e.a,{href:\"https://releasehub.com/ebook/the-complete-guide-to-automated-software-environments\",children:\"automated software development environment\"}),\" today to see how quickly and easily you can move your development projects forward. Automate your software development environment and eliminate manual steps to accelerate development timelines and reduce costs.\"]})]})}function k(i={}){let{wrapper:e}=i.components||{};return e?(0,n.jsx)(e,Object.assign({},i,{children:(0,n.jsx)(d,i)})):d(i)}var N=k;return y(C);})();\n;return Component;"
        },
        "_id": "blog/posts/6-best-eaas-product-alternatives-to-gitpod.mdx",
        "_raw": {
          "sourceFilePath": "blog/posts/6-best-eaas-product-alternatives-to-gitpod.mdx",
          "sourceFileName": "6-best-eaas-product-alternatives-to-gitpod.mdx",
          "sourceFileDir": "blog/posts",
          "contentType": "mdx",
          "flattenedPath": "blog/posts/6-best-eaas-product-alternatives-to-gitpod"
        },
        "type": "BlogPost",
        "computedSlug": "6-best-eaas-product-alternatives-to-gitpod"
      },
      "documentHash": "1739393595015",
      "hasWarnings": false,
      "documentTypeName": "BlogPost"
    },
    "blog/posts/6-docker-compose-best-practices-for-dev-and-prod.mdx": {
      "document": {
        "title": "6 Docker Compose Best Practices for Dev and Prod",
        "summary": "Docker Compose is an excellent optimization tool for development and production. Learn best practices for Docker Compose",
        "publishDate": "Wed Aug 10 2022 20:38:19 GMT+0000 (Coordinated Universal Time)",
        "author": "regis-wilson",
        "readingTime": 5,
        "categories": [
          "platform-engineering",
          "kubernetes"
        ],
        "mainImage": "/blog-images/3ae1bcb14fb4d151cd6a705f7e9101d8.jpg",
        "imageAlt": "a person holding a pen",
        "showCTA": true,
        "ctaCopy": "Simplify multi-container environments like Docker Compose for dev and prod with Release's on-demand environments. Accelerate collaboration and deployment cycles.",
        "ctaLink": "https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=6-docker-compose-best-practices-for-dev-and-prod",
        "relatedPosts": [
          ""
        ],
        "ogImage": "/blog-images/3ae1bcb14fb4d151cd6a705f7e9101d8.jpg",
        "excerpt": "Docker Compose is an excellent optimization tool for development and production. Learn best practices for Docker Compose",
        "tags": [
          "platform-engineering",
          "kubernetes"
        ],
        "ctaButton": "Try Release for Free",
        "body": {
          "raw": "\nDocker solves the \"but it runs on my machine\" problem by introducing containerization. However, with a multifaceted code base, you must simultaneously run several containers like the back and front end. Further, this will require you to leverage tools such as Docker Compose.\n\nDocker Compose is an excellent tool for optimizing the process of creating development, testing, staging, and production environments. With Docker Compose, you'll use a single file to build your environment instead of several files with complex scripts with a lot of branching logic. You can also share this single file with other developers on your team, making it easy to work from the same baseline environment.\n\nThis post is about the best practices of Docker Compose for development and production.\n\n![A picture containing text, stationaryDescription automatically generated](/blog-images/45fc21380e5ae96bca6b4050e7cf9ad8.jpeg)\n\n### What Is Docker Compose Good for?\n\nDocker Compose is a tool for defining and running multi-container Docker applications. It allows you to bring up and link multiple containers into one logical unit. If you want to use Docker containers, you create one container that listens on an unused port on your machine. All other containers will connect to this server container on the same machine. Linking is where the different Docker services are connected and communicate with each other through a central node. This enables them to share data like configuration or databases.\n\nDocker Compose allows you to deploy your application's services as containers and lets you manage these containers as an organized and working whole in a single place—without having to worry about configuring your application's dependencies. For instance, if your app depends on three other services—like a database, an email server, and a messaging server—using Compose means you won't have to manage them individually.\n\nInstead, Docker handles that part for you so that all four services are available within one cohesive environment. This significantly [reduces the time](https://release.com/blog/cutting-build-time-in-half-docker-buildx-kubernetes) needed to get a service up and running. You can make changes simultaneously across all services. Therefore, Docker Compose is an excellent tool for building complex applications that utilize several services.\n\n![Graphical user interface, applicationDescription automatically generated with medium confidence](/blog-images/cf39ebfc109dd02e05b0daad24a81828.png)\n\n### Docker Compose Best Practices for Development\n\nDuring development, you may have the advantage of leveraging local storage, which is not the case during production. In production, resources like storage are costly; thus, you must carefully structure the **docker-compose** file. Essentially, the configuration in development and production slightly differ, and the best practices also differ. Below are the best practices you should employ when using Docker Compose during development.\n\n### Mount Your Code as Volume to Avoid Unnecessary Rebuilds\n\nBy mounting the project directory (current directory) on the host to code within the container using the **new volumes** key, you may make changes to the code as you go without having to recompile the image. This also means you do not have to rebuild and push your image to change between development and production environments. You must delete or stop your local Docker machine and start it again.\n\nNote: You can also use a link in your code instead of the bind mount, but the downside to this approach is that you'll have to rebuild and repush your Docker image each time you change. With a bind mount, all you need to do is restart the container.\n\n![LogoDescription automatically generated with low confidence](/blog-images/5c7b48f49c580cf62a9e6f1bc35781fc.jpeg)\n\n### Use an Override File\n\nSome files are only necessary during development and not production. For example, developing a JavaScript application using any of its frameworks needs [webpack](https://webpack.js.org/). Thus, an override file will mimic the compose file but with webpack as a service. When you spin up the container, the compose files will bundle together during development. Therefore, when you make changes in the code base, you'll see the changes in real time. This allows you to have separate settings for the production and development environment by avoiding redundancy. Your **docker-compose.override.yml** file will have the following:\n\n```yaml\nservices:\n   webpack:\n     build:\n       context: \".\"\n       target: \"webpack\"\n     command: \"yarn run watch\"\n```\n\n### Use YAML Anchors\n\nYAML anchors let you reuse parts of your YAML file-like functions. You can use them to share default settings between services. For example, let's say you want to create two services: **api** and **web**. Both of them will use Redis as a cache and a database, but the **web** service will need additional volumes mounted. It would be cumbersome to set all these things in the web service's **docker-compose.yml** file because it would have to duplicate those lines in the **api** service's file.\n\nUsing YAML anchors to share those settings, you can set the volumes section in the **web** service's file like this:\n\n```yaml\n\nx-app: &default-app\n  build:\n    context: \".\"\n    target: \"app\"\n  depends_on:\n    - \"postgres\"\n    - \"redis\"\n  env_file:\n    - \".env\"\n  restart: \"${DOCKER_RESTART_POLICY:-unless-stopped}\"\n  api:\n    <<: *default-app\n    ports:\n      - \"8000:8000\"\n  web:\n    <<: *default-app\n    ports:\n      - \"8000:5000\"\n\n```\n\nAdditionally, you may alter an aliased property by overriding it in a particular service. If you wanted to, you could set **port:5000** in the example above to only the **web** service. The alias will give it precedence. This method is beneficial when two services could share a Dockerfile and a code base but have some slight variations.\n\n### Docker Compose Best Practices for Production\n\nAs mentioned, dev and production may have slight configuration differences. So now, let's look at some best practices to help your app be production ready.\n\n### Leverage the Docker Restart Policy\n\nOccasionally, you'll face a scenario when a service fails to start. A common reason is that another service on your host machine has changed, and Docker Compose uses the old environment variables. To ensure this doesn't happen, set the restart behavior to **restart: always** and configure your services with **update_config: true**. This will refresh the environment variables for each run. However, if your app relies on other services (MySQL, Redis, etc.) outside of Docker Compose, then you should take extra precautions. Make sure they are configured correctly.\n\n### Correct Cleanup Order of Docker Images\n\nYou need to clean up the order of your images during production. Do not use **docker rm -f** as it may destroy useful images. Always run docker **rm -f --remove-orphans**. If you're working in the dev stage, this is not an issue because Docker Compose builds images only once, then exposes them. Thus there's no need to worry about removing old images. However, in production, Docker loops through all images when the container stops and restarts.\n\nConsequently, there's no way for you to be sure that an image wasn't destroyed, even when **docker-compose down** is called. If a container is stopped and restarted, then the exposed images can change, and you can't be sure they're still in use. Using **docker rm -f** to delete containers is a mistake. Docker Compose reuses port bindings, so an old service is still available, even though its container was destroyed.\n\n![Graphical user interface, text, applicationDescription automatically generated with medium confidence](/blog-images/54d365baed2380c9df53ddc5f32fd4b8.png)\n\nSince you cannot tell which containers might be potentially in use, you must delete all of them using the **\\--remove-orphans** flag. If a container is restarted by Docker Compose (or something else) and it reuses the same port, the new image will have the same image ID as the old one.\n\nNotice we've added the **\\--remove-orphans** flag because that ensures Docker Compose only deletes containers and images that are no longer in use, regardless of whether we or a running container uses them. This is crucial if you have services restarting.\n\n### Setting Your Containers' CPU and Memory Limits\n\nYou can configure Docker to limit the CPU and memory of your containers by passing arguments into the **docker-compose.yml** file before starting your container. For example, the following command will start a web service with one CPU:\n\n```yaml\nweb:\n     deploy:\n       resources:\n         limits:\n           cpus: \"1\"\n```\n\nIf you set a specific number of CPUs in the **multi_cpu** key, it will only be used when available. If you fail to set the limit, the service will use the maximum resources it requires.\n\nTip: If you want to run multiple containers with different memory limits on the same machine, ensure that all your containers have different memory limits. This is because each container views how much memory it needs.\n\nNote: You can use this technique for multiple services if you'd like. Docker Compose will automatically get the values from the **env** file for each container when it starts up.\n\nConsequently, you need to understand the resource requirements of your service. This will prevent you from wasting resources and minimize production costs.\n\n### Conclusion\n\nHopefully, these tips will help you use Docker Compose more effectively in development and production. After trying out the above configuration and optimization, you should be able to build your containers efficiently. If you feel that the above approach reduces the complexity of your Docker composition setup, don't worry. There are much easier ways to organize your containerized services for development and production at [Release](https://release.com/).\n\n‍\n",
          "code": "var Component=(()=>{var h=Object.create;var r=Object.defineProperty;var u=Object.getOwnPropertyDescriptor;var m=Object.getOwnPropertyNames;var p=Object.getPrototypeOf,g=Object.prototype.hasOwnProperty;var f=(n,e)=>()=>(e||n((e={exports:{}}).exports,e),e.exports),y=(n,e)=>{for(var i in e)r(n,i,{get:e[i],enumerable:!0})},s=(n,e,i,a)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let t of m(e))!g.call(n,t)&&t!==i&&r(n,t,{get:()=>e[t],enumerable:!(a=u(e,t))||a.enumerable});return n};var v=(n,e,i)=>(i=n!=null?h(p(n)):{},s(e||!n||!n.__esModule?r(i,\"default\",{value:n,enumerable:!0}):i,n)),b=n=>s(r({},\"__esModule\",{value:!0}),n);var l=f((N,c)=>{c.exports=_jsx_runtime});var C={};y(C,{default:()=>D,frontmatter:()=>w});var o=v(l()),w={title:\"6 Docker Compose Best Practices for Dev and Prod\",summary:\"Docker Compose is an excellent optimization tool for development and production. Learn best practices for Docker Compose\",publishDate:\"Wed Aug 10 2022 20:38:19 GMT+0000 (Coordinated Universal Time)\",author:\"regis-wilson\",readingTime:5,categories:[\"platform-engineering\",\"kubernetes\"],mainImage:\"/blog-images/3ae1bcb14fb4d151cd6a705f7e9101d8.jpg\",imageAlt:\"a person holding a pen\",showCTA:!0,ctaCopy:\"Simplify multi-container environments like Docker Compose for dev and prod with Release's on-demand environments. Accelerate collaboration and deployment cycles.\",ctaLink:\"https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=6-docker-compose-best-practices-for-dev-and-prod\",relatedPosts:[\"\"],ogImage:\"/blog-images/3ae1bcb14fb4d151cd6a705f7e9101d8.jpg\",excerpt:\"Docker Compose is an excellent optimization tool for development and production. Learn best practices for Docker Compose\",tags:[\"platform-engineering\",\"kubernetes\"],ctaButton:\"Try Release for Free\"};function d(n){let e=Object.assign({p:\"p\",img:\"img\",h3:\"h3\",a:\"a\",span:\"span\",strong:\"strong\",pre:\"pre\",code:\"code\"},n.components);return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(e.p,{children:'Docker solves the \"but it runs on my machine\" problem by introducing containerization. However, with a multifaceted code base, you must simultaneously run several containers like the back and front end. Further, this will require you to leverage tools such as Docker Compose.'}),`\n`,(0,o.jsx)(e.p,{children:\"Docker Compose is an excellent tool for optimizing the process of creating development, testing, staging, and production environments. With Docker Compose, you'll use a single file to build your environment instead of several files with complex scripts with a lot of branching logic. You can also share this single file with other developers on your team, making it easy to work from the same baseline environment.\"}),`\n`,(0,o.jsx)(e.p,{children:\"This post is about the best practices of Docker Compose for development and production.\"}),`\n`,(0,o.jsx)(e.p,{children:(0,o.jsx)(e.img,{src:\"/blog-images/45fc21380e5ae96bca6b4050e7cf9ad8.jpeg\",alt:\"A picture containing text, stationaryDescription automatically generated\"})}),`\n`,(0,o.jsxs)(e.h3,{id:\"what-is-docker-compose-good-for\",children:[(0,o.jsx)(e.a,{className:\"anchor\",href:\"#what-is-docker-compose-good-for\",children:(0,o.jsx)(e.span,{className:\"icon icon-link\"})}),\"What Is Docker Compose Good for?\"]}),`\n`,(0,o.jsx)(e.p,{children:\"Docker Compose is a tool for defining and running multi-container Docker applications. It allows you to bring up and link multiple containers into one logical unit. If you want to use Docker containers, you create one container that listens on an unused port on your machine. All other containers will connect to this server container on the same machine. Linking is where the different Docker services are connected and communicate with each other through a central node. This enables them to share data like configuration or databases.\"}),`\n`,(0,o.jsx)(e.p,{children:\"Docker Compose allows you to deploy your application's services as containers and lets you manage these containers as an organized and working whole in a single place\\u2014without having to worry about configuring your application's dependencies. For instance, if your app depends on three other services\\u2014like a database, an email server, and a messaging server\\u2014using Compose means you won't have to manage them individually.\"}),`\n`,(0,o.jsxs)(e.p,{children:[\"Instead, Docker handles that part for you so that all four services are available within one cohesive environment. This significantly \",(0,o.jsx)(e.a,{href:\"https://release.com/blog/cutting-build-time-in-half-docker-buildx-kubernetes\",children:\"reduces the time\"}),\" needed to get a service up and running. You can make changes simultaneously across all services. Therefore, Docker Compose is an excellent tool for building complex applications that utilize several services.\"]}),`\n`,(0,o.jsx)(e.p,{children:(0,o.jsx)(e.img,{src:\"/blog-images/cf39ebfc109dd02e05b0daad24a81828.png\",alt:\"Graphical user interface, applicationDescription automatically generated with medium confidence\"})}),`\n`,(0,o.jsxs)(e.h3,{id:\"docker-compose-best-practices-for-development\",children:[(0,o.jsx)(e.a,{className:\"anchor\",href:\"#docker-compose-best-practices-for-development\",children:(0,o.jsx)(e.span,{className:\"icon icon-link\"})}),\"Docker Compose Best Practices for Development\"]}),`\n`,(0,o.jsxs)(e.p,{children:[\"During development, you may have the advantage of leveraging local storage, which is not the case during production. In production, resources like storage are costly; thus, you must carefully structure the \",(0,o.jsx)(e.strong,{children:\"docker-compose\"}),\" file. Essentially, the configuration in development and production slightly differ, and the best practices also differ. Below are the best practices you should employ when using Docker Compose during development.\"]}),`\n`,(0,o.jsxs)(e.h3,{id:\"mount-your-code-as-volume-to-avoid-unnecessary-rebuilds\",children:[(0,o.jsx)(e.a,{className:\"anchor\",href:\"#mount-your-code-as-volume-to-avoid-unnecessary-rebuilds\",children:(0,o.jsx)(e.span,{className:\"icon icon-link\"})}),\"Mount Your Code as Volume to Avoid Unnecessary Rebuilds\"]}),`\n`,(0,o.jsxs)(e.p,{children:[\"By mounting the project directory (current directory) on the host to code within the container using the \",(0,o.jsx)(e.strong,{children:\"new volumes\"}),\" key, you may make changes to the code as you go without having to recompile the image. This also means you do not have to rebuild and push your image to change between development and production environments. You must delete or stop your local Docker machine and start it again.\"]}),`\n`,(0,o.jsx)(e.p,{children:\"Note: You can also use a link in your code instead of the bind mount, but the downside to this approach is that you'll have to rebuild and repush your Docker image each time you change. With a bind mount, all you need to do is restart the container.\"}),`\n`,(0,o.jsx)(e.p,{children:(0,o.jsx)(e.img,{src:\"/blog-images/5c7b48f49c580cf62a9e6f1bc35781fc.jpeg\",alt:\"LogoDescription automatically generated with low confidence\"})}),`\n`,(0,o.jsxs)(e.h3,{id:\"use-an-override-file\",children:[(0,o.jsx)(e.a,{className:\"anchor\",href:\"#use-an-override-file\",children:(0,o.jsx)(e.span,{className:\"icon icon-link\"})}),\"Use an Override File\"]}),`\n`,(0,o.jsxs)(e.p,{children:[\"Some files are only necessary during development and not production. For example, developing a JavaScript application using any of its frameworks needs \",(0,o.jsx)(e.a,{href:\"https://webpack.js.org/\",children:\"webpack\"}),\". Thus, an override file will mimic the compose file but with webpack as a service. When you spin up the container, the compose files will bundle together during development. Therefore, when you make changes in the code base, you'll see the changes in real time. This allows you to have separate settings for the production and development environment by avoiding redundancy. Your \",(0,o.jsx)(e.strong,{children:\"docker-compose.override.yml\"}),\" file will have the following:\"]}),`\n`,(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:\"language-yaml\",children:`services:\n  \\xA0webpack:\n  \\xA0 \\xA0build:\n  \\xA0 \\xA0 \\xA0context: \".\"\n  \\xA0 \\xA0 \\xA0target: \"webpack\"\n  \\xA0 \\xA0command: \"yarn run watch\"\n`})}),`\n`,(0,o.jsxs)(e.h3,{id:\"use-yaml-anchors\",children:[(0,o.jsx)(e.a,{className:\"anchor\",href:\"#use-yaml-anchors\",children:(0,o.jsx)(e.span,{className:\"icon icon-link\"})}),\"Use YAML Anchors\"]}),`\n`,(0,o.jsxs)(e.p,{children:[\"YAML anchors let you reuse parts of your YAML file-like functions. You can use them to share default settings between services. For example, let's say you want to create two services: \",(0,o.jsx)(e.strong,{children:\"api\"}),\" and \",(0,o.jsx)(e.strong,{children:\"web\"}),\". Both of them will use Redis as a cache and a database, but the \",(0,o.jsx)(e.strong,{children:\"web\"}),\" service will need additional volumes mounted. It would be cumbersome to set all these things in the web service's \",(0,o.jsx)(e.strong,{children:\"docker-compose.yml\"}),\" file because it would have to duplicate those lines in the \",(0,o.jsx)(e.strong,{children:\"api\"}),\" service's file.\"]}),`\n`,(0,o.jsxs)(e.p,{children:[\"Using YAML anchors to share those settings, you can set the volumes section in the \",(0,o.jsx)(e.strong,{children:\"web\"}),\" service's file like this:\"]}),`\n`,(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:\"language-yaml\",children:`\nx-app: &default-app\n \\xA0build:\n \\xA0 \\xA0context: \".\"\n \\xA0 \\xA0target: \"app\"\n \\xA0depends_on:\n \\xA0 \\xA0- \"postgres\"\n \\xA0 \\xA0- \"redis\"\n \\xA0env_file:\n \\xA0 \\xA0- \".env\"\n \\xA0restart: \"\\${DOCKER_RESTART_POLICY:-unless-stopped}\"\n \\xA0api:\n \\xA0 \\xA0<<: *default-app\n \\xA0 \\xA0ports:\n \\xA0 \\xA0 \\xA0- \"8000:8000\"\n \\xA0web:\n \\xA0 \\xA0<<: *default-app\n \\xA0 \\xA0ports:\n \\xA0 \\xA0 \\xA0- \"8000:5000\"\n\n`})}),`\n`,(0,o.jsxs)(e.p,{children:[\"Additionally, you may alter an aliased property by overriding it in a particular service. If you wanted to, you could set \",(0,o.jsx)(e.strong,{children:\"port:5000\"}),\" in the example above to only the \",(0,o.jsx)(e.strong,{children:\"web\"}),\" service. The alias will give it precedence. This method is beneficial when two services could share a Dockerfile and a code base but have some slight variations.\"]}),`\n`,(0,o.jsxs)(e.h3,{id:\"docker-compose-best-practices-for-production\",children:[(0,o.jsx)(e.a,{className:\"anchor\",href:\"#docker-compose-best-practices-for-production\",children:(0,o.jsx)(e.span,{className:\"icon icon-link\"})}),\"Docker Compose Best Practices for Production\"]}),`\n`,(0,o.jsx)(e.p,{children:\"As mentioned, dev and production may have slight configuration differences. So now, let's look at some best practices to help your app be production ready.\"}),`\n`,(0,o.jsxs)(e.h3,{id:\"leverage-the-docker-restart-policy\",children:[(0,o.jsx)(e.a,{className:\"anchor\",href:\"#leverage-the-docker-restart-policy\",children:(0,o.jsx)(e.span,{className:\"icon icon-link\"})}),\"Leverage the Docker Restart Policy\"]}),`\n`,(0,o.jsxs)(e.p,{children:[\"Occasionally, you'll face a scenario when a service fails to start. A common reason is that another service on your host machine has changed, and Docker Compose uses the old environment variables. To ensure this doesn't happen, set the restart behavior to \",(0,o.jsx)(e.strong,{children:\"restart: always\"}),\" and configure your services with \",(0,o.jsx)(e.strong,{children:\"update_config: true\"}),\". This will refresh the environment variables for each run. However, if your app relies on other services (MySQL, Redis, etc.) outside of Docker Compose, then you should take extra precautions. Make sure they are configured correctly.\"]}),`\n`,(0,o.jsxs)(e.h3,{id:\"correct-cleanup-order-of-docker-images\",children:[(0,o.jsx)(e.a,{className:\"anchor\",href:\"#correct-cleanup-order-of-docker-images\",children:(0,o.jsx)(e.span,{className:\"icon icon-link\"})}),\"Correct Cleanup Order of Docker Images\"]}),`\n`,(0,o.jsxs)(e.p,{children:[\"You need to clean up the order of your images during production. Do not use \",(0,o.jsx)(e.strong,{children:\"docker rm -f\"}),\" as it may destroy useful images. Always run docker \",(0,o.jsx)(e.strong,{children:\"rm -f --remove-orphans\"}),\". If you're working in the dev stage, this is not an issue because Docker Compose builds images only once, then exposes them. Thus there's no need to worry about removing old images. However, in production, Docker loops through all images when the container stops and restarts.\"]}),`\n`,(0,o.jsxs)(e.p,{children:[\"Consequently, there's no way for you to be sure that an image wasn't destroyed, even when \",(0,o.jsx)(e.strong,{children:\"docker-compose down\"}),\" is called. If a container is stopped and restarted, then the exposed images can change, and you can't be sure they're still in use. Using \",(0,o.jsx)(e.strong,{children:\"docker rm -f\"}),\" to delete containers is a mistake. Docker Compose reuses port bindings, so an old service is still available, even though its container was destroyed.\"]}),`\n`,(0,o.jsx)(e.p,{children:(0,o.jsx)(e.img,{src:\"/blog-images/54d365baed2380c9df53ddc5f32fd4b8.png\",alt:\"Graphical user interface, text, applicationDescription automatically generated with medium confidence\"})}),`\n`,(0,o.jsxs)(e.p,{children:[\"Since you cannot tell which containers might be potentially in use, you must delete all of them using the \",(0,o.jsx)(e.strong,{children:\"--remove-orphans\"}),\" flag. If a container is restarted by Docker Compose (or something else) and it reuses the same port, the new image will have the same image ID as the old one.\"]}),`\n`,(0,o.jsxs)(e.p,{children:[\"Notice we've added the \",(0,o.jsx)(e.strong,{children:\"--remove-orphans\"}),\" flag because that ensures Docker Compose only deletes containers and images that are no longer in use, regardless of whether we or a running container uses them. This is crucial if you have services restarting.\"]}),`\n`,(0,o.jsxs)(e.h3,{id:\"setting-your-containers-cpu-and-memory-limits\",children:[(0,o.jsx)(e.a,{className:\"anchor\",href:\"#setting-your-containers-cpu-and-memory-limits\",children:(0,o.jsx)(e.span,{className:\"icon icon-link\"})}),\"Setting Your Containers' CPU and Memory Limits\"]}),`\n`,(0,o.jsxs)(e.p,{children:[\"You can configure Docker to limit the CPU and memory of your containers by passing arguments into the \",(0,o.jsx)(e.strong,{children:\"docker-compose.yml\"}),\" file before starting your container. For example, the following command will start a web service with one CPU:\"]}),`\n`,(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:\"language-yaml\",children:`web:\n  \\xA0 \\xA0deploy:\n  \\xA0 \\xA0 \\xA0resources:\n  \\xA0 \\xA0 \\xA0 \\xA0limits:\n  \\xA0 \\xA0 \\xA0 \\xA0 \\xA0cpus: \"1\"\n`})}),`\n`,(0,o.jsxs)(e.p,{children:[\"If you set a specific number of CPUs in the \",(0,o.jsx)(e.strong,{children:\"multi_cpu\"}),\" key, it will only be used when available. If you fail to set the limit, the service will use the maximum resources it requires.\"]}),`\n`,(0,o.jsx)(e.p,{children:\"Tip: If you want to run multiple containers with different memory limits on the same machine, ensure that all your containers have different memory limits. This is because each container views how much memory it needs.\"}),`\n`,(0,o.jsxs)(e.p,{children:[\"Note: You can use this technique for multiple services if you'd like. Docker Compose will automatically get the values from the \",(0,o.jsx)(e.strong,{children:\"env\"}),\" file for each container when it starts up.\"]}),`\n`,(0,o.jsx)(e.p,{children:\"Consequently, you need to understand the resource requirements of your service. This will prevent you from wasting resources and minimize production costs.\"}),`\n`,(0,o.jsxs)(e.h3,{id:\"conclusion\",children:[(0,o.jsx)(e.a,{className:\"anchor\",href:\"#conclusion\",children:(0,o.jsx)(e.span,{className:\"icon icon-link\"})}),\"Conclusion\"]}),`\n`,(0,o.jsxs)(e.p,{children:[\"Hopefully, these tips will help you use Docker Compose more effectively in development and production. After trying out the above configuration and optimization, you should be able to build your containers efficiently. If you feel that the above approach reduces the complexity of your Docker composition setup, don't worry. There are much easier ways to organize your containerized services for development and production at \",(0,o.jsx)(e.a,{href:\"https://release.com/\",children:\"Release\"}),\".\"]}),`\n`,(0,o.jsx)(e.p,{children:\"\\u200D\"})]})}function k(n={}){let{wrapper:e}=n.components||{};return e?(0,o.jsx)(e,Object.assign({},n,{children:(0,o.jsx)(d,n)})):d(n)}var D=k;return b(C);})();\n;return Component;"
        },
        "_id": "blog/posts/6-docker-compose-best-practices-for-dev-and-prod.mdx",
        "_raw": {
          "sourceFilePath": "blog/posts/6-docker-compose-best-practices-for-dev-and-prod.mdx",
          "sourceFileName": "6-docker-compose-best-practices-for-dev-and-prod.mdx",
          "sourceFileDir": "blog/posts",
          "contentType": "mdx",
          "flattenedPath": "blog/posts/6-docker-compose-best-practices-for-dev-and-prod"
        },
        "type": "BlogPost",
        "computedSlug": "6-docker-compose-best-practices-for-dev-and-prod"
      },
      "documentHash": "1739393595015",
      "hasWarnings": false,
      "documentTypeName": "BlogPost"
    },
    "blog/posts/6-software-development-environment-best-practices.mdx": {
      "document": {
        "title": "6 Software Development Environment Best Practices",
        "summary": "This post will cover a range of best practices to improve the productivity and quality of your software development proc",
        "publishDate": "Mon Jan 09 2023 07:53:51 GMT+0000 (Coordinated Universal Time)",
        "author": "mercy-kibet",
        "readingTime": 7,
        "categories": [
          "platform-engineering",
          "product"
        ],
        "mainImage": "/blog-images/2264393ccc1b4cef342847ddb6cca873.jpg",
        "imageAlt": "A group of people sitting at a table with laptops",
        "showCTA": true,
        "ctaCopy": "Improve software development with Release's on-demand environments for testing and staging. Accelerate deployment cycles and streamline workflows today!",
        "ctaLink": "https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=6-software-development-environment-best-practices",
        "relatedPosts": [
          ""
        ],
        "ogImage": "/blog-images/2264393ccc1b4cef342847ddb6cca873.jpg",
        "excerpt": "This post will cover a range of best practices to improve the productivity and quality of your software development proc",
        "tags": [
          "platform-engineering",
          "product"
        ],
        "ctaButton": "Try Release for Free",
        "body": {
          "raw": "\nA well-designed development environment is essential for efficient and effective software development. It provides developers with the tools and resources to write, test, and debug their code. \n\nBy following best practices for setting up and maintaining a development environment, you can improve the productivity and quality of your software development process. \n\nThis post will cover a range of best practices, including using version control, writing automated tests, and collaborating with other developers. So, let's dive in and explore the best practices for a software development environment. \n\n![](/blog-images/c7358d501c109a3f5bfc6faa3248131a.jpeg)\n\n### What is a Software Development Environment?\n\nA software development [environment](https://release.com/blog/remote-development-environments) is a set of tools and processes to create and develop software applications. \n\nA software development environment may include a code editor or integrated development environment (IDE), a compiler or interpreter, and other tools such as version control and testing frameworks. \n\nThe software development environment is the workspace in which developers write, test, and debug their code. It provides all the tools and resources for you as a developer to be productive and create high-quality software. \n\n### How Many Environments Should You Have for Software Development?\n\nThe number of [environments](https://release.com/blog/remote-development-environments) you should have for software development depends on your specific needs and the size and complexity of your project. \n\nHowever, you can have at least two environments: a development environment for writing and testing code and a production environment for running the final version of your software. \n\nHaving separate development and production environments allows you to test your code in a controlled environment before deploying it to users. This can help ensure that your software is reliable and free of bugs. \n\nBesides these two environments, you may also want a staging environment for testing code before deploying to production. This environment can help test new features or conduct performance or load testing. \n\nHaving the right balance of environments is essential to support your software's efficient and effective development and deployment. \n\n### 6 Software Development Environment Best Practices\n\nThere are many best practices for developing software effectively and efficiently. Some of these best practices include: \n\n#### 1\\. Organized File Structure\n\nAs a first step toward creating a consistent and productive development flow, you need to get organized around your project's file structure. The file structure should be based on the application's needs but not too rigid. \n\nStructure maps can help establish consistent naming conventions for all source files and the directories and files for each module. This is how you'll break down your application into modules and feature sets. \n\nMany development environments have a file structure variation of their own, making it easier to disentangle your legacy code into different modules that you can reuse or extract as libraries. Source control will help you keep track of all versions of implemented features, which helps with code maintenance. \n\n#### 2\\. Using Version/Source Control\n\nThe two key terms here are version control and source code control. Version control is tracking changes to your source code. The goal of source code control is to keep track of every change made to the source files. Version control saves the previous versions of your code so that you can easily roll back if necessary. \n\nSource control allows multiple developers to work collaboratively on the same codebase without worrying about overwriting each other’s changes. By having version and source control in place, developers can ensure that their code is always up to date and that their team is working on the same version of the code. \n\nSource code control will ensure you're ready for the next step if you want to use a different development tool or paradigm. Using a version control system such as Subversion (SVN), you can quickly generate a report on how different branches affect performance. \n\nExamples of version control tools include Git, SVN, Mercurial, and Team Foundation Server (TFS). Git is the most popular version control tool developers use to store and track changes to their code on platforms such as GitHub and GitLab. \n\n![](/blog-images/3f0594bf4e38b3b7277d5fe9be1ad16c.jpeg)\n\n#### 3\\. Testing\n\nTo ensure that applications function as designed, developers must test their code thoroughly before implementing it into an actual production environment. \n\nCode coverage reports can provide developers with valuable information about the percentage of their code that has been tested. This will serve as a guide to the portion of code that needs to be fixed. \n\nHere, you can employ different paradigms, including test-driven development (TDD). \n\nWith TDD, you go through a red-green-refactor process. First, write some tests that will fail on the first run (red). Next, you'll write enough code to pass the test (green). Finally, you make changes to optimize your code (refactor). \n\n#### 4\\. Performance Monitoring\n\nPerformance monitoring can help you identify areas for improvement and inform decisions regarding the allocation of resources. It can also help you diagnose and identify bugs that may have appeared during the development process. \n\nBeing proactive about addressing and monitoring issues related to the performance of your applications can help you meet your users' expectations. \n\nPerformance monitoring and cloud computing go hand in hand. You can use tools like [CloudWatch](https://aws.amazon.com/cloudwatch/) and New Relic to help you understand application performance in real-time. \n\nIf you're facing significant problems while developing software applications, there are a few things you can do to help improve your workflow. It's all about keeping track of release milestones, showing each set of changes separately, generating reports on the various impacts of different feature sets on an application's performance and code quality, and ensuring that everything works as planned when it goes out the door. \n\n#### 5\\. Documenting\n\nDocumentation is a best practice for software development [environments](https://release.com/blog/remote-development-environments) because it helps developers understand the code they're writing and how it interacts with other pieces of code. A good documentation process will help to ensure that changes to the code are captured. \n\nDocumentation makes it easier for other developers to understand your code and work with it. It provides a reference point for developers when they need to debug or troubleshoot problems. \n\nFinally, documentation can help prevent errors by providing instructions that developers can refer to when writing code. \n\n#### 6\\. Containerization\n\nContainers provide a consistent and isolated environment for applications to run, so developers can be sure that their applications will run the same way in any environment. This is especially important for distributed applications, as [containers](https://releasehub.com/blog/6-docker-compose-best-practices-for-dev-and-prod) make it easier to deploy the same application to multiple environments. \n\nContainers also offer portability, allowing applications to be moved from one environment to another quickly and easily. Containers are lightweight and efficient, reducing the overhead of running multiple applications in a single environment. \n\nFinally, containers offer scalability, making it easier to scale applications as needed. \n\n### What are Best Practices According to Different Roles?\n\nBelow are some best practices depending on the role you hold. \n\n#### Leadership Best Practices\n\n1.  Establish clear goals and objectives for the development team.\n2.  Foster a culture of innovation and collaboration.\n3.  Encourage developers to be innovative and develop creative solutions.\n4.  Provide continual training and support for developers.\n5.  Stay up to date on the latest development technologies and trends.\n6.  Be an effective communicator and listener.\n\n#### Secondary Employee Best Practices\n\n1.  Follow established development protocols and procedures.\n2.  Monitor project timelines and ensure deadlines are met.\n3.  Implement quality assurance protocols to ensure software stability.\n4.  Use debugging tools to identify and fix software issues.\n5.  Document development details and specifications for future reference.\n6.  Create user-friendly interfaces for the software.\n\n![](/blog-images/1b2f9cce4918635701da91e7f279d662.png)\n\n### Conclusion\n\nA well-designed software development environment is essential for efficient and effective software development. By following best practices for setting up and maintaining your development environment, you can improve the productivity and quality of your software development process. \n\nSome critical best practices include using version/source control, testing, documenting, and containerization. \n\nIf you want to learn more about best practices for a software development environment, check out [Release](https://release.com/blog/remote-development-environments). Release is a comprehensive resource for software development best practices, including tips and tools for setting up and maintaining a development environment. \n\nVisit Release today to learn more, and take your software development to the next level. \n\nWant to improve your software development skills? Check out our latest blog post on the six best practices for creating a productive and efficient development environment. \n\n_This post was written by Mercy Kibet._ [_Mercy_](https://hashnode.com/@eiMJay) _is a full-stack developer with a knack for learning and writing about new and intriguing tech stacks._\n",
          "code": "var Component=(()=>{var p=Object.create;var i=Object.defineProperty;var h=Object.getOwnPropertyDescriptor;var m=Object.getOwnPropertyNames;var u=Object.getPrototypeOf,f=Object.prototype.hasOwnProperty;var v=(o,e)=>()=>(e||o((e={exports:{}}).exports,e),e.exports),g=(o,e)=>{for(var t in e)i(o,t,{get:e[t],enumerable:!0})},s=(o,e,t,a)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let r of m(e))!f.call(o,r)&&r!==t&&i(o,r,{get:()=>e[r],enumerable:!(a=h(e,r))||a.enumerable});return o};var y=(o,e,t)=>(t=o!=null?p(u(o)):{},s(e||!o||!o.__esModule?i(t,\"default\",{value:o,enumerable:!0}):t,o)),w=o=>s(i({},\"__esModule\",{value:!0}),o);var l=v((S,c)=>{c.exports=_jsx_runtime});var T={};g(T,{default:()=>N,frontmatter:()=>b});var n=y(l()),b={title:\"6 Software Development Environment Best Practices\",summary:\"This post will cover a range of best practices to improve the productivity and quality of your software development proc\",publishDate:\"Mon Jan 09 2023 07:53:51 GMT+0000 (Coordinated Universal Time)\",author:\"mercy-kibet\",readingTime:7,categories:[\"platform-engineering\",\"product\"],mainImage:\"/blog-images/2264393ccc1b4cef342847ddb6cca873.jpg\",imageAlt:\"A group of people sitting at a table with laptops\",showCTA:!0,ctaCopy:\"Improve software development with Release's on-demand environments for testing and staging. Accelerate deployment cycles and streamline workflows today!\",ctaLink:\"https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=6-software-development-environment-best-practices\",relatedPosts:[\"\"],ogImage:\"/blog-images/2264393ccc1b4cef342847ddb6cca873.jpg\",excerpt:\"This post will cover a range of best practices to improve the productivity and quality of your software development proc\",tags:[\"platform-engineering\",\"product\"],ctaButton:\"Try Release for Free\"};function d(o){let e=Object.assign({p:\"p\",img:\"img\",h3:\"h3\",a:\"a\",span:\"span\",h4:\"h4\",ol:\"ol\",li:\"li\",em:\"em\"},o.components);return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(e.p,{children:\"A well-designed development environment is essential for efficient and effective software development. It provides developers with the tools and resources to write, test, and debug their code.\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:\"By following best practices for setting up and maintaining a development environment, you can improve the productivity and quality of your software development process.\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:\"This post will cover a range of best practices, including using version control, writing automated tests, and collaborating with other developers. So, let's dive in and explore the best practices for a software development environment.\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/c7358d501c109a3f5bfc6faa3248131a.jpeg\",alt:\"\"})}),`\n`,(0,n.jsxs)(e.h3,{id:\"what-is-a-software-development-environment\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#what-is-a-software-development-environment\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"What is a Software Development Environment?\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"A software development \",(0,n.jsx)(e.a,{href:\"https://release.com/blog/remote-development-environments\",children:\"environment\"}),\" is a set of tools and processes to create and develop software applications.\\xA0\"]}),`\n`,(0,n.jsx)(e.p,{children:\"A software development environment may include a code editor or integrated development environment (IDE), a compiler or interpreter, and other tools such as version control and testing frameworks.\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:\"The software development environment is the workspace in which developers write, test, and debug their code. It provides all the tools and resources for you as a developer to be productive and create high-quality software.\\xA0\"}),`\n`,(0,n.jsxs)(e.h3,{id:\"how-many-environments-should-you-have-for-software-development\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#how-many-environments-should-you-have-for-software-development\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"How Many Environments Should You Have for Software Development?\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"The number of \",(0,n.jsx)(e.a,{href:\"https://release.com/blog/remote-development-environments\",children:\"environments\"}),\" you should have for software development depends on your specific needs and the size and complexity of your project.\\xA0\"]}),`\n`,(0,n.jsx)(e.p,{children:\"However, you can have at least two environments: a development environment for writing and testing code and a production environment for running the final version of your software.\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:\"Having separate development and production environments allows you to test your code in a controlled environment before deploying it to users. This can help ensure that your software is reliable and free of bugs.\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:\"Besides these two environments, you may also want a staging environment for testing code before deploying to production. This environment can help test new features or conduct performance or load testing.\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:\"Having the right balance of environments is essential to support your software's efficient and effective development and deployment.\\xA0\"}),`\n`,(0,n.jsxs)(e.h3,{id:\"6-software-development-environment-best-practices\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#6-software-development-environment-best-practices\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"6 Software Development Environment Best Practices\"]}),`\n`,(0,n.jsx)(e.p,{children:\"There are many best practices for developing software effectively and efficiently. Some of these best practices include:\\xA0\"}),`\n`,(0,n.jsxs)(e.h4,{id:\"1-organized-file-structure\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#1-organized-file-structure\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"1. Organized File Structure\"]}),`\n`,(0,n.jsx)(e.p,{children:\"As a first step toward creating a consistent and productive development flow, you need to get organized around your project's file structure. The file structure should be based on the application's needs but not too rigid.\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:\"Structure maps can help establish consistent naming conventions for all source files and the directories and files for each module. This is how you'll break down your application into modules and feature sets.\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:\"Many development environments have a file structure variation of their own, making it easier to disentangle your legacy code into different modules that you can reuse or extract as libraries. Source control will help you keep track of all versions of implemented features, which helps with code maintenance.\\xA0\"}),`\n`,(0,n.jsxs)(e.h4,{id:\"2-using-versionsource-control\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#2-using-versionsource-control\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"2. Using Version/Source Control\"]}),`\n`,(0,n.jsx)(e.p,{children:\"The two key terms here are version control and source code control. Version control is tracking changes to your source code. The goal of source code control is to keep track of every change made to the source files. Version control saves the previous versions of your code so that you can easily roll back if necessary.\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:\"Source control allows multiple developers to work collaboratively on the same codebase without worrying about overwriting each other\\u2019s changes. By having version and source control in place, developers can ensure that their code is always up to date and that their team is working on the same version of the code.\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:\"Source code control will ensure you're ready for the next step if you want to use a different development tool or paradigm. Using a version control system such as Subversion (SVN), you can quickly generate a report on how different branches affect performance.\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:\"Examples of version control tools include Git, SVN, Mercurial, and Team Foundation Server (TFS). Git is the most popular version control tool developers use to store and track changes to their code on platforms such as GitHub and GitLab.\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/3f0594bf4e38b3b7277d5fe9be1ad16c.jpeg\",alt:\"\"})}),`\n`,(0,n.jsxs)(e.h4,{id:\"3-testing\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#3-testing\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"3. Testing\"]}),`\n`,(0,n.jsx)(e.p,{children:\"To ensure that applications function as designed, developers must test their code thoroughly before implementing it into an actual production environment.\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:\"Code coverage reports can provide developers with valuable information about the percentage of their code that has been tested. This will serve as a guide to the portion of code that needs to be fixed.\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:\"Here, you can employ different paradigms, including test-driven development (TDD).\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:\"With TDD, you go through a red-green-refactor process. First, write some tests that will fail on the first run (red). Next, you'll write enough code to pass the test (green). Finally, you make changes to optimize your code (refactor).\\xA0\"}),`\n`,(0,n.jsxs)(e.h4,{id:\"4-performance-monitoring\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#4-performance-monitoring\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"4. Performance Monitoring\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Performance monitoring can help you identify areas for improvement and inform decisions regarding the allocation of resources. It can also help you diagnose and identify bugs that may have appeared during the development process.\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:\"Being proactive about addressing and monitoring issues related to the performance of your applications can help you meet your users' expectations.\\xA0\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"Performance monitoring and cloud computing go hand in hand. You can use tools like \",(0,n.jsx)(e.a,{href:\"https://aws.amazon.com/cloudwatch/\",children:\"CloudWatch\"}),\" and New Relic to help you understand application performance in real-time.\\xA0\"]}),`\n`,(0,n.jsx)(e.p,{children:\"If you're facing significant problems while developing software applications, there are a few things you can do to help improve your workflow. It's all about keeping track of release milestones, showing each set of changes separately, generating reports on the various impacts of different feature sets on an application's performance and code quality, and ensuring that everything works as planned when it goes out the door.\\xA0\"}),`\n`,(0,n.jsxs)(e.h4,{id:\"5-documenting\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#5-documenting\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"5. Documenting\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"Documentation is a best practice for software development \",(0,n.jsx)(e.a,{href:\"https://release.com/blog/remote-development-environments\",children:\"environments\"}),\" because it helps developers understand the code they're writing and how it interacts with other pieces of code. A good documentation process will help to ensure that changes to the code are captured.\\xA0\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Documentation makes it easier for other developers to understand your code and work with it. It provides a reference point for developers when they need to debug or troubleshoot problems.\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:\"Finally, documentation can help prevent errors by providing instructions that developers can refer to when writing code.\\xA0\"}),`\n`,(0,n.jsxs)(e.h4,{id:\"6-containerization\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#6-containerization\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"6. Containerization\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"Containers provide a consistent and isolated environment for applications to run, so developers can be sure that their applications will run the same way in any environment. This is especially important for distributed applications, as \",(0,n.jsx)(e.a,{href:\"https://releasehub.com/blog/6-docker-compose-best-practices-for-dev-and-prod\",children:\"containers\"}),\" make it easier to deploy the same application to multiple environments.\\xA0\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Containers also offer portability, allowing applications to be moved from one environment to another quickly and easily. Containers are lightweight and efficient, reducing the overhead of running multiple applications in a single environment.\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:\"Finally, containers offer scalability, making it easier to scale applications as needed.\\xA0\"}),`\n`,(0,n.jsxs)(e.h3,{id:\"what-are-best-practices-according-to-different-roles\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#what-are-best-practices-according-to-different-roles\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"What are Best Practices According to Different Roles?\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Below are some best practices depending on the role you hold.\\xA0\"}),`\n`,(0,n.jsxs)(e.h4,{id:\"leadership-best-practices\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#leadership-best-practices\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Leadership Best Practices\"]}),`\n`,(0,n.jsxs)(e.ol,{children:[`\n`,(0,n.jsx)(e.li,{children:\"Establish clear goals and objectives for the development team.\"}),`\n`,(0,n.jsx)(e.li,{children:\"Foster a culture of innovation and collaboration.\"}),`\n`,(0,n.jsx)(e.li,{children:\"Encourage developers to be innovative and develop creative solutions.\"}),`\n`,(0,n.jsx)(e.li,{children:\"Provide continual training and support for developers.\"}),`\n`,(0,n.jsx)(e.li,{children:\"Stay up to date on the latest development technologies and trends.\"}),`\n`,(0,n.jsx)(e.li,{children:\"Be an effective communicator and listener.\"}),`\n`]}),`\n`,(0,n.jsxs)(e.h4,{id:\"secondary-employee-best-practices\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#secondary-employee-best-practices\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Secondary Employee Best Practices\"]}),`\n`,(0,n.jsxs)(e.ol,{children:[`\n`,(0,n.jsx)(e.li,{children:\"Follow established development protocols and procedures.\"}),`\n`,(0,n.jsx)(e.li,{children:\"Monitor project timelines and ensure deadlines are met.\"}),`\n`,(0,n.jsx)(e.li,{children:\"Implement quality assurance protocols to ensure software stability.\"}),`\n`,(0,n.jsx)(e.li,{children:\"Use debugging tools to identify and fix software issues.\"}),`\n`,(0,n.jsx)(e.li,{children:\"Document development details and specifications for future reference.\"}),`\n`,(0,n.jsx)(e.li,{children:\"Create user-friendly interfaces for the software.\"}),`\n`]}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/1b2f9cce4918635701da91e7f279d662.png\",alt:\"\"})}),`\n`,(0,n.jsxs)(e.h3,{id:\"conclusion\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#conclusion\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Conclusion\"]}),`\n`,(0,n.jsx)(e.p,{children:\"A well-designed software development environment is essential for efficient and effective software development. By following best practices for setting up and maintaining your development environment, you can improve the productivity and quality of your software development process.\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:\"Some critical best practices include using version/source control, testing, documenting, and containerization.\\xA0\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"If you want to learn more about best practices for a software development environment, check out \",(0,n.jsx)(e.a,{href:\"https://release.com/blog/remote-development-environments\",children:\"Release\"}),\". Release is a comprehensive resource for software development best practices, including tips and tools for setting up and maintaining a development environment.\\xA0\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Visit Release today to learn more, and take your software development to the next level.\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:\"Want to improve your software development skills? Check out our latest blog post on the six best practices for creating a productive and efficient development environment.\\xA0\"}),`\n`,(0,n.jsxs)(e.p,{children:[(0,n.jsx)(e.em,{children:\"This post was written by Mercy Kibet.\"}),\" \",(0,n.jsx)(e.a,{href:\"https://hashnode.com/@eiMJay\",children:(0,n.jsx)(e.em,{children:\"Mercy\"})}),\" \",(0,n.jsx)(e.em,{children:\"is a full-stack developer with a knack for learning and writing about new and intriguing tech stacks.\"})]})]})}function k(o={}){let{wrapper:e}=o.components||{};return e?(0,n.jsx)(e,Object.assign({},o,{children:(0,n.jsx)(d,o)})):d(o)}var N=k;return w(T);})();\n;return Component;"
        },
        "_id": "blog/posts/6-software-development-environment-best-practices.mdx",
        "_raw": {
          "sourceFilePath": "blog/posts/6-software-development-environment-best-practices.mdx",
          "sourceFileName": "6-software-development-environment-best-practices.mdx",
          "sourceFileDir": "blog/posts",
          "contentType": "mdx",
          "flattenedPath": "blog/posts/6-software-development-environment-best-practices"
        },
        "type": "BlogPost",
        "computedSlug": "6-software-development-environment-best-practices"
      },
      "documentHash": "1739393595015",
      "hasWarnings": false,
      "documentTypeName": "BlogPost"
    },
    "blog/posts/a-guide-to-configuring-and-deploying-hpa-on-kubernetes.mdx": {
      "document": {
        "title": "A Guide to Configuring and Deploying HPA on Kubernetes",
        "summary": "HPA in Kubernetes will make your Kubernetes cluster more self-sustainable and will offload you from repetitive tasks.",
        "publishDate": "Wed Sep 14 2022 16:07:00 GMT+0000 (Coordinated Universal Time)",
        "author": "ashley-penney",
        "readingTime": 5,
        "categories": [
          "kubernetes",
          "platform-engineering"
        ],
        "mainImage": "/blog-images/6939a8efb4225dcbacab89ad2bf333d3.jpg",
        "imageAlt": "a row of computer screens with a glass in front of them",
        "showCTA": true,
        "ctaCopy": "Automate scaling with Release's environments like HPA on Kubernetes for efficient resource management.",
        "ctaLink": "https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=a-guide-to-configuring-and-deploying-hpa-on-kubernetes",
        "relatedPosts": [
          ""
        ],
        "ogImage": "/blog-images/6939a8efb4225dcbacab89ad2bf333d3.jpg",
        "excerpt": "HPA in Kubernetes will make your Kubernetes cluster more self-sustainable and will offload you from repetitive tasks.",
        "tags": [
          "kubernetes",
          "platform-engineering"
        ],
        "ctaButton": "Try Release for Free",
        "body": {
          "raw": "\nOne of the main advantages of Kubernetes is that it can take care of your containers for you. This means, for example, that it will move containers around to distribute the load on the cluster evenly, automatically restart failed pods, and kill those that misbehave and try to eat too many resources. Another nice feature is Horizontal Pod Autoscaler. As the name suggests, HPA can automatically scale your pods. But how? I'm glad you asked, because that's exactly what this post is about. \n\n### What Is HPA in Kubernetes?\n\nNormally when you create a deployment in Kubernetes, you need to specify how many pods you want to run. This number is static. Therefore, every time you want to increase or decrease the number of pods, you need to edit the deployment. \n\nIf you only need to do that once or twice a year, it's not that big of a deal. But it's very unlikely that your traffic will be at the same exact level for the whole year. And the more spikes of traffic you have, the more time you'll have to spend editing your deployment to cope with the traffic. This also works the other way around: If you're running a very big cluster with lots of applications, you could save a lot of money by decreasing the number of replicas for each deployment during periods of less traffic, like during the night. But again, it would be a lot of work to make these adjustments manually all the time. And that brings us to HPA. \n\nThe main purpose of HPA is to automatically scale your deployments based on the load to match the demand. [Horizontal](https://en.wikipedia.org/wiki/Autoscaling#Kubernetes_Horizontal_Pod_Autoscaler:~:text=3%5D%5B33%5D-,Kubernetes%20Horizontal%20Pod%20Autoscaler,-%5Bedit%5D), in this case, means that we're talking about scaling the number of pods. You can specify the minimum and the maximum number of pods per deployment and a condition such as CPU or memory usage. Kubernetes will constantly monitor your deployment, and based on the condition you specified, it will increase or decrease the number of pods accordingly. \n\n‍\n\n![](/blog-images/e9b0bdf47e53fda5243b490cba26a58f.png)\n\n### What Is VPA in Kubernetes?\n\nIn Kubernetes, there is also a Vertical Pod Autoscaler (VPA). As you may guess by the name, it works contrary to Horizontal Pod Autoscaler. Instead of adjusting the number of pods up or down, as HPA does, VPA scales up or down the resource requests and limits for the pods. \n\nSo, in theory, VPA tries to achieve the same thing that HPA does, but in practice, they serve very different purposes, and you shouldn’t use them interchangeably. But to understand the difference, we need to take a step back and talk about how autoscaler knows when to scale your pods in the first place. \n\n### A Few Words About Resource Requests\n\nWe mentioned before that you need to provide a condition for HPA, such as CPU usage. So, for example, you can specify that you want your HPA to add an additional pod to the deployment when current pods have average CPU usage higher than 80%. But what does 80% mean? 80% of what? That's a very good question, and the answer will help you understand the main difference between HPA and VPA. \n\nYou see, if you create a very basic Kubernetes deployment just by specifying its name and which docker image to use, you won't be able to add HPA to it. Why is that? It's because you didn't specify resource requests and limits for it. Long story short, specifying resource requests and limits for your [pods](https://release.com/blog/kubernetes-pods-advanced-concepts-explained) isn't just a good practice, but also drastically helps Kubernetes do its job more efficiently. And that brings us back to the question of \"What does 80% usage mean?\" when setting up HPA thresholds. This percentage value is related to the pods' resource requests. And that's how HPA knows when to scale your pods up or down. \n\nIf you say that your application is using around 2GB of RAM under normal load, you set the resource requests accordingly: for example, to 2.5GB (you should always set the request to a little bit more than average). Then your HPA will know that it needs to schedule an additional pod for your deployment when the current one is using more than ~2.4GB (this value will depend on the target value that you specify when creating your HPA). \n\n### Figuring Correct Values for Resource Requests\n\nBut how do you know what resource requests to set in the first place? You could, of course, run your pods without requests first and check how many resources they normally use. But that's quite a time-consuming process, especially on big clusters with multiple applications. \n\nThat brings us back to the VPA. You see, the point of VPA isn't to scale your deployments up or down in order to keep up with sudden spikes in traffic. That's the HPA's job. VPA should be used to get you a good baseline of resource requests and limits for your pods. This way, you're free from that time-consuming task of monitoring pods' typical usage and setting requests and limits accordingly. When the traffic goes up and your pods can't keep up, the HPA should add one or more pods to the pack to get resource usage back to the \"average.\" At that point, VPA doesn't need to do anything. \n\nYou can think of it as VPA working much slower and more long-term, looking at patterns of usage, while HPA responds quicker and provides short-term solutions for load spikes. \n\n‍\n\n![](/blog-images/ea9f60b7a898e8d43fd9c3f87c85cf72.png)\n\n### How Do I Configure HPA in Kubernetes?\n\nNow that you know all the theory, let's create some HPAs. We'll start with creating an example deployment with a resource request set: \n\n```yaml\napiVersion: v1\nkind: Service\nmetadata:\n   name: nginx-svc\n   labels:\n     app: nginx\nspec:\n   ports:\n   - port: 80\n   selector:\n     app: nginx\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n   name: nginx-deployment\n   labels:\n     app: nginx\nspec:\n   replicas: 1\n   selector:\n     matchLabels:\n       app: nginx\n   template:\n     metadata:\n       labels:\n         app: nginx\n     spec:\n       containers:\n       - name: nginx\n         image: nginx\n         ports:\n         - containerPort: 80\n         resources:\n           requests:\n             memory: \"64Mi\"\n             cpu: \"100m\"\n```\n\nI'll save it as nginx.yaml and apply with **kubectl apply**: \n\n```yaml\n$ kubectl apply -f nginx.yaml\ndeployment.apps/nginx-deployment created\n\n$ kubectl get deploy\nNAME               READY   UP-TO-DATE   AVAILABLE   AGE\nnginx-deployment   1/1     1            1           115s\n```\n\nOur deployment is up and running, so we can now add HPA to it. Just like anything in Kubernetes, you can add HPA by creating and applying the YAML definition, just like we did with this deployment. Another option is to use the **kubectl autoscale** command. Let's start with the latter. To create HPA with **kubectl** **autoscale,** you need to execute the following command: \n\n```yaml\n$ kubectl autoscale deploy nginx-deployment --min=1 --max=5 --cpu-percent=80\nhorizontalpodautoscaler.autoscaling/nginx-deployment autoscaled\n```\n\nAfter **kubectl autoscale,** we need to specify the resource type we want to autoscale. In our case, it's **deploy** (short for deployment), and then we specify the deployment name. After that, we pass the minimum and maximum amount of pods HPA can create and the condition on which to scale. \n\n### Validating If HPA Works\n\nIn our example, we tell HPA to scale out pods when their CPU usage goes over 80%. And since in our deployment earlier we specified CPU requests of 100m, this means our HPA will start scaling out nginx-deployment when its average CPU usage goes over 80m. Let's validate that. First, we double-check if HPA is working using **kubectl get hpa:** \n\n```yaml\n$ kubectl get hpa\nNAME               REFERENCE                     TARGETS   MINPODS   MAXPODS   REPLICAS   AGE\nnginx-deployment   Deployment/nginx-deployment   0%/80%    1         5         1          54s\n```\n\nWe can see it's working, and our nginx deployment is currently running one pod. There is no traffic on it, so it makes sense. But let's see if HPA will do its job when we put some traffic on nginx. For that, I'll deploy a simple load generator: \n\n```yaml\n\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: load-generator\n  labels:\n    app: load\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: load\n  template:\n    metadata:\n      labels:\n        app: load\n    spec:\n      containers:\n      - command:\n            - \"/bin/sh\"\n            - \"-c\"\n            - \"while true; do wget -q -O /dev/null nginx-svc; done\"\n        name: load\n        image: busybox\n\n```\n\nOnce deployed (using **kubectl apply -f load-generator.yaml**) we can monitor your nginx deployment CPU usage with **kubectl top pods:** \n\n```yaml\n$ kubectl top pods\nNAME                               CPU(cores)   MEMORY(bytes)\nnginx-deployment-bc54c744b-tlpds   112m         4Mi\n```\n\nAnd when we see the CPU usage goes over 80m, we can check the status of HPA again: \n\n```yaml\n$  ~ kubectl get hpa\nNAME               REFERENCE                     TARGETS    MINPODS   MAXPODS   REPLICAS   AGE\nnginx-deployment   Deployment/nginx-deployment   138%/80%   1         5         2          16m\n```\n\nAnd shortly after that, the target CPU usage should also drop (since the traffic is now distributed to two pods): \n\n```yaml\n$ kubectl get hpa\nNAME               REFERENCE                     TARGETS   MINPODS   MAXPODS   REPLICAS   AGE\nnginx-deployment   Deployment/nginx-deployment   59%/80%   1         5         2          20m\n\n$ kubectl top pods\nNAME                               CPU(cores)   MEMORY(bytes)\nnginx-deployment-bc54c744b-qshtz   52m          3Mi\nnginx-deployment-bc54c744b-tlpds   59m          3Mi\n```\n\nSo, HPA is working as expected. It increased the number of pods for our deployment based on the load. Just keep in mind that HPA doesn't work instantly. It usually takes a few seconds before it will take any action, just to avoid unnecessary scaling actions on very short load spikes. \n\nYou now know how to create HPA with **kubectl,** and this is how you do the same with YAML definition: \n\n```yaml\napiVersion: autoscaling/v2\nkind: HorizontalPodAutoscaler\nmetadata:\n   name: nginx-hpa\nspec:\n   scaleTargetRef:\n     apiVersion: apps/v1\n     kind: Deployment\n     name: nginx-deployment\n   minReplicas: 1\n   maxReplicas: 5\n   metrics:\n   - type: Resource\n     resource:\n       name: cpu\n       target:\n         type: Utilization\n         averageUtilization: 80\n```\n\nI showed you the basic usage of HPA using only CPU metrics. But you can also use memory usage instead. For that, you only need to change **name:** to **memory** in your HPA YAML definition. \n\n### Custom Metrics\n\nHPA can also be configured using custom metrics. For example, take the average HTTP response time. For that, HPA offers either pod-related metrics or so-called object metrics, which can be related to anything other than pods, like networking. \n\nLet's see an example. If your application exposes a metric called \"orders-pending,\" we can use the AverageValue of that metric to configure HPA as follows: \n\n```yaml\napiVersion: autoscaling/v2\nkind: HorizontalPodAutoscaler\nmetadata:\n   name: nginx-hpa-custom\nspec:\n   scaleTargetRef:\n     apiVersion: apps/v1\n     kind: Deployment\n     name: nginx-deployment\n   minReplicas: 1\n   maxReplicas: 5\n   metrics:\n   - type: Pods\n     pods:\n       metric:\n         name: orders-pending\n       target:\n         type: AverageValue\n         averageValue: 10\n```\n\nCustom metrics are quite a broad topic and will depend on your use case. For more information on custom metrics, you can refer to Kubernetes documentation [here](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/#autoscaling-on-multiple-metrics-and-custom-metrics). \n\n### Summary\n\nAs you can see, HPA is relatively easy to set up. It only takes one kubectl command or a few lines of YAML file, and you can get even more benefits by instructing it to monitor custom metrics from your application. Clearly, HPA is a very nice thing to know. It will make your Kubernetes cluster even more self-sustainable and will offload you from repetitive tasks. \n\nIf you want to learn more about Kubernetes, feel free to take a look at [our blog here](https://release.com/blog).\n",
          "code": "var Component=(()=>{var u=Object.create;var s=Object.defineProperty;var h=Object.getOwnPropertyDescriptor;var p=Object.getOwnPropertyNames;var m=Object.getPrototypeOf,g=Object.prototype.hasOwnProperty;var y=(t,e)=>()=>(e||t((e={exports:{}}).exports,e),e.exports),f=(t,e)=>{for(var o in e)s(t,o,{get:e[o],enumerable:!0})},r=(t,e,o,i)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let a of p(e))!g.call(t,a)&&a!==o&&s(t,a,{get:()=>e[a],enumerable:!(i=h(e,a))||i.enumerable});return t};var b=(t,e,o)=>(o=t!=null?u(m(t)):{},r(e||!t||!t.__esModule?s(o,\"default\",{value:t,enumerable:!0}):o,t)),w=t=>r(s({},\"__esModule\",{value:!0}),t);var c=y((H,l)=>{l.exports=_jsx_runtime});var v={};f(v,{default:()=>P,frontmatter:()=>k});var n=b(c()),k={title:\"A Guide to Configuring and Deploying HPA on Kubernetes\",summary:\"HPA in Kubernetes will make your Kubernetes cluster more self-sustainable and will offload you from repetitive tasks.\",publishDate:\"Wed Sep 14 2022 16:07:00 GMT+0000 (Coordinated Universal Time)\",author:\"ashley-penney\",readingTime:5,categories:[\"kubernetes\",\"platform-engineering\"],mainImage:\"/blog-images/6939a8efb4225dcbacab89ad2bf333d3.jpg\",imageAlt:\"a row of computer screens with a glass in front of them\",showCTA:!0,ctaCopy:\"Automate scaling with Release's environments like HPA on Kubernetes for efficient resource management.\",ctaLink:\"https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=a-guide-to-configuring-and-deploying-hpa-on-kubernetes\",relatedPosts:[\"\"],ogImage:\"/blog-images/6939a8efb4225dcbacab89ad2bf333d3.jpg\",excerpt:\"HPA in Kubernetes will make your Kubernetes cluster more self-sustainable and will offload you from repetitive tasks.\",tags:[\"kubernetes\",\"platform-engineering\"],ctaButton:\"Try Release for Free\"};function d(t){let e=Object.assign({p:\"p\",h3:\"h3\",a:\"a\",span:\"span\",img:\"img\",pre:\"pre\",code:\"code\",strong:\"strong\"},t.components);return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(e.p,{children:\"One of the main advantages of Kubernetes is that it can take care of your containers for you. This means, for example, that it will move containers around to distribute the load on the cluster evenly, automatically restart failed pods, and kill those that misbehave and try to eat too many resources. Another nice feature is Horizontal Pod Autoscaler. As the name suggests, HPA can automatically scale your pods. But how? I'm glad you asked, because that's exactly what this post is about.\\xA0\"}),`\n`,(0,n.jsxs)(e.h3,{id:\"what-is-hpa-in-kubernetes\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#what-is-hpa-in-kubernetes\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"What Is HPA in Kubernetes?\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Normally when you create a deployment in Kubernetes, you need to specify how many pods you want to run. This number is static. Therefore, every time you want to increase or decrease the number of pods, you need to edit the deployment.\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:\"If you only need to do that once or twice a year, it's not that big of a deal. But it's very unlikely that your traffic will be at the same exact level for the whole year. And the more spikes of traffic you have, the more time you'll have to spend editing your deployment to cope with the traffic. This also works the other way around: If you're running a very big cluster with lots of applications, you could save a lot of money by decreasing the number of replicas for each deployment during periods of less traffic, like during the night. But again, it would be a lot of work to make these adjustments manually all the time. And that brings us to HPA.\\xA0\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"The main purpose of HPA is to automatically scale your deployments based on the load to match the demand. \",(0,n.jsx)(e.a,{href:\"https://en.wikipedia.org/wiki/Autoscaling#Kubernetes_Horizontal_Pod_Autoscaler:~:text=3%5D%5B33%5D-,Kubernetes%20Horizontal%20Pod%20Autoscaler,-%5Bedit%5D\",children:\"Horizontal\"}),\", in this case, means that we're talking about scaling the number of pods. You can specify the minimum and the maximum number of pods per deployment and a condition such as CPU or memory usage. Kubernetes will constantly monitor your deployment, and based on the condition you specified, it will increase or decrease the number of pods accordingly.\\xA0\"]}),`\n`,(0,n.jsx)(e.p,{children:\"\\u200D\"}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/e9b0bdf47e53fda5243b490cba26a58f.png\",alt:\"\"})}),`\n`,(0,n.jsxs)(e.h3,{id:\"what-is-vpa-in-kubernetes\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#what-is-vpa-in-kubernetes\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"What Is VPA in Kubernetes?\"]}),`\n`,(0,n.jsx)(e.p,{children:\"In Kubernetes, there is also a Vertical Pod Autoscaler (VPA). As you may guess by the name, it works contrary to Horizontal Pod Autoscaler. Instead of adjusting the number of pods up or down, as HPA does, VPA scales up or down the resource requests and limits for the pods.\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:\"So, in theory, VPA tries to achieve the same thing that HPA does, but in practice, they serve very different purposes, and you shouldn\\u2019t use them interchangeably. But to understand the difference, we need to take a step back and talk about how autoscaler knows when to scale your pods in the first place.\\xA0\"}),`\n`,(0,n.jsxs)(e.h3,{id:\"a-few-words-about-resource-requests\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#a-few-words-about-resource-requests\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"A Few Words About Resource Requests\"]}),`\n`,(0,n.jsx)(e.p,{children:\"We mentioned before that you need to provide a condition for HPA, such as CPU usage. So, for example, you can specify that you want your HPA to add an additional pod to the deployment when current pods have average CPU usage higher than 80%. But what does 80% mean? 80% of what? That's a very good question, and the answer will help you understand the main difference between HPA and VPA.\\xA0\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"You see, if you create a very basic Kubernetes deployment just by specifying its name and which docker image to use, you won't be able to add HPA to it. Why is that? It's because you didn't specify resource requests and limits for it. Long story short, specifying resource requests and limits for your \",(0,n.jsx)(e.a,{href:\"https://release.com/blog/kubernetes-pods-advanced-concepts-explained\",children:\"pods\"}),` isn't just a good practice, but also drastically helps Kubernetes do its job more efficiently. And that brings us back to the question of \"What does 80% usage mean?\" when setting up HPA thresholds. This percentage value is related to the pods' resource requests. And that's how HPA knows when to scale your pods up or down.\\xA0`]}),`\n`,(0,n.jsx)(e.p,{children:\"If you say that your application is using around 2GB of RAM under normal load, you set the resource requests accordingly: for example, to 2.5GB (you should always set the request to a little bit more than average). Then your HPA will know that it needs to schedule an additional pod for your deployment when the current one is using more than ~2.4GB (this value will depend on the target value that you specify when creating your HPA).\\xA0\"}),`\n`,(0,n.jsxs)(e.h3,{id:\"figuring-correct-values-for-resource-requests\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#figuring-correct-values-for-resource-requests\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Figuring Correct Values for Resource Requests\"]}),`\n`,(0,n.jsx)(e.p,{children:\"But how do you know what resource requests to set in the first place? You could, of course, run your pods without requests first and check how many resources they normally use. But that's quite a time-consuming process, especially on big clusters with multiple applications.\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:`That brings us back to the VPA. You see, the point of VPA isn't to scale your deployments up or down in order to keep up with sudden spikes in traffic. That's the HPA's job. VPA should be used to get you a good baseline of resource requests and limits for your pods. This way, you're free from that time-consuming task of monitoring pods' typical usage and setting requests and limits accordingly. When the traffic goes up and your pods can't keep up, the HPA should add one or more pods to the pack to get resource usage back to the \"average.\" At that point, VPA doesn't need to do anything.\\xA0`}),`\n`,(0,n.jsx)(e.p,{children:\"You can think of it as VPA working much slower and more long-term, looking at patterns of usage, while HPA responds quicker and provides short-term solutions for load spikes.\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:\"\\u200D\"}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/ea9f60b7a898e8d43fd9c3f87c85cf72.png\",alt:\"\"})}),`\n`,(0,n.jsxs)(e.h3,{id:\"how-do-i-configure-hpa-in-kubernetes\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#how-do-i-configure-hpa-in-kubernetes\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"How Do I Configure HPA in Kubernetes?\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Now that you know all the theory, let's create some HPAs. We'll start with creating an example deployment with a resource request set:\\xA0\"}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-yaml\",children:`apiVersion: v1\nkind: Service\nmetadata:\n  \\xA0name: nginx-svc\n  \\xA0labels:\n  \\xA0 \\xA0app: nginx\nspec:\n  \\xA0ports:\n  \\xA0- port: 80\n  \\xA0selector:\n  \\xA0 \\xA0app: nginx\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  \\xA0name: nginx-deployment\n  \\xA0labels:\n  \\xA0 \\xA0app: nginx\nspec:\n  \\xA0replicas: 1\n  \\xA0selector:\n  \\xA0 \\xA0matchLabels:\n  \\xA0 \\xA0 \\xA0app: nginx\n  \\xA0template:\n  \\xA0 \\xA0metadata:\n  \\xA0 \\xA0 \\xA0labels:\n  \\xA0 \\xA0 \\xA0 \\xA0app: nginx\n  \\xA0 \\xA0spec:\n  \\xA0 \\xA0 \\xA0containers:\n  \\xA0 \\xA0 \\xA0- name: nginx\n  \\xA0 \\xA0 \\xA0 \\xA0image: nginx\n  \\xA0 \\xA0 \\xA0 \\xA0ports:\n  \\xA0 \\xA0 \\xA0 \\xA0- containerPort: 80\n  \\xA0 \\xA0 \\xA0 \\xA0resources:\n  \\xA0 \\xA0 \\xA0 \\xA0 \\xA0requests:\n  \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0memory: \"64Mi\"\n  \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0cpu: \"100m\"\n`})}),`\n`,(0,n.jsxs)(e.p,{children:[\"I'll save it as nginx.yaml and apply with \",(0,n.jsx)(e.strong,{children:\"kubectl apply\"}),\":\\xA0\"]}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-yaml\",children:`$ kubectl apply -f nginx.yaml\ndeployment.apps/nginx-deployment created\n\n$ kubectl get deploy\nNAME \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 READY \\xA0 UP-TO-DATE \\xA0 AVAILABLE \\xA0 AGE\nnginx-deployment \\xA0 1/1 \\xA0 \\xA0 1 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA01 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 115s\n`})}),`\n`,(0,n.jsxs)(e.p,{children:[\"Our deployment is up and running, so we can now add HPA to it. Just like anything in Kubernetes, you can add HPA by creating and applying the YAML definition, just like we did with this deployment. Another option is to use the \",(0,n.jsx)(e.strong,{children:\"kubectl autoscale\"}),\" command. Let's start with the latter. To create HPA with \",(0,n.jsx)(e.strong,{children:\"kubectl\"}),\" \",(0,n.jsx)(e.strong,{children:\"autoscale,\"}),\" you need to execute the following command:\\xA0\"]}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-yaml\",children:`$ kubectl autoscale deploy nginx-deployment --min=1 --max=5 --cpu-percent=80\nhorizontalpodautoscaler.autoscaling/nginx-deployment autoscaled\n`})}),`\n`,(0,n.jsxs)(e.p,{children:[\"After \",(0,n.jsx)(e.strong,{children:\"kubectl autoscale,\"}),\" we need to specify the resource type we want to autoscale. In our case, it's \",(0,n.jsx)(e.strong,{children:\"deploy\"}),\" (short for deployment), and then we specify the deployment name. After that, we pass the minimum and maximum amount of pods HPA can create and the condition on which to scale.\\xA0\"]}),`\n`,(0,n.jsxs)(e.h3,{id:\"validating-if-hpa-works\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#validating-if-hpa-works\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Validating If HPA Works\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"In our example, we tell HPA to scale out pods when their CPU usage goes over 80%. And since in our deployment earlier we specified CPU requests of 100m, this means our HPA will start scaling out nginx-deployment when its average CPU usage goes over 80m. Let's validate that. First, we double-check if HPA is working using \",(0,n.jsx)(e.strong,{children:\"kubectl get hpa:\"}),\"\\xA0\"]}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-yaml\",children:`$ kubectl get hpa\nNAME \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 REFERENCE \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 TARGETS \\xA0 MINPODS \\xA0 MAXPODS \\xA0 REPLICAS \\xA0 AGE\nnginx-deployment \\xA0 Deployment/nginx-deployment \\xA0 0%/80% \\xA0 \\xA01 \\xA0 \\xA0 \\xA0 \\xA0 5 \\xA0 \\xA0 \\xA0 \\xA0 1 \\xA0 \\xA0 \\xA0 \\xA0 \\xA054s\n`})}),`\n`,(0,n.jsx)(e.p,{children:\"We can see it's working, and our nginx deployment is currently running one pod. There is no traffic on it, so it makes sense. But let's see if HPA will do its job when we put some traffic on nginx. For that, I'll deploy a simple load generator:\\xA0\"}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-yaml\",children:`\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n \\xA0name: load-generator\n \\xA0labels:\n \\xA0 \\xA0app: load\nspec:\n \\xA0replicas: 3\n \\xA0selector:\n \\xA0 \\xA0matchLabels:\n \\xA0 \\xA0 \\xA0app: load\n \\xA0template:\n \\xA0 \\xA0metadata:\n \\xA0 \\xA0 \\xA0labels:\n \\xA0 \\xA0 \\xA0 \\xA0app: load\n \\xA0 \\xA0spec:\n \\xA0 \\xA0 \\xA0containers:\n \\xA0 \\xA0 \\xA0- command:\n \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0- \"/bin/sh\"\n \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0- \"-c\"\n \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0- \"while true; do wget -q -O /dev/null nginx-svc; done\"\n \\xA0 \\xA0 \\xA0 \\xA0name: load\n \\xA0 \\xA0 \\xA0 \\xA0image: busybox\n\n`})}),`\n`,(0,n.jsxs)(e.p,{children:[\"Once deployed (using \",(0,n.jsx)(e.strong,{children:\"kubectl apply -f load-generator.yaml\"}),\") we can monitor your nginx deployment CPU usage with \",(0,n.jsx)(e.strong,{children:\"kubectl top pods:\"}),\"\\xA0\"]}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-yaml\",children:`$ kubectl top pods\nNAME \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 CPU(cores) \\xA0 MEMORY(bytes)\nnginx-deployment-bc54c744b-tlpds \\xA0 112m \\xA0 \\xA0 \\xA0 \\xA0 4Mi\n`})}),`\n`,(0,n.jsx)(e.p,{children:\"And when we see the CPU usage goes over 80m, we can check the status of HPA again:\\xA0\"}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-yaml\",children:`$ \\xA0~ kubectl get hpa\nNAME \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 REFERENCE \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 TARGETS \\xA0 \\xA0MINPODS \\xA0 MAXPODS \\xA0 REPLICAS \\xA0 AGE\nnginx-deployment \\xA0 Deployment/nginx-deployment \\xA0 138%/80% \\xA0 1 \\xA0 \\xA0 \\xA0 \\xA0 5 \\xA0 \\xA0 \\xA0 \\xA0 2 \\xA0 \\xA0 \\xA0 \\xA0 \\xA016m\n`})}),`\n`,(0,n.jsx)(e.p,{children:\"And shortly after that, the target CPU usage should also drop (since the traffic is now distributed to two pods):\\xA0\"}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-yaml\",children:`$ kubectl get hpa\nNAME \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 REFERENCE \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 TARGETS \\xA0 MINPODS \\xA0 MAXPODS \\xA0 REPLICAS \\xA0 AGE\nnginx-deployment \\xA0 Deployment/nginx-deployment \\xA0 59%/80% \\xA0 1 \\xA0 \\xA0 \\xA0 \\xA0 5 \\xA0 \\xA0 \\xA0 \\xA0 2 \\xA0 \\xA0 \\xA0 \\xA0 \\xA020m\n\n$ kubectl top pods\nNAME \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 CPU(cores) \\xA0 MEMORY(bytes)\nnginx-deployment-bc54c744b-qshtz \\xA0 52m \\xA0 \\xA0 \\xA0 \\xA0 \\xA03Mi\nnginx-deployment-bc54c744b-tlpds \\xA0 59m \\xA0 \\xA0 \\xA0 \\xA0 \\xA03Mi\n`})}),`\n`,(0,n.jsx)(e.p,{children:\"So, HPA is working as expected. It increased the number of pods for our deployment based on the load. Just keep in mind that HPA doesn't work instantly. It usually takes a few seconds before it will take any action, just to avoid unnecessary scaling actions on very short load spikes.\\xA0\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"You now know how to create HPA with \",(0,n.jsx)(e.strong,{children:\"kubectl,\"}),\" and this is how you do the same with YAML definition:\\xA0\"]}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-yaml\",children:`apiVersion: autoscaling/v2\nkind: HorizontalPodAutoscaler\nmetadata:\n  \\xA0name: nginx-hpa\nspec:\n  \\xA0scaleTargetRef:\n  \\xA0 \\xA0apiVersion: apps/v1\n  \\xA0 \\xA0kind: Deployment\n  \\xA0 \\xA0name: nginx-deployment\n  \\xA0minReplicas: 1\n  \\xA0maxReplicas: 5\n  \\xA0metrics:\n  \\xA0- type: Resource\n  \\xA0 \\xA0resource:\n  \\xA0 \\xA0 \\xA0name: cpu\n  \\xA0 \\xA0 \\xA0target:\n  \\xA0 \\xA0 \\xA0 \\xA0type: Utilization\n  \\xA0 \\xA0 \\xA0 \\xA0averageUtilization: 80\n`})}),`\n`,(0,n.jsxs)(e.p,{children:[\"I showed you the basic usage of HPA using only CPU metrics. But you can also use memory usage instead. For that, you only need to change \",(0,n.jsx)(e.strong,{children:\"name:\"}),\" to \",(0,n.jsx)(e.strong,{children:\"memory\"}),\" in your HPA YAML definition.\\xA0\"]}),`\n`,(0,n.jsxs)(e.h3,{id:\"custom-metrics\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#custom-metrics\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Custom Metrics\"]}),`\n`,(0,n.jsx)(e.p,{children:\"HPA can also be configured using custom metrics. For example, take the average HTTP response time. For that, HPA offers either pod-related metrics or so-called object metrics, which can be related to anything other than pods, like networking.\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:`Let's see an example. If your application exposes a metric called \"orders-pending,\" we can use the AverageValue of that metric to configure HPA as follows:\\xA0`}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-yaml\",children:`apiVersion: autoscaling/v2\nkind: HorizontalPodAutoscaler\nmetadata:\n  \\xA0name: nginx-hpa-custom\nspec:\n  \\xA0scaleTargetRef:\n  \\xA0 \\xA0apiVersion: apps/v1\n  \\xA0 \\xA0kind: Deployment\n  \\xA0 \\xA0name: nginx-deployment\n  \\xA0minReplicas: 1\n  \\xA0maxReplicas: 5\n  \\xA0metrics:\n  \\xA0- type: Pods\n  \\xA0 \\xA0pods:\n  \\xA0 \\xA0 \\xA0metric:\n  \\xA0 \\xA0 \\xA0 \\xA0name: orders-pending\n  \\xA0 \\xA0 \\xA0target:\n  \\xA0 \\xA0 \\xA0 \\xA0type: AverageValue\n  \\xA0 \\xA0 \\xA0 \\xA0averageValue: 10\n`})}),`\n`,(0,n.jsxs)(e.p,{children:[\"Custom metrics are quite a broad topic and will depend on your use case. For more information on custom metrics, you can refer to Kubernetes documentation \",(0,n.jsx)(e.a,{href:\"https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/#autoscaling-on-multiple-metrics-and-custom-metrics\",children:\"here\"}),\".\\xA0\"]}),`\n`,(0,n.jsxs)(e.h3,{id:\"summary\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#summary\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Summary\"]}),`\n`,(0,n.jsx)(e.p,{children:\"As you can see, HPA is relatively easy to set up. It only takes one kubectl command or a few lines of YAML file, and you can get even more benefits by instructing it to monitor custom metrics from your application. Clearly, HPA is a very nice thing to know. It will make your Kubernetes cluster even more self-sustainable and will offload you from repetitive tasks.\\xA0\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"If you want to learn more about Kubernetes, feel free to take a look at \",(0,n.jsx)(e.a,{href:\"https://release.com/blog\",children:\"our blog here\"}),\".\"]})]})}function A(t={}){let{wrapper:e}=t.components||{};return e?(0,n.jsx)(e,Object.assign({},t,{children:(0,n.jsx)(d,t)})):d(t)}var P=A;return w(v);})();\n;return Component;"
        },
        "_id": "blog/posts/a-guide-to-configuring-and-deploying-hpa-on-kubernetes.mdx",
        "_raw": {
          "sourceFilePath": "blog/posts/a-guide-to-configuring-and-deploying-hpa-on-kubernetes.mdx",
          "sourceFileName": "a-guide-to-configuring-and-deploying-hpa-on-kubernetes.mdx",
          "sourceFileDir": "blog/posts",
          "contentType": "mdx",
          "flattenedPath": "blog/posts/a-guide-to-configuring-and-deploying-hpa-on-kubernetes"
        },
        "type": "BlogPost",
        "computedSlug": "a-guide-to-configuring-and-deploying-hpa-on-kubernetes"
      },
      "documentHash": "1739393595015",
      "hasWarnings": false,
      "documentTypeName": "BlogPost"
    },
    "blog/posts/a-managers-guide-to-release-cycles.mdx": {
      "document": {
        "title": "A Manager's Guide to Release Cycles",
        "summary": "You will learn what a release cycle is and a manager's role within it. Receive some tips on how to manage a release.",
        "publishDate": "Tue Sep 20 2022 19:35:03 GMT+0000 (Coordinated Universal Time)",
        "author": "regis-wilson",
        "readingTime": 3,
        "categories": [
          "platform-engineering",
          "product"
        ],
        "mainImage": "/blog-images/713dfb4e7bd1426cfaf0edc7f377084e.jpg",
        "imageAlt": "A Manager's Guide to Release Cycles",
        "showCTA": true,
        "ctaCopy": "Simplify release cycles with Release's on-demand environments for seamless collaboration and efficient testing.",
        "ctaLink": "https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=a-managers-guide-to-release-cycles",
        "relatedPosts": [
          ""
        ],
        "ogImage": "/blog-images/713dfb4e7bd1426cfaf0edc7f377084e.jpg",
        "excerpt": "You will learn what a release cycle is and a manager's role within it. Receive some tips on how to manage a release.",
        "tags": [
          "platform-engineering",
          "product"
        ],
        "ctaButton": "Try Release for Free",
        "body": {
          "raw": "\nIf you're a project manager, engineering manager, or executive (like a CTO), you're probably involved in a lot of tedious planning and meetings in the lead-up to a new product release—if you're a new manager in your company or team, the lead-up is even more overwhelming.\n\nThe formula for your success as a manager doesn't rely solely on your tech knowledge or skills. It also relies on your ability to lead, plan, and release the right product at the right time.\n\nKnowledge of the product release cycle and your role as manager in it will help you get a handle on this key process. Read on for some tips on how to measure and manage a [release](https://en.wikipedia.org/wiki/Software_release_life_cycle) based on some best practices.\n\n### What is a Release Cycle?\n\nA **release cycle**, also called **release management**, is the process of planning, scheduling, managing, and controlling the progress of a software build through the various stages of development to the deployment of the product.\n\nRelease management requires some important skills, like technical knowledge of the software requirements and a firm grasp of risk management because at each stage of the cycle, you're placing the reputation of your organization on the line. You'll also want to master stakeholder engagement so that you can keep stakeholders informed and coordinate collaboration.\n\n![](/blog-images/8b070b22e8bd91c9b9c86f3ef1b832b5.png)\n\n#### What is a Software Release Life Cycle?\n\nA software release life cycle (SRLC) is the sum of the stages of software development from its initial conception phase to its release. This includes the final, updated version of the release version to improve the software or fix bugs still present in the software. The cycle includes: \n\n- **Pre-alpha phase:** In this stage, developers are building the software but have not formally started testing it yet.\n- **Alpha phase:** Here, developers begin formal testing of the software by doing white-box testing like unit testing or integration testing.\n- **Beta phase:** The beta stage comes after the requirements in the alpha phase of the software are completed. In this stage, users test the software, and user acceptance testing can be done. The beta test can be closed (limited to a certain number of users) or open to anyone (publicly available).\n- **Release candidate stage:** This stage is an iteration on the beta phase, fixing or improving the software based on issues discovered in beta. Beta testing is carried out to confirm that the software works correctly.\n- **Production or stable stage:** This is the final stage of software development, when the software is released to the market for the users. This version of the software is usually stable and free from crashes or system failures.\n\n#### What is a Manager's Role in a Release Cycle?\n\nThe release cycle includes numerous activities, and as manager, you need to make sure that everything goes as planned and the goals are achieved. One of your key responsibilities during a release cycle is effectively communicating with stakeholders and getting updates on the progress of the project. You also need to do the following:\n\n- Assess the risks involved in carrying out the project, or identifying the pitfalls.\n- Make scheduling decisions based on the progress of the project.\n- Assign tasks to stakeholders or developers involved in building the project.\n- Remove blocking issues that may hinder the success or progress of the project.\n- Run regular team meetings to get updates on the project.\n- Update project status and progress so things stay on track.\n\n![](/blog-images/67f67521f96ea7b841a9b431a189a114.png)\n\n#### What are the Steps in a Release Cycle?\n\nTo successfully manage the release cycle of your project, you'll want to time and schedule activities properly. You'll need to guide, collaborate, and communicate with your team members and colleagues at every stage of the release cycle to ensure that you release effectively and on time.\n\nLet's take a look at those stages below.\n\n##### Specification Stage\n\nIn the specification stage of the release cycle, the software specifications are laid out and the requirements to build it are planned. The manager ensures that the requirements and specifications for the software are achievable and executable.\n\n##### Development Stage\n\nThe development or execution stage of the release cycle involves the developers and other stakeholders responsible for building the software engaged in developing the product according to the specifications and requirements laid out in the specification stage.\n\n##### Testing Stage\n\nThe testing stage follows the successful build of the software or application, when a series of manual or automated testing is performed on the product. Some of this testing includes:\n\n- **Unit testing:** Testing different units or functional components of the software or application.\n- **Integration testing:** Testing the different integrations used across the software.\n- **System testing:** Testing the entire system for bugs or vulnerabilities.\n- **User acceptance testing:** To measure users' responses to the software, and how they interact with it.\n\n##### Sign-off Stage\n\nIn the sign-off stage, the manager performs a final set of checks. Once the software has passed through the previous stages of the release cycle to make sure it's ready for deployment or rollout to its end users, the manager can ensure the users are comfortable with how it works.\n\n##### Deployment Stage\n\nThis is the final stage of the release cycle. At this point, the finished product should be ready for the end users at the scheduled time. Also, this last phase should be free of issues to prevent production rollback, which can ‌lead to a terrible impression of the company—and ultimately, the manager.\n\n![](/blog-images/5f0316f352b15acc569134e0de4d6674.png)\n\n#### How to Measure a Release Cycle\n\nJust knowing the stages in the software release life cycle isn't enough. As a manager, it's also important that you know how to manage the release cycle and measure the success of each stage. Here are some tips on how to measure and manage your release cycle.\n\n#### Tips and Best Practices for Release Cycles\n\n- **Set realistic goals and KPIs.** Be truthful when setting goals and KPIs (Key Performance Indicators) for you and your teams during a release cycle so that you're guided by the deadlines you have set, and how much work you can do. Use these goals and KPIs to track what the stakeholders expect, and what you can accomplish on the project.**‍**\n- **Track and record all progress.** Enlist someone to help you track the daily or weekly progress of the project to ensure that you don't miss important events during the release cycle and ease your stress.**‍**\n- **Use a version control system.** In software development, version control systems don't only help in organizing the whole product development. They also serve as a source of truth when issues or conflicting errors arise. Some popular version control tools include Git, CVS, SVN, and Mercurial.**‍**\n- **Decide on framework adoption.** Consider adopting a management framework that fits your team to make releases more frequent and easy, so that you can deliver value to your organization faster. Consider breaking your workflow into sprints.**‍**\n- **Decide on testing.** Think about testing early in the process, and test your software as part of a routine throughout the lifecycle.**‍**\n- **Don't forget about culture.** Create a culture that encourages and invites others in the team to collaborate and make genuine reviews of the product release.**‍**\n- **Be communicative.** Communicate with your team members and stakeholders regularly about release sprints and schedules.**‍**\n- **Use productivity software.** Use productivity software to integrate your workflow and track the progress of the release.**‍**\n- **Limit post-production changes.** Avoid pushing changes directly to production, especially changes that haven't been reviewed or tested, as this can affect the business if something goes wrong.**‍**\n- **Have a rollback strategy to protect your organization.** This will help you roll back your software without affecting the user experience or the business.\n\n#### Conclusion\n\nHow disciplined, knowledgeable, and collaborative you are as a manager determines the success of your team or company. Releasing a product to the market takes a lot of effort and commitment and every release cycle comes with its share of difficulties, but a good manager knows when and how to handle these challenges. If you want to know more about how you can get an on demand environment for development, staging and production, check out [Release](https://release.com/).\n",
          "code": "var Component=(()=>{var d=Object.create;var o=Object.defineProperty;var g=Object.getOwnPropertyDescriptor;var u=Object.getOwnPropertyNames;var m=Object.getPrototypeOf,p=Object.prototype.hasOwnProperty;var f=(a,e)=>()=>(e||a((e={exports:{}}).exports,e),e.exports),y=(a,e)=>{for(var n in e)o(a,n,{get:e[n],enumerable:!0})},i=(a,e,n,r)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let s of u(e))!p.call(a,s)&&s!==n&&o(a,s,{get:()=>e[s],enumerable:!(r=g(e,s))||r.enumerable});return a};var w=(a,e,n)=>(n=a!=null?d(m(a)):{},i(e||!a||!a.__esModule?o(n,\"default\",{value:a,enumerable:!0}):n,a)),b=a=>i(o({},\"__esModule\",{value:!0}),a);var c=f((R,l)=>{l.exports=_jsx_runtime});var N={};y(N,{default:()=>T,frontmatter:()=>k});var t=w(c()),k={title:\"A Manager's Guide to Release Cycles\",summary:\"You will learn what a release cycle is and a manager's role within it. Receive some tips on how to manage a release.\",publishDate:\"Tue Sep 20 2022 19:35:03 GMT+0000 (Coordinated Universal Time)\",author:\"regis-wilson\",readingTime:3,categories:[\"platform-engineering\",\"product\"],mainImage:\"/blog-images/713dfb4e7bd1426cfaf0edc7f377084e.jpg\",imageAlt:\"A Manager's Guide to Release Cycles\",showCTA:!0,ctaCopy:\"Simplify release cycles with Release's on-demand environments for seamless collaboration and efficient testing.\",ctaLink:\"https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=a-managers-guide-to-release-cycles\",relatedPosts:[\"\"],ogImage:\"/blog-images/713dfb4e7bd1426cfaf0edc7f377084e.jpg\",excerpt:\"You will learn what a release cycle is and a manager's role within it. Receive some tips on how to manage a release.\",tags:[\"platform-engineering\",\"product\"],ctaButton:\"Try Release for Free\"};function h(a){let e=Object.assign({p:\"p\",a:\"a\",h3:\"h3\",span:\"span\",strong:\"strong\",img:\"img\",h4:\"h4\",ul:\"ul\",li:\"li\",h5:\"h5\"},a.components);return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(e.p,{children:\"If you're a project manager, engineering manager, or executive (like a CTO), you're probably involved in a lot of tedious planning and meetings in the lead-up to a new product release\\u2014if you're a new manager in your company or team, the lead-up is even more overwhelming.\"}),`\n`,(0,t.jsx)(e.p,{children:\"The formula for your success as a manager doesn't rely solely on your tech knowledge or skills. It also relies on your ability to lead, plan, and release the right product at the right time.\"}),`\n`,(0,t.jsxs)(e.p,{children:[\"Knowledge of the product release cycle and your role as manager in it will help you get a handle on this key process. Read on for some tips on how to measure and manage a \",(0,t.jsx)(e.a,{href:\"https://en.wikipedia.org/wiki/Software_release_life_cycle\",children:\"release\"}),\" based on some best practices.\"]}),`\n`,(0,t.jsxs)(e.h3,{id:\"what-is-a-release-cycle\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#what-is-a-release-cycle\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"What is a Release Cycle?\"]}),`\n`,(0,t.jsxs)(e.p,{children:[\"A \",(0,t.jsx)(e.strong,{children:\"release cycle\"}),\", also called \",(0,t.jsx)(e.strong,{children:\"release management\"}),\", is the process of planning, scheduling, managing, and controlling the progress of a software build through the various stages of development to the deployment of the product.\"]}),`\n`,(0,t.jsx)(e.p,{children:\"Release management requires some important skills, like technical knowledge of the software requirements and a firm grasp of risk management because at each stage of the cycle, you're placing the reputation of your organization on the line. You'll also want to master stakeholder engagement\\xA0so that you can keep stakeholders informed and coordinate collaboration.\"}),`\n`,(0,t.jsx)(e.p,{children:(0,t.jsx)(e.img,{src:\"/blog-images/8b070b22e8bd91c9b9c86f3ef1b832b5.png\",alt:\"\"})}),`\n`,(0,t.jsxs)(e.h4,{id:\"what-is-a-software-release-life-cycle\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#what-is-a-software-release-life-cycle\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"What is a Software Release Life Cycle?\"]}),`\n`,(0,t.jsx)(e.p,{children:\"A software release life cycle (SRLC) is the sum of the stages of software development from its initial conception phase to its release. This includes the final, updated version of the release version to improve the software or fix bugs still present in the software. The cycle includes:\\xA0\"}),`\n`,(0,t.jsxs)(e.ul,{children:[`\n`,(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:\"Pre-alpha phase:\"}),\" In this stage, developers are building the software but have not formally started testing it yet.\"]}),`\n`,(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:\"Alpha phase:\"}),\" Here, developers begin formal testing of the software by doing white-box testing like unit testing or integration testing.\"]}),`\n`,(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:\"Beta phase:\"}),\" The beta stage comes after the requirements in the alpha phase of the software are completed. In this stage, users test the software, and user acceptance testing can be done. The beta test can be closed (limited to a certain number of users) or open to anyone (publicly available).\"]}),`\n`,(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:\"Release candidate stage:\"}),\" This stage is an iteration on the beta phase, fixing or improving the software based on issues discovered in beta. Beta testing is carried out to confirm that the software works correctly.\"]}),`\n`,(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:\"Production or stable stage:\"}),\" This is the final stage of software development, when the software is released to the market for the users. This version of the software is usually stable and free from crashes or system failures.\"]}),`\n`]}),`\n`,(0,t.jsxs)(e.h4,{id:\"what-is-a-managers-role-in-a-release-cycle\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#what-is-a-managers-role-in-a-release-cycle\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"What is a Manager's Role in a Release Cycle?\"]}),`\n`,(0,t.jsx)(e.p,{children:\"The release cycle includes numerous activities, and as manager, you need to make sure that everything goes as planned and the goals are achieved. One of your key responsibilities during a release cycle is effectively communicating with stakeholders and getting updates on the progress of the project. You also need to do the following:\"}),`\n`,(0,t.jsxs)(e.ul,{children:[`\n`,(0,t.jsx)(e.li,{children:\"Assess the risks involved in carrying out the project, or identifying the pitfalls.\"}),`\n`,(0,t.jsx)(e.li,{children:\"Make scheduling decisions based on the progress of the project.\"}),`\n`,(0,t.jsx)(e.li,{children:\"Assign tasks to stakeholders or developers involved in building the project.\"}),`\n`,(0,t.jsx)(e.li,{children:\"Remove blocking issues that may hinder the success or progress of the project.\"}),`\n`,(0,t.jsx)(e.li,{children:\"Run regular team meetings to get updates on the project.\"}),`\n`,(0,t.jsx)(e.li,{children:\"Update project status and progress so things stay on track.\"}),`\n`]}),`\n`,(0,t.jsx)(e.p,{children:(0,t.jsx)(e.img,{src:\"/blog-images/67f67521f96ea7b841a9b431a189a114.png\",alt:\"\"})}),`\n`,(0,t.jsxs)(e.h4,{id:\"what-are-the-steps-in-a-release-cycle\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#what-are-the-steps-in-a-release-cycle\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"What are the Steps in a Release Cycle?\"]}),`\n`,(0,t.jsx)(e.p,{children:\"To successfully manage the release cycle of your project, you'll want to time and schedule activities properly. You'll need to guide, collaborate, and communicate with your team members and colleagues at every stage of the release cycle to ensure that you release effectively and on time.\"}),`\n`,(0,t.jsx)(e.p,{children:\"Let's take a look at those stages below.\"}),`\n`,(0,t.jsxs)(e.h5,{id:\"specification-stage\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#specification-stage\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Specification Stage\"]}),`\n`,(0,t.jsx)(e.p,{children:\"In the specification stage of the release cycle, the software specifications are laid out and the requirements to build it are planned. The manager ensures that the requirements and specifications for the software are achievable and executable.\"}),`\n`,(0,t.jsxs)(e.h5,{id:\"development-stage\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#development-stage\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Development Stage\"]}),`\n`,(0,t.jsx)(e.p,{children:\"The development or execution stage of the release cycle involves the developers and other stakeholders responsible for building the software engaged in developing the product according to the specifications and requirements laid out in the specification stage.\"}),`\n`,(0,t.jsxs)(e.h5,{id:\"testing-stage\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#testing-stage\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Testing Stage\"]}),`\n`,(0,t.jsx)(e.p,{children:\"The testing stage follows the successful build of the software or application, when a series of manual or automated testing is performed on the product. Some of this testing includes:\"}),`\n`,(0,t.jsxs)(e.ul,{children:[`\n`,(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:\"Unit testing:\"}),\" Testing different units or functional components of the software or application.\"]}),`\n`,(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:\"Integration testing:\"}),\" Testing the different integrations used across the software.\"]}),`\n`,(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:\"System testing:\"}),\" Testing the entire system for bugs or vulnerabilities.\"]}),`\n`,(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:\"User acceptance testing:\"}),\" To measure users' responses to the software, and how they interact with it.\"]}),`\n`]}),`\n`,(0,t.jsxs)(e.h5,{id:\"sign-off-stage\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#sign-off-stage\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Sign-off Stage\"]}),`\n`,(0,t.jsx)(e.p,{children:\"In the sign-off stage, the manager performs a final set of checks. Once the software has passed through the previous stages of the release cycle to make sure it's ready for deployment or rollout to its end users, the manager can ensure the users are comfortable with how it works.\"}),`\n`,(0,t.jsxs)(e.h5,{id:\"deployment-stage\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#deployment-stage\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Deployment Stage\"]}),`\n`,(0,t.jsx)(e.p,{children:\"This is the final stage of the release cycle. At this point, the finished product should be ready for the end users at the scheduled time. Also, this last phase should be free of issues to prevent production rollback, which can \\u200Clead to a terrible impression of the company\\u2014and ultimately, the manager.\"}),`\n`,(0,t.jsx)(e.p,{children:(0,t.jsx)(e.img,{src:\"/blog-images/5f0316f352b15acc569134e0de4d6674.png\",alt:\"\"})}),`\n`,(0,t.jsxs)(e.h4,{id:\"how-to-measure-a-release-cycle\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#how-to-measure-a-release-cycle\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"How to Measure a Release Cycle\"]}),`\n`,(0,t.jsx)(e.p,{children:\"Just knowing the stages in the software release life cycle isn't enough. As a manager, it's also important that you know how to manage the release cycle and measure the success of each stage. Here are some tips on how to measure and manage your release cycle.\"}),`\n`,(0,t.jsxs)(e.h4,{id:\"tips-and-best-practices-for-release-cycles\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#tips-and-best-practices-for-release-cycles\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Tips and Best Practices for Release Cycles\"]}),`\n`,(0,t.jsxs)(e.ul,{children:[`\n`,(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:\"Set realistic goals and KPIs.\"}),\" Be truthful when setting goals and KPIs (Key Performance Indicators) for you and your teams during a release cycle so that you're guided by the deadlines you have set, and how much work you can do. Use these goals and KPIs to track what the stakeholders expect, and what you can accomplish on the project.\",(0,t.jsx)(e.strong,{children:\"\\u200D\"})]}),`\n`,(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:\"Track and record all progress.\"}),\" Enlist someone to help you track the daily or weekly progress of the project to ensure that you don't miss important events during the release cycle and ease your stress.\",(0,t.jsx)(e.strong,{children:\"\\u200D\"})]}),`\n`,(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:\"Use a version control system.\"}),\" In software development, version control systems don't only help in organizing the whole product development. They also serve as a source of truth when issues or conflicting errors arise. Some popular version control tools include Git, CVS, SVN, and Mercurial.\",(0,t.jsx)(e.strong,{children:\"\\u200D\"})]}),`\n`,(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:\"Decide on framework adoption.\"}),\" Consider adopting a management framework that fits your team to make releases more frequent and easy, so that you can deliver value to your organization faster. Consider breaking your workflow into sprints.\",(0,t.jsx)(e.strong,{children:\"\\u200D\"})]}),`\n`,(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:\"Decide on testing.\"}),\" Think about testing early in the process, and test your software as part of a routine throughout the lifecycle.\",(0,t.jsx)(e.strong,{children:\"\\u200D\"})]}),`\n`,(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:\"Don't forget about culture.\"}),\" Create a culture that encourages and invites others in the team to collaborate and make genuine reviews of the product release.\",(0,t.jsx)(e.strong,{children:\"\\u200D\"})]}),`\n`,(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:\"Be communicative.\"}),\" Communicate with your team members and stakeholders regularly about release sprints and schedules.\",(0,t.jsx)(e.strong,{children:\"\\u200D\"})]}),`\n`,(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:\"Use productivity software.\"}),\" Use productivity software to integrate your workflow and track the progress of the release.\",(0,t.jsx)(e.strong,{children:\"\\u200D\"})]}),`\n`,(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:\"Limit post-production changes.\"}),\" Avoid pushing changes directly to production, especially changes that haven't been reviewed or tested, as this can affect the business if something goes wrong.\",(0,t.jsx)(e.strong,{children:\"\\u200D\"})]}),`\n`,(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:\"Have a rollback strategy to protect your organization.\"}),\" This will help you roll back your software without affecting the user experience or the business.\"]}),`\n`]}),`\n`,(0,t.jsxs)(e.h4,{id:\"conclusion\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#conclusion\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Conclusion\"]}),`\n`,(0,t.jsxs)(e.p,{children:[\"How disciplined, knowledgeable, and collaborative you are as a manager determines the success of your team or company. Releasing a product to the market takes a lot of effort and commitment and every release cycle comes with its share of difficulties, but a good manager knows when and how to handle these challenges. If you want to know more about how you can get an on demand environment for development, staging and production, check out \",(0,t.jsx)(e.a,{href:\"https://release.com/\",children:\"Release\"}),\".\"]})]})}function v(a={}){let{wrapper:e}=a.components||{};return e?(0,t.jsx)(e,Object.assign({},a,{children:(0,t.jsx)(h,a)})):h(a)}var T=v;return b(N);})();\n;return Component;"
        },
        "_id": "blog/posts/a-managers-guide-to-release-cycles.mdx",
        "_raw": {
          "sourceFilePath": "blog/posts/a-managers-guide-to-release-cycles.mdx",
          "sourceFileName": "a-managers-guide-to-release-cycles.mdx",
          "sourceFileDir": "blog/posts",
          "contentType": "mdx",
          "flattenedPath": "blog/posts/a-managers-guide-to-release-cycles"
        },
        "type": "BlogPost",
        "computedSlug": "a-managers-guide-to-release-cycles"
      },
      "documentHash": "1739393595015",
      "hasWarnings": false,
      "documentTypeName": "BlogPost"
    },
    "blog/posts/a-simple-guide-to-software-environments.mdx": {
      "document": {
        "title": "A Simple Guide to Software Environments",
        "summary": "Learn about tools to scope and limit instances of your software as it progresses from development to a finished product.",
        "publishDate": "Tue Apr 11 2023 21:10:39 GMT+0000 (Coordinated Universal Time)",
        "author": "alexander-fridman",
        "readingTime": 5,
        "categories": [
          "platform-engineering",
          "product"
        ],
        "mainImage": "/blog-images/b3927fc80db8ab1869298638402348e7.png",
        "imageAlt": "A Simple Guide to Software Environments",
        "showCTA": true,
        "ctaCopy": "Empower developers to streamline software environments from local to production with Release's on-demand, full-stack environments.",
        "ctaLink": "https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=a-simple-guide-to-software-environments",
        "relatedPosts": [
          ""
        ],
        "ogImage": "/blog-images/b3927fc80db8ab1869298638402348e7.png",
        "excerpt": "Learn about tools to scope and limit instances of your software as it progresses from development to a finished product.",
        "tags": [
          "platform-engineering",
          "product"
        ],
        "ctaButton": "Try Release for Free",
        "body": {
          "raw": "\n### Introduction\n\nBuilding software is hard. It takes a lot of time and effort and it’s a somewhat cumbersome process. Software development passes through various stages in the product lifecycle before it reaches the end user. A spec must be defined, developers have to code the product, QA has to test it, and DevOps engineers have to deploy it. During this process, the code passes through several environments, from the developers’ local PC to containers in the cloud. \n\n### What Are Software Environments\n\nSoftware environments are tools to scope and limit instances of your software as it progresses from development to a customer facing product. Each environment contains the following: \n\n- Copy of your software code\n- Copy of the database tables you use with (possibly) environment-specific data\n- Internal network so the different microservices your code uses can communicate with each other and with external services\n- Global values (environment variables)\n- Access control to limit who can view the application and make changes to it in the specific environment\n- Infrastructure to support it (servers and such)\n- Testing software to test the code\n\nLet's elaborate by giving specific examples of different environments. \n\n![A computer sits on a window sillDescription automatically generated with medium confidence](/blog-images/1897df55a97102dba7c22d77ec9fd988.jpeg)\n\n### Local Environment\n\nThe local environment is where a developer writes software code. They may be writing the whole application or just a specific feature for the application. The developer usually has a local database that works with the application code, and sometimes access to external microservices needed by the code. In other scenarios, where direct access from the developer’s laptop to external services is not possible, we use software mocks instead. When the app runs locally on the developer’s laptop, performance of the app in terms of CPU and RAM is low, compared with running it on a dedicated server or in other environments. In addition, there’s no access control for this environment. Anyone who can access the laptop has access to the application. On the other hand, it's the simplest and most immediate way to develop and test code. \n\n### Development Environment\n\nThe development environment is where multiple developers upload their code to. It’s meant to be the first place where other stakeholders can test the code and the application’s different features. Developers merge the code and run a CI/CD pipeline here. The [CI/CD pipeline](https://en.wikipedia.org/wiki/CI/CD) usually consists of the following steps: \n\n- Changes to the database (database migrations)\n- Static code analysis\n- Vulnerability scanning\n- Deployment to the development environment of the end artifact (JAR in Java or a binary)\n\nIn addition, this environment has more robust and thorough tests than those a developer runs on their laptop (e.g., integration tests). Developers apply the latest updates to the application in the development environment. Because it’s constantly being updated, the application is not stable in the development environment. It has a dedicated URL, environment variables, and better performance than versions run on individual developers’ local PCs. Usually all the developers on the team have access to this the development environment and to its database, which are not accessible outside of the organization (to clients, for instance). \n\n### Staging Environment\n\nThe staging environment is the next environment the software passes through before it’s made accessible to end users. The staging environment should be very similar to the production environment, mirroring it as accurately as possible, because its purpose is to be the last checkpoint before making the latest software publicly available. It should run on the same infrastructure, have the same database content, and the same environment variables (or as similar as possible) to those in the production environment. \n\n![](/blog-images/d163f05b3191007407e8aa9e95e7728d.png)\n\nTraditionally, the CI/CD pipeline that runs in the staging environment involves the most extensive testing of all the environments. All stakeholders in the organization have access to view this environment, but only a handful of key employees have direct access to modify it (IT team, DevOps, senior developers). It's a stable environment and after the code passes the tests, it will be deployed to production. However, recently companies started shifting testing to the left and conducting robust testing earlier in the cycle. \n\n### Production Environment\n\nThe production environment runs the code that serves end users — your customers. It's a stable environment with powerful infrastructure that can handle spikes in demand and has extensive monitoring and logging. Unlike in other environments, down time here results in an immediate service outage. To protect the infrastructure, environment variables, database, and code, access to the production environment is usually restricted to only system administrators. Because automatic tests can reduce performance and change the database, no tests are run in the production environment. \n\n### Customer Demo / Customer Specific Environment\n\nIt sometimes happens that you want to create a version of your software specific to a certain client or to demonstrate a certain feature before it’s available to customers. This happens in the [customer demo environment](https://release.com/blog/great-saas-sales-demos). It has all the bells and whistles of the production environment, but it might have a superset or subset of features from the production environment and a client-specific GUI or dedicated access control, relevant to one client only. \n\n### Environment Pain Points\n\nProvisioning environments is difficult and time consuming. Provisioning new environments requires servers, a deployment mechanism, design of CI/CD pipelines, database servers, access control definitions and so forth. In addition, every environment consumes significant resources to maintain it. For instance, the IT team needs to make sure that connectivity works as expected, make snapshots of the database, edit the list of stakeholders who have access to each environment, and so on. \n\n![A person holding a cigaretteDescription automatically generated with low confidence](/blog-images/b84ab91f3b3c2644c684fc86f02766c0.jpeg)\n\n### Easing Pain Points\n\nRecently, new products that provide [environments as a service (EaaS)](https://release.com/blog/environments-as-a-service-eaas-top-3-benefits) appeared on the market to address those pain points. They provide an unlimited number of automated environments with the click of a button, centralized management of environments, and a lower overhead for making changes to existing environments. \n\n### Increased Velocity\n\nIn addition to simplifying and speeding up environment deployment, EaaS solutions facilitate increased velocity of the R&D process as a whole because developers can spend less time in each environment and can push more code to production faster. For example, they don’t have to pause pushing code for monthly maintenance. Basically, EaaS is another building block every organization should have in their DevOps tool belt to allow the R&D department to deliver more without requiring additional resources. \n\n![](/blog-images/94d763d02b3c630507739be73eeb7155.png)\n\n### Conclusion\n\nEnvironments are an integral part of software development. As software progresses from development to production, it goes through different environments along the way, each with its own purpose. Per its purpose, each environment offers a certain level of performance, access control, code version, and database content. Provisioning and managing environments manually is time consuming and resource intensive. So are creating VMs or containers, setting up networking, adding SSL certificates, providing access control and so forth. EaaS tools have emerged to solve those issues. They provide environment provisioning with a click, an unlimited number of environments, central management, and more. [Eliminating the bottleneck of environment management](https://release.com/blog/increase-developer-velocity-by-removing-environment-bottlenecks) will allow every R&D team to increase development velocity and produce more with the same amount of resources.\n",
          "code": "var Component=(()=>{var h=Object.create;var a=Object.defineProperty;var m=Object.getOwnPropertyDescriptor;var p=Object.getOwnPropertyNames;var u=Object.getPrototypeOf,v=Object.prototype.hasOwnProperty;var f=(t,e)=>()=>(e||t((e={exports:{}}).exports,e),e.exports),g=(t,e)=>{for(var o in e)a(t,o,{get:e[o],enumerable:!0})},r=(t,e,o,s)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let i of p(e))!v.call(t,i)&&i!==o&&a(t,i,{get:()=>e[i],enumerable:!(s=m(e,i))||s.enumerable});return t};var b=(t,e,o)=>(o=t!=null?h(u(t)):{},r(e||!t||!t.__esModule?a(o,\"default\",{value:t,enumerable:!0}):o,t)),w=t=>r(a({},\"__esModule\",{value:!0}),t);var l=f((N,c)=>{c.exports=_jsx_runtime});var C={};g(C,{default:()=>I,frontmatter:()=>y});var n=b(l()),y={title:\"A Simple Guide to Software Environments\",summary:\"Learn about tools to scope and limit instances of your software as it progresses from development to a finished product.\",publishDate:\"Tue Apr 11 2023 21:10:39 GMT+0000 (Coordinated Universal Time)\",author:\"alexander-fridman\",readingTime:5,categories:[\"platform-engineering\",\"product\"],mainImage:\"/blog-images/b3927fc80db8ab1869298638402348e7.png\",imageAlt:\"A Simple Guide to Software Environments\",showCTA:!0,ctaCopy:\"Empower developers to streamline software environments from local to production with Release's on-demand, full-stack environments.\",ctaLink:\"https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=a-simple-guide-to-software-environments\",relatedPosts:[\"\"],ogImage:\"/blog-images/b3927fc80db8ab1869298638402348e7.png\",excerpt:\"Learn about tools to scope and limit instances of your software as it progresses from development to a finished product.\",tags:[\"platform-engineering\",\"product\"],ctaButton:\"Try Release for Free\"};function d(t){let e=Object.assign({h3:\"h3\",a:\"a\",span:\"span\",p:\"p\",ul:\"ul\",li:\"li\",img:\"img\"},t.components);return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsxs)(e.h3,{id:\"introduction\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#introduction\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Introduction\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Building software is hard. It takes a lot of time and effort and it\\u2019s a somewhat cumbersome process. Software development passes through various stages in the product lifecycle before it reaches the end user. A spec must be defined, developers have to code the product, QA has to test it, and DevOps engineers have to deploy it. During this process, the code passes through several environments, from the developers\\u2019 local PC to containers in the cloud.\\xA0\"}),`\n`,(0,n.jsxs)(e.h3,{id:\"what-are-software-environments\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#what-are-software-environments\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"What Are Software Environments\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Software environments are tools to scope and limit instances of your software as it progresses from development to a customer facing product. Each environment contains the following:\\xA0\"}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsx)(e.li,{children:\"Copy of your software code\"}),`\n`,(0,n.jsx)(e.li,{children:\"Copy of the database tables you use with (possibly) environment-specific data\"}),`\n`,(0,n.jsx)(e.li,{children:\"Internal network so the different microservices your code uses can communicate with each other and with external services\"}),`\n`,(0,n.jsx)(e.li,{children:\"Global values (environment variables)\"}),`\n`,(0,n.jsx)(e.li,{children:\"Access control to limit who can view the application and make changes to it in the specific environment\"}),`\n`,(0,n.jsx)(e.li,{children:\"Infrastructure to support it (servers and such)\"}),`\n`,(0,n.jsx)(e.li,{children:\"Testing software to test the code\"}),`\n`]}),`\n`,(0,n.jsx)(e.p,{children:\"Let's elaborate by giving specific examples of different environments.\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/1897df55a97102dba7c22d77ec9fd988.jpeg\",alt:\"A computer sits on a window sillDescription automatically generated with medium confidence\"})}),`\n`,(0,n.jsxs)(e.h3,{id:\"local-environment\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#local-environment\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Local Environment\"]}),`\n`,(0,n.jsx)(e.p,{children:\"The local environment is where a developer writes software code. They may be writing the whole application or just a specific feature for the application. The developer usually has a local database that works with the application code, and sometimes access to external microservices needed by the code. In other scenarios, where direct access from the developer\\u2019s laptop to external services is not possible, we use software mocks instead. When the app runs locally on the developer\\u2019s laptop, performance of the app in terms of CPU and RAM is low, compared with running it on a dedicated server or in other environments. In addition, there\\u2019s no access control for this environment. Anyone who can access the laptop has access to the application. On the other hand, it's the simplest and most immediate way to develop and test code.\\xA0\"}),`\n`,(0,n.jsxs)(e.h3,{id:\"development-environment\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#development-environment\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Development Environment\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"The development environment is where multiple developers upload their code to. It\\u2019s meant to be the first place where other stakeholders can test the code and the application\\u2019s different features. Developers merge the code and run a CI/CD pipeline here. The \",(0,n.jsx)(e.a,{href:\"https://en.wikipedia.org/wiki/CI/CD\",children:\"CI/CD pipeline\"}),\" usually consists of the following steps:\\xA0\"]}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsx)(e.li,{children:\"Changes to the database (database migrations)\"}),`\n`,(0,n.jsx)(e.li,{children:\"Static code analysis\"}),`\n`,(0,n.jsx)(e.li,{children:\"Vulnerability scanning\"}),`\n`,(0,n.jsx)(e.li,{children:\"Deployment to the development environment of the end artifact (JAR in Java or a binary)\"}),`\n`]}),`\n`,(0,n.jsx)(e.p,{children:\"In addition, this environment has more robust and thorough tests than those a developer runs on their laptop (e.g., integration tests). Developers apply the latest updates to the application in the development environment. Because it\\u2019s constantly being updated, the application is not stable in the development environment. It has a dedicated URL, environment variables, and better performance than versions run on individual developers\\u2019 local PCs. Usually all the developers on the team have access to this the development environment and to its database, which are not accessible outside of the organization (to clients, for instance).\\xA0\"}),`\n`,(0,n.jsxs)(e.h3,{id:\"staging-environment\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#staging-environment\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Staging Environment\"]}),`\n`,(0,n.jsx)(e.p,{children:\"The staging environment is the next environment the software passes through before it\\u2019s made accessible to end users. The staging environment should be very similar to the production environment, mirroring it as accurately as possible, because its purpose is to be the last checkpoint before making the latest software publicly available. It should run on the same infrastructure, have the same database content, and the same environment variables (or as similar as possible) to those in the production environment.\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/d163f05b3191007407e8aa9e95e7728d.png\",alt:\"\"})}),`\n`,(0,n.jsx)(e.p,{children:\"Traditionally, the CI/CD pipeline that runs in the staging environment involves the most extensive testing of all the environments. All stakeholders in the organization have access to view this environment, but only a handful of key employees have direct access to modify it (IT team, DevOps, senior developers). It's a stable environment and after the code passes the tests, it will be deployed to production. However, recently companies started shifting testing to the left and conducting robust testing earlier in the cycle.\\xA0\"}),`\n`,(0,n.jsxs)(e.h3,{id:\"production-environment\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#production-environment\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Production Environment\"]}),`\n`,(0,n.jsx)(e.p,{children:\"The production environment runs the code that serves end users \\u2014 your customers. It's a stable environment with powerful infrastructure that can handle spikes in demand and has extensive monitoring and logging. Unlike in other environments, down time here results in an immediate service outage. To protect the infrastructure, environment variables, database, and code, access to the production environment is usually restricted to only system administrators. Because automatic tests can reduce performance and change the database, no tests are run in the production environment.\\xA0\"}),`\n`,(0,n.jsxs)(e.h3,{id:\"customer-demo--customer-specific-environment\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#customer-demo--customer-specific-environment\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Customer Demo / Customer Specific Environment\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"It sometimes happens that you want to create a version of your software specific to a certain client or to demonstrate a certain feature before it\\u2019s available to customers. This happens in the \",(0,n.jsx)(e.a,{href:\"https://release.com/blog/great-saas-sales-demos\",children:\"customer demo environment\"}),\". It has all the bells and whistles of the production environment, but it might have a superset or subset of features from the production environment and a client-specific GUI or dedicated access control, relevant to one client only.\\xA0\"]}),`\n`,(0,n.jsxs)(e.h3,{id:\"environment-pain-points\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#environment-pain-points\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Environment Pain Points\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Provisioning environments is difficult and time consuming. Provisioning new environments requires servers, a deployment mechanism, design of CI/CD pipelines, database servers, access control definitions and so forth. In addition, every environment consumes significant resources to maintain it. For instance, the IT team needs to make sure that connectivity works as expected, make snapshots of the database, edit the list of stakeholders who have access to each environment, and so on.\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/b84ab91f3b3c2644c684fc86f02766c0.jpeg\",alt:\"A person holding a cigaretteDescription automatically generated with low confidence\"})}),`\n`,(0,n.jsxs)(e.h3,{id:\"easing-pain-points\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#easing-pain-points\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Easing Pain Points\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"Recently, new products that provide \",(0,n.jsx)(e.a,{href:\"https://release.com/blog/environments-as-a-service-eaas-top-3-benefits\",children:\"environments as a service (EaaS)\"}),\" appeared on the market to address those pain points. They provide an unlimited number of automated environments with the click of a button, centralized management of environments, and a lower overhead for making changes to existing environments.\\xA0\"]}),`\n`,(0,n.jsxs)(e.h3,{id:\"increased-velocity\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#increased-velocity\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Increased Velocity\"]}),`\n`,(0,n.jsx)(e.p,{children:\"In addition to simplifying and speeding up environment deployment, EaaS solutions facilitate increased velocity of the R&D process as a whole because developers can spend less time in each environment and can push more code to production faster. For example, they don\\u2019t have to pause pushing code for monthly maintenance. Basically, EaaS is another building block every organization should have in their DevOps tool belt to allow the R&D department to deliver more without requiring additional resources.\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/94d763d02b3c630507739be73eeb7155.png\",alt:\"\"})}),`\n`,(0,n.jsxs)(e.h3,{id:\"conclusion\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#conclusion\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Conclusion\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"Environments are an integral part of software development. As software progresses from development to production, it goes through different environments along the way, each with its own purpose. Per its purpose, each environment offers a certain level of performance, access control, code version, and database content. Provisioning and managing environments manually is time consuming and resource intensive. So are creating VMs or containers, setting up networking, adding SSL certificates, providing access control and so forth. EaaS tools have emerged to solve those issues. They provide environment provisioning with a click, an unlimited number of environments, central management, and more. \",(0,n.jsx)(e.a,{href:\"https://release.com/blog/increase-developer-velocity-by-removing-environment-bottlenecks\",children:\"Eliminating the bottleneck of environment management\"}),\" will allow every R&D team to increase development velocity and produce more with the same amount of resources.\"]})]})}function k(t={}){let{wrapper:e}=t.components||{};return e?(0,n.jsx)(e,Object.assign({},t,{children:(0,n.jsx)(d,t)})):d(t)}var I=k;return w(C);})();\n;return Component;"
        },
        "_id": "blog/posts/a-simple-guide-to-software-environments.mdx",
        "_raw": {
          "sourceFilePath": "blog/posts/a-simple-guide-to-software-environments.mdx",
          "sourceFileName": "a-simple-guide-to-software-environments.mdx",
          "sourceFileDir": "blog/posts",
          "contentType": "mdx",
          "flattenedPath": "blog/posts/a-simple-guide-to-software-environments"
        },
        "type": "BlogPost",
        "computedSlug": "a-simple-guide-to-software-environments"
      },
      "documentHash": "1739393595016",
      "hasWarnings": false,
      "documentTypeName": "BlogPost"
    },
    "blog/posts/agile-release-plan.mdx": {
      "document": {
        "title": "How to Make an Agile Release Plan, With Examples",
        "summary": "How do you create an Agile release plan? What are the steps, and what's it like in action? Here's how to get started and",
        "publishDate": "Tue Feb 15 2022 22:18:07 GMT+0000 (Coordinated Universal Time)",
        "author": "eric-goebelbecker",
        "readingTime": 8,
        "categories": [
          "platform-engineering",
          "product"
        ],
        "mainImage": "/blog-images/98ce19f7da96651f52b9c66a708c58ea.jpg",
        "imageAlt": "How to make an agile release plan",
        "showCTA": true,
        "ctaCopy": "Improve Agile release planning with Release's on-demand environments for seamless collaboration and faster software deployment.",
        "ctaLink": "https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=agile-release-plan",
        "relatedPosts": [
          ""
        ],
        "ogImage": "/blog-images/98ce19f7da96651f52b9c66a708c58ea.jpg",
        "excerpt": "How do you create an Agile release plan? What are the steps, and what's it like in action? Here's how to get started and",
        "tags": [
          "platform-engineering",
          "product"
        ],
        "ctaButton": "Try Release for Free",
        "body": {
          "raw": "\n_\"Change is the only constant in life\" -_ [_Heraclitus_](https://www.worldhistory.org/Heraclitus_of_Ephesos/#:~:text=expected%20him%20to.-,Life%20is%20Flux,-Following%20in%20the)\n\nWas Heraclitus a developer, or did the Oracle of Delphi show him where things were going? Either way, his most famous quote is an apt description of the world we work in. We have to navigate constant transitions while maintaining forward progress. The Agile process helps us cope with that chaos. It's right there in the [name](https://www.merriam-webster.com/dictionary/agile_): _having a quick, resourceful, and adaptable character_. So when it's time to plan the first or next increment in your product, you need an Agile release plan. \n\nLet's look at what an Agile release plan is and how you can go about creating one. \n\n## **What's an Agile Release Plan?**\n\nMany bloggers have spilled a great deal of digital ink over the topic of Agile, where it comes from, and how to implement it. But we have the [Agile Principles.](https://www.agilealliance.org/agile101/12-principles-behind-the-agile-manifesto/) They're the essential guide on what it means and why we need it. So, before we delve into creating an Agile release plan, let's level set. \n\n### **The Agile Principles**\n\n cover the four principles and how they relate to an agile release plan. \n\n_#1 - Our highest priority is to satisfy the customer through early and continuous delivery of valuable software._\n\nAny successful software development team focuses on customer satisfaction, but this principle introduces the concepts of **early** and **continuous**. \n\nEarly and continuous means releasing features and fixes as you build and test. It's Agile's alternative to holding onto new code or rushing new features in the service of meeting an arbitrary date. \n\n_#2 - Welcome changing requirements, even late in development. Agile processes harness change for the customer’s competitive advantage._\n\nHere's the hard part. Your release plan needs to keep your development efforts on target, but you still need to address changing requirements. \n\n_#3 - Deliver working software frequently, from a couple of weeks to a couple of months, with a preference to the shorter timescale._\n\nPrinciple #3 echoes the salient points in #1 but helps us out by establishing a solid time range. Agile works in increments of weeks. \n\n#4 - Business people and developers must work together daily throughout the project.\n\nFinally, we get a hint on how to formulate and execute an Agile release plan; by getting developers and business people to work together. Moreover, they don't work together on the project and head back to their respective departments. They work together _daily_. \n\nBefore we move on, it's worth paying [Principle #6](https://www.agilealliance.org/agile101/12-principles-behind-the-agile-manifesto/) a brief visit. It refines and reinforces #4 when it calls face-to-face conversations the most effective form of communication. You don't do Agile Release Planning over emails and Jira tickets. You do it by sitting down as a group and formulating a plan. \n\n### **What's an Agile Release?**\n\nBased on the Agile principles, we can infer what makes a release **agile**. A release is a set of features you deliver to customers in a single increment. It's **agile** when you bake in the ability to adjust. \n\n![](/blog-images/b90a1452e3b2688aea7b28297ec0bf73.png)\n\nAgile release plans adapt to changing requirements and conditions. They don't pick a date and decide what fits in or set a date and drive mercilessly to meet it. They set priorities, estimate the effort required, do the work, and make adjustments based on updated conditions and results. \n\n## **Agile Release Planning Process**\n\nThe release planning process helps your team determine how to channel their efforts into the next increment of features and fixes. \n\n### **Step 1: Establish the Release Vision**\n\nBefore planning a release, you need a vision for your product and its next step. This vision guides you as you decide which features to prioritize, which to slip if the situation changes, and which to set aside for the next pass. \n\nThis is the first step in the process and the first opportunity for business people and developers to collaborate. The release vision must align with business goals, market conditions, and engineering reality. \n\n### **Step 2: Evaluate Your Backlog**\n\nNow, it's time to look at your product backlog and sort the features by priority. This is where you put your vision to work. \n\nHere again, everyone is involved; engineers and stakeholders apply a shared vision to the product backlog and product roadmap to determine what the priorities are for your product. \n\nFinally, create a basic release plan that outlines the goal, a target release date, and a complete set of ranked user stories with story points. This is the output you'll need for the next step. \n\n![](/blog-images/821f083f34e477a96d74d4ede40b6b54.png)\n\n### **Step 3: Review the Agile Release Plan**\n\nNext, take your draft release plan, assemble all stakeholders, and hold a release planning session. Developers hate meetings, but this one is essential. As we covered above, Agile places great importance on face-to-face interaction and alignment between the business and engineering. This meeting will ensure that everyone understands and agrees with the plan.\n\nThe meeting agenda should cover the following items. \n\n#### **Review Roadmap**\n\nReview the vision and the product roadmap. If it seems like these steps include a lot of repetition and review, that means you're paying attention! This is another opportunity to ensure that the stakeholders are on the same page regarding the product and the release. \n\n#### **Review Design and Architecture**\n\nNext, review the technical details and design for the release. Are there dependencies or gaps that can affect the release schedule? How will the plan be adjusted if these issues can't be overcome? \n\n#### **Review Iteration schedule**\n\nThe draft release plan has user stories, story points, and proposed sprints. This step takes those stories and arranges them into sprints. One method is to arrange the stories into sprints based on [velocity](https://www.agilealliance.org/glossary/velocity/). How much can you reasonably expect to get done in each sprint based on development resources the points assigned to each story? \n\n#### **Define \"Done\"**\n\nWhat does \"done\" mean? Do the stakeholders and the developers agree on what the completed release will look and act like? This is where the [Definition of Done](https://www.agilealliance.org/glossary/definition-of-done/) needs to be codified by all stakeholders. \n\n#### **Step 4: Execute and Update**\n\nFinally, it's time to execute the plan. Agile release plans are living documents, so putting them into action is part of the planning process. \n\nA review follows each iteration. What went wrong? What went better than expected? These inputs are critical for planning the next sprint. This step is crucial in keeping your development efforts on track. \n\n## **Agile Release Plan Examples**\n\nLet's finish up by looking at how to apply this process to the two most common product release scenarios. \n\n### **New Release**\n\nWhen you're planning a new release of an existing product, you already have a lot of information. There are few things more valuable than \"we've seen this before\" when planning and adjusting an Agile release plan! \n\nThere's already a product out there in the world, which means someone had a vision for what it should do. It may not have been formally recorded or discussed, but there's something there. Now is your chance to get everyone together and make sure the current vision is in harmony with market conditions and business needs. Then, you can either update or create your roadmap and your design. \n\nThe previous releases mean data on current efforts and timelines. You might be new to Agile, but you know how long the last release took and how long it takes to turn around a bug fix. That's all valuable input to the process. \n\nFinally, the success or failure of previous releases will be a valuable guide when getting the stakeholders together to define \"Done.\" \n\n### **New Product**\n\nWhen planning the first release for a new product, you have to rely on the team's collective experience combined with data gathered in the testing and prototyping stages. (Of course, those stages should have gone through this process, too?) Maybe you're planning one of those stages and have nothing to go by. \n\nIt's hard to say that there's a most critical step of the plan, but if it exists, it's the vision, especially when talking about a new product! If the interested parties can't agree on what the new product will be, how can you define the effort required to build it? How can you define \"Done?\" \n\nOnce you've agreed on the vision, you're going to do a lot of estimating and a lot of adjusting. That's okay! The estimating and adjustment are features, not bugs. \n\n## Start Your Agile Release Plan\n\nThis post covered what an Agile release plan is and how to create one. We started with [first principles](https://www.agilealliance.org/agile101/12-principles-behind-the-agile-manifesto/) (or at least a subset) and applied them to planning and executing a release. Then we wrapped up with an overview of two common release scenarios. \n\nGet started with your next Agile [Release](https://releasehub.com/blog/awesome-release-docker-compose-examples-working-in-release-and-kubernetes) now!\n",
          "code": "var Component=(()=>{var d=Object.create;var s=Object.defineProperty;var p=Object.getOwnPropertyDescriptor;var g=Object.getOwnPropertyNames;var u=Object.getPrototypeOf,m=Object.prototype.hasOwnProperty;var w=(a,e)=>()=>(e||a((e={exports:{}}).exports,e),e.exports),f=(a,e)=>{for(var t in e)s(a,t,{get:e[t],enumerable:!0})},o=(a,e,t,r)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let i of g(e))!m.call(a,i)&&i!==t&&s(a,i,{get:()=>e[i],enumerable:!(r=p(e,i))||r.enumerable});return a};var y=(a,e,t)=>(t=a!=null?d(u(a)):{},o(e||!a||!a.__esModule?s(t,\"default\",{value:a,enumerable:!0}):t,a)),v=a=>o(s({},\"__esModule\",{value:!0}),a);var h=w((T,l)=>{l.exports=_jsx_runtime});var A={};f(A,{default:()=>x,frontmatter:()=>b});var n=y(h()),b={title:\"How to Make an Agile Release Plan, With Examples\",summary:\"How do you create an Agile release plan? What are the steps, and what's it like in action? Here's how to get started and\",publishDate:\"Tue Feb 15 2022 22:18:07 GMT+0000 (Coordinated Universal Time)\",author:\"eric-goebelbecker\",readingTime:8,categories:[\"platform-engineering\",\"product\"],mainImage:\"/blog-images/98ce19f7da96651f52b9c66a708c58ea.jpg\",imageAlt:\"How to make an agile release plan\",showCTA:!0,ctaCopy:\"Improve Agile release planning with Release's on-demand environments for seamless collaboration and faster software deployment.\",ctaLink:\"https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=agile-release-plan\",relatedPosts:[\"\"],ogImage:\"/blog-images/98ce19f7da96651f52b9c66a708c58ea.jpg\",excerpt:\"How do you create an Agile release plan? What are the steps, and what's it like in action? Here's how to get started and\",tags:[\"platform-engineering\",\"product\"],ctaButton:\"Try Release for Free\"};function c(a){let e=Object.assign({p:\"p\",em:\"em\",a:\"a\",h2:\"h2\",span:\"span\",strong:\"strong\",h3:\"h3\",img:\"img\",h4:\"h4\"},a.components);return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsxs)(e.p,{children:[(0,n.jsx)(e.em,{children:'\"Change is the only constant in life\" -'}),\" \",(0,n.jsx)(e.a,{href:\"https://www.worldhistory.org/Heraclitus_of_Ephesos/#:~:text=expected%20him%20to.-,Life%20is%20Flux,-Following%20in%20the\",children:(0,n.jsx)(e.em,{children:\"Heraclitus\"})})]}),`\n`,(0,n.jsxs)(e.p,{children:[\"Was Heraclitus a developer, or did the Oracle of Delphi show him where things were going? Either way, his most famous quote is an apt description of the world we work in. We have to navigate constant transitions while maintaining forward progress. The Agile process helps us cope with that chaos. It's right there in the \",(0,n.jsx)(e.a,{href:\"https://www.merriam-webster.com/dictionary/agile_\",children:\"name\"}),\": \",(0,n.jsx)(e.em,{children:\"having a quick, resourceful, and adaptable character\"}),\". So when it's time to plan the first or next increment in your product, you need an Agile release plan.\\xA0\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Let's look at what an Agile release plan is and how you can go about creating one.\\xA0\"}),`\n`,(0,n.jsxs)(e.h2,{id:\"whats-an-agile-release-plan\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#whats-an-agile-release-plan\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.strong,{children:\"What's an Agile Release Plan?\"})]}),`\n`,(0,n.jsxs)(e.p,{children:[\"Many bloggers have spilled a great deal of digital ink over the topic of Agile, where it comes from, and how to implement it. But we have the \",(0,n.jsx)(e.a,{href:\"https://www.agilealliance.org/agile101/12-principles-behind-the-agile-manifesto/\",children:\"Agile Principles.\"}),\" They're the essential guide on what it means and why we need it. So, before we delve into creating an Agile release plan, let's level set.\\xA0\"]}),`\n`,(0,n.jsxs)(e.h3,{id:\"the-agile-principles\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#the-agile-principles\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.strong,{children:\"The Agile Principles\"})]}),`\n`,(0,n.jsx)(e.p,{children:\"\\xA0cover the four principles and how they relate to an agile release plan.\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.em,{children:\"#1 - Our highest priority is to satisfy the customer through early and continuous delivery of valuable software.\"})}),`\n`,(0,n.jsxs)(e.p,{children:[\"Any successful software development team focuses on customer satisfaction, but this principle introduces the concepts of \",(0,n.jsx)(e.strong,{children:\"early\"}),\" and \",(0,n.jsx)(e.strong,{children:\"continuous\"}),\".\\xA0\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Early and continuous means releasing features and fixes as you build and test. It's Agile's alternative to holding onto new code or rushing new features in the service of meeting an arbitrary date.\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.em,{children:\"#2 - Welcome changing requirements, even late in development. Agile processes harness change for the customer\\u2019s competitive advantage.\"})}),`\n`,(0,n.jsx)(e.p,{children:\"Here's the hard part. Your release plan needs to keep your development efforts on target, but you still need to address changing requirements.\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.em,{children:\"#3 - Deliver working software frequently, from a couple of weeks to a couple of months, with a preference to the shorter timescale.\"})}),`\n`,(0,n.jsx)(e.p,{children:\"Principle #3 echoes the salient points in #1 but helps us out by establishing a solid time range. Agile works in increments of weeks.\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:\"#4 - Business people and developers must work together daily throughout the project.\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"Finally, we get a hint on how to formulate and execute an Agile release plan; by getting developers and business people to work together. Moreover, they don't work together on the project and head back to their respective departments. They work together \",(0,n.jsx)(e.em,{children:\"daily\"}),\".\\xA0\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"Before we move on, it's worth paying \",(0,n.jsx)(e.a,{href:\"https://www.agilealliance.org/agile101/12-principles-behind-the-agile-manifesto/\",children:\"Principle #6\"}),\" a brief visit. It refines and reinforces #4 when it calls face-to-face conversations the most effective form of communication. You don't do Agile Release Planning over emails and Jira tickets. You do it by sitting down as a group and formulating a plan.\\xA0\"]}),`\n`,(0,n.jsxs)(e.h3,{id:\"whats-an-agile-release\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#whats-an-agile-release\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.strong,{children:\"What's an Agile Release?\"})]}),`\n`,(0,n.jsxs)(e.p,{children:[\"Based on the Agile principles, we can infer what makes a release \",(0,n.jsx)(e.strong,{children:\"agile\"}),\". A release is a set of features you deliver to customers in a single increment. It's \",(0,n.jsx)(e.strong,{children:\"agile\"}),\" when you bake in the ability to adjust.\\xA0\"]}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/b90a1452e3b2688aea7b28297ec0bf73.png\",alt:\"\"})}),`\n`,(0,n.jsx)(e.p,{children:\"Agile release plans adapt to changing requirements and conditions. They don't pick a date and decide what fits in or set a date and drive mercilessly to meet it. They set priorities, estimate the effort required, do the work, and make adjustments based on updated conditions and results.\\xA0\"}),`\n`,(0,n.jsxs)(e.h2,{id:\"agile-release-planning-process\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#agile-release-planning-process\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.strong,{children:\"Agile Release Planning Process\"})]}),`\n`,(0,n.jsx)(e.p,{children:\"The release planning process helps your team determine how to channel their efforts into the next increment of features and fixes.\\xA0\"}),`\n`,(0,n.jsxs)(e.h3,{id:\"step-1-establish-the-release-vision\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#step-1-establish-the-release-vision\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.strong,{children:\"Step 1: Establish the Release Vision\"})]}),`\n`,(0,n.jsx)(e.p,{children:\"Before planning a release, you need a vision for your product and its next step. This vision guides you as you decide which features to prioritize, which to slip if the situation changes, and which to set aside for the next pass.\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:\"This is the first step in the process and the first opportunity for business people and developers to collaborate. The release vision must align with business goals, market conditions, and engineering reality.\\xA0\"}),`\n`,(0,n.jsxs)(e.h3,{id:\"step-2-evaluate-your-backlog\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#step-2-evaluate-your-backlog\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.strong,{children:\"Step 2: Evaluate Your Backlog\"})]}),`\n`,(0,n.jsx)(e.p,{children:\"Now, it's time to look at your product backlog and sort the features by priority. This is where you put your vision to work.\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:\"Here again, everyone is involved; engineers and stakeholders apply a shared vision to the product backlog and product roadmap to determine what the priorities are for your product.\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:\"Finally, create a basic release plan that outlines the goal, a target release date, and a complete set of ranked user stories with story points. This is the output you'll need for the next step.\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/821f083f34e477a96d74d4ede40b6b54.png\",alt:\"\"})}),`\n`,(0,n.jsxs)(e.h3,{id:\"step-3-review-the-agile-release-plan\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#step-3-review-the-agile-release-plan\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.strong,{children:\"Step 3: Review the Agile Release Plan\"})]}),`\n`,(0,n.jsx)(e.p,{children:\"Next, take your draft release plan, assemble all stakeholders, and hold a release planning session. Developers hate meetings, but this one is essential. As we covered above, Agile places great importance on face-to-face interaction and alignment between the business and engineering. This meeting will ensure that everyone understands and agrees with the plan.\"}),`\n`,(0,n.jsx)(e.p,{children:\"The meeting agenda should cover the following items.\\xA0\"}),`\n`,(0,n.jsxs)(e.h4,{id:\"review-roadmap\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#review-roadmap\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.strong,{children:\"Review Roadmap\"})]}),`\n`,(0,n.jsx)(e.p,{children:\"Review the vision and the product roadmap. If it seems like these steps include a lot of repetition and review, that means you're paying attention! This is another opportunity to ensure that the stakeholders are on the same page regarding the product and the release.\\xA0\"}),`\n`,(0,n.jsxs)(e.h4,{id:\"review-design-and-architecture\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#review-design-and-architecture\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.strong,{children:\"Review Design and Architecture\"})]}),`\n`,(0,n.jsx)(e.p,{children:\"Next, review the technical details and design for the release. Are there dependencies or gaps that can affect the release schedule? How will the plan be adjusted if these issues can't be overcome?\\xA0\"}),`\n`,(0,n.jsxs)(e.h4,{id:\"review-iteration-schedule\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#review-iteration-schedule\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.strong,{children:\"Review Iteration schedule\"})]}),`\n`,(0,n.jsxs)(e.p,{children:[\"The draft release plan has user stories, story points, and proposed sprints. This step takes those stories and arranges them into sprints. One method is to arrange the stories into sprints based on \",(0,n.jsx)(e.a,{href:\"https://www.agilealliance.org/glossary/velocity/\",children:\"velocity\"}),\". How much can you reasonably expect to get done in each sprint based on development resources the points assigned to each story?\\xA0\"]}),`\n`,(0,n.jsxs)(e.h4,{id:\"define-done\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#define-done\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.strong,{children:'Define \"Done\"'})]}),`\n`,(0,n.jsxs)(e.p,{children:['What does \"done\" mean? Do the stakeholders and the developers agree on what the completed release will look and act like? This is where the ',(0,n.jsx)(e.a,{href:\"https://www.agilealliance.org/glossary/definition-of-done/\",children:\"Definition of Done\"}),\" needs to be codified by all stakeholders.\\xA0\"]}),`\n`,(0,n.jsxs)(e.h4,{id:\"step-4-execute-and-update\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#step-4-execute-and-update\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.strong,{children:\"Step 4: Execute and Update\"})]}),`\n`,(0,n.jsx)(e.p,{children:\"Finally, it's time to execute the plan. Agile release plans are living documents, so putting them into action is part of the planning process.\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:\"A review follows each iteration. What went wrong? What went better than expected? These inputs are critical for planning the next sprint. This step is crucial in keeping your development efforts on track.\\xA0\"}),`\n`,(0,n.jsxs)(e.h2,{id:\"agile-release-plan-examples\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#agile-release-plan-examples\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.strong,{children:\"Agile Release Plan Examples\"})]}),`\n`,(0,n.jsx)(e.p,{children:\"Let's finish up by looking at how to apply this process to the two most common product release scenarios.\\xA0\"}),`\n`,(0,n.jsxs)(e.h3,{id:\"new-release\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#new-release\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.strong,{children:\"New Release\"})]}),`\n`,(0,n.jsx)(e.p,{children:`When you're planning a new release of an existing product, you already have a lot of information. There are few things more valuable than \"we've seen this before\" when planning and adjusting an Agile release plan!\\xA0`}),`\n`,(0,n.jsx)(e.p,{children:\"There's already a product out there in the world, which means someone had a vision for what it should do. It may not have been formally recorded or discussed, but there's something there. Now is your chance to get everyone together and make sure the current vision is in harmony with market conditions and business needs. Then, you can either update or create your roadmap and your design.\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:\"The previous releases mean data on current efforts and timelines. You might be new to Agile, but you know how long the last release took and how long it takes to turn around a bug fix. That's all valuable input to the process.\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:'Finally, the success or failure of previous releases will be a valuable guide when getting the stakeholders together to define \"Done.\"\\xA0'}),`\n`,(0,n.jsxs)(e.h3,{id:\"new-product\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#new-product\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.strong,{children:\"New Product\"})]}),`\n`,(0,n.jsx)(e.p,{children:\"When planning the first release for a new product, you have to rely on the team's collective experience combined with data gathered in the testing and prototyping stages. (Of course, those stages should have gone through this process, too?) Maybe you're planning one of those stages and have nothing to go by.\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:`It's hard to say that there's a most critical step of the plan, but if it exists, it's the vision, especially when talking about a new product! If the interested parties can't agree on what the new product will be, how can you define the effort required to build it? How can you define \"Done?\"\\xA0`}),`\n`,(0,n.jsx)(e.p,{children:\"Once you've agreed on the vision, you're going to do a lot of estimating and a lot of adjusting. That's okay! The estimating and adjustment are features, not bugs.\\xA0\"}),`\n`,(0,n.jsxs)(e.h2,{id:\"start-your-agile-release-plan\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#start-your-agile-release-plan\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Start Your Agile Release Plan\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"This post covered what an Agile release plan is and how to create one. We started with \",(0,n.jsx)(e.a,{href:\"https://www.agilealliance.org/agile101/12-principles-behind-the-agile-manifesto/\",children:\"first principles\"}),\" (or at least a subset) and applied them to planning and executing a release. Then we wrapped up with an overview of two common release scenarios.\\xA0\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"Get started with your next Agile \",(0,n.jsx)(e.a,{href:\"https://releasehub.com/blog/awesome-release-docker-compose-examples-working-in-release-and-kubernetes\",children:\"Release\"}),\" now!\"]})]})}function k(a={}){let{wrapper:e}=a.components||{};return e?(0,n.jsx)(e,Object.assign({},a,{children:(0,n.jsx)(c,a)})):c(a)}var x=k;return v(A);})();\n;return Component;"
        },
        "_id": "blog/posts/agile-release-plan.mdx",
        "_raw": {
          "sourceFilePath": "blog/posts/agile-release-plan.mdx",
          "sourceFileName": "agile-release-plan.mdx",
          "sourceFileDir": "blog/posts",
          "contentType": "mdx",
          "flattenedPath": "blog/posts/agile-release-plan"
        },
        "type": "BlogPost",
        "computedSlug": "agile-release-plan"
      },
      "documentHash": "1739393595016",
      "hasWarnings": false,
      "documentTypeName": "BlogPost"
    },
    "blog/posts/automating-your-day-to-day-workflows-with-the-release-cli.mdx": {
      "document": {
        "title": "Automating your day-to-day workflows with the Release CLI",
        "summary": "Walk you through a hypothetical bug scenario and how Release can make your day-to-day workflows more enjoyable.",
        "publishDate": "Tue Apr 04 2023 18:19:50 GMT+0000 (Coordinated Universal Time)",
        "author": "luiz-felipe",
        "readingTime": 4,
        "categories": [
          "platform-engineering",
          "product"
        ],
        "mainImage": "/blog-images/a07cfb35baa2e31696bd68b5af23bda0.png",
        "imageAlt": "Automating your day-to-day workflows with the Release CLI",
        "showCTA": true,
        "ctaCopy": "Automate bug replication and ticket handling with Release's instant environments. Streamline workflows and focus on fixing bugs faster.",
        "ctaLink": "https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=automating-your-day-to-day-workflows-with-the-release-cli",
        "relatedPosts": [
          ""
        ],
        "ogImage": "/blog-images/a07cfb35baa2e31696bd68b5af23bda0.png",
        "excerpt": "Walk you through a hypothetical bug scenario and how Release can make your day-to-day workflows more enjoyable.",
        "tags": [
          "platform-engineering",
          "product"
        ],
        "ctaButton": "Try Release for Free",
        "body": {
          "raw": "\nA typical day for a developer is filled with coming up with great ideas for applications, writing flawless code and seeing everything work exactly as expected, right? Well, sometimes. Often, our days are filled with repetitive tasks, and if you’re anything like me, once you’ve done the same task a few times you get an urge to automate it. One of these common repetitive tasks is fixing pesky bugs. Today, I will walk you through a hypothetical bug scenario where we’ll be using Release for ephemeral environments and Release CLI to make your day-to-day workflows more enjoyable. This might give you some time back to focus on that perfect app! \n\nBefore we begin, make sure to set up your version of Release. You can get a free trial [here](https://beta.release.com/register). \n\n### The dreaded bug\n\nWhen fixing bugs one of the first steps towards a fix is to replicate them. However, doing that can be very tricky. Our development environments are often pristine and perfect (right?) and bugs rarely happen! Somehow customers (or hopefully QA) find new ways to break your application and it’s our job to try to break it in the same way, so we can fix it for good.\n\nSometimes you have good error handling and reporting which gives you all of the context needed for a fix. But if that’s not the case we have to dig deeper to uncover the application state when the error occurred.\n\nBut let’s not get ahead of ourselves here. First things first: we must keep management happy by properly handling our ticket. Sadly this is one of the hardest things to automate because of the sheer diversity of ticketing systems and maturity of these products. Although most of them offer APIs, almost none offer good CLI tools which means you might have to resort to using the dreaded browser/mouse combo.\n\nHere at Release we use Linear which has an API but unfortunately has no CLI tool ready for us to use. Which means we have a choice: do we keep this step manual or do we automate it? I took the plunge and wrote myself a simple script, because at the end of the day, I’m going to have to deal with these tickets hundreds of times in the future. The few minutes spent automating this will pay off in dividends (and your wrist will thank you too).\n\nIf you’re interested the script is [available here](https://gist.github.com/Draiken/f23a98de94015b794219d9e64ef5c642). Maybe you’re lucky and your ticketing system offers a CLI or you too will have to take the plunge and write something to automate this (you know you want to). Whatever the means, we can use that to tell the world we’re starting our ticket!\n\n### Creating our environment\n\nNow the first thing we need is an environment to reproduce our bug. Luckily Release offers a feature on top of the ephemeral environments that’s perfect for these situations: [instant datasets](https://docs.release.com/reference-documentation/instant-datasets-aws). We can spin up environments with production-like data within minutes. Depending on how fresh the snapshot we’re using is, the data that caused the bug we need to reproduce might be there already.\n\nFor this we’re going to use the [Release CLI](https://docs.release.com/cli/getting-started). Once that’s setup we’ll login and we’ll be ready to create an environment. Here’s what I use:\n\n```yalm line-numbers\n\nrelease environments create –app my_app –wait | notify-send “Environment ready!”\n\n# or for our friends that run the fruit system\n\nrelease environments create –app my_app –wait | osascript -e 'display notification \"Environment Ready!\"\n\n```\n\nOnce the environment is up and running we get a notification and we can start our investigation.\n\nFiguring out the state around the bug more often than not requires you to dig through the data either directly on the database or through some sort of application console. Ideally we’d just have a perfect admin page showing what’s what, but that’s not always the case.\n\nOur Release CLI allows us to very easily dive right into the environment we just spun up and poke around. For this we’re going to use the **instances** command, which lists the running instances and lets us open up a terminal inside those containers. All we need to do is type:\n\n    \t`release instances terminal`\n\nSelect our app/instance and boom: we’re in! If we use something like Rails we can access our console here or alternatively we can connect to the database directly, given this environment is attached to our database and we have the environment variables needed to connect to it.\n\n### Fix all the things\n\nNow that we’ve investigated and replicated our bug, we can finally fix it! For this we can use another very handy tool from Release: [development environments](https://docs.release.com/cli/remote-dev). Rather than making changes, deploying them to the environment and then checking it, we can simplify all that by running our local code inside the release environment. Let’s start our development environment:\n\n    \t`release development start –app my_app –environment my_env`\n\nThis will insert our local machine’s code into the environment and forward the configured ports right into our localhost. We get the best of both worlds: release is running our code (along all those services it depends on) and we get instant feedback from our changes. We can now fix the issue and close this ticket once and for all.\n\nWith our changes done, we can commit those in and create our pull request. Tools like GitHub and GitLab provide nice CLIs which make it easy to automate this part. At Release we use GitHub and I personally use the [hub](https://github.com/github/hub) CLI for this.\n\n    \t`hub pull-request -o -m “Fix all the things”`\n\nWith our pull request open, we can also update our ticket to the appropriate state, if it’s not already done automatically. Now we can ask our teammates to review our code, give us the all important “LGTM” (Let Go The Mayo!) and merge that bad boy in.\n\n### Automate it all\n\nThere’s much more in our daily workflow that we can automate, and hopefully this inspires you to tackle this piece with Release. Once you bite the bullet and make the effort to automate these small tasks you start to realize how much time is wasted outside of our beloved CLI.\n\nWhat about you? What else have you automated on your day-to-day?\n",
          "code": "var Component=(()=>{var d=Object.create;var i=Object.defineProperty;var u=Object.getOwnPropertyDescriptor;var m=Object.getOwnPropertyNames;var p=Object.getPrototypeOf,g=Object.prototype.hasOwnProperty;var f=(a,e)=>()=>(e||a((e={exports:{}}).exports,e),e.exports),y=(a,e)=>{for(var n in e)i(a,n,{get:e[n],enumerable:!0})},s=(a,e,n,r)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let o of m(e))!g.call(a,o)&&o!==n&&i(a,o,{get:()=>e[o],enumerable:!(r=u(e,o))||r.enumerable});return a};var w=(a,e,n)=>(n=a!=null?d(p(a)):{},s(e||!a||!a.__esModule?i(n,\"default\",{value:a,enumerable:!0}):n,a)),b=a=>s(i({},\"__esModule\",{value:!0}),a);var l=f((R,h)=>{h.exports=_jsx_runtime});var I={};y(I,{default:()=>x,frontmatter:()=>v});var t=w(l()),v={title:\"Automating your day-to-day workflows with the Release CLI\",summary:\"Walk you through a hypothetical bug scenario and how Release can make your day-to-day workflows more enjoyable.\",publishDate:\"Tue Apr 04 2023 18:19:50 GMT+0000 (Coordinated Universal Time)\",author:\"luiz-felipe\",readingTime:4,categories:[\"platform-engineering\",\"product\"],mainImage:\"/blog-images/a07cfb35baa2e31696bd68b5af23bda0.png\",imageAlt:\"Automating your day-to-day workflows with the Release CLI\",showCTA:!0,ctaCopy:\"Automate bug replication and ticket handling with Release's instant environments. Streamline workflows and focus on fixing bugs faster.\",ctaLink:\"https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=automating-your-day-to-day-workflows-with-the-release-cli\",relatedPosts:[\"\"],ogImage:\"/blog-images/a07cfb35baa2e31696bd68b5af23bda0.png\",excerpt:\"Walk you through a hypothetical bug scenario and how Release can make your day-to-day workflows more enjoyable.\",tags:[\"platform-engineering\",\"product\"],ctaButton:\"Try Release for Free\"};function c(a){let e=Object.assign({p:\"p\",a:\"a\",h3:\"h3\",span:\"span\",pre:\"pre\",code:\"code\",strong:\"strong\"},a.components);return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(e.p,{children:\"A typical day for a developer is filled with coming up with great ideas for applications, writing flawless code and seeing everything work exactly as expected, right? Well, sometimes. Often, our days are filled with repetitive tasks, and if you\\u2019re anything like me, once you\\u2019ve done the same task a few times you get an urge to automate it. One of these common repetitive tasks is fixing pesky bugs. Today, I will walk you through a hypothetical bug scenario where we\\u2019ll be using Release for ephemeral environments and Release CLI to make your day-to-day workflows more enjoyable. This might give you some time back to focus on that perfect app!\\xA0\"}),`\n`,(0,t.jsxs)(e.p,{children:[\"Before we begin, make sure to set up your version of Release. You can get a free trial \",(0,t.jsx)(e.a,{href:\"https://beta.release.com/register\",children:\"here\"}),\".\\xA0\"]}),`\n`,(0,t.jsxs)(e.h3,{id:\"the-dreaded-bug\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#the-dreaded-bug\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"The dreaded bug\"]}),`\n`,(0,t.jsx)(e.p,{children:\"When fixing bugs one of the first steps towards a fix is to replicate them. However, doing that can be very tricky. Our development environments are often pristine and perfect (right?) and bugs rarely happen! Somehow customers (or hopefully QA) find new ways to break your application and it\\u2019s our job to try to break it in the same way, so we can fix it for good.\"}),`\n`,(0,t.jsx)(e.p,{children:\"Sometimes you have good error handling and reporting which gives you all of the context needed for a fix. But if that\\u2019s not the case we have to dig deeper to uncover the application state when the error occurred.\"}),`\n`,(0,t.jsx)(e.p,{children:\"But let\\u2019s not get ahead of ourselves here. First things first: we must keep management happy by properly handling our ticket. Sadly this is one of the hardest things to automate because of the sheer diversity of ticketing systems and maturity of these products. Although most of them offer APIs, almost none offer good CLI tools which means you might have to resort to using the dreaded browser/mouse combo.\"}),`\n`,(0,t.jsx)(e.p,{children:\"Here at Release we use Linear which has an API but unfortunately has no CLI tool ready for us to use. Which means we have a choice: do we keep this step manual or do we automate it? I took the plunge and wrote myself a simple script, because at the end of the day, I\\u2019m going to have to deal with these tickets hundreds of times in the future. The few minutes spent automating this will pay off in dividends (and your wrist will thank you too).\"}),`\n`,(0,t.jsxs)(e.p,{children:[\"If you\\u2019re interested the script is \",(0,t.jsx)(e.a,{href:\"https://gist.github.com/Draiken/f23a98de94015b794219d9e64ef5c642\",children:\"available here\"}),\". Maybe you\\u2019re lucky and your ticketing system offers a CLI or you too will have to take the plunge and write something to automate this (you know you want to). Whatever the means, we can use that to tell the world we\\u2019re starting our ticket!\"]}),`\n`,(0,t.jsxs)(e.h3,{id:\"creating-our-environment\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#creating-our-environment\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Creating our environment\"]}),`\n`,(0,t.jsxs)(e.p,{children:[\"Now the first thing we need is an environment to reproduce our bug. Luckily Release offers a feature on top of the ephemeral environments that\\u2019s perfect for these situations: \",(0,t.jsx)(e.a,{href:\"https://docs.release.com/reference-documentation/instant-datasets-aws\",children:\"instant datasets\"}),\". We can spin up environments with production-like data within minutes. Depending on how fresh the snapshot we\\u2019re using is, the data that caused the bug we need to reproduce might be there already.\"]}),`\n`,(0,t.jsxs)(e.p,{children:[\"For this we\\u2019re going to use the \",(0,t.jsx)(e.a,{href:\"https://docs.release.com/cli/getting-started\",children:\"Release CLI\"}),\". Once that\\u2019s setup we\\u2019ll login and we\\u2019ll be ready to create an environment. Here\\u2019s what I use:\"]}),`\n`,(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:\"language-yalm\",children:`\nrelease environments create \\u2013app my_app \\u2013wait | notify-send \\u201CEnvironment ready!\\u201D\n\n# or for our friends that run the fruit system\n\nrelease environments create \\u2013app my_app \\u2013wait | osascript -e 'display notification \"Environment Ready!\"\n\n`})}),`\n`,(0,t.jsx)(e.p,{children:\"Once the environment is up and running we get a notification and we can start our investigation.\"}),`\n`,(0,t.jsx)(e.p,{children:\"Figuring out the state around the bug more often than not requires you to dig through the data either directly on the database or through some sort of application console. Ideally we\\u2019d just have a perfect admin page showing what\\u2019s what, but that\\u2019s not always the case.\"}),`\n`,(0,t.jsxs)(e.p,{children:[\"Our Release CLI allows us to very easily dive right into the environment we just spun up and poke around. For this we\\u2019re going to use the \",(0,t.jsx)(e.strong,{children:\"instances\"}),\" command, which lists the running instances and lets us open up a terminal inside those containers. All we need to do is type:\"]}),`\n`,(0,t.jsx)(e.p,{children:(0,t.jsx)(e.code,{children:\"release instances terminal\"})}),`\n`,(0,t.jsx)(e.p,{children:\"Select our app/instance and boom: we\\u2019re in! If we use something like Rails we can access our console here or alternatively we can connect to the database directly, given this environment is attached to our database and we have the environment variables needed to connect to it.\"}),`\n`,(0,t.jsxs)(e.h3,{id:\"fix-all-the-things\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#fix-all-the-things\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Fix all the things\"]}),`\n`,(0,t.jsxs)(e.p,{children:[\"Now that we\\u2019ve investigated and replicated our bug, we can finally fix it! For this we can use another very handy tool from Release: \",(0,t.jsx)(e.a,{href:\"https://docs.release.com/cli/remote-dev\",children:\"development environments\"}),\". Rather than making changes, deploying them to the environment and then checking it, we can simplify all that by running our local code inside the release environment. Let\\u2019s start our development environment:\"]}),`\n`,(0,t.jsx)(e.p,{children:(0,t.jsx)(e.code,{children:\"release development start \\u2013app my_app \\u2013environment my_env\"})}),`\n`,(0,t.jsx)(e.p,{children:\"This will insert our local machine\\u2019s code into the environment and forward the configured ports right into our localhost. We get the best of both worlds: release is running our code (along all those services it depends on) and we get instant feedback from our changes. We can now fix the issue and close this ticket once and for all.\"}),`\n`,(0,t.jsxs)(e.p,{children:[\"With our changes done, we can commit those in and create our pull request. Tools like GitHub and GitLab provide nice CLIs which make it easy to automate this part. At Release we use GitHub and I personally use the \",(0,t.jsx)(e.a,{href:\"https://github.com/github/hub\",children:\"hub\"}),\" CLI for this.\"]}),`\n`,(0,t.jsx)(e.p,{children:(0,t.jsx)(e.code,{children:\"hub pull-request -o -m \\u201CFix all the things\\u201D\"})}),`\n`,(0,t.jsx)(e.p,{children:\"With our pull request open, we can also update our ticket to the appropriate state, if it\\u2019s not already done automatically. Now we can ask our teammates to review our code, give us the all important \\u201CLGTM\\u201D (Let Go The Mayo!) and merge that bad boy in.\"}),`\n`,(0,t.jsxs)(e.h3,{id:\"automate-it-all\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#automate-it-all\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Automate it all\"]}),`\n`,(0,t.jsx)(e.p,{children:\"There\\u2019s much more in our daily workflow that we can automate, and hopefully this inspires you to tackle this piece with Release. Once you bite the bullet and make the effort to automate these small tasks you start to realize how much time is wasted outside of our beloved CLI.\"}),`\n`,(0,t.jsx)(e.p,{children:\"What about you? What else have you automated on your day-to-day?\"})]})}function k(a={}){let{wrapper:e}=a.components||{};return e?(0,t.jsx)(e,Object.assign({},a,{children:(0,t.jsx)(c,a)})):c(a)}var x=k;return b(I);})();\n;return Component;"
        },
        "_id": "blog/posts/automating-your-day-to-day-workflows-with-the-release-cli.mdx",
        "_raw": {
          "sourceFilePath": "blog/posts/automating-your-day-to-day-workflows-with-the-release-cli.mdx",
          "sourceFileName": "automating-your-day-to-day-workflows-with-the-release-cli.mdx",
          "sourceFileDir": "blog/posts",
          "contentType": "mdx",
          "flattenedPath": "blog/posts/automating-your-day-to-day-workflows-with-the-release-cli"
        },
        "type": "BlogPost",
        "computedSlug": "automating-your-day-to-day-workflows-with-the-release-cli"
      },
      "documentHash": "1739393595016",
      "hasWarnings": false,
      "documentTypeName": "BlogPost"
    },
    "blog/posts/awesome-release-docker-compose-examples-working-in-release-and-kubernetes.mdx": {
      "document": {
        "title": "Awesome-release: Tons of Working Release Examples Running in Hosted Kubernetes",
        "summary": "I Need/Want/Love Examples I know that when I try out a new product, if it’s hard to see what it does quickly I usually",
        "publishDate": "Wed Feb 03 2021 05:02:01 GMT+0000 (Coordinated Universal Time)",
        "author": "tommy-mcclung",
        "readingTime": 2,
        "categories": [
          "kubernetes",
          "platform-engineering"
        ],
        "mainImage": "/blog-images/b4025551da748296b172cbcc181dfe6f.jpg",
        "imageAlt": "A man driving a boat",
        "showCTA": true,
        "ctaCopy": "Simplify environment setup with Release's managed Kubernetes clusters for seamless deployment of working examples. Try now!",
        "ctaLink": "https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=awesome-release-docker-compose-examples-working-in-release-and-kubernetes",
        "relatedPosts": [
          ""
        ],
        "ogImage": "/blog-images/b4025551da748296b172cbcc181dfe6f.jpg",
        "excerpt": "I Need/Want/Love Examples I know that when I try out a new product, if it’s hard to see what it does quickly I usually",
        "tags": [
          "kubernetes",
          "platform-engineering"
        ],
        "ctaButton": "Try Release for Free",
        "body": {
          "raw": "\n### I Need/Want/Love Examples\n\nI know that when I try out a new product, if it’s hard to see what it does quickly I usually move on. This is a shame because I’m sure there are products out there that I could have benefited from but it was just too difficult to get something up and running.\n\nRelease is the easiest way to create environments from any code change, Pull Request or the click of a button. We deploy your environments in a Kubernetes cluster that’s completely managed for you. _But in order to see how Release can change how you build software, you have to get your application running in Release._ Most of the time this is pretty straightforward, but there is definitely a setup curve to get over before you can see our product in full form.\n\nThe best technical products I’ve used always have a TON of great examples. Stripe is definitely the king of this idea… we were just recently integrating their checkout feature into Release and [their examples repo](https://github.com/stripe-samples) is a model for how this can be done. It made our integration with Stripe so much easier than digging through docs to figure this all out.\n\nI think examples are highly under-done, especially with highly technical products. I’m pretty sure I know why (especially after this project): it’s hard to spend the time and energy building out examples when you have features to build. But at Release, examples are a first class citizen and we believe will make getting up and running with Release easier.\n\n### awesome-compose is awesome\n\nAs we started building and testing Release, we came across this fantastic repository, [https://github.com/docker/awesome-compose](https://github.com/docker/awesome-compose), that has community created applications known to work with docker-compose.\n\nSince Release can easily use a docker-compose file to create blueprints for environments, this was a perfect way for us to test out Release and make sure all the various ways docker-compose files are used are supported in the platform. Enter awesome-release.\n\n### awesome-release is awesome++\n\nWe put together an _awesome_ organization in Github with a ton of repositories that just work in Release that are derived from awesome-compose.\n\n[https://github.com/awesome-release](https://github.com/awesome-release) is chock full of amazing projects that just work. The great part about all of these projects is that they show the range of how making simple applications to complex applications can be run and environments created within Release.\n\nSo if you’re getting started and need a project that will just work out of the box, I highly recommend forking or copying any of these repos and use it to create an app in Release. Every repository in _awesome-release_ just works.\n\nWe’ve made modifications to some of the awesome-compose repos that needed slight tweaks. In each README we’ve described what we did to make the project work. Most of these things are minor adjustments that enable the project to run in a hosted Kubernetes environment vs being run locally via docker-compose.\n\n[Click here to signup for Release and give it a shot!](https://releasehub.com/)\n\nPlease let us know if there is an example you’d like to see and we’ll put it together for you. Just send us an email at [support@release.com](mailto:support@release.com) and we’ll add it to the list.\n**Have fun and happy Releasing!**\n",
          "code": "var Component=(()=>{var m=Object.create;var n=Object.defineProperty;var d=Object.getOwnPropertyDescriptor;var u=Object.getOwnPropertyNames;var p=Object.getPrototypeOf,g=Object.prototype.hasOwnProperty;var w=(a,e)=>()=>(e||a((e={exports:{}}).exports,e),e.exports),f=(a,e)=>{for(var o in e)n(a,o,{get:e[o],enumerable:!0})},r=(a,e,o,i)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let s of u(e))!g.call(a,s)&&s!==o&&n(a,s,{get:()=>e[s],enumerable:!(i=d(e,s))||i.enumerable});return a};var y=(a,e,o)=>(o=a!=null?m(p(a)):{},r(e||!a||!a.__esModule?n(o,\"default\",{value:a,enumerable:!0}):o,a)),b=a=>r(n({},\"__esModule\",{value:!0}),a);var c=w((R,l)=>{l.exports=_jsx_runtime});var I={};f(I,{default:()=>x,frontmatter:()=>k});var t=y(c()),k={title:\"Awesome-release: Tons of Working Release Examples Running in Hosted Kubernetes\",summary:\"I Need/Want/Love Examples I know that when I try out a new product, if it\\u2019s hard to see what it does quickly I usually\",publishDate:\"Wed Feb 03 2021 05:02:01 GMT+0000 (Coordinated Universal Time)\",author:\"tommy-mcclung\",readingTime:2,categories:[\"kubernetes\",\"platform-engineering\"],mainImage:\"/blog-images/b4025551da748296b172cbcc181dfe6f.jpg\",imageAlt:\"A man driving a boat\",showCTA:!0,ctaCopy:\"Simplify environment setup with Release's managed Kubernetes clusters for seamless deployment of working examples. Try now!\",ctaLink:\"https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=awesome-release-docker-compose-examples-working-in-release-and-kubernetes\",relatedPosts:[\"\"],ogImage:\"/blog-images/b4025551da748296b172cbcc181dfe6f.jpg\",excerpt:\"I Need/Want/Love Examples I know that when I try out a new product, if it\\u2019s hard to see what it does quickly I usually\",tags:[\"kubernetes\",\"platform-engineering\"],ctaButton:\"Try Release for Free\"};function h(a){let e=Object.assign({h3:\"h3\",a:\"a\",span:\"span\",p:\"p\",em:\"em\",strong:\"strong\"},a.components);return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsxs)(e.h3,{id:\"i-needwantlove-examples\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#i-needwantlove-examples\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"I Need/Want/Love Examples\"]}),`\n`,(0,t.jsx)(e.p,{children:\"I know that when I try out a new product, if it\\u2019s hard to see what it does quickly I usually move on. This is a shame because I\\u2019m sure there are products out there that I could have benefited from but it was just too difficult to get something up and running.\"}),`\n`,(0,t.jsxs)(e.p,{children:[\"Release is the easiest way to create environments from any code change, Pull Request or the click of a button. We deploy your environments in a Kubernetes cluster that\\u2019s completely managed for you. \",(0,t.jsx)(e.em,{children:\"But in order to see how Release can change how you build software, you have to get your application running in Release.\"}),\" Most of the time this is pretty straightforward, but there is definitely a setup curve to get over before you can see our product in full form.\"]}),`\n`,(0,t.jsxs)(e.p,{children:[\"The best technical products I\\u2019ve used always have a TON of great examples. Stripe is definitely the king of this idea\\u2026 we were just recently integrating their checkout feature into Release and \",(0,t.jsx)(e.a,{href:\"https://github.com/stripe-samples\",children:\"their examples repo\"}),\" is a model for how this can be done. It made our integration with Stripe so much easier than digging through docs to figure this all out.\"]}),`\n`,(0,t.jsx)(e.p,{children:\"I think examples are highly under-done, especially with highly technical products. I\\u2019m pretty sure I know why (especially after this project): it\\u2019s hard to spend the time and energy building out examples when you have features to build. But at Release, examples are a first class citizen and we believe will make getting up and running with Release easier.\"}),`\n`,(0,t.jsxs)(e.h3,{id:\"awesome-compose-is-awesome\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#awesome-compose-is-awesome\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"awesome-compose is awesome\"]}),`\n`,(0,t.jsxs)(e.p,{children:[\"As we started building and testing Release, we came across this fantastic repository, \",(0,t.jsx)(e.a,{href:\"https://github.com/docker/awesome-compose\",children:\"https://github.com/docker/awesome-compose\"}),\", that has community created applications known to work with docker-compose.\"]}),`\n`,(0,t.jsx)(e.p,{children:\"Since Release can easily use a docker-compose file to create blueprints for environments, this was a perfect way for us to test out Release and make sure all the various ways docker-compose files are used are supported in the platform. Enter awesome-release.\"}),`\n`,(0,t.jsxs)(e.h3,{id:\"awesome-release-is-awesome\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#awesome-release-is-awesome\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"awesome-release is awesome++\"]}),`\n`,(0,t.jsxs)(e.p,{children:[\"We put together an \",(0,t.jsx)(e.em,{children:\"awesome\"}),\" organization in Github with a ton of repositories that just work in Release that are derived from awesome-compose.\"]}),`\n`,(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.a,{href:\"https://github.com/awesome-release\",children:\"https://github.com/awesome-release\"}),\" is chock full of amazing projects that just work. The great part about all of these projects is that they show the range of how making simple applications to complex applications can be run and environments created within Release.\"]}),`\n`,(0,t.jsxs)(e.p,{children:[\"So if you\\u2019re getting started and need a project that will just work out of the box, I highly recommend forking or copying any of these repos and use it to create an app in Release. Every repository in \",(0,t.jsx)(e.em,{children:\"awesome-release\"}),\" just works.\"]}),`\n`,(0,t.jsx)(e.p,{children:\"We\\u2019ve made modifications to some of the awesome-compose repos that needed slight tweaks. In each README we\\u2019ve described what we did to make the project work. Most of these things are minor adjustments that enable the project to run in a hosted Kubernetes environment vs being run locally via docker-compose.\"}),`\n`,(0,t.jsx)(e.p,{children:(0,t.jsx)(e.a,{href:\"https://releasehub.com/\",children:\"Click here to signup for Release and give it a shot!\"})}),`\n`,(0,t.jsxs)(e.p,{children:[\"Please let us know if there is an example you\\u2019d like to see and we\\u2019ll put it together for you. Just send us an email at \",(0,t.jsx)(e.a,{href:\"mailto:support@release.com\",children:\"support@release.com\"}),` and we\\u2019ll add it to the list.\n`,(0,t.jsx)(e.strong,{children:\"Have fun and happy Releasing!\"})]})]})}function v(a={}){let{wrapper:e}=a.components||{};return e?(0,t.jsx)(e,Object.assign({},a,{children:(0,t.jsx)(h,a)})):h(a)}var x=v;return b(I);})();\n;return Component;"
        },
        "_id": "blog/posts/awesome-release-docker-compose-examples-working-in-release-and-kubernetes.mdx",
        "_raw": {
          "sourceFilePath": "blog/posts/awesome-release-docker-compose-examples-working-in-release-and-kubernetes.mdx",
          "sourceFileName": "awesome-release-docker-compose-examples-working-in-release-and-kubernetes.mdx",
          "sourceFileDir": "blog/posts",
          "contentType": "mdx",
          "flattenedPath": "blog/posts/awesome-release-docker-compose-examples-working-in-release-and-kubernetes"
        },
        "type": "BlogPost",
        "computedSlug": "awesome-release-docker-compose-examples-working-in-release-and-kubernetes"
      },
      "documentHash": "1739393595016",
      "hasWarnings": false,
      "documentTypeName": "BlogPost"
    },
    "blog/posts/aws-re-invent-2023-what-to-expect.mdx": {
      "document": {
        "title": "AWS re:Invent 2023 - What to Expect",
        "summary": "Join Release at AWS re:Invent in Las Vegas ",
        "publishDate": "Wed Nov 15 2023 20:01:39 GMT+0000 (Coordinated Universal Time)",
        "author": "ira-casteel",
        "readingTime": 3,
        "categories": [
          "events",
          "platform-engineering"
        ],
        "mainImage": "/blog-images/5c25faaf55cd7c4b49a1b67fd71f046f.jpg",
        "imageAlt": "AWS re:Invent 2023 - What to Expect",
        "showCTA": true,
        "ctaCopy": "Unlock seamless collaboration and faster testing with Release's on-demand environments, perfect for hands-on AWS re:Invent experiences.",
        "ctaLink": "https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=aws-re-invent-2023-what-to-expect",
        "relatedPosts": [
          ""
        ],
        "ogImage": "/blog-images/5c25faaf55cd7c4b49a1b67fd71f046f.jpg",
        "excerpt": "Join Release at AWS re:Invent in Las Vegas ",
        "tags": [
          "events",
          "platform-engineering"
        ],
        "ctaButton": "Try Release for Free",
        "body": {
          "raw": "\nRelease is heading to AWS re:Invent where we will showcase our latest product developments, share previews of upcoming releases, and network with fellow cloud enthusiasts. Come see us at Booth #503 of the Expo Hall for a custom demo and some Release swag.\n\nAs we gear up for [AWS re:Invent 2023](https://reinvent.awsevents.com/) in Las Vegas (Nov 27 - Dec 1), here's a concise guide to help you navigate and make the most out of this expansive cloud computing event.\n\n‍**Why Go:**\n\n- **Networking Galore:** With over 60,000 attendees, it's a melting pot of cloud enthusiasts, including developers, system administrators, engineers, and IT execs.\n- **Learning Opportunities:** Expect over 2,000 sessions covering a range of topics from AWS innovations to practical DevOps strategies.\n- **Hands-On Experience:** Interactive sessions like Builders’ Sessions and Workshops are perfect for us to get our hands dirty with real-world AWS applications.\n- ‍**Keynote Insights**: Hear from AWS leaders like CEO Adam Selipsky and CTO Dr. Werner Vogels for the latest AWS developments and future directions. See the full list of keynotes [here](https://reinvent.awsevents.com/keynotes/).\n\n**What to Expect:**\n\n- **Diverse Sessions:** Tailored for various expertise levels, these sessions cover everything from AWS basics to advanced topics in various technologies. Use the [Sessions Catalog](https://hub.reinvent.awsevents.com/attendee-portal/catalog/) to plan your agenda. The handy filter on the left side sorts by topic, complexity, and even your role.\n- **Expo Hall:** A great place to see live demos and discuss with AWS partners about their latest solutions. Find Release at Booth #503 of the [Expo](https://reinvent.awsevents.com/learn/expo).\n- **After-Hours Fun:** Don’t miss the [re:Play](https://reinvent.awsevents.com/community/replay) party for some downtime. Las Vegas itself offers a plethora of entertainment options to unwind.\n\n‍**Practical Tips:**\n\n- **Early Arrival:** Consider arriving a day early to settle in and maybe explore Vegas.\n- **Hotel Choices:** Take advantage of special rates for attendees. Options range from the luxury of Encore to the unique ambiance of The Venetian.\n- **Getting Around:** Free campus shuttles and monorail services are available for easy movement between venues.\n\n‍**Maximizing Your Experience:**\n\n- **Set Clear Goals:** The event can be overwhelming, so deciding what you want to achieve up-front can help. Whether it's learning, networking, or exploring business opportunities, set a few goals and set some time aside for exploration.\n- **Leverage Event Intelligence:** Use tools (e.g. AWS events app) to get insights on attendees and sessions. Many things will be happening at the same time, so jotting down a custom agenda will help you navigate the event.\n- **Post-Event Follow-Up:** Reflect on your interactions and plan follow-ups to maintain the momentum.\n\n‍**Final Thoughts:**\n\nAWS re:Invent 2023 isn't just another tech conference; it's one of the largest outlets for anyone in the cloud computing and DevOps space to engage. It's an opportunity to stay ahead in the industry, learn from the best, and make connections that matter.\n\nSee you in Vegas!\n",
          "code": "var Component=(()=>{var d=Object.create;var r=Object.defineProperty;var g=Object.getOwnPropertyDescriptor;var p=Object.getOwnPropertyNames;var u=Object.getPrototypeOf,m=Object.prototype.hasOwnProperty;var f=(t,e)=>()=>(e||t((e={exports:{}}).exports,e),e.exports),v=(t,e)=>{for(var s in e)r(t,s,{get:e[s],enumerable:!0})},a=(t,e,s,i)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let o of p(e))!m.call(t,o)&&o!==s&&r(t,o,{get:()=>e[o],enumerable:!(i=g(e,o))||i.enumerable});return t};var w=(t,e,s)=>(s=t!=null?d(u(t)):{},a(e||!t||!t.__esModule?r(s,\"default\",{value:t,enumerable:!0}):s,t)),y=t=>a(r({},\"__esModule\",{value:!0}),t);var c=f((I,l)=>{l.exports=_jsx_runtime});var A={};v(A,{default:()=>W,frontmatter:()=>x});var n=w(c()),x={title:\"AWS re:Invent 2023 - What to Expect\",summary:\"Join Release at AWS re:Invent in Las Vegas \",publishDate:\"Wed Nov 15 2023 20:01:39 GMT+0000 (Coordinated Universal Time)\",author:\"ira-casteel\",readingTime:3,categories:[\"events\",\"platform-engineering\"],mainImage:\"/blog-images/5c25faaf55cd7c4b49a1b67fd71f046f.jpg\",imageAlt:\"AWS re:Invent 2023 - What to Expect\",showCTA:!0,ctaCopy:\"Unlock seamless collaboration and faster testing with Release's on-demand environments, perfect for hands-on AWS re:Invent experiences.\",ctaLink:\"https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=aws-re-invent-2023-what-to-expect\",relatedPosts:[\"\"],ogImage:\"/blog-images/5c25faaf55cd7c4b49a1b67fd71f046f.jpg\",excerpt:\"Join Release at AWS re:Invent in Las Vegas \",tags:[\"events\",\"platform-engineering\"],ctaButton:\"Try Release for Free\"};function h(t){let e=Object.assign({p:\"p\",a:\"a\",strong:\"strong\",ul:\"ul\",li:\"li\"},t.components);return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(e.p,{children:\"Release is heading to AWS re:Invent where we will showcase our latest product developments, share previews of upcoming releases, and network with fellow cloud enthusiasts. Come see us at Booth #503 of the Expo Hall for a custom demo and some Release swag.\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"As we gear up for \",(0,n.jsx)(e.a,{href:\"https://reinvent.awsevents.com/\",children:\"AWS re:Invent 2023\"}),\" in Las Vegas (Nov 27 - Dec 1), here's a concise guide to help you navigate and make the most out of this expansive cloud computing event.\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"\\u200D\",(0,n.jsx)(e.strong,{children:\"Why Go:\"})]}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"Networking Galore:\"}),\" With over 60,000 attendees, it's a melting pot of cloud enthusiasts, including developers, system administrators, engineers, and IT execs.\"]}),`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"Learning Opportunities:\"}),\" Expect over 2,000 sessions covering a range of topics from AWS innovations to practical DevOps strategies.\"]}),`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"Hands-On Experience:\"}),\" Interactive sessions like Builders\\u2019 Sessions and Workshops are perfect for us to get our hands dirty with real-world AWS applications.\"]}),`\n`,(0,n.jsxs)(e.li,{children:[\"\\u200D\",(0,n.jsx)(e.strong,{children:\"Keynote Insights\"}),\": Hear from AWS leaders like CEO Adam Selipsky and CTO Dr. Werner Vogels for the latest AWS developments and future directions. See the full list of keynotes \",(0,n.jsx)(e.a,{href:\"https://reinvent.awsevents.com/keynotes/\",children:\"here\"}),\".\"]}),`\n`]}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.strong,{children:\"What to Expect:\"})}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"Diverse Sessions:\"}),\" Tailored for various expertise levels, these sessions cover everything from AWS basics to advanced topics in various technologies. Use the \",(0,n.jsx)(e.a,{href:\"https://hub.reinvent.awsevents.com/attendee-portal/catalog/\",children:\"Sessions Catalog\"}),\" to plan your agenda. The handy filter on the left side sorts by topic, complexity, and even your role.\"]}),`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"Expo Hall:\"}),\" A great place to see live demos and discuss with AWS partners about their latest solutions. Find Release at Booth #503 of the \",(0,n.jsx)(e.a,{href:\"https://reinvent.awsevents.com/learn/expo\",children:\"Expo\"}),\".\"]}),`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"After-Hours Fun:\"}),\" Don\\u2019t miss the \",(0,n.jsx)(e.a,{href:\"https://reinvent.awsevents.com/community/replay\",children:\"re:Play\"}),\" party for some downtime. Las Vegas itself offers a plethora of entertainment options to unwind.\"]}),`\n`]}),`\n`,(0,n.jsxs)(e.p,{children:[\"\\u200D\",(0,n.jsx)(e.strong,{children:\"Practical Tips:\"})]}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"Early Arrival:\"}),\" Consider arriving a day early to settle in and maybe explore Vegas.\"]}),`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"Hotel Choices:\"}),\" Take advantage of special rates for attendees. Options range from the luxury of Encore to the unique ambiance of The Venetian.\"]}),`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"Getting Around:\"}),\" Free campus shuttles and monorail services are available for easy movement between venues.\"]}),`\n`]}),`\n`,(0,n.jsxs)(e.p,{children:[\"\\u200D\",(0,n.jsx)(e.strong,{children:\"Maximizing Your Experience:\"})]}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"Set Clear Goals:\"}),\" The event can be overwhelming, so deciding what you want to achieve up-front can help. Whether it's learning, networking, or exploring business opportunities, set a few goals and set some time aside for exploration.\"]}),`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"Leverage Event Intelligence:\"}),\" Use tools (e.g. AWS events app) to get insights on attendees and sessions. Many things will be happening at the same time, so jotting down a custom agenda will help you navigate the event.\"]}),`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"Post-Event Follow-Up:\"}),\" Reflect on your interactions and plan follow-ups to maintain the momentum.\"]}),`\n`]}),`\n`,(0,n.jsxs)(e.p,{children:[\"\\u200D\",(0,n.jsx)(e.strong,{children:\"Final Thoughts:\"})]}),`\n`,(0,n.jsx)(e.p,{children:\"AWS re:Invent 2023 isn't just another tech conference; it's one of the largest outlets for anyone in the cloud computing and DevOps space to engage. It's an opportunity to stay ahead in the industry, learn from the best, and make connections that matter.\"}),`\n`,(0,n.jsx)(e.p,{children:\"See you in Vegas!\"})]})}function b(t={}){let{wrapper:e}=t.components||{};return e?(0,n.jsx)(e,Object.assign({},t,{children:(0,n.jsx)(h,t)})):h(t)}var W=b;return y(A);})();\n;return Component;"
        },
        "_id": "blog/posts/aws-re-invent-2023-what-to-expect.mdx",
        "_raw": {
          "sourceFilePath": "blog/posts/aws-re-invent-2023-what-to-expect.mdx",
          "sourceFileName": "aws-re-invent-2023-what-to-expect.mdx",
          "sourceFileDir": "blog/posts",
          "contentType": "mdx",
          "flattenedPath": "blog/posts/aws-re-invent-2023-what-to-expect"
        },
        "type": "BlogPost",
        "computedSlug": "aws-re-invent-2023-what-to-expect"
      },
      "documentHash": "1739393595016",
      "hasWarnings": false,
      "documentTypeName": "BlogPost"
    },
    "blog/posts/beyond-k8s-introduction-to-ephemeral-environments.mdx": {
      "document": {
        "title": "Beyond K8s: Introduction to Ephemeral Environments",
        "summary": "Everything you need to know about Ephemeral Environments",
        "publishDate": "Thu Feb 10 2022 19:55:34 GMT+0000 (Coordinated Universal Time)",
        "author": "tommy-mcclung",
        "readingTime": 1,
        "categories": [
          "platform-engineering",
          "kubernetes"
        ],
        "mainImage": "/blog-images/7cdfe0cce22f9827989363208a2c2488.jpg",
        "imageAlt": "Beyond K8s: Introduction to Ephemeral Environments",
        "showCTA": true,
        "ctaCopy": "Simplify environment setup with Release: automate ephemeral environments for seamless production mirroring and faster testing cycles.",
        "ctaLink": "https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=beyond-k8s-introduction-to-ephemeral-environments",
        "relatedPosts": [
          ""
        ],
        "ogImage": "/blog-images/7cdfe0cce22f9827989363208a2c2488.jpg",
        "excerpt": "Everything you need to know about Ephemeral Environments",
        "tags": [
          "platform-engineering",
          "kubernetes"
        ],
        "ctaButton": "Try Release for Free",
        "body": {
          "raw": "\nReplicating the production environment to pre-production is the key to higher quality code and more frequent releases. Kubernetes is great, but with today’s complex application process, production environments are not just about the application itself. They are about cloud native services such as lambda, databases such as RDS, name servers, and more.  \nIn this webinar, we discuss how to set up your environments that are as close to production as possible and will explain how to do that so that your environment is embedded with your development process, available on-the-fly for developers, all while removing the bottlenecks associated with a single staging environment.  \nIn this webinar, you will learn:\n\n\\- When to use shared environments and when to use ephemeral environments  \n\\- How to set up the right data in the right environment  \n\\- About shifting left – bringing production environments to the developer branch\n\n[Watch the webinar on demand.](https://www.bigmarker.com/techwell-corporation/Beyond-K8s-Introduction-to-Ephemeral-Environments)\n\n‍\n",
          "code": "var Component=(()=>{var h=Object.create;var a=Object.defineProperty;var u=Object.getOwnPropertyDescriptor;var d=Object.getOwnPropertyNames;var p=Object.getPrototypeOf,g=Object.prototype.hasOwnProperty;var b=(t,e)=>()=>(e||t((e={exports:{}}).exports,e),e.exports),v=(t,e)=>{for(var o in e)a(t,o,{get:e[o],enumerable:!0})},s=(t,e,o,i)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let r of d(e))!g.call(t,r)&&r!==o&&a(t,r,{get:()=>e[r],enumerable:!(i=u(e,r))||i.enumerable});return t};var y=(t,e,o)=>(o=t!=null?h(p(t)):{},s(e||!t||!t.__esModule?a(o,\"default\",{value:t,enumerable:!0}):o,t)),w=t=>s(a({},\"__esModule\",{value:!0}),t);var c=b((k,m)=>{m.exports=_jsx_runtime});var _={};v(_,{default:()=>E,frontmatter:()=>f});var n=y(c()),f={title:\"Beyond K8s: Introduction to Ephemeral Environments\",summary:\"Everything you need to know about Ephemeral Environments\",publishDate:\"Thu Feb 10 2022 19:55:34 GMT+0000 (Coordinated Universal Time)\",author:\"tommy-mcclung\",readingTime:1,categories:[\"platform-engineering\",\"kubernetes\"],mainImage:\"/blog-images/7cdfe0cce22f9827989363208a2c2488.jpg\",imageAlt:\"Beyond K8s: Introduction to Ephemeral Environments\",showCTA:!0,ctaCopy:\"Simplify environment setup with Release: automate ephemeral environments for seamless production mirroring and faster testing cycles.\",ctaLink:\"https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=beyond-k8s-introduction-to-ephemeral-environments\",relatedPosts:[\"\"],ogImage:\"/blog-images/7cdfe0cce22f9827989363208a2c2488.jpg\",excerpt:\"Everything you need to know about Ephemeral Environments\",tags:[\"platform-engineering\",\"kubernetes\"],ctaButton:\"Try Release for Free\"};function l(t){let e=Object.assign({p:\"p\",br:\"br\",a:\"a\"},t.components);return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsxs)(e.p,{children:[\"Replicating the production environment to pre-production is the key to higher quality code and more frequent releases. Kubernetes is great, but with today\\u2019s complex application process, production environments are not just about the application itself. They are about cloud native services such as lambda, databases such as RDS, name servers, and more.\",(0,n.jsx)(e.br,{}),`\n`,\"In this webinar, we discuss how to set up your environments that are as close to production as possible and will explain how to do that so that your environment is embedded with your development process, available on-the-fly for developers, all while removing the bottlenecks associated with a single staging environment.\",(0,n.jsx)(e.br,{}),`\n`,\"In this webinar, you will learn:\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"- When to use shared environments and when to use ephemeral environments\",(0,n.jsx)(e.br,{}),`\n`,\"- How to set up the right data in the right environment\",(0,n.jsx)(e.br,{}),`\n`,\"- About shifting left \\u2013 bringing production environments to the developer branch\"]}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.a,{href:\"https://www.bigmarker.com/techwell-corporation/Beyond-K8s-Introduction-to-Ephemeral-Environments\",children:\"Watch the webinar on demand.\"})}),`\n`,(0,n.jsx)(e.p,{children:\"\\u200D\"})]})}function x(t={}){let{wrapper:e}=t.components||{};return e?(0,n.jsx)(e,Object.assign({},t,{children:(0,n.jsx)(l,t)})):l(t)}var E=x;return w(_);})();\n;return Component;"
        },
        "_id": "blog/posts/beyond-k8s-introduction-to-ephemeral-environments.mdx",
        "_raw": {
          "sourceFilePath": "blog/posts/beyond-k8s-introduction-to-ephemeral-environments.mdx",
          "sourceFileName": "beyond-k8s-introduction-to-ephemeral-environments.mdx",
          "sourceFileDir": "blog/posts",
          "contentType": "mdx",
          "flattenedPath": "blog/posts/beyond-k8s-introduction-to-ephemeral-environments"
        },
        "type": "BlogPost",
        "computedSlug": "beyond-k8s-introduction-to-ephemeral-environments"
      },
      "documentHash": "1739393595016",
      "hasWarnings": false,
      "documentTypeName": "BlogPost"
    },
    "blog/posts/boost-developer-happiness-with-ephemeral-environments.mdx": {
      "document": {
        "title": "Boost Developer Happiness with Ephemeral Environments",
        "summary": "Ephemeral environments boost developer happiness by enabling quick, autonomous, and consistent setups delighting devs.",
        "publishDate": "Wed Oct 30 2024 22:17:44 GMT+0000 (Coordinated Universal Time)",
        "author": "tommy-mcclung",
        "readingTime": 5,
        "categories": [
          "platform-engineering",
          "product"
        ],
        "mainImage": "/blog-images/86d3b5f1d0989a43021e7acce0eee8de.webp",
        "imageAlt": "Happy Developers using Ephemeral Environments",
        "showCTA": true,
        "ctaCopy": "Unlock developer happiness with on-demand, isolated environments like ephemeral setups. Streamline workflows with Release's environment management platform.",
        "ctaLink": "https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=boost-developer-happiness-with-ephemeral-environments",
        "relatedPosts": [
          "how-debtbook-ships-6x-faster-with-release; ephemeral-environments-9-tips-for-seamless-deployment; 6-software-development-environment-best-practices"
        ],
        "ogImage": "/blog-images/86d3b5f1d0989a43021e7acce0eee8de.webp",
        "excerpt": "Ephemeral environments boost developer happiness by enabling quick, autonomous, and consistent setups delighting devs.",
        "tags": [
          "platform-engineering",
          "product"
        ],
        "ctaButton": "Try Release for Free",
        "body": {
          "raw": "\n## Boosting Developer Happiness with Ephemeral Environments: A Game Changer for Developer Experience Teams\n\nIn today's fast-paced development landscape, keeping developers happy isn't just a nicety—it’s a necessity. Happy developers are more productive, write better code, and contribute to a positive team culture. Developer Experience (DevEx) teams are constantly seeking tools and practices that enhance satisfaction and streamline workflows. One such innovation that's making waves is the use of **ephemeral environments**. But what are they, and how can they align with your team's charter to amplify developer happiness? Let’s dive in.\n\n### What Are Ephemeral Environments?\n\nEphemeral environments are temporary, on-demand computing spaces that developers can spin up and tear down effortlessly. Think of them as isolated sandboxes that replicate production environments without the overhead of long-term maintenance. They are typically used for testing, feature development, and experimentation, providing a unique blend of flexibility, consistency, and speed.\n\n### Why Ephemeral Environments Matter for Developer Happiness\n\n#### Instant Gratification with On-Demand Resources\n\nWaiting is the enemy of productivity. Ephemeral environments allow developers to get immediate access to the resources they need. No more waiting for shared environments to be available or for lengthy setup processes. This instant access reduces frustration and keeps momentum going.\n\n#### Reduced \"It Works on My Machine\" Syndrome\n\nBy providing consistent, isolated environments, ephemeral setups minimize discrepancies between development and production. This consistency means fewer bugs slip through the cracks and less time is spent debugging environment-specific issues.\n\n#### Empowerment Through Autonomy\n\nDevelopers can spin up their own environments without needing to coordinate with other teams. This autonomy not only speeds up the development process but also gives developers a sense of ownership over their work.\n\n#### Safe Space for Experimentation\n\nEphemeral environments are perfect for testing new ideas without the risk of affecting the main codebase or shared resources. This safety net encourages innovation and creative problem-solving.\n\n### Real-World Success Story: DebtBook’s Transformation with Ephemeral Environments\n\nDebtBook, a leading provider of debt and lease management software for public sector organizations, faced challenges with its development and testing environments. With complex systems and dependencies, DebtBook’s team often struggled with environment setup and maintenance, causing delays and impacting productivity.\n\nBy adopting **Release's** ephemeral environments, DebtBook streamlined its workflow and enabled developers to quickly spin up isolated environments tailored for specific features or testing needs. This shift allowed developers to deploy changes to isolated environments at any stage, ensuring consistent, reliable, and reproducible setups that closely mirrored production. The results were significant:\n\n\\- **Reduced Time to Environment Setup**: Developers could now focus on coding rather than waiting for environment setup, accelerating productivity.\n\n‍  \n\\- **Improved Collaboration**: With consistent environments available to all team members, communication and collaboration became seamless, reducing misalignment caused by environment inconsistencies.\n\n‍  \n\\- **Faster Releases**: By simplifying testing and reducing setup times, DebtBook was able to reduce its overall release cycle and deliver value to clients faster.\n\nThis real-world example illustrates how ephemeral environments can be transformative, providing measurable improvements in efficiency and satisfaction. [Read the DebtBook case study.](https://release.com/casestudy/debtbook)\n\n### Getting Started with Ephemeral Environments\n\nGetting started with ephemeral environments can be simplified with tools like Release, which provides comprehensive documentation and guidelines. Here’s a quick guide to help DevEx teams kickstart ephemeral environments based on [Release’s documentation](https://docs.release.com/):\n\n#### 1\\. Set Up the Environment Template\n\nStart by defining templates for your environments. These templates can include configurations, services, and dependencies. This setup ensures consistency across all environments and saves time when developers need to quickly spin up a new one.\n\n#### 2\\. Integrate with Your CI/CD Pipeline\n\nIntegrate your ephemeral environments with your existing CI/CD pipelines. This integration enables automated provisioning and teardown of environments for each feature branch or pull request, so developers can test their code in isolated, production-like setups on demand.\n\n#### 3\\. Automate Environment Teardowns\n\nTo prevent environment sprawl and control costs, set up automated teardown rules. Ephemeral environments are designed to be short-lived, so automating the teardown ensures that no environment outlasts its usefulness and resources are optimally utilized.\n\n#### 4\\. Monitor and Gather Feedback\n\nUse monitoring tools and gather feedback from developers to understand how these environments are impacting their workflow. Insights from monitoring and direct feedback can help refine your approach and make adjustments that further enhance developer experience.\n\n#### 5\\. Educate Your Team\n\nProvide training sessions or documentation to help your developers make the most of ephemeral environments. The more comfortable they are with the tools, the more benefits you'll reap.\n\n### Conclusion\n\nEphemeral environments are more than just a trendy buzzword—they’re a powerful tool that can significantly enhance developer happiness and productivity. For Developer Experience teams committed to improving workflows and satisfaction, embracing ephemeral environments aligns perfectly with your charter. By reducing friction, empowering developers, and promoting best practices, you’re not just making your team’s life easier; you’re driving better business outcomes.\n**So, are you ready to make your developers happier than ever? Embrace ephemeral environments and watch your team's productivity soar!** [**Signup for Release for free.**](https://release.com/signup)\n\n‍\n",
          "code": "var Component=(()=>{var m=Object.create;var o=Object.defineProperty;var h=Object.getOwnPropertyDescriptor;var p=Object.getOwnPropertyNames;var u=Object.getPrototypeOf,v=Object.prototype.hasOwnProperty;var g=(t,e)=>()=>(e||t((e={exports:{}}).exports,e),e.exports),f=(t,e)=>{for(var a in e)o(t,a,{get:e[a],enumerable:!0})},s=(t,e,a,r)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let i of p(e))!v.call(t,i)&&i!==a&&o(t,i,{get:()=>e[i],enumerable:!(r=h(e,i))||r.enumerable});return t};var y=(t,e,a)=>(a=t!=null?m(u(t)):{},s(e||!t||!t.__esModule?o(a,\"default\",{value:t,enumerable:!0}):a,t)),w=t=>s(o({},\"__esModule\",{value:!0}),t);var l=g((T,c)=>{c.exports=_jsx_runtime});var N={};f(N,{default:()=>E,frontmatter:()=>b});var n=y(l()),b={title:\"Boost Developer Happiness with Ephemeral Environments\",summary:\"Ephemeral environments boost developer happiness by enabling quick, autonomous, and consistent setups delighting devs.\",publishDate:\"Wed Oct 30 2024 22:17:44 GMT+0000 (Coordinated Universal Time)\",author:\"tommy-mcclung\",readingTime:5,categories:[\"platform-engineering\",\"product\"],mainImage:\"/blog-images/86d3b5f1d0989a43021e7acce0eee8de.webp\",imageAlt:\"Happy Developers using Ephemeral Environments\",showCTA:!0,ctaCopy:\"Unlock developer happiness with on-demand, isolated environments like ephemeral setups. Streamline workflows with Release's environment management platform.\",ctaLink:\"https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=boost-developer-happiness-with-ephemeral-environments\",relatedPosts:[\"how-debtbook-ships-6x-faster-with-release; ephemeral-environments-9-tips-for-seamless-deployment; 6-software-development-environment-best-practices\"],ogImage:\"/blog-images/86d3b5f1d0989a43021e7acce0eee8de.webp\",excerpt:\"Ephemeral environments boost developer happiness by enabling quick, autonomous, and consistent setups delighting devs.\",tags:[\"platform-engineering\",\"product\"],ctaButton:\"Try Release for Free\"};function d(t){let e=Object.assign({h2:\"h2\",a:\"a\",span:\"span\",p:\"p\",strong:\"strong\",h3:\"h3\",h4:\"h4\",br:\"br\"},t.components);return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsxs)(e.h2,{id:\"boosting-developer-happiness-with-ephemeral-environments-a-game-changer-for-developer-experience-teams\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#boosting-developer-happiness-with-ephemeral-environments-a-game-changer-for-developer-experience-teams\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Boosting Developer Happiness with Ephemeral Environments: A Game Changer for Developer Experience Teams\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"In today's fast-paced development landscape, keeping developers happy isn't just a nicety\\u2014it\\u2019s a necessity. Happy developers are more productive, write better code, and contribute to a positive team culture. Developer Experience (DevEx) teams are constantly seeking tools and practices that enhance satisfaction and streamline workflows. One such innovation that's making waves is the use of \",(0,n.jsx)(e.strong,{children:\"ephemeral environments\"}),\". But what are they, and how can they align with your team's charter to amplify developer happiness? Let\\u2019s dive in.\"]}),`\n`,(0,n.jsxs)(e.h3,{id:\"what-are-ephemeral-environments\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#what-are-ephemeral-environments\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"What Are Ephemeral Environments?\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Ephemeral environments are temporary, on-demand computing spaces that developers can spin up and tear down effortlessly. Think of them as isolated sandboxes that replicate production environments without the overhead of long-term maintenance. They are typically used for testing, feature development, and experimentation, providing a unique blend of flexibility, consistency, and speed.\"}),`\n`,(0,n.jsxs)(e.h3,{id:\"why-ephemeral-environments-matter-for-developer-happiness\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#why-ephemeral-environments-matter-for-developer-happiness\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Why Ephemeral Environments Matter for Developer Happiness\"]}),`\n`,(0,n.jsxs)(e.h4,{id:\"instant-gratification-with-on-demand-resources\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#instant-gratification-with-on-demand-resources\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Instant Gratification with On-Demand Resources\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Waiting is the enemy of productivity. Ephemeral environments allow developers to get immediate access to the resources they need. No more waiting for shared environments to be available or for lengthy setup processes. This instant access reduces frustration and keeps momentum going.\"}),`\n`,(0,n.jsxs)(e.h4,{id:\"reduced-it-works-on-my-machine-syndrome\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#reduced-it-works-on-my-machine-syndrome\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),'Reduced \"It Works on My Machine\" Syndrome']}),`\n`,(0,n.jsx)(e.p,{children:\"By providing consistent, isolated environments, ephemeral setups minimize discrepancies between development and production. This consistency means fewer bugs slip through the cracks and less time is spent debugging environment-specific issues.\"}),`\n`,(0,n.jsxs)(e.h4,{id:\"empowerment-through-autonomy\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#empowerment-through-autonomy\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Empowerment Through Autonomy\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Developers can spin up their own environments without needing to coordinate with other teams. This autonomy not only speeds up the development process but also gives developers a sense of ownership over their work.\"}),`\n`,(0,n.jsxs)(e.h4,{id:\"safe-space-for-experimentation\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#safe-space-for-experimentation\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Safe Space for Experimentation\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Ephemeral environments are perfect for testing new ideas without the risk of affecting the main codebase or shared resources. This safety net encourages innovation and creative problem-solving.\"}),`\n`,(0,n.jsxs)(e.h3,{id:\"real-world-success-story-debtbooks-transformation-with-ephemeral-environments\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#real-world-success-story-debtbooks-transformation-with-ephemeral-environments\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Real-World Success Story: DebtBook\\u2019s Transformation with Ephemeral Environments\"]}),`\n`,(0,n.jsx)(e.p,{children:\"DebtBook, a leading provider of debt and lease management software for public sector organizations, faced challenges with its development and testing environments. With complex systems and dependencies, DebtBook\\u2019s team often struggled with environment setup and maintenance, causing delays and impacting productivity.\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"By adopting \",(0,n.jsx)(e.strong,{children:\"Release's\"}),\" ephemeral environments, DebtBook streamlined its workflow and enabled developers to quickly spin up isolated environments tailored for specific features or testing needs. This shift allowed developers to deploy changes to isolated environments at any stage, ensuring consistent, reliable, and reproducible setups that closely mirrored production. The results were significant:\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"- \",(0,n.jsx)(e.strong,{children:\"Reduced Time to Environment Setup\"}),\": Developers could now focus on coding rather than waiting for environment setup, accelerating productivity.\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"\\u200D\",(0,n.jsx)(e.br,{}),`\n`,\"- \",(0,n.jsx)(e.strong,{children:\"Improved Collaboration\"}),\": With consistent environments available to all team members, communication and collaboration became seamless, reducing misalignment caused by environment inconsistencies.\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"\\u200D\",(0,n.jsx)(e.br,{}),`\n`,\"- \",(0,n.jsx)(e.strong,{children:\"Faster Releases\"}),\": By simplifying testing and reducing setup times, DebtBook was able to reduce its overall release cycle and deliver value to clients faster.\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"This real-world example illustrates how ephemeral environments can be transformative, providing measurable improvements in efficiency and satisfaction. \",(0,n.jsx)(e.a,{href:\"https://release.com/casestudy/debtbook\",children:\"Read the DebtBook case study.\"})]}),`\n`,(0,n.jsxs)(e.h3,{id:\"getting-started-with-ephemeral-environments\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#getting-started-with-ephemeral-environments\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Getting Started with Ephemeral Environments\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"Getting started with ephemeral environments can be simplified with tools like Release, which provides comprehensive documentation and guidelines. Here\\u2019s a quick guide to help DevEx teams kickstart ephemeral environments based on \",(0,n.jsx)(e.a,{href:\"https://docs.release.com/\",children:\"Release\\u2019s documentation\"}),\":\"]}),`\n`,(0,n.jsxs)(e.h4,{id:\"1-set-up-the-environment-template\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#1-set-up-the-environment-template\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"1. Set Up the Environment Template\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Start by defining templates for your environments. These templates can include configurations, services, and dependencies. This setup ensures consistency across all environments and saves time when developers need to quickly spin up a new one.\"}),`\n`,(0,n.jsxs)(e.h4,{id:\"2-integrate-with-your-cicd-pipeline\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#2-integrate-with-your-cicd-pipeline\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"2. Integrate with Your CI/CD Pipeline\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Integrate your ephemeral environments with your existing CI/CD pipelines. This integration enables automated provisioning and teardown of environments for each feature branch or pull request, so developers can test their code in isolated, production-like setups on demand.\"}),`\n`,(0,n.jsxs)(e.h4,{id:\"3-automate-environment-teardowns\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#3-automate-environment-teardowns\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"3. Automate Environment Teardowns\"]}),`\n`,(0,n.jsx)(e.p,{children:\"To prevent environment sprawl and control costs, set up automated teardown rules. Ephemeral environments are designed to be short-lived, so automating the teardown ensures that no environment outlasts its usefulness and resources are optimally utilized.\"}),`\n`,(0,n.jsxs)(e.h4,{id:\"4-monitor-and-gather-feedback\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#4-monitor-and-gather-feedback\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"4. Monitor and Gather Feedback\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Use monitoring tools and gather feedback from developers to understand how these environments are impacting their workflow. Insights from monitoring and direct feedback can help refine your approach and make adjustments that further enhance developer experience.\"}),`\n`,(0,n.jsxs)(e.h4,{id:\"5-educate-your-team\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#5-educate-your-team\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"5. Educate Your Team\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Provide training sessions or documentation to help your developers make the most of ephemeral environments. The more comfortable they are with the tools, the more benefits you'll reap.\"}),`\n`,(0,n.jsxs)(e.h3,{id:\"conclusion\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#conclusion\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Conclusion\"]}),`\n`,(0,n.jsxs)(e.p,{children:[`Ephemeral environments are more than just a trendy buzzword\\u2014they\\u2019re a powerful tool that can significantly enhance developer happiness and productivity. For Developer Experience teams committed to improving workflows and satisfaction, embracing ephemeral environments aligns perfectly with your charter. By reducing friction, empowering developers, and promoting best practices, you\\u2019re not just making your team\\u2019s life easier; you\\u2019re driving better business outcomes.\n`,(0,n.jsx)(e.strong,{children:\"So, are you ready to make your developers happier than ever? Embrace ephemeral environments and watch your team's productivity soar!\"}),\" \",(0,n.jsx)(e.a,{href:\"https://release.com/signup\",children:(0,n.jsx)(e.strong,{children:\"Signup for Release for free.\"})})]}),`\n`,(0,n.jsx)(e.p,{children:\"\\u200D\"})]})}function k(t={}){let{wrapper:e}=t.components||{};return e?(0,n.jsx)(e,Object.assign({},t,{children:(0,n.jsx)(d,t)})):d(t)}var E=k;return w(N);})();\n;return Component;"
        },
        "_id": "blog/posts/boost-developer-happiness-with-ephemeral-environments.mdx",
        "_raw": {
          "sourceFilePath": "blog/posts/boost-developer-happiness-with-ephemeral-environments.mdx",
          "sourceFileName": "boost-developer-happiness-with-ephemeral-environments.mdx",
          "sourceFileDir": "blog/posts",
          "contentType": "mdx",
          "flattenedPath": "blog/posts/boost-developer-happiness-with-ephemeral-environments"
        },
        "type": "BlogPost",
        "computedSlug": "boost-developer-happiness-with-ephemeral-environments"
      },
      "documentHash": "1739393595016",
      "hasWarnings": false,
      "documentTypeName": "BlogPost"
    },
    "blog/posts/build-vs-buy-where-to-focus-your-energy-with-idps.mdx": {
      "document": {
        "title": "Build vs. Buy: Where to Focus Your Energy with IDPs",
        "summary": "Let’s explore a significant decision in every software endeavor: do we build or do we buy?",
        "publishDate": "Tue Sep 12 2023 17:44:15 GMT+0000 (Coordinated Universal Time)",
        "author": "sylvia-fronczak",
        "readingTime": 10,
        "categories": [
          "platform-engineering",
          "product"
        ],
        "mainImage": "/blog-images/1b6998ea6557914087a8b77eda4fe01b.jpg",
        "imageAlt": "Credit: Google DeepMind",
        "showCTA": true,
        "ctaCopy": "Empower your team to focus on building with Release's ephemeral environments for faster deployment cycles.",
        "ctaLink": "https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=build-vs-buy-where-to-focus-your-energy-with-idps",
        "relatedPosts": [
          "components-of-a-successful-idp-build-a-product-your-developers-actually-want-to-use"
        ],
        "ogImage": "/blog-images/1b6998ea6557914087a8b77eda4fe01b.jpg",
        "excerpt": "Let’s explore a significant decision in every software endeavor: do we build or do we buy?",
        "tags": [
          "platform-engineering",
          "product"
        ],
        "ctaButton": "Try Release for Free",
        "body": {
          "raw": "\nPreviously, we wrote about the goals and outcomes of [an Internal Developer Platform (IDP](https://release.com/blog/what-is-an-internal-developer-platform-and-why-should-i-have-one)), as well as the [components of a successful IDP](https://release.com/blog/components-of-a-successful-idp-build-a-product-your-developers-actually-want-to-use). At this point, you may be either excited by or overwhelmed with where your platform could go and the efficiencies that it could create.\n\nAs the next step, let’s explore a significant decision in every software endeavor: do we build or do we buy?\n\nConsidering this is rarely a clean “yes or no” question, we’re also going to look at the options that sit between the two. In actuality, the solution you choose will be somewhere on a large spectrum between a complete build from scratch and entirely buying software that you never have to maintain yourself. Where your org falls on that spectrum depends on your current needs and capabilities.\n\nFirst, let’s look at some of the options available.\n\n#### Build or Buy, or Else?\n\nWhen we make decisions around build vs. buy, we should first realize that when it comes to an IDP, it’s not all or nothing. This is a spectrum, where you can buy a managed IDP and all related components or you can pick and choose what you build and what you buy. As a reminder, an IDP isn’t one solution, but a platform with various components that provide solutions to different problems. Therefore, you can choose to build or buy the platform and the separate components.\n\n![](/blog-images/1863e6f1f0c7adae1cc585f2ba216bd3.png)\n\n‍**🛠️ Fully Build the IDP and Integrations**\n\nThe most involved option would be to build everything yourself from scratch. This will be the most expensive option and will require a large team, but in theory, it will give you the most flexibility.\n\nSpotify did this when they built Backstage. Now, you may think that that’s the way to go. But do realize that Spotify had a large number of people working on this both internally and later externally when the platform was released as open source. Once they went open source, they had both individual contributors and partner companies that provided time and engineering efforts toward the build. Their [two-year anniversary called out 5,000 contributors](https://backstage.io/blog/2022/03/16/backstage-turns-two/) to the project.\n\nMost companies do not have hundreds, let alone thousands, of developers who can put in time and effort to building an IDP from scratch. In addition to the large number of resources, that level of contribution requires forgoing other projects within your organization that involve shipping features. As importantly, this platform is just that. A platform. Once you have the platform built, you will then need to configure the necessary components and add-ons that your organization needs. Each of these components will result in a build vs. buy decision, so focus on the most important features for your organization first.\n\n##### ‍**🏗️ Build Atop Open-Source Frameworks and Commercial Components**\n\nIf you’re not at the stage where you can build everything (and most organizations aren’t), another option would be to take an open-source framework and build on top of that.\n\nIn this scenario, you’re using something like backstage.io but building out the implementation.\n\nThis is a better option over a complete build as the base framework. However, you’ll still need to build integrations and quickly form opinions on how this should work for your organization.\n\nMany prefer this to the build option, but it will still take a considerable team to make it happen. And it’s not the ideal choice if you’re just starting to learn about IDPs and how they can help your org.\n\nFor example, companies could choose a managed observability component like Datadog or New Relic. These tools aren’t cheap but will get your organization started with solid monitoring and observability. However, as costs increase, you could find yourself building your observability tools over open-source libraries like Prometheus and Grafana.\n\nComing at this from a similar angle, you could handle your environment management in-house using Ansible, Terraform, or Docker Compose. You might begin by building out your environment management using these tools as the solution seems simple. As the complexity of your environment management increases, and as the problems you need to solve become more sophisticated, choosing an ephemeral environment solution like Release can reduce the complexity for your teams.\n\n![](/blog-images/2939d24e6fa24544309c8c562fa26486.png)\n\n##### **🤝Hire Someone to Build It**\n\nFor some software projects, you could consider hiring out the build. For example, you could hire third-party contractors or consultants to build your IDP. Or you could directly hire new employees with the necessary experience and onboard them into your organization to build it out.\n\nOrganizations will take this option if they don’t have the necessary knowledge in-house. Though this can work, it often has poor outcomes. The folks coming into the org don’t know the culture and processes of your teams and will need to learn them, or they’ll build something not based on your internal culture and processes. Frequently, for third-party development teams, they don’t have the level of ownership needed for projects of this size. And hiring new employees for this work will also increase the burden on hiring resources. And if we don’t have existing employees with this expertise, we will oftentimes make expensive hiring decisions.\n\nTo mitigate some of the risks with these options, you could have a small number of experts build alongside your team so that you have more of your own long-term workforce working on the project than you have temporary parties. That would be the only way I’d consider this option.\n\n##### ‍**💰Buy It: Vendor-Managed Options**\n\nBuying enterprise software isn’t a plug-it-in-and-forget-it endeavor. There will still be work involved in configuration, management, and onboarding teams. This is even more true for IDPs, as they are simply platforms and will need the right components and integrations added on to make a usable product.\n\nEven though there’s not a complete buy-it option, buying the right components that provide value to your org will reduce the development burden on your teams. You’ll be able to focus on the integrations that solve your organization’s biggest development workflow pain points, which can free you up to decide how you want to incorporate the use of the IDP into your organizational culture.\n\nAdditionally, for an IDP, you have managed solutions available for a number of components. For example, there are organizations that provide managed IDP solutions like [Cortex](https://www.cortex.io/) or [OpsLevel](https://www.opslevel.com/) for service catalogs and integrations to other tools.\n\nYou can also consider whether it makes sense to buy components and capabilities that extend your IDP. Let’s consider the monitoring and observability components of your IDP. We already mentioned purchasing products like Datadog or New Relic. You can further this by adding paging options like [PagerDuty](https://www.pagerduty.com/) or [Opsgenie](https://www.atlassian.com/software/opsgenie). These problems have robust solutions on the market that can fill your organization’s needs.\n\nWhen it comes to environment management, Release provides a managed platform as a service (PAAS) solution for managing both standard and ephemeral environments. This provides additional building blocks for your IDP, adding environment management capabilities to further increase your development team efficiency. And yes, you could build it yourself, scripting atop open-source solutions like [Terraform](https://www.terraform.io/) or [Docker Compose](https://docs.docker.com/compose/). But again, these types of products are not something that will take a few days and still scale well.\n\nWhen considering what components to build or buy, consider how the value compares to the effort. What components can provide big benefits to your engineering teams that will take a lot of time and effort to build? And consider your most pressing needs. Do you need a central place to locate tools and services? How about robust monitoring tools to operate your systems? Or environment management to increase developer velocity? And which do you want to benefit from quickly?\n\n![](/blog-images/cd72cd05beb337ab5405213445b5211a.png)\n\n#### Build vs. Buy Factors to Consider\n\nWe’ve gone over many alternatives in the build, build on open source, hire out, and buy spectrum. There isn’t a one-size-fits-all solution, and you’ll have to consider what’s best for your organization.\n\nBut there are a number of things to consider when deciding if you will build or buy your IDP.\n\n##### 1\\. Cost\n\nCost isn’t just the amount of money that you’ll spend on the software licenses or the developer headcount to build the IDP. If you buy software, you’ll still have to spend time on training, evangelizing, and configuring the IDP for your organization. If you build, you’ll need to include engineering time as well as the training, evangelizing, and configuring just like when you buy. Building also includes opportunity costs. What could your engineering staff be building to further differentiate your product in the market? Should you focus their energies on your product or on integrations that all organizations need?\n\n##### 2\\. Expertise and Capabilities\n\nBuilding software like an IDP requires a team with the right skills and expertise. And don’t think your team of five production engineers can take on this effort. Organizations like Netflix, Facebook, and Spotify can build tools like this in-house because they have hundreds, if not more, of employees working on internal systems and efficiencies. These folks are dedicated to improving the developer experience.\n\nIf you have a small development team focused on application or product development, developing a [production engineering](https://engineering.fb.com/category/production-engineering/) or platform enablement team will not work well. It won’t take advantage of their expertise in your domain and will take them away from building the product that is your market differentiator.\n\n##### 3\\. Scalability\n\nWith the build option, you do control the ability to scale the needs of the system yourself to your specific loads and use cases. However, consider whether you could start with buying a managed IDP option and then move over to build after you’ve outgrown the capabilities and scale of that option.\n\n##### 4\\. Custom Integrations\n\nIf your organization has its own custom-built tools for testing, security, governance, or more, integrations through an IDP may be more difficult. Or you’ll potentially have the same amount of effort between the build and the buy option.\n\n##### 5\\. Security and Compliance\n\nWhen you build the software, you can control the security and compliance features. But that also means you must be the experts in security and compliance. This loops back to #2. Do you have the expertise, or do you need to offload that need for expertise to another organization where it’s their product differentiator?\n\n##### 6\\. Competitive Advantage\n\nIf you’re an org like Spotify, Netflix, Meta, or similar, then squeezing every bit of efficiency out of your development team by ensuring everything is custom-built for their needs makes sense. But if you’re not at that scale, consider starting smaller.\n\n##### 7\\. Usability and Adoption\n\nIf you build a custom IDP but don’t consider usability, then you will have poor adoption. This means not only UI concerns of the IDP but also usability of configurations and interactions with the developer workflow.\n\n##### 8\\. Support\n\nCustom-built solutions won’t have an internet full of adopters that use the tool or similar tools for debugging, tutorials, and instructions. Buying software solutions through a vendor-managed option means you don’t have to create a large support network and can offload much of that to the wider developer ecosystem.\n\n##### 9\\. Course Correction\n\nOne final factor involves transitioning from one decision to another. Consider this. Would it be easier and more economical to move from a “buy” solution that doesn’t meet your needs or from a “build” solution that fails? If you decide to build but find that this is not maintainable or salable, then the effort put into building will be wasted.\n\nIf you find that a buy solution doesn’t work for your needs, yes, you’ll still have sunk costs, but they should be more manageable. Depending on the components involved and your expertise, your answer can vary.\n\nHowever, if you start with a buy solution or use a PaaS or managed solution, you can transition to build when you’re ready. You’ll build up the experience needed and the understanding of what you need from the product. For something like an IDP that consists of many integrated components, you can build piece by piece, taking on more of the build as your organization’s expertise and understanding of the component grows.\n\n#### Making a Decision\n\nMaking the right decision depends on many factors that vary across organizations. Consider what’s best for your situation today and focus on solving for the 80% before deciding to move to a build solution. This may consist of a combination of building, building on top of open source, buying, or hiring out.\n\nIn the next installment of this series, we’ll talk about using product thinking to establish your IDP. Stay tuned!\n\n‍*This post was written by Sylvia Fronczak.* [_Sylvia_](https://sylviafronczak.com/) _is a software developer who has worked in various industries with various software methodologies. She’s currently focused on design practices that the whole team can own, understand, and evolve over time._\n",
          "code": "var Component=(()=>{var h=Object.create;var a=Object.defineProperty;var u=Object.getOwnPropertyDescriptor;var p=Object.getOwnPropertyNames;var m=Object.getPrototypeOf,y=Object.prototype.hasOwnProperty;var f=(n,e)=>()=>(e||n((e={exports:{}}).exports,e),e.exports),g=(n,e)=>{for(var t in e)a(n,t,{get:e[t],enumerable:!0})},s=(n,e,t,r)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let i of p(e))!y.call(n,i)&&i!==t&&a(n,i,{get:()=>e[i],enumerable:!(r=u(e,i))||r.enumerable});return n};var b=(n,e,t)=>(t=n!=null?h(m(n)):{},s(e||!n||!n.__esModule?a(t,\"default\",{value:n,enumerable:!0}):t,n)),w=n=>s(a({},\"__esModule\",{value:!0}),n);var d=f((N,l)=>{l.exports=_jsx_runtime});var x={};g(x,{default:()=>I,frontmatter:()=>v});var o=b(d()),v={title:\"Build vs. Buy: Where to Focus Your Energy with IDPs\",summary:\"Let\\u2019s explore a significant decision in every software endeavor: do we build or do we buy?\",publishDate:\"Tue Sep 12 2023 17:44:15 GMT+0000 (Coordinated Universal Time)\",author:\"sylvia-fronczak\",readingTime:10,categories:[\"platform-engineering\",\"product\"],mainImage:\"/blog-images/1b6998ea6557914087a8b77eda4fe01b.jpg\",imageAlt:\"Credit: Google DeepMind\",showCTA:!0,ctaCopy:\"Empower your team to focus on building with Release's ephemeral environments for faster deployment cycles.\",ctaLink:\"https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=build-vs-buy-where-to-focus-your-energy-with-idps\",relatedPosts:[\"components-of-a-successful-idp-build-a-product-your-developers-actually-want-to-use\"],ogImage:\"/blog-images/1b6998ea6557914087a8b77eda4fe01b.jpg\",excerpt:\"Let\\u2019s explore a significant decision in every software endeavor: do we build or do we buy?\",tags:[\"platform-engineering\",\"product\"],ctaButton:\"Try Release for Free\"};function c(n){let e=Object.assign({p:\"p\",a:\"a\",h4:\"h4\",span:\"span\",img:\"img\",strong:\"strong\",h5:\"h5\",em:\"em\"},n.components);return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsxs)(e.p,{children:[\"Previously, we wrote about the goals and outcomes of \",(0,o.jsx)(e.a,{href:\"https://release.com/blog/what-is-an-internal-developer-platform-and-why-should-i-have-one\",children:\"an Internal Developer Platform (IDP\"}),\"), as well as the \",(0,o.jsx)(e.a,{href:\"https://release.com/blog/components-of-a-successful-idp-build-a-product-your-developers-actually-want-to-use\",children:\"components of a successful IDP\"}),\". At this point, you may be either excited by or overwhelmed with where your platform could go and the efficiencies that it could create.\"]}),`\n`,(0,o.jsx)(e.p,{children:\"As the next step, let\\u2019s explore a significant decision in every software endeavor: do we build or do we buy?\"}),`\n`,(0,o.jsx)(e.p,{children:\"Considering this is rarely a clean \\u201Cyes or no\\u201D question, we\\u2019re also going to look at the options that sit between the two. In actuality, the solution you choose will be somewhere on a large spectrum between a complete build from scratch and entirely buying software that you never have to maintain yourself. Where your org falls on that spectrum depends on your current needs and capabilities.\"}),`\n`,(0,o.jsx)(e.p,{children:\"First, let\\u2019s look at some of the options available.\"}),`\n`,(0,o.jsxs)(e.h4,{id:\"build-or-buy-or-else\",children:[(0,o.jsx)(e.a,{className:\"anchor\",href:\"#build-or-buy-or-else\",children:(0,o.jsx)(e.span,{className:\"icon icon-link\"})}),\"Build or Buy, or Else?\"]}),`\n`,(0,o.jsx)(e.p,{children:\"When we make decisions around build vs. buy, we should first realize that when it comes to an IDP, it\\u2019s not all or nothing. This is a spectrum, where you can buy a managed IDP and all related components or you can pick and choose what you build and what you buy. As a reminder, an IDP isn\\u2019t one solution, but a platform with various components that provide solutions to different problems. Therefore, you can choose to build or buy the platform and the separate components.\"}),`\n`,(0,o.jsx)(e.p,{children:(0,o.jsx)(e.img,{src:\"/blog-images/1863e6f1f0c7adae1cc585f2ba216bd3.png\",alt:\"\"})}),`\n`,(0,o.jsxs)(e.p,{children:[\"\\u200D\",(0,o.jsx)(e.strong,{children:\"\\u{1F6E0}\\uFE0F Fully Build the IDP and Integrations\"})]}),`\n`,(0,o.jsx)(e.p,{children:\"The most involved option would be to build everything yourself from scratch. This will be the most expensive option and will require a large team, but in theory, it will give you the most flexibility.\"}),`\n`,(0,o.jsxs)(e.p,{children:[\"Spotify did this when they built Backstage. Now, you may think that that\\u2019s the way to go. But do realize that Spotify had a large number of people working on this both internally and later externally when the platform was released as open source. Once they went open source, they had both individual contributors and partner companies that provided time and engineering efforts toward the build. Their \",(0,o.jsx)(e.a,{href:\"https://backstage.io/blog/2022/03/16/backstage-turns-two/\",children:\"two-year anniversary called out 5,000 contributors\"}),\" to the project.\"]}),`\n`,(0,o.jsx)(e.p,{children:\"Most companies do not have hundreds, let alone thousands, of developers who can put in time and effort to building an IDP from scratch. In addition to the large number of resources, that level of contribution requires forgoing other projects within your organization that involve shipping features. As importantly, this platform is just that. A platform. Once you have the platform built, you will then need to configure the necessary components and add-ons that your organization needs. Each of these components will result in a build vs. buy decision, so focus on the most important features for your organization first.\"}),`\n`,(0,o.jsxs)(e.h5,{id:\"\\uFE0F-build-atop-open-source-frameworks-and-commercial-components\",children:[(0,o.jsx)(e.a,{className:\"anchor\",href:\"#\\uFE0F-build-atop-open-source-frameworks-and-commercial-components\",children:(0,o.jsx)(e.span,{className:\"icon icon-link\"})}),\"\\u200D\",(0,o.jsx)(e.strong,{children:\"\\u{1F3D7}\\uFE0F Build Atop Open-Source Frameworks and Commercial Components\"})]}),`\n`,(0,o.jsx)(e.p,{children:\"If you\\u2019re not at the stage where you can build everything (and most organizations aren\\u2019t), another option would be to take an open-source framework and build on top of that.\"}),`\n`,(0,o.jsx)(e.p,{children:\"In this scenario, you\\u2019re using something like backstage.io but building out the implementation.\"}),`\n`,(0,o.jsx)(e.p,{children:\"This is a better option over a complete build as the base framework. However, you\\u2019ll still need to build integrations and quickly form opinions on how this should work for your organization.\"}),`\n`,(0,o.jsx)(e.p,{children:\"Many prefer this to the build option, but it will still take a considerable team to make it happen. And it\\u2019s not the ideal choice if you\\u2019re just starting to learn about IDPs and how they can help your org.\"}),`\n`,(0,o.jsx)(e.p,{children:\"For example, companies could choose a managed observability component like Datadog or New Relic. These tools aren\\u2019t cheap but will get your organization started with solid monitoring and observability. However, as costs increase, you could find yourself building your observability tools over open-source libraries like Prometheus and Grafana.\"}),`\n`,(0,o.jsx)(e.p,{children:\"Coming at this from a similar angle, you could handle your environment management in-house using Ansible, Terraform, or Docker Compose. You might begin by building out your environment management using these tools as the solution seems simple. As the complexity of your environment management increases, and as the problems you need to solve become more sophisticated, choosing an ephemeral environment solution like Release can reduce the complexity for your teams.\"}),`\n`,(0,o.jsx)(e.p,{children:(0,o.jsx)(e.img,{src:\"/blog-images/2939d24e6fa24544309c8c562fa26486.png\",alt:\"\"})}),`\n`,(0,o.jsxs)(e.h5,{id:\"hire-someone-to-build-it\",children:[(0,o.jsx)(e.a,{className:\"anchor\",href:\"#hire-someone-to-build-it\",children:(0,o.jsx)(e.span,{className:\"icon icon-link\"})}),(0,o.jsx)(e.strong,{children:\"\\u{1F91D}Hire Someone to Build It\"})]}),`\n`,(0,o.jsx)(e.p,{children:\"For some software projects, you could consider hiring out the build. For example, you could hire third-party contractors or consultants to build your IDP. Or you could directly hire new employees with the necessary experience and onboard them into your organization to build it out.\"}),`\n`,(0,o.jsx)(e.p,{children:\"Organizations will take this option if they don\\u2019t have the necessary knowledge in-house. Though this can work, it often has poor outcomes. The folks coming into the org don\\u2019t know the culture and processes of your teams and will need to learn them, or they\\u2019ll build something not based on your internal culture and processes. Frequently, for third-party development teams, they don\\u2019t have the level of ownership needed for projects of this size. And hiring new employees for this work will also increase the burden on hiring resources. And if we don\\u2019t have existing employees with this expertise, we will oftentimes make expensive hiring decisions.\"}),`\n`,(0,o.jsx)(e.p,{children:\"To mitigate some of the risks with these options, you could have a small number of experts build alongside your team so that you have more of your own long-term workforce working on the project than you have temporary parties. That would be the only way I\\u2019d consider this option.\"}),`\n`,(0,o.jsxs)(e.h5,{id:\"buy-it-vendor-managed-options\",children:[(0,o.jsx)(e.a,{className:\"anchor\",href:\"#buy-it-vendor-managed-options\",children:(0,o.jsx)(e.span,{className:\"icon icon-link\"})}),\"\\u200D\",(0,o.jsx)(e.strong,{children:\"\\u{1F4B0}Buy It: Vendor-Managed Options\"})]}),`\n`,(0,o.jsx)(e.p,{children:\"Buying enterprise software isn\\u2019t a plug-it-in-and-forget-it endeavor. There will still be work involved in configuration, management, and onboarding teams. This is even more true for IDPs, as they are simply platforms and will need the right components and integrations added on to make a usable product.\"}),`\n`,(0,o.jsx)(e.p,{children:\"Even though there\\u2019s not a complete buy-it option, buying the right components that provide value to your org will reduce the development burden on your teams. You\\u2019ll be able to focus on the integrations that solve your organization\\u2019s biggest development workflow pain points, which can free you up to decide how you want to incorporate the use of the IDP into your organizational culture.\"}),`\n`,(0,o.jsxs)(e.p,{children:[\"Additionally, for an IDP, you have managed solutions available for a number of components. For example, there are organizations that provide managed IDP solutions like \",(0,o.jsx)(e.a,{href:\"https://www.cortex.io/\",children:\"Cortex\"}),\" or \",(0,o.jsx)(e.a,{href:\"https://www.opslevel.com/\",children:\"OpsLevel\"}),\" for service catalogs and integrations to other tools.\"]}),`\n`,(0,o.jsxs)(e.p,{children:[\"You can also consider whether it makes sense to buy components and capabilities that extend your IDP. Let\\u2019s consider the monitoring and observability components of your IDP. We already mentioned purchasing products like Datadog or New Relic. You can further this by adding paging options like \",(0,o.jsx)(e.a,{href:\"https://www.pagerduty.com/\",children:\"PagerDuty\"}),\" or \",(0,o.jsx)(e.a,{href:\"https://www.atlassian.com/software/opsgenie\",children:\"Opsgenie\"}),\". These problems have robust solutions on the market that can fill your organization\\u2019s needs.\"]}),`\n`,(0,o.jsxs)(e.p,{children:[\"When it comes to environment management, Release provides a managed platform as a service (PAAS) solution for managing both standard and ephemeral environments. This provides additional building blocks for your IDP, adding environment management capabilities to further increase your development team efficiency. And yes, you could build it yourself, scripting atop open-source solutions like \",(0,o.jsx)(e.a,{href:\"https://www.terraform.io/\",children:\"Terraform\"}),\" or \",(0,o.jsx)(e.a,{href:\"https://docs.docker.com/compose/\",children:\"Docker Compose\"}),\". But again, these types of products are not something that will take a few days and still scale well.\"]}),`\n`,(0,o.jsx)(e.p,{children:\"When considering what components to build or buy, consider how the value compares to the effort. What components can provide big benefits to your engineering teams that will take a lot of time and effort to build? And consider your most pressing needs. Do you need a central place to locate tools and services? How about robust monitoring tools to operate your systems? Or environment management to increase developer velocity? And which do you want to benefit from quickly?\"}),`\n`,(0,o.jsx)(e.p,{children:(0,o.jsx)(e.img,{src:\"/blog-images/cd72cd05beb337ab5405213445b5211a.png\",alt:\"\"})}),`\n`,(0,o.jsxs)(e.h4,{id:\"build-vs-buy-factors-to-consider\",children:[(0,o.jsx)(e.a,{className:\"anchor\",href:\"#build-vs-buy-factors-to-consider\",children:(0,o.jsx)(e.span,{className:\"icon icon-link\"})}),\"Build vs. Buy Factors to Consider\"]}),`\n`,(0,o.jsx)(e.p,{children:\"We\\u2019ve gone over many alternatives in the build, build on open source, hire out, and buy spectrum. There isn\\u2019t a one-size-fits-all solution, and you\\u2019ll have to consider what\\u2019s best for your organization.\"}),`\n`,(0,o.jsx)(e.p,{children:\"But there are a number of things to consider when deciding if you will build or buy your IDP.\"}),`\n`,(0,o.jsxs)(e.h5,{id:\"1-cost\",children:[(0,o.jsx)(e.a,{className:\"anchor\",href:\"#1-cost\",children:(0,o.jsx)(e.span,{className:\"icon icon-link\"})}),\"1. Cost\"]}),`\n`,(0,o.jsx)(e.p,{children:\"Cost isn\\u2019t just the amount of money that you\\u2019ll spend on the software licenses or the developer headcount to build the IDP. If you buy software, you\\u2019ll still have to spend time on training, evangelizing, and configuring the IDP for your organization. If you build, you\\u2019ll need to include engineering time as well as the training, evangelizing, and configuring just like when you buy. Building also includes opportunity costs. What could your engineering staff be building to further differentiate your product in the market? Should you focus their energies on your product or on integrations that all organizations need?\"}),`\n`,(0,o.jsxs)(e.h5,{id:\"2-expertise-and-capabilities\",children:[(0,o.jsx)(e.a,{className:\"anchor\",href:\"#2-expertise-and-capabilities\",children:(0,o.jsx)(e.span,{className:\"icon icon-link\"})}),\"2. Expertise and Capabilities\"]}),`\n`,(0,o.jsx)(e.p,{children:\"Building software like an IDP requires a team with the right skills and expertise. And don\\u2019t think your team of five production engineers can take on this effort. Organizations like Netflix, Facebook, and Spotify can build tools like this in-house because they have hundreds, if not more, of employees working on internal systems and efficiencies. These folks are dedicated to improving the developer experience.\"}),`\n`,(0,o.jsxs)(e.p,{children:[\"If you have a small development team focused on application or product development, developing a \",(0,o.jsx)(e.a,{href:\"https://engineering.fb.com/category/production-engineering/\",children:\"production engineering\"}),\" or platform enablement team will not work well. It won\\u2019t take advantage of their expertise in your domain and will take them away from building the product that is your market differentiator.\"]}),`\n`,(0,o.jsxs)(e.h5,{id:\"3-scalability\",children:[(0,o.jsx)(e.a,{className:\"anchor\",href:\"#3-scalability\",children:(0,o.jsx)(e.span,{className:\"icon icon-link\"})}),\"3. Scalability\"]}),`\n`,(0,o.jsx)(e.p,{children:\"With the build option, you do control the ability to scale the needs of the system yourself to your specific loads and use cases. However, consider whether you could start with buying a managed IDP option and then move over to build after you\\u2019ve outgrown the capabilities and scale of that option.\"}),`\n`,(0,o.jsxs)(e.h5,{id:\"4-custom-integrations\",children:[(0,o.jsx)(e.a,{className:\"anchor\",href:\"#4-custom-integrations\",children:(0,o.jsx)(e.span,{className:\"icon icon-link\"})}),\"4. Custom Integrations\"]}),`\n`,(0,o.jsx)(e.p,{children:\"If your organization has its own custom-built tools for testing, security, governance, or more, integrations through an IDP may be more difficult. Or you\\u2019ll potentially have the same amount of effort between the build and the buy option.\"}),`\n`,(0,o.jsxs)(e.h5,{id:\"5-security-and-compliance\",children:[(0,o.jsx)(e.a,{className:\"anchor\",href:\"#5-security-and-compliance\",children:(0,o.jsx)(e.span,{className:\"icon icon-link\"})}),\"5. Security and Compliance\"]}),`\n`,(0,o.jsx)(e.p,{children:\"When you build the software, you can control the security and compliance features. But that also means you must be the experts in security and compliance. This loops back to #2. Do you have the expertise, or do you need to offload that need for expertise to another organization where it\\u2019s their product differentiator?\"}),`\n`,(0,o.jsxs)(e.h5,{id:\"6-competitive-advantage\",children:[(0,o.jsx)(e.a,{className:\"anchor\",href:\"#6-competitive-advantage\",children:(0,o.jsx)(e.span,{className:\"icon icon-link\"})}),\"6. Competitive Advantage\"]}),`\n`,(0,o.jsx)(e.p,{children:\"If you\\u2019re an org like Spotify, Netflix, Meta, or similar, then squeezing every bit of efficiency out of your development team by ensuring everything is custom-built for their needs makes sense. But if you\\u2019re not at that scale, consider starting smaller.\"}),`\n`,(0,o.jsxs)(e.h5,{id:\"7-usability-and-adoption\",children:[(0,o.jsx)(e.a,{className:\"anchor\",href:\"#7-usability-and-adoption\",children:(0,o.jsx)(e.span,{className:\"icon icon-link\"})}),\"7. Usability and Adoption\"]}),`\n`,(0,o.jsx)(e.p,{children:\"If you build a custom IDP but don\\u2019t consider usability, then you will have poor adoption. This means not only UI concerns of the IDP but also usability of configurations and interactions with the developer workflow.\"}),`\n`,(0,o.jsxs)(e.h5,{id:\"8-support\",children:[(0,o.jsx)(e.a,{className:\"anchor\",href:\"#8-support\",children:(0,o.jsx)(e.span,{className:\"icon icon-link\"})}),\"8. Support\"]}),`\n`,(0,o.jsx)(e.p,{children:\"Custom-built solutions won\\u2019t have an internet full of adopters that use the tool or similar tools for debugging, tutorials, and instructions. Buying software solutions through a vendor-managed option means you don\\u2019t have to create a large support network and can offload much of that to the wider developer ecosystem.\"}),`\n`,(0,o.jsxs)(e.h5,{id:\"9-course-correction\",children:[(0,o.jsx)(e.a,{className:\"anchor\",href:\"#9-course-correction\",children:(0,o.jsx)(e.span,{className:\"icon icon-link\"})}),\"9. Course Correction\"]}),`\n`,(0,o.jsx)(e.p,{children:\"One final factor involves transitioning from one decision to another. Consider this. Would it be easier and more economical to move from a \\u201Cbuy\\u201D solution that doesn\\u2019t meet your needs or from a \\u201Cbuild\\u201D solution that fails? If you decide to build but find that this is not maintainable or salable, then the effort put into building will be wasted.\"}),`\n`,(0,o.jsx)(e.p,{children:\"If you find that a buy solution doesn\\u2019t work for your needs, yes, you\\u2019ll still have sunk costs, but they should be more manageable. Depending on the components involved and your expertise, your answer can vary.\"}),`\n`,(0,o.jsx)(e.p,{children:\"However, if you start with a buy solution or use a PaaS or managed solution, you can transition to build when you\\u2019re ready. You\\u2019ll build up the experience needed and the understanding of what you need from the product. For something like an IDP that consists of many integrated components, you can build piece by piece, taking on more of the build as your organization\\u2019s expertise and understanding of the component grows.\"}),`\n`,(0,o.jsxs)(e.h4,{id:\"making-a-decision\",children:[(0,o.jsx)(e.a,{className:\"anchor\",href:\"#making-a-decision\",children:(0,o.jsx)(e.span,{className:\"icon icon-link\"})}),\"Making a Decision\"]}),`\n`,(0,o.jsx)(e.p,{children:\"Making the right decision depends on many factors that vary across organizations. Consider what\\u2019s best for your situation today and focus on solving for the 80% before deciding to move to a build solution. This may consist of a combination of building, building on top of open source, buying, or hiring out.\"}),`\n`,(0,o.jsx)(e.p,{children:\"In the next installment of this series, we\\u2019ll talk about using product thinking to establish your IDP. Stay tuned!\"}),`\n`,(0,o.jsxs)(e.p,{children:[\"\\u200D\",(0,o.jsx)(e.em,{children:\"This post was written by Sylvia Fronczak.\"}),\" \",(0,o.jsx)(e.a,{href:\"https://sylviafronczak.com/\",children:(0,o.jsx)(e.em,{children:\"Sylvia\"})}),\" \",(0,o.jsx)(e.em,{children:\"is a software developer who has worked in various industries with various software methodologies. She\\u2019s currently focused on design practices that the whole team can own, understand, and evolve over time.\"})]})]})}function k(n={}){let{wrapper:e}=n.components||{};return e?(0,o.jsx)(e,Object.assign({},n,{children:(0,o.jsx)(c,n)})):c(n)}var I=k;return w(x);})();\n;return Component;"
        },
        "_id": "blog/posts/build-vs-buy-where-to-focus-your-energy-with-idps.mdx",
        "_raw": {
          "sourceFilePath": "blog/posts/build-vs-buy-where-to-focus-your-energy-with-idps.mdx",
          "sourceFileName": "build-vs-buy-where-to-focus-your-energy-with-idps.mdx",
          "sourceFileDir": "blog/posts",
          "contentType": "mdx",
          "flattenedPath": "blog/posts/build-vs-buy-where-to-focus-your-energy-with-idps"
        },
        "type": "BlogPost",
        "computedSlug": "build-vs-buy-where-to-focus-your-energy-with-idps"
      },
      "documentHash": "1739393595016",
      "hasWarnings": false,
      "documentTypeName": "BlogPost"
    },
    "blog/posts/cache-bundle-install-with-buildkit.mdx": {
      "document": {
        "title": "Cache Bundle Install with BuildKit",
        "summary": "",
        "publishDate": "Mon May 01 2023 20:56:45 GMT+0000 (Coordinated Universal Time)",
        "author": "jeremy-kreutzbender",
        "readingTime": 4,
        "categories": [
          "platform-engineering",
          "product"
        ],
        "mainImage": "/blog-images/54f242bfc5acedea0993fbf3735bc4af.png",
        "imageAlt": "Cache Bundle Install with BuildKit",
        "showCTA": true,
        "ctaCopy": "Discover how Release's ephemeral environments streamline caching for faster builds and efficient testing.",
        "ctaLink": "https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=cache-bundle-install-with-buildkit",
        "relatedPosts": [
          ""
        ],
        "ogImage": "/blog-images/54f242bfc5acedea0993fbf3735bc4af.png",
        "excerpt": "At Release, we've been using BuildKit to do our own builds for some time now and BuildKit does an awesome job of caching Docker image layers! But one thing t...",
        "tags": [
          "platform-engineering",
          "product"
        ],
        "ctaButton": "Try Release for Free",
        "body": {
          "raw": "\nAt Release, we've been using BuildKit to do our own builds for some time now and BuildKit does an awesome job of caching Docker image layers! But one thing that continued to slow down our builds was running `bundle install` when upgrading gems in our Rails application. I decided to start researching if there was any way that we could **cache our `bundle install` commands over many builds** because a cold `bundle install` is very slow, but **incremental changes are quite fast**. In my search, I came across a KubeCon video about BuildKit, [\"Running Cache-Efficient Builds at Scale on Kubernetes with BuildKit - Gautier Delorme, Apple Inc.\"](https://www.youtube.com/watch?v=wTENRhYt3mw) which had a very interesting slide.\n\n![](/blog-images/271d7cec8f533d977b812f76403497cd.png)\n\nThis is exactly the type of solution I was looking for and it comes built in with BuildKit!\n\nLet's take a look at an example from the [mount cache documentation](https://docs.docker.com/engine/reference/builder/#run---mounttypecache) to start.\n\nExample: cache Go packages\n\n```ruby line-numbers\n\n# syntax=docker/dockerfile:1\n\nFROM golang\nRUN --mount=type=cache,target=/root/.cache/go-build \\\n  go build ...\n\n```\n\nIn this example, the `go build` command uses the `/root/.cache/go-build` directory to store the packages in between builds. Because the output of `go build` is a binary and does not require anything to run besides that binary, this example makes correct use of the cache. If we think of the cache directory as a named volume from the host server into the container we can create a picture of how this example works. When `go build` is run, the cache directory is populated on the host server and the resulting binary ends up in the container. The problem is that this mount cache functionality wasn't built with the idea that the packages in the cache needed to be pulled into the resulting image.\n\nTo solve this problem I continued my search and came across this issue on the BuildKit repository, \"[Am I misunderstanding RUN mount=type=cache?](https://github.com/moby/buildkit/issues/1173)\". The writer of the issue explains how this functionality isn't working with `bundle install` and is trying to figure out what to do. In one of the answers a link to blog post in Japanese is provided, \"[Dockerfile for Rails6のベストプラクティスを解説](https://qiita.com/k_kind/items/836bc7ba2e33dc2ed3e7)\". A Dockerfile is provided in the post, which has the solution we've been searching for.\n\n```ruby line-numbers\n\nWORKDIR /app\n…\nRUN bundle config set app_config .bundle\nRUN bundle config set path .cache/bundle\n\n# mount cacheを利用する\n\nRUN --mount=type=cache,uid=1000,target=/app/.cache/bundle \\\n    bundle install && \\\n    mkdir -p vendor && \\\n    cp -ar .cache/bundle vendor/bundle\nRUN bundle config set path vendor/bundle\n\n```\n\nNow that we know what the solution is, let's go through it line by line to make sure we fully understand what is happening.\n\n```ruby line-numbers\n\nWORKDIR /app\n\n```\n\nFirst, we set the working directory for the Dockerfile to app\n\n```ruby line-numbers\n\nRUN bundle config set app_config .bundle\n\n```\n\nNext, we set Bundler's config to the `.bundle` directory.\n\n```ruby line-numbers\n\nRUN bundle config set path .cache/bundle\n\n```\n\nThen, we set Bundler's path `.cache/bundle` which is the directory the gems will be installed into.\n\n```ruby line-numbers\n\nRUN --mount=type=cache,uid=1000,target=/app/.cache/bundle \\\n    bundle install && \\\n    mkdir -p vendor && \\\n    cp -ar .cache/bundle vendor/bundle\n\n```\n\nNow the important part! We use the `--mount=type=cache` and set the cache to be the same location as Bundler's path. But a key here is to include the `WORKDIR` path so it becomes `target=/app/.cache/bundler`. This means our directory of installed gems will be persisted from build to build. We run `bundle install` to install the gems and then create a vendor directory. The last step here is to copy `.cache/bundle` into `vendor/bundle` because, if you recall from the Go example, the contents of the cache are not included in the layer.\n\n```ruby line-numbers\n\nRUN bundle config set path vendor/bundle\n\n```\n\nFinally we set Bundler's path to the directory we copied the files into and we're good to go.\n\nAnd now we fully understand what is happening! To wrap up, there are a few final points to cover. The first is that the code above is not quite ideal. It works, but it is missing a few options to add some safety and reliability. There will be a full example shown below.\n\nThe mount cache accepts an `id` as a parameter. The [documentation](https://docs.docker.com/engine/reference/builder/#run---mounttypecache) says:\n\n> Optional ID to identify separate/different caches. Defaults to the value of the target.\n\nSetting an ID is valuable if there are potentially lots of Dockerfiles running on the same BuildKit server who might be attempting to use the same cache location; imagine if two different Rails projects started sharing the same directory!\n\nWhich leads us to the second parameter of `sharing`. The [documentation](https://docs.docker.com/engine/reference/builder/#run---mounttypecache) says:\n\n> One of shared, private, or locked. Defaults to shared. A shared cache mount can be used concurrently by multiple writers. private creates a new mount if there are multiple writers. locked pauses the second writer until the first one releases the mount.\n\nWe want to opt for `sharing=locked` meaning that if two builds of the same Dockerfile are running at the same time, only one can access the cache at a time. This ensures that the output of `bundle install` won't be mangled when the `cp` command is issued.\n\nThis is our suggestion for a full solution.\n\n```ruby line-numbers\n\nWORKDIR /app/\nRUN gem install Bundler\nRUN bundle config set app_config .bundle\nRUN bundle config set path .cache/bundle\nCOPY Gemfile Gemfile.lock ./\nRUN --mount=type=cache,id=-gem-cache,sharing=locked,target=/app/.cache/bundle \\\nbundle install && \\\n  mkdir -p vendor && \\\n  cp -ar .cache/bundle vendor/bundle\nRUN bundle config set path vendor/bundle\n\n```\n\nIf you would like to read more about how the caching works, there is an issue on the BuildKit repository, \"[mount=type=cache more in-depth explanation?](https://github.com/moby/buildkit/issues/1673)\" that has a great discussion on how this functionality actually works.\n\nAnd if you would like to apply this caching mechanism to your build process, sign-up for Release and take advantage of our BuildKit servers!\n\n```css\nblockquote {\n  font-size: 1.25rem;\n}\n\n@media screen and (max-width: 479px) {\n  blockquote {\n    font-size: 1rem;\n  }\n}\n```\n",
          "code": "var Component=(()=>{var h=Object.create;var o=Object.defineProperty;var u=Object.getOwnPropertyDescriptor;var p=Object.getOwnPropertyNames;var m=Object.getPrototypeOf,g=Object.prototype.hasOwnProperty;var b=(t,e)=>()=>(e||t((e={exports:{}}).exports,e),e.exports),f=(t,e)=>{for(var i in e)o(t,i,{get:e[i],enumerable:!0})},r=(t,e,i,c)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let a of p(e))!g.call(t,a)&&a!==i&&o(t,a,{get:()=>e[a],enumerable:!(c=u(e,a))||c.enumerable});return t};var w=(t,e,i)=>(i=t!=null?h(m(t)):{},r(e||!t||!t.__esModule?o(i,\"default\",{value:t,enumerable:!0}):i,t)),y=t=>r(o({},\"__esModule\",{value:!0}),t);var d=b((x,l)=>{l.exports=_jsx_runtime});var N={};f(N,{default:()=>R,frontmatter:()=>k});var n=w(d()),k={title:\"Cache Bundle Install with BuildKit\",summary:\"\",publishDate:\"Mon May 01 2023 20:56:45 GMT+0000 (Coordinated Universal Time)\",author:\"jeremy-kreutzbender\",readingTime:4,categories:[\"platform-engineering\",\"product\"],mainImage:\"/blog-images/54f242bfc5acedea0993fbf3735bc4af.png\",imageAlt:\"Cache Bundle Install with BuildKit\",showCTA:!0,ctaCopy:\"Discover how Release's ephemeral environments streamline caching for faster builds and efficient testing.\",ctaLink:\"https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=cache-bundle-install-with-buildkit\",relatedPosts:[\"\"],ogImage:\"/blog-images/54f242bfc5acedea0993fbf3735bc4af.png\",excerpt:\"At Release, we've been using BuildKit to do our own builds for some time now and BuildKit does an awesome job of caching Docker image layers! But one thing t...\",tags:[\"platform-engineering\",\"product\"],ctaButton:\"Try Release for Free\"};function s(t){let e=Object.assign({p:\"p\",code:\"code\",strong:\"strong\",a:\"a\",img:\"img\",pre:\"pre\",blockquote:\"blockquote\"},t.components);return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsxs)(e.p,{children:[\"At Release, we've been using BuildKit to do our own builds for some time now and BuildKit does an awesome job of caching Docker image layers! But one thing that continued to slow down our builds was running \",(0,n.jsx)(e.code,{children:\"bundle install\"}),\" when upgrading gems in our Rails application. I decided to start researching if there was any way that we could \",(0,n.jsxs)(e.strong,{children:[\"cache our \",(0,n.jsx)(e.code,{children:\"bundle install\"}),\" commands over many builds\"]}),\" because a cold \",(0,n.jsx)(e.code,{children:\"bundle install\"}),\" is very slow, but \",(0,n.jsx)(e.strong,{children:\"incremental changes are quite fast\"}),\". In my search, I came across a KubeCon video about BuildKit, \",(0,n.jsx)(e.a,{href:\"https://www.youtube.com/watch?v=wTENRhYt3mw\",children:'\"Running Cache-Efficient Builds at Scale on Kubernetes with BuildKit - Gautier Delorme, Apple Inc.\"'}),\" which had a very interesting slide.\"]}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/271d7cec8f533d977b812f76403497cd.png\",alt:\"\"})}),`\n`,(0,n.jsx)(e.p,{children:\"This is exactly the type of solution I was looking for and it comes built in with BuildKit!\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"Let's take a look at an example from the \",(0,n.jsx)(e.a,{href:\"https://docs.docker.com/engine/reference/builder/#run---mounttypecache\",children:\"mount cache documentation\"}),\" to start.\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Example: cache Go packages\"}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-ruby\",children:`\n# syntax=docker/dockerfile:1\n\nFROM golang\nRUN --mount=type=cache,target=/root/.cache/go-build \\\\\n  go build ...\n\n`})}),`\n`,(0,n.jsxs)(e.p,{children:[\"In this example, the \",(0,n.jsx)(e.code,{children:\"go build\"}),\" command uses the \",(0,n.jsx)(e.code,{children:\"/root/.cache/go-build\"}),\" directory to store the packages in between builds. Because the output of \",(0,n.jsx)(e.code,{children:\"go build\"}),\" is a binary and does not require anything to run besides that binary, this example makes correct use of the cache. If we think of the cache directory as a named volume from the host server into the container we can create a picture of how this example works. When \",(0,n.jsx)(e.code,{children:\"go build\"}),\" is run, the cache directory is populated on the host server and the resulting binary ends up in the container. The problem is that this mount cache functionality wasn't built with the idea that the packages in the cache needed to be pulled into the resulting image.\"]}),`\n`,(0,n.jsxs)(e.p,{children:['To solve this problem I continued my search and came across this issue on the BuildKit repository, \"',(0,n.jsx)(e.a,{href:\"https://github.com/moby/buildkit/issues/1173\",children:\"Am I misunderstanding RUN mount=type=cache?\"}),`\". The writer of the issue explains how this functionality isn't working with `,(0,n.jsx)(e.code,{children:\"bundle install\"}),' and is trying to figure out what to do. In one of the answers a link to blog post in Japanese is provided, \"',(0,n.jsx)(e.a,{href:\"https://qiita.com/k_kind/items/836bc7ba2e33dc2ed3e7\",children:\"Dockerfile for Rails6\\u306E\\u30D9\\u30B9\\u30C8\\u30D7\\u30E9\\u30AF\\u30C6\\u30A3\\u30B9\\u3092\\u89E3\\u8AAC\"}),`\". A Dockerfile is provided in the post, which has the solution we've been searching for.`]}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-ruby\",children:`\nWORKDIR /app\n\\u2026\nRUN bundle config set app_config .bundle\nRUN bundle config set path .cache/bundle\n\n# mount cache\\u3092\\u5229\\u7528\\u3059\\u308B\n\nRUN --mount=type=cache,uid=1000,target=/app/.cache/bundle \\\\\n    bundle install && \\\\\n    mkdir -p vendor && \\\\\n    cp -ar .cache/bundle vendor/bundle\nRUN bundle config set path vendor/bundle\n\n`})}),`\n`,(0,n.jsx)(e.p,{children:\"Now that we know what the solution is, let's go through it line by line to make sure we fully understand what is happening.\"}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-ruby\",children:`\nWORKDIR /app\n\n`})}),`\n`,(0,n.jsx)(e.p,{children:\"First, we set the working directory for the Dockerfile to app\"}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-ruby\",children:`\nRUN bundle config set app_config .bundle\n\n`})}),`\n`,(0,n.jsxs)(e.p,{children:[\"Next, we set Bundler's config to the \",(0,n.jsx)(e.code,{children:\".bundle\"}),\" directory.\"]}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-ruby\",children:`\nRUN bundle config set path .cache/bundle\n\n`})}),`\n`,(0,n.jsxs)(e.p,{children:[\"Then, we set Bundler's path \",(0,n.jsx)(e.code,{children:\".cache/bundle\"}),\" which is the directory the gems will be installed into.\"]}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-ruby\",children:`\nRUN --mount=type=cache,uid=1000,target=/app/.cache/bundle \\\\\n    bundle install && \\\\\n    mkdir -p vendor && \\\\\n    cp -ar .cache/bundle vendor/bundle\n\n`})}),`\n`,(0,n.jsxs)(e.p,{children:[\"Now the important part! We use the \",(0,n.jsx)(e.code,{children:\"--mount=type=cache\"}),\" and set the cache to be the same location as Bundler's path. But a key here is to include the \",(0,n.jsx)(e.code,{children:\"WORKDIR\"}),\" path so it becomes \",(0,n.jsx)(e.code,{children:\"target=/app/.cache/bundler\"}),\". This means our directory of installed gems will be persisted from build to build. We run \",(0,n.jsx)(e.code,{children:\"bundle install\"}),\" to install the gems and then create a vendor directory. The last step here is to copy \",(0,n.jsx)(e.code,{children:\".cache/bundle\"}),\" into \",(0,n.jsx)(e.code,{children:\"vendor/bundle\"}),\" because, if you recall from the Go example, the contents of the cache are not included in the layer.\"]}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-ruby\",children:`\nRUN bundle config set path vendor/bundle\n\n`})}),`\n`,(0,n.jsx)(e.p,{children:\"Finally we set Bundler's path to the directory we copied the files into and we're good to go.\"}),`\n`,(0,n.jsx)(e.p,{children:\"And now we fully understand what is happening! To wrap up, there are a few final points to cover. The first is that the code above is not quite ideal. It works, but it is missing a few options to add some safety and reliability. There will be a full example shown below.\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"The mount cache accepts an \",(0,n.jsx)(e.code,{children:\"id\"}),\" as a parameter. The \",(0,n.jsx)(e.a,{href:\"https://docs.docker.com/engine/reference/builder/#run---mounttypecache\",children:\"documentation\"}),\" says:\"]}),`\n`,(0,n.jsxs)(e.blockquote,{children:[`\n`,(0,n.jsx)(e.p,{children:\"Optional ID to identify separate/different caches. Defaults to the value of the target.\"}),`\n`]}),`\n`,(0,n.jsx)(e.p,{children:\"Setting an ID is valuable if there are potentially lots of Dockerfiles running on the same BuildKit server who might be attempting to use the same cache location; imagine if two different Rails projects started sharing the same directory!\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"Which leads us to the second parameter of \",(0,n.jsx)(e.code,{children:\"sharing\"}),\". The \",(0,n.jsx)(e.a,{href:\"https://docs.docker.com/engine/reference/builder/#run---mounttypecache\",children:\"documentation\"}),\" says:\"]}),`\n`,(0,n.jsxs)(e.blockquote,{children:[`\n`,(0,n.jsx)(e.p,{children:\"One of shared, private, or locked. Defaults to shared. A shared cache mount can be used concurrently by multiple writers. private creates a new mount if there are multiple writers. locked pauses the second writer until the first one releases the mount.\"}),`\n`]}),`\n`,(0,n.jsxs)(e.p,{children:[\"We want to opt for \",(0,n.jsx)(e.code,{children:\"sharing=locked\"}),\" meaning that if two builds of the same Dockerfile are running at the same time, only one can access the cache at a time. This ensures that the output of \",(0,n.jsx)(e.code,{children:\"bundle install\"}),\" won't be mangled when the \",(0,n.jsx)(e.code,{children:\"cp\"}),\" command is issued.\"]}),`\n`,(0,n.jsx)(e.p,{children:\"This is our suggestion for a full solution.\"}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-ruby\",children:`\nWORKDIR /app/\nRUN gem install Bundler\nRUN bundle config set app_config .bundle\nRUN bundle config set path .cache/bundle\nCOPY Gemfile Gemfile.lock ./\nRUN --mount=type=cache,id=-gem-cache,sharing=locked,target=/app/.cache/bundle \\\\\nbundle install && \\\\\n  mkdir -p vendor && \\\\\n  cp -ar .cache/bundle vendor/bundle\nRUN bundle config set path vendor/bundle\n\n`})}),`\n`,(0,n.jsxs)(e.p,{children:['If you would like to read more about how the caching works, there is an issue on the BuildKit repository, \"',(0,n.jsx)(e.a,{href:\"https://github.com/moby/buildkit/issues/1673\",children:\"mount=type=cache more in-depth explanation?\"}),'\" that has a great discussion on how this functionality actually works.']}),`\n`,(0,n.jsx)(e.p,{children:\"And if you would like to apply this caching mechanism to your build process, sign-up for Release and take advantage of our BuildKit servers!\"}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-css\",children:`blockquote {\n  font-size: 1.25rem;\n}\n\n@media screen and (max-width: 479px) {\n  blockquote {\n    font-size: 1rem;\n  }\n}\n`})})]})}function v(t={}){let{wrapper:e}=t.components||{};return e?(0,n.jsx)(e,Object.assign({},t,{children:(0,n.jsx)(s,t)})):s(t)}var R=v;return y(N);})();\n;return Component;"
        },
        "_id": "blog/posts/cache-bundle-install-with-buildkit.mdx",
        "_raw": {
          "sourceFilePath": "blog/posts/cache-bundle-install-with-buildkit.mdx",
          "sourceFileName": "cache-bundle-install-with-buildkit.mdx",
          "sourceFileDir": "blog/posts",
          "contentType": "mdx",
          "flattenedPath": "blog/posts/cache-bundle-install-with-buildkit"
        },
        "type": "BlogPost",
        "computedSlug": "cache-bundle-install-with-buildkit"
      },
      "documentHash": "1739393595016",
      "hasWarnings": false,
      "documentTypeName": "BlogPost"
    },
    "blog/posts/code-to-cloud-simplified-how-release-uses-ai-to-make-deployment-easier.mdx": {
      "document": {
        "title": "Code to Cloud simplified: how Release uses AI to make deployment easier  ",
        "summary": "Getting your code into the cloud can be a challenge, especially when there is no Dockerfile to help with configuration.",
        "publishDate": "Thu Apr 20 2023 18:06:33 GMT+0000 (Coordinated Universal Time)",
        "author": "erik-landerholm",
        "readingTime": 3,
        "categories": [
          "ai",
          "kubernetes",
          "platform-engineering"
        ],
        "mainImage": "/blog-images/5c008c46a8e48c9fb0a3a6846cd4f95f.jpg",
        "imageAlt": "Release + Chat GTP4",
        "showCTA": true,
        "ctaCopy": "Simplify deployment with AI like Release does. Try Release.com for streamlined environment management.",
        "ctaLink": "https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=code-to-cloud-simplified-how-release-uses-ai-to-make-deployment-easier",
        "relatedPosts": [
          ""
        ],
        "ogImage": "/blog-images/5c008c46a8e48c9fb0a3a6846cd4f95f.jpg",
        "excerpt": "Getting your code into the cloud can be a challenge, especially when there is no Dockerfile to help with configuration.",
        "tags": [
          "ai",
          "kubernetes",
          "platform-engineering"
        ],
        "ctaButton": "Try Release for Free",
        "body": {
          "raw": "\nGetting your code into the cloud can be a challenge, especially when there is no Dockerfile to help with configuration. With our latest AI project, Release is here to solve this problem once and for all!\n\nWhile the cloud brought some parity between big companies’ infrastructure and the rest of us, it also created unnecessary complexity. Release was founded to make it easier for companies to virtualize their environments and utilize the cloud to its full potential, all while being as easy as Heroku.\n\nAt Release, we tackled the problem of organizing and configuring applications with a combination of software and people, making it easy to transition from code to cloud. Our newest AI project makes it even easier for our customers to get their modern apps, utilizing [Kubernetes](https://docs.release.com/guides-and-examples/advanced-guides/kubernetes), [Terraform](https://docs.release.com/guides-and-examples/advanced-guides/infrastructure/terraform), [Helm](https://docs.release.com/reference-documentation/helm), Pulumi, and more in the cloud. Without requiring expertise in every cloud and native service involved. **You code, and Release makes it run!**\n\nHow does Release use AI to accomplish this feat? The answer is generative AI + iteration! With GPT-4’s help, Release automatically dockerizes your application and runs it in the cloud!\n\n### How does it work?  \n\n![](/blog-images/bd169793166fe036ccc12c00466eea9f.png)\n\n_Figure 1. High level diagram showing how GPT-4 is utilized to dockerize and deploy your application with Release_\n\nHere's how it works: Typically, we need some basic configuration from your repository in order to get your application running in AWS and/or GCP. We start with Dockerfiles and Docker Compose files which ultimately are translated into k8s manifests that can run in EKS and/or GKE. Now, with the help of AI, Release is able to generate and test the Docker files for your repository; point Release at your repository and it fills in the missing pieces. \n\nRelease goes through an iterative process to generate and test the requisite files. It presents the output to the user so they can make any changes, and then tests the files again. Through this process you end up with requisite artifacts (Dockerfile, docker-compose.yml and .dockerignore) and Release gets your software up and running in the cloud!\n\nThe steps are straightforward: \n\n1.  Associate your repository with Release\n2.  Release will interrogate your repository and generate three files  \n    a. Dockerfile, .dockerignore, and a docker-compose.yml\n\n![](/blog-images/f67a743a9e94b2e4070d98404bb48170.gif)\n\n_Figure 2. Release auto-generating docker artifacts from repository interrogation + AI_ 3. Release runs a build and tests your generated files  \n a. If the build fails, you can see the logs and make changes and try again\n\n![](/blog-images/8beb1b9c4a3d8400e0183395466b3107.gif)\n\n_Figure 3. Shows a build error being automatically corrected by Release based on the errors in the docker build. The package list was missing “shared-mime-info”.  Release noticed this and regenerated a Docker file, this time including “shared-mime-info”._ 4. Once the build is working, Release deploys your application into your EKS/GKE cluster\n\n![](/blog-images/7a61ac7e9f28cc3aa8848e995c7e041c.png)\n\n_Figure 4. Once the build passes a deployment kicks off automatically and we now have a fully working environment, in the cloud without having to create any docker files, k8s manifests, terraforms, etc.  \n_ 5. Now you can instantly create ephemeral, production, and cloud development environments. And even deploy your software into your customer’s VPCs with [Release Delivery](https://release.com/blog/release-delivery-helps-saas-companies-meet-the-needs-of-their-enterprise-customers)! \n\n### What’s next?\n\nThis is just the beginning of our code to cloud dream! We have plenty of things left to do:\n\n- We will continue to optimize our prompts and repository interrogation. We are using Rails applications as a proof of concept, but we will add support for all the major frameworks and improve the file generation through prompt optimization. \n- We have an iterative loop for creating a successful build, but it currently requires manual approval to start the next iteration. We believe there is room to let our system and GPT-4 automatically build and apply fixes itself until the build succeeds.  \n- We are creating a virtual assistant to explain various aspects of our documentation and feature sets. Using AI we will be able to provide even more accessible documentation and examples that are context aware. We will open source this effort so other companies can add the same functionality to their applications.\n\nIf you would like to see what’s possible with Release + GPT come by our booth at [2023 RailsConf](https://railsconf.org/) or take a look at this [video](https://vimeo.com/819902308). To try our development platform that is as easy as Heroku, but with the power of the largest cloud providers, use the promo code “**RailsConf**” when you [sign up](https://release.com/get-started) or contact sales for a [demo](https://release.com/book-a-demo)!\n",
          "code": "var Component=(()=>{var h=Object.create;var n=Object.defineProperty;var u=Object.getOwnPropertyDescriptor;var p=Object.getOwnPropertyNames;var m=Object.getPrototypeOf,g=Object.prototype.hasOwnProperty;var f=(o,e)=>()=>(e||o((e={exports:{}}).exports,e),e.exports),y=(o,e)=>{for(var i in e)n(o,i,{get:e[i],enumerable:!0})},s=(o,e,i,r)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let a of p(e))!g.call(o,a)&&a!==i&&n(o,a,{get:()=>e[a],enumerable:!(r=u(e,a))||r.enumerable});return o};var b=(o,e,i)=>(i=o!=null?h(m(o)):{},s(e||!o||!o.__esModule?n(i,\"default\",{value:o,enumerable:!0}):i,o)),w=o=>s(n({},\"__esModule\",{value:!0}),o);var c=f((A,l)=>{l.exports=_jsx_runtime});var x={};y(x,{default:()=>R,frontmatter:()=>k});var t=b(c()),k={title:\"Code to Cloud simplified: how Release uses AI to make deployment easier  \",summary:\"Getting your code into the cloud can be a challenge, especially when there is no Dockerfile to help with configuration.\",publishDate:\"Thu Apr 20 2023 18:06:33 GMT+0000 (Coordinated Universal Time)\",author:\"erik-landerholm\",readingTime:3,categories:[\"ai\",\"kubernetes\",\"platform-engineering\"],mainImage:\"/blog-images/5c008c46a8e48c9fb0a3a6846cd4f95f.jpg\",imageAlt:\"Release + Chat GTP4\",showCTA:!0,ctaCopy:\"Simplify deployment with AI like Release does. Try Release.com for streamlined environment management.\",ctaLink:\"https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=code-to-cloud-simplified-how-release-uses-ai-to-make-deployment-easier\",relatedPosts:[\"\"],ogImage:\"/blog-images/5c008c46a8e48c9fb0a3a6846cd4f95f.jpg\",excerpt:\"Getting your code into the cloud can be a challenge, especially when there is no Dockerfile to help with configuration.\",tags:[\"ai\",\"kubernetes\",\"platform-engineering\"],ctaButton:\"Try Release for Free\"};function d(o){let e=Object.assign({p:\"p\",a:\"a\",strong:\"strong\",h3:\"h3\",span:\"span\",img:\"img\",em:\"em\",ol:\"ol\",li:\"li\",br:\"br\",ul:\"ul\"},o.components);return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(e.p,{children:\"Getting your code into the cloud can be a challenge, especially when there is no Dockerfile to help with configuration. With our latest AI project, Release is here to solve this problem once and for all!\"}),`\n`,(0,t.jsx)(e.p,{children:\"While the cloud brought some parity between big companies\\u2019 infrastructure and the rest of us, it also created unnecessary complexity. Release was founded to make it easier for companies to virtualize their environments and utilize the cloud to its full potential, all while being as easy as Heroku.\"}),`\n`,(0,t.jsxs)(e.p,{children:[\"At Release, we tackled the problem of organizing and configuring applications with a combination of software and people, making it easy to transition from code to cloud. Our newest AI project makes it even easier for our customers to get their modern apps, utilizing \",(0,t.jsx)(e.a,{href:\"https://docs.release.com/guides-and-examples/advanced-guides/kubernetes\",children:\"Kubernetes\"}),\", \",(0,t.jsx)(e.a,{href:\"https://docs.release.com/guides-and-examples/advanced-guides/infrastructure/terraform\",children:\"Terraform\"}),\", \",(0,t.jsx)(e.a,{href:\"https://docs.release.com/reference-documentation/helm\",children:\"Helm\"}),\", Pulumi, and more in the cloud. Without requiring expertise in every cloud and native service involved. \",(0,t.jsx)(e.strong,{children:\"You code, and Release makes it run!\"})]}),`\n`,(0,t.jsx)(e.p,{children:\"How does Release use AI to accomplish this feat? The answer is generative AI + iteration! With GPT-4\\u2019s help, Release automatically dockerizes your application and runs it in the cloud!\"}),`\n`,(0,t.jsxs)(e.h3,{id:\"how-does-it-work\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#how-does-it-work\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"How does it work?\\xA0\\xA0\"]}),`\n`,(0,t.jsx)(e.p,{children:(0,t.jsx)(e.img,{src:\"/blog-images/bd169793166fe036ccc12c00466eea9f.png\",alt:\"\"})}),`\n`,(0,t.jsx)(e.p,{children:(0,t.jsx)(e.em,{children:\"Figure 1. High level diagram showing how GPT-4 is utilized to dockerize and deploy your application with Release\"})}),`\n`,(0,t.jsx)(e.p,{children:\"Here's how it works: Typically, we need some basic configuration from your repository in order to get your application running in AWS and/or GCP. We start with Dockerfiles and Docker Compose files which ultimately are translated into k8s manifests that can run in EKS and/or GKE. Now, with the help of AI, Release is able to generate and test the Docker files for your repository; point Release at your repository and it fills in the missing pieces.\\xA0\"}),`\n`,(0,t.jsx)(e.p,{children:\"Release goes through an iterative process to generate and test the requisite files. It presents the output to the user so they can make any changes, and then tests the files again. Through this process you end up with requisite artifacts (Dockerfile, docker-compose.yml and .dockerignore) and Release gets your software up and running in the cloud!\"}),`\n`,(0,t.jsx)(e.p,{children:\"The steps are straightforward:\\xA0\"}),`\n`,(0,t.jsxs)(e.ol,{children:[`\n`,(0,t.jsx)(e.li,{children:\"Associate your repository with Release\"}),`\n`,(0,t.jsxs)(e.li,{children:[\"Release will interrogate your repository and generate three files\",(0,t.jsx)(e.br,{}),`\n`,\"a. Dockerfile, .dockerignore, and a docker-compose.yml\"]}),`\n`]}),`\n`,(0,t.jsx)(e.p,{children:(0,t.jsx)(e.img,{src:\"/blog-images/f67a743a9e94b2e4070d98404bb48170.gif\",alt:\"\"})}),`\n`,(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.em,{children:\"Figure 2. Release auto-generating docker artifacts from repository interrogation + AI\"}),\" 3. Release runs a build and tests your generated files\",(0,t.jsx)(e.br,{}),`\n`,\"a. If the build fails, you can see the logs and make changes and try again\"]}),`\n`,(0,t.jsx)(e.p,{children:(0,t.jsx)(e.img,{src:\"/blog-images/8beb1b9c4a3d8400e0183395466b3107.gif\",alt:\"\"})}),`\n`,(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.em,{children:\"Figure 3. Shows a build error being automatically corrected by Release based on the errors in the docker build. The package list was missing \\u201Cshared-mime-info\\u201D.\\xA0 Release noticed this and regenerated a Docker file, this time including \\u201Cshared-mime-info\\u201D.\"}),\" 4. Once the build is working, Release deploys your application into your EKS/GKE cluster\"]}),`\n`,(0,t.jsx)(e.p,{children:(0,t.jsx)(e.img,{src:\"/blog-images/7a61ac7e9f28cc3aa8848e995c7e041c.png\",alt:\"\"})}),`\n`,(0,t.jsxs)(e.p,{children:[\"_Figure 4. Once the build passes a deployment kicks off automatically and we now have a fully working environment, in the cloud without having to create any docker files, k8s manifests, terraforms, etc.\",(0,t.jsx)(e.br,{}),`\n`,\"_ 5. Now you can instantly create ephemeral, production, and cloud development environments. And even deploy your software into your customer\\u2019s VPCs with \",(0,t.jsx)(e.a,{href:\"https://release.com/blog/release-delivery-helps-saas-companies-meet-the-needs-of-their-enterprise-customers\",children:\"Release Delivery\"}),\"!\\xA0\"]}),`\n`,(0,t.jsxs)(e.h3,{id:\"whats-next\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#whats-next\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"What\\u2019s next?\"]}),`\n`,(0,t.jsx)(e.p,{children:\"This is just the beginning of our code to cloud dream!\\xA0We have plenty of things left to do:\"}),`\n`,(0,t.jsxs)(e.ul,{children:[`\n`,(0,t.jsx)(e.li,{children:\"We will continue to optimize our prompts and repository interrogation. We are using Rails applications as a proof of concept, but we will add support for all the major frameworks and improve the file generation through prompt optimization.\\xA0\"}),`\n`,(0,t.jsx)(e.li,{children:\"We have an iterative loop for creating a successful build, but it currently requires manual approval to start the next iteration. We believe there is room to let our system and GPT-4 automatically build and apply fixes itself until the build succeeds.\\xA0\\xA0\"}),`\n`,(0,t.jsx)(e.li,{children:\"We are creating a virtual assistant to explain various aspects of our documentation and feature sets. Using AI we will be able to provide even more accessible documentation and examples that are context aware. We will open source this effort so other companies can add the same functionality to their applications.\"}),`\n`]}),`\n`,(0,t.jsxs)(e.p,{children:[\"If you would like to see what\\u2019s possible with Release + GPT come by our booth at \",(0,t.jsx)(e.a,{href:\"https://railsconf.org/\",children:\"2023 RailsConf\"}),\" or take a look at this \",(0,t.jsx)(e.a,{href:\"https://vimeo.com/819902308\",children:\"video\"}),\". To try our development platform that is as easy as Heroku, but with the power of the largest cloud providers, use the promo code \\u201C\",(0,t.jsx)(e.strong,{children:\"RailsConf\"}),\"\\u201D when you \",(0,t.jsx)(e.a,{href:\"https://release.com/get-started\",children:\"sign up\"}),\" or contact sales for a \",(0,t.jsx)(e.a,{href:\"https://release.com/book-a-demo\",children:\"demo\"}),\"!\"]})]})}function v(o={}){let{wrapper:e}=o.components||{};return e?(0,t.jsx)(e,Object.assign({},o,{children:(0,t.jsx)(d,o)})):d(o)}var R=v;return w(x);})();\n;return Component;"
        },
        "_id": "blog/posts/code-to-cloud-simplified-how-release-uses-ai-to-make-deployment-easier.mdx",
        "_raw": {
          "sourceFilePath": "blog/posts/code-to-cloud-simplified-how-release-uses-ai-to-make-deployment-easier.mdx",
          "sourceFileName": "code-to-cloud-simplified-how-release-uses-ai-to-make-deployment-easier.mdx",
          "sourceFileDir": "blog/posts",
          "contentType": "mdx",
          "flattenedPath": "blog/posts/code-to-cloud-simplified-how-release-uses-ai-to-make-deployment-easier"
        },
        "type": "BlogPost",
        "computedSlug": "code-to-cloud-simplified-how-release-uses-ai-to-make-deployment-easier"
      },
      "documentHash": "1739393595016",
      "hasWarnings": false,
      "documentTypeName": "BlogPost"
    },
    "blog/posts/components-of-a-successful-idp-build-a-product-your-developers-actually-want-to-use.mdx": {
      "document": {
        "title": "Components of a Successful IDP: Build a Product Your Developers Actually Want to Use",
        "summary": "Learn about different qualities and components of successful IDPs, and explore some ways IDPs could fail.",
        "publishDate": "Wed Aug 16 2023 16:18:56 GMT+0000 (Coordinated Universal Time)",
        "author": "sylvia-fronczak",
        "readingTime": 10,
        "categories": [
          "platform-engineering",
          "product"
        ],
        "mainImage": "/blog-images/dbc2f9c040038b3d718491e42e3fa3ab.jpg",
        "imageAlt": "Photo Credit: Dan Cristian Paduree",
        "showCTA": true,
        "ctaCopy": "Build self-service solutions with automated environments using Release's platform for streamlined workflows and faster deployments.",
        "ctaLink": "https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=components-of-a-successful-idp-build-a-product-your-developers-actually-want-to-use",
        "relatedPosts": [
          ""
        ],
        "ogImage": "/blog-images/dbc2f9c040038b3d718491e42e3fa3ab.jpg",
        "excerpt": "Learn about different qualities and components of successful IDPs, and explore some ways IDPs could fail.",
        "tags": [
          "platform-engineering",
          "product"
        ],
        "ctaButton": "Try Release for Free",
        "body": {
          "raw": "\nAfter reading about the [Internal Developer Platforms (IDPs) and why you should build one](https://release.com/blog/what-is-an-internal-developer-platform-and-why-should-i-have-one), you might be seeing certain benefits your organization could use. At this point, you might already be considering the first moves towards implementing an IDP. After all, an internal developer platform that provides automated self-service solutions for developers to simplify and standardize software practices, infrastructure, environments, and operations sounds like a dream come true. But what specifically should an IDP do? What functionality should it cover? And how can you ensure that teams adopt the IDP and that it provides real benefits?\n\nIn this post, we’ll cover different qualities and components of a successful IDP, and share some ways in which IDPs can fail. Let’s dig in!\n\n### Qualities of successful IDPs\n\nBefore you roll up your sleeves and start building your IDP, consider a set of qualities a successful IDP needs to solve for. These are the high level objectives an IDP addresses, without accomplishing these can easily become yet another tool gathering dust on the shelf. When building an IDP keep these three qualities at the top of your mind:\n\n1.  IDPs provide self-service solutions\n2.  IDPs automate processes\n3.  IDPs enforce guardrails and standards\n\n#### **1\\. IDPs Provide Self-Service Solutions**\n\nFirst, let’s consider self-service. Successful IDPs enable developers to access resources without tying up resources and people in other groups.\n\nThe IDP should empower teams and developers to solve their own problems when it makes sense for the developer based on their timelines and needs.\n\nOf course, there’s a caveat with this as too much empowerment can lead to failure. For full self-service, this could mean that you give your development teams access to everything, allowing full self-service all the time. But that’s not going to go well.\n\nIf you were to just provide access to everything, development teams would need to learn the ins and outs of infrastructure, tooling, governance, and multiple standards to self-service everything they need. Sure, they’d be able to self-service. But they wouldn’t have time to ship features. And they would end up re-learning similar tools and remaking similar mistakes that other teams or developers already made.\n\nFor IDPs, we do need to provide self-service functionality. But **the surface area of the necessary knowledge for that self-service ability should be as small as possible.** That way, developers can focus on creating a product with their development tools, and not tinkering with infrastructure.\n\nSo how do we make that surface area small? Through automation and standards. In the next section, let’s consider automation.\n\n‍\n\n![](/blog-images/941d45aab99ced30f672f083a86f9cae.png)\n\n#### **2\\. IDPs Automate Processes**\n\nWhen you read the word _automation_, you may think you can skip this section. Let’s hold up a second, though.\n\nYou may say, “Sure, I know what automation is. I create scripts that build and deploy apps, for example.”\n\nYes, [CI/CD pipelines](https://prod.releasehub.com/blog/11-continuous-deployment-tools-and-how-to-choose-one) are oftentimes the first thing that comes to mind when we think about automation and the development process, which is essential. But we should consider automation outside of the basics.\n\nThe successful IDP is a product built for developers. It can provide solutions for all stages of a developer’s work, from the day they start at your company to the day they leave. And it involves automated self-service solutions that ideally do not require ticket queues, manual approvals, or reliance on other teams to complete the request.\n\nLet’s talk about approvals for a minute. In some cases, we do need manual approvals, and that’s OK for limited and special use cases. But the majority of the time, approvals don’t provide a lot of value. Consider access requests. If you need to manually approve every single request for access to your code repository, deployment tools, and monitoring solutions, then you will waste time. What developers on your team don’t need access to those tools—or can work without them? Probably all of them need some access.\n\nYou may say these approvals are needed for auditing and validation. But are there better ways to get what you need? If every developer is given required access automatically as part of their first day’s onboarding, as part of the standard automated process, then can that fulfill your requirements? Alternatively, can you implement automations that verify that developers have access to the proper resources, that they’ve taken the required training, and that automate attestation of need to access those resources? When solving for compliance or auditing requirements, there are different ways to automate based on your specific needs.\n\nFrequently, we automate parts of a process while not always automating the most significant [bottlenecks](https://release.com/blog/increase-developer-velocity-by-removing-environment-bottlenecks). For example, if you’ve automated a request process to provision new databases but do not automate standard configuration, you’ve saved only a few minutes of someone’s time. Or if you pull reports frequently to verify that databases continue to conform to standards or don’t have unused capacity, then there is still room for improvement.\n\nWhen you consider automation, do not just consider automating the simple button clicks or simple scripting. Think about how your whole development process works and where the time and effort is spent. And then iterate. Once you’ve improved one part of the process, continuously re-evaluate how your automations can further improve the whole processes.\n\n#### **3\\. IDPs Enforce Guardrails and Standards**\n\nOK, after the two previous sections, you may know where this is going.\n\nWith a successful IDP, your self-service tools and automation conform to the standards and practices that your subject matter experts have determined work best.\n\nNow, in some cases, we do need to provide more freedom to developers to work outside of the normal guardrails or standards. And these standards should not limit your development teams to current use cases, as needs change. To provide flexibility for niche use cases, consider how you can enforce these guardrails while providing workarounds or approved exceptions.\n\nHowever, consider the ways that IDPs can standardize not just tools like CI/CD, but basic provisioning, configuration, monitoring, and more.\n\nKeeping the Qualities in mind, let’s look at the specific Components you should consider.  \n\n![](/blog-images/094b8afc10a92ab374e4c478f504025c.png)\n\n### Components of successful IDPs\n\nThe components that make up successful IDPs vary and don’t all need to be used at the same time. In fact, when getting started, you should never attempt to use this as a checklist, assuming your IDP will be successful when you’ve added all the components.\n\nAdditionally, not all components need to be part of a central IDP deployment. They can involve other tools and bits of automation that work to improve the efficiency of your development teams, but function outside of the central IDP deployment.\n\nWhen you’re planning what to tackle first, consider which components of an IDP will provide your organization with the most value.\n\nSome components are necessary for all organizations, like code repositories, IDEs and CI/CD. Without them you’re missing the basics that all teams that write code need.\n\nOther components should be present, but not all orgs have automated them or added any integrations into the developer workflow to make them worthwhile just yet. This includes items like automated security scanning, typically integrated through your CI/CD pipeline.\n\nAnd then there are items specific to certain industries. For example, not every organization needs full integration with compliance and regulatory features.\n\nFinally, not all of these components need to be integrated through an IDP from the start. You can build upon and expand as the value becomes clear. But once you have a number of these components that need to work together, incorporating them and integrating them into your IDP will simplify the development workflow. These integrations can remove unnecessary cognitive load of configuring and context-switching for your dev team and allow them to focus on more valuable tasks.\n\nIDP components can include but are not limited to:\n\n1.  **Code repositories:** OK, so there are some of these components that are non-negotiable. And our repositories, like git, are # 1.\n2.  **Infrastructure:** This can include building, orchestrating, configuration management, [environment management](https://prod.releasehub.com/blog/a-simple-guide-to-software-environments), and monitoring of the infrastructure used by your development team.\n3.  **Development tools:** This includes IDEs, plugins, and extensions as well as stand-alone tools. A good IDP can integrate with other tools through plugins and extensions, all built out as part of the IDP.\n4.  **CI/CD:** As this is one of the basic tools IDPs use, consider ways to supercharge this functionality. In addition to the usual CI/CD functionality, can you automate changelogs, notify operations of deployments, and track failures and rollbacks?\n5.  **Automated testing:** In addition to automating unit tests, integration tests, and others, a successful IDP will drive advanced test features like flaky test detection, load testing, and even chaos engineering.\n6.  **Security:** The basics of security could involve static analysis tools in your CI/CD pipeline, [secrets management](https://release.com/blog/kubernetes-secrets-management-a-practical-guide), or live monitoring of security threats.\n7.  **Monitoring/observability:** Monitoring and observability provide your teams with the ability to ensure your code actually does what it is meant to do. It also can help teams not only identify problems (through monitoring alerts around availability or critical functionality) but also help find the root causes. We can also think about more than just a link from our IDP to our monitoring solutions, but integrations with our CI/CD tools to identify where problems occur and when the problems were introduced.\n8.  **Project management and collaboration tools:** When you’re looking at the IDP, do not just consider purely developer-focused tools. How can your IDP integrate your deployment processes with your project management and notifications?\n9.  **Compliance and governance tools and workflows:** We talked a bit about this when discussing access requests earlier. Consider what other manual processes and reporting could be automated and integrated into your IDP. And don’t think of this as just automating a report. Consider what can be done to remove the need for reports and manual intervention.\n10. **Workflow automation:** Workflow automation is a bit of a catchall for tools that automate workflows. Once you’ve covered the basic components, you can begin to automate processes like creating new services, upgrading systems, or creating changelogs and API versions. Since the IDP centers around improving developer workflows, you’ll need to look outside of the basic components and find new ways to reduce toil.\n\n### Common Failures of IDPs\n\nSo, if an IDP covers all or a majority of the key components, why do many still fail to provide value? Why do they struggle with adoption?\n\nWell, several things can go wrong. We’ll cover just four of them.\n\nFirst, the IDP may be **focused on niche activities and processes and not the 80% of functionality that most developers need**. For example, if you’re in a monolithic environment, automating project initiation for microservices obviously won’t add much value. On the other hand, if your [team struggles with getting fast feedback, slow velocity, or environment bottlenecks for testing](https://prod.releasehub.com/blog/improve-developer-velocity-with-ephemeral-environments), then spinning up [ephemeral environments](https://release.com/ephemeral-environments) might be worth your time.\n\nSecond, **to understand your developers, you must consider _all_ of your developers**. Do not consider just the strongest, newest, or loudest developers. Interview a wide sample of your teams and consider their entire processes, so that you build a truly user-centric product.\n\nThird, sometimes **in an effort to remove security risks, companies will remove flexibility or functionality that the development team relies on**. For example, in an attempt to automate environment provisioning, an IDP may lock down security on configuration that developers used to have access to. This may be the right move, but you have to have a way to adjust to specific use cases. There should be guardrails while still making it possible to work outside of those guardrails when necessary.\n\nAnd finally, perhaps you **haven’t considered or built a rollout strategy**. You will need to consider how to onboard teams, get feedback, and quickly adjust and pivot based on that feedback and data. You can’t just build it and assume everyone will start using the tools and integrations available. It will take time and effort.\n\n### Summary\n\nWhen considering your future IDP or finding the next component your IDP should cover, consider the 80% of the functionality that your development teams need frequently. Add automated, standardized, self-serve functionality that addresses the majority of development workflows. And make sure you have enough of a feedback loop to ensure you’re building the right platform and improving the development cycle. In our next chapter we will look at the Build vs Buy question. Stay tuned!\n",
          "code": "var Component=(()=>{var u=Object.create;var a=Object.defineProperty;var h=Object.getOwnPropertyDescriptor;var m=Object.getOwnPropertyNames;var p=Object.getPrototypeOf,f=Object.prototype.hasOwnProperty;var g=(o,e)=>()=>(e||o((e={exports:{}}).exports,e),e.exports),y=(o,e)=>{for(var n in e)a(o,n,{get:e[n],enumerable:!0})},r=(o,e,n,i)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let s of m(e))!f.call(o,s)&&s!==n&&a(o,s,{get:()=>e[s],enumerable:!(i=h(e,s))||i.enumerable});return o};var v=(o,e,n)=>(n=o!=null?u(p(o)):{},r(e||!o||!o.__esModule?a(n,\"default\",{value:o,enumerable:!0}):n,o)),w=o=>r(a({},\"__esModule\",{value:!0}),o);var d=g((C,l)=>{l.exports=_jsx_runtime});var D={};y(D,{default:()=>k,frontmatter:()=>b});var t=v(d()),b={title:\"Components of a Successful IDP: Build a Product Your Developers Actually Want to Use\",summary:\"Learn about different qualities and components of successful IDPs, and explore some ways IDPs could fail.\",publishDate:\"Wed Aug 16 2023 16:18:56 GMT+0000 (Coordinated Universal Time)\",author:\"sylvia-fronczak\",readingTime:10,categories:[\"platform-engineering\",\"product\"],mainImage:\"/blog-images/dbc2f9c040038b3d718491e42e3fa3ab.jpg\",imageAlt:\"Photo Credit: Dan Cristian Paduree\",showCTA:!0,ctaCopy:\"Build self-service solutions with automated environments using Release's platform for streamlined workflows and faster deployments.\",ctaLink:\"https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=components-of-a-successful-idp-build-a-product-your-developers-actually-want-to-use\",relatedPosts:[\"\"],ogImage:\"/blog-images/dbc2f9c040038b3d718491e42e3fa3ab.jpg\",excerpt:\"Learn about different qualities and components of successful IDPs, and explore some ways IDPs could fail.\",tags:[\"platform-engineering\",\"product\"],ctaButton:\"Try Release for Free\"};function c(o){let e=Object.assign({p:\"p\",a:\"a\",h3:\"h3\",span:\"span\",ol:\"ol\",li:\"li\",h4:\"h4\",strong:\"strong\",img:\"img\",em:\"em\"},o.components);return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsxs)(e.p,{children:[\"After reading about the \",(0,t.jsx)(e.a,{href:\"https://release.com/blog/what-is-an-internal-developer-platform-and-why-should-i-have-one\",children:\"Internal Developer Platforms (IDPs) and why you should build one\"}),\", you might be seeing certain benefits your organization could use. At this point, you might already be considering the first moves towards implementing an IDP. After all, an internal developer platform that provides automated self-service solutions for developers to simplify and standardize software practices, infrastructure, environments, and operations sounds like a dream come true. But what specifically should an IDP do? What functionality should it cover? And how can you ensure that teams adopt the IDP and that it provides real benefits?\"]}),`\n`,(0,t.jsx)(e.p,{children:\"In this post, we\\u2019ll cover different qualities and components of a successful IDP, and share some ways in which IDPs can fail. Let\\u2019s dig in!\"}),`\n`,(0,t.jsxs)(e.h3,{id:\"qualities-of-successful-idps\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#qualities-of-successful-idps\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Qualities of successful IDPs\"]}),`\n`,(0,t.jsx)(e.p,{children:\"Before you roll up your sleeves and start building your IDP, consider a set of qualities a successful IDP needs to solve for. These are the high level objectives an IDP addresses, without accomplishing these can easily become yet another tool gathering dust on the shelf. When building an IDP keep these three qualities at the top of your mind:\"}),`\n`,(0,t.jsxs)(e.ol,{children:[`\n`,(0,t.jsx)(e.li,{children:\"IDPs provide self-service solutions\"}),`\n`,(0,t.jsx)(e.li,{children:\"IDPs automate processes\"}),`\n`,(0,t.jsx)(e.li,{children:\"IDPs enforce guardrails and standards\"}),`\n`]}),`\n`,(0,t.jsxs)(e.h4,{id:\"1-idps-provide-self-service-solutions\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#1-idps-provide-self-service-solutions\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),(0,t.jsx)(e.strong,{children:\"1. IDPs Provide Self-Service Solutions\"})]}),`\n`,(0,t.jsx)(e.p,{children:\"First, let\\u2019s consider self-service. Successful IDPs enable developers to access resources without tying up resources and people in other groups.\"}),`\n`,(0,t.jsx)(e.p,{children:\"The IDP should empower teams and developers to solve their own problems when it makes sense for the developer based on their timelines and needs.\"}),`\n`,(0,t.jsx)(e.p,{children:\"Of course, there\\u2019s a caveat with this as too much empowerment can lead to failure. For full self-service, this could mean that you give your development teams access to everything, allowing full self-service all the time. But that\\u2019s not going to go well.\"}),`\n`,(0,t.jsx)(e.p,{children:\"If you were to just provide access to everything, development teams would need to learn the ins and outs of infrastructure, tooling, governance, and multiple standards to self-service everything they need. Sure, they\\u2019d be able to self-service. But they wouldn\\u2019t have time to ship features. And they would end up re-learning similar tools and remaking similar mistakes that other teams or developers already made.\"}),`\n`,(0,t.jsxs)(e.p,{children:[\"For IDPs, we do need to provide self-service functionality. But \",(0,t.jsx)(e.strong,{children:\"the surface area of the necessary knowledge for that self-service ability should be as small as possible.\"}),\" That way, developers can focus on creating a product with their development tools, and not tinkering with infrastructure.\"]}),`\n`,(0,t.jsx)(e.p,{children:\"So how do we make that surface area small? Through automation and standards. In the next section, let\\u2019s consider automation.\"}),`\n`,(0,t.jsx)(e.p,{children:\"\\u200D\"}),`\n`,(0,t.jsx)(e.p,{children:(0,t.jsx)(e.img,{src:\"/blog-images/941d45aab99ced30f672f083a86f9cae.png\",alt:\"\"})}),`\n`,(0,t.jsxs)(e.h4,{id:\"2-idps-automate-processes\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#2-idps-automate-processes\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),(0,t.jsx)(e.strong,{children:\"2. IDPs Automate Processes\"})]}),`\n`,(0,t.jsxs)(e.p,{children:[\"When you read the word \",(0,t.jsx)(e.em,{children:\"automation\"}),\", you may think you can skip this section. Let\\u2019s hold up a second, though.\"]}),`\n`,(0,t.jsx)(e.p,{children:\"You may say, \\u201CSure, I know what automation is. I create scripts that build and deploy apps, for example.\\u201D\"}),`\n`,(0,t.jsxs)(e.p,{children:[\"Yes, \",(0,t.jsx)(e.a,{href:\"https://prod.releasehub.com/blog/11-continuous-deployment-tools-and-how-to-choose-one\",children:\"CI/CD pipelines\"}),\" are oftentimes the first thing that comes to mind when we think about automation and the development process, which is essential. But we should consider automation outside of the basics.\"]}),`\n`,(0,t.jsx)(e.p,{children:\"The successful IDP is a product built for developers. It can provide solutions for all stages of a developer\\u2019s work, from the day they start at your company to the day they leave. And it involves automated self-service solutions that ideally do not require ticket queues, manual approvals, or reliance on other teams to complete the request.\"}),`\n`,(0,t.jsx)(e.p,{children:\"Let\\u2019s talk about approvals for a minute. In some cases, we do need manual approvals, and that\\u2019s OK for limited and special use cases. But the majority of the time, approvals don\\u2019t provide a lot of value. Consider access requests. If you need to manually approve every single request for access to your code repository, deployment tools, and monitoring solutions, then you will waste time. What developers on your team don\\u2019t need access to those tools\\u2014or can work without them? Probably all of them need some access.\"}),`\n`,(0,t.jsx)(e.p,{children:\"You may say these approvals are needed for auditing and validation. But are there better ways to get what you need? If every developer is given required access automatically as part of their first day\\u2019s onboarding, as part of the standard automated process, then can that fulfill your requirements? Alternatively, can you implement automations that verify that developers have access to the proper resources, that they\\u2019ve taken the required training, and that automate attestation of need to access those resources? When solving for compliance or auditing requirements, there are different ways to automate based on your specific needs.\"}),`\n`,(0,t.jsxs)(e.p,{children:[\"Frequently, we automate parts of a process while not always automating the most significant \",(0,t.jsx)(e.a,{href:\"https://release.com/blog/increase-developer-velocity-by-removing-environment-bottlenecks\",children:\"bottlenecks\"}),\". For example, if you\\u2019ve automated a request process to provision new databases but do not automate standard configuration, you\\u2019ve saved only a few minutes of someone\\u2019s time. Or if you pull reports frequently to verify that databases continue to conform to standards or don\\u2019t have unused capacity, then there is still room for improvement.\"]}),`\n`,(0,t.jsx)(e.p,{children:\"When you consider automation, do not just consider automating the simple button clicks or simple scripting. Think about how your whole development process works and where the time and effort is spent. And then iterate. Once you\\u2019ve improved one part of the process, continuously re-evaluate how your automations can further improve the whole processes.\"}),`\n`,(0,t.jsxs)(e.h4,{id:\"3-idps-enforce-guardrails-and-standards\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#3-idps-enforce-guardrails-and-standards\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),(0,t.jsx)(e.strong,{children:\"3. IDPs Enforce Guardrails and Standards\"})]}),`\n`,(0,t.jsx)(e.p,{children:\"OK, after the two previous sections, you may know where this is going.\"}),`\n`,(0,t.jsx)(e.p,{children:\"With a successful IDP, your self-service tools and automation conform to the standards and practices that your subject matter experts have determined work best.\"}),`\n`,(0,t.jsx)(e.p,{children:\"Now, in some cases, we do need to provide more freedom to developers to work outside of the normal guardrails or standards. And these standards should not limit your development teams to current use cases, as needs change. To provide flexibility for niche use cases, consider how you can enforce these guardrails while providing workarounds or approved exceptions.\"}),`\n`,(0,t.jsx)(e.p,{children:\"However, consider the ways that IDPs can standardize not just tools like CI/CD, but basic provisioning, configuration, monitoring, and more.\"}),`\n`,(0,t.jsx)(e.p,{children:\"Keeping the Qualities in mind, let\\u2019s look at the specific Components you should consider. \\xA0\"}),`\n`,(0,t.jsx)(e.p,{children:(0,t.jsx)(e.img,{src:\"/blog-images/094b8afc10a92ab374e4c478f504025c.png\",alt:\"\"})}),`\n`,(0,t.jsxs)(e.h3,{id:\"components-of-successful-idps\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#components-of-successful-idps\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Components of successful IDPs\"]}),`\n`,(0,t.jsx)(e.p,{children:\"The components that make up successful IDPs vary and don\\u2019t all need to be used at the same time. In fact, when getting started, you should never attempt to use this as a checklist, assuming your IDP will be successful when you\\u2019ve added all the components.\"}),`\n`,(0,t.jsx)(e.p,{children:\"Additionally, not all components need to be part of a central IDP deployment. They can involve other tools and bits of automation that work to improve the efficiency of your development teams, but function outside of the central IDP deployment.\"}),`\n`,(0,t.jsx)(e.p,{children:\"When you\\u2019re planning what to tackle first, consider which components of an IDP will provide your organization with the most value.\"}),`\n`,(0,t.jsx)(e.p,{children:\"Some components are necessary for all organizations, like code repositories, IDEs and CI/CD. Without them you\\u2019re missing the basics that all teams that write code need.\"}),`\n`,(0,t.jsx)(e.p,{children:\"Other components should be present, but not all orgs have automated them or added any integrations into the developer workflow to make them worthwhile just yet. This includes items like automated security scanning, typically integrated through your CI/CD pipeline.\"}),`\n`,(0,t.jsx)(e.p,{children:\"And then there are items specific to certain industries. For example, not every organization needs full integration with compliance and regulatory features.\"}),`\n`,(0,t.jsx)(e.p,{children:\"Finally, not all of these components need to be integrated through an IDP from the start. You can build upon and expand as the value becomes clear. But once you have a number of these components that need to work together, incorporating them and integrating them into your IDP will simplify the development workflow. These integrations can remove unnecessary cognitive load of configuring and context-switching for your dev team and allow them to focus on more valuable tasks.\"}),`\n`,(0,t.jsx)(e.p,{children:\"IDP components can include but are not limited to:\"}),`\n`,(0,t.jsxs)(e.ol,{children:[`\n`,(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:\"Code repositories:\"}),\" OK, so there are some of these components that are non-negotiable. And our repositories, like git, are # 1.\"]}),`\n`,(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:\"Infrastructure:\"}),\" This can include building, orchestrating, configuration management, \",(0,t.jsx)(e.a,{href:\"https://prod.releasehub.com/blog/a-simple-guide-to-software-environments\",children:\"environment management\"}),\", and monitoring of the infrastructure used by your development team.\"]}),`\n`,(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:\"Development tools:\"}),\" This includes IDEs, plugins, and extensions as well as stand-alone tools. A good IDP can integrate with other tools through plugins and extensions, all built out as part of the IDP.\"]}),`\n`,(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:\"CI/CD:\"}),\" As this is one of the basic tools IDPs use, consider ways to supercharge this functionality. In addition to the usual CI/CD functionality, can you automate changelogs, notify operations of deployments, and track failures and rollbacks?\"]}),`\n`,(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:\"Automated testing:\"}),\" In addition to automating unit tests, integration tests, and others, a successful IDP will drive advanced test features like flaky test detection, load testing, and even chaos engineering.\"]}),`\n`,(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:\"Security:\"}),\" The basics of security could involve static analysis tools in your CI/CD pipeline, \",(0,t.jsx)(e.a,{href:\"https://release.com/blog/kubernetes-secrets-management-a-practical-guide\",children:\"secrets management\"}),\", or live monitoring of security threats.\"]}),`\n`,(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:\"Monitoring/observability:\"}),\" Monitoring and observability provide your teams with the ability to ensure your code actually does what it is meant to do. It also can help teams not only identify problems (through monitoring alerts around availability or critical functionality) but also help find the root causes. We can also think about more than just a link from our IDP to our monitoring solutions, but integrations with our CI/CD tools to identify where problems occur and when the problems were introduced.\"]}),`\n`,(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:\"Project management and collaboration tools:\"}),\" When you\\u2019re looking at the IDP, do not just consider purely developer-focused tools. How can your IDP integrate your deployment processes with your project management and notifications?\"]}),`\n`,(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:\"Compliance and governance tools and workflows:\"}),\" We talked a bit about this when discussing access requests earlier. Consider what other manual processes and reporting could be automated and integrated into your IDP. And don\\u2019t think of this as just automating a report. Consider what can be done to remove the need for reports and manual intervention.\"]}),`\n`,(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:\"Workflow automation:\"}),\"\\xA0Workflow automation is a bit of a catchall for tools that automate workflows. Once you\\u2019ve covered the basic components, you can begin to automate processes like creating new services, upgrading systems, or creating changelogs and API versions. Since the IDP centers around improving developer workflows, you\\u2019ll need to look outside of the basic components and find new ways to reduce toil.\"]}),`\n`]}),`\n`,(0,t.jsxs)(e.h3,{id:\"common-failures-of-idps\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#common-failures-of-idps\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Common Failures of IDPs\"]}),`\n`,(0,t.jsx)(e.p,{children:\"So, if an IDP covers all or a majority of the key components, why do many still fail to provide value? Why do they struggle with adoption?\"}),`\n`,(0,t.jsx)(e.p,{children:\"Well, several things can go wrong. We\\u2019ll cover just four of them.\"}),`\n`,(0,t.jsxs)(e.p,{children:[\"First, the IDP may be \",(0,t.jsx)(e.strong,{children:\"focused on niche activities and processes and not the 80% of functionality that most developers need\"}),\". For example, if you\\u2019re in a monolithic environment, automating project initiation for microservices obviously won\\u2019t add much value. On the other hand, if your \",(0,t.jsx)(e.a,{href:\"https://prod.releasehub.com/blog/improve-developer-velocity-with-ephemeral-environments\",children:\"team struggles with getting fast feedback, slow velocity, or environment bottlenecks for testing\"}),\", then spinning up \",(0,t.jsx)(e.a,{href:\"https://release.com/ephemeral-environments\",children:\"ephemeral environments\"}),\" might be worth your time.\"]}),`\n`,(0,t.jsxs)(e.p,{children:[\"Second, \",(0,t.jsxs)(e.strong,{children:[\"to understand your developers, you must consider \",(0,t.jsx)(e.em,{children:\"all\"}),\" of your developers\"]}),\". Do not consider just the strongest, newest, or loudest developers. Interview a wide sample of your teams and consider their entire processes, so that you build a truly user-centric product.\"]}),`\n`,(0,t.jsxs)(e.p,{children:[\"Third, sometimes \",(0,t.jsx)(e.strong,{children:\"in an effort to remove security risks, companies will remove flexibility or functionality that the development team relies on\"}),\". For example, in an attempt to automate environment provisioning, an IDP may lock down security on configuration that developers used to have access to. This may be the right move, but you have to have a way to adjust to specific use cases. There should be guardrails while still making it possible to work outside of those guardrails when necessary.\"]}),`\n`,(0,t.jsxs)(e.p,{children:[\"And finally, perhaps you \",(0,t.jsx)(e.strong,{children:\"haven\\u2019t considered or built a rollout strategy\"}),\". You will need to consider how to onboard teams, get feedback, and quickly adjust and pivot based on that feedback and data. You can\\u2019t just build it and assume everyone will start using the tools and integrations available. It will take time and effort.\"]}),`\n`,(0,t.jsxs)(e.h3,{id:\"summary\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#summary\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Summary\"]}),`\n`,(0,t.jsx)(e.p,{children:\"When considering your future IDP or finding the next component your IDP should cover, consider the 80% of the functionality that your development teams need frequently. Add automated, standardized, self-serve functionality that addresses the majority of development workflows. And make sure you have enough of a feedback loop to ensure you\\u2019re building the right platform and improving the development cycle. In our next chapter we will look at the Build vs Buy question. Stay tuned!\"})]})}function I(o={}){let{wrapper:e}=o.components||{};return e?(0,t.jsx)(e,Object.assign({},o,{children:(0,t.jsx)(c,o)})):c(o)}var k=I;return w(D);})();\n;return Component;"
        },
        "_id": "blog/posts/components-of-a-successful-idp-build-a-product-your-developers-actually-want-to-use.mdx",
        "_raw": {
          "sourceFilePath": "blog/posts/components-of-a-successful-idp-build-a-product-your-developers-actually-want-to-use.mdx",
          "sourceFileName": "components-of-a-successful-idp-build-a-product-your-developers-actually-want-to-use.mdx",
          "sourceFileDir": "blog/posts",
          "contentType": "mdx",
          "flattenedPath": "blog/posts/components-of-a-successful-idp-build-a-product-your-developers-actually-want-to-use"
        },
        "type": "BlogPost",
        "computedSlug": "components-of-a-successful-idp-build-a-product-your-developers-actually-want-to-use"
      },
      "documentHash": "1739393595017",
      "hasWarnings": false,
      "documentTypeName": "BlogPost"
    },
    "blog/posts/continuous-deployment-webinar-recap.mdx": {
      "document": {
        "title": "Continuous Deployment Webinar Recap",
        "summary": "Tommy McClung joins the DevOps.com webinar to discuss the challenges of Continuous Deployment and what the future holds",
        "publishDate": "Tue Jul 19 2022 16:22:24 GMT+0000 (Coordinated Universal Time)",
        "author": "anna-chandler",
        "readingTime": 3,
        "categories": [
          "platform-engineering",
          "product"
        ],
        "mainImage": "/blog-images/4580125372e2a0f502b051d022ee3301.png",
        "imageAlt": "Continuous Deployment Webinar Recap",
        "showCTA": true,
        "ctaCopy": "Accelerate CD with on-demand environments for seamless collaboration, faster bug resolution, and consistent deployments.",
        "ctaLink": "https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=continuous-deployment-webinar-recap",
        "relatedPosts": [
          ""
        ],
        "ogImage": "/blog-images/4580125372e2a0f502b051d022ee3301.png",
        "excerpt": "Tommy McClung joins the DevOps.com webinar to discuss the challenges of Continuous Deployment and what the future holds",
        "tags": [
          "platform-engineering",
          "product"
        ],
        "ctaButton": "Try Release for Free",
        "body": {
          "raw": "\nTommy McClung is featured on a DevOps.com [webinar](https://webinars.devops.com/continuous-deployment?utm_campaign=%242022.07.11%24_EdCal_Panel_Webinar_DO&utm_source=Release) to discuss the ins and outs of Continuous Deployment.  He is joined by Field CTO of Harness and Senior Director of Engineering at Everbridge as they dive into the challenges of CD.\n\nTommy, the co-founder and CEO of Release, has been building scalable infrastructure for at least over 20 years. He’s also a serial entrepreneur. He co-founded CarWoo!, which was acquired by TrueCar.\n\nIt’s no secret that continuous deployment is challenging, because automating deployments across platforms has never been especially easy. Continuous Deployment also implies that you know where everything is being deployed into and you have environments that are stood up.  Often platforms are customized to the point where it is difficult to automate application deployments—each platform is its own unique \"snowflake.\"\n\nHowever, the growing complexity of application development projects coupled with increased reliance on software to drive digital business transformation initiatives and concerns over software supply chain integrity requires organizations to revisit their approach to application delivery.\n\n![](/blog-images/89d12d93c3ac871880032477371b92eb.png)\n\nTommy has been working on this problem for many years. As a developer at heart, he cares about getting great ideas to the world quickly, and environments tend to be a key ingredient in making that happen. This is why he created ReleaseHub and the concept of ephemeral environments, or “environments as a service”. With ReleaseHub, developers can create on demand environments with the click of a button or via a pull request, while maintaining robust velocity and faster deployment.\n\nIf you’re looking to have some burning questions answered such as “How automated can CD get?” or “How do DevOps teams roll things back when something goes wrong?”, you’re in the right place.  Tommy, along with his fellow speakers answer these questions along with many others.  \n\nCheck it out [here](https://webinars.devops.com/continuous-deployment?utm_campaign=%242022.07.11%24_EdCal_Panel_Webinar_DO&utm_source=Release)\n\nWant to try Release for yourself? [Request a demo.](https://releasehub.com/)\n",
          "code": "var Component=(()=>{var m=Object.create;var i=Object.defineProperty;var h=Object.getOwnPropertyDescriptor;var d=Object.getOwnPropertyNames;var p=Object.getPrototypeOf,g=Object.prototype.hasOwnProperty;var y=(n,e)=>()=>(e||n((e={exports:{}}).exports,e),e.exports),f=(n,e)=>{for(var o in e)i(n,o,{get:e[o],enumerable:!0})},r=(n,e,o,s)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let a of d(e))!g.call(n,a)&&a!==o&&i(n,a,{get:()=>e[a],enumerable:!(s=h(e,a))||s.enumerable});return n};var b=(n,e,o)=>(o=n!=null?m(p(n)):{},r(e||!n||!n.__esModule?i(o,\"default\",{value:n,enumerable:!0}):o,n)),w=n=>r(i({},\"__esModule\",{value:!0}),n);var c=y((k,l)=>{l.exports=_jsx_runtime});var D={};f(D,{default:()=>_,frontmatter:()=>v});var t=b(c()),v={title:\"Continuous Deployment Webinar Recap\",summary:\"Tommy McClung joins the DevOps.com webinar to discuss the challenges of Continuous Deployment and what the future holds\",publishDate:\"Tue Jul 19 2022 16:22:24 GMT+0000 (Coordinated Universal Time)\",author:\"anna-chandler\",readingTime:3,categories:[\"platform-engineering\",\"product\"],mainImage:\"/blog-images/4580125372e2a0f502b051d022ee3301.png\",imageAlt:\"Continuous Deployment Webinar Recap\",showCTA:!0,ctaCopy:\"Accelerate CD with on-demand environments for seamless collaboration, faster bug resolution, and consistent deployments.\",ctaLink:\"https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=continuous-deployment-webinar-recap\",relatedPosts:[\"\"],ogImage:\"/blog-images/4580125372e2a0f502b051d022ee3301.png\",excerpt:\"Tommy McClung joins the DevOps.com webinar to discuss the challenges of Continuous Deployment and what the future holds\",tags:[\"platform-engineering\",\"product\"],ctaButton:\"Try Release for Free\"};function u(n){let e=Object.assign({p:\"p\",a:\"a\",img:\"img\"},n.components);return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsxs)(e.p,{children:[\"Tommy McClung is featured on a DevOps.com \",(0,t.jsx)(e.a,{href:\"https://webinars.devops.com/continuous-deployment?utm_campaign=%242022.07.11%24_EdCal_Panel_Webinar_DO&utm_source=Release\",children:\"webinar\"}),\" to discuss the ins and outs of Continuous Deployment. \\xA0He is joined by Field CTO of Harness and Senior Director of Engineering at Everbridge as they dive into the challenges of CD.\"]}),`\n`,(0,t.jsx)(e.p,{children:\"Tommy, the co-founder and CEO of Release, has been building scalable infrastructure for at least over 20 years. He\\u2019s also a serial entrepreneur. He co-founded CarWoo!, which was acquired by TrueCar.\"}),`\n`,(0,t.jsx)(e.p,{children:'It\\u2019s no secret that continuous deployment is challenging, because automating deployments across platforms has never been especially easy. Continuous Deployment also implies that you know where everything is being deployed into and you have environments that are stood up. \\xA0Often platforms are customized to the point where it is difficult to automate application deployments\\u2014each platform is its own unique \"snowflake.\"'}),`\n`,(0,t.jsx)(e.p,{children:\"However, the growing complexity of application development projects coupled with increased reliance on software to drive digital business transformation initiatives and concerns over software supply chain integrity requires organizations to revisit their approach to application delivery.\"}),`\n`,(0,t.jsx)(e.p,{children:(0,t.jsx)(e.img,{src:\"/blog-images/89d12d93c3ac871880032477371b92eb.png\",alt:\"\"})}),`\n`,(0,t.jsx)(e.p,{children:\"Tommy has been working on this problem for many years. As a developer at heart, he cares about getting great ideas to the world quickly, and environments tend to be a key ingredient in making that happen. This is why he created ReleaseHub and the concept of ephemeral environments, or \\u201Cenvironments as a service\\u201D. With ReleaseHub, developers can create on demand environments with the click of a button or via a pull request, while maintaining robust velocity and faster deployment.\"}),`\n`,(0,t.jsx)(e.p,{children:\"If you\\u2019re looking to have some burning questions answered such as \\u201CHow automated can CD get?\\u201D or \\u201CHow do DevOps teams roll things back when something goes wrong?\\u201D, you\\u2019re in the right place. \\xA0Tommy, along with his fellow speakers answer these questions along with many others. \\xA0\"}),`\n`,(0,t.jsxs)(e.p,{children:[\"Check it out \",(0,t.jsx)(e.a,{href:\"https://webinars.devops.com/continuous-deployment?utm_campaign=%242022.07.11%24_EdCal_Panel_Webinar_DO&utm_source=Release\",children:\"here\"})]}),`\n`,(0,t.jsxs)(e.p,{children:[\"Want to try Release for yourself? \",(0,t.jsx)(e.a,{href:\"https://releasehub.com/\",children:\"Request a demo.\"})]})]})}function C(n={}){let{wrapper:e}=n.components||{};return e?(0,t.jsx)(e,Object.assign({},n,{children:(0,t.jsx)(u,n)})):u(n)}var _=C;return w(D);})();\n;return Component;"
        },
        "_id": "blog/posts/continuous-deployment-webinar-recap.mdx",
        "_raw": {
          "sourceFilePath": "blog/posts/continuous-deployment-webinar-recap.mdx",
          "sourceFileName": "continuous-deployment-webinar-recap.mdx",
          "sourceFileDir": "blog/posts",
          "contentType": "mdx",
          "flattenedPath": "blog/posts/continuous-deployment-webinar-recap"
        },
        "type": "BlogPost",
        "computedSlug": "continuous-deployment-webinar-recap"
      },
      "documentHash": "1739393595017",
      "hasWarnings": false,
      "documentTypeName": "BlogPost"
    },
    "blog/posts/contributing-to-open-source.mdx": {
      "document": {
        "title": "Contributing To Open Source - Getting Started",
        "summary": "Contributing to open source projects can be very fulfilling, but getting started may be intimidating and confusing. Most",
        "publishDate": "Fri Jun 11 2021 03:05:52 GMT+0000 (Coordinated Universal Time)",
        "author": "vicky-koblinski",
        "readingTime": 7,
        "categories": [
          "platform-engineering",
          "product"
        ],
        "mainImage": "/blog-images/d5645b39f3265e59b32fe6a9f7a9af31.jpg",
        "imageAlt": "Group of friends hugging each other conveying the idea of community",
        "showCTA": true,
        "ctaCopy": "Improve your open source contributions with Release's ephemeral environments for streamlined collaboration and efficient testing.",
        "ctaLink": "https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=contributing-to-open-source",
        "relatedPosts": [
          ""
        ],
        "ogImage": "/blog-images/d5645b39f3265e59b32fe6a9f7a9af31.jpg",
        "excerpt": "Contributing to open source projects can be very fulfilling, but getting started may be intimidating and confusing. Most",
        "tags": [
          "platform-engineering",
          "product"
        ],
        "ctaButton": "Try Release for Free",
        "body": {
          "raw": "\nContributing to open source projects can be very fulfilling, but getting started may be intimidating and confusing. Most large open source projects have steep learning curves which can be off-putting for the first-time contributor. However, there are a lot of opportunities to get started and thrive, even if you’ve never contributed to an open source project before. \n\nOpen source is incredibly valuable. Where would the Linux kernel be without the 13,500 developers and 1,300 companies that have contributed since 2005? And what of the thousands of projects built on its back?\n\nThese days, every project you start is built using packages and building blocks from those who came before you. You can stand on the shoulders of giants. The wheel has already been invented, and it’s more reliable and battle-tested than anything you could build in-house. Open source software has been essential for developers, companies, and, ideally, consumers.\n\nHowever, there would be no giants, no blocks, no wheels without communities, contributors, and passionate people to drive these projects forward. These are ordinary people doing extraordinary things. \n\nAlthough the main objective of open source is to create accessible and valuable software for everyone to build off of, contributors can find a plethora of personal benefits from contributing. You will sharpen your technical skills, nurture your interpersonal communication skills, and practice giving and receiving feedback. \n\n### The Culture\n\nOkay, so you’ve decided you want to become a contributor but you’re not convinced that you’ve got what it takes. You’re feeling quite intimidated actually. That’s okay. You’ve never done this before. Every contributor has had a first contribution and open source projects tend to culture empathy, patience, and understanding.\n\nIn my experience, successful open source projects have maintainers and communities that are the friendliest on the planet. They are excited and enthusiastic about someone wanting to contribute. Often they’ll be active on Twitter, Slack, Discord, and/or another platform that you can join and speak directly with the maintainers and other contributors. Many large open source communities even have their own community guidelines that boil down to “Don’t be a jerk”. These communities welcome new contributors with open arms.\n\n### Finding the Right Fit\n\nOne of the best ways to find an open source project to work on is to look to open source software that you already use. Tools, packages, frameworks, or languages that you work with regularly and enjoy using could be great candidates. To find out if the project is open source, check its license and if it accepts contributions. It’s also important to check that the project is actively maintained. Are the pull-requests sitting dormant or are the maintainers providing feedback? Are the issues getting acknowledged or are they sitting there stale?\n\nIf that approach doesn’t work for you, try using GitHub to explore. I recommend starting your contribution journey with languages and frameworks that you’re familiar with. It’ll be difficult enough to get up to speed on the codebase, but if you’re already familiar with a framework’s best practices and typical layout, you’ll have a more successful time hitting the ground running.\n\nGitHub has excellent search capabilities to find open source projects that are actively seeking out new contributors. By searching for the right tags and filtering by languages you know best, you can quickly find new issues that beg for assistance. Some great tags to search for are:\n\n- [contributions-welcome](https://github.com/topics/contributions-welcome)\n- [good-first-issue](https://github.com/topics/good-first-issue)\n- [hacktoberfest](https://github.com/topics/hacktoberfest)\n- [beginner-friendly](https://github.com/topics/beginner-friendly)\n- good-first-bug\n- [easy](https://github.com/topics/easy)\n- [low-hanging-fruit](https://github.com/topics/low-hanging-fruit)\n- [first-timers-only](https://github.com/topics/first-timers-only)\n\nGitHub also suggests projects you may like based on the people and repositories that you have starred, follow, or watch. \n\n### Your First Contribution\n\nOnce you find an issue that you feel confident in tackling in an active project that has a culture you feel comfortable in, it’s time to get your hands dirty! The first thing you will want to do is to “claim” the ticket. It’s a bad idea to run off and solve the problem without communicating your intentions to the maintainers. Instead, reply to the ticket in question by volunteering yourself for the task. This allows the maintainers and other contributors to know this ticket is being actively worked on and the maintainers may have requests, suggestions, or guidance to help solve the problem.\n\nStart small. Pick tickets that are easy and have the smallest contribution and code changes when you first start. Not only will this enable you to slowly get familiar with the codebase, but it will also build your confidence and credibility before you try taking on harder tasks. \n\nAfter you have picked up a ticket and think you have finished it, do your research before you open your first pull-request. Carefully read the documentation, code, and discussions related to this ticket to get the best understanding of how to handle the problem. If you’re stuck, reach out to the community and ask for guidance, clarification, or mentorship. \n\nOnce you feel confident that you’ve solved the problem, it’s time to submit a PR. Look in the project for a **CONTRIBUTORS.md** file--most open source projects on GitHub will likely have this within their project. This file will contain instructions on how this project would like PRs to be submitted by contributors. They may request strict branch naming conventions, PR titles, documentation or tests, comments, or other things in pull-requests. GitHub also has a [great checklist](https://opensource.guide/how-to-contribute/#a-checklist-before-you-contribute) on what to check for before you open your pull-request. \n\nCreate the PR and follow the PR template if the project has one. Be sure to link back to the original ticket. The maintainer may request changes or want to have a discussion about your changes. It’s normal to have a back-and-forth before a PR is accepted. Work with the maintainer until your PR is ready to be accepted.\n\nOnce your PR passes the maintainer’s review, they will merge your code in.   \n**Congratulations!** You’re an open source contributor!!\n\n### Are You An Open Source Project Maintainer? Release Wants To Support You!\n\nOpen source projects benefit dramatically from Ephemeral Environments on every pull-request. We love open source and are dedicated to giving back. [Contact us](https://share.hsforms.com/1YM0zqIgsTc2aD2oPDQUQJA4shs5) to find out how we can help fuel your rocket through preview environments and help put confidence in contributor’s PRs.\n",
          "code": "var Component=(()=>{var h=Object.create;var r=Object.defineProperty;var d=Object.getOwnPropertyDescriptor;var p=Object.getOwnPropertyNames;var g=Object.getPrototypeOf,f=Object.prototype.hasOwnProperty;var m=(n,e)=>()=>(e||n((e={exports:{}}).exports,e),e.exports),b=(n,e)=>{for(var o in e)r(n,o,{get:e[o],enumerable:!0})},s=(n,e,o,a)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let i of p(e))!f.call(n,i)&&i!==o&&r(n,i,{get:()=>e[i],enumerable:!(a=d(e,i))||a.enumerable});return n};var y=(n,e,o)=>(o=n!=null?h(g(n)):{},s(e||!n||!n.__esModule?r(o,\"default\",{value:n,enumerable:!0}):o,n)),w=n=>s(r({},\"__esModule\",{value:!0}),n);var u=m((C,c)=>{c.exports=_jsx_runtime});var T={};b(T,{default:()=>j,frontmatter:()=>k});var t=y(u()),k={title:\"Contributing To Open Source - Getting Started\",summary:\"Contributing to open source projects can be very fulfilling, but getting started may be intimidating and confusing. Most\",publishDate:\"Fri Jun 11 2021 03:05:52 GMT+0000 (Coordinated Universal Time)\",author:\"vicky-koblinski\",readingTime:7,categories:[\"platform-engineering\",\"product\"],mainImage:\"/blog-images/d5645b39f3265e59b32fe6a9f7a9af31.jpg\",imageAlt:\"Group of friends hugging each other conveying the idea of community\",showCTA:!0,ctaCopy:\"Improve your open source contributions with Release's ephemeral environments for streamlined collaboration and efficient testing.\",ctaLink:\"https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=contributing-to-open-source\",relatedPosts:[\"\"],ogImage:\"/blog-images/d5645b39f3265e59b32fe6a9f7a9af31.jpg\",excerpt:\"Contributing to open source projects can be very fulfilling, but getting started may be intimidating and confusing. Most\",tags:[\"platform-engineering\",\"product\"],ctaButton:\"Try Release for Free\"};function l(n){let e=Object.assign({p:\"p\",h3:\"h3\",a:\"a\",span:\"span\",ul:\"ul\",li:\"li\",strong:\"strong\",br:\"br\"},n.components);return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(e.p,{children:\"Contributing to open source projects can be very fulfilling, but getting started may be intimidating and confusing. Most large open source projects have steep learning curves which can be off-putting for the first-time contributor. However, there are a lot of opportunities to get started and thrive, even if you\\u2019ve never contributed to an open source project before.\\xA0\"}),`\n`,(0,t.jsx)(e.p,{children:\"Open source is incredibly valuable. Where would the Linux kernel be without the 13,500 developers and 1,300 companies that have contributed since 2005? And what of the thousands of projects built on its back?\"}),`\n`,(0,t.jsx)(e.p,{children:\"These days, every project you start is built using packages and building blocks from those who came before you. You can stand on the shoulders of giants. The wheel has already been invented, and it\\u2019s more reliable and battle-tested than anything you could build in-house. Open source software has been essential for developers, companies, and, ideally, consumers.\"}),`\n`,(0,t.jsx)(e.p,{children:\"However, there would be no giants, no blocks, no wheels without communities, contributors, and passionate people to drive these projects forward. These are ordinary people doing extraordinary things.\\xA0\"}),`\n`,(0,t.jsx)(e.p,{children:\"Although the main objective of open source is to create accessible and valuable software for everyone to build off of, contributors can find a plethora of personal benefits from contributing. You will sharpen your technical skills, nurture your interpersonal communication skills, and practice giving and receiving feedback.\\xA0\"}),`\n`,(0,t.jsxs)(e.h3,{id:\"the-culture\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#the-culture\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"The Culture\"]}),`\n`,(0,t.jsx)(e.p,{children:\"Okay, so you\\u2019ve decided you want to become a contributor but you\\u2019re not convinced that you\\u2019ve got what it takes. You\\u2019re feeling quite intimidated actually. That\\u2019s okay. You\\u2019ve never done this before. Every contributor has had a first contribution and open source projects tend to culture empathy, patience, and understanding.\"}),`\n`,(0,t.jsx)(e.p,{children:\"In my experience, successful open source projects have maintainers and communities that are the friendliest on the planet. They are excited and enthusiastic about someone wanting to contribute. Often they\\u2019ll be active on Twitter, Slack, Discord, and/or another platform that you can join and speak directly with the maintainers and other contributors. Many large open source communities even have their own community guidelines that boil down to \\u201CDon\\u2019t be a jerk\\u201D. These communities welcome new contributors with open arms.\"}),`\n`,(0,t.jsxs)(e.h3,{id:\"finding-the-right-fit\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#finding-the-right-fit\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Finding the Right Fit\"]}),`\n`,(0,t.jsx)(e.p,{children:\"One of the best ways to find an open source project to work on is to look to open source software that you already use. Tools, packages, frameworks, or languages that you work with regularly and enjoy using could be great candidates. To find out if the project is open source, check its license and if it accepts contributions. It\\u2019s also important to check that the project is actively maintained. Are the pull-requests sitting dormant or are the maintainers providing feedback? Are the issues getting acknowledged or are they sitting there stale?\"}),`\n`,(0,t.jsx)(e.p,{children:\"If that approach doesn\\u2019t work for you, try using GitHub to explore. I recommend starting your contribution journey with languages and frameworks that you\\u2019re familiar with. It\\u2019ll be difficult enough to get up to speed on the codebase, but if you\\u2019re already familiar with a framework\\u2019s best practices and typical layout, you\\u2019ll have a more successful time hitting the ground running.\"}),`\n`,(0,t.jsx)(e.p,{children:\"GitHub has excellent search capabilities to find open source projects that are actively seeking out new contributors. By searching for the right tags and filtering by languages you know best, you can quickly find new issues that beg for assistance. Some great tags to search for are:\"}),`\n`,(0,t.jsxs)(e.ul,{children:[`\n`,(0,t.jsx)(e.li,{children:(0,t.jsx)(e.a,{href:\"https://github.com/topics/contributions-welcome\",children:\"contributions-welcome\"})}),`\n`,(0,t.jsx)(e.li,{children:(0,t.jsx)(e.a,{href:\"https://github.com/topics/good-first-issue\",children:\"good-first-issue\"})}),`\n`,(0,t.jsx)(e.li,{children:(0,t.jsx)(e.a,{href:\"https://github.com/topics/hacktoberfest\",children:\"hacktoberfest\"})}),`\n`,(0,t.jsx)(e.li,{children:(0,t.jsx)(e.a,{href:\"https://github.com/topics/beginner-friendly\",children:\"beginner-friendly\"})}),`\n`,(0,t.jsx)(e.li,{children:\"good-first-bug\"}),`\n`,(0,t.jsx)(e.li,{children:(0,t.jsx)(e.a,{href:\"https://github.com/topics/easy\",children:\"easy\"})}),`\n`,(0,t.jsx)(e.li,{children:(0,t.jsx)(e.a,{href:\"https://github.com/topics/low-hanging-fruit\",children:\"low-hanging-fruit\"})}),`\n`,(0,t.jsx)(e.li,{children:(0,t.jsx)(e.a,{href:\"https://github.com/topics/first-timers-only\",children:\"first-timers-only\"})}),`\n`]}),`\n`,(0,t.jsx)(e.p,{children:\"GitHub also suggests projects you may like based on the people and repositories that you have starred, follow, or watch.\\xA0\"}),`\n`,(0,t.jsxs)(e.h3,{id:\"your-first-contribution\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#your-first-contribution\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Your First Contribution\"]}),`\n`,(0,t.jsx)(e.p,{children:\"Once you find an issue that you feel confident in tackling in an active project that has a culture you feel comfortable in, it\\u2019s time to get your hands dirty! The first thing you will want to do is to \\u201Cclaim\\u201D the ticket. It\\u2019s a bad idea to run off and solve the problem without communicating your intentions to the maintainers. Instead, reply to the ticket in question by volunteering yourself for the task. This allows the maintainers and other contributors to know this ticket is being actively worked on and the maintainers may have requests, suggestions, or guidance to help solve the problem.\"}),`\n`,(0,t.jsx)(e.p,{children:\"Start small. Pick tickets that are easy and have the smallest contribution and code changes when you first start. Not only will this enable you to slowly get familiar with the codebase, but it will also build your confidence and credibility before you try taking on harder tasks.\\xA0\"}),`\n`,(0,t.jsx)(e.p,{children:\"After you have picked up a ticket and think you have finished it, do your research before you open your first pull-request. Carefully read the documentation, code, and discussions related to this ticket to get the best understanding of how to handle the problem. If you\\u2019re stuck, reach out to the community and ask for guidance, clarification, or mentorship.\\xA0\"}),`\n`,(0,t.jsxs)(e.p,{children:[\"Once you feel confident that you\\u2019ve solved the problem, it\\u2019s time to submit a PR. Look in the project for a \",(0,t.jsx)(e.strong,{children:\"CONTRIBUTORS.md\"}),\" file--most open source projects on GitHub will likely have this within their project. This file will contain instructions on how this project would like PRs to be submitted by contributors. They may request strict branch naming conventions, PR titles, documentation or tests, comments, or other things in pull-requests. GitHub also has a \",(0,t.jsx)(e.a,{href:\"https://opensource.guide/how-to-contribute/#a-checklist-before-you-contribute\",children:\"great checklist\"}),\" on what to check for before you open your pull-request.\\xA0\"]}),`\n`,(0,t.jsx)(e.p,{children:\"Create the PR and follow the PR template if the project has one. Be sure to link back to the original ticket. The maintainer may request changes or want to have a discussion about your changes. It\\u2019s normal to have a back-and-forth before a PR is accepted. Work with the maintainer until your PR is ready to be accepted.\"}),`\n`,(0,t.jsxs)(e.p,{children:[\"Once your PR passes the maintainer\\u2019s review, they will merge your code in.\\xA0\",(0,t.jsx)(e.br,{}),`\n`,(0,t.jsx)(e.strong,{children:\"Congratulations!\"}),\" You\\u2019re an open source contributor!!\"]}),`\n`,(0,t.jsxs)(e.h3,{id:\"are-you-an-open-source-project-maintainer-release-wants-to-support-you\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#are-you-an-open-source-project-maintainer-release-wants-to-support-you\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Are You An Open Source Project Maintainer? Release Wants To Support You!\"]}),`\n`,(0,t.jsxs)(e.p,{children:[\"Open source projects benefit dramatically from Ephemeral Environments on every pull-request. We love open source and are dedicated to giving back. \",(0,t.jsx)(e.a,{href:\"https://share.hsforms.com/1YM0zqIgsTc2aD2oPDQUQJA4shs5\",children:\"Contact us\"}),\" to find out how we can help fuel your rocket through preview environments and help put confidence in contributor\\u2019s PRs.\"]})]})}function v(n={}){let{wrapper:e}=n.components||{};return e?(0,t.jsx)(e,Object.assign({},n,{children:(0,t.jsx)(l,n)})):l(n)}var j=v;return w(T);})();\n;return Component;"
        },
        "_id": "blog/posts/contributing-to-open-source.mdx",
        "_raw": {
          "sourceFilePath": "blog/posts/contributing-to-open-source.mdx",
          "sourceFileName": "contributing-to-open-source.mdx",
          "sourceFileDir": "blog/posts",
          "contentType": "mdx",
          "flattenedPath": "blog/posts/contributing-to-open-source"
        },
        "type": "BlogPost",
        "computedSlug": "contributing-to-open-source"
      },
      "documentHash": "1739393595017",
      "hasWarnings": false,
      "documentTypeName": "BlogPost"
    },
    "blog/posts/creating-private-ai-environments-with-release-ai-keep-your-data-and-intelligence-under-your-control.mdx": {
      "document": {
        "title": "Creating Private AI Environments with Release AI: Keep Your Data and Intelligence Under Your Control",
        "summary": "Learn the simple steps to set up a secure, efficient AI workspace without sending sensitive data over the internet.",
        "publishDate": "Tue Aug 27 2024 19:26:18 GMT+0000 (Coordinated Universal Time)",
        "author": "tommy-mcclung",
        "readingTime": 5,
        "categories": [
          "ai",
          "platform-engineering"
        ],
        "mainImage": "/blog-images/f8fb624d18480efe410061a55bcb695a.jpg",
        "imageAlt": "Creating Private AI Environments with Release AI: Keep Your Data and Intelligence Under Your Control",
        "showCTA": true,
        "ctaCopy": "Secure your AI data by hosting models within your control with Release's private environments management platform.",
        "ctaLink": "https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=creating-private-ai-environments-with-release-ai-keep-your-data-and-intelligence-under-your-control",
        "relatedPosts": [
          ""
        ],
        "ogImage": "/blog-images/f8fb624d18480efe410061a55bcb695a.jpg",
        "excerpt": "Learn the simple steps to set up a secure, efficient AI workspace without sending sensitive data over the internet.",
        "tags": [
          "ai",
          "platform-engineering"
        ],
        "ctaButton": "Try Release for Free",
        "body": {
          "raw": "\nLet's talk about how to create AI environments in Release AI that keep your AI data and intelligence fully within your control. To understand why this matters, we'll start by looking at how traditional AI applications work when you're not fully in control.\n\n#### The Problem with Traditional AI Applications\n\n![](/blog-images/fc88b2cd61dbddda01a22fdc52b07b2d.png)\n\n_Traditional AI application data flow diagram_\n\nHere's what typically happens:\n\n1.  Your application runs in your data center.\n2.  When you need AI capabilities - like observations, reasoning, or actions - you send data over the internet.\n3.  This data might include context, embeddings, or other sensitive information (think RAG applications).\n4.  Your data hits inference servers and GPUs in someone else's data center.\n5.  Your data could end up in their AI models, potentially used for training.\n6.  You have no control over anything outside your VPC or cloud.\n\n#### Introducing Release AI's Private AI Environments\n\nNow, let's look at how Release AI changes this:\n\n‍\n\n![](/blog-images/de2f5740a575897b930d7ac64a4bf4e7.png)\n\n_Release AI private environment diagram_\n\nWith Release AI's private AI environments:\n\n- Everything stays within your control.\n- Your application and data sources live in your VPC and cloud account.\n- Your data never goes out to the internet.\n- You host inference with models you've either gotten (like open-source models) or trained yourself.\n- All your data and AI intelligence stays completely within your control.\n\nThe best part? Setting this up with Release AI is incredibly simple. Let me show you how.\n\n#### Creating a Private AI Environment with Release AI\n\n1.  Connect to Your Cloud Account\n\n- Click \"Create Cloud Integration\"\n- Give it a name\n- Pick your cloud provider (AWS or GCP)\n\n‍\n\n![](/blog-images/37700bc0f118e71ef08a90e0b9907104.png)\n\n_Cloud integration setup screen_\n\nThis lets Release AI interact with your cloud to create an environment that's completely under your control. 2. Create a Cluster\n\n![](/blog-images/c224023da67fa279fa32dccc420f5517.png)\n\nCluster creation screen\n\nNext, you create a cluster that runs in your cloud account. In this example, I've already made one called \"release-ai-demo\". 3. Configure Your Cluster\n\nLet's look at the details:\n\n- It's using the cloud integration we just set up\n- It's a Kubernetes EKS cluster running version 1.29\n- It's fully running within your AWS or GCP account\n- All the nodes (in this case, I'm using G5 instances for AI workloads) are in your AWS account\n\n![](/blog-images/7dc3afccc0ccb87126e2e1822db79e91.png)\n\nCluster creation screen - details\n\n#### **Why This Matters**\n\nWhen you make queries to these instances, no data ever escapes your AI environment. Everything stays put:\n\n- All your data\n- All your compute\n- All your infrastructure\n\nIt's completely within your control. You don't have to worry about sending your data over the internet into somebody else's SaaS application where they could use it to train, fine-tune, or do who-knows-what with it.\n\n#### **Bottom Line**\n\nBy having a private AI environment running in your AWS account on your infrastructure, your applications, AI intelligence, and data stay fully within your control. It's that simple.  \n‍\n\nWatch the full demo [here](https://www.youtube.com/watch?v=0ysZ_bY0Gv8&t=2s).\n",
          "code": "var Component=(()=>{var d=Object.create;var a=Object.defineProperty;var u=Object.getOwnPropertyDescriptor;var p=Object.getOwnPropertyNames;var m=Object.getPrototypeOf,g=Object.prototype.hasOwnProperty;var y=(i,e)=>()=>(e||i((e={exports:{}}).exports,e),e.exports),v=(i,e)=>{for(var t in e)a(i,t,{get:e[t],enumerable:!0})},l=(i,e,t,o)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let r of p(e))!g.call(i,r)&&r!==t&&a(i,r,{get:()=>e[r],enumerable:!(o=u(e,r))||o.enumerable});return i};var w=(i,e,t)=>(t=i!=null?d(m(i)):{},l(e||!i||!i.__esModule?a(t,\"default\",{value:i,enumerable:!0}):t,i)),f=i=>l(a({},\"__esModule\",{value:!0}),i);var s=y((Y,c)=>{c.exports=_jsx_runtime});var C={};v(C,{default:()=>I,frontmatter:()=>b});var n=w(s()),b={title:\"Creating Private AI Environments with Release AI: Keep Your Data and Intelligence Under Your Control\",summary:\"Learn the simple steps to set up a secure, efficient AI workspace without sending sensitive data over the internet.\",publishDate:\"Tue Aug 27 2024 19:26:18 GMT+0000 (Coordinated Universal Time)\",author:\"tommy-mcclung\",readingTime:5,categories:[\"ai\",\"platform-engineering\"],mainImage:\"/blog-images/f8fb624d18480efe410061a55bcb695a.jpg\",imageAlt:\"Creating Private AI Environments with Release AI: Keep Your Data and Intelligence Under Your Control\",showCTA:!0,ctaCopy:\"Secure your AI data by hosting models within your control with Release's private environments management platform.\",ctaLink:\"https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=creating-private-ai-environments-with-release-ai-keep-your-data-and-intelligence-under-your-control\",relatedPosts:[\"\"],ogImage:\"/blog-images/f8fb624d18480efe410061a55bcb695a.jpg\",excerpt:\"Learn the simple steps to set up a secure, efficient AI workspace without sending sensitive data over the internet.\",tags:[\"ai\",\"platform-engineering\"],ctaButton:\"Try Release for Free\"};function h(i){let e=Object.assign({p:\"p\",h4:\"h4\",a:\"a\",span:\"span\",img:\"img\",em:\"em\",ol:\"ol\",li:\"li\",ul:\"ul\",strong:\"strong\",br:\"br\"},i.components);return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(e.p,{children:\"Let's talk about how to create AI environments in Release AI that keep your AI data and intelligence fully within your control. To understand why this matters, we'll start by looking at how traditional AI applications work when you're not fully in control.\"}),`\n`,(0,n.jsxs)(e.h4,{id:\"the-problem-with-traditional-ai-applications\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#the-problem-with-traditional-ai-applications\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"The Problem with Traditional AI Applications\"]}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/fc88b2cd61dbddda01a22fdc52b07b2d.png\",alt:\"\"})}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.em,{children:\"Traditional AI application data flow diagram\"})}),`\n`,(0,n.jsx)(e.p,{children:\"Here's what typically happens:\"}),`\n`,(0,n.jsxs)(e.ol,{children:[`\n`,(0,n.jsx)(e.li,{children:\"Your application runs in your data center.\"}),`\n`,(0,n.jsx)(e.li,{children:\"When you need AI capabilities - like observations, reasoning, or actions - you send data over the internet.\"}),`\n`,(0,n.jsx)(e.li,{children:\"This data might include context, embeddings, or other sensitive information (think RAG applications).\"}),`\n`,(0,n.jsx)(e.li,{children:\"Your data hits inference servers and GPUs in someone else's data center.\"}),`\n`,(0,n.jsx)(e.li,{children:\"Your data could end up in their AI models, potentially used for training.\"}),`\n`,(0,n.jsx)(e.li,{children:\"You have no control over anything outside your VPC or cloud.\"}),`\n`]}),`\n`,(0,n.jsxs)(e.h4,{id:\"introducing-release-ais-private-ai-environments\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#introducing-release-ais-private-ai-environments\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Introducing Release AI's Private AI Environments\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Now, let's look at how Release AI changes this:\"}),`\n`,(0,n.jsx)(e.p,{children:\"\\u200D\"}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/de2f5740a575897b930d7ac64a4bf4e7.png\",alt:\"\"})}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.em,{children:\"Release AI private environment diagram\"})}),`\n`,(0,n.jsx)(e.p,{children:\"With Release AI's private AI environments:\"}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsx)(e.li,{children:\"Everything stays within your control.\"}),`\n`,(0,n.jsx)(e.li,{children:\"Your application and data sources live in your VPC and cloud account.\"}),`\n`,(0,n.jsx)(e.li,{children:\"Your data never goes out to the internet.\"}),`\n`,(0,n.jsx)(e.li,{children:\"You host inference with models you've either gotten (like open-source models) or trained yourself.\"}),`\n`,(0,n.jsx)(e.li,{children:\"All your data and AI intelligence stays completely within your control.\"}),`\n`]}),`\n`,(0,n.jsx)(e.p,{children:\"The best part? Setting this up with Release AI is incredibly simple. Let me show you how.\"}),`\n`,(0,n.jsxs)(e.h4,{id:\"creating-a-private-ai-environment-with-release-ai\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#creating-a-private-ai-environment-with-release-ai\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Creating a Private AI Environment with Release AI\"]}),`\n`,(0,n.jsxs)(e.ol,{children:[`\n`,(0,n.jsx)(e.li,{children:\"Connect to Your Cloud Account\"}),`\n`]}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsx)(e.li,{children:'Click \"Create Cloud Integration\"'}),`\n`,(0,n.jsx)(e.li,{children:\"Give it a name\"}),`\n`,(0,n.jsx)(e.li,{children:\"Pick your cloud provider (AWS or GCP)\"}),`\n`]}),`\n`,(0,n.jsx)(e.p,{children:\"\\u200D\"}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/37700bc0f118e71ef08a90e0b9907104.png\",alt:\"\"})}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.em,{children:\"Cloud integration setup screen\"})}),`\n`,(0,n.jsx)(e.p,{children:\"This lets Release AI interact with your cloud to create an environment that's completely under your control. 2. Create a Cluster\"}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/c224023da67fa279fa32dccc420f5517.png\",alt:\"\"})}),`\n`,(0,n.jsx)(e.p,{children:\"Cluster creation screen\"}),`\n`,(0,n.jsx)(e.p,{children:`Next, you create a cluster that runs in your cloud account. In this example, I've already made one called \"release-ai-demo\". 3. Configure Your Cluster`}),`\n`,(0,n.jsx)(e.p,{children:\"Let's look at the details:\"}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsx)(e.li,{children:\"It's using the cloud integration we just set up\"}),`\n`,(0,n.jsx)(e.li,{children:\"It's a Kubernetes EKS cluster running version 1.29\"}),`\n`,(0,n.jsx)(e.li,{children:\"It's fully running within your AWS or GCP account\"}),`\n`,(0,n.jsx)(e.li,{children:\"All the nodes (in this case, I'm using G5 instances for AI workloads) are in your AWS account\"}),`\n`]}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/7dc3afccc0ccb87126e2e1822db79e91.png\",alt:\"\"})}),`\n`,(0,n.jsx)(e.p,{children:\"Cluster creation screen - details\"}),`\n`,(0,n.jsxs)(e.h4,{id:\"why-this-matters\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#why-this-matters\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.strong,{children:\"Why This Matters\"})]}),`\n`,(0,n.jsx)(e.p,{children:\"When you make queries to these instances, no data ever escapes your AI environment. Everything stays put:\"}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsx)(e.li,{children:\"All your data\"}),`\n`,(0,n.jsx)(e.li,{children:\"All your compute\"}),`\n`,(0,n.jsx)(e.li,{children:\"All your infrastructure\"}),`\n`]}),`\n`,(0,n.jsx)(e.p,{children:\"It's completely within your control. You don't have to worry about sending your data over the internet into somebody else's SaaS application where they could use it to train, fine-tune, or do who-knows-what with it.\"}),`\n`,(0,n.jsxs)(e.h4,{id:\"bottom-line\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#bottom-line\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.strong,{children:\"Bottom Line\"})]}),`\n`,(0,n.jsxs)(e.p,{children:[\"By having a private AI environment running in your AWS account on your infrastructure, your applications, AI intelligence, and data stay fully within your control. It's that simple.\",(0,n.jsx)(e.br,{}),`\n`,\"\\u200D\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"Watch the full demo \",(0,n.jsx)(e.a,{href:\"https://www.youtube.com/watch?v=0ysZ_bY0Gv8&t=2s\",children:\"here\"}),\".\"]})]})}function A(i={}){let{wrapper:e}=i.components||{};return e?(0,n.jsx)(e,Object.assign({},i,{children:(0,n.jsx)(h,i)})):h(i)}var I=A;return f(C);})();\n;return Component;"
        },
        "_id": "blog/posts/creating-private-ai-environments-with-release-ai-keep-your-data-and-intelligence-under-your-control.mdx",
        "_raw": {
          "sourceFilePath": "blog/posts/creating-private-ai-environments-with-release-ai-keep-your-data-and-intelligence-under-your-control.mdx",
          "sourceFileName": "creating-private-ai-environments-with-release-ai-keep-your-data-and-intelligence-under-your-control.mdx",
          "sourceFileDir": "blog/posts",
          "contentType": "mdx",
          "flattenedPath": "blog/posts/creating-private-ai-environments-with-release-ai-keep-your-data-and-intelligence-under-your-control"
        },
        "type": "BlogPost",
        "computedSlug": "creating-private-ai-environments-with-release-ai-keep-your-data-and-intelligence-under-your-control"
      },
      "documentHash": "1739393595017",
      "hasWarnings": false,
      "documentTypeName": "BlogPost"
    },
    "blog/posts/creating-your-first-application-in-release-with-docker-compose.mdx": {
      "document": {
        "title": "Creating your first Application in Release with Docker Compose",
        "summary": "Take your Docker Compose file and Crete an Application to run on Kubernetes in six easy steps.",
        "publishDate": "Wed Jun 28 2023 14:46:59 GMT+0000 (Coordinated Universal Time)",
        "author": "jeremy-kreutzbender",
        "readingTime": 10,
        "categories": [
          "platform-engineering",
          "kubernetes"
        ],
        "mainImage": "/blog-images/24d9e9521de18493481de2d2321d4516.jpg",
        "imageAlt": "Creating your first Application in Release with Docker Compose",
        "showCTA": true,
        "ctaCopy": "Explore how Release simplifies deploying Kubernetes applications with ephemeral environments for faster testing and streamlined collaboration.",
        "ctaLink": "https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=creating-your-first-application-in-release-with-docker-compose",
        "relatedPosts": [
          ""
        ],
        "ogImage": "/blog-images/24d9e9521de18493481de2d2321d4516.jpg",
        "excerpt": "Take your Docker Compose file and Crete an Application to run on Kubernetes in six easy steps.",
        "tags": [
          "platform-engineering",
          "kubernetes"
        ],
        "ctaButton": "Try Release for Free",
        "body": {
          "raw": "\nIf you’re using Docker compose for local development but have been interested in running your application on Kubernetes or creating ephemeral environments for your application, keep on reading, this post is for you!    \n\nAt Release we know that Applications consist of more than just your repository and code. There are other services that are required, such as databases or key value stores. An application usually cannot run without environment variables, backing data, infrastructure, or storage component(s). That’s why we think that a Docker compose file is one of the best ways to describe your application for local development. It is also a perfect way to get started on Release and get your near-production environments spun up with each pull request. In this blog, we will walk through the steps to create an Application, highlight how Release helps transform your compose file into an Application Template and ultimately deploy it on Kubernetes (which we will cover in part two of this series).\n\nWe’ll be using [https://github.com/awesome-release/rails_postgres_redis](https://github.com/awesome-release/rails_postgres_redis) as the example in this post. It is a small application that runs a Ruby on Rails server, has requirements of a Postgresql database and a Redis server, as well as runs Sidekiq, which is a background job processor.\n\nLet’s take a look at the compose file and then jump into creating our Application.\n\n```yaml\n\nversion: '3'\nservices:\n  api:\n    build: .\n    image: rails_postgres_redis:latest\n    command: bash -c \"(rake db:exists && rake db:migrate || rake db:setup) && bundle exec rails s -b 0.0.0.0\"\n    environment:\n      REDIS_URL: redis://redis:6379/0\n      POSTGRES_PASSWORD: postgres\n      POSTGRES_USER: postgres\n      POSTGRES_HOST: db\n    ports:\n      - \"3000:3000\"\n    depends_on:\n      - db\n      - redis\n  \n  sidekiq:\n    image: rails_postgres_redis:latest\n    command: bundle exec sidekiq\n    environment:\n      REDIS_URL: redis://redis:6379/0\n      POSTGRES_PASSWORD: postgres\n      POSTGRES_USER: postgres\n      POSTGRES_HOST: db\n    depends_on:\n      - api\n      - redis\n      - db    \n\n  db:\n    image: postgres:12-alpine\n    ports:\n      - \"5432\"\n    volumes:\n      - postgres-data:/var/lib/postgresql/data\n    environment:\n      POSTGRES_PASSWORD: postgres\n      POSTGRES_USER: postgres\n\n  redis:\n    image: redis\n    ports:\n      - \"6379\"\n    volumes:\n      - redis:/data\n\nvolumes:\n  postgres-data: {}\n  redis: {}\n\n```\n\n‍\n\nWe won’t go through everything line by line but we wanted to make sure to note that in this compose file certain ports are exposed, as well as volumes are being defined to retain data between restarts. We also see that the containers have their `depends_on` defined to tell us in which order the containers should be started in. We encourage you to take a look at our [Docker Compose conversion support documentation](https://docs.release.com/reference-documentation/docker-compose-conversion-support) to see the full list of supported attributes.\n\n### Translating a Compose file into an Application\n\nNow that we have a Docker Compose file ready, let’s see how to translate it into an Application in Release.\n\n- **Step 1: Create your application**\n\nThe first step in creating your Application is to give it a name and select which repository we’ll be using. I’ve named mine “release-docker-compose-demo” and selected the awesome-release/rails_postgres_redis repository (linked above).\n\n![](/blog-images/ff3c06a3bb3d132f792d59947fecbbcc.png)\n\n- **Step 2: Analyze your repository**\n\nThe next step is to analyze the repository. We’ve selected the main branch and are shown some options of files we can select in a dropdown. The files we select will be analyzed and converted into what Release calls Services. These Services can have many types, such as Containers, Static Javascript Builds, Helm, and Terraform; we’ll primarily focus on Containers in this post but look for future posts where we’ll cover the other options.\n\nWe’ve selected our docker-compose.yml file and we will click the Start Analysis button.\n\n![](/blog-images/b96ad97ba2f3abdcdf27b467438e0e79.png)\n\nWe see that Release created four Services for us with the same names as what is in the compose file. The dependencies are also listed. In this case we know that we want all these Services, but if something had been displayed that we didn’t want to deploy on Release, we could uncheck the Service to remove it. Now we’ll click Next Step to move on.\n\n![](/blog-images/e0c91cdc3c2141465fd17ada6c65ce82.png)\n\n‍\\* **Step 3: Generate a template**\n\nIn this stage we get our first view of an Application Template. We won’t go through everything in the template here, however we encourage you to read through the documentation on [Application Templates](https://docs.release.com/reference-documentation/application-settings/application-template) to understand all the possibilities. Instead we’ll highlight how Release has translated the Services from the last page into this yaml format. We see the familiar names of  `api`, `db`, and `redis` as well as the ports and volumes that were defined in the compose file.\n\n![](/blog-images/2061a6acb4335f3eebcdc64912fd9f5d.png)\n\nThe definitions of Services help to describe _what_ Release will deploy, but we also want to know _how_ Release will deploy these Services. That information is contained in the workflows stanza. There are three types of workflows defined: **setup**, **patch**, and **teardown**.\n\nA **setup** workflow defines a deployment where infrastructure can be deployed for the first time or if there are subsequent changes to the infrastructure; think, changing your Postgresql version, the number of replicas of the  `api` Service, or changing environment variables for your Application. We can also see that the order of the Services from the compose file  `depends_on` is translated into the setup workflow. `db` and `redis` will be deployed in parallel first. Once both of those Services are up and running, Release will move to the next step and deploy  `api`. Finally  `sidekiq` will be deployed.\n\nThe **patch** workflow is used when only code changes need to be deployed. In our case, both the `api` and `sidekiq` Services contain the code from the repository and would need to be deployed when we push new changes. The  `db` and `redis` Services don’t require any changes so they don’t need to be referenced in a patch.\n\nThe final workflow is the **teardown** which uses a Release defined task called `remove_environment`. This task will tear down all the infrastructure in Kubernetes and free up the resources that were being used. Additional steps can be added to a teardown workflow but the `remove_environment` is a requirement.\n\nNow that we’ve had a quick runthrough of parts of our Application Template, we’ll click Next Step to move on.\n\n![](/blog-images/eb1e9b99de4ff152fbb5eb1e8135f1f5.png)\n\n- **Step 4: Set Environment Variables**\n\nHere we are presented with the Environment Variables that Release was able to extract from the compose file. If we wanted to add additional variables here we could but for this Application we won’t need any more so we’ll click Next Step.\n\n![](/blog-images/6fc90f447bd025a35d9feff557a5fd2b.png)\n\n- **Step 5: Set Build Arguments**\n\nAt this stage we are presented with the ability to add build arguments if we need to explicitly pass anything into our Docker build. For this Application, the [Dockerfile](https://github.com/awesome-release/rails_postgres_redis/blob/main/Dockerfile) accepts a build argument for `RUBY_VERSION` if we want to use a newer version than the `3.0.0` default. We’ll add an argument for the `3.2.0` version. We’ll be able to see this version used when we look at the build in part two of this series. After clicking the check mark to add our build argument, we’ll click Next Step to move on.\n\n![](/blog-images/f7f0a2dc9c90964f8e213dbd200096c1.png)\n\n- **Step 6: save and deploy**  \n  ** The final step in creating our Application is to create a deployment. By clicking the “Deploy your app!” button, Release will create an ephemeral environment and start a **setup\\*\\* workflow.\n\nThat’s it! 🎉 In short six steps we took our Docker Compose file and created an Application Template that we can now use to spin up ephemeral environments on demand.\n\nIn part two of this series, we’ll cover what happens during the deployment, as well as what Kubernetes objects were created. We will also use the ephemeral environment that was created to ensure that all of our Services are up and running. Stay tuned for part two and in the meantime [take Release for a spin](https://web.release.com/register) and let us know if you have any questions.\n",
          "code": "var Component=(()=>{var h=Object.create;var r=Object.defineProperty;var p=Object.getOwnPropertyDescriptor;var u=Object.getOwnPropertyNames;var m=Object.getPrototypeOf,g=Object.prototype.hasOwnProperty;var f=(t,e)=>()=>(e||t((e={exports:{}}).exports,e),e.exports),w=(t,e)=>{for(var o in e)r(t,o,{get:e[o],enumerable:!0})},s=(t,e,o,a)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let i of u(e))!g.call(t,i)&&i!==o&&r(t,i,{get:()=>e[i],enumerable:!(a=p(e,i))||a.enumerable});return t};var b=(t,e,o)=>(o=t!=null?h(m(t)):{},s(e||!t||!t.__esModule?r(o,\"default\",{value:t,enumerable:!0}):o,t)),y=t=>s(r({},\"__esModule\",{value:!0}),t);var d=f((_,l)=>{l.exports=_jsx_runtime});var R={};w(R,{default:()=>S,frontmatter:()=>v});var n=b(d()),v={title:\"Creating your first Application in Release with Docker Compose\",summary:\"Take your Docker Compose file and Crete an Application to run on Kubernetes in six easy steps.\",publishDate:\"Wed Jun 28 2023 14:46:59 GMT+0000 (Coordinated Universal Time)\",author:\"jeremy-kreutzbender\",readingTime:10,categories:[\"platform-engineering\",\"kubernetes\"],mainImage:\"/blog-images/24d9e9521de18493481de2d2321d4516.jpg\",imageAlt:\"Creating your first Application in Release with Docker Compose\",showCTA:!0,ctaCopy:\"Explore how Release simplifies deploying Kubernetes applications with ephemeral environments for faster testing and streamlined collaboration.\",ctaLink:\"https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=creating-your-first-application-in-release-with-docker-compose\",relatedPosts:[\"\"],ogImage:\"/blog-images/24d9e9521de18493481de2d2321d4516.jpg\",excerpt:\"Take your Docker Compose file and Crete an Application to run on Kubernetes in six easy steps.\",tags:[\"platform-engineering\",\"kubernetes\"],ctaButton:\"Try Release for Free\"};function c(t){let e=Object.assign({p:\"p\",a:\"a\",pre:\"pre\",code:\"code\",h3:\"h3\",span:\"span\",ul:\"ul\",li:\"li\",strong:\"strong\",img:\"img\",em:\"em\",br:\"br\"},t.components);return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(e.p,{children:\"If you\\u2019re using Docker compose for local development but have been interested in running your application on Kubernetes or creating ephemeral environments for your application, keep on reading, this post is for you! \\xA0 \\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:\"At Release we know that Applications consist of more than just your repository and code. There are other services that are required, such as databases or key value stores. An application usually cannot run without environment variables, backing data, infrastructure, or storage component(s). That\\u2019s why we think that a Docker compose file is one of the best ways to describe your application for local development. It is also a perfect way to get started on Release and get your near-production environments spun up with each pull request. In this blog, we will walk through the steps to create an Application, highlight how Release helps transform your compose file into an Application Template and ultimately deploy it on Kubernetes (which we will cover in part two of this series).\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"We\\u2019ll be using \",(0,n.jsx)(e.a,{href:\"https://github.com/awesome-release/rails_postgres_redis\",children:\"https://github.com/awesome-release/rails_postgres_redis\"}),\" as the example in this post. It is a small application that runs a Ruby on Rails server, has requirements of a Postgresql database and a Redis server, as well as runs Sidekiq, which is a background job processor.\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Let\\u2019s take a look at the compose file and then jump into creating our Application.\"}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-yaml\",children:`\nversion: '3'\nservices:\n \\xA0api:\n \\xA0 \\xA0build: .\n \\xA0 \\xA0image: rails_postgres_redis:latest\n \\xA0 \\xA0command: bash -c \"(rake db:exists && rake db:migrate || rake db:setup) && bundle exec rails s -b 0.0.0.0\"\n \\xA0 \\xA0environment:\n \\xA0 \\xA0 \\xA0REDIS_URL: redis://redis:6379/0\n \\xA0 \\xA0 \\xA0POSTGRES_PASSWORD: postgres\n \\xA0 \\xA0 \\xA0POSTGRES_USER: postgres\n \\xA0 \\xA0 \\xA0POSTGRES_HOST: db\n \\xA0 \\xA0ports:\n \\xA0 \\xA0 \\xA0- \"3000:3000\"\n \\xA0 \\xA0depends_on:\n \\xA0 \\xA0 \\xA0- db\n \\xA0 \\xA0 \\xA0- redis\n \\xA0\n \\xA0sidekiq:\n \\xA0 \\xA0image: rails_postgres_redis:latest\n \\xA0 \\xA0command: bundle exec sidekiq\n \\xA0 \\xA0environment:\n \\xA0 \\xA0 \\xA0REDIS_URL: redis://redis:6379/0\n \\xA0 \\xA0 \\xA0POSTGRES_PASSWORD: postgres\n \\xA0 \\xA0 \\xA0POSTGRES_USER: postgres\n \\xA0 \\xA0 \\xA0POSTGRES_HOST: db\n \\xA0 \\xA0depends_on:\n \\xA0 \\xA0 \\xA0- api\n \\xA0 \\xA0 \\xA0- redis\n \\xA0 \\xA0 \\xA0- db \\xA0 \\xA0\n\n \\xA0db:\n \\xA0 \\xA0image: postgres:12-alpine\n \\xA0 \\xA0ports:\n \\xA0 \\xA0 \\xA0- \"5432\"\n \\xA0 \\xA0volumes:\n \\xA0 \\xA0 \\xA0- postgres-data:/var/lib/postgresql/data\n \\xA0 \\xA0environment:\n \\xA0 \\xA0 \\xA0POSTGRES_PASSWORD: postgres\n \\xA0 \\xA0 \\xA0POSTGRES_USER: postgres\n\n \\xA0redis:\n \\xA0 \\xA0image: redis\n \\xA0 \\xA0ports:\n \\xA0 \\xA0 \\xA0- \"6379\"\n \\xA0 \\xA0volumes:\n \\xA0 \\xA0 \\xA0- redis:/data\n\nvolumes:\n \\xA0postgres-data: {}\n \\xA0redis: {}\n\n`})}),`\n`,(0,n.jsx)(e.p,{children:\"\\u200D\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"We won\\u2019t go through everything line by line but we wanted to make sure to note that in this compose file certain ports are exposed, as well as volumes are being defined to retain data between restarts. We also see that the containers have their \",(0,n.jsx)(e.code,{children:\"depends_on\"}),\" defined to tell us in which order the containers should be started in. We encourage you to take a look at our \",(0,n.jsx)(e.a,{href:\"https://docs.release.com/reference-documentation/docker-compose-conversion-support\",children:\"Docker Compose conversion support documentation\"}),\" to see the full list of supported attributes.\"]}),`\n`,(0,n.jsxs)(e.h3,{id:\"translating-a-compose-file-into-an-application\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#translating-a-compose-file-into-an-application\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Translating a Compose file into an Application\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Now that we have a Docker Compose file ready, let\\u2019s see how to translate it into an Application in Release.\"}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsx)(e.li,{children:(0,n.jsx)(e.strong,{children:\"Step 1: Create your application\"})}),`\n`]}),`\n`,(0,n.jsx)(e.p,{children:\"The first step in creating your Application is to give it a name and select which repository we\\u2019ll be using. I\\u2019ve named mine \\u201Crelease-docker-compose-demo\\u201D and selected the awesome-release/rails_postgres_redis repository (linked above).\"}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/ff3c06a3bb3d132f792d59947fecbbcc.png\",alt:\"\"})}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsx)(e.li,{children:(0,n.jsx)(e.strong,{children:\"Step 2: Analyze your repository\"})}),`\n`]}),`\n`,(0,n.jsx)(e.p,{children:\"The next step is to analyze the repository. We\\u2019ve selected the main branch and are shown some options of files we can select in a dropdown. The files we select will be analyzed and converted into what Release calls Services. These Services can have many types, such as Containers, Static Javascript Builds, Helm, and Terraform; we\\u2019ll primarily focus on Containers in this post but look for future posts where we\\u2019ll cover the other options.\"}),`\n`,(0,n.jsx)(e.p,{children:\"We\\u2019ve selected our docker-compose.yml file and we will click the Start Analysis button.\"}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/b96ad97ba2f3abdcdf27b467438e0e79.png\",alt:\"\"})}),`\n`,(0,n.jsx)(e.p,{children:\"We see that Release created four Services for us with the same names as what is in the compose file. The dependencies are also listed. In this case we know that we want all these Services, but if something had been displayed that we didn\\u2019t want to deploy on Release, we could uncheck the Service to remove it. Now we\\u2019ll click Next Step to move on.\"}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/e0c91cdc3c2141465fd17ada6c65ce82.png\",alt:\"\"})}),`\n`,(0,n.jsxs)(e.p,{children:[\"\\u200D* \",(0,n.jsx)(e.strong,{children:\"Step 3: Generate a template\"})]}),`\n`,(0,n.jsxs)(e.p,{children:[\"In this stage we get our first view of an Application Template. We won\\u2019t go through everything in the template here, however we encourage you to read through the documentation on \",(0,n.jsx)(e.a,{href:\"https://docs.release.com/reference-documentation/application-settings/application-template\",children:\"Application Templates\"}),\" to understand all the possibilities. Instead we\\u2019ll highlight how Release has translated the Services from the last page into this yaml format. We see the familiar names of \\xA0\",(0,n.jsx)(e.code,{children:\"api\"}),\", \",(0,n.jsx)(e.code,{children:\"db\"}),\", and \",(0,n.jsx)(e.code,{children:\"redis\"}),\" as well as the ports and volumes that were defined in the compose file.\"]}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/2061a6acb4335f3eebcdc64912fd9f5d.png\",alt:\"\"})}),`\n`,(0,n.jsxs)(e.p,{children:[\"The definitions of Services help to describe \",(0,n.jsx)(e.em,{children:\"what\"}),\" Release will deploy, but we also want to know \",(0,n.jsx)(e.em,{children:\"how\"}),\" Release will deploy these Services. That information is contained in the workflows stanza. There are three types of workflows defined: \",(0,n.jsx)(e.strong,{children:\"setup\"}),\", \",(0,n.jsx)(e.strong,{children:\"patch\"}),\", and \",(0,n.jsx)(e.strong,{children:\"teardown\"}),\".\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"A \",(0,n.jsx)(e.strong,{children:\"setup\"}),\" workflow defines a deployment where infrastructure can be deployed for the first time or if there are subsequent changes to the infrastructure; think, changing your Postgresql version, the number of replicas of the \\xA0\",(0,n.jsx)(e.code,{children:\"api\"}),\" Service, or changing environment variables for your Application. We can also see that the order of the Services from the compose file \\xA0\",(0,n.jsx)(e.code,{children:\"depends_on\"}),\" is translated into the setup workflow. \",(0,n.jsx)(e.code,{children:\"db\"}),\" and \",(0,n.jsx)(e.code,{children:\"redis\"}),\" will be deployed in parallel first. Once both of those Services are up and running, Release will move to the next step and deploy \\xA0\",(0,n.jsx)(e.code,{children:\"api\"}),\". Finally \\xA0\",(0,n.jsx)(e.code,{children:\"sidekiq\"}),\" will be deployed.\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"The \",(0,n.jsx)(e.strong,{children:\"patch\"}),\" workflow is used when only code changes need to be deployed. In our case, both the \",(0,n.jsx)(e.code,{children:\"api\"}),\" and \",(0,n.jsx)(e.code,{children:\"sidekiq\"}),\" Services contain the code from the repository and would need to be deployed when we push new changes. The \\xA0\",(0,n.jsx)(e.code,{children:\"db\"}),\" and \",(0,n.jsx)(e.code,{children:\"redis\"}),\" Services don\\u2019t require any changes so they don\\u2019t need to be referenced in a patch.\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"The final workflow is the \",(0,n.jsx)(e.strong,{children:\"teardown\"}),\" which uses a Release defined task called \",(0,n.jsx)(e.code,{children:\"remove_environment\"}),\". This task will tear down all the infrastructure in Kubernetes and free up the resources that were being used. Additional steps can be added to a teardown workflow but the \",(0,n.jsx)(e.code,{children:\"remove_environment\"}),\" is a requirement.\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Now that we\\u2019ve had a quick runthrough of parts of our Application Template, we\\u2019ll click Next Step to move on.\"}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/eb1e9b99de4ff152fbb5eb1e8135f1f5.png\",alt:\"\"})}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsx)(e.li,{children:(0,n.jsx)(e.strong,{children:\"Step 4: Set Environment Variables\"})}),`\n`]}),`\n`,(0,n.jsx)(e.p,{children:\"Here we are presented with the Environment Variables that Release was able to extract from the compose file. If we wanted to add additional variables here we could but for this Application we won\\u2019t need any more so we\\u2019ll click Next Step.\"}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/6fc90f447bd025a35d9feff557a5fd2b.png\",alt:\"\"})}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsx)(e.li,{children:(0,n.jsx)(e.strong,{children:\"Step 5: Set Build Arguments\"})}),`\n`]}),`\n`,(0,n.jsxs)(e.p,{children:[\"At this stage we are presented with the ability to add build arguments if we need to explicitly pass anything into our Docker build. For this Application, the \",(0,n.jsx)(e.a,{href:\"https://github.com/awesome-release/rails_postgres_redis/blob/main/Dockerfile\",children:\"Dockerfile\"}),\" accepts a build argument for \",(0,n.jsx)(e.code,{children:\"RUBY_VERSION\"}),\" if we want to use a newer version than the \",(0,n.jsx)(e.code,{children:\"3.0.0\"}),\" default. We\\u2019ll add an argument for the \",(0,n.jsx)(e.code,{children:\"3.2.0\"}),\" version. We\\u2019ll be able to see this version used when we look at the build in part two of this series. After clicking the check mark to add our build argument, we\\u2019ll click Next Step to move on.\"]}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/f7f0a2dc9c90964f8e213dbd200096c1.png\",alt:\"\"})}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"Step 6: save and deploy\"}),(0,n.jsx)(e.br,{}),`\n`,\"** The final step in creating our Application is to create a deployment. By clicking the \\u201CDeploy your app!\\u201D button, Release will create an ephemeral environment and start a **setup** workflow.\"]}),`\n`]}),`\n`,(0,n.jsx)(e.p,{children:\"That\\u2019s it! \\u{1F389} In short six steps we took our Docker Compose file and created an Application Template that we can now use to spin up ephemeral environments on demand.\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"In part two of this series, we\\u2019ll cover what happens during the deployment, as well as what Kubernetes objects were created. We will also use the ephemeral environment that was created to ensure that all of our Services are up and running. Stay tuned for part two and in the meantime \",(0,n.jsx)(e.a,{href:\"https://web.release.com/register\",children:\"take Release for a spin\"}),\" and let us know if you have any questions.\"]})]})}function k(t={}){let{wrapper:e}=t.components||{};return e?(0,n.jsx)(e,Object.assign({},t,{children:(0,n.jsx)(c,t)})):c(t)}var S=k;return y(R);})();\n;return Component;"
        },
        "_id": "blog/posts/creating-your-first-application-in-release-with-docker-compose.mdx",
        "_raw": {
          "sourceFilePath": "blog/posts/creating-your-first-application-in-release-with-docker-compose.mdx",
          "sourceFileName": "creating-your-first-application-in-release-with-docker-compose.mdx",
          "sourceFileDir": "blog/posts",
          "contentType": "mdx",
          "flattenedPath": "blog/posts/creating-your-first-application-in-release-with-docker-compose"
        },
        "type": "BlogPost",
        "computedSlug": "creating-your-first-application-in-release-with-docker-compose"
      },
      "documentHash": "1739393595017",
      "hasWarnings": false,
      "documentTypeName": "BlogPost"
    },
    "blog/posts/crv-release-aligned-with-20mil-new-fuel.mdx": {
      "document": {
        "title": "CRV + Release - aligned from the beginning - now with $20M in new fuel",
        "summary": "Message from Co-Founder, Tommy McClung, regarding Release's $20M Series A funding round led by CRV.",
        "publishDate": "Tue Oct 05 2021 01:38:20 GMT+0000 (Coordinated Universal Time)",
        "author": "tommy-mcclung",
        "readingTime": 3,
        "categories": [
          "platform-engineering",
          "product"
        ],
        "mainImage": "/blog-images/4d56af93689fb5d3011dd9def65915e6.jpg",
        "imageAlt": "Founders pictures with Release Series A funding announcement",
        "showCTA": true,
        "ctaCopy": "Simplify dev workflows with on-demand environments like Release's Environments as a Service. Eliminate complexity, speed up deployments.",
        "ctaLink": "https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=crv-release-aligned-with-20mil-new-fuel",
        "relatedPosts": [
          ""
        ],
        "ogImage": "/blog-images/4d56af93689fb5d3011dd9def65915e6.jpg",
        "excerpt": "Message from Co-Founder, Tommy McClung, regarding Release's $20M Series A funding round led by CRV.",
        "tags": [
          "platform-engineering",
          "product"
        ],
        "ctaButton": "Try Release for Free",
        "body": {
          "raw": "\nToday [we’re announcing a $20M Series A funding](https://releasehub.webflow.io/blog/releasehub-20-million-series-a-led-by-crv) round led by CRV with participation from Sequoia, Y Combinator, Bow Capital, Artisanal Ventures, SOMA, Amit Agarwal (CPO of DataDog), Chase Gilbert (Built) and Bill Clerico (WePay). This is on the heels of the Seed Round we announced back in April of 2021 that closed in April of 2020.\n\nWhile a funding round is just a step along the way, I wanted to take a quick minute and talk about why we, along with our investors, are so excited about what is being built at Release. I’ll do that by talking through the engagement we had with CRV and why we chose them as our Series A partners.\n\nBack in October of last year, like many entrepreneurs, I received a cold email from CRV saying they were interested in what we’re building and wanted to chat. I decided to ignore the email initially (like any good focused founder would do) but a line in the email stuck out in my head:\n\n> “I've recently been exploring the developer tools space, and Release stood out to me. I think your approach to helping developers create on-demand staging environments is compelling, and would eliminate a lot of unnecessary complexity for developers.”\n\nIt really was the last part of that sentence that kept ringing in my head: “eliminate a lot of unnecessary complexity for developers.” This is precisely the reason Erik, David and I started Release after our time at TrueCar. Building software in 2021 is still incredibly complex for developers. Devs today spend far too much time working on non-value added work. If environments worked for developers instead of against them, more ideas would make their way to the world. From the first interaction with CRV, we were aligned on the problem we’ve set out to solve with Environments as a Service here at Release.\n\nI decided to reply a couple weeks later and let CRV know we had just raised a Seed round earlier in the year, but we should chat in early 2021. They reached back out and the rest is history.\n\nFrom the very first conversations with Brittany Walker, James Green and Murat Bicer, it was obvious CRV had a thesis on our space and had really been doing their research. The questions were insightful and as we got to know each other, it was clear we had an aligned vision for what the future of developing on the cloud will look like. Environments are a manifestation of an application running in a complex cloud ecosystem, and if those environments are easy to reproduce then developers can move faster with higher quality output. Back to the original statement in the email they sent to me, “I think your approach to helping developers create on-demand staging environments is compelling, and would eliminate a lot of unnecessary complexity for developers.” Full alignment from the beginning.\n\nWe’ve been growing like mad here at Release and this round of funding led by CRV will enable us to continue delivering Environments as a Service to companies who value velocity, quality and ensuring developers spend their time on value-added work. Too much time is wasted by organizations messing around with unnecessary complexity.\n\nIf you’re an organization that’s tired of wasting time and you want to deliver quickly for your customers, hit us up at [hello@release.com](mailto:hello@release.com) and we can show you how Environments as a Service can enable your organization to deliver like never before.\n\nLast, I wanted to say how proud I am to work with this team and what we’ve accomplished together so far. Thank you all for your hard work and dedication. More to come from this amazing team (and maybe you?).\n\nWe’re building some amazing technology at Release and helping our customers achieve their missions. We’re heads down making AWS, GCP, Kubernetes, Terraform, Pulumi, RDS, and tons of other great technologies work seamlessly and effortlessly into the developer experience. We build Release with Release, so day to day we see how our solutions can make the lives of developers better. If you’re interested in being a part of the journey let us know! [We have a ton of open roles.](https://releasehub.com/company)\n",
          "code": "var Component=(()=>{var u=Object.create;var i=Object.defineProperty;var c=Object.getOwnPropertyDescriptor;var m=Object.getOwnPropertyNames;var g=Object.getPrototypeOf,p=Object.prototype.hasOwnProperty;var w=(n,e)=>()=>(e||n((e={exports:{}}).exports,e),e.exports),f=(n,e)=>{for(var t in e)i(n,t,{get:e[t],enumerable:!0})},s=(n,e,t,r)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let a of m(e))!p.call(n,a)&&a!==t&&i(n,a,{get:()=>e[a],enumerable:!(r=c(e,a))||r.enumerable});return n};var y=(n,e,t)=>(t=n!=null?u(g(n)):{},s(e||!n||!n.__esModule?i(t,\"default\",{value:n,enumerable:!0}):t,n)),v=n=>s(i({},\"__esModule\",{value:!0}),n);var d=w((I,l)=>{l.exports=_jsx_runtime});var R={};f(R,{default:()=>C,frontmatter:()=>b});var o=y(d()),b={title:\"CRV + Release - aligned from the beginning - now with $20M in new fuel\",summary:\"Message from Co-Founder, Tommy McClung, regarding Release's $20M Series A funding round led by CRV.\",publishDate:\"Tue Oct 05 2021 01:38:20 GMT+0000 (Coordinated Universal Time)\",author:\"tommy-mcclung\",readingTime:3,categories:[\"platform-engineering\",\"product\"],mainImage:\"/blog-images/4d56af93689fb5d3011dd9def65915e6.jpg\",imageAlt:\"Founders pictures with Release Series A funding announcement\",showCTA:!0,ctaCopy:\"Simplify dev workflows with on-demand environments like Release's Environments as a Service. Eliminate complexity, speed up deployments.\",ctaLink:\"https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=crv-release-aligned-with-20mil-new-fuel\",relatedPosts:[\"\"],ogImage:\"/blog-images/4d56af93689fb5d3011dd9def65915e6.jpg\",excerpt:\"Message from Co-Founder, Tommy McClung, regarding Release's $20M Series A funding round led by CRV.\",tags:[\"platform-engineering\",\"product\"],ctaButton:\"Try Release for Free\"};function h(n){let e=Object.assign({p:\"p\",a:\"a\",blockquote:\"blockquote\"},n.components);return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsxs)(e.p,{children:[\"Today \",(0,o.jsx)(e.a,{href:\"https://releasehub.webflow.io/blog/releasehub-20-million-series-a-led-by-crv\",children:\"we\\u2019re announcing a $20M Series A funding\"}),\" round led by CRV with participation from Sequoia, Y Combinator, Bow Capital, Artisanal Ventures, SOMA, Amit Agarwal (CPO of DataDog), Chase Gilbert (Built) and Bill Clerico (WePay). This is on the heels of the Seed Round we announced back in April of 2021 that closed in April of 2020.\"]}),`\n`,(0,o.jsx)(e.p,{children:\"While a funding round is just a step along the way, I wanted to take a quick minute and talk about why we, along with our investors, are so excited about what is being built at Release. I\\u2019ll do that by talking through the engagement we had with CRV and why we chose them as our Series A partners.\"}),`\n`,(0,o.jsx)(e.p,{children:\"Back in October of last year, like many entrepreneurs, I received a cold email from CRV saying they were interested in what we\\u2019re building and wanted to chat. I decided to ignore the email initially (like any good focused founder would do) but a line in the email stuck out in my head:\"}),`\n`,(0,o.jsxs)(e.blockquote,{children:[`\n`,(0,o.jsx)(e.p,{children:\"\\u201CI've recently been exploring the developer tools space, and Release stood out to me. I think your approach to helping developers create on-demand staging environments is compelling, and would eliminate a lot of unnecessary complexity for developers.\\u201D\"}),`\n`]}),`\n`,(0,o.jsx)(e.p,{children:\"It really was the last part of that sentence that kept ringing in my head: \\u201Celiminate a lot of unnecessary complexity for developers.\\u201D This is precisely the reason Erik, David and I started Release after our time at TrueCar. Building software in 2021 is still incredibly complex for developers. Devs today spend far too much time working on non-value added work. If environments worked for developers instead of against them, more ideas would make their way to the world. From the first interaction with CRV, we were aligned on the problem we\\u2019ve set out to solve with Environments as a Service here at Release.\"}),`\n`,(0,o.jsx)(e.p,{children:\"I decided to reply a couple weeks later and let CRV know we had just raised a Seed round earlier in the year, but we should chat in early 2021. They reached back out and the rest is history.\"}),`\n`,(0,o.jsx)(e.p,{children:\"From the very first conversations with Brittany Walker, James Green and Murat Bicer, it was obvious CRV had a thesis on our space and had really been doing their research. The questions were insightful and as we got to know each other, it was clear we had an aligned vision for what the future of developing on the cloud will look like. Environments are a manifestation of an application running in a complex cloud ecosystem, and if those environments are easy to reproduce then developers can move faster with higher quality output. Back to the original statement in the email they sent to me, \\u201CI think your approach to helping developers create on-demand staging environments is compelling, and would eliminate a lot of unnecessary complexity for developers.\\u201D Full alignment from the beginning.\"}),`\n`,(0,o.jsx)(e.p,{children:\"We\\u2019ve been growing like mad here at Release and this round of funding led by CRV will enable us to continue delivering Environments as a Service to companies who value velocity, quality and ensuring developers spend their time on value-added work. Too much time is wasted by organizations messing around with unnecessary complexity.\"}),`\n`,(0,o.jsxs)(e.p,{children:[\"If you\\u2019re an organization that\\u2019s tired of wasting time and you want to deliver quickly for your customers, hit us up at \",(0,o.jsx)(e.a,{href:\"mailto:hello@release.com\",children:\"hello@release.com\"}),\" and we can show you how Environments as a Service can enable your organization to deliver like never before.\"]}),`\n`,(0,o.jsx)(e.p,{children:\"Last, I wanted to say how proud I am to work with this team and what we\\u2019ve accomplished together so far. Thank you all for your hard work and dedication. More to come from this amazing team (and maybe you?).\"}),`\n`,(0,o.jsxs)(e.p,{children:[\"We\\u2019re building some amazing technology at Release and helping our customers achieve their missions. We\\u2019re heads down making AWS, GCP, Kubernetes, Terraform, Pulumi, RDS, and tons of other great technologies work seamlessly and effortlessly into the developer experience. We build Release with Release, so day to day we see how our solutions can make the lives of developers better. If you\\u2019re interested in being a part of the journey let us know! \",(0,o.jsx)(e.a,{href:\"https://releasehub.com/company\",children:\"We have a ton of open roles.\"})]})]})}function k(n={}){let{wrapper:e}=n.components||{};return e?(0,o.jsx)(e,Object.assign({},n,{children:(0,o.jsx)(h,n)})):h(n)}var C=k;return v(R);})();\n;return Component;"
        },
        "_id": "blog/posts/crv-release-aligned-with-20mil-new-fuel.mdx",
        "_raw": {
          "sourceFilePath": "blog/posts/crv-release-aligned-with-20mil-new-fuel.mdx",
          "sourceFileName": "crv-release-aligned-with-20mil-new-fuel.mdx",
          "sourceFileDir": "blog/posts",
          "contentType": "mdx",
          "flattenedPath": "blog/posts/crv-release-aligned-with-20mil-new-fuel"
        },
        "type": "BlogPost",
        "computedSlug": "crv-release-aligned-with-20mil-new-fuel"
      },
      "documentHash": "1739393595017",
      "hasWarnings": false,
      "documentTypeName": "BlogPost"
    },
    "blog/posts/cutting-build-time-in-half-docker-buildx-kubernetes.mdx": {
      "document": {
        "title": "Cutting Build Time In Half with Docker’s Buildx Kubernetes Driver",
        "summary": "At Release, environments are our main focus, but we can’t create environments without builds. Recently we undertook a pr",
        "publishDate": "Thu Feb 25 2021 01:40:58 GMT+0000 (Coordinated Universal Time)",
        "author": "jeremy-kreutzbender",
        "readingTime": 5,
        "categories": [
          "kubernetes",
          "platform-engineering"
        ],
        "mainImage": "/blog-images/0e25329b235434c91db7ac0845e8f6a1.jpg",
        "imageAlt": "A red and an orange container",
        "showCTA": true,
        "ctaCopy": "Discover how Release's environment management platform optimizes Docker builds for faster delivery cycles.",
        "ctaLink": "https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=cutting-build-time-in-half-docker-buildx-kubernetes",
        "relatedPosts": [
          ""
        ],
        "ogImage": "/blog-images/0e25329b235434c91db7ac0845e8f6a1.jpg",
        "excerpt": "At Release, environments are our main focus, but we can’t create environments without builds. Recently we undertook a pr",
        "tags": [
          "kubernetes",
          "platform-engineering"
        ],
        "ctaButton": "Try Release for Free",
        "body": {
          "raw": "\nAt Release, environments are our main focus, but we can’t create environments without builds. Recently we undertook a project to revisit our build infrastructure and determine if it needed to be upgraded. Build times had become a big factor in our service delivery and we needed a way to improve our customers’ experiences. One of the main areas that we wanted to improve upon was the parallelism of building multiple docker images for a single application.\n\nThe title of the article already spoiled the solution, and the alternative ‘Release Did This One Thing To Cut Their Build Time In Half!’ didn’t quite fly with the rest of the company, but Docker’s new [buildx](https://github.com/docker/buildx) project fit the bill. First, we’ll cover what our original infrastructure looked like and how long builds on an example project were taking. Then, we’ll describe the changes we made to use buildx and the speed increases we observed.\n\nLet’s start off with a diagram of what our original infrastructure looked like.\n\n![release-builder-architecture](/blog-images/269489a2760994029b69e9de205f1093.png)\n\nAs you can see, the requests for builds would flow into our main Rails application and then divvied out to the different builder instances through Sidekiq. The builder container is Ruby code that would authenticate to Github, clone the repository, check out the correct SHA, and then execute the docker build. Due to the way we built the authentication to pull the code from Github, a single builder container could only clone one repository at a time. Which meant that the container could only do a single build request at a time. We added threading in the Ruby code to be able to execute multiple docker build commands at a time, but the number of builder containers we had spun up limited our concurrent builds. While it’s not hard to horizontally scale with Kubernetes, we saw this authentication setup as a major bottleneck.\n\nAnother issue we encountered was that we had no mechanism for attempting to place builds on servers where they had been previously built, instead opting for grabbing the first free server. This meant there was very little chance to land on the same server and get the full benefit of Docker caching. While this isn’t a deal breaker for us, we still believed we could do better when creating the version of our build infrastructure. Enough of the theoretical, let’s actually build something!\n\nRelease Applications can contain many docker images and one of our favorite example repositories to showcase this is our fork of [example-voting-app](https://github.com/awesome-release/release-example-voting-app). Looking at the [docker-compose](https://github.com/awesome-release/release-example-voting-app/blob/master/docker-compose.yml) we see that there are 3 different Docker images that we have to build, result, vote, and worker. Now that we have an understanding of Release’s original infrastructure and the application we want to build, let’s start up a fresh build and see the results.\n\n_NOTE_ I forked the awesome-release repo to my own Github, jer-k for the following results.\n\n![uncached-release](/blog-images/024b50f0b01574bdd659c22bdfc36edf.png)\n\nWe can see that this brand new build with no cache hits took two minutes and 15 seconds to complete. Next, we want to make a few changes to ensure that each Docker image needs to be rebuilt. The changes are listed below.\n\n```ruby\n\ngit status\nOn branch release_builders\nChanges to be committed:\n  (use \"git restore --staged ...\" to unstage)\n    modified:   result/views/index.html\n    modified:   vote/app.py\n    modified:   worker/src/main/java/worker/Worker.java\n\n```\n\nFor the purpose of this blog post, I ensured the following build ran on the same builder as the first and that we will have cache hits. As noted before, this wasn’t always the case in our production environment.\n\n![cached-release](/blog-images/c3660987194315b5d66f3fd5a371546d.png)\n\nThe caching helps and cuts 45 seconds off the build! The uncached build took almost twice as long as the second build with caching, but our assumption was that we could do a lot better (cached and uncached) with some new technology.\n\n### Enter Docker’s Buildx Kubernetes Driver\n\nOne of the first things we wanted to solve was the concurrency issue and we set out to ensure that Docker itself was able to handle a larger workload. We came across the issue [Concurrent “docker build” takes longer than sequential builds](https://github.com/moby/moby/issues/9656) where people were describing what we feared; Docker slowed down when many builds were being run at the same time. Lucky for us, that issue was opened in 2014 and plenty of work had been done to resolve this issue. The final comment, by a member of the Docker team, was [“Closing this. BuildKit is highly optimized for parallel workloads. If you see anything like this in buildkit or buildkit compared to legacy builder please report a new issue with a repro case.”](https://github.com/moby/moby/issues/9656#issuecomment-610476810) Thus we set out to learn more about [BuildKit](https://docs.docker.com/develop/develop-images/build_enhancements/) (the Github repository is located [here](https://github.com/moby/buildkit)). While researching, we came across [buildx](https://github.com/docker/buildx), which ended up having three key features we believed would resolve many of our issues. These three features were the [bake](https://github.com/docker/buildx#buildx-bake-options-target) command, the [buildx kubernetes driver](https://github.com/docker/buildx#--driver-driver), and the ability for the Kubernetes driver to consistently send builds to the same server. Let’s cover each of these, first up the bake command.\n\n```none\n\nbuildx bake [OPTIONS] [TARGET...]\nBake is a high-level build command.\n\nEach specified target will run in parallel as part of the build.\n\n```\n\nbake intrigued us because it seemed to be a built-in command for us to avoid using Ruby threading for our parallelism. bake takes an input of a file, which can either be in the form of a docker-compose, .json, or .hcl. We initially tested bake with the docker-compose from example-voting-app and we were blown away at how smoothly it built directly out of the box and how quickly it was able to build the three images! However, we opted to create our own .json file generator in Ruby, parsing our [Application Template](https://releasehub.com/blog/cutting-build-time-in-half-docker-buildx-kubernetes) into an output. Here is our generated file for example-voting-app.\n\n```json\n{\n  \"group\": {\n    \"default\": {\n      \"targets\": [\"vote\", \"result\", \"worker\"]\n    }\n  },\n  \"target\": {\n    \"vote\": {\n      \"context\": \"./vote\",\n      \"tags\": [\n        \".dkr.ecr.us-west-2.amazonaws.com/jer-k/release-example-voting-app/vote:latest\",\n        \".dkr.ecr.us-west-2.amazonaws.com/jer-k/release-example-voting-app/vote:buildx-builders\"\n      ]\n    },\n    \"result\": {\n      \"context\": \"./result\",\n      \"tags\": [\n        \".dkr.ecr.us-west-2.amazonaws.com/jer-k/release-example-voting-app/result:latest\",\n        \".dkr.ecr.us-west-2.amazonaws.com/jer-k/release-example-voting-app/result:buildx-builders\"\n      ]\n    },\n    \"worker\": {\n      \"context\": \"./worker\",\n      \"tags\": [\n        \".dkr.ecr.us-west-2.amazonaws.com/jer-k/release-example-voting-app/worker:latest\",\n        \".dkr.ecr.us-west-2.amazonaws.com/jer-k/release-example-voting-app/worker:buildx-builders\"\n      ]\n    }\n  }\n}\n```\n\nThere are other inputs which can make their way into this file, such as build args, but since example-voting-app does not have any, they are omitted.\n\nNext, we wanted to find more information on the Kubernetes driver and we found this blog post [Kubernetes driver for Docker BuildX](https://medium.com/nttlabs/buildx-kubernetes-ad0fe59b0c64) from the author of the [Pull Request](https://github.com/docker/buildx/pull/167). We encourage you to read the latter as it covers getting up and running with the Kubernetes driver as well how the caching works, which is exactly what we needed. With that information in hand, we were able to start work on adding the buildx servers to our cluster. We created a generic way to deploy the servers into different clusters and adjust the number of replicas with the final command being\n\n```none\n\ndocker buildx create --name #{name} --driver kubernetes --driver-opt replicas=#{num_replicas},namespace=#{builder_namespace} --use\n\n```\n\nFor us, we created a release-builder namespace with five replicas, in our development cluster. We can see the output by querying for the pods\n\n```none\n\nkubectl get pods --namespace=release-builder\nNAME                            READY   STATUS    RESTARTS   AGE\ndevelopment0-86d99fcf46-26j9f   1/1     Running   0          6d10h\ndevelopment0-86d99fcf46-5scpq   1/1     Running   0          6d13h\ndevelopment0-86d99fcf46-jkk2b   1/1     Running   0          15d\ndevelopment0-86d99fcf46-llkgq   1/1     Running   0          18d\ndevelopment0-86d99fcf46-mr9jt   1/1     Running   0          20d\n\n```\n\nSince we have five replicas, we wanted to ensure that when we build applications, they end up on the same server so that we get the greatest amount of caching possible (distributed caching is a topic for another day). Luckily for us, buildx, with the Kubernetes driver, has an option for where to send the builds called loadbalance.  \nThe default sticky means that the builds should always end up on the same server due to the hashing (more detailed information on this is described in the aforementioned blog post). With all of that in place, we are ready to test out our new setup!\n\n```none\n\nloadbalance=(sticky|random) - Load-balancing strategy.\nIf set to \"sticky\", the pod is chosen using the hash of the context path. Defaults to \"sticky\"\n\n```\n\nUsing the same example-voting-app repository as before, I created a new branch buildx_builders and pointed the code to the buildx servers.\n\n![uncached-buildx](/blog-images/6301520361c0659740b319b30b8d8bee.png)\n\nWhat we see is that this uncached build was more than twice as fast as the other uncached build and even faster than the cached build on the old infrastructure! But uncached builds should be a thing of the past with the sticky load balancing, so let’s make the same changes as the previous branch and see the results.\n\n![cached-buildx](/blog-images/d22dd82384c12f3bab0bbe23e33457e7.png)\n\nThis build finished three times faster than the previous cached build! These types of speed increases are the reason we set out to redo our build infrastructure. The faster the builds complete, the faster we can create environments and help our customers deliver their products.\n\nWe’re still experimenting with buildx and learning as we go, but the initial results were more than enough for us to migrate our own production builds to the new infrastructure. We’re going to continue to blog about this topic as we learn more and scale so check back in with the Release blog in the future!\n",
          "code": "var Component=(()=>{var c=Object.create;var i=Object.defineProperty;var u=Object.getOwnPropertyDescriptor;var b=Object.getOwnPropertyNames;var m=Object.getPrototypeOf,p=Object.prototype.hasOwnProperty;var g=(n,e)=>()=>(e||n((e={exports:{}}).exports,e),e.exports),w=(n,e)=>{for(var a in e)i(n,a,{get:e[a],enumerable:!0})},s=(n,e,a,o)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let r of b(e))!p.call(n,r)&&r!==a&&i(n,r,{get:()=>e[r],enumerable:!(o=u(e,r))||o.enumerable});return n};var f=(n,e,a)=>(a=n!=null?c(m(n)):{},s(e||!n||!n.__esModule?i(a,\"default\",{value:n,enumerable:!0}):a,n)),k=n=>s(i({},\"__esModule\",{value:!0}),n);var d=g((R,l)=>{l.exports=_jsx_runtime});var T={};w(T,{default:()=>x,frontmatter:()=>v});var t=f(d()),v={title:\"Cutting Build Time In Half with Docker\\u2019s Buildx Kubernetes Driver\",summary:\"At Release, environments are our main focus, but we can\\u2019t create environments without builds. Recently we undertook a pr\",publishDate:\"Thu Feb 25 2021 01:40:58 GMT+0000 (Coordinated Universal Time)\",author:\"jeremy-kreutzbender\",readingTime:5,categories:[\"kubernetes\",\"platform-engineering\"],mainImage:\"/blog-images/0e25329b235434c91db7ac0845e8f6a1.jpg\",imageAlt:\"A red and an orange container\",showCTA:!0,ctaCopy:\"Discover how Release's environment management platform optimizes Docker builds for faster delivery cycles.\",ctaLink:\"https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=cutting-build-time-in-half-docker-buildx-kubernetes\",relatedPosts:[\"\"],ogImage:\"/blog-images/0e25329b235434c91db7ac0845e8f6a1.jpg\",excerpt:\"At Release, environments are our main focus, but we can\\u2019t create environments without builds. Recently we undertook a pr\",tags:[\"kubernetes\",\"platform-engineering\"],ctaButton:\"Try Release for Free\"};function h(n){let e=Object.assign({p:\"p\",a:\"a\",img:\"img\",em:\"em\",pre:\"pre\",code:\"code\",h3:\"h3\",span:\"span\",br:\"br\"},n.components);return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(e.p,{children:\"At Release, environments are our main focus, but we can\\u2019t create environments without builds. Recently we undertook a project to revisit our build infrastructure and determine if it needed to be upgraded. Build times had become a big factor in our service delivery and we needed a way to improve our customers\\u2019 experiences. One of the main areas that we wanted to improve upon was the parallelism of building multiple docker images for a single application.\"}),`\n`,(0,t.jsxs)(e.p,{children:[\"The title of the article already spoiled the solution, and the alternative \\u2018Release Did This One Thing To Cut Their Build Time In Half!\\u2019 didn\\u2019t quite fly with the rest of the company, but Docker\\u2019s new \",(0,t.jsx)(e.a,{href:\"https://github.com/docker/buildx\",children:\"buildx\"}),\" project fit the bill. First, we\\u2019ll cover what our original infrastructure looked like and how long builds on an example project were taking. Then, we\\u2019ll describe the changes we made to use buildx and the speed increases we observed.\"]}),`\n`,(0,t.jsx)(e.p,{children:\"Let\\u2019s start off with a diagram of what our original infrastructure looked like.\"}),`\n`,(0,t.jsx)(e.p,{children:(0,t.jsx)(e.img,{src:\"/blog-images/269489a2760994029b69e9de205f1093.png\",alt:\"release-builder-architecture\"})}),`\n`,(0,t.jsx)(e.p,{children:\"As you can see, the requests for builds would flow into our main Rails application and then divvied out to the different builder instances through Sidekiq. The builder container is Ruby code that would authenticate to Github, clone the repository, check out the correct SHA, and then execute the docker build. Due to the way we built the authentication to pull the code from Github, a single builder container could only clone one repository at a time. Which meant that the container could only do a single build request at a time. We added threading in the Ruby code to be able to execute multiple docker build commands at a time, but the number of builder containers we had spun up limited our concurrent builds. While it\\u2019s not hard to horizontally scale with Kubernetes, we saw this authentication setup as a major bottleneck.\"}),`\n`,(0,t.jsx)(e.p,{children:\"Another issue we encountered was that we had no mechanism for attempting to place builds on servers where they had been previously built, instead opting for grabbing the first free server. This meant there was very little chance to land on the same server and get the full benefit of Docker caching. While this isn\\u2019t a deal breaker for us, we still believed we could do better when creating the version of our build infrastructure. Enough of the theoretical, let\\u2019s actually build something!\"}),`\n`,(0,t.jsxs)(e.p,{children:[\"Release Applications can contain many docker images and one of our favorite example repositories to showcase this is our fork of \",(0,t.jsx)(e.a,{href:\"https://github.com/awesome-release/release-example-voting-app\",children:\"example-voting-app\"}),\". Looking at the \",(0,t.jsx)(e.a,{href:\"https://github.com/awesome-release/release-example-voting-app/blob/master/docker-compose.yml\",children:\"docker-compose\"}),\" we see that there are 3 different Docker images that we have to build, result, vote, and worker. Now that we have an understanding of Release\\u2019s original infrastructure and the application we want to build, let\\u2019s start up a fresh build and see the results.\"]}),`\n`,(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.em,{children:\"NOTE\"}),\" I forked the awesome-release repo to my own Github, jer-k for the following results.\"]}),`\n`,(0,t.jsx)(e.p,{children:(0,t.jsx)(e.img,{src:\"/blog-images/024b50f0b01574bdd659c22bdfc36edf.png\",alt:\"uncached-release\"})}),`\n`,(0,t.jsx)(e.p,{children:\"We can see that this brand new build with no cache hits took two minutes and 15 seconds to complete. Next, we want to make a few changes to ensure that each Docker image needs to be rebuilt. The changes are listed below.\"}),`\n`,(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:\"language-ruby\",children:`\ngit status\nOn branch release_builders\nChanges to be committed:\n \\xA0(use \"git restore --staged ...\" to unstage)\n \\xA0 \\xA0modified: \\xA0 result/views/index.html\n \\xA0 \\xA0modified: \\xA0 vote/app.py\n \\xA0 \\xA0modified: \\xA0 worker/src/main/java/worker/Worker.java\n\n`})}),`\n`,(0,t.jsx)(e.p,{children:\"For the purpose of this blog post, I ensured the following build ran on the same builder as the first and that we will have cache hits. As noted before, this wasn\\u2019t always the case in our production environment.\"}),`\n`,(0,t.jsx)(e.p,{children:(0,t.jsx)(e.img,{src:\"/blog-images/c3660987194315b5d66f3fd5a371546d.png\",alt:\"cached-release\"})}),`\n`,(0,t.jsx)(e.p,{children:\"The caching helps and cuts 45 seconds off the build! The uncached build took almost twice as long as the second build with caching, but our assumption was that we could do a lot better (cached and uncached) with some new technology.\"}),`\n`,(0,t.jsxs)(e.h3,{id:\"enter-dockers-buildx-kubernetes-driver\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#enter-dockers-buildx-kubernetes-driver\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Enter Docker\\u2019s Buildx Kubernetes Driver\"]}),`\n`,(0,t.jsxs)(e.p,{children:[\"One of the first things we wanted to solve was the concurrency issue and we set out to ensure that Docker itself was able to handle a larger workload. We came across the issue \",(0,t.jsx)(e.a,{href:\"https://github.com/moby/moby/issues/9656\",children:\"Concurrent \\u201Cdocker build\\u201D takes longer than sequential builds\"}),\" where people were describing what we feared; Docker slowed down when many builds were being run at the same time. Lucky for us, that issue was opened in 2014 and plenty of work had been done to resolve this issue. The final comment, by a member of the Docker team, was \",(0,t.jsx)(e.a,{href:\"https://github.com/moby/moby/issues/9656#issuecomment-610476810\",children:\"\\u201CClosing this. BuildKit is highly optimized for parallel workloads. If you see anything like this in buildkit or buildkit compared to legacy builder please report a new issue with a repro case.\\u201D\"}),\" Thus we set out to learn more about \",(0,t.jsx)(e.a,{href:\"https://docs.docker.com/develop/develop-images/build_enhancements/\",children:\"BuildKit\"}),\" (the Github repository is located \",(0,t.jsx)(e.a,{href:\"https://github.com/moby/buildkit\",children:\"here\"}),\"). While researching, we came across \",(0,t.jsx)(e.a,{href:\"https://github.com/docker/buildx\",children:\"buildx\"}),\", which ended up having three key features we believed would resolve many of our issues. These three features were the \",(0,t.jsx)(e.a,{href:\"https://github.com/docker/buildx#buildx-bake-options-target\",children:\"bake\"}),\" command, the \",(0,t.jsx)(e.a,{href:\"https://github.com/docker/buildx#--driver-driver\",children:\"buildx kubernetes driver\"}),\", and the ability for the Kubernetes driver to consistently send builds to the same server. Let\\u2019s cover each of these, first up the bake command.\"]}),`\n`,(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:\"language-none\",children:`\nbuildx bake [OPTIONS] [TARGET...]\nBake is a high-level build command.\n\nEach specified target will run in parallel as part of the build.\n\n`})}),`\n`,(0,t.jsxs)(e.p,{children:[\"bake intrigued us because it seemed to be a built-in command for us to avoid using Ruby threading for our parallelism. bake takes an input of a file, which can either be in the form of a docker-compose, .json, or .hcl. We initially tested bake with the docker-compose from example-voting-app and we were blown away at how smoothly it built directly out of the box and how quickly it was able to build the three images! However, we opted to create our own .json file generator in Ruby, parsing our \",(0,t.jsx)(e.a,{href:\"https://releasehub.com/blog/cutting-build-time-in-half-docker-buildx-kubernetes\",children:\"Application Template\"}),\" into an output. Here is our generated file for example-voting-app.\"]}),`\n`,(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:\"language-json\",children:`{\n  \"group\": {\n    \"default\": {\n      \"targets\": [\"vote\", \"result\", \"worker\"]\n    }\n  },\n  \"target\": {\n    \"vote\": {\n      \"context\": \"./vote\",\n      \"tags\": [\n        \".dkr.ecr.us-west-2.amazonaws.com/jer-k/release-example-voting-app/vote:latest\",\n        \".dkr.ecr.us-west-2.amazonaws.com/jer-k/release-example-voting-app/vote:buildx-builders\"\n      ]\n    },\n    \"result\": {\n      \"context\": \"./result\",\n      \"tags\": [\n        \".dkr.ecr.us-west-2.amazonaws.com/jer-k/release-example-voting-app/result:latest\",\n        \".dkr.ecr.us-west-2.amazonaws.com/jer-k/release-example-voting-app/result:buildx-builders\"\n      ]\n    },\n    \"worker\": {\n      \"context\": \"./worker\",\n      \"tags\": [\n        \".dkr.ecr.us-west-2.amazonaws.com/jer-k/release-example-voting-app/worker:latest\",\n        \".dkr.ecr.us-west-2.amazonaws.com/jer-k/release-example-voting-app/worker:buildx-builders\"\n      ]\n    }\n  }\n}\n`})}),`\n`,(0,t.jsx)(e.p,{children:\"There are other inputs which can make their way into this file, such as build args, but since example-voting-app does not have any, they are omitted.\"}),`\n`,(0,t.jsxs)(e.p,{children:[\"Next, we wanted to find more information on the Kubernetes driver and we found this blog post \",(0,t.jsx)(e.a,{href:\"https://medium.com/nttlabs/buildx-kubernetes-ad0fe59b0c64\",children:\"Kubernetes driver for Docker BuildX\"}),\" from the author of the \",(0,t.jsx)(e.a,{href:\"https://github.com/docker/buildx/pull/167\",children:\"Pull Request\"}),\". We encourage you to read the latter as it covers getting up and running with the Kubernetes driver as well how the caching works, which is exactly what we needed. With that information in hand, we were able to start work on adding the buildx servers to our cluster. We created a generic way to deploy the servers into different clusters and adjust the number of replicas with the final command being\"]}),`\n`,(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:\"language-none\",children:`\ndocker buildx create --name #{name} --driver kubernetes --driver-opt replicas=#{num_replicas},namespace=#{builder_namespace} --use\n\n`})}),`\n`,(0,t.jsx)(e.p,{children:\"For us, we created a release-builder namespace with five replicas, in our development cluster. We can see the output by querying for the pods\"}),`\n`,(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:\"language-none\",children:`\nkubectl get pods --namespace=release-builder\nNAME \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0READY \\xA0 STATUS \\xA0 \\xA0RESTARTS \\xA0 AGE\ndevelopment0-86d99fcf46-26j9f \\xA0 1/1 \\xA0 \\xA0 Running \\xA0 0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA06d10h\ndevelopment0-86d99fcf46-5scpq \\xA0 1/1 \\xA0 \\xA0 Running \\xA0 0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA06d13h\ndevelopment0-86d99fcf46-jkk2b \\xA0 1/1 \\xA0 \\xA0 Running \\xA0 0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA015d\ndevelopment0-86d99fcf46-llkgq \\xA0 1/1 \\xA0 \\xA0 Running \\xA0 0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA018d\ndevelopment0-86d99fcf46-mr9jt \\xA0 1/1 \\xA0 \\xA0 Running \\xA0 0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA020d\n\n`})}),`\n`,(0,t.jsxs)(e.p,{children:[\"Since we have five replicas, we wanted to ensure that when we build applications, they end up on the same server so that we get the greatest amount of caching possible (distributed caching is a topic for another day). Luckily for us, buildx, with the Kubernetes driver, has an option for where to send the builds called loadbalance.\",(0,t.jsx)(e.br,{}),`\n`,\"The default sticky means that the builds should always end up on the same server due to the hashing (more detailed information on this is described in the aforementioned blog post). With all of that in place, we are ready to test out our new setup!\"]}),`\n`,(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:\"language-none\",children:`\nloadbalance=(sticky|random) - Load-balancing strategy.\nIf set to \"sticky\", the pod is chosen using the hash of the context path. Defaults to \"sticky\"\n\n`})}),`\n`,(0,t.jsx)(e.p,{children:\"Using the same example-voting-app repository as before, I created a new branch buildx_builders and pointed the code to the buildx servers.\"}),`\n`,(0,t.jsx)(e.p,{children:(0,t.jsx)(e.img,{src:\"/blog-images/6301520361c0659740b319b30b8d8bee.png\",alt:\"uncached-buildx\"})}),`\n`,(0,t.jsx)(e.p,{children:\"What we see is that this uncached build was more than twice as fast as the other uncached build and even faster than the cached build on the old infrastructure! But uncached builds should be a thing of the past with the sticky load balancing, so let\\u2019s make the same changes as the previous branch and see the results.\"}),`\n`,(0,t.jsx)(e.p,{children:(0,t.jsx)(e.img,{src:\"/blog-images/d22dd82384c12f3bab0bbe23e33457e7.png\",alt:\"cached-buildx\"})}),`\n`,(0,t.jsx)(e.p,{children:\"This build finished three times faster than the previous cached build! These types of speed increases are the reason we set out to redo our build infrastructure. The faster the builds complete, the faster we can create environments and help our customers deliver their products.\"}),`\n`,(0,t.jsx)(e.p,{children:\"We\\u2019re still experimenting with buildx and learning as we go, but the initial results were more than enough for us to migrate our own production builds to the new infrastructure. We\\u2019re going to continue to blog about this topic as we learn more and scale so check back in with the Release blog in the future!\"})]})}function y(n={}){let{wrapper:e}=n.components||{};return e?(0,t.jsx)(e,Object.assign({},n,{children:(0,t.jsx)(h,n)})):h(n)}var x=y;return k(T);})();\n;return Component;"
        },
        "_id": "blog/posts/cutting-build-time-in-half-docker-buildx-kubernetes.mdx",
        "_raw": {
          "sourceFilePath": "blog/posts/cutting-build-time-in-half-docker-buildx-kubernetes.mdx",
          "sourceFileName": "cutting-build-time-in-half-docker-buildx-kubernetes.mdx",
          "sourceFileDir": "blog/posts",
          "contentType": "mdx",
          "flattenedPath": "blog/posts/cutting-build-time-in-half-docker-buildx-kubernetes"
        },
        "type": "BlogPost",
        "computedSlug": "cutting-build-time-in-half-docker-buildx-kubernetes"
      },
      "documentHash": "1739393595017",
      "hasWarnings": false,
      "documentTypeName": "BlogPost"
    },
    "blog/posts/development-environments-and-how-to-manage-them.mdx": {
      "document": {
        "title": "Development Environments and how to Manage Them",
        "summary": "A development environment is a set of processes and tools to develop and maintain software. Read on to learn more.",
        "publishDate": "Mon Jan 30 2023 08:21:39 GMT+0000 (Coordinated Universal Time)",
        "author": "",
        "readingTime": 6,
        "categories": [
          "platform-engineering",
          "product"
        ],
        "mainImage": "/blog-images/6c6c81dcb9cb3f96e09acfd56f4f46fa.jpeg",
        "imageAlt": "Development Environments and how to Manage Them",
        "showCTA": true,
        "ctaCopy": "Improve code stability and collaboration with Release's ephemeral environments for seamless development and testing. Try now!",
        "ctaLink": "https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=development-environments-and-how-to-manage-them",
        "relatedPosts": [
          ""
        ],
        "ogImage": "/blog-images/6c6c81dcb9cb3f96e09acfd56f4f46fa.jpeg",
        "excerpt": "A development environment is a set of processes and tools to develop and maintain software. Read on to learn more.",
        "tags": [
          "platform-engineering",
          "product"
        ],
        "ctaButton": "Try Release for Free",
        "body": {
          "raw": "\nDifferent environments are used at different stages of a software project's development cycle. Each stage has a unique purpose and contributes to the software's stability and reliability.\n\nOne of the stages in a software project's development cycle is the development phase, which uses the development environment. This environment contains various processes and programming tools for developing the project's source code and everything needed to maintain and scale the project.\n\nThis post will look at the importance of development environments and how to manage them.\n\n![](/blog-images/8e1ce614846f92e1ba0adb4060446ba9.png)\n\n## What is a Development Environment?\n\nA development environment is a set of processes and tools to develop and maintain software. It includes the entire environment that supports the development process, from writing and testing the source code to debugging, patching, and updating the project.\n\nEvery project uses this environment, especially long-term, extensive software maintenance and configuration management.\n\n## What is the use of Development Environments?\n\nWorking in a development environment allows developers to create, innovate, and test code without disrupting the user experience. The development environment also helps reduce costs and improve safety and privacy by enabling developers to work with simulated dependencies instead of real services, which may raise security or privacy concerns. \n\nUltimately, the dev environment is used to build your application. This environment allows developers to create better code more efficiently and safely by automating or facilitating the routines involved in software development and maintenance.\n\nAll of these ensure that nothing breaks in a live environment.\n\n## Types of Development Environments\n\nDevelopers can use several types of development environments to create and maintain software. These include:\n\n- **Local development environment**: A development environment set up on a local computer or server. This environment allows developers to work on code without affecting the user experience or production environment. One advantage of this development environment is that it doesn't require an internet connection. However, it can be disadvantageous because of its limited resources and difficulty collaborating with team members.\n- **Virtual development environment**: A dev environment created using virtualization software. This type of environment allows developers to set up an isolated environment for development and testing purposes. Some advantages of this development environment include its ease of set up and configuration, and how it can mimic the production environment. On the other hand, a virtual development environment needs a host operating system and has the potential for slower performance compared to a local development environment.\n- **Cloud-based development environment**: A development environment hosted on a cloud computing platform. Developers can access a cloud-based development environment from anywhere with an internet connection. This type of development environment has some benefits, such as facilitating simple collaboration and not requiring local resources. It also has drawbacks, including shared cloud space and potential security issues.\n- **Integrated development environment (IDE)**: IDEs are programming tools with a code editor, debugger, and other features to help developers write and test code more efficiently. Some IDEs have a development environment, but others are used in conjunction with a separate development environment. The ability to streamline the dev process and the tooling and feature range this environment offers are some of its perks. However, it's only suitable for some projects and has a steep learning curve.\n\n## How to Manage a Development Environment\n\nAs a software developer, you know the importance of having a well-managed and optimized dev environment. You also understand how running and testing application code in a well-maintained environment is essential. To achieve this, you need to follow some best practices.\n\n- First, define the components and configuration of your development environment. This involves identifying and specifying all the hardware, software, and dependencies needed for your project. You should also document any customizations or specific settings relevant to your project. \n- Automating the set-up process will save you a lot of time and effort, allowing you to quickly and easily set up a development environment with all the necessary components and configurations. Tools like Ansible can help you automate the configuring of software and dependencies. In contrast, containerization tools like Docker enable you to package your application and its dependencies into a self-contained unit that can be easily deployed and run in different environments.\n- Host your development environment on a cloud platform like Amazon Web Services (AWS) or Google Cloud Platform (GCP) and use an on-demand environment-as-a-service platform like [Release](https://releasehub.com/). These platforms help with scaling and improve team collaboration.\n- Use version control systems like Git to track and manage changes to your code and environment. Version control is handy, especially when working with a team, as it drives collaboration by keeping track of who made what changes.\n- Use IDEs because they simplify the development process by providing tools for writing and debugging code more effectively.\n- Regularly test and debug your development environment so applications meet your users' needs. By doing this, you can ensure smooth-running and bug-free applications post-deployment. We have a [comprehensive guide](https://releasehub.com/blog/setup-test-environment) on setting up easy-to-maintain test environments.\n- Keep detailed documentation for your development environment. It provides an understanding of your environment configuration and simplifies troubleshooting.\n\n![](/blog-images/c3992daf6658f64766d4f6413c8d8a39.jpeg)\n\n## Development Environments vs. Testing and Staging Environments\n\nDevelopment environments are workspaces for software developers to create, run, and test their application code in a simulated environment. Here, codes are written and tested before being deployed to a production environment, where end users will access the application.\n\nTesting and staging environments, on the other hand, are different.\n\nTesting environments validate the functionality and performance of an application. They're typically used to catch bugs and defects not detected during development. While they can be separate from the dev environment, sometimes they're just a copy of the development environment with additional testing tools and resources.\n\nA separate testing environment from the dev environment allows developers to focus on writing and debugging code without worrying about the impact on testing efforts. In addition, it helps to ensure that the testing process is consistent and thorough.\n\nHowever, the critical point is that the testing environment has more resources or different configurations to simulate better real-world scenarios. Thus, issues are more apparent here.\n\nStaging environments test, validate changes, and update changes to the application before it's deployed to the production environment. For example, you might make changes to the code in your dev environment and then deploy those changes to the staging environment to test them.\n\nThe staging environment should be a replica of the production environment, so you can be confident that the changes will work correctly in the live environment.\n\n![](/blog-images/6eef86d56773c332de4a13378d9a2da0.png)\n\n## Integration of IDEs and Cloud-Based Development Environments\n\nThe convergence of IDEs and cloud-based development environments has revolutionized the software development process by providing developers with increased accessibility, flexibility, and efficiency.\n\nOne such convergence can be seen in IDEs now integrated with cloud-based environments. These cloud-based IDEs, like Gitpod, allow developers to access and work on their projects from any location and device.\n\nGitpod is an open-source, cloud-based developer platform by Google. This cloud-based IDE offers developers a pre-configured, ready-to-code dev environment directly from their Git repository, thus eliminating the need for local set up and configurations.\n\nThis helps streamline development by providing a web-based, extensible IDE that supports various programming languages and technologies. It also offers other comprehensive features like automatic dependency management with a built-in terminal and command-line interface, which supports debugging and testing.\n\nOverall, integrating IDEs and cloud-based development environments has enabled organizations to support hybrid development teams of any size and allocate more computing power for high-demand workloads. These tools play a vital role in this convergence.\n\n## In a Nutshell\n\nDevelopment environments are crucial for the creation and maintenance of software. Because there are various dev environments, it's essential to carefully consider which type of dev environment is best suited for a particular project. \n\nProper development environment management is also essential for ensuring smooth and efficient software development. While you can do this by following best practices, the process can slow down, reduce your product velocity, and add bottlenecks in the development cycle.\n\nThat's why [on-demand environment-as-a-service platforms](https://releasehub.com/whitepaper/easy-environments-management) like [Release](https://prod.releasehub.com/#organizations) can come in for easier environment management. With a low maintenance cost and reduced turnaround time for product features, your developers can spend more time optimizing code and less time worrying about downtime.\n\nSet up a demo with the team today; let's discuss how [Release](https://releasehub.com/book-a-demo) will help your business.\n\n## Social Media Blurb\n\nDid you know that development environments are crucial for successful software development? This post explores the different types of dev environments and how to manage them effectively. Click to learn more!\n",
          "code": "var Component=(()=>{var m=Object.create;var a=Object.defineProperty;var h=Object.getOwnPropertyDescriptor;var p=Object.getOwnPropertyNames;var v=Object.getPrototypeOf,u=Object.prototype.hasOwnProperty;var g=(t,e)=>()=>(e||t((e={exports:{}}).exports,e),e.exports),f=(t,e)=>{for(var o in e)a(t,o,{get:e[o],enumerable:!0})},s=(t,e,o,r)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let i of p(e))!u.call(t,i)&&i!==o&&a(t,i,{get:()=>e[i],enumerable:!(r=h(e,i))||r.enumerable});return t};var y=(t,e,o)=>(o=t!=null?m(v(t)):{},s(e||!t||!t.__esModule?a(o,\"default\",{value:t,enumerable:!0}):o,t)),w=t=>s(a({},\"__esModule\",{value:!0}),t);var d=g((I,l)=>{l.exports=_jsx_runtime});var D={};f(D,{default:()=>T,frontmatter:()=>b});var n=y(d()),b={title:\"Development Environments and how to Manage Them\",summary:\"A development environment is a set of processes and tools to develop and maintain software. Read on to learn more.\",publishDate:\"Mon Jan 30 2023 08:21:39 GMT+0000 (Coordinated Universal Time)\",author:\"\",readingTime:6,categories:[\"platform-engineering\",\"product\"],mainImage:\"/blog-images/6c6c81dcb9cb3f96e09acfd56f4f46fa.jpeg\",imageAlt:\"Development Environments and how to Manage Them\",showCTA:!0,ctaCopy:\"Improve code stability and collaboration with Release's ephemeral environments for seamless development and testing. Try now!\",ctaLink:\"https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=development-environments-and-how-to-manage-them\",relatedPosts:[\"\"],ogImage:\"/blog-images/6c6c81dcb9cb3f96e09acfd56f4f46fa.jpeg\",excerpt:\"A development environment is a set of processes and tools to develop and maintain software. Read on to learn more.\",tags:[\"platform-engineering\",\"product\"],ctaButton:\"Try Release for Free\"};function c(t){let e=Object.assign({p:\"p\",img:\"img\",h2:\"h2\",a:\"a\",span:\"span\",ul:\"ul\",li:\"li\",strong:\"strong\"},t.components);return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(e.p,{children:\"Different environments are used at different stages of a software project's development cycle. Each stage has a unique purpose and contributes to the software's stability and reliability.\"}),`\n`,(0,n.jsx)(e.p,{children:\"One of the stages in a software project's development cycle is the development phase, which uses the development environment. This environment contains various processes and programming tools for developing the project's source code and everything needed to maintain and scale the project.\"}),`\n`,(0,n.jsx)(e.p,{children:\"This post will look at the importance of development environments and how to manage them.\"}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/8e1ce614846f92e1ba0adb4060446ba9.png\",alt:\"\"})}),`\n`,(0,n.jsxs)(e.h2,{id:\"what-is-a-development-environment\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#what-is-a-development-environment\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"What is a Development Environment?\"]}),`\n`,(0,n.jsx)(e.p,{children:\"A development environment is a set of processes and tools to develop and maintain software. It includes the entire environment that supports the development process, from writing and testing the source code to debugging, patching, and updating the project.\"}),`\n`,(0,n.jsx)(e.p,{children:\"Every project uses this environment, especially long-term, extensive software maintenance and configuration management.\"}),`\n`,(0,n.jsxs)(e.h2,{id:\"what-is-the-use-of-development-environments\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#what-is-the-use-of-development-environments\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"What is the use of Development Environments?\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Working in a development environment allows developers to create, innovate, and test code without disrupting the user experience. The development environment also helps reduce costs and improve safety and privacy by enabling developers to work with simulated dependencies instead of real services, which may raise security or privacy concerns.\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:\"Ultimately, the dev environment is used to build your application. This environment allows developers to create better code more efficiently and safely by automating or facilitating the routines involved in software development and maintenance.\"}),`\n`,(0,n.jsx)(e.p,{children:\"All of these ensure that nothing breaks in a live environment.\"}),`\n`,(0,n.jsxs)(e.h2,{id:\"types-of-development-environments\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#types-of-development-environments\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Types of Development Environments\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Developers can use several types of development environments to create and maintain software. These include:\"}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"Local development environment\"}),\": A development environment set up on a local computer or server. This environment allows developers to work on code without affecting the user experience or production environment. One advantage of this development environment is that it doesn't require an internet connection. However, it can be disadvantageous because of its limited resources and difficulty collaborating with team members.\"]}),`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"Virtual development environment\"}),\": A dev environment created using virtualization software. This type of environment allows developers to set up an isolated environment for development and testing purposes. Some advantages of this development environment include its ease of set up and configuration, and how it can mimic the production environment. On the other hand, a virtual development environment needs a host operating system and has the potential for slower performance compared to a local development environment.\"]}),`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"Cloud-based development environment\"}),\": A development environment hosted on a cloud computing platform. Developers can access a cloud-based development environment from anywhere with an internet connection. This type of development environment has some benefits, such as facilitating simple collaboration and not requiring local resources. It also has drawbacks, including shared cloud space and potential security issues.\"]}),`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"Integrated development environment (IDE)\"}),\": IDEs are programming tools with a code editor, debugger, and other features to help developers write and test code more efficiently. Some IDEs have a development environment, but others are used in conjunction with a separate development environment. The ability to streamline the dev process and the tooling and feature range this environment offers are some of its perks. However, it's only suitable for some projects and has a steep learning curve.\"]}),`\n`]}),`\n`,(0,n.jsxs)(e.h2,{id:\"how-to-manage-a-development-environment\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#how-to-manage-a-development-environment\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"How to Manage a Development Environment\"]}),`\n`,(0,n.jsx)(e.p,{children:\"As a software developer, you know the importance of having a well-managed and optimized dev environment. You also understand how running and testing application code in a well-maintained environment is essential. To achieve this, you need to follow some best practices.\"}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsx)(e.li,{children:\"First, define the components and configuration of your development environment. This involves identifying and specifying all the hardware, software, and dependencies needed for your project. You should also document any customizations or specific settings relevant to your project.\\xA0\"}),`\n`,(0,n.jsx)(e.li,{children:\"Automating the set-up process will save you a lot of time and effort, allowing you to quickly and easily set up a development environment with all the necessary components and configurations. Tools like Ansible can help you automate the configuring of software and dependencies. In contrast, containerization tools like Docker enable you to package your application and its dependencies into a self-contained unit that can be easily deployed and run in different environments.\"}),`\n`,(0,n.jsxs)(e.li,{children:[\"Host your development environment on a cloud platform like Amazon Web Services (AWS) or Google Cloud Platform (GCP) and use an on-demand environment-as-a-service platform like \",(0,n.jsx)(e.a,{href:\"https://releasehub.com/\",children:\"Release\"}),\". These platforms help with scaling and improve team collaboration.\"]}),`\n`,(0,n.jsx)(e.li,{children:\"Use version control systems like Git to track and manage changes to your code and environment. Version control is handy, especially when working with a team, as it drives collaboration by keeping track of who made what changes.\"}),`\n`,(0,n.jsx)(e.li,{children:\"Use IDEs because they simplify the development process by providing tools for writing and debugging code more effectively.\"}),`\n`,(0,n.jsxs)(e.li,{children:[\"Regularly test and debug your development environment so applications meet your users' needs. By doing this, you can ensure smooth-running and bug-free applications post-deployment. We have a \",(0,n.jsx)(e.a,{href:\"https://releasehub.com/blog/setup-test-environment\",children:\"comprehensive guide\"}),\" on setting up easy-to-maintain test environments.\"]}),`\n`,(0,n.jsx)(e.li,{children:\"Keep detailed documentation for your development environment. It provides an understanding of your environment configuration and simplifies troubleshooting.\"}),`\n`]}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/c3992daf6658f64766d4f6413c8d8a39.jpeg\",alt:\"\"})}),`\n`,(0,n.jsxs)(e.h2,{id:\"development-environments-vs-testing-and-staging-environments\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#development-environments-vs-testing-and-staging-environments\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Development Environments vs. Testing and Staging Environments\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Development environments are workspaces for software developers to create, run, and test their application code in a simulated environment. Here, codes are written and tested before being deployed to a production environment, where end users will access the application.\"}),`\n`,(0,n.jsx)(e.p,{children:\"Testing and staging environments, on the other hand, are different.\"}),`\n`,(0,n.jsx)(e.p,{children:\"Testing environments validate the functionality and performance of an application. They're typically used to catch bugs and defects not detected during development. While they can be separate from the dev environment, sometimes they're just a copy of the development environment with additional testing tools and resources.\"}),`\n`,(0,n.jsx)(e.p,{children:\"A separate testing environment from the dev environment allows developers to focus on writing and debugging code without worrying about the impact on testing efforts. In addition, it helps to ensure that the testing process is consistent and thorough.\"}),`\n`,(0,n.jsx)(e.p,{children:\"However, the critical point is that the testing environment has more resources or different configurations to simulate better real-world scenarios. Thus, issues are more apparent here.\"}),`\n`,(0,n.jsx)(e.p,{children:\"Staging environments test, validate changes, and update changes to the application before it's deployed to the production environment. For example, you might make changes to the code in your dev environment and then deploy those changes to the staging environment to test them.\"}),`\n`,(0,n.jsx)(e.p,{children:\"The staging environment should be a replica of the production environment, so you can be confident that the changes will work correctly in the live environment.\"}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/6eef86d56773c332de4a13378d9a2da0.png\",alt:\"\"})}),`\n`,(0,n.jsxs)(e.h2,{id:\"integration-of-ides-and-cloud-based-development-environments\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#integration-of-ides-and-cloud-based-development-environments\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Integration of IDEs and Cloud-Based Development Environments\"]}),`\n`,(0,n.jsx)(e.p,{children:\"The convergence of IDEs and cloud-based development environments has revolutionized the software development process by providing developers with increased accessibility, flexibility, and efficiency.\"}),`\n`,(0,n.jsx)(e.p,{children:\"One such convergence can be seen in IDEs now integrated with cloud-based environments. These cloud-based IDEs, like Gitpod, allow developers to access and work on their projects from any location and device.\"}),`\n`,(0,n.jsx)(e.p,{children:\"Gitpod is an open-source, cloud-based developer platform by Google. This cloud-based IDE offers developers a pre-configured, ready-to-code dev environment directly from their Git repository, thus eliminating the need for local set up and configurations.\"}),`\n`,(0,n.jsx)(e.p,{children:\"This helps streamline development by providing a web-based, extensible IDE that supports various programming languages and technologies. It also offers other comprehensive features like automatic dependency management with a built-in terminal and command-line interface, which supports debugging and testing.\"}),`\n`,(0,n.jsx)(e.p,{children:\"Overall, integrating IDEs and cloud-based development environments has enabled organizations to support hybrid development teams of any size and allocate more computing power for high-demand workloads. These tools play a vital role in this convergence.\"}),`\n`,(0,n.jsxs)(e.h2,{id:\"in-a-nutshell\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#in-a-nutshell\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"In a Nutshell\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Development environments are crucial for the creation and maintenance of software. Because there are various dev environments, it's essential to carefully consider which type of dev environment is best suited for a particular project.\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:\"Proper development environment management is also essential for ensuring smooth and efficient software development. While you can do this by following best practices, the process can slow down, reduce your product velocity, and add bottlenecks in the development cycle.\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"That's why \",(0,n.jsx)(e.a,{href:\"https://releasehub.com/whitepaper/easy-environments-management\",children:\"on-demand environment-as-a-service platforms\"}),\" like \",(0,n.jsx)(e.a,{href:\"https://prod.releasehub.com/#organizations\",children:\"Release\"}),\" can come in for easier environment management. With a low maintenance cost and reduced turnaround time for product features, your developers can spend more time optimizing code and less time worrying about downtime.\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"Set up a demo with the team today; let's discuss how \",(0,n.jsx)(e.a,{href:\"https://releasehub.com/book-a-demo\",children:\"Release\"}),\" will help your business.\"]}),`\n`,(0,n.jsxs)(e.h2,{id:\"social-media-blurb\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#social-media-blurb\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Social Media Blurb\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Did you know that development environments are crucial for successful software development? This post explores the different types of dev environments and how to manage them effectively. Click to learn more!\"})]})}function k(t={}){let{wrapper:e}=t.components||{};return e?(0,n.jsx)(e,Object.assign({},t,{children:(0,n.jsx)(c,t)})):c(t)}var T=k;return w(D);})();\n;return Component;"
        },
        "_id": "blog/posts/development-environments-and-how-to-manage-them.mdx",
        "_raw": {
          "sourceFilePath": "blog/posts/development-environments-and-how-to-manage-them.mdx",
          "sourceFileName": "development-environments-and-how-to-manage-them.mdx",
          "sourceFileDir": "blog/posts",
          "contentType": "mdx",
          "flattenedPath": "blog/posts/development-environments-and-how-to-manage-them"
        },
        "type": "BlogPost",
        "computedSlug": "development-environments-and-how-to-manage-them"
      },
      "documentHash": "1739393595017",
      "hasWarnings": false,
      "documentTypeName": "BlogPost"
    },
    "blog/posts/development-vs-staging-vs-production-whats-the-difference-na.mdx": {
      "document": {
        "title": "Development vs Staging vs Production: What's the Difference?",
        "summary": "We'll take a look at the development, staging, and production environments of the software development lifecycle.",
        "publishDate": "Fri Feb 18 2022 20:56:25 GMT+0000 (Coordinated Universal Time)",
        "author": "nuryani-asari",
        "readingTime": 7,
        "categories": [
          "platform-engineering",
          "product"
        ],
        "mainImage": "/blog-images/f80609b63a214983c328d0066450fb8c.jpg",
        "imageAlt": "Development vs Staging vs Production: What's the Difference?",
        "showCTA": true,
        "ctaCopy": "Improve code quality and streamline workflows with Release's ephemeral environments, mirroring production for seamless collaboration and faster testing.",
        "ctaLink": "https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=development-vs-staging-vs-production-whats-the-difference-na",
        "relatedPosts": [
          ""
        ],
        "ogImage": "/blog-images/f80609b63a214983c328d0066450fb8c.jpg",
        "excerpt": "We'll take a look at the development, staging, and production environments of the software development lifecycle.",
        "tags": [
          "platform-engineering",
          "product"
        ],
        "ctaButton": "Try Release for Free",
        "body": {
          "raw": "\nThe lines between development, staging, and production environments are often blurred. The distinctions may vary depending on many factors, including: \n\n- the scale of the organization,\n- the codebase, or \n- whether you're viewing the environment from a product, unit testing, or security standpoint.\n\nIn this post, I use interviews with fellow developers to understand each environment's purpose and how it's distinct from the others. It's particularly challenging to differentiate between the development and staging environments, and some organizations forgo the staging environment altogether. \n\nLet’s get to it. \n\n## What Is a Development Environment?\n\nGenerally, the development environment is the first environment developers use to check if all code changes work well with each other. It's known as a “sandbox” for developers to work in. Examples of commonly used integrated development environments (IDEs) are Visual Studio Code and Atom. You should note that historically the development environment is usually on a developer's laptop—a local machine, but with the emergence of cloud, on-demand computing and [ephemeral environments](http://releasehub.com/ephemeral-environments), those environments are now being deployed in the cloud. \n\nIn my experience as a developer, the IDE is where developers’ workflow with code reloading and debugging aids are enabled. Also, this environment is where developers can make necessary code changes. In the IDE, approved code can merge with code from other developers working on the same project. \n\nDevelopers commonly use this space to experiment and receive feedback on what improvements they can make to their work. Consequently, this environment is the most unstable. It's also the most susceptible to bugs and potentially broken code. But, on the upside, in allowing mistakes to happen, this is the most conducive environment to learn collaboratively and create a standardized process.  \n\nFurthermore, there are three environments within the development environment, [according to the Indeed Editorial Team](https://www.indeed.com/career-advice/career-development/development-environment). Besides the most commonly known local machine, there are virtual and cloud-based development environments. The reason for the virtual and cloud-based environments mainly depends on whether multiple platforms and machines are used. \n\nDevelopment environments historically would only include a small subset of the entire application and many times would lack elements like security, 3rd party APIs and cloud native services. Those would typically be introduced later in the development process and tested in staging. The results, however, would be frequent rollbacks and bottlenecks in staging. To enable better code quality in development and more frequent release cycles, companies like [Releasehub](https://releasehub.com) came up with [ephemeral environments](http://releasehub.com/ephemeral-environments), a production-like replica that allows developers to properly test their code (i.e shift-left) and isolate bugs to a single branch, while ensuring a smooth merge to staging and production. \n\n## What Is a Staging Environment?\n\nAccording to [Releasehub](https://releasehub.com/staging-environments), “A staging environment is the environment where your code is 'staged' prior to being run in front of users so you can ensure it works as designed.” \n\nThe staging environment should mirror production as much as possible. It reflects a production environment not yet exposed to clients, customers, and the general public. \n\n![development staging production PQ01](/blog-images/6dc42994f06abe08d6b970f9bbbee2ca.png)\n\nThis environment is primarily used for system integration testing (SIT) and in-depth manual testing conducted before the client receives code changes. According to [Unitrends](https://www.unitrends.com/blog/development-test-environments), other tests developers perform in this environment are quality assurance (QA), security testing, chaos testing, alpha testing, beta testing, and end-to-end (E2E) testing. \n\nAdditionally, [user acceptance testing (UAT)](https://releasehub.com/user-acceptance-testing-with-ephemeral-environments) often happens before production. In UAT, users can test changes they requested before the new code goes to a production environment. \n\nHow you carry out testing in the staging environment can depend on what programming language you're using. For example, Ruby on Rails doesn't have a mode for staging. Rails developers switch modes to a test environment that they use to run testing tools and debug failures. Technically, the [Rails Guide](https://guides.rubyonrails.org/configuring.html) delves into how to customize configurations and initialization on applications. \n\n## Development vs. Staging Environments \n\nSo, now that you know what development and staging environments are, you're probably wondering if you should need both. Truth be told, the answer depends on the size of your organization. \n\nSometimes smaller companies start out with fewer environments. One developer I interviewed shared, “You just end up with multiple environments as the organization scales up.”  \n\nThere are cases in which organizations with fewer users don't have staging environments. As another developer I interviewed elaborated at length, \n\n> *“Instead, we can deploy as such that 1% of the traffic will go to each one branch and main branch. Then, we can check the monitoring to see if there are differences between the two. When we are certain that at the most we will affect 1% of traffic and everything is fine, we will then proceed with merging the two branches. I think it would be ideal if the continuous integration (CI) and continuous deployment (CD) process were to set up that 1%, then we could verify the results. This is the same as I have seen for verifying front-end changes in continuous integration.”* \n\n### Is a Staging Environment Necessary? \n\nIt's better to merge and deploy code quickly rather than have a staging environment where untested code might linger. Using CI/CD in testing can have many benefits that may prevent problems. Tommy McClung, the former CTO of Truecar discussed his experience with staging environments [here](https://www.bigmarker.com/techwell-corporation/Beyond-K8s-Introduction-to-Ephemeral-Environments?utm_bmcr_source=TWComm), . \n\nOne benefit is that it balances out real input: you risk a bad result for only a small amount of your live traffic while you're able to get the most accurate test results. \n\nDevelopers say they like to see how real traffic works through the codebase and compare this technique to feature flagging. This may eliminate the need for a beta environment. This results in the concept of \"staging\" not being a distinct environment. \n\nHowever, developers agree that it's useful to have a separate beta domain to make significant changes. According to [Atlassian CI/CD](https://www.atlassian.com/continuous-delivery/principles/feature-flags), feature flagging allows developers to turn functionality on and off during runtime. That way, you don't need to deploy code at every update. \n\n## What Is a Production Environment?\n\nThe production environment is the live site complete with performance optimizations. The codebase must be secure, performant, stable, and able to sustain heavy traffic as the client, customers, and public use it.  \n\nTherefore, you must treat it with great care. You should restrict who updates the production code. Ideally, you won't be building new versions of the codebase for the production environment; it's better to deploy the same builds to the staging environment. \n\n![development staging production PQ02](/blog-images/f8c8c7b2894d93f11bb40ec129345352.png)\n\nAt this point of the software development lifecycle, the code shouldn't have any bugs or require fixes. To avoid a poor user experience, you should consider it the final product. \n\nHowever, you can make urgent fixes in the production environment if needed. In doing so, you can consistently improve upon quality control for product releases, making it easier to keep tabs on new product updates. \n\n## Conclusion\n\nAlthough the development, staging, and production environments converge, they have their own significance in the larger software development lifecycle. The significance of each environment depends on the organization running the system. \n\nThe way a company treats and leverages these environments today differs wildly depending on the organization and its DevOps practices and policies. Sometimes teams within the same organization use these environments in different ways and have different philosophies of what they mean. \n\nFrom my conversations with many individuals that play different roles in the tech industry, I can say the overall development culture is shifting progressively toward promoting new code to all these environments as soon as possible. One developer expressed, \"The idea is that even the smallest code change gets released to production in a matter of minutes, not months.\" \n\nWith that in mind, the common goal is that the folks responsible for the software development life cycle want more efficient environments for producing the highest quality codebases. These people continuously strive to find new methods to make that process easier. \n\nFor a better understanding of what environments are and to be inspired about optimizing them, read more about [staging environments](https://releasehub.com/staging-environments), [ephemeral environments](https://releasehub.com/ephemeral-environments), and [UAT](https://releasehub.com/user-acceptance-testing-with-ephemeral-environments) with release ephemeral environments.\n",
          "code": "var Component=(()=>{var d=Object.create;var a=Object.defineProperty;var m=Object.getOwnPropertyDescriptor;var p=Object.getOwnPropertyNames;var u=Object.getPrototypeOf,g=Object.prototype.hasOwnProperty;var v=(t,e)=>()=>(e||t((e={exports:{}}).exports,e),e.exports),f=(t,e)=>{for(var o in e)a(t,o,{get:e[o],enumerable:!0})},s=(t,e,o,r)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let i of p(e))!g.call(t,i)&&i!==o&&a(t,i,{get:()=>e[i],enumerable:!(r=m(e,i))||r.enumerable});return t};var w=(t,e,o)=>(o=t!=null?d(u(t)):{},s(e||!t||!t.__esModule?a(o,\"default\",{value:t,enumerable:!0}):o,t)),y=t=>s(a({},\"__esModule\",{value:!0}),t);var h=v((D,l)=>{l.exports=_jsx_runtime});var I={};f(I,{default:()=>T,frontmatter:()=>b});var n=w(h()),b={title:\"Development vs Staging vs Production: What's the Difference?\",summary:\"We'll take a look at the development, staging, and production environments of the software development lifecycle.\",publishDate:\"Fri Feb 18 2022 20:56:25 GMT+0000 (Coordinated Universal Time)\",author:\"nuryani-asari\",readingTime:7,categories:[\"platform-engineering\",\"product\"],mainImage:\"/blog-images/f80609b63a214983c328d0066450fb8c.jpg\",imageAlt:\"Development vs Staging vs Production: What's the Difference?\",showCTA:!0,ctaCopy:\"Improve code quality and streamline workflows with Release's ephemeral environments, mirroring production for seamless collaboration and faster testing.\",ctaLink:\"https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=development-vs-staging-vs-production-whats-the-difference-na\",relatedPosts:[\"\"],ogImage:\"/blog-images/f80609b63a214983c328d0066450fb8c.jpg\",excerpt:\"We'll take a look at the development, staging, and production environments of the software development lifecycle.\",tags:[\"platform-engineering\",\"product\"],ctaButton:\"Try Release for Free\"};function c(t){let e=Object.assign({p:\"p\",ul:\"ul\",li:\"li\",h2:\"h2\",a:\"a\",span:\"span\",img:\"img\",blockquote:\"blockquote\",em:\"em\",h3:\"h3\"},t.components);return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(e.p,{children:\"The lines between development, staging, and production environments are often blurred. The distinctions may vary depending on many factors, including:\\xA0\"}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsx)(e.li,{children:\"the scale of the organization,\"}),`\n`,(0,n.jsx)(e.li,{children:\"the codebase, or\\xA0\"}),`\n`,(0,n.jsx)(e.li,{children:\"whether you're viewing the environment from a product, unit testing, or security standpoint.\"}),`\n`]}),`\n`,(0,n.jsx)(e.p,{children:\"In this post, I use interviews with fellow developers to understand each environment's purpose and how it's distinct from the others. It's particularly challenging to differentiate between the development and staging environments, and some organizations forgo the staging environment altogether.\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:\"Let\\u2019s get to it.\\xA0\"}),`\n`,(0,n.jsxs)(e.h2,{id:\"what-is-a-development-environment\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#what-is-a-development-environment\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"What Is a Development Environment?\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"Generally, the development environment is the first environment developers use to check if all code changes work well with each other. It's known as a \\u201Csandbox\\u201D for developers to work in. Examples of commonly used integrated development environments (IDEs) are Visual Studio Code and Atom. You should note that historically the development environment is usually on a developer's laptop\\u2014a local machine, but with the emergence of cloud, on-demand computing and \",(0,n.jsx)(e.a,{href:\"http://releasehub.com/ephemeral-environments\",children:\"ephemeral environments\"}),\", those environments are now being deployed in the cloud.\\xA0\"]}),`\n`,(0,n.jsx)(e.p,{children:\"In my experience as a developer, the IDE is where developers\\u2019 workflow with code reloading and debugging aids are enabled. Also, this environment is where developers can make necessary code changes. In the IDE, approved code can merge with code from other developers working on the same project.\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:\"Developers commonly use this space to experiment and receive feedback on what improvements they can make to their work. Consequently, this environment is the most unstable. It's also the most susceptible to bugs and potentially broken code. But, on the upside, in allowing mistakes to happen, this is the most conducive environment to learn collaboratively and create a standardized process. \\xA0\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"Furthermore, there are three environments within the development environment, \",(0,n.jsx)(e.a,{href:\"https://www.indeed.com/career-advice/career-development/development-environment\",children:\"according to the Indeed Editorial Team\"}),\". Besides the most commonly known local machine, there are virtual and cloud-based development environments. The reason for the virtual and cloud-based environments mainly depends on whether multiple platforms and machines are used.\\xA0\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"Development environments historically would only include a small subset of the entire application and many times would lack elements like security, 3rd party APIs and cloud native services. Those would typically be introduced later in the development process and tested in staging. The results, however, would be frequent rollbacks and bottlenecks in staging. To enable better code quality in development and more frequent release cycles, companies like \",(0,n.jsx)(e.a,{href:\"https://releasehub.com\",children:\"Releasehub\"}),\" came up with \",(0,n.jsx)(e.a,{href:\"http://releasehub.com/ephemeral-environments\",children:\"ephemeral environments\"}),\", a production-like replica that allows developers to properly test their code (i.e shift-left) and isolate bugs to a single branch, while ensuring a smooth merge to staging and production.\\xA0\"]}),`\n`,(0,n.jsxs)(e.h2,{id:\"what-is-a-staging-environment\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#what-is-a-staging-environment\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"What Is a Staging Environment?\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"According to \",(0,n.jsx)(e.a,{href:\"https://releasehub.com/staging-environments\",children:\"Releasehub\"}),\", \\u201CA staging environment is the environment where your code is 'staged' prior to being run in front of users so you can ensure it works as designed.\\u201D\\xA0\"]}),`\n`,(0,n.jsx)(e.p,{children:\"The staging environment should mirror production as much as possible. It reflects a production environment not yet exposed to clients, customers, and the general public.\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/6dc42994f06abe08d6b970f9bbbee2ca.png\",alt:\"development staging production PQ01\"})}),`\n`,(0,n.jsxs)(e.p,{children:[\"This environment is primarily used for system integration testing (SIT) and in-depth manual testing conducted before the client receives code changes. According to \",(0,n.jsx)(e.a,{href:\"https://www.unitrends.com/blog/development-test-environments\",children:\"Unitrends\"}),\", other tests developers perform in this environment are quality assurance (QA), security testing, chaos testing, alpha testing, beta testing, and end-to-end (E2E) testing.\\xA0\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"Additionally, \",(0,n.jsx)(e.a,{href:\"https://releasehub.com/user-acceptance-testing-with-ephemeral-environments\",children:\"user acceptance testing (UAT)\"}),\" often happens before production. In UAT, users can test changes they requested before the new code goes to a production environment.\\xA0\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"How you carry out testing in the staging environment can depend on what programming language you're using. For example, Ruby on Rails doesn't have a mode for staging. Rails developers switch modes to a test environment that they use to run testing tools and debug failures. Technically, the \",(0,n.jsx)(e.a,{href:\"https://guides.rubyonrails.org/configuring.html\",children:\"Rails Guide\"}),\" delves into how to customize configurations and initialization on applications.\\xA0\"]}),`\n`,(0,n.jsxs)(e.h2,{id:\"development-vs-staging-environments\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#development-vs-staging-environments\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Development vs. Staging Environments\\xA0\"]}),`\n`,(0,n.jsx)(e.p,{children:\"So, now that you know what development and staging environments are, you're probably wondering if you should need both. Truth be told, the answer depends on the size of your organization.\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:\"Sometimes smaller companies start out with fewer environments. One developer I interviewed shared, \\u201CYou just end up with multiple environments as the organization scales up.\\u201D \\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:\"There are cases in which organizations with fewer users don't have staging environments. As another developer I interviewed elaborated at length,\\xA0\"}),`\n`,(0,n.jsxs)(e.blockquote,{children:[`\n`,(0,n.jsxs)(e.p,{children:[(0,n.jsx)(e.em,{children:\"\\u201CInstead, we can deploy as such that 1% of the traffic will go to each one branch and main branch. Then, we can check the monitoring to see if there are differences between the two. When we are certain that at the most we will affect 1% of traffic and everything is fine, we will then proceed with merging the two branches. I think it would be ideal if the continuous integration (CI) and continuous deployment (CD) process were to set up that 1%, then we could verify the results. This is the same as I have seen for verifying front-end changes in continuous integration.\\u201D\"}),\"\\xA0\"]}),`\n`]}),`\n`,(0,n.jsxs)(e.h3,{id:\"is-a-staging-environment-necessary\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#is-a-staging-environment-necessary\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Is a Staging Environment Necessary?\\xA0\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"It's better to merge and deploy code quickly rather than have a staging environment where untested code might linger. Using CI/CD in testing can have many benefits that may prevent problems. Tommy McClung, the former CTO of Truecar discussed his experience with staging environments \",(0,n.jsx)(e.a,{href:\"https://www.bigmarker.com/techwell-corporation/Beyond-K8s-Introduction-to-Ephemeral-Environments?utm_bmcr_source=TWComm\",children:\"here\"}),\", .\\xA0\"]}),`\n`,(0,n.jsx)(e.p,{children:\"One benefit is that it balances out real input: you risk a bad result for only a small amount of your live traffic while you're able to get the most accurate test results.\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:'Developers say they like to see how real traffic works through the codebase and compare this technique to feature flagging. This may eliminate the need for a beta environment. This results in the concept of \"staging\" not being a distinct environment.\\xA0'}),`\n`,(0,n.jsxs)(e.p,{children:[\"However, developers agree that it's useful to have a separate beta domain to make significant changes. According to \",(0,n.jsx)(e.a,{href:\"https://www.atlassian.com/continuous-delivery/principles/feature-flags\",children:\"Atlassian CI/CD\"}),\", feature flagging allows developers to turn functionality on and off during runtime. That way, you don't need to deploy code at every update.\\xA0\"]}),`\n`,(0,n.jsxs)(e.h2,{id:\"what-is-a-production-environment\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#what-is-a-production-environment\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"What Is a Production Environment?\"]}),`\n`,(0,n.jsx)(e.p,{children:\"The production environment is the live site complete with performance optimizations. The codebase must be secure, performant, stable, and able to sustain heavy traffic as the client, customers, and public use it. \\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:\"Therefore, you must treat it with great care. You should restrict who updates the production code. Ideally, you won't be building new versions of the codebase for the production environment; it's better to deploy the same builds to the staging environment.\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/f8c8c7b2894d93f11bb40ec129345352.png\",alt:\"development staging production PQ02\"})}),`\n`,(0,n.jsx)(e.p,{children:\"At this point of the software development lifecycle, the code shouldn't have any bugs or require fixes. To avoid a poor user experience, you should consider it the final product.\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:\"However, you can make urgent fixes in the production environment if needed. In doing so, you can consistently improve upon quality control for product releases, making it easier to keep tabs on new product updates.\\xA0\"}),`\n`,(0,n.jsxs)(e.h2,{id:\"conclusion\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#conclusion\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Conclusion\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Although the development, staging, and production environments converge, they have their own significance in the larger software development lifecycle. The significance of each environment depends on the organization running the system.\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:\"The way a company treats and leverages these environments today differs wildly depending on the organization and its DevOps practices and policies. Sometimes teams within the same organization use these environments in different ways and have different philosophies of what they mean.\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:'From my conversations with many individuals that play different roles in the tech industry, I can say the overall development culture is shifting progressively toward promoting new code to all these environments as soon as possible. One developer expressed, \"The idea is that even the smallest code change gets released to production in a matter of minutes, not months.\"\\xA0'}),`\n`,(0,n.jsx)(e.p,{children:\"With that in mind, the common goal is that the folks responsible for the software development life cycle want more efficient environments for producing the highest quality codebases. These people continuously strive to find new methods to make that process easier.\\xA0\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"For a better understanding of what environments are and to be inspired about optimizing them, read more about \",(0,n.jsx)(e.a,{href:\"https://releasehub.com/staging-environments\",children:\"staging environments\"}),\", \",(0,n.jsx)(e.a,{href:\"https://releasehub.com/ephemeral-environments\",children:\"ephemeral environments\"}),\", and \",(0,n.jsx)(e.a,{href:\"https://releasehub.com/user-acceptance-testing-with-ephemeral-environments\",children:\"UAT\"}),\" with release ephemeral environments.\"]})]})}function k(t={}){let{wrapper:e}=t.components||{};return e?(0,n.jsx)(e,Object.assign({},t,{children:(0,n.jsx)(c,t)})):c(t)}var T=k;return y(I);})();\n;return Component;"
        },
        "_id": "blog/posts/development-vs-staging-vs-production-whats-the-difference-na.mdx",
        "_raw": {
          "sourceFilePath": "blog/posts/development-vs-staging-vs-production-whats-the-difference-na.mdx",
          "sourceFileName": "development-vs-staging-vs-production-whats-the-difference-na.mdx",
          "sourceFileDir": "blog/posts",
          "contentType": "mdx",
          "flattenedPath": "blog/posts/development-vs-staging-vs-production-whats-the-difference-na"
        },
        "type": "BlogPost",
        "computedSlug": "development-vs-staging-vs-production-whats-the-difference-na"
      },
      "documentHash": "1739393595017",
      "hasWarnings": false,
      "documentTypeName": "BlogPost"
    },
    "blog/posts/development-vs-staging-vs-production-whats-the-difference.mdx": {
      "document": {
        "title": "Development vs Staging vs Production: What's the Difference?",
        "summary": "A closer look at the development, staging, and production environments of the software development cycle.",
        "publishDate": "Tue Jun 27 2023 14:34:52 GMT+0000 (Coordinated Universal Time)",
        "author": "regis-wilson",
        "readingTime": 8,
        "categories": [
          "platform-engineering",
          "product"
        ],
        "mainImage": "/blog-images/599175103f1e1df59989125faa773f6f.jpg",
        "imageAlt": "Development vs Staging vs Production: What's the Difference?",
        "showCTA": true,
        "ctaCopy": "Experience seamless collaboration and faster testing with Release's ephemeral environments, mirroring production for efficient development cycles.",
        "ctaLink": "https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=development-vs-staging-vs-production-whats-the-difference",
        "relatedPosts": [
          ""
        ],
        "ogImage": "/blog-images/599175103f1e1df59989125faa773f6f.jpg",
        "excerpt": "A closer look at the development, staging, and production environments of the software development cycle.",
        "tags": [
          "platform-engineering",
          "product"
        ],
        "ctaButton": "Try Release for Free",
        "body": {
          "raw": "\nThe lines between development, staging, and production environments are often blurred. The distinctions may vary depending on many factors, including:\n\n- the scale of the organization,\n- the codebase, or\n- whether you're viewing the environment from a product, unit testing, or security standpoint.\n\nIn this post, I use interviews with fellow developers to understand each environment's purpose and how it's distinct from the others. It's particularly challenging to differentiate between the development and staging environments, and some organizations forgo the staging environment altogether. Let’s find out why, and when it makes sense.\n\n### **What Is a Development Environment?**\n\nGenerally, the development environment is the first environment developers use to check if all code changes work well with each other. It's known as a “sandbox” for developers to work in. Examples of commonly used integrated development environments (IDEs) are [Visual Studio Code](https://code.visualstudio.com/), [Eclipse,](https://www.eclipse.org/ide/) [JetBrains tools](https://www.jetbrains.com/), and many others. Note that historically development environments were based on a developer's laptop—a local machine, but with the emergence of cloud, on-demand computing and [ephemeral environments](http://releasehub.com/ephemeral-environments), those environments are now being deployed in the cloud.\n\nIDE is where developers’ workflow with code takes place, reloading and debugging aids are enabled here. Also, this environment is where developers can make necessary code changes. In the IDE, approved code can merge with code from other developers working on the same project.\n\nDevelopers commonly use this space to experiment and receive feedback on what improvements they can make to their work. Consequently, this environment is the most unstable. It's also the most susceptible to bugs and potentially broken code. But, on the upside, in allowing mistakes to happen, this is the most conducive environment to learn collaboratively and create a standardized process.  \n\nBesides the most commonly known local machine, there are virtual and cloud-based development environments. Your team might use the virtual and cloud-based environments mainly depending on whether multiple platforms and machines are needed to effectively test and run the code they are writing.\n\nDevelopment environments historically only include a small subset of the entire application and often would lack elements like security, 3rd party APIs and cloud native services. Those would typically be introduced later in the development process and tested in staging. The result, however, turns into frequent rollbacks and bottlenecks in staging. To enable better code quality in development and more frequent release cycles, companies like [Release](https://release.com/) came up with [ephemeral environments](http://release.com/ephemeral-environments), a production-like replica that allows developers to properly test their code (i.e shift-left) and isolate bugs to a single branch, while ensuring a smooth merge to staging and production.\n\n### What Is a Staging Environment?\n\n[Staging environment](https://release.com/staging-environments) is the environment where your code is 'staged' prior to being run in front of users so you can ensure it works as designed. The staging environment should mirror production as much as possible. It reflects a production environment not yet exposed to clients, customers, and the general public.\n\n![](/blog-images/7889fa49b5c1fd3e80dd96dda8886942.png)\n\nThis environment is primarily used for system integration testing (SIT) and in-depth manual testing conducted before the client receives code changes. Developers also [perform](https://www.unitrends.com/blog/development-test-environments) quality assurance (QA), security testing, chaos testing, alpha testing, beta testing, and end-to-end (E2E) testing in this environment.\n\nAdditionally, [User acceptance testing (UAT)](https://release.com/user-acceptance-testing-with-ephemeral-environments) often happens here. In UAT, users can test changes they requested before the new code goes to a production environment.\n\nHow you carry out testing in the staging environment can depend on what programming language you're using. For example, Ruby on Rails doesn't have a mode for staging. Rails developers switch modes to a test environment that they use to run testing tools and debug failures. Technically, the [Rails Guide](https://guides.rubyonrails.org/configuring.html) delves into how to customize configurations and initialization on applications.\n\n### **Development vs. Staging Environments**\n\nSo, now that you know what development and staging environments are, you're probably wondering if you need both. Ultimately, the answer depends on the size of your organization, appetite for risk and speed of change, and your position on making a tradeoff between slowing down the process for quality and testing versus launching new features quickly.\n\nSometimes smaller companies start out with fewer environments. One developer shared, “You just end up with multiple environments as the organization scales up.”  \n\nIn some cases organizations with fewer users don't have staging environments. As another developer elaborated: _“Instead, we can deploy in a way that 1% of the traffic will go to each one branch and main branch. Then, we can check the monitoring to see if there are differences between the two. When we are certain that at the most we will affect 1% of traffic and everything is fine, we will then proceed with merging the two branches. I think it would be ideal if the continuous integration (CI) and continuous deployment (CD) process were to set up that 1%, then we could verify the results. This is the same as I have seen for verifying front-end changes in continuous integration.”_\n\n### **Is a Staging Environment Necessary?**\n\nDeploying to staging is safe, because it will not affect users, but is not necessarily effective because you might not test all the features or combinations that end users will be using. The general solution to this problem is to deploy to production as quickly as you can but only enable or test subsets of new features with flags or canary testing. This way you are only risking challenges for a small subset of users, and are able to see the application perform with live traffic in the production environment.\n\nDevelopers say they like to see how real traffic works through the codebase and compare this technique to feature flagging. This may eliminate the need for a beta environment. This results in the concept of \"staging\" not being a distinct environment.\n\nHowever, developers agree that it's useful to have a separate beta domain to make significant changes. According to [Atlassian CI/CD](https://www.atlassian.com/continuous-delivery/principles/feature-flags), feature flagging allows developers to turn functionality on and off during runtime. That way, you don't need to deploy code at every update.\n\n### **What Is a Production Environment?**\n\nThe production environment is the live site complete with performance optimizations. The codebase must be secure, performant, stable, and able to sustain heavy traffic as the client, customers, and public use it.  \n\nThere is a common misconception that production is more important than development or staging. Actually, the reverse could be true: development environments could be so critical to the business that they cannot tolerate any downtime at all but production can tolerate some downtime.\n\nAs an example at Truecar and in most other companies I worked at, the website could be broken for some amount of time as long as it came back up relatively quickly. However, if development was down for more than an hour, you could be looking at losing an entire day of developer features for the whole company!\n\nRegardless of your setup, you should treat production with care, and restrict who updates the production code. Ideally, you won't be building new versions of the codebase for the production environment; it's better to deploy the same builds to the staging environment.\n\n![](/blog-images/436b6ae884f81c86dc705a7163ab8dfa.png)\n\nAt this point of the software development lifecycle, the code shouldn't have any bugs or require fixes. To avoid a poor user experience, you should consider it the final product.\n\nHowever, you can make urgent fixes in the production environment if needed. In doing so, you can consistently improve upon quality control for product releases, making it easier to keep tabs on new product updates.\n\n### **Conclusion**\n\nAlthough the development, staging, and production environments converge, they have their own significance in the larger software development lifecycle. The significance of each environment depends on the organization running the system.\n\nThe way a company treats and leverages these environments today differs wildly depending on the organization and its DevOps practices and policies. Sometimes teams within the same organization use these environments in different ways and have different philosophies of what they mean, and how critical they are to the company’s mission.\n\nFrom my conversations with individuals who play different roles in the tech industry, I can say the overall development culture is shifting progressively toward promoting new code to all these environments as soon as possible. One developer expressed, \"The idea is that even the smallest code change gets released to production in a matter of minutes, not months.\"\n\nWith that in mind, the common goal is that the folks responsible for the software development life cycle want more efficient environments for producing the highest quality codebases. These people continuously strive to find new methods to make that process easier.\n\nFor a better understanding of what environments are and to be inspired about optimizing them, read more about [staging environments](https://release.com/staging-environments), [ephemeral environments](https://release.com/ephemeral-environments), and [UAT](https://release.com/user-acceptance-testing-with-ephemeral-environments) with Release ephemeral environments.\n\n‍\n",
          "code": "var Component=(()=>{var d=Object.create;var a=Object.defineProperty;var m=Object.getOwnPropertyDescriptor;var p=Object.getOwnPropertyNames;var u=Object.getPrototypeOf,g=Object.prototype.hasOwnProperty;var f=(t,e)=>()=>(e||t((e={exports:{}}).exports,e),e.exports),v=(t,e)=>{for(var o in e)a(t,o,{get:e[o],enumerable:!0})},r=(t,e,o,s)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let i of p(e))!g.call(t,i)&&i!==o&&a(t,i,{get:()=>e[i],enumerable:!(s=m(e,i))||s.enumerable});return t};var w=(t,e,o)=>(o=t!=null?d(u(t)):{},r(e||!t||!t.__esModule?a(o,\"default\",{value:t,enumerable:!0}):o,t)),y=t=>r(a({},\"__esModule\",{value:!0}),t);var c=f((D,l)=>{l.exports=_jsx_runtime});var I={};v(I,{default:()=>T,frontmatter:()=>b});var n=w(c()),b={title:\"Development vs Staging vs Production: What's the Difference?\",summary:\"A closer look at the development, staging, and production environments of the software development cycle.\",publishDate:\"Tue Jun 27 2023 14:34:52 GMT+0000 (Coordinated Universal Time)\",author:\"regis-wilson\",readingTime:8,categories:[\"platform-engineering\",\"product\"],mainImage:\"/blog-images/599175103f1e1df59989125faa773f6f.jpg\",imageAlt:\"Development vs Staging vs Production: What's the Difference?\",showCTA:!0,ctaCopy:\"Experience seamless collaboration and faster testing with Release's ephemeral environments, mirroring production for efficient development cycles.\",ctaLink:\"https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=development-vs-staging-vs-production-whats-the-difference\",relatedPosts:[\"\"],ogImage:\"/blog-images/599175103f1e1df59989125faa773f6f.jpg\",excerpt:\"A closer look at the development, staging, and production environments of the software development cycle.\",tags:[\"platform-engineering\",\"product\"],ctaButton:\"Try Release for Free\"};function h(t){let e=Object.assign({p:\"p\",ul:\"ul\",li:\"li\",h3:\"h3\",a:\"a\",span:\"span\",strong:\"strong\",img:\"img\",em:\"em\"},t.components);return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(e.p,{children:\"The lines between development, staging, and production environments are often blurred. The distinctions may vary depending on many factors, including:\"}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsx)(e.li,{children:\"the scale of the organization,\"}),`\n`,(0,n.jsx)(e.li,{children:\"the codebase, or\"}),`\n`,(0,n.jsx)(e.li,{children:\"whether you're viewing the environment from a product, unit testing, or security standpoint.\"}),`\n`]}),`\n`,(0,n.jsx)(e.p,{children:\"In this post, I use interviews with fellow developers to understand each environment's purpose and how it's distinct from the others. It's particularly challenging to differentiate between the development and staging environments, and some organizations forgo the staging environment altogether. Let\\u2019s find out why, and when it makes sense.\"}),`\n`,(0,n.jsxs)(e.h3,{id:\"what-is-a-development-environment\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#what-is-a-development-environment\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.strong,{children:\"What Is a Development Environment?\"})]}),`\n`,(0,n.jsxs)(e.p,{children:[\"Generally, the development environment is the first environment developers use to check if all code changes work well with each other. It's known as a \\u201Csandbox\\u201D for developers to work in. Examples of commonly used integrated development environments (IDEs) are \",(0,n.jsx)(e.a,{href:\"https://code.visualstudio.com/\",children:\"Visual Studio Code\"}),\", \",(0,n.jsx)(e.a,{href:\"https://www.eclipse.org/ide/\",children:\"Eclipse,\"}),\" \",(0,n.jsx)(e.a,{href:\"https://www.jetbrains.com/\",children:\"JetBrains tools\"}),\", and many others. Note that historically development environments were based on a developer's laptop\\u2014a local machine, but with the emergence of cloud, on-demand computing and \",(0,n.jsx)(e.a,{href:\"http://releasehub.com/ephemeral-environments\",children:\"ephemeral environments\"}),\", those environments are now being deployed in the cloud.\"]}),`\n`,(0,n.jsx)(e.p,{children:\"IDE is where developers\\u2019 workflow with code takes place, reloading and debugging aids are enabled here. Also, this environment is where developers can make necessary code changes. In the IDE, approved code can merge with code from other developers working on the same project.\"}),`\n`,(0,n.jsx)(e.p,{children:\"Developers commonly use this space to experiment and receive feedback on what improvements they can make to their work. Consequently, this environment is the most unstable. It's also the most susceptible to bugs and potentially broken code. But, on the upside, in allowing mistakes to happen, this is the most conducive environment to learn collaboratively and create a standardized process. \\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:\"Besides the most commonly known local machine, there are virtual and cloud-based development environments. Your team might use the virtual and cloud-based environments mainly depending on whether multiple platforms and machines are needed to effectively test and run the code they are writing.\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"Development environments historically only include a small subset of the entire application and often would lack elements like security, 3rd party APIs and cloud native services. Those would typically be introduced later in the development process and tested in staging. The result, however, turns into frequent rollbacks and bottlenecks in staging. To enable better code quality in development and more frequent release cycles, companies like \",(0,n.jsx)(e.a,{href:\"https://release.com/\",children:\"Release\"}),\" came up with \",(0,n.jsx)(e.a,{href:\"http://release.com/ephemeral-environments\",children:\"ephemeral environments\"}),\", a production-like replica that allows developers to properly test their code (i.e shift-left) and isolate bugs to a single branch, while ensuring a smooth merge to staging and production.\"]}),`\n`,(0,n.jsxs)(e.h3,{id:\"what-is-a-staging-environment\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#what-is-a-staging-environment\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"What Is a Staging Environment?\"]}),`\n`,(0,n.jsxs)(e.p,{children:[(0,n.jsx)(e.a,{href:\"https://release.com/staging-environments\",children:\"Staging environment\"}),\" is the environment where your code is 'staged' prior to being run in front of users so you can ensure it works as designed. The staging environment should mirror production as much as possible. It reflects a production environment not yet exposed to clients, customers, and the general public.\"]}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/7889fa49b5c1fd3e80dd96dda8886942.png\",alt:\"\"})}),`\n`,(0,n.jsxs)(e.p,{children:[\"This environment is primarily used for system integration testing (SIT) and in-depth manual testing conducted before the client receives code changes. Developers also \",(0,n.jsx)(e.a,{href:\"https://www.unitrends.com/blog/development-test-environments\",children:\"perform\"}),\" quality assurance (QA), security testing, chaos testing, alpha testing, beta testing, and end-to-end (E2E) testing in this environment.\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"Additionally, \",(0,n.jsx)(e.a,{href:\"https://release.com/user-acceptance-testing-with-ephemeral-environments\",children:\"User acceptance testing (UAT)\"}),\" often happens here. In UAT, users can test changes they requested before the new code goes to a production environment.\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"How you carry out testing in the staging environment can depend on what programming language you're using. For example, Ruby on Rails doesn't have a mode for staging. Rails developers switch modes to a test environment that they use to run testing tools and debug failures. Technically, the \",(0,n.jsx)(e.a,{href:\"https://guides.rubyonrails.org/configuring.html\",children:\"Rails Guide\"}),\" delves into how to customize configurations and initialization on applications.\"]}),`\n`,(0,n.jsxs)(e.h3,{id:\"development-vs-staging-environments\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#development-vs-staging-environments\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.strong,{children:\"Development vs. Staging Environments\"})]}),`\n`,(0,n.jsx)(e.p,{children:\"So, now that you know what development and staging environments are, you're probably wondering if you need both. Ultimately, the answer depends on the size of your organization, appetite for risk and speed of change, and your position on making a tradeoff between slowing down the process for quality and testing versus launching new features quickly.\"}),`\n`,(0,n.jsx)(e.p,{children:\"Sometimes smaller companies start out with fewer environments. One developer shared, \\u201CYou just end up with multiple environments as the organization scales up.\\u201D \\xA0\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"In some cases organizations with fewer users don't have staging environments. As another developer elaborated: \",(0,n.jsx)(e.em,{children:\"\\u201CInstead, we can deploy in a way that 1% of the traffic will go to each one branch and main branch. Then, we can check the monitoring to see if there are differences between the two. When we are certain that at the most we will affect 1% of traffic and everything is fine, we will then proceed with merging the two branches. I think it would be ideal if the continuous integration (CI) and continuous deployment (CD) process were to set up that 1%, then we could verify the results. This is the same as I have seen for verifying front-end changes in continuous integration.\\u201D\"})]}),`\n`,(0,n.jsxs)(e.h3,{id:\"is-a-staging-environment-necessary\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#is-a-staging-environment-necessary\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.strong,{children:\"Is a Staging Environment Necessary?\"})]}),`\n`,(0,n.jsx)(e.p,{children:\"Deploying to staging is safe, because it will not affect users, but is not necessarily effective because you might not test all the features or combinations that end users will be using. The general solution to this problem is to deploy to production as quickly as you can but only enable or test subsets of new features with flags or canary testing. This way you are only risking challenges for a small subset of users, and are able to see the application perform with live traffic in the production environment.\"}),`\n`,(0,n.jsx)(e.p,{children:'Developers say they like to see how real traffic works through the codebase and compare this technique to feature flagging. This may eliminate the need for a beta environment. This results in the concept of \"staging\" not being a distinct environment.'}),`\n`,(0,n.jsxs)(e.p,{children:[\"However, developers agree that it's useful to have a separate beta domain to make significant changes. According to \",(0,n.jsx)(e.a,{href:\"https://www.atlassian.com/continuous-delivery/principles/feature-flags\",children:\"Atlassian CI/CD\"}),\", feature flagging allows developers to turn functionality on and off during runtime. That way, you don't need to deploy code at every update.\"]}),`\n`,(0,n.jsxs)(e.h3,{id:\"what-is-a-production-environment\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#what-is-a-production-environment\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.strong,{children:\"What Is a Production Environment?\"})]}),`\n`,(0,n.jsx)(e.p,{children:\"The production environment is the live site complete with performance optimizations. The codebase must be secure, performant, stable, and able to sustain heavy traffic as the client, customers, and public use it. \\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:\"There is a common misconception that production is more important than development or staging. Actually, the reverse could be true: development environments could be so critical to the business that they cannot tolerate any downtime at all but production can tolerate some downtime.\"}),`\n`,(0,n.jsx)(e.p,{children:\"As an example at Truecar and in most other companies I worked at, the website could be broken for some amount of time as long as it came back up relatively quickly. However, if development was down for more than an hour, you could be looking at losing an entire day of developer features for the whole company!\"}),`\n`,(0,n.jsx)(e.p,{children:\"Regardless of your setup, you should treat production with care, and restrict who updates the production code. Ideally, you won't be building new versions of the codebase for the production environment; it's better to deploy the same builds to the staging environment.\"}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/436b6ae884f81c86dc705a7163ab8dfa.png\",alt:\"\"})}),`\n`,(0,n.jsx)(e.p,{children:\"At this point of the software development lifecycle, the code shouldn't have any bugs or require fixes. To avoid a poor user experience, you should consider it the final product.\"}),`\n`,(0,n.jsx)(e.p,{children:\"However, you can make urgent fixes in the production environment if needed. In doing so, you can consistently improve upon quality control for product releases, making it easier to keep tabs on new product updates.\"}),`\n`,(0,n.jsxs)(e.h3,{id:\"conclusion\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#conclusion\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.strong,{children:\"Conclusion\"})]}),`\n`,(0,n.jsx)(e.p,{children:\"Although the development, staging, and production environments converge, they have their own significance in the larger software development lifecycle. The significance of each environment depends on the organization running the system.\"}),`\n`,(0,n.jsx)(e.p,{children:\"The way a company treats and leverages these environments today differs wildly depending on the organization and its DevOps practices and policies. Sometimes teams within the same organization use these environments in different ways and have different philosophies of what they mean, and how critical they are to the company\\u2019s mission.\"}),`\n`,(0,n.jsx)(e.p,{children:'From my conversations with individuals who play different roles in the tech industry, I can say the overall development culture is shifting progressively toward promoting new code to all these environments as soon as possible. One developer expressed, \"The idea is that even the smallest code change gets released to production in a matter of minutes, not months.\"'}),`\n`,(0,n.jsx)(e.p,{children:\"With that in mind, the common goal is that the folks responsible for the software development life cycle want more efficient environments for producing the highest quality codebases. These people continuously strive to find new methods to make that process easier.\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"For a better understanding of what environments are and to be inspired about optimizing them, read more about \",(0,n.jsx)(e.a,{href:\"https://release.com/staging-environments\",children:\"staging environments\"}),\", \",(0,n.jsx)(e.a,{href:\"https://release.com/ephemeral-environments\",children:\"ephemeral environments\"}),\", and \",(0,n.jsx)(e.a,{href:\"https://release.com/user-acceptance-testing-with-ephemeral-environments\",children:\"UAT\"}),\" with Release ephemeral environments.\"]}),`\n`,(0,n.jsx)(e.p,{children:\"\\u200D\"})]})}function k(t={}){let{wrapper:e}=t.components||{};return e?(0,n.jsx)(e,Object.assign({},t,{children:(0,n.jsx)(h,t)})):h(t)}var T=k;return y(I);})();\n;return Component;"
        },
        "_id": "blog/posts/development-vs-staging-vs-production-whats-the-difference.mdx",
        "_raw": {
          "sourceFilePath": "blog/posts/development-vs-staging-vs-production-whats-the-difference.mdx",
          "sourceFileName": "development-vs-staging-vs-production-whats-the-difference.mdx",
          "sourceFileDir": "blog/posts",
          "contentType": "mdx",
          "flattenedPath": "blog/posts/development-vs-staging-vs-production-whats-the-difference"
        },
        "type": "BlogPost",
        "computedSlug": "development-vs-staging-vs-production-whats-the-difference"
      },
      "documentHash": "1739393595017",
      "hasWarnings": false,
      "documentTypeName": "BlogPost"
    },
    "blog/posts/devops-metrics-that-actually-matter.mdx": {
      "document": {
        "title": "DevOps Metrics That Actually Matter",
        "summary": "In this post, we'll take a look at DevOps metrics and cover some of the most important metrics.",
        "publishDate": "Wed Jan 26 2022 20:34:28 GMT+0000 (Coordinated Universal Time)",
        "author": "omkar-hiremath",
        "readingTime": 6,
        "categories": [
          "platform-engineering"
        ],
        "mainImage": "/blog-images/478fb21daa42c61be39106d5d88894cc.jpg",
        "imageAlt": "Devops metrics that matter",
        "showCTA": true,
        "ctaCopy": "Automate environment setup for accurate DevOps metrics tracking with Release's on-demand environments. Accelerate DevOps transformation today.",
        "ctaLink": "https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=devops-metrics-that-actually-matter",
        "relatedPosts": [
          ""
        ],
        "ogImage": "/blog-images/478fb21daa42c61be39106d5d88894cc.jpg",
        "excerpt": "In this post, we'll take a look at DevOps metrics and cover some of the most important metrics.",
        "tags": [
          "platform-engineering"
        ],
        "ctaButton": "Try Release for Free",
        "body": {
          "raw": "\nDevOps is one of the greatest cultural shifts the IT industry has ever had. It's a set of practices that brings together the development and operations teams to deliver high-quality products and services in a more efficient, faster way. \n\nA large number of organizations have shifted to the DevOps culture because they realize the benefits of DevOpss. But getting the best out of DevOps isn't as easy as installing software and getting results. This is because DevOps is not just a tool but also a mindset. \n\n### Measuring Change\n\nOnce organizations start practicing DevOps, the next step is to assess how this shift is performing. It's very difficult to know the amount of transformation a change has brought without measuring it. Therefore, it's important to measure the change, and that's where metrics come in. \n\nIn this post, we'll focus on various DevOps metrics, how they help measure DevOps transformation, and how tools can help. But before getting to that, let's try to understand why it's difficult to measure the outcomes of a DevOps team. \n\n![DevOps Metrics Pull quote](/blog-images/ee50945eb94b4e56996885017ca01738.png)\n\n### Why Can It Be Hard to Measure Outcomes of DevOps?\n\nTo make this easy to understand, I'll first give an example of something that's easy to measure and then compare it with measuring a DevOps outcome. \n\nLet's say you want to measure your return on investment in the stock market. This is straightforward. You know the price you bought a stock for, and you know the price you sold it for. So, it's easy to calculate your ROI. \n\nNow, let's step it up a bit. How would you measure your success in the past year? \n\nClearly, this is a bit more difficult than the previous example. First, there are multiple factors here. Success means different things to different people. It can involve salary, promotions, appreciation, revenue, and so on. Secondly, not all of these factors are quantitative. \n\nDevOps is even more complicated than the second example. There are a great number of factors and variables in the organization to measure the ROI or outcome. Also, most of these factors aren't quantitative. That's why it can be hard to measure the outcomes of a DevOps team. \n\nNevertheless, you can still use some metrics to measure your DevOps success. And to make things easier, you can also use some [tools](https://releasehub.com/). \n**DevOps Metrics That Actually Matter**\n\nDepending on what changes you've made and what you want to measure, you can select various metrics for your use case. But for this post, we'll stick to the ones that are universal. We're going to refer to [DORA's State of the DevOps Report](https://services.google.com/fh/files/misc/state-of-devops-2021.pdf) and the [SPACE framework](https://queue.acm.org/detail.cfm?id=3454124) for these metrics. \n\n### DORA's State of the DevOps Report\n\nDORA's State of the DevOps Report is the result of years of research. It addresses capabilities and practices that drive software delivery, operational performance, and organizational performance. This report talks about four major metrics that you can use to measure DevOps outcomes. \n\n#### Metric 1: Deployment Frequency\n\nOne of the main pillars of DevOps is [continuous delivery](https://continuousdelivery.com/). And deployment frequency is a direct measure of continuous delivery. \n\nDeployment frequency means how often code deploys. Code deployment can be bug fixes, new features, updates for performance, configuration changes, and so on. Measuring deployment frequency helps you understand how smooth the workflow of your team is. Lower deployment frequency or frequent delays are indicators of problems in the workflow. By measuring this metric, you can improve the process to ensure a smoother workflow. \n\nAs a side note, I found that on-demand readily available [ephemeral environments](http://releasehub.com/ephemeral-environments) that replicate production as key to improving release frequency. The code quality the developer produces is higher and the likelihood that it will require rollback during the release process is lower \n\n#### Metric 2: Lead Time for Change\n\nLead time for change means how long the team takes to get the committed code to run in production. This metric is a measure of the performance, productivity and skills of the team. High lead times can occur when the change is too large, but it can also be an indication of performance.\n\nAccording to DORA's research, low performers reported lead time for change to be more than six months. In contrast, elite performers reported it to be less than an hour! So, this metric can help identify a lack of skills and experience. It makes sense to use this method to prioritize training and skill improvement. \n\nMore on how to improve developers productivity [here](https://releasehub.com/blog/increase-developer-velocity-by-removing-environment-bottlenecks).\n\n#### Metric 3: Change Failure Rate\n\nChange failure rate is the percentage of deployments to production that have resulted in degraded services. Degraded can mean bugs, outages, performance problems, and so on. Change failure rate is a metric that represents the efficiency of the deployment process. A high value of this metric indicates that there's a serious problem in the deployment process, and it needs immediate attention. \n\n#### Metric 4: Mean Time to Restore Service\n\nMean time to restore service is the average time a team takes to recover from a production failure. This metric helps you understand how efficient your team is in recovering from an incident. \n\nThere are a lot of factors involved in recovering from an incident, from the point of detecting a failure to troubleshooting it and finding a fix or rolling back. Use this to determine the efficiency, skills, and stability of your DevOps team. \n\nSo there you have it: the four main metrics used to measure the performance and efficiency of DevOps teams. Let's look at this process from another angle, though. \n\n### SPACE Framework\n\nThe SPACE framework focuses on the [productivity](https://releasehub.com/blog/improving-developer-productivity-with-ephemeral-environments) of teams. SPACE stands for satisfaction, performance, activity, communication and collaboration, and efficiency. Let's look at these factors one by one. \n\n#### SPACE Metric 1: Satisfaction\n\nSatisfaction is the value teams get from their work. If a person doesn't like what they're doing, then it's obvious that their productivity will drop down. This metric helps you understand the alignment of an employee's interest with the plan of the organization or manager for that employee. \n\n#### SPACE Metric 2: Performance\n\nThis metric is straightforward and helps measure how well a team is functioning. The SPACE framework suggests measuring performance by outcomes rather than by output of tasks. The main reason for this is that a team's task is difficult to score as it's a combination of quality and quantity. \n\n#### SPACE Metric 3: Activity\n\nActivity is the count of actions a team has performed or the number of outputs a team has produced. You can use this metric to measure the productivity and efficiency of a team to a certain extent. (Why only to a certain extent? Well, the tasks of a DevOps team are difficult to quantify.) \n\n#### SPACE Metric 4: Communication and Collaboration\n\nFor an organization to function smoothly and produce high-quality outputs, it's important for teams and team members to communicate and collaborate. This is especially vital for DevOps teams because different specialists work together. Teams with good understanding and rapport are more productive. And this metric helps you understand how easy (or difficult) it is for teams and team members to work with one another. \n\n#### SPACE Metric 5: Efficiency\n\nEfficiency is the measure of completing a task with minimal delays. This metric is closely associated with productivity. Minimal interruptions and delays help maintain better flow. As a result, productivity also increases. \n\nIn addition to the metrics I've discussed in this article, there are many others that help you measure various tasks and processes. But the ones I've listed are enough to give you a high-level general understanding of DevOps outcomes. \n\n### The More Metrics You Use, the Bigger the Picture\n\nYou can measure and derive most DevOps metrics individually. But is it OK to also look at them independently and together and then draw conclusions? \n\nEach metric provides valuable information but not complete information. So to get the whole story, you need to make these metrics work together. \n\nFor example, a high value of deployment frequency is a good thing for a team. But if the same team has the highest failure rate, then that changes the overall performance rating of the team. Therefore, you must use all relevant metrics to get the bigger picture. \n\n![DevOps Metrics Pull quote](/blog-images/ee5893e351bccf0af855de69f25d116f.png)\n\n### What Does It All Mean?\n\nA shift to DevOps is something a lot of organizations have been very excited about. And DevOps metrics are a way to see and interpret results. \n\nMeasuring these metrics and correlating them isn't easy. So you might want to consider using tools to collect these metrics, process them, and also visualize them for you. \n\nThere are a lot of tools that can help you not only measure the metrics but also improve them. For example, [Release](http://releasehub.com/company) allows you to set up production-like environments, including the right dataset, cloud native services, security policies, that are automatically  available with each PR and automatically tear down when the PR is merged.. You can learn more about Release's [use cases](http://release.com/use-cases) or check out the company's [blog](http://release.com/blog). \n\nWe've discussed some of the most important metrics that almost every organization can use. And you can try out even more metrics to measure specific things as per your use case. Once you know where you're at with the help of these metrics, you can plan a better future. \n\n#### Additional Resources\n\n- [Increase Developer Velocity by Removing Environment Bottlenecks](https://release.com/blog/increase-developer-velocity-by-removing-environment-bottlenecks)\n- [What is an Ephemeral Environment?](https://release.com/ephemeral-environments)\n",
          "code": "var Component=(()=>{var m=Object.create;var i=Object.defineProperty;var d=Object.getOwnPropertyDescriptor;var u=Object.getOwnPropertyNames;var p=Object.getPrototypeOf,f=Object.prototype.hasOwnProperty;var g=(a,e)=>()=>(e||a((e={exports:{}}).exports,e),e.exports),y=(a,e)=>{for(var o in e)i(a,o,{get:e[o],enumerable:!0})},s=(a,e,o,r)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let n of u(e))!f.call(a,n)&&n!==o&&i(a,n,{get:()=>e[n],enumerable:!(r=d(e,n))||r.enumerable});return a};var v=(a,e,o)=>(o=a!=null?m(p(a)):{},s(e||!a||!a.__esModule?i(o,\"default\",{value:a,enumerable:!0}):o,a)),b=a=>s(i({},\"__esModule\",{value:!0}),a);var h=g((N,c)=>{c.exports=_jsx_runtime});var O={};y(O,{default:()=>D,frontmatter:()=>w});var t=v(h()),w={title:\"DevOps Metrics That Actually Matter\",summary:\"In this post, we'll take a look at DevOps metrics and cover some of the most important metrics.\",publishDate:\"Wed Jan 26 2022 20:34:28 GMT+0000 (Coordinated Universal Time)\",author:\"omkar-hiremath\",readingTime:6,categories:[\"platform-engineering\"],mainImage:\"/blog-images/478fb21daa42c61be39106d5d88894cc.jpg\",imageAlt:\"Devops metrics that matter\",showCTA:!0,ctaCopy:\"Automate environment setup for accurate DevOps metrics tracking with Release's on-demand environments. Accelerate DevOps transformation today.\",ctaLink:\"https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=devops-metrics-that-actually-matter\",relatedPosts:[\"\"],ogImage:\"/blog-images/478fb21daa42c61be39106d5d88894cc.jpg\",excerpt:\"In this post, we'll take a look at DevOps metrics and cover some of the most important metrics.\",tags:[\"platform-engineering\"],ctaButton:\"Try Release for Free\"};function l(a){let e=Object.assign({p:\"p\",h3:\"h3\",a:\"a\",span:\"span\",img:\"img\",strong:\"strong\",h4:\"h4\",ul:\"ul\",li:\"li\"},a.components);return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(e.p,{children:\"DevOps is one of the greatest cultural shifts the IT industry has ever had. It's a set of practices that brings together the development and operations teams to deliver high-quality products and services in a more efficient, faster way.\\xA0\"}),`\n`,(0,t.jsx)(e.p,{children:\"A large number of organizations have shifted to the DevOps culture because they realize the benefits of DevOpss. But getting the best out of DevOps isn't as easy as installing software and getting results. This is because DevOps is not just a tool but also a mindset.\\xA0\"}),`\n`,(0,t.jsxs)(e.h3,{id:\"measuring-change\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#measuring-change\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Measuring Change\"]}),`\n`,(0,t.jsx)(e.p,{children:\"Once organizations start practicing DevOps, the next step is to assess how this shift is performing. It's very difficult to know the amount of transformation a change has brought without measuring it. Therefore, it's important to measure the change, and that's where metrics come in.\\xA0\"}),`\n`,(0,t.jsx)(e.p,{children:\"In this post, we'll focus on various DevOps metrics, how they help measure DevOps transformation, and how tools can help. But before getting to that, let's try to understand why it's difficult to measure the outcomes of a DevOps team.\\xA0\"}),`\n`,(0,t.jsx)(e.p,{children:(0,t.jsx)(e.img,{src:\"/blog-images/ee50945eb94b4e56996885017ca01738.png\",alt:\"DevOps Metrics Pull quote\"})}),`\n`,(0,t.jsxs)(e.h3,{id:\"why-can-it-be-hard-to-measure-outcomes-of-devops\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#why-can-it-be-hard-to-measure-outcomes-of-devops\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Why Can It Be Hard to Measure Outcomes of DevOps?\"]}),`\n`,(0,t.jsx)(e.p,{children:\"To make this easy to understand, I'll first give an example of something that's easy to measure and then compare it with measuring a DevOps outcome.\\xA0\"}),`\n`,(0,t.jsx)(e.p,{children:\"Let's say you want to measure your return on investment in the stock market. This is straightforward. You know the price you bought a stock for, and you know the price you sold it for. So, it's easy to calculate your ROI.\\xA0\"}),`\n`,(0,t.jsx)(e.p,{children:\"Now, let's step it up a bit. How would you measure your success in the past year?\\xA0\"}),`\n`,(0,t.jsx)(e.p,{children:\"Clearly, this is a bit more difficult than the previous example. First, there are multiple factors here. Success means different things to different people. It can involve salary, promotions, appreciation, revenue, and so on. Secondly, not all of these factors are quantitative.\\xA0\"}),`\n`,(0,t.jsx)(e.p,{children:\"DevOps is even more complicated than the second example. There are a great number of factors and variables in the organization to measure the ROI or outcome. Also, most of these factors aren't quantitative. That's why it can be hard to measure the outcomes of a DevOps team.\\xA0\"}),`\n`,(0,t.jsxs)(e.p,{children:[\"Nevertheless, you can still use some metrics to measure your DevOps success. And to make things easier, you can also use some \",(0,t.jsx)(e.a,{href:\"https://releasehub.com/\",children:\"tools\"}),`.\\xA0\n`,(0,t.jsx)(e.strong,{children:\"DevOps Metrics That Actually Matter\"})]}),`\n`,(0,t.jsxs)(e.p,{children:[\"Depending on what changes you've made and what you want to measure, you can select various metrics for your use case. But for this post, we'll stick to the ones that are universal. We're going to refer to \",(0,t.jsx)(e.a,{href:\"https://services.google.com/fh/files/misc/state-of-devops-2021.pdf\",children:\"DORA's State of the DevOps Report\"}),\" and the \",(0,t.jsx)(e.a,{href:\"https://queue.acm.org/detail.cfm?id=3454124\",children:\"SPACE framework\"}),\" for these metrics.\\xA0\"]}),`\n`,(0,t.jsxs)(e.h3,{id:\"doras-state-of-the-devops-report\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#doras-state-of-the-devops-report\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"DORA's State of the DevOps Report\"]}),`\n`,(0,t.jsx)(e.p,{children:\"DORA's State of the DevOps Report is the result of years of research. It addresses capabilities and practices that drive software delivery, operational performance, and organizational performance. This report talks about four major metrics that you can use to measure DevOps outcomes.\\xA0\"}),`\n`,(0,t.jsxs)(e.h4,{id:\"metric-1-deployment-frequency\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#metric-1-deployment-frequency\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Metric 1: Deployment Frequency\"]}),`\n`,(0,t.jsxs)(e.p,{children:[\"One of the main pillars of DevOps is \",(0,t.jsx)(e.a,{href:\"https://continuousdelivery.com/\",children:\"continuous delivery\"}),\". And deployment frequency is a direct measure of continuous delivery.\\xA0\"]}),`\n`,(0,t.jsx)(e.p,{children:\"Deployment frequency means how often code deploys. Code deployment can be bug fixes, new features, updates for performance, configuration changes, and so on. Measuring deployment frequency helps you understand how smooth the workflow of your team is. Lower deployment frequency or frequent delays are indicators of problems in the workflow. By measuring this metric, you can improve the process to ensure a smoother workflow.\\xA0\"}),`\n`,(0,t.jsxs)(e.p,{children:[\"As a side note, I found that on-demand readily available \",(0,t.jsx)(e.a,{href:\"http://releasehub.com/ephemeral-environments\",children:\"ephemeral environments\"}),\" that replicate production as key to improving release frequency. The code quality the developer produces is higher and the likelihood that it will require rollback during the release process is lower\\xA0\"]}),`\n`,(0,t.jsxs)(e.h4,{id:\"metric-2-lead-time-for-change\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#metric-2-lead-time-for-change\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Metric 2: Lead Time for Change\"]}),`\n`,(0,t.jsx)(e.p,{children:\"Lead time for change means how long the team takes to get the committed code to run in production. This metric is a measure of the performance, productivity and skills of the team. High lead times can occur when the change is too large, but it can also be an indication of performance.\"}),`\n`,(0,t.jsx)(e.p,{children:\"According to DORA's research, low performers reported lead time for change to be more than six months. In contrast, elite performers reported it to be less than an hour! So, this metric can help identify a lack of skills and experience. It makes sense to use this method to prioritize training and skill improvement.\\xA0\"}),`\n`,(0,t.jsxs)(e.p,{children:[\"More on how to improve developers productivity \",(0,t.jsx)(e.a,{href:\"https://releasehub.com/blog/increase-developer-velocity-by-removing-environment-bottlenecks\",children:\"here\"}),\".\"]}),`\n`,(0,t.jsxs)(e.h4,{id:\"metric-3-change-failure-rate\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#metric-3-change-failure-rate\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Metric 3: Change Failure Rate\"]}),`\n`,(0,t.jsx)(e.p,{children:\"Change failure rate is the percentage of deployments to production that have resulted in degraded services. Degraded can mean bugs, outages, performance problems, and so on. Change failure rate is a metric that represents the efficiency of the deployment process. A high value of this metric indicates that there's a serious problem in the deployment process, and it needs immediate attention.\\xA0\"}),`\n`,(0,t.jsxs)(e.h4,{id:\"metric-4-mean-time-to-restore-service\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#metric-4-mean-time-to-restore-service\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Metric 4: Mean Time to Restore Service\"]}),`\n`,(0,t.jsx)(e.p,{children:\"Mean time to restore service is the average time a team takes to recover from a production failure. This metric helps you understand how efficient your team is in recovering from an incident.\\xA0\"}),`\n`,(0,t.jsx)(e.p,{children:\"There are a lot of factors involved in recovering from an incident, from the point of detecting a failure to troubleshooting it and finding a fix or rolling back. Use this to determine the efficiency, skills, and stability of your DevOps team.\\xA0\"}),`\n`,(0,t.jsx)(e.p,{children:\"So there you have it: the four main metrics used to measure the performance and efficiency of DevOps teams. Let's look at this process from another angle, though.\\xA0\"}),`\n`,(0,t.jsxs)(e.h3,{id:\"space-framework\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#space-framework\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"SPACE Framework\"]}),`\n`,(0,t.jsxs)(e.p,{children:[\"The SPACE framework focuses on the \",(0,t.jsx)(e.a,{href:\"https://releasehub.com/blog/improving-developer-productivity-with-ephemeral-environments\",children:\"productivity\"}),\" of teams. SPACE stands for satisfaction, performance, activity, communication and collaboration, and efficiency. Let's look at these factors one by one.\\xA0\"]}),`\n`,(0,t.jsxs)(e.h4,{id:\"space-metric-1-satisfaction\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#space-metric-1-satisfaction\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"SPACE Metric 1: Satisfaction\"]}),`\n`,(0,t.jsx)(e.p,{children:\"Satisfaction is the value teams get from their work. If a person doesn't like what they're doing, then it's obvious that their productivity will drop down. This metric helps you understand the alignment of an employee's interest with the plan of the organization or manager for that employee.\\xA0\"}),`\n`,(0,t.jsxs)(e.h4,{id:\"space-metric-2-performance\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#space-metric-2-performance\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"SPACE Metric 2: Performance\"]}),`\n`,(0,t.jsx)(e.p,{children:\"This metric is straightforward and helps measure how well a team is functioning. The SPACE framework suggests measuring performance by outcomes rather than by output of tasks. The main reason for this is that a team's task is difficult to score as it's a combination of quality and quantity.\\xA0\"}),`\n`,(0,t.jsxs)(e.h4,{id:\"space-metric-3-activity\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#space-metric-3-activity\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"SPACE Metric 3: Activity\"]}),`\n`,(0,t.jsx)(e.p,{children:\"Activity is the count of actions a team has performed or the number of outputs a team has produced. You can use this metric to measure the productivity and efficiency of a team to a certain extent. (Why only to a certain extent? Well, the tasks of a DevOps team are difficult to quantify.)\\xA0\"}),`\n`,(0,t.jsxs)(e.h4,{id:\"space-metric-4-communication-and-collaboration\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#space-metric-4-communication-and-collaboration\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"SPACE Metric 4: Communication and Collaboration\"]}),`\n`,(0,t.jsx)(e.p,{children:\"For an organization to function smoothly and produce high-quality outputs, it's important for teams and team members to communicate and collaborate. This is especially vital for DevOps teams because different specialists work together. Teams with good understanding and rapport are more productive. And this metric helps you understand how easy (or difficult) it is for teams and team members to work with one another.\\xA0\"}),`\n`,(0,t.jsxs)(e.h4,{id:\"space-metric-5-efficiency\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#space-metric-5-efficiency\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"SPACE Metric 5: Efficiency\"]}),`\n`,(0,t.jsx)(e.p,{children:\"Efficiency is the measure of completing a task with minimal delays. This metric is closely associated with productivity. Minimal interruptions and delays help maintain better flow. As a result, productivity also increases.\\xA0\"}),`\n`,(0,t.jsx)(e.p,{children:\"In addition to the metrics I've discussed in this article, there are many others that help you measure various tasks and processes. But the ones I've listed are enough to give you a high-level general understanding of DevOps outcomes.\\xA0\"}),`\n`,(0,t.jsxs)(e.h3,{id:\"the-more-metrics-you-use-the-bigger-the-picture\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#the-more-metrics-you-use-the-bigger-the-picture\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"The More Metrics You Use, the Bigger the Picture\"]}),`\n`,(0,t.jsx)(e.p,{children:\"You can measure and derive most DevOps metrics individually. But is it OK to also look at them independently and together and then draw conclusions?\\xA0\"}),`\n`,(0,t.jsx)(e.p,{children:\"Each metric provides valuable information but not complete information. So to get the whole story, you need to make these metrics work together.\\xA0\"}),`\n`,(0,t.jsx)(e.p,{children:\"For example, a high value of deployment frequency is a good thing for a team. But if the same team has the highest failure rate, then that changes the overall performance rating of the team. Therefore, you must use all relevant metrics to get the bigger picture.\\xA0\"}),`\n`,(0,t.jsx)(e.p,{children:(0,t.jsx)(e.img,{src:\"/blog-images/ee5893e351bccf0af855de69f25d116f.png\",alt:\"DevOps Metrics Pull quote\"})}),`\n`,(0,t.jsxs)(e.h3,{id:\"what-does-it-all-mean\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#what-does-it-all-mean\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"What Does It All Mean?\"]}),`\n`,(0,t.jsx)(e.p,{children:\"A shift to DevOps is something a lot of organizations have been very excited about. And DevOps metrics are a way to see and interpret results.\\xA0\"}),`\n`,(0,t.jsx)(e.p,{children:\"Measuring these metrics and correlating them isn't easy. So you might want to consider using tools to collect these metrics, process them, and also visualize them for you.\\xA0\"}),`\n`,(0,t.jsxs)(e.p,{children:[\"There are a lot of tools that can help you not only measure the metrics but also improve them. For example, \",(0,t.jsx)(e.a,{href:\"http://releasehub.com/company\",children:\"Release\"}),\" allows you to set up production-like environments, including the right dataset, cloud native services, security policies, that are automatically\\xA0 available with each PR and automatically tear down when the PR is merged.. You can learn more about Release's \",(0,t.jsx)(e.a,{href:\"http://release.com/use-cases\",children:\"use cases\"}),\" or check out the company's \",(0,t.jsx)(e.a,{href:\"http://release.com/blog\",children:\"blog\"}),\".\\xA0\"]}),`\n`,(0,t.jsx)(e.p,{children:\"We've discussed some of the most important metrics that almost every organization can use. And you can try out even more metrics to measure specific things as per your use case. Once you know where you're at with the help of these metrics, you can plan a better future.\\xA0\"}),`\n`,(0,t.jsxs)(e.h4,{id:\"additional-resources\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#additional-resources\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Additional Resources\"]}),`\n`,(0,t.jsxs)(e.ul,{children:[`\n`,(0,t.jsx)(e.li,{children:(0,t.jsx)(e.a,{href:\"https://release.com/blog/increase-developer-velocity-by-removing-environment-bottlenecks\",children:\"Increase Developer Velocity by Removing Environment Bottlenecks\"})}),`\n`,(0,t.jsx)(e.li,{children:(0,t.jsx)(e.a,{href:\"https://release.com/ephemeral-environments\",children:\"What is an Ephemeral Environment?\"})}),`\n`]})]})}function k(a={}){let{wrapper:e}=a.components||{};return e?(0,t.jsx)(e,Object.assign({},a,{children:(0,t.jsx)(l,a)})):l(a)}var D=k;return b(O);})();\n;return Component;"
        },
        "_id": "blog/posts/devops-metrics-that-actually-matter.mdx",
        "_raw": {
          "sourceFilePath": "blog/posts/devops-metrics-that-actually-matter.mdx",
          "sourceFileName": "devops-metrics-that-actually-matter.mdx",
          "sourceFileDir": "blog/posts",
          "contentType": "mdx",
          "flattenedPath": "blog/posts/devops-metrics-that-actually-matter"
        },
        "type": "BlogPost",
        "computedSlug": "devops-metrics-that-actually-matter"
      },
      "documentHash": "1739393595017",
      "hasWarnings": false,
      "documentTypeName": "BlogPost"
    },
    "blog/posts/docker-genai-stack-now-available-as-release-ai-template.mdx": {
      "document": {
        "title": "Docker GenAI Stack Now Available as Release.ai Template",
        "summary": "Learn how the Docker-Release collaboration simplifies AI development, from local setup to cloud production.",
        "publishDate": "Wed Aug 07 2024 04:08:45 GMT+0000 (Coordinated Universal Time)",
        "author": "tommy-mcclung",
        "readingTime": 5,
        "categories": [
          "ai",
          "platform-engineering"
        ],
        "mainImage": "/blog-images/ad4f0d9a6a9336170e2e4fc9145f03d9.jpg",
        "imageAlt": "Docker GenAI Stack Now Available as Release.ai Template",
        "showCTA": true,
        "ctaCopy": "Simplify AI deployment with Release.ai's managed environments. Accelerate innovation while eliminating infrastructure setup complexities.",
        "ctaLink": "https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=docker-genai-stack-now-available-as-release-ai-template",
        "relatedPosts": [
          "introducing-release-share-a-docker-desktop-extension"
        ],
        "ogImage": "/blog-images/ad4f0d9a6a9336170e2e4fc9145f03d9.jpg",
        "excerpt": "Learn how the Docker-Release collaboration simplifies AI development, from local setup to cloud production.",
        "tags": [
          "ai",
          "platform-engineering"
        ],
        "ctaButton": "Try Release for Free",
        "body": {
          "raw": "\nRelease.ai is an orchestration and infrastructure management platform for custom AI applications. We handle the complex backend work, allowing developers and data scientists to focus on innovation. Our self-service platform provides powerful building blocks that simplify AI development and deployment, enabling teams to create advanced applications more efficiently.\n\nWe are excited to announce the addition of Docker's GenAI Stack to our library of pre-configured templates, simplifying AI application development and deployment. The Docker GenAI Stack serves as a comprehensive template for building Retrieval-Augmented Generation (RAG) applications, allowing developers to quickly set up and experiment with AI-powered systems on their local machines. This collaboration combines Docker's containerization expertise with Release.ai's cloud deployment capabilities, enabling a seamless transition from local development to cloud-based production. With Release.ai, deploying your Docker GenAI Stack to the cloud becomes as straightforward as running it locally, offering developers a streamlined solution for building and scaling AI applications without the complexity typically associated with cloud infrastructure management.\n\n\"By integrating Docker's GenAI Stack, we're helping developers spend more time on innovation and less on infrastructure setup.\" explains Tommy McClung, Release founder.\n\nKey benefits of this collaboration include:\n\n- **Simplified Deployment:** With just a few clicks, developers can create a fully functional GenAI environment, eliminating the need for manual configuration.\n- **Time Savings:** Early adopters report cutting setup time by up to 50%, allowing teams to focus on application development rather than infrastructure management.\n- **Standardization:** The template ensures consistency across different projects and teams, improving collaboration and reducing potential issues.\n- **Scalability:** Release.ai's managed Kubernetes infrastructure provides the ability to easily scale AI applications as needed.\n\nThere are several Docker GenAI Stack templates on Release.ai:\n\n- Ollama, LangChain, Neo4j, with Open Source Models (default llama3:8b)\n- Ollama, LangChain, Chroma, with Open Source Models (default llama3:8b)\n- Ollama, LangChain, Chroma, with OpenAI\n\nDocker and Release partnership represents a significant step forward in democratizing advanced AI technologies. Simplifying the deployment process opens up new possibilities for innovation across various industries. Looking ahead, we plan to expand the collaboration to integrate more AI-focused templates and explore ways to optimize AI workflows further.\n\nReady to accelerate your AI development?\n\n1\\. Sign up for a free Release.ai account at Release.ai/sign-up\n\n2\\. Choose a Docker Gen AI template from our library when creating your application\n\n3\\. Launch your AI environment and start building\n\nIn this [demo](https://www.youtube.com/watch?v=-OdWRxMX1iA), Docker GenAI Stack drives our documentation chatbot, showcasing the entire platform and demonstrating how to easily manage new data and changes using the RAG stack of your choice.\n\nOur sandbox comes with pre-set templates and free compute time to get you started. Don't miss this opportunity to transform your AI development process. Get started with Release.ai and Docker Gen AI today!\n",
          "code": "var Component=(()=>{var p=Object.create;var o=Object.defineProperty;var m=Object.getOwnPropertyDescriptor;var u=Object.getOwnPropertyNames;var h=Object.getPrototypeOf,g=Object.prototype.hasOwnProperty;var f=(a,e)=>()=>(e||a((e={exports:{}}).exports,e),e.exports),y=(a,e)=>{for(var t in e)o(a,t,{get:e[t],enumerable:!0})},l=(a,e,t,r)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let i of u(e))!g.call(a,i)&&i!==t&&o(a,i,{get:()=>e[i],enumerable:!(r=m(e,i))||r.enumerable});return a};var b=(a,e,t)=>(t=a!=null?p(h(a)):{},l(e||!a||!a.__esModule?o(t,\"default\",{value:a,enumerable:!0}):t,a)),w=a=>l(o({},\"__esModule\",{value:!0}),a);var c=f((R,s)=>{s.exports=_jsx_runtime});var I={};y(I,{default:()=>k,frontmatter:()=>v});var n=b(c()),v={title:\"Docker GenAI Stack Now Available as Release.ai Template\",summary:\"Learn how the Docker-Release collaboration simplifies AI development, from local setup to cloud production.\",publishDate:\"Wed Aug 07 2024 04:08:45 GMT+0000 (Coordinated Universal Time)\",author:\"tommy-mcclung\",readingTime:5,categories:[\"ai\",\"platform-engineering\"],mainImage:\"/blog-images/ad4f0d9a6a9336170e2e4fc9145f03d9.jpg\",imageAlt:\"Docker GenAI Stack Now Available as Release.ai Template\",showCTA:!0,ctaCopy:\"Simplify AI deployment with Release.ai's managed environments. Accelerate innovation while eliminating infrastructure setup complexities.\",ctaLink:\"https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=docker-genai-stack-now-available-as-release-ai-template\",relatedPosts:[\"introducing-release-share-a-docker-desktop-extension\"],ogImage:\"/blog-images/ad4f0d9a6a9336170e2e4fc9145f03d9.jpg\",excerpt:\"Learn how the Docker-Release collaboration simplifies AI development, from local setup to cloud production.\",tags:[\"ai\",\"platform-engineering\"],ctaButton:\"Try Release for Free\"};function d(a){let e=Object.assign({p:\"p\",ul:\"ul\",li:\"li\",strong:\"strong\",a:\"a\"},a.components);return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(e.p,{children:\"Release.ai is an orchestration and infrastructure management platform for custom AI applications. We handle the complex backend work, allowing developers and data scientists to focus on innovation. Our self-service platform provides powerful building blocks that simplify AI development and deployment, enabling teams to create advanced applications more efficiently.\"}),`\n`,(0,n.jsx)(e.p,{children:\"We are excited to announce the addition of Docker's GenAI Stack to our library of pre-configured templates, simplifying AI application development and deployment. The Docker GenAI Stack serves as a comprehensive template for building Retrieval-Augmented Generation (RAG) applications, allowing developers to quickly set up and experiment with AI-powered systems on their local machines. This collaboration combines Docker's containerization expertise with Release.ai's cloud deployment capabilities, enabling a seamless transition from local development to cloud-based production. With Release.ai, deploying your Docker GenAI Stack to the cloud becomes as straightforward as running it locally, offering developers a streamlined solution for building and scaling AI applications without the complexity typically associated with cloud infrastructure management.\"}),`\n`,(0,n.jsx)(e.p,{children:`\"By integrating Docker's GenAI Stack, we're helping developers spend more time on innovation and less on infrastructure setup.\" explains Tommy McClung, Release founder.`}),`\n`,(0,n.jsx)(e.p,{children:\"Key benefits of this collaboration include:\"}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"Simplified Deployment:\"}),\" With just a few clicks, developers can create a fully functional GenAI environment, eliminating the need for manual configuration.\"]}),`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"Time Savings:\"}),\" Early adopters report cutting setup time by up to 50%, allowing teams to focus on application development rather than infrastructure management.\"]}),`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"Standardization:\"}),\" The template ensures consistency across different projects and teams, improving collaboration and reducing potential issues.\"]}),`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"Scalability:\"}),\" Release.ai's managed Kubernetes infrastructure provides the ability to easily scale AI applications as needed.\"]}),`\n`]}),`\n`,(0,n.jsx)(e.p,{children:\"There are several Docker GenAI Stack templates on Release.ai:\"}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsx)(e.li,{children:\"Ollama, LangChain, Neo4j, with Open Source Models (default llama3:8b)\"}),`\n`,(0,n.jsx)(e.li,{children:\"Ollama, LangChain, Chroma, with Open Source Models (default llama3:8b)\"}),`\n`,(0,n.jsx)(e.li,{children:\"Ollama, LangChain, Chroma, with OpenAI\"}),`\n`]}),`\n`,(0,n.jsx)(e.p,{children:\"Docker and Release partnership represents a significant step forward in democratizing advanced AI technologies. Simplifying the deployment process opens up new possibilities for innovation across various industries. Looking ahead, we plan to expand the collaboration to integrate more AI-focused templates and explore ways to optimize AI workflows further.\"}),`\n`,(0,n.jsx)(e.p,{children:\"Ready to accelerate your AI development?\"}),`\n`,(0,n.jsx)(e.p,{children:\"1. Sign up for a free Release.ai account at Release.ai/sign-up\"}),`\n`,(0,n.jsx)(e.p,{children:\"2. Choose a Docker Gen AI template from our library when creating your application\"}),`\n`,(0,n.jsx)(e.p,{children:\"3. Launch your AI environment and start building\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"In this \",(0,n.jsx)(e.a,{href:\"https://www.youtube.com/watch?v=-OdWRxMX1iA\",children:\"demo\"}),\", Docker GenAI Stack drives our documentation chatbot, showcasing the entire platform and demonstrating how to easily manage new data and changes using the RAG stack of your choice.\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Our sandbox comes with pre-set templates and free compute time to get you started. Don't miss this opportunity to transform your AI development process. Get started with Release.ai and Docker Gen AI today!\"})]})}function A(a={}){let{wrapper:e}=a.components||{};return e?(0,n.jsx)(e,Object.assign({},a,{children:(0,n.jsx)(d,a)})):d(a)}var k=A;return w(I);})();\n;return Component;"
        },
        "_id": "blog/posts/docker-genai-stack-now-available-as-release-ai-template.mdx",
        "_raw": {
          "sourceFilePath": "blog/posts/docker-genai-stack-now-available-as-release-ai-template.mdx",
          "sourceFileName": "docker-genai-stack-now-available-as-release-ai-template.mdx",
          "sourceFileDir": "blog/posts",
          "contentType": "mdx",
          "flattenedPath": "blog/posts/docker-genai-stack-now-available-as-release-ai-template"
        },
        "type": "BlogPost",
        "computedSlug": "docker-genai-stack-now-available-as-release-ai-template"
      },
      "documentHash": "1739393595017",
      "hasWarnings": false,
      "documentTypeName": "BlogPost"
    },
    "blog/posts/dockercon-2023-takeaways-ai-security-and-devex-are-top-of-mind.mdx": {
      "document": {
        "title": "DockerCon 2023 Takeaways: AI, Security and DevEx are Top of Mind",
        "summary": "DockerCon showcased new tools in the Docker ecosystem and emphasized a move to local and cloud environments.",
        "publishDate": "Wed Oct 11 2023 02:42:04 GMT+0000 (Coordinated Universal Time)",
        "author": "ira-casteel",
        "readingTime": 6,
        "categories": [
          "ai",
          "events",
          "platform-engineering"
        ],
        "mainImage": "/blog-images/f82724b7a47addc9497ce33ccd4fa59c.jpg",
        "imageAlt": "DockerCoon 2023 Keynote October 5",
        "showCTA": true,
        "ctaCopy": "Simplify AI model testing with Release's on-demand environments for secure, collaborative development and seamless deployment workflows.",
        "ctaLink": "https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=dockercon-2023-takeaways-ai-security-and-devex-are-top-of-mind",
        "relatedPosts": [
          ""
        ],
        "ogImage": "/blog-images/f82724b7a47addc9497ce33ccd4fa59c.jpg",
        "excerpt": "DockerCon showcased new tools in the Docker ecosystem and emphasized a move to local and cloud environments.",
        "tags": [
          "ai",
          "events",
          "platform-engineering"
        ],
        "ctaButton": "Try Release for Free",
        "body": {
          "raw": "\nThis year DockerCon returned to an in-person format (along with the online option) and Release traveled to sunny LA to participate. We spent two days listening to talks, learning from peers and presenting our ideas and new products. As we digest the event, here are some standout moments and learnings we're keen to share.\n\n![](/blog-images/13b906b70d214ca3f28af18356bd120f.jpeg)\n\n### **Keynote Takeaways:**\n\nEach day started with a keynote. Led by the CEO Scott Johnston and CTO Justin Cormack, the talks featured Docker experts, customers and partners, and focused on improvements, accomplishments and new initiatives Docker and the wider community are working towards. The announcements included:\n\n[**Docker Scout**](https://www.docker.com/products/docker-scout/) **in GA:** A new tool for analyzing Docker images, highlighting vulnerabilities, and offering fixes, that seamlessly integrates with platforms like Docker Hub and GitHub Actions.\n**Udemy & Docker Collaboration:** A dedicated Docker learning path on Udemy, that includes course discounts and exclusive content.\n**Next Generation Docker Build:** Faster, cloud-assisted Docker image creation. Currently in public beta.\n**Docker Debug:** A tool providing a local debugging feel for remote containerized apps. Also in public beta.\n\n[**Docker AI**](https://www.docker.com/ai-early-access-program/): Context-specific, automated guidance for editing a Dockerfile or Docker Compose file, debugging local ‘docker build,’ or running a test locally. In early access.\n\nAll these new tools and initiatives underscore the idea of combining the best of local and cloud development environments, so that teams can collaboratively, quickly, and securely build, share, and run any app, anywhere.\n\n![](/blog-images/87705afc739ade1f37e4154fa553bdfa.jpeg)\n\n### **Live Session Lineup:**\n\nThis year's sessions were full of insights and learning. From advanced container orchestration techniques to nuanced deployment strategies, the breadth and depth of topics discussed were on-par with what you’d expect from a vibrant Docker community.\n\nFor the visually-inclined, [Aurélie Vache](https://www.dockercon.com/2023/speaker/895940/aur%C3%A9lie-vache) shared the [“Understanding Docker in a Visual Way”](https://www.dockercon.com/2023/session/1736615/understanding-docker-in-a-visual-way) session and later signed copies of her book. For the practical-minded, a Docker engineer shared [“15 Tips & Tricks to Improve Your Compose Experience”](https://www.dockercon.com/2023/session/1736584/15-tips-tricks-to-improve-your-compose-experience). And for those building an internal platform or surviving a version already created in their organization, a principal software engineer at Docker shared his observations on [“Things You Always Wanted to Know About Building Platforms”](https://www.dockercon.com/2023/session/1736606/things-you-always-wanted-to-know-about-building-platforms).\n\nRelease presented a lightning talk on shared environments and a recording can be found [here](https://vimeo.com/872652454?utm_source=blog&utm_medium=blog&utm_campaign=dockercon23) (spoiler alert: isolated on-demand environments win over sharing).  \n\n![](/blog-images/e10ea90a0ba21c35cdd749a6b726255a.jpeg)\n\nIf you missed any of the other sessions, stay tuned for the on-demand recordings available on the [DockerCon portal](https://www.dockercon.com/2023) soon.\n\n### **Sharing Release with the Docker community:**\n\nBesides sessions, and hallway talks, our team got a chance to showcase Release products in our own booth. The conversations ranged from curious inquiries to in-depth discussions about integration possibilities and specific projects teams are tackling. It was great to share what we built and explore how customers could use it to improve their workflows and DevEx. And collected actionable feedback that we can apply to our product roadmap.\n\nOn-demand environments are still a challenge for some teams. And many folks were impressed with the simplicity and power of the **Release platform**, and how effortlessly it provides isolated environments to every developer and every use case.\n\nIn time for DockerCon, we launched [**Release Share**](https://open.docker.com/extensions/marketplace?extensionId=releasecom/docker-extension), our Docker Desktop extension that lets you share your work in progress from your local laptop. Now, any running container can have a customizable URL that you can quickly share and use to test things with your team. The extension is free to use and you can try it [here](https://open.docker.com/extensions/marketplace?extensionId=releasecom/docker-extension).\n\nLastly, we had lots of interesting discussions and useful feedback on our **Release AI** offering, including some working notes from the Docker team. Check out what we built so far and see how you can talk to your infrastructure in plain English today. Get your free access at [release.ai](http://release.ai/)\n\n![](/blog-images/87c088c9d9e3a6883663db01e641dcfa.jpeg)\n\nAs with all conferences, there was a ton to learn, a ton to share and not enough time to do everything. So if you didn't get a chance to talk with us at DockerCon but want to know more about what we do, just send a message at regis\\_wilson@release.com. We're always ready to chat.\n",
          "code": "var Component=(()=>{var h=Object.create;var r=Object.defineProperty;var m=Object.getOwnPropertyDescriptor;var u=Object.getOwnPropertyNames;var p=Object.getPrototypeOf,g=Object.prototype.hasOwnProperty;var w=(o,e)=>()=>(e||o((e={exports:{}}).exports,e),e.exports),f=(o,e)=>{for(var a in e)r(o,a,{get:e[a],enumerable:!0})},i=(o,e,a,s)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let t of u(e))!g.call(o,t)&&t!==a&&r(o,t,{get:()=>e[t],enumerable:!(s=m(e,t))||s.enumerable});return o};var k=(o,e,a)=>(a=o!=null?h(p(o)):{},i(e||!o||!o.__esModule?r(a,\"default\",{value:o,enumerable:!0}):a,o)),y=o=>i(r({},\"__esModule\",{value:!0}),o);var d=w((C,c)=>{c.exports=_jsx_runtime});var x={};f(x,{default:()=>D,frontmatter:()=>b});var n=k(d()),b={title:\"DockerCon 2023 Takeaways: AI, Security and DevEx are Top of Mind\",summary:\"DockerCon showcased new tools in the Docker ecosystem and emphasized a move to local and cloud environments.\",publishDate:\"Wed Oct 11 2023 02:42:04 GMT+0000 (Coordinated Universal Time)\",author:\"ira-casteel\",readingTime:6,categories:[\"ai\",\"events\",\"platform-engineering\"],mainImage:\"/blog-images/f82724b7a47addc9497ce33ccd4fa59c.jpg\",imageAlt:\"DockerCoon 2023 Keynote October 5\",showCTA:!0,ctaCopy:\"Simplify AI model testing with Release's on-demand environments for secure, collaborative development and seamless deployment workflows.\",ctaLink:\"https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=dockercon-2023-takeaways-ai-security-and-devex-are-top-of-mind\",relatedPosts:[\"\"],ogImage:\"/blog-images/f82724b7a47addc9497ce33ccd4fa59c.jpg\",excerpt:\"DockerCon showcased new tools in the Docker ecosystem and emphasized a move to local and cloud environments.\",tags:[\"ai\",\"events\",\"platform-engineering\"],ctaButton:\"Try Release for Free\"};function l(o){let e=Object.assign({p:\"p\",img:\"img\",h3:\"h3\",a:\"a\",span:\"span\",strong:\"strong\"},o.components);return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(e.p,{children:\"This year DockerCon returned to an in-person format (along with the online option) and Release traveled to sunny LA to participate. We spent two days listening to talks, learning from peers and presenting our ideas and new products. As we digest the event, here are some standout moments and learnings we're keen to share.\"}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/13b906b70d214ca3f28af18356bd120f.jpeg\",alt:\"\"})}),`\n`,(0,n.jsxs)(e.h3,{id:\"keynote-takeaways\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#keynote-takeaways\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.strong,{children:\"Keynote Takeaways:\"})]}),`\n`,(0,n.jsx)(e.p,{children:\"Each day started with a keynote. Led by the CEO Scott Johnston and CTO Justin Cormack, the talks featured Docker experts, customers and partners, and focused on improvements, accomplishments and new initiatives Docker and the wider community are working towards. The announcements included:\"}),`\n`,(0,n.jsxs)(e.p,{children:[(0,n.jsx)(e.a,{href:\"https://www.docker.com/products/docker-scout/\",children:(0,n.jsx)(e.strong,{children:\"Docker Scout\"})}),\" \",(0,n.jsx)(e.strong,{children:\"in GA:\"}),` A new tool for analyzing Docker images, highlighting vulnerabilities, and offering fixes, that seamlessly integrates with platforms like Docker Hub and GitHub Actions.\n`,(0,n.jsx)(e.strong,{children:\"Udemy & Docker Collaboration:\"}),` A dedicated Docker learning path on Udemy, that includes course discounts and exclusive content.\n`,(0,n.jsx)(e.strong,{children:\"Next Generation Docker Build:\"}),` Faster, cloud-assisted Docker image creation. Currently in public beta.\n`,(0,n.jsx)(e.strong,{children:\"Docker Debug:\"}),\" A tool providing a local debugging feel for remote containerized apps. Also in public beta.\"]}),`\n`,(0,n.jsxs)(e.p,{children:[(0,n.jsx)(e.a,{href:\"https://www.docker.com/ai-early-access-program/\",children:(0,n.jsx)(e.strong,{children:\"Docker AI\"})}),\": Context-specific, automated guidance for editing a Dockerfile or Docker Compose file, debugging local \\u2018docker build,\\u2019 or running a test locally. In early access.\"]}),`\n`,(0,n.jsx)(e.p,{children:\"All these new tools and initiatives underscore the idea of combining the best of local and cloud development environments, so that teams can collaboratively, quickly, and securely build, share, and run any app, anywhere.\"}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/87705afc739ade1f37e4154fa553bdfa.jpeg\",alt:\"\"})}),`\n`,(0,n.jsxs)(e.h3,{id:\"live-session-lineup\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#live-session-lineup\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.strong,{children:\"Live Session Lineup:\"})]}),`\n`,(0,n.jsx)(e.p,{children:\"This year's sessions were full of insights and learning. From advanced container orchestration techniques to nuanced deployment strategies, the breadth and depth of topics discussed were on-par with what you\\u2019d expect from a vibrant Docker community.\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"For the visually-inclined, \",(0,n.jsx)(e.a,{href:\"https://www.dockercon.com/2023/speaker/895940/aur%C3%A9lie-vache\",children:\"Aur\\xE9lie Vache\"}),\" shared the \",(0,n.jsx)(e.a,{href:\"https://www.dockercon.com/2023/session/1736615/understanding-docker-in-a-visual-way\",children:\"\\u201CUnderstanding Docker in a Visual Way\\u201D\"}),\" session and later signed copies of her book. For the practical-minded, a Docker engineer shared \",(0,n.jsx)(e.a,{href:\"https://www.dockercon.com/2023/session/1736584/15-tips-tricks-to-improve-your-compose-experience\",children:\"\\u201C15 Tips & Tricks to Improve Your Compose Experience\\u201D\"}),\". And for those building an internal platform or surviving a version already created in their organization, a principal software engineer at Docker shared his observations on \",(0,n.jsx)(e.a,{href:\"https://www.dockercon.com/2023/session/1736606/things-you-always-wanted-to-know-about-building-platforms\",children:\"\\u201CThings You Always Wanted to Know About Building Platforms\\u201D\"}),\".\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"Release presented a lightning talk on shared environments and a recording can be found \",(0,n.jsx)(e.a,{href:\"https://vimeo.com/872652454?utm_source=blog&utm_medium=blog&utm_campaign=dockercon23\",children:\"here\"}),\" (spoiler alert: isolated on-demand environments win over sharing). \\xA0\"]}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/e10ea90a0ba21c35cdd749a6b726255a.jpeg\",alt:\"\"})}),`\n`,(0,n.jsxs)(e.p,{children:[\"If you missed any of the other sessions, stay tuned for the on-demand recordings available on the \",(0,n.jsx)(e.a,{href:\"https://www.dockercon.com/2023\",children:\"DockerCon portal\"}),\" soon.\"]}),`\n`,(0,n.jsxs)(e.h3,{id:\"sharing-release-with-the-docker-community\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#sharing-release-with-the-docker-community\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.strong,{children:\"Sharing Release with the Docker community:\"})]}),`\n`,(0,n.jsx)(e.p,{children:\"Besides sessions, and hallway talks, our team got a chance to showcase Release products in our own booth. The conversations ranged from curious inquiries to in-depth discussions about integration possibilities and specific projects teams are tackling. It was great to share what we built and explore how customers could use it to improve their workflows and DevEx. And collected actionable feedback that we can apply to our product roadmap.\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"On-demand environments are still a challenge for some teams. And many folks were impressed with the simplicity and power of the \",(0,n.jsx)(e.strong,{children:\"Release platform\"}),\", and how effortlessly it provides isolated environments to every developer and every use case.\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"In time for DockerCon, we launched \",(0,n.jsx)(e.a,{href:\"https://open.docker.com/extensions/marketplace?extensionId=releasecom/docker-extension\",children:(0,n.jsx)(e.strong,{children:\"Release Share\"})}),\", our Docker Desktop extension that lets you share your work in progress from your local laptop. Now, any running container can have a customizable URL that you can quickly share and use to test things with your team. The extension is free to use and you can try it \",(0,n.jsx)(e.a,{href:\"https://open.docker.com/extensions/marketplace?extensionId=releasecom/docker-extension\",children:\"here\"}),\".\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"Lastly, we had lots of interesting discussions and useful feedback on our \",(0,n.jsx)(e.strong,{children:\"Release AI\"}),\" offering, including some working notes from the Docker team. Check out what we built so far and see how you can talk to your infrastructure in plain English today. Get your free access at \",(0,n.jsx)(e.a,{href:\"http://release.ai/\",children:\"release.ai\"})]}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/87c088c9d9e3a6883663db01e641dcfa.jpeg\",alt:\"\"})}),`\n`,(0,n.jsxs)(e.p,{children:[\"As with all conferences, there was a ton to learn, a ton to share and not enough time to do everything. So if you didn't get a chance to talk with us at DockerCon but want to know more about what we do, just send a message at \",(0,n.jsx)(e.a,{href:\"mailto:regis_wilson@release.com\",children:\"regis_wilson@release.com\"}),\". We're always ready to chat.\"]})]})}function v(o={}){let{wrapper:e}=o.components||{};return e?(0,n.jsx)(e,Object.assign({},o,{children:(0,n.jsx)(l,o)})):l(o)}var D=v;return y(x);})();\n;return Component;"
        },
        "_id": "blog/posts/dockercon-2023-takeaways-ai-security-and-devex-are-top-of-mind.mdx",
        "_raw": {
          "sourceFilePath": "blog/posts/dockercon-2023-takeaways-ai-security-and-devex-are-top-of-mind.mdx",
          "sourceFileName": "dockercon-2023-takeaways-ai-security-and-devex-are-top-of-mind.mdx",
          "sourceFileDir": "blog/posts",
          "contentType": "mdx",
          "flattenedPath": "blog/posts/dockercon-2023-takeaways-ai-security-and-devex-are-top-of-mind"
        },
        "type": "BlogPost",
        "computedSlug": "dockercon-2023-takeaways-ai-security-and-devex-are-top-of-mind"
      },
      "documentHash": "1739393595018",
      "hasWarnings": false,
      "documentTypeName": "BlogPost"
    },
    "blog/posts/environment-as-a-service-eaas-a-comprehensive-guide.mdx": {
      "document": {
        "title": "Environment as a Service (EaaS): A Comprehensive Guide",
        "summary": "This is a complete guide to EaaS, including how it works, the benefits, and how to deploy it in your organization.",
        "publishDate": "Tue Jan 17 2023 10:00:34 GMT+0000 (Coordinated Universal Time)",
        "author": "",
        "readingTime": 6,
        "categories": [
          "platform-engineering",
          "product"
        ],
        "mainImage": "/blog-images/b9e9931ceeaedfd86a9c16fcec42a1e0.jpeg",
        "imageAlt": "Environment as a Service (EaaS): A Comprehensive Guide",
        "showCTA": true,
        "ctaCopy": "Automate environment provisioning with Release for faster software deployment and seamless collaboration. Say goodbye to manual setup headaches!",
        "ctaLink": "https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=environment-as-a-service-eaas-a-comprehensive-guide",
        "relatedPosts": [
          ""
        ],
        "ogImage": "/blog-images/b9e9931ceeaedfd86a9c16fcec42a1e0.jpeg",
        "excerpt": "This is a complete guide to EaaS, including how it works, the benefits, and how to deploy it in your organization.",
        "tags": [
          "platform-engineering",
          "product"
        ],
        "ctaButton": "Try Release for Free",
        "body": {
          "raw": "\n![](/blog-images/0d632a38a55d38c9f131b8d865d864b7.jpeg)\n\nCompanies are creating more and more applications and services as they continue to embrace digital transformation and automation. In fact, the [average enterprise](https://www.k2io.com/average-enterprise-runs-464-custom-applications/#:~:text=Average%20Enterprise%20Runs%20464%20Custom%20Applications%20%7C%20K2%20Cyber%20Security) now runs 464 custom applications, according to a recent report. Even small organizations with fewer than 1,000 employees maintain around 22 custom applications on average.\n\nAs a result, businesses are looking for ways to make software development faster, easier, more cost-effective, and more collaborative. To do this, many organizations are turning to on-demand environment as a service (EaaS) models. In fact, EaaS is increasingly helping engineering teams produce software at scale. \n\nIt’s a good idea to have a working knowledge of EaaS heading into 2023. Keep reading for a complete guide to EaaS, including how it works, the benefits that it offers, and how to deploy it in your organization.\n\n![](/blog-images/e6d7233f375c86ed90be41d3f29cf386.png)\n\n## What Is Environment as a Service?\n\nEaaS is a cloud-based delivery model that streamlines the way developers create and deploy environments. The concept is an extension of infrastructure as a service (IaaS). But while IaaS only involves hardware and basic software, EaaS encompasses much more—like settings and code, as well as the underlying software and infrastructure that’s necessary for operating in an isolated environment.\n\nAn environment in this case refers to the infrastructure, software, and platform services that engineers need to complete different tasks. Software engineers today use live production environments as well as nonproduction environments for things like development, staging, and testing.\n\nOn one hand, environments are a necessary part of software production. However, they're expensive and time-consuming to provision, deploy, configure, and decommission. High demand often leads to bottlenecks and delays, preventing teams from shipping software quickly. \n\nBy providing all necessary components in one central portal, EaaS eliminates the burden of having to manually create and manage software environments.\n\n## What do Developers use EaaS for?\n\nEngineering teams are finding more and more ways to use EaaS, with new use cases constantly arising. With that in mind, let’s examine three common examples of how you can use EaaS in your organization.\n\n### Performance Testing\n\nSoftware performance testing is more important than ever due to skyrocketing customer expectations and rising competition. In fact, [demand for QA engineers and software testers](https://www.bls.gov/ooh/computer-and-information-technology/software-developers.htm#:~:text=in%20May%202021.-,Job%20Outlook,the%20average%20for%20all%20occupations.) is on pace to grow by 25% over the next decade.\n\nBusinesses are now using EaaS to enable performance testing at scale while keeping costs low and ensuring accuracy. \n\n### Sharing Sales Demos\n\nBusinesses need to ensure maximum performance and stability while sharing sales demos, especially when selling mission-critical software or products with speed and reliability guarantees.\n\nBy using EaaS, companies are able to avoid sharing system resources with other users and programs, thus providing a more stable and predictable operating experience. \n\n### Migrations \n\nBusinesses often need to transfer data between storage systems and cloud environments. However, this can be dangerous without the right tools and resources.\n\nEaaS enables stable, flexible, and risk-free storage during a migration project. EaaS environments are also fast and easy to manage and decommission, which can eliminate security violations and data leaks after a migration is complete.\n\n## Top Benefits of EaaS\n\nThere are numerous [benefits to adding EaaS](https://releasehub.com/blog/environments-as-a-service-eaas-top-3-benefits) to your business’s software engineering toolkit.\n\n### 1\\. Improve Software Velocity\n\nSoftware production teams are under rising pressure to improve velocity and deliver more updates and applications. To this end, velocity remains one of the top KPIs heading into 2023.\n\nEaaS increases software velocity by decreasing rework and removing bottlenecks during production. This is due to the fact that EaaS provides the resources to support staging and QA initiatives.\n\n### 2\\. Save Money \n\nCompanies are also under pressure to reduce production costs and maintain lean development environments. According to Gartner, cutting costs by optimizing IT is among the [top technology trends for 2023](https://www.cio.com/article/407109/top-10-strategic-technology-trends-for-2023-gartner.html#:~:text=Top%2010%20strategic%20technology%20trends%20for%202023%3A%20Gartner,to%20respond%20to%20organizational%20change%20...%20More%20items).\n\nEaaS is very cost-efficient. It reduces maintenance costs and enables businesses to use resources more effectively. At the same time, EaaS lowers project cycle times, leading to further cost savings. \n\n### 3\\. Increase Flexibility for Developers\n\nMost organizations are looking for ways to enable remote work in order to attract and retain talent. In light of this, another benefit to EaaS is that it increases flexibility for developers. Participants can access and modify environments from any location instead of having to be on-site. \n\n### 4\\. Free Teams to Focus on Other Work\n\nAt the end of the day, engineers need to be focused on high-value work, not spending time creating and managing environments. EaaS removes the burden of having to create environments, thus freeing team members to tackle the most important tasks.\n\n## Are There Drawbacks to EaaS?\n\nUnless your business has strict rules in place requiring on-site development tools and resources, then you shouldn't have any issues implementing EaaS. The technology is secure, flexible, and noninvasive, as it doesn’t require purchasing or managing any infrastructure. You can also use it alongside local developer resources. \n\nTruth be told, it’s riskier to disregard EaaS when considering the numerous benefits that it offers. EaaS positions companies to develop software at a faster clip without sacrificing quality or performance.\n\nContinuing to produce software without isolated, on-demand environments could present a major disadvantage as more and more competitors adopt flexible EaaS models that enable them to accelerate production. \n\n## Implementing EaaS: Build or Buy?\n\nSome companies choose to build their own EaaS platforms. However, this is extremely time- and cost-intensive, and most companies lack the bandwidth to ensure optimal results. For example, some organizations are now dealing with outdated internal EaaS systems and are struggling to retrofit their platforms for [Kubernetes](https://releasehub.com/blog/kubernetes-pod-a-beginners-guide-to-an-essential-resource). \n\nUltimately, EaaS systems are very complicated to implement, manage, and maintain. It’s much easier to work with a third-party EaaS provider that can ensure a smooth transition and future-proof your platform for long-term success.\n\n![](/blog-images/5ae05ef5667d6883dff1f821322aee77.png)\n\n## Best Practices for Implementing EaaS\n\nDeciding to work with a third-party EaaS provider can save your business a significant amount of time and effort. However, there are a few things to keep in mind when implementing a solution.\n\n### Consult With Your Stakeholders\n\nBefore you move forward with EaaS, it’s a good idea to bring your team up to speed about why you're making the change. During this strategy session, you can explain how EaaS works, the benefits that it offers, and how it will improve speed and productivity. \n\n### Prioritize Security\n\nWhen browsing for a provider, it’s important to select one that features enterprise-grade security and authentication features. You should also ask for a bill of materials and analyze the company’s supply chain to explore where any open-source components come from.\n\nThis is critical if your team intends to use EaaS to store sensitive data. After all, not all vendors offer the same commitment to security. \n\n### Consider the User Experience \n\nYour engineers are going to spend a significant amount of time on the EaaS platform. As a result, it’s a good idea to factor in the user experience.\n\nYou may also want to invite engineers to demo platforms and participate in the sourcing process. Otherwise, developers may turn to other tools for testing software, which could lead to shadow IT and create privacy and security issues.\n\n## Streamline Development With ReleaseHub’s On-Demand Environments \n\nAs you can see, there are numerous advantages to using an on-demand EaaS model, like cost savings, faster time to market, and fewer production errors. \n\nReleaseHub offers a purpose-built EaaS platform that lets you instantly spin up environments for testing, production, demoing, and more. It’s ideal for businesses of all sizes—from small startups to large enterprises and everything in between. \n\nTo experience how ReleaseHub can change the way your team produces software, [try a demo today](https://releasehub.com/book-a-demo). \n\n## Social Blurb\n\nMany companies are turning to environment as a service (EaaS) solutions to turbocharge software production. Learn all about EaaS in our latest [blog](https://releasehub.com/blog).\n",
          "code": "var Component=(()=>{var h=Object.create;var t=Object.defineProperty;var m=Object.getOwnPropertyDescriptor;var p=Object.getOwnPropertyNames;var u=Object.getPrototypeOf,g=Object.prototype.hasOwnProperty;var f=(a,e)=>()=>(e||a((e={exports:{}}).exports,e),e.exports),v=(a,e)=>{for(var o in e)t(a,o,{get:e[o],enumerable:!0})},s=(a,e,o,r)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let i of p(e))!g.call(a,i)&&i!==o&&t(a,i,{get:()=>e[i],enumerable:!(r=m(e,i))||r.enumerable});return a};var y=(a,e,o)=>(o=a!=null?h(u(a)):{},s(e||!a||!a.__esModule?t(o,\"default\",{value:a,enumerable:!0}):o,a)),w=a=>s(t({},\"__esModule\",{value:!0}),a);var l=f((N,c)=>{c.exports=_jsx_runtime});var E={};v(E,{default:()=>S,frontmatter:()=>b});var n=y(l()),b={title:\"Environment as a Service (EaaS): A Comprehensive Guide\",summary:\"This is a complete guide to EaaS, including how it works, the benefits, and how to deploy it in your organization.\",publishDate:\"Tue Jan 17 2023 10:00:34 GMT+0000 (Coordinated Universal Time)\",author:\"\",readingTime:6,categories:[\"platform-engineering\",\"product\"],mainImage:\"/blog-images/b9e9931ceeaedfd86a9c16fcec42a1e0.jpeg\",imageAlt:\"Environment as a Service (EaaS): A Comprehensive Guide\",showCTA:!0,ctaCopy:\"Automate environment provisioning with Release for faster software deployment and seamless collaboration. Say goodbye to manual setup headaches!\",ctaLink:\"https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=environment-as-a-service-eaas-a-comprehensive-guide\",relatedPosts:[\"\"],ogImage:\"/blog-images/b9e9931ceeaedfd86a9c16fcec42a1e0.jpeg\",excerpt:\"This is a complete guide to EaaS, including how it works, the benefits, and how to deploy it in your organization.\",tags:[\"platform-engineering\",\"product\"],ctaButton:\"Try Release for Free\"};function d(a){let e=Object.assign({p:\"p\",img:\"img\",a:\"a\",h2:\"h2\",span:\"span\",h3:\"h3\"},a.components);return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/0d632a38a55d38c9f131b8d865d864b7.jpeg\",alt:\"\"})}),`\n`,(0,n.jsxs)(e.p,{children:[\"Companies are creating more and more applications and services as they continue to embrace digital transformation and automation. In fact, the \",(0,n.jsx)(e.a,{href:\"https://www.k2io.com/average-enterprise-runs-464-custom-applications/#:~:text=Average%20Enterprise%20Runs%20464%20Custom%20Applications%20%7C%20K2%20Cyber%20Security\",children:\"average enterprise\"}),\" now runs 464 custom applications, according to a recent report. Even small organizations with fewer than 1,000 employees maintain around 22 custom applications on average.\"]}),`\n`,(0,n.jsx)(e.p,{children:\"As a result, businesses are looking for ways to make software development faster, easier, more cost-effective, and more collaborative. To do this, many organizations are turning to on-demand environment as a service (EaaS) models. In fact, EaaS is increasingly helping engineering teams produce software at scale.\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:\"It\\u2019s a good idea to have a working knowledge of EaaS heading into 2023. Keep reading for a complete guide to EaaS, including how it works, the benefits that it offers, and how to deploy it in your organization.\"}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/e6d7233f375c86ed90be41d3f29cf386.png\",alt:\"\"})}),`\n`,(0,n.jsxs)(e.h2,{id:\"what-is-environment-as-a-service\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#what-is-environment-as-a-service\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"What Is Environment as a Service?\"]}),`\n`,(0,n.jsx)(e.p,{children:\"EaaS is a cloud-based delivery model that streamlines the way developers create and deploy environments. The concept is an extension of infrastructure as a service (IaaS). But while IaaS only involves hardware and basic software, EaaS encompasses much more\\u2014like settings and code, as well as the underlying software and infrastructure that\\u2019s necessary for operating in an isolated environment.\"}),`\n`,(0,n.jsx)(e.p,{children:\"An environment in this case refers to the infrastructure, software, and platform services that engineers need to complete different tasks. Software engineers today use live production environments as well as nonproduction environments for things like development, staging, and testing.\"}),`\n`,(0,n.jsx)(e.p,{children:\"On one hand, environments are a necessary part of software production. However, they're expensive and time-consuming to provision, deploy, configure, and decommission. High demand often leads to bottlenecks and delays, preventing teams from shipping software quickly.\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:\"By providing all necessary components in one central portal, EaaS eliminates the burden of having to manually create and manage software environments.\"}),`\n`,(0,n.jsxs)(e.h2,{id:\"what-do-developers-use-eaas-for\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#what-do-developers-use-eaas-for\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"What do Developers use EaaS for?\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Engineering teams are finding more and more ways to use EaaS, with new use cases constantly arising. With that in mind, let\\u2019s examine three common examples of how you can use EaaS in your organization.\"}),`\n`,(0,n.jsxs)(e.h3,{id:\"performance-testing\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#performance-testing\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Performance Testing\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"Software performance testing is more important than ever due to skyrocketing customer expectations and rising competition. In fact, \",(0,n.jsx)(e.a,{href:\"https://www.bls.gov/ooh/computer-and-information-technology/software-developers.htm#:~:text=in%20May%202021.-,Job%20Outlook,the%20average%20for%20all%20occupations.\",children:\"demand for QA engineers and software testers\"}),\" is on pace to grow by 25% over the next decade.\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Businesses are now using EaaS to enable performance testing at scale while keeping costs low and ensuring accuracy.\\xA0\"}),`\n`,(0,n.jsxs)(e.h3,{id:\"sharing-sales-demos\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#sharing-sales-demos\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Sharing Sales Demos\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Businesses need to ensure maximum performance and stability while sharing sales demos, especially when selling mission-critical software or products with speed and reliability guarantees.\"}),`\n`,(0,n.jsx)(e.p,{children:\"By using EaaS, companies are able to avoid sharing system resources with other users and programs, thus providing a more stable and predictable operating experience.\\xA0\"}),`\n`,(0,n.jsxs)(e.h3,{id:\"migrations\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#migrations\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Migrations\\xA0\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Businesses often need to transfer data between storage systems and cloud environments. However, this can be dangerous without the right tools and resources.\"}),`\n`,(0,n.jsx)(e.p,{children:\"EaaS enables stable, flexible, and risk-free storage during a migration project. EaaS environments are also fast and easy to manage and decommission, which can eliminate security violations and data leaks after a migration is complete.\"}),`\n`,(0,n.jsxs)(e.h2,{id:\"top-benefits-of-eaas\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#top-benefits-of-eaas\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Top Benefits of EaaS\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"There are numerous \",(0,n.jsx)(e.a,{href:\"https://releasehub.com/blog/environments-as-a-service-eaas-top-3-benefits\",children:\"benefits to adding EaaS\"}),\" to your business\\u2019s software engineering toolkit.\"]}),`\n`,(0,n.jsxs)(e.h3,{id:\"1-improve-software-velocity\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#1-improve-software-velocity\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"1. Improve Software Velocity\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Software production teams are under rising pressure to improve velocity and deliver more updates and applications. To this end, velocity remains one of the top KPIs heading into 2023.\"}),`\n`,(0,n.jsx)(e.p,{children:\"EaaS increases software velocity by decreasing rework and removing bottlenecks during production. This is due to the fact that EaaS provides the resources to support staging and QA initiatives.\"}),`\n`,(0,n.jsxs)(e.h3,{id:\"2-save-money\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#2-save-money\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"2. Save Money\\xA0\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"Companies are also under pressure to reduce production costs and maintain lean development environments. According to Gartner, cutting costs by optimizing IT is among the \",(0,n.jsx)(e.a,{href:\"https://www.cio.com/article/407109/top-10-strategic-technology-trends-for-2023-gartner.html#:~:text=Top%2010%20strategic%20technology%20trends%20for%202023%3A%20Gartner,to%20respond%20to%20organizational%20change%20...%20More%20items\",children:\"top technology trends for 2023\"}),\".\"]}),`\n`,(0,n.jsx)(e.p,{children:\"EaaS is very cost-efficient. It reduces maintenance costs and enables businesses to use resources more effectively. At the same time, EaaS lowers project cycle times, leading to further cost savings.\\xA0\"}),`\n`,(0,n.jsxs)(e.h3,{id:\"3-increase-flexibility-for-developers\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#3-increase-flexibility-for-developers\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"3. Increase Flexibility for Developers\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Most organizations are looking for ways to enable remote work in order to attract and retain talent. In light of this, another benefit to EaaS is that it increases flexibility for developers. Participants can access and modify environments from any location instead of having to be on-site.\\xA0\"}),`\n`,(0,n.jsxs)(e.h3,{id:\"4-free-teams-to-focus-on-other-work\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#4-free-teams-to-focus-on-other-work\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"4. Free Teams to Focus on Other Work\"]}),`\n`,(0,n.jsx)(e.p,{children:\"At the end of the day, engineers need to be focused on high-value work, not spending time creating and managing environments. EaaS removes the burden of having to create environments, thus freeing team members to tackle the most important tasks.\"}),`\n`,(0,n.jsxs)(e.h2,{id:\"are-there-drawbacks-to-eaas\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#are-there-drawbacks-to-eaas\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Are There Drawbacks to EaaS?\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Unless your business has strict rules in place requiring on-site development tools and resources, then you shouldn't have any issues implementing EaaS. The technology is secure, flexible, and noninvasive, as it doesn\\u2019t require purchasing or managing any infrastructure. You can also use it alongside local developer resources.\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:\"Truth be told, it\\u2019s riskier to disregard EaaS when considering the numerous benefits that it offers. EaaS positions companies to develop software at a faster clip without sacrificing quality or performance.\"}),`\n`,(0,n.jsx)(e.p,{children:\"Continuing to produce software without isolated, on-demand environments could present a major disadvantage as more and more competitors adopt flexible EaaS models that enable them to accelerate production.\\xA0\"}),`\n`,(0,n.jsxs)(e.h2,{id:\"implementing-eaas-build-or-buy\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#implementing-eaas-build-or-buy\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Implementing EaaS: Build or Buy?\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"Some companies choose to build their own EaaS platforms. However, this is extremely time- and cost-intensive, and most companies lack the bandwidth to ensure optimal results. For example, some organizations are now dealing with outdated internal EaaS systems and are struggling to retrofit their platforms for \",(0,n.jsx)(e.a,{href:\"https://releasehub.com/blog/kubernetes-pod-a-beginners-guide-to-an-essential-resource\",children:\"Kubernetes\"}),\".\\xA0\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Ultimately, EaaS systems are very complicated to implement, manage, and maintain. It\\u2019s much easier to work with a third-party EaaS provider that can ensure a smooth transition and future-proof your platform for long-term success.\"}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/5ae05ef5667d6883dff1f821322aee77.png\",alt:\"\"})}),`\n`,(0,n.jsxs)(e.h2,{id:\"best-practices-for-implementing-eaas\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#best-practices-for-implementing-eaas\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Best Practices for Implementing EaaS\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Deciding to work with a third-party EaaS provider can save your business a significant amount of time and effort. However, there are a few things to keep in mind when implementing a solution.\"}),`\n`,(0,n.jsxs)(e.h3,{id:\"consult-with-your-stakeholders\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#consult-with-your-stakeholders\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Consult With Your Stakeholders\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Before you move forward with EaaS, it\\u2019s a good idea to bring your team up to speed about why you're making the change. During this strategy session, you can explain how EaaS works, the benefits that it offers, and how it will improve speed and productivity.\\xA0\"}),`\n`,(0,n.jsxs)(e.h3,{id:\"prioritize-security\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#prioritize-security\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Prioritize Security\"]}),`\n`,(0,n.jsx)(e.p,{children:\"When browsing for a provider, it\\u2019s important to select one that features enterprise-grade security and authentication features. You should also ask for a bill of materials and analyze the company\\u2019s supply chain to explore where any open-source components come from.\"}),`\n`,(0,n.jsx)(e.p,{children:\"This is critical if your team intends to use EaaS to store sensitive data. After all, not all vendors offer the same commitment to security.\\xA0\"}),`\n`,(0,n.jsxs)(e.h3,{id:\"consider-the-user-experience\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#consider-the-user-experience\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Consider the User Experience\\xA0\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Your engineers are going to spend a significant amount of time on the EaaS platform. As a result, it\\u2019s a good idea to factor in the user experience.\"}),`\n`,(0,n.jsx)(e.p,{children:\"You may also want to invite engineers to demo platforms and participate in the sourcing process. Otherwise, developers may turn to other tools for testing software, which could lead to shadow IT and create privacy and security issues.\"}),`\n`,(0,n.jsxs)(e.h2,{id:\"streamline-development-with-releasehubs-on-demand-environments\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#streamline-development-with-releasehubs-on-demand-environments\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Streamline Development With ReleaseHub\\u2019s On-Demand Environments\\xA0\"]}),`\n`,(0,n.jsx)(e.p,{children:\"As you can see, there are numerous advantages to using an on-demand EaaS model, like cost savings, faster time to market, and fewer production errors.\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:\"ReleaseHub offers a purpose-built EaaS platform that lets you instantly spin up environments for testing, production, demoing, and more. It\\u2019s ideal for businesses of all sizes\\u2014from small startups to large enterprises and everything in between.\\xA0\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"To experience how ReleaseHub can change the way your team produces software, \",(0,n.jsx)(e.a,{href:\"https://releasehub.com/book-a-demo\",children:\"try a demo today\"}),\".\\xA0\"]}),`\n`,(0,n.jsxs)(e.h2,{id:\"social-blurb\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#social-blurb\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Social Blurb\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"Many companies are turning to environment as a service (EaaS) solutions to turbocharge software production. Learn all about EaaS in our latest \",(0,n.jsx)(e.a,{href:\"https://releasehub.com/blog\",children:\"blog\"}),\".\"]})]})}function k(a={}){let{wrapper:e}=a.components||{};return e?(0,n.jsx)(e,Object.assign({},a,{children:(0,n.jsx)(d,a)})):d(a)}var S=k;return w(E);})();\n;return Component;"
        },
        "_id": "blog/posts/environment-as-a-service-eaas-a-comprehensive-guide.mdx",
        "_raw": {
          "sourceFilePath": "blog/posts/environment-as-a-service-eaas-a-comprehensive-guide.mdx",
          "sourceFileName": "environment-as-a-service-eaas-a-comprehensive-guide.mdx",
          "sourceFileDir": "blog/posts",
          "contentType": "mdx",
          "flattenedPath": "blog/posts/environment-as-a-service-eaas-a-comprehensive-guide"
        },
        "type": "BlogPost",
        "computedSlug": "environment-as-a-service-eaas-a-comprehensive-guide"
      },
      "documentHash": "1739393595018",
      "hasWarnings": false,
      "documentTypeName": "BlogPost"
    },
    "blog/posts/environments-as-a-service-eaas-top-3-benefits.mdx": {
      "document": {
        "title": "Environments as a Service (EaaS) - Top 3 benefits",
        "summary": "Everything is a service these days; Everything as a Service (XaaS) actually exists. So you can be forgiven for not",
        "publishDate": "Fri Mar 12 2021 22:29:07 GMT+0000 (Coordinated Universal Time)",
        "author": "erik-landerholm",
        "readingTime": 5,
        "categories": [
          "platform-engineering",
          "product"
        ],
        "mainImage": "/blog-images/07630124bcf2ad1c2a94c9a06b56585f.jpg",
        "imageAlt": "Dozens of colorful sunflowers representing the diversity of EaaS",
        "showCTA": true,
        "ctaCopy": "Release's EaaS platform simplifies cost control, scalability, and ephemeral environments, aligning with EaaS benefits mentioned in the blog.",
        "ctaLink": "https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=environments-as-a-service-eaas-top-3-benefits",
        "relatedPosts": [
          ""
        ],
        "ogImage": "/blog-images/07630124bcf2ad1c2a94c9a06b56585f.jpg",
        "excerpt": "Everything is a service these days; Everything as a Service (XaaS) actually exists. So you can be forgiven for not",
        "tags": [
          "platform-engineering",
          "product"
        ],
        "ctaButton": "Try Release for Free",
        "body": {
          "raw": "\n### Environments as a Service\n\nEverything is a service these days; Everything as a Service ([XaaS](https://simple.wikipedia.org/wiki/Everything_as_a_service)) actually exists. So you can be forgiven for not knowing exactly what Environments as a Service (EaaS) is and why it could be a game changer for your business.\n\nEaaS is the natural extension of infrastructure as a service (IaaS, e.g. AWS, GCP, etc.), but instead of just the hardware and base software, EaaS includes all your code and settings as well as the infrastructure and software to run your application in an isolated environment. You describe your application to the system and the EaaS platform does the rest.\n\nThese environments can be used for performance testing, QA, Sales Demos, large software and/or data migrations—even production. Aside from production, these environments are ephemeral; they come and go based on your particular SDLC.\n\nThere are a lot of reasons to implement an EaaS, but there are 3 specific benefits that can change the whole trajectory of your business.\n\n### Cost Control\n\nUsing a cloud provider has huge advantages, but there are some downsides, and a big one is cost. It isn’t that using a cloud provider means your infrastructure costs are necessarily higher, but the risk of accidentally spending a lot more money than you meant to is real! An EaaS can make this a non-issue. Once you know what an environment costs to create, you can understand your expenditures in a way not possible when dealing with AWS or GCP directly.\n\n> _Don’t forget to create all the correct billing alarms in AWS, they aren’t on by default!_\n\nYour EaaS provider gives you total control over your cloud bill by allowing you to limit the number of environments created and what they consist of… Your environments can be scaled to match your needs for each environment so that costs can be contained. For example, your demo environments need not be as big or fast as your load testing environment. Also, as noted above, your environments are ephemeral so they can be spun up and deleted automatically for only as long as you are actually going to use them.\n\nThe costs of building your own internal EaaS is complicated to calculate, but you need to take into account a team of specialized devops engineers working on the project for 6-18+ months (depending on complexity), maintenance of the platform each year, cost of adopting new technologies, handling all your own AWS or other cloud costs, internal product management to make sure it stays competitive, and so on. It’s not cheap or easy to understand the costs of this kind of system. Even a team of only 3 Devops engineers working on an EaaS for about 6 months is over half a million dollars and that does not take into account opportunity cost and maintenance/upgrading costs going forward.\n\nBuying the right EaaS will be usable sooner than if you build it yourself and the costs will be simple to understand and more affordable. There are many things to spend your limited resources on to move your business forward, but building your own EaaS is not one of them.\n\n### Massive Increases in Speed\n\n> _“Speed Kills.” - Al Davis_\n\nAll things being equal you have an advantage in football and business if you are more agile than your opponents or competitors. Obviously, in software development you need to define speed with a component of quality. Deploying a bunch of changes quickly that result in lower conversion or (worse) down-time is a huge problem, and that’s not how we want to measure speed: it’s too naive.\n\nWe need to measure our velocity by only counting product deliverables that meet or exceed your key metrics and don’t compromise the stability of the application. Having a fast and capable EaaS could increase your teams’ velocity more than implementing any other kind of platform. An EaaS can improve your velocity in at least two dimensions by removing bottlenecks and decreasing rework.\n\nWith an EaaS your releases will never get stuck because of a lack of staging or QA environments. The ability to create and destroy production like environments with your EaaS means your releases aren’t being delayed because of environmental bottlenecks. Since your environments are now ephemeral you can create more when your teams need them and shut them down when they aren’t needed. Capacity planning can now be done in real time with the platform reacting to your teams’ changes in velocity.\n\n![Ephemeral Environments](/blog-images/dcb11bff94c7277ee59cddb74df6be58.png)\n\n> _So many beautiful Ephemeral Environments to work with!_\n\nRework is the most costly kind of work you can do. You always want to tackle rework as early as possible. You can’t avoid it completely, but you can minimize its impact. Often features are complex—even the smallest ones—with all the ways people may interact with it. From mobile apps to APIs, features often need to be seen or experienced by many people and multiple teams before release to avoid rework. Having access to an isolated, ephemeral environment that looks like ‘production + the new feature’ gives each team the ability to test earlier on and give feedback while the developers and designers are creating it. A good EaaS will give you the confidence in your quality and remove bottlenecks, allowing all your teams to achieve more in less time than ever before.\n\n### Implementing New Technology\n\n![Devops Tools and Technolgies](/blog-images/89eb8ffe69f92eea204cf6945c432912.png)\n\n> _Simplified Devops technolgies image, courtesy of OSOLABS._\n\nTechnologies are constantly changing and evolving. It wasn’t that long ago that open source databases and the cloud weren’t considered mature enough technologies for the enterprise. It was even less time ago when you didn’t use containers, but just virtual machines (VMs) on AWS using EC2. If you started building an internal EaaS 4-5 years ago you most likely did not build it using Kubernetes (k8s); it just wasn’t ready for production workloads at the time. The retro-fit of an internal EaaS from managing VMs or containers to managing k8s is a non-trivial process. This is one of the major reasons you want to use an external EaaS.\n\nKubernetes is amazing when it’s running well and managed by an experienced group of people. Implementing it is not for the faint of heart and it changes rapidly compared to more mature software. It’s probably the ultimate “[footgun](https://en.wiktionary.org/wiki/footgun)” in devops at the moment.\n\nK8s is not an EaaS by itself, but just a piece of the system—albeit an important one. In order to implement an internal EaaS on Kubernetes you need a deep understanding of it and a group of talented engineers to create an EaaS on top of it. And after you do all that, what happens if Kubernetes gets supplanted by something else? You will be stuck at that decision again, needing to invest all the time and resources to implement something new, or stay with what you have and hope it doesn’t hold you back compared to your competitors.\n\nCloud providers are always supporting new technologies and many times the documentation, support, and stability of these technologies leaves something to be desired. An EaaS can help you avoid the time and distraction it takes to learn and implement all these changing technologies. You can think of Kubernetes as an engine and an EaaS as a car. Most of us buy a car, not all the parts of the car to make it yourself, unless we are in the business of building cars. You are in the business of creating amazing AI products, collaboration software, or just about anything other than an EaaS.\n\n### Conclusion\n\nAn EaaS is crucial for your business to move as fast as possible while not sacrificing quality. The ability to control and predict costs while producing high quality work as quickly as possible is the holy grail of product development. The inability to quickly produce isolated environments of any specification will hold you back in innumerable ways.\n\nEaaS platforms are extremely complicated systems with rapidly changing technologies underpinning them. Just as most companies shouldn’t create their own alerting and monitoring solution, but instead use Datadog or an analog, they shouldn’t be creating their own EaaS.\n\n_At Release we work tirelessly to bring your application to life in an orchestrated, human interface. We write software to deal with all the complexity, difficulty, and strain so that no one else has to (unless they want to!) We create the engine that drives the Kubernetes vehicle, and we deliver solutions that our customers can use to get on with their business of doing business. Checkout_ [_Release_](https://releasehub.com) _and let us help your business streamline feature development with EaaS!_\n\nPhoto by [Alexander Schimmeck](https://unsplash.com/@alschim?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) on [Unsplash](https://unsplash.com/)\n",
          "code": "var Component=(()=>{var d=Object.create;var i=Object.defineProperty;var u=Object.getOwnPropertyDescriptor;var m=Object.getOwnPropertyNames;var p=Object.getPrototypeOf,g=Object.prototype.hasOwnProperty;var y=(a,e)=>()=>(e||a((e={exports:{}}).exports,e),e.exports),f=(a,e)=>{for(var t in e)i(a,t,{get:e[t],enumerable:!0})},r=(a,e,t,s)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let o of m(e))!g.call(a,o)&&o!==t&&i(a,o,{get:()=>e[o],enumerable:!(s=u(e,o))||s.enumerable});return a};var b=(a,e,t)=>(t=a!=null?d(p(a)):{},r(e||!a||!a.__esModule?i(t,\"default\",{value:a,enumerable:!0}):t,a)),v=a=>r(i({},\"__esModule\",{value:!0}),a);var c=y((A,l)=>{l.exports=_jsx_runtime});var E={};f(E,{default:()=>S,frontmatter:()=>w});var n=b(c()),w={title:\"Environments as a Service (EaaS) - Top 3 benefits\",summary:\"Everything is a service these days; Everything as a Service (XaaS) actually exists. So you can be forgiven for not\",publishDate:\"Fri Mar 12 2021 22:29:07 GMT+0000 (Coordinated Universal Time)\",author:\"erik-landerholm\",readingTime:5,categories:[\"platform-engineering\",\"product\"],mainImage:\"/blog-images/07630124bcf2ad1c2a94c9a06b56585f.jpg\",imageAlt:\"Dozens of colorful sunflowers representing the diversity of EaaS\",showCTA:!0,ctaCopy:\"Release's EaaS platform simplifies cost control, scalability, and ephemeral environments, aligning with EaaS benefits mentioned in the blog.\",ctaLink:\"https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=environments-as-a-service-eaas-top-3-benefits\",relatedPosts:[\"\"],ogImage:\"/blog-images/07630124bcf2ad1c2a94c9a06b56585f.jpg\",excerpt:\"Everything is a service these days; Everything as a Service (XaaS) actually exists. So you can be forgiven for not\",tags:[\"platform-engineering\",\"product\"],ctaButton:\"Try Release for Free\"};function h(a){let e=Object.assign({h3:\"h3\",a:\"a\",span:\"span\",p:\"p\",blockquote:\"blockquote\",em:\"em\",img:\"img\"},a.components);return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsxs)(e.h3,{id:\"environments-as-a-service\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#environments-as-a-service\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Environments as a Service\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"Everything is a service these days; Everything as a Service (\",(0,n.jsx)(e.a,{href:\"https://simple.wikipedia.org/wiki/Everything_as_a_service\",children:\"XaaS\"}),\") actually exists. So you can be forgiven for not knowing exactly what Environments as a Service (EaaS) is and why it could be a game changer for your business.\"]}),`\n`,(0,n.jsx)(e.p,{children:\"EaaS is the natural extension of infrastructure as a service (IaaS, e.g. AWS, GCP, etc.), but instead of just the hardware and base software, EaaS includes all your code and settings as well as the infrastructure and software to run your application in an isolated environment. You describe your application to the system and the EaaS platform does the rest.\"}),`\n`,(0,n.jsx)(e.p,{children:\"These environments can be used for performance testing, QA, Sales Demos, large software and/or data migrations\\u2014even production. Aside from production, these environments are ephemeral; they come and go based on your particular SDLC.\"}),`\n`,(0,n.jsx)(e.p,{children:\"There are a lot of reasons to implement an EaaS, but there are 3 specific benefits that can change the whole trajectory of your business.\"}),`\n`,(0,n.jsxs)(e.h3,{id:\"cost-control\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#cost-control\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Cost Control\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Using a cloud provider has huge advantages, but there are some downsides, and a big one is cost. It isn\\u2019t that using a cloud provider means your infrastructure costs are necessarily higher, but the risk of accidentally spending a lot more money than you meant to is real! An EaaS can make this a non-issue. Once you know what an environment costs to create, you can understand your expenditures in a way not possible when dealing with AWS or GCP directly.\"}),`\n`,(0,n.jsxs)(e.blockquote,{children:[`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.em,{children:\"Don\\u2019t forget to create all the correct billing alarms in AWS, they aren\\u2019t on by default!\"})}),`\n`]}),`\n`,(0,n.jsx)(e.p,{children:\"Your EaaS provider gives you total control over your cloud bill by allowing you to limit the number of environments created and what they consist of\\u2026 Your environments can be scaled to match your needs for each environment so that costs can be contained. For example, your demo environments need not be as big or fast as your load testing environment. Also, as noted above, your environments are ephemeral so they can be spun up and deleted automatically for only as long as you are actually going to use them.\"}),`\n`,(0,n.jsx)(e.p,{children:\"The costs of building your own internal EaaS is complicated to calculate, but you need to take into account a team of specialized devops engineers working on the project for 6-18+ months (depending on complexity), maintenance of the platform each year, cost of adopting new technologies, handling all your own AWS or other cloud costs, internal product management to make sure it stays competitive, and so on. It\\u2019s not cheap or easy to understand the costs of this kind of system. Even a team of only 3 Devops engineers working on an EaaS for about 6 months is over half a million dollars and that does not take into account opportunity cost and maintenance/upgrading costs going forward.\"}),`\n`,(0,n.jsx)(e.p,{children:\"Buying the right EaaS will be usable sooner than if you build it yourself and the costs will be simple to understand and more affordable. There are many things to spend your limited resources on to move your business forward, but building your own EaaS is not one of them.\"}),`\n`,(0,n.jsxs)(e.h3,{id:\"massive-increases-in-speed\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#massive-increases-in-speed\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Massive Increases in Speed\"]}),`\n`,(0,n.jsxs)(e.blockquote,{children:[`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.em,{children:\"\\u201CSpeed Kills.\\u201D - Al Davis\"})}),`\n`]}),`\n`,(0,n.jsx)(e.p,{children:\"All things being equal you have an advantage in football and business if you are more agile than your opponents or competitors. Obviously, in software development you need to define speed with a component of quality. Deploying a bunch of changes quickly that result in lower conversion or (worse) down-time is a huge problem, and that\\u2019s not how we want to measure speed: it\\u2019s too naive.\"}),`\n`,(0,n.jsx)(e.p,{children:\"We need to measure our velocity by only counting product deliverables that meet or exceed your key metrics and don\\u2019t compromise the stability of the application. Having a fast and capable EaaS could increase your teams\\u2019 velocity more than implementing any other kind of platform. An EaaS can improve your velocity in at least two dimensions by removing bottlenecks and decreasing rework.\"}),`\n`,(0,n.jsx)(e.p,{children:\"With an EaaS your releases will never get stuck because of a lack of staging or QA environments. The ability to create and destroy production like environments with your EaaS means your releases aren\\u2019t being delayed because of environmental bottlenecks. Since your environments are now ephemeral you can create more when your teams need them and shut them down when they aren\\u2019t needed. Capacity planning can now be done in real time with the platform reacting to your teams\\u2019 changes in velocity.\"}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/dcb11bff94c7277ee59cddb74df6be58.png\",alt:\"Ephemeral Environments\"})}),`\n`,(0,n.jsxs)(e.blockquote,{children:[`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.em,{children:\"So many beautiful Ephemeral Environments to work with!\"})}),`\n`]}),`\n`,(0,n.jsx)(e.p,{children:\"Rework is the most costly kind of work you can do. You always want to tackle rework as early as possible. You can\\u2019t avoid it completely, but you can minimize its impact. Often features are complex\\u2014even the smallest ones\\u2014with all the ways people may interact with it. From mobile apps to APIs, features often need to be seen or experienced by many people and multiple teams before release to avoid rework. Having access to an isolated, ephemeral environment that looks like \\u2018production + the new feature\\u2019 gives each team the ability to test earlier on and give feedback while the developers and designers are creating it. A good EaaS will give you the confidence in your quality and remove bottlenecks, allowing all your teams to achieve more in less time than ever before.\"}),`\n`,(0,n.jsxs)(e.h3,{id:\"implementing-new-technology\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#implementing-new-technology\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Implementing New Technology\"]}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/89eb8ffe69f92eea204cf6945c432912.png\",alt:\"Devops Tools and Technolgies\"})}),`\n`,(0,n.jsxs)(e.blockquote,{children:[`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.em,{children:\"Simplified Devops technolgies image, courtesy of OSOLABS.\"})}),`\n`]}),`\n`,(0,n.jsx)(e.p,{children:\"Technologies are constantly changing and evolving. It wasn\\u2019t that long ago that open source databases and the cloud weren\\u2019t considered mature enough technologies for the enterprise. It was even less time ago when you didn\\u2019t use containers, but just virtual machines (VMs) on AWS using EC2. If you started building an internal EaaS 4-5 years ago you most likely did not build it using Kubernetes (k8s); it just wasn\\u2019t ready for production workloads at the time. The retro-fit of an internal EaaS from managing VMs or containers to managing k8s is a non-trivial process. This is one of the major reasons you want to use an external EaaS.\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"Kubernetes is amazing when it\\u2019s running well and managed by an experienced group of people. Implementing it is not for the faint of heart and it changes rapidly compared to more mature software. It\\u2019s probably the ultimate \\u201C\",(0,n.jsx)(e.a,{href:\"https://en.wiktionary.org/wiki/footgun\",children:\"footgun\"}),\"\\u201D in devops at the moment.\"]}),`\n`,(0,n.jsx)(e.p,{children:\"K8s is not an EaaS by itself, but just a piece of the system\\u2014albeit an important one. In order to implement an internal EaaS on Kubernetes you need a deep understanding of it and a group of talented engineers to create an EaaS on top of it. And after you do all that, what happens if Kubernetes gets supplanted by something else? You will be stuck at that decision again, needing to invest all the time and resources to implement something new, or stay with what you have and hope it doesn\\u2019t hold you back compared to your competitors.\"}),`\n`,(0,n.jsx)(e.p,{children:\"Cloud providers are always supporting new technologies and many times the documentation, support, and stability of these technologies leaves something to be desired. An EaaS can help you avoid the time and distraction it takes to learn and implement all these changing technologies. You can think of Kubernetes as an engine and an EaaS as a car. Most of us buy a car, not all the parts of the car to make it yourself, unless we are in the business of building cars. You are in the business of creating amazing AI products, collaboration software, or just about anything other than an EaaS.\"}),`\n`,(0,n.jsxs)(e.h3,{id:\"conclusion\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#conclusion\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Conclusion\"]}),`\n`,(0,n.jsx)(e.p,{children:\"An EaaS is crucial for your business to move as fast as possible while not sacrificing quality. The ability to control and predict costs while producing high quality work as quickly as possible is the holy grail of product development. The inability to quickly produce isolated environments of any specification will hold you back in innumerable ways.\"}),`\n`,(0,n.jsx)(e.p,{children:\"EaaS platforms are extremely complicated systems with rapidly changing technologies underpinning them. Just as most companies shouldn\\u2019t create their own alerting and monitoring solution, but instead use Datadog or an analog, they shouldn\\u2019t be creating their own EaaS.\"}),`\n`,(0,n.jsxs)(e.p,{children:[(0,n.jsx)(e.em,{children:\"At Release we work tirelessly to bring your application to life in an orchestrated, human interface. We write software to deal with all the complexity, difficulty, and strain so that no one else has to (unless they want to!) We create the engine that drives the Kubernetes vehicle, and we deliver solutions that our customers can use to get on with their business of doing business. Checkout\"}),\" \",(0,n.jsx)(e.a,{href:\"https://releasehub.com\",children:(0,n.jsx)(e.em,{children:\"Release\"})}),\" \",(0,n.jsx)(e.em,{children:\"and let us help your business streamline feature development with EaaS!\"})]}),`\n`,(0,n.jsxs)(e.p,{children:[\"Photo by \",(0,n.jsx)(e.a,{href:\"https://unsplash.com/@alschim?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText\",children:\"Alexander Schimmeck\"}),\" on \",(0,n.jsx)(e.a,{href:\"https://unsplash.com/\",children:\"Unsplash\"})]})]})}function k(a={}){let{wrapper:e}=a.components||{};return e?(0,n.jsx)(e,Object.assign({},a,{children:(0,n.jsx)(h,a)})):h(a)}var S=k;return v(E);})();\n;return Component;"
        },
        "_id": "blog/posts/environments-as-a-service-eaas-top-3-benefits.mdx",
        "_raw": {
          "sourceFilePath": "blog/posts/environments-as-a-service-eaas-top-3-benefits.mdx",
          "sourceFileName": "environments-as-a-service-eaas-top-3-benefits.mdx",
          "sourceFileDir": "blog/posts",
          "contentType": "mdx",
          "flattenedPath": "blog/posts/environments-as-a-service-eaas-top-3-benefits"
        },
        "type": "BlogPost",
        "computedSlug": "environments-as-a-service-eaas-top-3-benefits"
      },
      "documentHash": "1739393595018",
      "hasWarnings": false,
      "documentTypeName": "BlogPost"
    },
    "blog/posts/ephemeral-environments-9-tips-for-seamless-deployment.mdx": {
      "document": {
        "title": "Ephemeral Environments: 9 Tips for Seamless Deployment",
        "summary": "Unlock the power of ephemeral environments for seamless deployment. Streamline workflows, enhance collaboration & more.",
        "publishDate": "Wed Feb 21 2024 21:58:58 GMT+0000 (Coordinated Universal Time)",
        "author": "regis-wilson",
        "readingTime": 5,
        "categories": [
          "platform-engineering",
          "product"
        ],
        "mainImage": "/blog-images/642d153bfae976b30e326c4257455d2a.png",
        "imageAlt": "ephemeral environments release",
        "showCTA": true,
        "ctaCopy": "Unlock rapid deployment and testing with Release's automated ephemeral environments, aligning with key characteristics discussed. Optimize your workflows now.",
        "ctaLink": "https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=ephemeral-environments-9-tips-for-seamless-deployment",
        "relatedPosts": [
          ""
        ],
        "ogImage": "/blog-images/642d153bfae976b30e326c4257455d2a.png",
        "excerpt": "Unlock the power of ephemeral environments for seamless deployment. Streamline workflows, enhance collaboration & more.",
        "tags": [
          "platform-engineering",
          "product"
        ],
        "ctaButton": "Try Release for Free",
        "body": {
          "raw": "\nEphemeral environments became a game-changer in modern software development. They are temporary, short-lived, and created as needed. These environments are perfect for specific tasks like testing new features or fixing bugs. Their main purpose is to give developers a safe space to try out and validate changes without affecting the main codebase or ongoing operations.\n**Key benefits of ephemeral environments are:**\n\n- **Risk Reduction:** Isolating changes in temporary environments minimizes the potential for disruptions in the production environment.\n- **Resource Efficiency:** These on-demand environments require resources only when active, freeing up computational power and reducing costs when not in use.\n- **Speed and Flexibility:** On-demand creation allows for rapid testing cycles and quick pivots based on real-time results.\n\nThese advantages are just the beginning. As we explore further, we'll see how ephemeral environments not only improve development workflows but also align with broader goals like continuous integration and deployment, ultimately fostering a culture of innovation and efficiency. We will go over 9 areas you need to understand to successfully implement ephemeral environments in your organizatuon. Let’s get started.\n\n## **1\\. Understand the Key Characteristics of Ephemeral Environments**\n\nEphemeral environments are catalysts in software development, closely mirroring production environments to provide a realistic testing ground for new features and updates. These dynamic setups are designed to be short-lived, with several key characteristics that make them a valuable asset for today’s development teams:\n\n### **Resemblance to Production**\n\nBy closely emulating the production environment, ephemeral environments allow developers and testers to interact with applications under conditions that are nearly identical to the live production setup. This similarity ensures that any functionality, behaviors, or issues observed during testing will likely hold true after deployment.\n\n### **Automated Creation and Fast Provisioning**\n\nSpeed is of the essence in modern development workflows. Ephemeral environments thrive on automation for their creation and provisioning, which allows them to be spun up quickly as needed. This rapid availability is essential for maintaining their temporary nature while supporting continuous integration and delivery practices.\n\n### **Replicated Data Consistency**\n\nData plays a crucial role in testing and validating application behavior. Ephemeral environments often include mechanisms for replicating data from production or using synthetic data sets that maintain consistency across test cases. This replication ensures that tests are not only relevant but also reliable.\n\n### **Accessibility via Unique URLs**\n\nStakeholders from developers to product managers require easy access to these environments. Unique URLs enable this accessibility, allowing for seamless sharing and review processes. Whether it's for internal reviews or external stakeholder demonstrations, these URLs provide direct entry points into the temporary world where the latest features reside.\n\nFor teams looking to leverage on-demand ephemeral staging environments, exploring services like [Release](https://release.com/usecase/on-demand-ephemeral-staging-environments) can offer insight into how these environments streamline development and deployment processes.\n\nBy understanding these foundational elements of ephemeral environments, organizations equip themselves with the tools necessary for efficient and effective software development cycles. Moving forward, embracing these characteristics can significantly transform how teams approach development challenges.\n\n## **2\\. Embrace the Benefits of Using Ephemeral Environments in Your Development Workflow**\n\nEphemeral environments offer numerous benefits that can transform your development workflow. By embracing these advantages, you can streamline your development process, improve code quality, and foster a more collaborative working environment.\n\n### **Reducing Rework and Decrease Cycle Time**\n\nOne such advantage includes reducing rework, a key strategy to enhance productivity and minimize errors. Another advantage is getting results quickly up front during development before reaching production or staging. These environments provide an identical replica of your production environment, enabling developers to identify and fix issues prior to deployment. This process saves time, resources, and reduces the likelihood of recurring problems.\n\n### **Self-Service Capabilities**\n\nDevelopers often require access to different environments at various stages of their workflow. Ephemeral environments empower them with self-service capabilities on internal platforms, facilitating faster iterations. With automated creation and provisioning, developers can spin up as many environments as needed without waiting for manual provisioning or risking conflicts in shared spaces.\n\n### **Running Production Workloads with Aligned Data**\n\nAnother significant benefit is the capacity to run production workloads with aligned data. This feature allows you to validate system behavior under realistic conditions, mitigating risks associated with deploying untested code into production. With data consistency ensured through mechanisms like replicated and scrubbed data, you can confidently assess how new features or changes will perform when actually deployed.\n\n### **Improving Collaboration**\n\nLastly, ephemeral environments play a vital role in improving collaboration and gathering early feedback from stakeholders. Through the use of automated preview environments that facilitate [measuring and improving developer velocity](https://release.com/blog/improve-developer-velocity-with-ephemeral-environments), stakeholders can easily access and review changes via unique URLs. This real-time collaboration fosters transparency, accelerates decision-making, and keeps everyone informed about development progress.\n\n## **3\\. Leveraging Ephemeral Environments for Different Use Cases**\n\nEphemeral environments have many practical uses in different situations, each with its own advantages. Here are two common examples:\n\n### **Development and Testing of New Features**\n\nThink of ephemeral environments as sandboxes that provide a controlled yet realistic setup. Developers can build features with confidence, knowing they are working in an environment that closely mirrors production conditions. This practice not only enhances code reliability but also minimizes surprises during the deployment phase.\n\nA perfect example of this is creating a new feature for an e-commerce site, like a personalized recommendation engine. An ephemeral environment allows developers to assess the impact of this feature in isolation from the rest of the application, ensuring it performs as expected when integrated into the larger system.\n\n### **Running Performance-Intensive or Distributed Applications**\n\nThis use case applies to applications that require significant computing resources or need to handle high volumes of data. Ephemeral environments excel in situations where you need to:\n\n- Test how well your application scales under heavy load.\n- Evaluate the performance of individual components or services.\n- Validate the behavior of distributed systems.\n\nFor instance, consider a microservices-based application that needs to scale up rapidly during peak traffic hours. In an ephemeral environment, you can simulate this scenario and assess how well your application scales under load, well before deploying it into production. Once the tests are completed, the whole environment can be torn down automatically to free up valuable resources, which could be quite expensive to build, maintain, or configure otherwise.\n\nAs you can see, ephemeral environments offer flexibility and control while providing a realistic preview of production conditions. They are undoubtedly a powerful tool in any developer's toolbox.\n\nTo delve deeper into ephemeral environments, check out Release's insightful article on [Beyond K8s: Introduction to Ephemeral Environments](https://release.com/blog/beyond-k8s-introduction-to-ephemeral-environments).\n\n## **4\\. Integration Possibilities with Collaboration Tools like GitHub and Jira**\n\nIn the realm of software development, **GitHub** and **Jira** stand as titans of collaboration, offering robust platforms for code management and issue tracking, respectively. Ephemeral environments gain an added layer of efficiency when integrated with these tools, streamlining workflows and enhancing productivity.\n\n### **Seamless Integration with GitHub**\n\n- **Automated Environment Spin-up**: Upon a new pull request in GitHub, an ephemeral environment can be automatically created. This provides immediate feedback on how code changes will perform in a live setting.\n- **Status Checks**: Integrating ephemeral environments into GitHub's status checks allows developers to see if their environment is ready for review directly from the pull request, ensuring that only fully provisioned environments are tested.\n- **Bot Notifications**: Custom bots can comment on pull requests with ephemeral environment URLs and deployment statuses, making it effortless for reviewers to access the latest version of the application.\n\n### **Streamlining Workflows with Jira**\n\n- **Linking Environments to Issues**: Attach ephemeral environment URLs to relevant Jira tickets. This encourages a clear association between task progress and the actual environment where the feature is implemented.\n- **Transition Automation**: Trigger the creation or teardown of ephemeral environments based on issue status transitions within Jira. For example, an environment can be spun up when an issue moves to \"In Progress\" and torn down once it reaches \"Done.\"\n\nBy weaving ephemeral environments into the fabric of GitHub and Jira workflows, teams harness easy sharing capabilities that complement Agile practices. The result is a streamlined process where code merges and feature developments are transparently connected to dynamic testing environments, fostering an ecosystem where sharing becomes second nature to development processes.\n\n## **5\\. Ensuring Quality in Ephemeral Environments through Effective Testing Strategies**\n\n**Unit tests** are the backbone of software testing, but they often fall short in evaluating **system behavior outside unit tests**. The complexity of modern applications necessitates comprehensive testing strategies that cover more ground. Enter **smoke and integration tests**—essential tools that probe the interactions between various components and ensure seamless deployments.\n\nWhen applied to **live ephemeral environments**, these tests do more than just verify code correctness; they simulate real-world usage to expose issues that would otherwise remain hidden until production. This is crucial because while unit tests validate individual pieces, smoke and integration tests examine the assembled puzzle, catching errors that occur when all pieces work together.\n\n### **Key Strategies for Effective Testing in Ephemeral Environments:**\n\n- **Parallel Testing:** Managing multiple ephemeral environments allows teams to run concurrent tests for different features or branches, significantly reducing the time to release.\n- **Automated Test Suites:** By automating smoke and integration tests within ephemeral environments, developers can quickly identify defects early in the development cycle.\n- **Dynamic Resource Allocation:** Allocating resources on-the-fly to handle a large number of parallel environments ensures that testing is not bottlenecked by infrastructure limitations.\n- **Continuous Monitoring:** Integrating monitoring tools to track the health and performance of ephemeral environments during testing can provide immediate feedback on system stability.\n\nIncorporating these strategies into your development workflow can transform the quality assurance process. Teams become equipped to deliver robust software at a faster pace by leveraging the unique benefits of ephemeral environments for comprehensive testing. For insights into how this approach can increase developer velocity, consider exploring Release's whitepaper on [increasing developer velocity by removing environment bottlenecks](https://release.com/blog/increase-developer-velocity-by-removing-environment-bottlenecks) using Environments as a Service.\n\nBy ensuring thorough testing in environments that mimic production closely, software teams can confidently push new features, knowing they've been vetted in conditions that match what users will encounter.\n\n## **6\\. Realizing the Agile Potential of Ephemeral Environments in Software Development**\n\nEphemeral environments play a significant role in fostering _Agile/Scrum practices_ within software development teams. With their dynamic and transient nature, they align perfectly with the iterative and adaptive nature of Agile methodologies.\n\n### **Supporting Continuous Delivery with Ephemeral Environments**\n\nOne of the key principles of Agile is _continuous delivery_, and ephemeral environments are instrumental in supporting this. They allow constant production-like testing and validation, enabling software updates to be developed, tested, and released rapidly and frequently. As such, developers can:\n\n- Test code changes immediately in a production-like environment.\n- Detect and resolve issues early before they reach production.\n- Accelerate the feedback loop with stakeholders for quicker iterations.\n\nIn essence, ephemeral environments serve as an enabler for continuous delivery – one of the cornerstones of Agile.\n\n### **Facilitating Iterative Software Development with Ephemeral Environments**\n\nAnother attribute of Agile is its emphasis on _iterative software development_. Here, ephemeral environments shine by facilitating rapid iterations and feedback loops. For instance, developers can share unique URLs of these temporary environments with stakeholders to gather early feedback. The possibility to quickly set up, test, and tear down these environments aligns perfectly with the iterative cycles of Agile development.\n\nIncorporating ephemeral environments into an Agile workflow thus enhances efficiency while maintaining high quality standards – a win-win for any modern software development team.\n\n## **7\\. The DevOps Connection: Ephemeral Environments as a Catalyst for Collaboration and Efficiency**\n\nEphemeral environments are a perfect fit for DevOps and Platform Engineering, where teams prioritize automation and collaboration. These dynamic setups are specifically designed to work within a DevOps or PE framework, [bridging the gap between software development and IT operations](https://release.com/blog/extend-your-idp-with-environments-for-every-developer-and-every-change).\n\n### **How Ephemeral Environments Benefit DevOps and Platform Engineering**\n\nHere's how ephemeral environments contribute to the success of DevOps and PE:\n\n#### **Automation Aligned with DevOps**\n\n- Ephemeral environments automate the process of creating and tearing down environments, aligning with the DevOps principle of streamlining the software development pipeline.\n- This automation reduces the manual effort required for environment setup, allowing teams to focus on more important tasks.\n\n#### **Collaboration Across Teams for Platform Engineering**\n\n- Ephemeral environments can be spun up at any stage of the development process for various purposes, such as development or testing.\n- This shared access promotes collaboration between different teams involved in the software lifecycle, breaking down silos and fostering a culture of teamwork. This platform allows a common place for all self-service environments to be tested, shared, and reviewed.\n\n### **The Role of Ephemeral Environments in CI/CD Pipelines**\n\nIntegrating ephemeral environment provisioning into continuous integration (CI) and continuous delivery (CD) pipelines can revolutionize the deployment process. Here's how it works:\n\n- A new ephemeral environment is automatically created by the CI/CD tool/platform whenever there's a code commit or pull request.\n- Developers receive immediate feedback on their changes in an environment that closely resembles the production setup.\n- The team can perform tests and quality assurance processes in real-time, ensuring that only thoroughly tested code moves forward in the pipeline.\n\nThis approach allows organizations to make the most out of their DevOps investment by speeding up deployment cycles while maintaining high standards of quality and collaboration.\n\n## **8\\. Configurability for Rapid Application Development and Testing in Ephemeral Environments**\n\nRapid application development and [testing](https://release.com/blog/test-environment-a-definition-and-how-to-guide) thrive on the ability to quickly adapt to different requirements and scenarios. Ephemeral environments extend this flexibility with their inherently dynamic nature. The key to harnessing this potential lies in the configurability of these temporary spaces, which can be tailored to match a myriad of production setups.\n\n### **How Configurability Enhances Ephemeral Environments**\n\nHere are some ways configurability enhances ephemeral environments for rapid application development and testing:\n\n- **Customization of Infrastructure Components**: Teams can customize OS, servers, memory, and storage parameters to simulate various target environments. This customization ensures that applications are tested under conditions that closely replicate those they will encounter in real-world deployments.\n- **Utilization of Deployable Artifacts**: An essential aspect is the use of deployable artifacts, which are pre-built versions of software ready to be launched into the environment. These artifacts are essential for replicating the software deployment process and can range from binary executables to Docker containers, depending on the technology stack utilized.\n- **Automated Deployment Processes**: Automation is at the core of ephemeral environments, with pipelines designed to provision infrastructure, deploy applications, and tear down resources without manual intervention. Automated processes not only ensure efficiency but also contribute significantly to consistency across testing scenarios.\n\nThe streamlined deployment process not only saves time but also reduces errors by minimizing manual setup steps. By integrating these capabilities into ephemeral environments, teams can focus on developing and testing rather than managing infrastructure details.\n\n### **Benefits of Configurability in Ephemeral Environments**\n\nBy optimizing these elements within ephemeral environments, organizations can achieve a significant competitive edge—accelerating time-to-market while ensuring high-quality standards are met before any release.\n\n## **9\\. Advantages of Ephemeral Environments over Traditional Staging Approaches**\n\n### **Asynchronous Collaboration Across Time Zones**\n\nEphemeral environments facilitate asynchronous collaboration across distributed teams by providing on-demand access to consistent testing and development environments. This feature is a game-changer for global teams working across different time zones, enabling them to work together seamlessly.\n\n### **Cost-Effective Infrastructure**\n\nCompared to traditional staging setups that require dedicated infrastructure and maintenance, ephemeral environments offer a more cost-effective solution. Since these environments are only activated when needed and decommissioned after use, they significantly reduce the overhead costs associated with maintaining permanent staging servers.\n\n### **Agile and Scalable**\n\nEphemeral environments provide unmatched agility and scalability. Teams can quickly set up, modify, or tear down environments as required, thus facilitating flexible scaling and testing processes. This capability enables companies to adapt rapidly to changing requirements without incurring additional costs or delays.\n\nOne key benefit of decreasing cycle time and per-use costs is that productivity and utilization will actually increase. As an example, a single shared environment might support one team for 24 hours of usage costs, but 24 teams or individuals can use one-hour ephemeral environments for the same overall cost. If appropriate auto-scaling is used, resource utilization costs could go to nearly zero when not used after hours or on the weekend, for example. However, utilization and productivity during normal work hours could skyrocket!\n\n### **Increased Security and Reliability**\n\nAnother advantage of ephemeral environments over traditional staging approaches is their enhanced security and reliability. Since each environment is isolated and short-lived, the risk of lingering vulnerabilities or data breaches is minimized. Moreover, these dynamic environments can be replicated exactly as per production standards, ensuring reliable testing outcomes. Not only that, but security tests, penetration tests, and destructive testing can happen without affecting the live production site, enabling the security posture to be verified and tested before reaching production. This is a massive boost in confidence on security practices that most production environments miss out on.\n\nFor a deeper dive into the benefits of ephemeral environments as part of [Environments as a Service (EaaS) offerings](https://release.com/blog/environments-as-a-service-eaas-top-3-benefits), you might find this article helpful.\n\nWith these advantages in mind, it's clear why ephemeral environments are becoming an integral part of modern software development workflows.\n\n## **Why should you care?** \n\nEphemeral environments are an innovative approach to software development that can greatly benefit your team. By creating temporary environments that closely resemble your production settings, you can streamline your development workflow and improve collaboration among team members, and make sure you stay competitive in your industry. \n\nHere are some key takeaways from this article:\n\n- **Streamline your development workflow:** Ephemeral environments allow for faster iteration cycles, as you can quickly spin up new environments for testing and debugging.\n- **Enhance collaboration:** With on-demand setups, developers, QA teams, and stakeholders can easily access and work in the same environment, reducing communication barriers.\n- **Improve testing strategies:** Ephemeral environments provide an isolated space for thorough validation of system behavior before deploying to production.\n\nReady to give ephemeral environments a try? Check out [Release](https://www.example.com/) - a platform specifically designed for managing ephemeral environments.\n**Sign up for a free trial** today and see how Release can help your team achieve greater agility and flexibility in your software development process.\n\n‍\n\n‍\n",
          "code": "var Component=(()=>{var d=Object.create;var a=Object.defineProperty;var m=Object.getOwnPropertyDescriptor;var p=Object.getOwnPropertyNames;var u=Object.getPrototypeOf,g=Object.prototype.hasOwnProperty;var f=(i,e)=>()=>(e||i((e={exports:{}}).exports,e),e.exports),v=(i,e)=>{for(var t in e)a(i,t,{get:e[t],enumerable:!0})},s=(i,e,t,o)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let r of p(e))!g.call(i,r)&&r!==t&&a(i,r,{get:()=>e[r],enumerable:!(o=m(e,r))||o.enumerable});return i};var y=(i,e,t)=>(t=i!=null?d(u(i)):{},s(e||!i||!i.__esModule?a(t,\"default\",{value:i,enumerable:!0}):t,i)),w=i=>s(a({},\"__esModule\",{value:!0}),i);var c=f((A,l)=>{l.exports=_jsx_runtime});var N={};v(N,{default:()=>E,frontmatter:()=>b});var n=y(c()),b={title:\"Ephemeral Environments: 9 Tips for Seamless Deployment\",summary:\"Unlock the power of ephemeral environments for seamless deployment. Streamline workflows, enhance collaboration & more.\",publishDate:\"Wed Feb 21 2024 21:58:58 GMT+0000 (Coordinated Universal Time)\",author:\"regis-wilson\",readingTime:5,categories:[\"platform-engineering\",\"product\"],mainImage:\"/blog-images/642d153bfae976b30e326c4257455d2a.png\",imageAlt:\"ephemeral environments release\",showCTA:!0,ctaCopy:\"Unlock rapid deployment and testing with Release's automated ephemeral environments, aligning with key characteristics discussed. Optimize your workflows now.\",ctaLink:\"https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=ephemeral-environments-9-tips-for-seamless-deployment\",relatedPosts:[\"\"],ogImage:\"/blog-images/642d153bfae976b30e326c4257455d2a.png\",excerpt:\"Unlock the power of ephemeral environments for seamless deployment. Streamline workflows, enhance collaboration & more.\",tags:[\"platform-engineering\",\"product\"],ctaButton:\"Try Release for Free\"};function h(i){let e=Object.assign({p:\"p\",strong:\"strong\",ul:\"ul\",li:\"li\",h2:\"h2\",a:\"a\",span:\"span\",h3:\"h3\",em:\"em\",h4:\"h4\"},i.components);return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsxs)(e.p,{children:[`Ephemeral environments became a game-changer in modern software development. They are temporary, short-lived, and created as needed. These environments are perfect for specific tasks like testing new features or fixing bugs. Their main purpose is to give developers a safe space to try out and validate changes without affecting the main codebase or ongoing operations.\n`,(0,n.jsx)(e.strong,{children:\"Key benefits of ephemeral environments are:\"})]}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"Risk Reduction:\"}),\" Isolating changes in temporary environments minimizes the potential for disruptions in the production environment.\"]}),`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"Resource Efficiency:\"}),\" These on-demand environments require resources only when active, freeing up computational power and reducing costs when not in use.\"]}),`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"Speed and Flexibility:\"}),\" On-demand creation allows for rapid testing cycles and quick pivots based on real-time results.\"]}),`\n`]}),`\n`,(0,n.jsx)(e.p,{children:\"These advantages are just the beginning. As we explore further, we'll see how ephemeral environments not only improve development workflows but also align with broader goals like continuous integration and deployment, ultimately fostering a culture of innovation and efficiency. We will go over 9 areas you need to understand to successfully implement ephemeral environments in your organizatuon. Let\\u2019s get started.\"}),`\n`,(0,n.jsxs)(e.h2,{id:\"1-understand-the-key-characteristics-of-ephemeral-environments\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#1-understand-the-key-characteristics-of-ephemeral-environments\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.strong,{children:\"1. Understand the Key Characteristics of Ephemeral Environments\"})]}),`\n`,(0,n.jsx)(e.p,{children:\"Ephemeral environments are catalysts in software development, closely mirroring production environments to provide a realistic testing ground for new features and updates. These dynamic setups are designed to be short-lived, with several key characteristics that make them a valuable asset for today\\u2019s development teams:\"}),`\n`,(0,n.jsxs)(e.h3,{id:\"resemblance-to-production\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#resemblance-to-production\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.strong,{children:\"Resemblance to Production\"})]}),`\n`,(0,n.jsx)(e.p,{children:\"By closely emulating the production environment, ephemeral environments allow developers and testers to interact with applications under conditions that are nearly identical to the live production setup. This similarity ensures that any functionality, behaviors, or issues observed during testing will likely hold true after deployment.\"}),`\n`,(0,n.jsxs)(e.h3,{id:\"automated-creation-and-fast-provisioning\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#automated-creation-and-fast-provisioning\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.strong,{children:\"Automated Creation and Fast Provisioning\"})]}),`\n`,(0,n.jsx)(e.p,{children:\"Speed is of the essence in modern development workflows. Ephemeral environments thrive on automation for their creation and provisioning, which allows them to be spun up quickly as needed. This rapid availability is essential for maintaining their temporary nature while supporting continuous integration and delivery practices.\"}),`\n`,(0,n.jsxs)(e.h3,{id:\"replicated-data-consistency\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#replicated-data-consistency\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.strong,{children:\"Replicated Data Consistency\"})]}),`\n`,(0,n.jsx)(e.p,{children:\"Data plays a crucial role in testing and validating application behavior. Ephemeral environments often include mechanisms for replicating data from production or using synthetic data sets that maintain consistency across test cases. This replication ensures that tests are not only relevant but also reliable.\"}),`\n`,(0,n.jsxs)(e.h3,{id:\"accessibility-via-unique-urls\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#accessibility-via-unique-urls\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.strong,{children:\"Accessibility via Unique URLs\"})]}),`\n`,(0,n.jsx)(e.p,{children:\"Stakeholders from developers to product managers require easy access to these environments. Unique URLs enable this accessibility, allowing for seamless sharing and review processes. Whether it's for internal reviews or external stakeholder demonstrations, these URLs provide direct entry points into the temporary world where the latest features reside.\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"For teams looking to leverage on-demand ephemeral staging environments, exploring services like \",(0,n.jsx)(e.a,{href:\"https://release.com/usecase/on-demand-ephemeral-staging-environments\",children:\"Release\"}),\" can offer insight into how these environments streamline development and deployment processes.\"]}),`\n`,(0,n.jsx)(e.p,{children:\"By understanding these foundational elements of ephemeral environments, organizations equip themselves with the tools necessary for efficient and effective software development cycles. Moving forward, embracing these characteristics can significantly transform how teams approach development challenges.\"}),`\n`,(0,n.jsxs)(e.h2,{id:\"2-embrace-the-benefits-of-using-ephemeral-environments-in-your-development-workflow\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#2-embrace-the-benefits-of-using-ephemeral-environments-in-your-development-workflow\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.strong,{children:\"2. Embrace the Benefits of Using Ephemeral Environments in Your Development Workflow\"})]}),`\n`,(0,n.jsx)(e.p,{children:\"Ephemeral environments offer numerous benefits that can transform your development workflow. By embracing these advantages, you can streamline your development process, improve code quality, and foster a more collaborative working environment.\"}),`\n`,(0,n.jsxs)(e.h3,{id:\"reducing-rework-and-decrease-cycle-time\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#reducing-rework-and-decrease-cycle-time\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.strong,{children:\"Reducing Rework and Decrease Cycle Time\"})]}),`\n`,(0,n.jsx)(e.p,{children:\"One such advantage includes reducing rework, a key strategy to enhance productivity and minimize errors. Another advantage is getting results quickly up front during development before reaching production or staging. These environments provide an identical replica of your production environment, enabling developers to identify and fix issues prior to deployment. This process saves time, resources, and reduces the likelihood of recurring problems.\"}),`\n`,(0,n.jsxs)(e.h3,{id:\"self-service-capabilities\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#self-service-capabilities\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.strong,{children:\"Self-Service Capabilities\"})]}),`\n`,(0,n.jsx)(e.p,{children:\"Developers often require access to different environments at various stages of their workflow. Ephemeral environments empower them with self-service capabilities on internal platforms, facilitating faster iterations. With automated creation and provisioning, developers can spin up as many environments as needed without waiting for manual provisioning or risking conflicts in shared spaces.\"}),`\n`,(0,n.jsxs)(e.h3,{id:\"running-production-workloads-with-aligned-data\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#running-production-workloads-with-aligned-data\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.strong,{children:\"Running Production Workloads with Aligned Data\"})]}),`\n`,(0,n.jsx)(e.p,{children:\"Another significant benefit is the capacity to run production workloads with aligned data. This feature allows you to validate system behavior under realistic conditions, mitigating risks associated with deploying untested code into production. With data consistency ensured through mechanisms like replicated and scrubbed data, you can confidently assess how new features or changes will perform when actually deployed.\"}),`\n`,(0,n.jsxs)(e.h3,{id:\"improving-collaboration\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#improving-collaboration\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.strong,{children:\"Improving Collaboration\"})]}),`\n`,(0,n.jsxs)(e.p,{children:[\"Lastly, ephemeral environments play a vital role in improving collaboration and gathering early feedback from stakeholders. Through the use of automated preview environments that facilitate \",(0,n.jsx)(e.a,{href:\"https://release.com/blog/improve-developer-velocity-with-ephemeral-environments\",children:\"measuring and improving developer velocity\"}),\", stakeholders can easily access and review changes via unique URLs. This real-time collaboration fosters transparency, accelerates decision-making, and keeps everyone informed about development progress.\"]}),`\n`,(0,n.jsxs)(e.h2,{id:\"3-leveraging-ephemeral-environments-for-different-use-cases\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#3-leveraging-ephemeral-environments-for-different-use-cases\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.strong,{children:\"3. Leveraging Ephemeral Environments for Different Use Cases\"})]}),`\n`,(0,n.jsx)(e.p,{children:\"Ephemeral environments have many practical uses in different situations, each with its own advantages. Here are two common examples:\"}),`\n`,(0,n.jsxs)(e.h3,{id:\"development-and-testing-of-new-features\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#development-and-testing-of-new-features\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.strong,{children:\"Development and Testing of New Features\"})]}),`\n`,(0,n.jsx)(e.p,{children:\"Think of ephemeral environments as sandboxes that provide a controlled yet realistic setup. Developers can build features with confidence, knowing they are working in an environment that closely mirrors production conditions. This practice not only enhances code reliability but also minimizes surprises during the deployment phase.\"}),`\n`,(0,n.jsx)(e.p,{children:\"A perfect example of this is creating a new feature for an e-commerce site, like a personalized recommendation engine. An ephemeral environment allows developers to assess the impact of this feature in isolation from the rest of the application, ensuring it performs as expected when integrated into the larger system.\"}),`\n`,(0,n.jsxs)(e.h3,{id:\"running-performance-intensive-or-distributed-applications\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#running-performance-intensive-or-distributed-applications\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.strong,{children:\"Running Performance-Intensive or Distributed Applications\"})]}),`\n`,(0,n.jsx)(e.p,{children:\"This use case applies to applications that require significant computing resources or need to handle high volumes of data. Ephemeral environments excel in situations where you need to:\"}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsx)(e.li,{children:\"Test how well your application scales under heavy load.\"}),`\n`,(0,n.jsx)(e.li,{children:\"Evaluate the performance of individual components or services.\"}),`\n`,(0,n.jsx)(e.li,{children:\"Validate the behavior of distributed systems.\"}),`\n`]}),`\n`,(0,n.jsx)(e.p,{children:\"For instance, consider a microservices-based application that needs to scale up rapidly during peak traffic hours. In an ephemeral environment, you can simulate this scenario and assess how well your application scales under load, well before deploying it into production. Once the tests are completed, the whole environment can be torn down automatically to free up valuable resources, which could be quite expensive to build, maintain, or configure otherwise.\"}),`\n`,(0,n.jsx)(e.p,{children:\"As you can see, ephemeral environments offer flexibility and control while providing a realistic preview of production conditions. They are undoubtedly a powerful tool in any developer's toolbox.\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"To delve deeper into ephemeral environments, check out Release's insightful article on \",(0,n.jsx)(e.a,{href:\"https://release.com/blog/beyond-k8s-introduction-to-ephemeral-environments\",children:\"Beyond K8s: Introduction to Ephemeral Environments\"}),\".\"]}),`\n`,(0,n.jsxs)(e.h2,{id:\"4-integration-possibilities-with-collaboration-tools-like-github-and-jira\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#4-integration-possibilities-with-collaboration-tools-like-github-and-jira\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.strong,{children:\"4. Integration Possibilities with Collaboration Tools like GitHub and Jira\"})]}),`\n`,(0,n.jsxs)(e.p,{children:[\"In the realm of software development, \",(0,n.jsx)(e.strong,{children:\"GitHub\"}),\" and \",(0,n.jsx)(e.strong,{children:\"Jira\"}),\" stand as titans of collaboration, offering robust platforms for code management and issue tracking, respectively. Ephemeral environments gain an added layer of efficiency when integrated with these tools, streamlining workflows and enhancing productivity.\"]}),`\n`,(0,n.jsxs)(e.h3,{id:\"seamless-integration-with-github\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#seamless-integration-with-github\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.strong,{children:\"Seamless Integration with GitHub\"})]}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"Automated Environment Spin-up\"}),\": Upon a new pull request in GitHub, an ephemeral environment can be automatically created. This provides immediate feedback on how code changes will perform in a live setting.\"]}),`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"Status Checks\"}),\": Integrating ephemeral environments into GitHub's status checks allows developers to see if their environment is ready for review directly from the pull request, ensuring that only fully provisioned environments are tested.\"]}),`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"Bot Notifications\"}),\": Custom bots can comment on pull requests with ephemeral environment URLs and deployment statuses, making it effortless for reviewers to access the latest version of the application.\"]}),`\n`]}),`\n`,(0,n.jsxs)(e.h3,{id:\"streamlining-workflows-with-jira\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#streamlining-workflows-with-jira\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.strong,{children:\"Streamlining Workflows with Jira\"})]}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"Linking Environments to Issues\"}),\": Attach ephemeral environment URLs to relevant Jira tickets. This encourages a clear association between task progress and the actual environment where the feature is implemented.\"]}),`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"Transition Automation\"}),': Trigger the creation or teardown of ephemeral environments based on issue status transitions within Jira. For example, an environment can be spun up when an issue moves to \"In Progress\" and torn down once it reaches \"Done.\"']}),`\n`]}),`\n`,(0,n.jsx)(e.p,{children:\"By weaving ephemeral environments into the fabric of GitHub and Jira workflows, teams harness easy sharing capabilities that complement Agile practices. The result is a streamlined process where code merges and feature developments are transparently connected to dynamic testing environments, fostering an ecosystem where sharing becomes second nature to development processes.\"}),`\n`,(0,n.jsxs)(e.h2,{id:\"5-ensuring-quality-in-ephemeral-environments-through-effective-testing-strategies\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#5-ensuring-quality-in-ephemeral-environments-through-effective-testing-strategies\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.strong,{children:\"5. Ensuring Quality in Ephemeral Environments through Effective Testing Strategies\"})]}),`\n`,(0,n.jsxs)(e.p,{children:[(0,n.jsx)(e.strong,{children:\"Unit tests\"}),\" are the backbone of software testing, but they often fall short in evaluating \",(0,n.jsx)(e.strong,{children:\"system behavior outside unit tests\"}),\". The complexity of modern applications necessitates comprehensive testing strategies that cover more ground. Enter \",(0,n.jsx)(e.strong,{children:\"smoke and integration tests\"}),\"\\u2014essential tools that probe the interactions between various components and ensure seamless deployments.\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"When applied to \",(0,n.jsx)(e.strong,{children:\"live ephemeral environments\"}),\", these tests do more than just verify code correctness; they simulate real-world usage to expose issues that would otherwise remain hidden until production. This is crucial because while unit tests validate individual pieces, smoke and integration tests examine the assembled puzzle, catching errors that occur when all pieces work together.\"]}),`\n`,(0,n.jsxs)(e.h3,{id:\"key-strategies-for-effective-testing-in-ephemeral-environments\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#key-strategies-for-effective-testing-in-ephemeral-environments\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.strong,{children:\"Key Strategies for Effective Testing in Ephemeral Environments:\"})]}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"Parallel Testing:\"}),\" Managing multiple ephemeral environments allows teams to run concurrent tests for different features or branches, significantly reducing the time to release.\"]}),`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"Automated Test Suites:\"}),\" By automating smoke and integration tests within ephemeral environments, developers can quickly identify defects early in the development cycle.\"]}),`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"Dynamic Resource Allocation:\"}),\" Allocating resources on-the-fly to handle a large number of parallel environments ensures that testing is not bottlenecked by infrastructure limitations.\"]}),`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"Continuous Monitoring:\"}),\" Integrating monitoring tools to track the health and performance of ephemeral environments during testing can provide immediate feedback on system stability.\"]}),`\n`]}),`\n`,(0,n.jsxs)(e.p,{children:[\"Incorporating these strategies into your development workflow can transform the quality assurance process. Teams become equipped to deliver robust software at a faster pace by leveraging the unique benefits of ephemeral environments for comprehensive testing. For insights into how this approach can increase developer velocity, consider exploring Release's whitepaper on \",(0,n.jsx)(e.a,{href:\"https://release.com/blog/increase-developer-velocity-by-removing-environment-bottlenecks\",children:\"increasing developer velocity by removing environment bottlenecks\"}),\" using Environments as a Service.\"]}),`\n`,(0,n.jsx)(e.p,{children:\"By ensuring thorough testing in environments that mimic production closely, software teams can confidently push new features, knowing they've been vetted in conditions that match what users will encounter.\"}),`\n`,(0,n.jsxs)(e.h2,{id:\"6-realizing-the-agile-potential-of-ephemeral-environments-in-software-development\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#6-realizing-the-agile-potential-of-ephemeral-environments-in-software-development\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.strong,{children:\"6. Realizing the Agile Potential of Ephemeral Environments in Software Development\"})]}),`\n`,(0,n.jsxs)(e.p,{children:[\"Ephemeral environments play a significant role in fostering \",(0,n.jsx)(e.em,{children:\"Agile/Scrum practices\"}),\" within software development teams. With their dynamic and transient nature, they align perfectly with the iterative and adaptive nature of Agile methodologies.\"]}),`\n`,(0,n.jsxs)(e.h3,{id:\"supporting-continuous-delivery-with-ephemeral-environments\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#supporting-continuous-delivery-with-ephemeral-environments\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.strong,{children:\"Supporting Continuous Delivery with Ephemeral Environments\"})]}),`\n`,(0,n.jsxs)(e.p,{children:[\"One of the key principles of Agile is \",(0,n.jsx)(e.em,{children:\"continuous delivery\"}),\", and ephemeral environments are instrumental in supporting this. They allow constant production-like testing and validation, enabling software updates to be developed, tested, and released rapidly and frequently. As such, developers can:\"]}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsx)(e.li,{children:\"Test code changes immediately in a production-like environment.\"}),`\n`,(0,n.jsx)(e.li,{children:\"Detect and resolve issues early before they reach production.\"}),`\n`,(0,n.jsx)(e.li,{children:\"Accelerate the feedback loop with stakeholders for quicker iterations.\"}),`\n`]}),`\n`,(0,n.jsx)(e.p,{children:\"In essence, ephemeral environments serve as an enabler for continuous delivery \\u2013 one of the cornerstones of Agile.\"}),`\n`,(0,n.jsxs)(e.h3,{id:\"facilitating-iterative-software-development-with-ephemeral-environments\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#facilitating-iterative-software-development-with-ephemeral-environments\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.strong,{children:\"Facilitating Iterative Software Development with Ephemeral Environments\"})]}),`\n`,(0,n.jsxs)(e.p,{children:[\"Another attribute of Agile is its emphasis on \",(0,n.jsx)(e.em,{children:\"iterative software development\"}),\". Here, ephemeral environments shine by facilitating rapid iterations and feedback loops. For instance, developers can share unique URLs of these temporary environments with stakeholders to gather early feedback. The possibility to quickly set up, test, and tear down these environments aligns perfectly with the iterative cycles of Agile development.\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Incorporating ephemeral environments into an Agile workflow thus enhances efficiency while maintaining high quality standards \\u2013 a win-win for any modern software development team.\"}),`\n`,(0,n.jsxs)(e.h2,{id:\"7-the-devops-connection-ephemeral-environments-as-a-catalyst-for-collaboration-and-efficiency\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#7-the-devops-connection-ephemeral-environments-as-a-catalyst-for-collaboration-and-efficiency\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.strong,{children:\"7. The DevOps Connection: Ephemeral Environments as a Catalyst for Collaboration and Efficiency\"})]}),`\n`,(0,n.jsxs)(e.p,{children:[\"Ephemeral environments are a perfect fit for DevOps and Platform Engineering, where teams prioritize automation and collaboration. These dynamic setups are specifically designed to work within a DevOps or PE framework, \",(0,n.jsx)(e.a,{href:\"https://release.com/blog/extend-your-idp-with-environments-for-every-developer-and-every-change\",children:\"bridging the gap between software development and IT operations\"}),\".\"]}),`\n`,(0,n.jsxs)(e.h3,{id:\"how-ephemeral-environments-benefit-devops-and-platform-engineering\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#how-ephemeral-environments-benefit-devops-and-platform-engineering\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.strong,{children:\"How Ephemeral Environments Benefit DevOps and Platform Engineering\"})]}),`\n`,(0,n.jsx)(e.p,{children:\"Here's how ephemeral environments contribute to the success of DevOps and PE:\"}),`\n`,(0,n.jsxs)(e.h4,{id:\"automation-aligned-with-devops\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#automation-aligned-with-devops\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.strong,{children:\"Automation Aligned with DevOps\"})]}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsx)(e.li,{children:\"Ephemeral environments automate the process of creating and tearing down environments, aligning with the DevOps principle of streamlining the software development pipeline.\"}),`\n`,(0,n.jsx)(e.li,{children:\"This automation reduces the manual effort required for environment setup, allowing teams to focus on more important tasks.\"}),`\n`]}),`\n`,(0,n.jsxs)(e.h4,{id:\"collaboration-across-teams-for-platform-engineering\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#collaboration-across-teams-for-platform-engineering\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.strong,{children:\"Collaboration Across Teams for Platform Engineering\"})]}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsx)(e.li,{children:\"Ephemeral environments can be spun up at any stage of the development process for various purposes, such as development or testing.\"}),`\n`,(0,n.jsx)(e.li,{children:\"This shared access promotes collaboration between different teams involved in the software lifecycle, breaking down silos and fostering a culture of teamwork. This platform allows a common place for all self-service environments to be tested, shared, and reviewed.\"}),`\n`]}),`\n`,(0,n.jsxs)(e.h3,{id:\"the-role-of-ephemeral-environments-in-cicd-pipelines\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#the-role-of-ephemeral-environments-in-cicd-pipelines\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.strong,{children:\"The Role of Ephemeral Environments in CI/CD Pipelines\"})]}),`\n`,(0,n.jsx)(e.p,{children:\"Integrating ephemeral environment provisioning into continuous integration (CI) and continuous delivery (CD) pipelines can revolutionize the deployment process. Here's how it works:\"}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsx)(e.li,{children:\"A new ephemeral environment is automatically created by the CI/CD tool/platform whenever there's a code commit or pull request.\"}),`\n`,(0,n.jsx)(e.li,{children:\"Developers receive immediate feedback on their changes in an environment that closely resembles the production setup.\"}),`\n`,(0,n.jsx)(e.li,{children:\"The team can perform tests and quality assurance processes in real-time, ensuring that only thoroughly tested code moves forward in the pipeline.\"}),`\n`]}),`\n`,(0,n.jsx)(e.p,{children:\"This approach allows organizations to make the most out of their DevOps investment by speeding up deployment cycles while maintaining high standards of quality and collaboration.\"}),`\n`,(0,n.jsxs)(e.h2,{id:\"8-configurability-for-rapid-application-development-and-testing-in-ephemeral-environments\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#8-configurability-for-rapid-application-development-and-testing-in-ephemeral-environments\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.strong,{children:\"8. Configurability for Rapid Application Development and Testing in Ephemeral Environments\"})]}),`\n`,(0,n.jsxs)(e.p,{children:[\"Rapid application development and \",(0,n.jsx)(e.a,{href:\"https://release.com/blog/test-environment-a-definition-and-how-to-guide\",children:\"testing\"}),\" thrive on the ability to quickly adapt to different requirements and scenarios. Ephemeral environments extend this flexibility with their inherently dynamic nature. The key to harnessing this potential lies in the configurability of these temporary spaces, which can be tailored to match a myriad of production setups.\"]}),`\n`,(0,n.jsxs)(e.h3,{id:\"how-configurability-enhances-ephemeral-environments\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#how-configurability-enhances-ephemeral-environments\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.strong,{children:\"How Configurability Enhances Ephemeral Environments\"})]}),`\n`,(0,n.jsx)(e.p,{children:\"Here are some ways configurability enhances ephemeral environments for rapid application development and testing:\"}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"Customization of Infrastructure Components\"}),\": Teams can customize OS, servers, memory, and storage parameters to simulate various target environments. This customization ensures that applications are tested under conditions that closely replicate those they will encounter in real-world deployments.\"]}),`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"Utilization of Deployable Artifacts\"}),\": An essential aspect is the use of deployable artifacts, which are pre-built versions of software ready to be launched into the environment. These artifacts are essential for replicating the software deployment process and can range from binary executables to Docker containers, depending on the technology stack utilized.\"]}),`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"Automated Deployment Processes\"}),\": Automation is at the core of ephemeral environments, with pipelines designed to provision infrastructure, deploy applications, and tear down resources without manual intervention. Automated processes not only ensure efficiency but also contribute significantly to consistency across testing scenarios.\"]}),`\n`]}),`\n`,(0,n.jsx)(e.p,{children:\"The streamlined deployment process not only saves time but also reduces errors by minimizing manual setup steps. By integrating these capabilities into ephemeral environments, teams can focus on developing and testing rather than managing infrastructure details.\"}),`\n`,(0,n.jsxs)(e.h3,{id:\"benefits-of-configurability-in-ephemeral-environments\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#benefits-of-configurability-in-ephemeral-environments\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.strong,{children:\"Benefits of Configurability in Ephemeral Environments\"})]}),`\n`,(0,n.jsx)(e.p,{children:\"By optimizing these elements within ephemeral environments, organizations can achieve a significant competitive edge\\u2014accelerating time-to-market while ensuring high-quality standards are met before any release.\"}),`\n`,(0,n.jsxs)(e.h2,{id:\"9-advantages-of-ephemeral-environments-over-traditional-staging-approaches\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#9-advantages-of-ephemeral-environments-over-traditional-staging-approaches\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.strong,{children:\"9. Advantages of Ephemeral Environments over Traditional Staging Approaches\"})]}),`\n`,(0,n.jsxs)(e.h3,{id:\"asynchronous-collaboration-across-time-zones\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#asynchronous-collaboration-across-time-zones\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.strong,{children:\"Asynchronous Collaboration Across Time Zones\"})]}),`\n`,(0,n.jsx)(e.p,{children:\"Ephemeral environments facilitate asynchronous collaboration across distributed teams by providing on-demand access to consistent testing and development environments. This feature is a game-changer for global teams working across different time zones, enabling them to work together seamlessly.\"}),`\n`,(0,n.jsxs)(e.h3,{id:\"cost-effective-infrastructure\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#cost-effective-infrastructure\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.strong,{children:\"Cost-Effective Infrastructure\"})]}),`\n`,(0,n.jsx)(e.p,{children:\"Compared to traditional staging setups that require dedicated infrastructure and maintenance, ephemeral environments offer a more cost-effective solution. Since these environments are only activated when needed and decommissioned after use, they significantly reduce the overhead costs associated with maintaining permanent staging servers.\"}),`\n`,(0,n.jsxs)(e.h3,{id:\"agile-and-scalable\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#agile-and-scalable\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.strong,{children:\"Agile and Scalable\"})]}),`\n`,(0,n.jsx)(e.p,{children:\"Ephemeral environments provide unmatched agility and scalability. Teams can quickly set up, modify, or tear down environments as required, thus facilitating flexible scaling and testing processes. This capability enables companies to adapt rapidly to changing requirements without incurring additional costs or delays.\"}),`\n`,(0,n.jsx)(e.p,{children:\"One key benefit of decreasing cycle time and per-use costs is that productivity and utilization will actually increase. As an example, a single shared environment might support one team for 24 hours of usage costs, but 24 teams or individuals can use one-hour ephemeral environments for the same overall cost. If appropriate auto-scaling is used, resource utilization costs could go to nearly zero when not used after hours or on the weekend, for example. However, utilization and productivity during normal work hours could skyrocket!\"}),`\n`,(0,n.jsxs)(e.h3,{id:\"increased-security-and-reliability\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#increased-security-and-reliability\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.strong,{children:\"Increased Security and Reliability\"})]}),`\n`,(0,n.jsx)(e.p,{children:\"Another advantage of ephemeral environments over traditional staging approaches is their enhanced security and reliability. Since each environment is isolated and short-lived, the risk of lingering vulnerabilities or data breaches is minimized. Moreover, these dynamic environments can be replicated exactly as per production standards, ensuring reliable testing outcomes. Not only that, but security tests, penetration tests, and destructive testing can happen without affecting the live production site, enabling the security posture to be verified and tested before reaching production. This is a massive boost in confidence on security practices that most production environments miss out on.\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"For a deeper dive into the benefits of ephemeral environments as part of \",(0,n.jsx)(e.a,{href:\"https://release.com/blog/environments-as-a-service-eaas-top-3-benefits\",children:\"Environments as a Service (EaaS) offerings\"}),\", you might find this article helpful.\"]}),`\n`,(0,n.jsx)(e.p,{children:\"With these advantages in mind, it's clear why ephemeral environments are becoming an integral part of modern software development workflows.\"}),`\n`,(0,n.jsxs)(e.h2,{id:\"why-should-you-care\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#why-should-you-care\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.strong,{children:\"Why should you care?\"}),\"\\xA0\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Ephemeral environments are an innovative approach to software development that can greatly benefit your team. By creating temporary environments that closely resemble your production settings, you can streamline your development workflow and improve collaboration among team members, and make sure you stay competitive in your industry.\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:\"Here are some key takeaways from this article:\"}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"Streamline your development workflow:\"}),\" Ephemeral environments allow for faster iteration cycles, as you can quickly spin up new environments for testing and debugging.\"]}),`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"Enhance collaboration:\"}),\" With on-demand setups, developers, QA teams, and stakeholders can easily access and work in the same environment, reducing communication barriers.\"]}),`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"Improve testing strategies:\"}),\" Ephemeral environments provide an isolated space for thorough validation of system behavior before deploying to production.\"]}),`\n`]}),`\n`,(0,n.jsxs)(e.p,{children:[\"Ready to give ephemeral environments a try? Check out \",(0,n.jsx)(e.a,{href:\"https://www.example.com/\",children:\"Release\"}),` - a platform specifically designed for managing ephemeral environments.\n`,(0,n.jsx)(e.strong,{children:\"Sign up for a free trial\"}),\" today and see how Release can help your team achieve greater agility and flexibility in your software development process.\"]}),`\n`,(0,n.jsx)(e.p,{children:\"\\u200D\"}),`\n`,(0,n.jsx)(e.p,{children:\"\\u200D\"})]})}function k(i={}){let{wrapper:e}=i.components||{};return e?(0,n.jsx)(e,Object.assign({},i,{children:(0,n.jsx)(h,i)})):h(i)}var E=k;return w(N);})();\n;return Component;"
        },
        "_id": "blog/posts/ephemeral-environments-9-tips-for-seamless-deployment.mdx",
        "_raw": {
          "sourceFilePath": "blog/posts/ephemeral-environments-9-tips-for-seamless-deployment.mdx",
          "sourceFileName": "ephemeral-environments-9-tips-for-seamless-deployment.mdx",
          "sourceFileDir": "blog/posts",
          "contentType": "mdx",
          "flattenedPath": "blog/posts/ephemeral-environments-9-tips-for-seamless-deployment"
        },
        "type": "BlogPost",
        "computedSlug": "ephemeral-environments-9-tips-for-seamless-deployment"
      },
      "documentHash": "1739393595018",
      "hasWarnings": false,
      "documentTypeName": "BlogPost"
    },
    "blog/posts/extend-your-idp-with-environments-for-every-developer-and-every-change.mdx": {
      "document": {
        "title": "Extend your IDP with Environments for Every Developer and Every Change",
        "summary": "Explore environments as a way to enhance your IDP and elevate the development process.",
        "publishDate": "Thu Oct 19 2023 19:02:24 GMT+0000 (Coordinated Universal Time)",
        "author": "sylvia-fronczak",
        "readingTime": 10,
        "categories": [
          "platform-engineering",
          "product"
        ],
        "mainImage": "/blog-images/09075670f023d3719e9920f0dfb758ec.jpg",
        "imageAlt": "Extend your IDP with Environments for Every Developer and Every Change",
        "showCTA": true,
        "ctaCopy": "Unlock flexible and automated environment management to eliminate configuration drift and testing bottlenecks with Release.",
        "ctaLink": "https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=extend-your-idp-with-environments-for-every-developer-and-every-change",
        "relatedPosts": [
          ""
        ],
        "ogImage": "/blog-images/09075670f023d3719e9920f0dfb758ec.jpg",
        "excerpt": "Explore environments as a way to enhance your IDP and elevate the development process.",
        "tags": [
          "platform-engineering",
          "product"
        ],
        "ctaButton": "Try Release for Free",
        "body": {
          "raw": "\nIn our [previous posts](https://release.com/blog/components-of-a-successful-idp-build-a-product-your-developers-actually-want-to-use), we talked about how the internal developer platform (IDP) is comprised of many components and tools working together to optimize your development team’s workflow. Some of these components, like code repositories, CI/CD, test environments, and development tools, are must-haves.\n\nFor these must-have components, you have two choices. You can cover the basics and call it good, or you can look at your organization’s needs and enhance the components to provide your teams with extra leverage.\n\nIn this post, we’ll talk about environments and how you can extend and enhance your IDP through flexible and automated environment management that mimics your production environment. This flexibility will enable your developers to isolate changes, reduce configuration-drift-related bugs, remove testing bottlenecks, and improve time-to-market in complex environments.\n\nWith Release, you can provide on-demand [environments](https://release.com/get-started) for every change and every pull request (PR), providing multiple efficiencies for your development team.\n\n### Why Do We Need More Flexible Environment Management?\n\nTo begin, let’s talk about common problems that developers experience with static environments and local environments.\n\n#### **#1 It Works on My Machine**\n\nEnvironment discrepancies have been a software development headache since we stopped coding live in production. You may not remember that, but on mainframes, it wasn’t unusual to hop into production and change code live. To be fair, even non-mainframe programmers occasionally would hop onto production servers to make small HTML or JavaScript tweaks. Yes, I know, scary. But in small orgs with few controls, it did happen. Thankfully, we’ve evolved our development practices, and we know we don’t want to go back to those days.\n\nHowever, environment differences continue to plague us with bugs and replication difficulties. Whether it’s differences in the operating system, container configuration, or dependencies like the database, caches, and queues, all of these can cause churning due to unexpected behaviors in each environment, leading to the dreaded problem “but it works on my machine”.\n\n#### **#2 The Overhead of Microservices**\n\nIf you’ve ever been on a team that works in a microservices environment, you already know the pain of trying to run systems locally.\n\nYears ago, I frequently had to start up three, five, eight, or more microservices on my laptop to validate a change, test functionality, or debug a production problem. The fan noise coming off the laptop could be startling, and the heat emanating from the machine could warm a small room in the winter. It was slow, painful, and finicky to get everything running consistently.\n\nIn addition to the local resource burden, testing was often done in static test or staging environments and required multiple validations and deploys to ensure the right versions of APIs and implementation were where you thought they should be.\n\n#### **#3 Load Testing Challenges**\n\nFor operations like load testing or performance benchmarking, we turn to static load testing environments. In previous organizations, I’d see folks scheduling time in environments to conduct load tests, often having to wait weeks for the environment and tools to free up to complete testing.\n\nNot being able to quickly access environments that allow proper load testing delays critical tests and makes automation of these load tests difficult.\n\n![](/blog-images/b035f8ba3bb3806ba73ca14a239184b4.png)\n\n#### **#4 Slow Pivots to Changing Priorities**\n\nPicture this. A developer has been coding a new feature for several days. Though they’re committing and pushing frequently, they still have their local development set up specifically for this work. They’ve set up the data they’re testing with and ensured the environment is right for this particular change. Maybe they’ve even changed their local environment configuration to optimize it for their current task. But suddenly, a hot bug comes in, and they need to switch and focus on shipping a fix.\n\nWithout fast ephemeral development environments, they not only have to check out a new branch and switch focus, but potentially, they’ll have to undo data schema changes they’re in the middle of and then recreate the data for the production scenario on their machine.\n\nIt’s bad enough that your engineering teams have to context switch at times. But switching over local dev environments or test environments to hot priorities shouldn’t be a big ordeal.\n\n#### **#5 Data Degradation**\n\nWith static environments, after a while the data or the whole environment becomes degraded. Sometimes, this is due to testing in that environment with different versions of the code and not as much backward compatibility as we need. Other times, it’s because we’ve had to contrive data into a particular state to test or investigate issues.\n\nRegardless of the reason, static environments experience data degradation and require periodic cleanup. For example, I worked in a group that would go through routine quarterly weeklong data refreshes to get non-production environments to a healthy and working state in order to support further rounds of testing, for the whole next quarter.\n\nIf we treat environments as static, long-living beasts, we aren’t primed to wipe them out and start fresh frequently.\n\n#### **#6 Configuration Drift**\n\nConfiguration drift occurs when our configuration between environments falls out of sync. This can include application configuration, infrastructure configurations or versions, and network differences between environments.\n\nThis is especially true when working in environments where most configurations are managed manually.\n\nConfiguration drift can cause pain when testing, reproducing bugs, or making assumptions about what an environment’s configuration looks like.\n\n#### **#7 Slow Feedback Loops**\n\nShared environments can make testing and feedback loops slow and inefficient because of waiting for environments or debugging issues unrelated to your changes.\n\n#### **#8 Inconsistent Security Measures**\n\nSimilar to configuration drift, security constraints in some environments are not replicated in local or test environments, causing unexpected bugs or security flaws in production.\n\nThough we can’t replicate everything between all environments, key differences can mean more bugs and incidents once the code is deployed to production.\n\n### How Can Development Environments Help?\n\nBefore we dive into the benefits of ephemeral development environments, let’s review typical environment types:\n\n1.  The **developer’s local environment is** where we write code and tests. The audience is limited to the developer(s) on this machine.\n2.  **Static non-production environment**s like test, QA, UAT (user acceptance testing), or SIT (system integration testing), where additional testing is completed. These environments are used by development teams, QA/testing, and both internal and external partner teams for testing and validation. These can also be used for demos both within and outside the organization.\n3.  **Production**, where your product is live for your customers.\n\nWhen bottlenecks emerge with testing in static non-prod environments, organizations might expand the number of environments, hoping it relieves some of the pressure. This does temporarily provide folks with more test environments, but it also creates more of the problems we saw in the previous section.\n\nInstead of adding yet another static environment, consider investing in automated, fine-grained, flexible, and ephemeral environments. With these, spinning up environments as part of your development workflow and IDP takes little to no effort after the initial investment. Your team has purposeful environments for quick bug testing or feature deployment tied to a PR or branch of code.\n\nSo how do these environments solve the problems we see with static environments? Let’s look at some of the answers.\n\n#### **Isolation and Consistency**\n\nWhen testing changes, your development team won’t struggle with isolating one change’s impact from others in static test environments. They’ll work in a production-like setup, and then the CI/CD pipeline will create an environment for further validation/testing after the pull request is made.\n\n#### **Reduced Configuration Drift**\n\nIn long-lived environments, either on a developer’s machine or in a static testing environment, we can’t eliminate drift. However, if we’re frequently rebuilding environments for each change, we keep configurations in sync with production and provide a consistent experience across environments.\n\n![](/blog-images/832e584189dad339f0de0d28182a6e1c.png)\n\n#### **Dependency Management**\n\nWith ephemeral and on-demand environments, you can provide easy dependency management. Developers won’t need to manage their own dependencies (the databases, caches, and queues mentioned above) while developing and testing their app. It can all be included in the release environment setup.\n\nFurthermore, if developers are specifically working on changes to dependencies, they’ll have a full environment to validate their changes.\n\n#### **Fast Feedback**\n\nFirst, automated tests are necessary, and you can’t go without them. But they’re not bulletproof. You can’t test what you don’t expect. With environments set up for each story or issue, you can get fast feedback by starting up an environment, testing your change, and exploring side effects. Then you can receive that feedback immediately instead of waiting to get the change into a test environment.\n\n#### **Production-Like Data**\n\nWith on-demand environments, your development team will have the data and scenarios they need without excessive data setup. Features like [Instant Datasets](https://release.com/product/instant-datasets) preload a pool of production-like datasets (sanitized and truncated, if needed) and have even terabytes of data available in minutes.\n\n#### **Consistent Development Workflows**\n\nFinally, with ready-to-use and fully automated environments, development teams will have a consistent experience rolling out their changes to each environment in your CI/CD pipeline.\n\n### How Do Ephemeral Environments Tie In with Your IDP?\n\nYour IDP consists of multiple components and integrations. Once you have the essential components like CI/CD, where you’re automating basic workflows, then you’re ready to level up.\n\nIf your organization is small and you don’t yet have environmental bottlenecks, adding ephemeral environments might not be your top priority on enhancing your IDP.\n\nOn the other hand, if your teams experience pain or bottlenecks involved in using static development or non-production environments, then it’s time to experiment with [Release](https://release.com/signup). Incorporating automated on-demand ephemeral environments provides benefits like consistency, dependency management, and fast feedback. This helps your teams get new features into production faster and with fewer defects. You’ll manage, provision, and eventually destroy environments quickly and automatically, so that you get the most use out of them when your development team needs them.\n\nIn short, IDPs integrate developer environments into the development workflow. They improve efficiency, collaboration, and quality.\n\n‍*This post was written by Sylvia Fronczak.* [_Sylvia_](https://sylviafronczak.com/) _is a software developer who has worked in various industries with various software methodologies. She’s currently focused on design practices that the whole team can own, understand, and evolve over time._\n",
          "code": "var Component=(()=>{var h=Object.create;var a=Object.defineProperty;var m=Object.getOwnPropertyDescriptor;var p=Object.getOwnPropertyNames;var u=Object.getPrototypeOf,g=Object.prototype.hasOwnProperty;var v=(t,e)=>()=>(e||t((e={exports:{}}).exports,e),e.exports),f=(t,e)=>{for(var o in e)a(t,o,{get:e[o],enumerable:!0})},s=(t,e,o,r)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let i of p(e))!g.call(t,i)&&i!==o&&a(t,i,{get:()=>e[i],enumerable:!(r=m(e,i))||r.enumerable});return t};var y=(t,e,o)=>(o=t!=null?h(u(t)):{},s(e||!t||!t.__esModule?a(o,\"default\",{value:t,enumerable:!0}):o,t)),w=t=>s(a({},\"__esModule\",{value:!0}),t);var l=v((D,c)=>{c.exports=_jsx_runtime});var I={};f(I,{default:()=>x,frontmatter:()=>b});var n=y(l()),b={title:\"Extend your IDP with Environments for Every Developer and Every Change\",summary:\"Explore environments as a way to enhance your IDP and elevate the development process.\",publishDate:\"Thu Oct 19 2023 19:02:24 GMT+0000 (Coordinated Universal Time)\",author:\"sylvia-fronczak\",readingTime:10,categories:[\"platform-engineering\",\"product\"],mainImage:\"/blog-images/09075670f023d3719e9920f0dfb758ec.jpg\",imageAlt:\"Extend your IDP with Environments for Every Developer and Every Change\",showCTA:!0,ctaCopy:\"Unlock flexible and automated environment management to eliminate configuration drift and testing bottlenecks with Release.\",ctaLink:\"https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=extend-your-idp-with-environments-for-every-developer-and-every-change\",relatedPosts:[\"\"],ogImage:\"/blog-images/09075670f023d3719e9920f0dfb758ec.jpg\",excerpt:\"Explore environments as a way to enhance your IDP and elevate the development process.\",tags:[\"platform-engineering\",\"product\"],ctaButton:\"Try Release for Free\"};function d(t){let e=Object.assign({p:\"p\",a:\"a\",h3:\"h3\",span:\"span\",h4:\"h4\",strong:\"strong\",img:\"img\",ol:\"ol\",li:\"li\",em:\"em\"},t.components);return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsxs)(e.p,{children:[\"In our \",(0,n.jsx)(e.a,{href:\"https://release.com/blog/components-of-a-successful-idp-build-a-product-your-developers-actually-want-to-use\",children:\"previous posts\"}),\", we talked about how the internal developer platform (IDP) is comprised of many components and tools working together to optimize your development team\\u2019s workflow. Some of these components, like code repositories, CI/CD, test environments, and development tools, are must-haves.\"]}),`\n`,(0,n.jsx)(e.p,{children:\"For these must-have components, you have two choices. You can cover the basics and call it good, or you can look at your organization\\u2019s needs and enhance the components to provide your teams with extra leverage.\"}),`\n`,(0,n.jsx)(e.p,{children:\"In this post, we\\u2019ll talk about environments and how you can extend and enhance your IDP through flexible and automated environment management that mimics your production environment. This flexibility will enable your developers to isolate changes, reduce configuration-drift-related bugs, remove testing bottlenecks, and improve time-to-market in complex environments.\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"With Release, you can provide on-demand \",(0,n.jsx)(e.a,{href:\"https://release.com/get-started\",children:\"environments\"}),\" for every change and every pull request (PR), providing multiple efficiencies for your development team.\"]}),`\n`,(0,n.jsxs)(e.h3,{id:\"why-do-we-need-more-flexible-environment-management\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#why-do-we-need-more-flexible-environment-management\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Why Do We Need More Flexible Environment Management?\"]}),`\n`,(0,n.jsx)(e.p,{children:\"To begin, let\\u2019s talk about common problems that developers experience with static environments and local environments.\"}),`\n`,(0,n.jsxs)(e.h4,{id:\"1-it-works-on-my-machine\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#1-it-works-on-my-machine\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.strong,{children:\"#1 It Works on My Machine\"})]}),`\n`,(0,n.jsx)(e.p,{children:\"Environment discrepancies have been a software development headache since we stopped coding live in production. You may not remember that, but on mainframes, it wasn\\u2019t unusual to hop into production and change code live. To be fair, even non-mainframe programmers occasionally would hop onto production servers to make small HTML or JavaScript tweaks. Yes, I know, scary. But in small orgs with few controls, it did happen. Thankfully, we\\u2019ve evolved our development practices, and we know we don\\u2019t want to go back to those days.\"}),`\n`,(0,n.jsx)(e.p,{children:\"However, environment differences continue to plague us with bugs and replication difficulties. Whether it\\u2019s differences in the operating system, container configuration, or dependencies like the database, caches, and queues, all of these can cause churning due to unexpected behaviors in each environment, leading to the dreaded problem \\u201Cbut it works on my machine\\u201D.\"}),`\n`,(0,n.jsxs)(e.h4,{id:\"2-the-overhead-of-microservices\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#2-the-overhead-of-microservices\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.strong,{children:\"#2 The Overhead of Microservices\"})]}),`\n`,(0,n.jsx)(e.p,{children:\"If you\\u2019ve ever been on a team that works in a microservices environment, you already know the pain of trying to run systems locally.\"}),`\n`,(0,n.jsx)(e.p,{children:\"Years ago, I frequently had to start up three, five, eight, or more microservices on my laptop to validate a change, test functionality, or debug a production problem. The fan noise coming off the laptop could be startling, and the heat emanating from the machine could warm a small room in the winter. It was slow, painful, and finicky to get everything running consistently.\"}),`\n`,(0,n.jsx)(e.p,{children:\"In addition to the local resource burden, testing was often done in static test or staging environments and required multiple validations and deploys to ensure the right versions of APIs and implementation were where you thought they should be.\"}),`\n`,(0,n.jsxs)(e.h4,{id:\"3-load-testing-challenges\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#3-load-testing-challenges\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.strong,{children:\"#3 Load Testing Challenges\"})]}),`\n`,(0,n.jsx)(e.p,{children:\"For operations like load testing or performance benchmarking, we turn to static load testing environments. In previous organizations, I\\u2019d see folks scheduling time in environments to conduct load tests, often having to wait weeks for the environment and tools to free up to complete testing.\"}),`\n`,(0,n.jsx)(e.p,{children:\"Not being able to quickly access environments that allow proper load testing delays critical tests and makes automation of these load tests difficult.\"}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/b035f8ba3bb3806ba73ca14a239184b4.png\",alt:\"\"})}),`\n`,(0,n.jsxs)(e.h4,{id:\"4-slow-pivots-to-changing-priorities\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#4-slow-pivots-to-changing-priorities\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.strong,{children:\"#4 Slow Pivots to Changing Priorities\"})]}),`\n`,(0,n.jsx)(e.p,{children:\"Picture this. A developer has been coding a new feature for several days. Though they\\u2019re committing and pushing frequently, they still have their local development set up specifically for this work. They\\u2019ve set up the data they\\u2019re testing with and ensured the environment is right for this particular change. Maybe they\\u2019ve even changed their local environment configuration to optimize it for their current task. But suddenly, a hot bug comes in, and they need to switch and focus on shipping a fix.\"}),`\n`,(0,n.jsx)(e.p,{children:\"Without fast ephemeral development environments, they not only have to check out a new branch and switch focus, but potentially, they\\u2019ll have to undo data schema changes they\\u2019re in the middle of and then recreate the data for the production scenario on their machine.\"}),`\n`,(0,n.jsx)(e.p,{children:\"It\\u2019s bad enough that your engineering teams have to context switch at times. But switching over local dev environments or test environments to hot priorities shouldn\\u2019t be a big ordeal.\"}),`\n`,(0,n.jsxs)(e.h4,{id:\"5-data-degradation\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#5-data-degradation\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.strong,{children:\"#5 Data Degradation\"})]}),`\n`,(0,n.jsx)(e.p,{children:\"With static environments, after a while the data or the whole environment becomes degraded. Sometimes, this is due to testing in that environment with different versions of the code and not as much backward compatibility as we need. Other times, it\\u2019s because we\\u2019ve had to contrive data into a particular state to test or investigate issues.\"}),`\n`,(0,n.jsx)(e.p,{children:\"Regardless of the reason, static environments experience data degradation and require periodic cleanup. For example, I worked in a group that would go through routine quarterly weeklong data refreshes to get non-production environments to a healthy and working state in order to support further rounds of testing, for the whole next quarter.\"}),`\n`,(0,n.jsx)(e.p,{children:\"If we treat environments as static, long-living beasts, we aren\\u2019t primed to wipe them out and start fresh frequently.\"}),`\n`,(0,n.jsxs)(e.h4,{id:\"6-configuration-drift\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#6-configuration-drift\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.strong,{children:\"#6 Configuration Drift\"})]}),`\n`,(0,n.jsx)(e.p,{children:\"Configuration drift occurs when our configuration between environments falls out of sync. This can include application configuration, infrastructure configurations or versions, and network differences between environments.\"}),`\n`,(0,n.jsx)(e.p,{children:\"This is especially true when working in environments where most configurations are managed manually.\"}),`\n`,(0,n.jsx)(e.p,{children:\"Configuration drift can cause pain when testing, reproducing bugs, or making assumptions about what an environment\\u2019s configuration looks like.\"}),`\n`,(0,n.jsxs)(e.h4,{id:\"7-slow-feedback-loops\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#7-slow-feedback-loops\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.strong,{children:\"#7 Slow Feedback Loops\"})]}),`\n`,(0,n.jsx)(e.p,{children:\"Shared environments can make testing and feedback loops slow and inefficient because of waiting for environments or debugging issues unrelated to your changes.\"}),`\n`,(0,n.jsxs)(e.h4,{id:\"8-inconsistent-security-measures\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#8-inconsistent-security-measures\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.strong,{children:\"#8 Inconsistent Security Measures\"})]}),`\n`,(0,n.jsx)(e.p,{children:\"Similar to configuration drift, security constraints in some environments are not replicated in local or test environments, causing unexpected bugs or security flaws in production.\"}),`\n`,(0,n.jsx)(e.p,{children:\"Though we can\\u2019t replicate everything between all environments, key differences can mean more bugs and incidents once the code is deployed to production.\"}),`\n`,(0,n.jsxs)(e.h3,{id:\"how-can-development-environments-help\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#how-can-development-environments-help\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"How Can Development Environments Help?\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Before we dive into the benefits of ephemeral development environments, let\\u2019s review typical environment types:\"}),`\n`,(0,n.jsxs)(e.ol,{children:[`\n`,(0,n.jsxs)(e.li,{children:[\"The \",(0,n.jsx)(e.strong,{children:\"developer\\u2019s local environment is\"}),\" where we write code and tests. The audience is limited to the developer(s) on this machine.\"]}),`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"Static non-production environment\"}),\"s like test, QA, UAT (user acceptance testing), or SIT (system integration testing), where additional testing is completed. These environments are used by development teams, QA/testing, and both internal and external partner teams for testing and validation. These can also be used for demos both within and outside the organization.\"]}),`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"Production\"}),\", where your product is live for your customers.\"]}),`\n`]}),`\n`,(0,n.jsx)(e.p,{children:\"When bottlenecks emerge with testing in static non-prod environments, organizations might expand the number of environments, hoping it relieves some of the pressure. This does temporarily provide folks with more test environments, but it also creates more of the problems we saw in the previous section.\"}),`\n`,(0,n.jsx)(e.p,{children:\"Instead of adding yet another static environment, consider investing in automated, fine-grained, flexible, and ephemeral environments. With these, spinning up environments as part of your development workflow and IDP takes little to no effort after the initial investment. Your team has purposeful environments for quick bug testing or feature deployment tied to a PR or branch of code.\"}),`\n`,(0,n.jsx)(e.p,{children:\"So how do these environments solve the problems we see with static environments? Let\\u2019s look at some of the answers.\"}),`\n`,(0,n.jsxs)(e.h4,{id:\"isolation-and-consistency\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#isolation-and-consistency\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.strong,{children:\"Isolation and Consistency\"})]}),`\n`,(0,n.jsx)(e.p,{children:\"When testing changes, your development team won\\u2019t struggle with isolating one change\\u2019s impact from others in static test environments. They\\u2019ll work in a production-like setup, and then the CI/CD pipeline will create an environment for further validation/testing after the pull request is made.\"}),`\n`,(0,n.jsxs)(e.h4,{id:\"reduced-configuration-drift\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#reduced-configuration-drift\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.strong,{children:\"Reduced Configuration Drift\"})]}),`\n`,(0,n.jsx)(e.p,{children:\"In long-lived environments, either on a developer\\u2019s machine or in a static testing environment, we can\\u2019t eliminate drift. However, if we\\u2019re frequently rebuilding environments for each change, we keep configurations in sync with production and provide a consistent experience across environments.\"}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/832e584189dad339f0de0d28182a6e1c.png\",alt:\"\"})}),`\n`,(0,n.jsxs)(e.h4,{id:\"dependency-management\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#dependency-management\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.strong,{children:\"Dependency Management\"})]}),`\n`,(0,n.jsx)(e.p,{children:\"With ephemeral and on-demand environments, you can provide easy dependency management. Developers won\\u2019t need to manage their own dependencies (the databases, caches, and queues mentioned above) while developing and testing their app. It can all be included in the release environment setup.\"}),`\n`,(0,n.jsx)(e.p,{children:\"Furthermore, if developers are specifically working on changes to dependencies, they\\u2019ll have a full environment to validate their changes.\"}),`\n`,(0,n.jsxs)(e.h4,{id:\"fast-feedback\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#fast-feedback\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.strong,{children:\"Fast Feedback\"})]}),`\n`,(0,n.jsx)(e.p,{children:\"First, automated tests are necessary, and you can\\u2019t go without them. But they\\u2019re not bulletproof. You can\\u2019t test what you don\\u2019t expect. With environments set up for each story or issue, you can get fast feedback by starting up an environment, testing your change, and exploring side effects. Then you can receive that feedback immediately instead of waiting to get the change into a test environment.\"}),`\n`,(0,n.jsxs)(e.h4,{id:\"production-like-data\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#production-like-data\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.strong,{children:\"Production-Like Data\"})]}),`\n`,(0,n.jsxs)(e.p,{children:[\"With on-demand environments, your development team will have the data and scenarios they need without excessive data setup. Features like \",(0,n.jsx)(e.a,{href:\"https://release.com/product/instant-datasets\",children:\"Instant Datasets\"}),\" preload a pool of production-like datasets (sanitized and truncated, if needed) and have even terabytes of data available in minutes.\"]}),`\n`,(0,n.jsxs)(e.h4,{id:\"consistent-development-workflows\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#consistent-development-workflows\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.strong,{children:\"Consistent Development Workflows\"})]}),`\n`,(0,n.jsx)(e.p,{children:\"Finally, with ready-to-use and fully automated environments, development teams will have a consistent experience rolling out their changes to each environment in your CI/CD pipeline.\"}),`\n`,(0,n.jsxs)(e.h3,{id:\"how-do-ephemeral-environments-tie-in-with-your-idp\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#how-do-ephemeral-environments-tie-in-with-your-idp\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"How Do Ephemeral Environments Tie In with Your IDP?\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Your IDP consists of multiple components and integrations. Once you have the essential components like CI/CD, where you\\u2019re automating basic workflows, then you\\u2019re ready to level up.\"}),`\n`,(0,n.jsx)(e.p,{children:\"If your organization is small and you don\\u2019t yet have environmental bottlenecks, adding ephemeral environments might not be your top priority on enhancing your IDP.\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"On the other hand, if your teams experience pain or bottlenecks involved in using static development or non-production environments, then it\\u2019s time to experiment with \",(0,n.jsx)(e.a,{href:\"https://release.com/signup\",children:\"Release\"}),\". Incorporating automated on-demand ephemeral environments provides benefits like consistency, dependency management, and fast feedback. This helps your teams get new features into production faster and with fewer defects. You\\u2019ll manage, provision, and eventually destroy environments quickly and automatically, so that you get the most use out of them when your development team needs them.\"]}),`\n`,(0,n.jsx)(e.p,{children:\"In short, IDPs integrate developer environments into the development workflow. They improve efficiency, collaboration, and quality.\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"\\u200D\",(0,n.jsx)(e.em,{children:\"This post was written by Sylvia Fronczak.\"}),\" \",(0,n.jsx)(e.a,{href:\"https://sylviafronczak.com/\",children:(0,n.jsx)(e.em,{children:\"Sylvia\"})}),\" \",(0,n.jsx)(e.em,{children:\"is a software developer who has worked in various industries with various software methodologies. She\\u2019s currently focused on design practices that the whole team can own, understand, and evolve over time.\"})]})]})}function k(t={}){let{wrapper:e}=t.components||{};return e?(0,n.jsx)(e,Object.assign({},t,{children:(0,n.jsx)(d,t)})):d(t)}var x=k;return w(I);})();\n;return Component;"
        },
        "_id": "blog/posts/extend-your-idp-with-environments-for-every-developer-and-every-change.mdx",
        "_raw": {
          "sourceFilePath": "blog/posts/extend-your-idp-with-environments-for-every-developer-and-every-change.mdx",
          "sourceFileName": "extend-your-idp-with-environments-for-every-developer-and-every-change.mdx",
          "sourceFileDir": "blog/posts",
          "contentType": "mdx",
          "flattenedPath": "blog/posts/extend-your-idp-with-environments-for-every-developer-and-every-change"
        },
        "type": "BlogPost",
        "computedSlug": "extend-your-idp-with-environments-for-every-developer-and-every-change"
      },
      "documentHash": "1739393595018",
      "hasWarnings": false,
      "documentTypeName": "BlogPost"
    },
    "blog/posts/extending-releasehub-environments-with-terraform.mdx": {
      "document": {
        "title": "Extending Release Environments with Terraform",
        "summary": "This blog shows how to utilize Terraform alongside Release to provision additional resources in your cloud account.",
        "publishDate": "Tue Aug 09 2022 23:58:58 GMT+0000 (Coordinated Universal Time)",
        "author": "josh-dirkx",
        "readingTime": 5,
        "categories": [
          "platform-engineering",
          "product"
        ],
        "mainImage": "/blog-images/c710bf4817cae68e73eeb96d596fa5a4.jpg",
        "imageAlt": "Extending Release Environments with Terraform",
        "showCTA": true,
        "ctaCopy": "Simplify managing Terraform environments with Release's ephemeral environments for seamless collaboration and consistent deployments.",
        "ctaLink": "https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=extending-releasehub-environments-with-terraform",
        "relatedPosts": [
          ""
        ],
        "ogImage": "/blog-images/c710bf4817cae68e73eeb96d596fa5a4.jpg",
        "excerpt": "This blog shows how to utilize Terraform alongside Release to provision additional resources in your cloud account.",
        "tags": [
          "platform-engineering",
          "product"
        ],
        "ctaButton": "Try Release for Free",
        "body": {
          "raw": "\n[  \n](https://app.diagrams.net/?page-id=4EaHbZaufD5BJVQ07xAE&scale=auto#G1ZjCA67a8oFC8ztyAz2rx3ehoodHwKE2D)\n\n![](/blog-images/23436a0da5744c17b36da806862964fc.png)\n\n[Release](https://release.com/) offers the ability to easily reproduce your production environment _N_ times in your cloud account. The construction of environments can be accomplished using our website, CLI or API and are useful for a variety of purposes - from quality assurance testing and user acceptance to sales demonstrations and penetration testing.\n\nEnvironments today typically extend beyond a set of Docker images to include a variety of resources from your chosen cloud provider. In order to consistently get those resources alongside your Docker images when building environments, the instructions to get those resources instantiated must be codified somewhere in your application with some flavor of Infrastructure-as-Code (IaC).\n\nA recently launched abstraction within the Release Application Template removes the previous requirements of packaging your Terraform into a Dockerfile, creating the scripts to apply/update/teardown your resources, and managing the variables required to namespace appropriately. Using this new abstraction, the process becomes much more streamlined. Write your Terraform, tell us where to find it, and then tell us when to execute it.\n\n### What is Terraform?\n\n[Terraform](https://www.terraform.io/) is declarative Infrastructure as Code. Said another way, it is a way of programmatically specifying what resources you need created in your cloud provider.\n\nThere are two approaches to IaC - declarative (what) versus imperative (how). Declarative focuses on the desired end state; you tell the system what resources you want provisioned and it figures out how to get you there. Imperative, on the other hand, focuses on the instruction set to get to your end state; you are responsible for maintaining the order of operations required to get to your desired end state.\n\nAnother important concept behind IaC is state and state management. Terraform is no exception here, by default it uses JSON in a local file to persist the state of real world resources created by it. Configurations can be added to allow for remote storage, such as in S3.\n\n### Why should I use Terraform?\n\nTerraform, or any IaC, is an important piece of the puzzle to modern, cloud infrastructure stacks. Codifying the instructions to your production environment allows you to increase visibility and auditability of changes - alterations to your stack are now managed within your source control, allowing you to use change requests for granular inspection of changes before they are applied.\n\nYour infrastructure becomes repeatable, predictable and consistent. Running the _terraform apply_ command will produce the same results every time. This stands in contrast to the manual process of clicking through a web console based on a document containing instructions, or from memory.\n\nIn the event of a full or partial failure of your application, you can quickly and easily replace _everything_.\n\n### _How_ do I use Terraform with Release?\n\n‍*The following example assumes a project layout as outlined in the image below*\n\n![](/blog-images/ba50f60eb0679702ad5c7fae80ba5645.png)\n\nWhen writing your Terraform, it's important to keep in mind that you want it to be repeatable without resource naming collisions, regardless of how many times you execute the same combination of resources. This can be achieved by using dynamic values to namespace resources. Release makes available to you all of the Release generated environment variables for use within your _.tfvars_ file, allowing you to use the environments' context as the namespacing material. For example, you might have the following inside of your _.tfvars_ file\n\n```bash\n# .release/lambda.tfvars\nnamespace = \"${RELEASE_ACCOUNT_ID}/${RELEASE_ENV_ID}/\"\n```\n\n‍\n\nWhen specifying the name of a resource, we'll use a Lambda as an example, you would reference that namespace in the name, like below\n\n```hcl\nresource \"aws_lambda_function\" \"lambda\" {\n  function_name = \"${vars.namespace}/lambda\"\n  # ...\n}\n```\n\n‍\n\nThe inclusion of the environment_id ensures that each time this Terraform is executed by Release, it will be unique.\n\nWIthin the Application Template for a given Release Application, there is an abstraction over IaC providers, such as Terraform, that we can leverage. For the purposes of this demonstration, we will be using Terraform in conjunction with AWS resources.\n\nA new section has been added to the template at the top level - infrastructure. This section takes an array with a few required and optional fields. Required fields are name (this is how you will reference the aforementioned Terraform in the workflows section) and type (for now, the only supported type is Terraform but over time we will add abstractions for Pulumi, CDK, etc.). Optionally, you can also specify a directory (where to find the Terraform) and values (where to find environment variables). By default, if no directory is specified then Release will assume your Terraform is at the root of the repository and if no values file is given, we assume this file does not exist.\n\nWe need to add a few things to our Application Template to support this. We need to first tell Release what to build, then we specify when we execute it.\n\nTo tell Release what we need it to build, we add a new array item to the infrastructure key in our Application Template.\n\n‍\n\n\\# Application Template\n\ninfrastructure:  \n\\- name: lambda  \n **type**: **terraform**  \n directory: \"./terraform\"  \n values: \".release/lambda.tfvar\"\n\n‍\n\nThis section tells Release where within the repository to find the code it needs for execution in the workflows. In the future, Release will support remote repositories. Now to execute this, it must be added to your workflows section\n\n\\# Application Template\n\nworkflows:  \n\\- name: setup  \n order_from:  \n - infrastructure.lambda\n\n‍\n\nThis adds the creation of the Lambda function synchronously to the workflow. Workflows may be parallelized if order of execution is not important during a given step, for more information on how to parallelize your workflows, visit our [documentation](https://docs.releasehub.com/reference-documentation/application-settings/application-template/schema-definition#workflow-parallelization).\n\nResources created during the execution of the workflow will have their state written to the local file store. In an already planned update, Release will extend this functionality to include the additional support of remote backends, like [Amazon S3](https://aws.amazon.com/s3/).\n\n### Summary\n\nThis blog covered at a high level the concepts around Infrastructure-as-Code, some specifics related to Terraform and how to use Terraform in combination with Release Environments to extend their functionality. A repository containing the code written in this sample can be found [here](https://github.com/releasehub-samples/terraform-runner-example). If you have any questions, find me on [LinkedIn](https://www.linkedin.com/in/joshdirkx/) or reach me at josh@release.com.\n",
          "code": "var Component=(()=>{var d=Object.create;var a=Object.defineProperty;var u=Object.getOwnPropertyDescriptor;var m=Object.getOwnPropertyNames;var p=Object.getPrototypeOf,f=Object.prototype.hasOwnProperty;var w=(t,e)=>()=>(e||t((e={exports:{}}).exports,e),e.exports),g=(t,e)=>{for(var n in e)a(t,n,{get:e[n],enumerable:!0})},s=(t,e,n,i)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let o of m(e))!f.call(t,o)&&o!==n&&a(t,o,{get:()=>e[o],enumerable:!(i=u(e,o))||i.enumerable});return t};var y=(t,e,n)=>(n=t!=null?d(p(t)):{},s(e||!t||!t.__esModule?a(n,\"default\",{value:t,enumerable:!0}):n,t)),b=t=>s(a({},\"__esModule\",{value:!0}),t);var c=w((I,l)=>{l.exports=_jsx_runtime});var k={};g(k,{default:()=>x,frontmatter:()=>v});var r=y(c()),v={title:\"Extending Release Environments with Terraform\",summary:\"This blog shows how to utilize Terraform alongside Release to provision additional resources in your cloud account.\",publishDate:\"Tue Aug 09 2022 23:58:58 GMT+0000 (Coordinated Universal Time)\",author:\"josh-dirkx\",readingTime:5,categories:[\"platform-engineering\",\"product\"],mainImage:\"/blog-images/c710bf4817cae68e73eeb96d596fa5a4.jpg\",imageAlt:\"Extending Release Environments with Terraform\",showCTA:!0,ctaCopy:\"Simplify managing Terraform environments with Release's ephemeral environments for seamless collaboration and consistent deployments.\",ctaLink:\"https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=extending-releasehub-environments-with-terraform\",relatedPosts:[\"\"],ogImage:\"/blog-images/c710bf4817cae68e73eeb96d596fa5a4.jpg\",excerpt:\"This blog shows how to utilize Terraform alongside Release to provision additional resources in your cloud account.\",tags:[\"platform-engineering\",\"product\"],ctaButton:\"Try Release for Free\"};function h(t){let e=Object.assign({p:\"p\",a:\"a\",br:\"br\",img:\"img\",em:\"em\",h3:\"h3\",span:\"span\",pre:\"pre\",code:\"code\",strong:\"strong\"},t.components);return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(e.p,{children:(0,r.jsxs)(e.a,{href:\"https://app.diagrams.net/?page-id=4EaHbZaufD5BJVQ07xAE&scale=auto#G1ZjCA67a8oFC8ztyAz2rx3ehoodHwKE2D\",children:[(0,r.jsx)(e.br,{}),`\n`]})}),`\n`,(0,r.jsx)(e.p,{children:(0,r.jsx)(e.img,{src:\"/blog-images/23436a0da5744c17b36da806862964fc.png\",alt:\"\"})}),`\n`,(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.a,{href:\"https://release.com/\",children:\"Release\"}),\" offers the ability to easily reproduce your production environment \",(0,r.jsx)(e.em,{children:\"N\"}),\" times in your cloud account. The construction of environments can be accomplished using our website, CLI or API and are useful for a variety of purposes - from quality assurance testing and user acceptance to sales demonstrations and penetration testing.\"]}),`\n`,(0,r.jsx)(e.p,{children:\"Environments today typically extend beyond a set of Docker images to include a variety of resources from your chosen cloud provider. In order to consistently get those resources alongside your Docker images when building environments, the instructions to get those resources instantiated must be codified somewhere in your application with some flavor of Infrastructure-as-Code (IaC).\"}),`\n`,(0,r.jsx)(e.p,{children:\"A recently launched abstraction within the Release Application Template removes the previous requirements of packaging your Terraform into a Dockerfile, creating the scripts to apply/update/teardown your resources, and managing the variables required to namespace appropriately. Using this new abstraction, the process becomes much more streamlined. Write your Terraform, tell us where to find it, and then tell us when to execute it.\"}),`\n`,(0,r.jsxs)(e.h3,{id:\"what-is-terraform\",children:[(0,r.jsx)(e.a,{className:\"anchor\",href:\"#what-is-terraform\",children:(0,r.jsx)(e.span,{className:\"icon icon-link\"})}),\"What is Terraform?\"]}),`\n`,(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.a,{href:\"https://www.terraform.io/\",children:\"Terraform\"}),\" is declarative Infrastructure as Code. Said another way, it is a way of programmatically specifying what resources you need created in your cloud provider.\"]}),`\n`,(0,r.jsx)(e.p,{children:\"There are two approaches to IaC - declarative (what) versus imperative (how). Declarative focuses on the desired end state; you tell the system what resources you want provisioned and it figures out how to get you there. Imperative, on the other hand, focuses on the instruction set to get to your end state; you are responsible for maintaining the order of operations required to get to your desired end state.\"}),`\n`,(0,r.jsx)(e.p,{children:\"Another important concept behind IaC is state and state management. Terraform is no exception here, by default it uses JSON in a local file to persist the state of real world resources created by it. Configurations can be added to allow for remote storage, such as in S3.\"}),`\n`,(0,r.jsxs)(e.h3,{id:\"why-should-i-use-terraform\",children:[(0,r.jsx)(e.a,{className:\"anchor\",href:\"#why-should-i-use-terraform\",children:(0,r.jsx)(e.span,{className:\"icon icon-link\"})}),\"Why should I use Terraform?\"]}),`\n`,(0,r.jsx)(e.p,{children:\"Terraform, or any IaC, is an important piece of the puzzle to modern, cloud infrastructure stacks. Codifying the instructions to your production environment allows you to increase visibility and auditability of changes - alterations to your stack are now managed within your source control, allowing you to use change requests for granular inspection of changes before they are applied.\"}),`\n`,(0,r.jsxs)(e.p,{children:[\"Your infrastructure becomes repeatable, predictable and consistent. Running the \",(0,r.jsx)(e.em,{children:\"terraform apply\"}),\" command will produce the same results every time. This stands in contrast to the manual process of clicking through a web console based on a document containing instructions, or from memory.\"]}),`\n`,(0,r.jsxs)(e.p,{children:[\"In the event of a full or partial failure of your application, you can quickly and easily replace \",(0,r.jsx)(e.em,{children:\"everything\"}),\".\"]}),`\n`,(0,r.jsxs)(e.h3,{id:\"how-do-i-use-terraform-with-release\",children:[(0,r.jsx)(e.a,{className:\"anchor\",href:\"#how-do-i-use-terraform-with-release\",children:(0,r.jsx)(e.span,{className:\"icon icon-link\"})}),(0,r.jsx)(e.em,{children:\"How\"}),\" do I use Terraform with Release?\"]}),`\n`,(0,r.jsxs)(e.p,{children:[\"\\u200D\",(0,r.jsx)(e.em,{children:\"The following example assumes a project layout as outlined in the image below\"})]}),`\n`,(0,r.jsx)(e.p,{children:(0,r.jsx)(e.img,{src:\"/blog-images/ba50f60eb0679702ad5c7fae80ba5645.png\",alt:\"\"})}),`\n`,(0,r.jsxs)(e.p,{children:[\"When writing your Terraform, it's important to keep in mind that you want it to be repeatable without resource naming collisions, regardless of how many times you execute the same combination of resources. This can be achieved by using dynamic values to namespace resources. Release makes available to you all of the Release generated environment variables for use within your \",(0,r.jsx)(e.em,{children:\".tfvars\"}),\" file, allowing you to use the environments' context as the namespacing material. For example, you might have the following inside of your \",(0,r.jsx)(e.em,{children:\".tfvars\"}),\" file\"]}),`\n`,(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:\"language-bash\",children:'# .release/lambda.tfvars\\nnamespace = \"${RELEASE_ACCOUNT_ID}/${RELEASE_ENV_ID}/\"\\n'})}),`\n`,(0,r.jsx)(e.p,{children:\"\\u200D\"}),`\n`,(0,r.jsx)(e.p,{children:\"When specifying the name of a resource, we'll use a Lambda as an example, you would reference that namespace in the name, like below\"}),`\n`,(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:\"language-hcl\",children:`resource \"aws_lambda_function\" \"lambda\" {\n  function_name = \"\\${vars.namespace}/lambda\"\n  # ...\n}\n`})}),`\n`,(0,r.jsx)(e.p,{children:\"\\u200D\"}),`\n`,(0,r.jsx)(e.p,{children:\"The inclusion of the environment_id ensures that each time this Terraform is executed by Release, it will be unique.\"}),`\n`,(0,r.jsx)(e.p,{children:\"WIthin the Application Template for a given Release Application, there is an abstraction over IaC providers, such as Terraform, that we can leverage. For the purposes of this demonstration, we will be using Terraform in conjunction with AWS resources.\"}),`\n`,(0,r.jsx)(e.p,{children:\"A new section has been added to the template at the top level - infrastructure. This section takes an array with a few required and optional fields. Required fields are name (this is how you will reference the aforementioned Terraform in the workflows section) and type (for now, the only supported type is Terraform but over time we will add abstractions for Pulumi, CDK, etc.). Optionally, you can also specify a directory (where to find the Terraform) and values (where to find environment variables). By default, if no directory is specified then Release will assume your Terraform is at the root of the repository and if no values file is given, we assume this file does not exist.\"}),`\n`,(0,r.jsx)(e.p,{children:\"We need to add a few things to our Application Template to support this. We need to first tell Release what to build, then we specify when we execute it.\"}),`\n`,(0,r.jsx)(e.p,{children:\"To tell Release what we need it to build, we add a new array item to the infrastructure key in our Application Template.\"}),`\n`,(0,r.jsx)(e.p,{children:\"\\u200D\"}),`\n`,(0,r.jsx)(e.p,{children:\"# Application Template\"}),`\n`,(0,r.jsxs)(e.p,{children:[\"infrastructure:\",(0,r.jsx)(e.br,{}),`\n`,\"- name: lambda\",(0,r.jsx)(e.br,{}),`\n`,(0,r.jsx)(e.strong,{children:\"type\"}),\": \",(0,r.jsx)(e.strong,{children:\"terraform\"}),(0,r.jsx)(e.br,{}),`\n`,'directory: \"./terraform\"',(0,r.jsx)(e.br,{}),`\n`,'values: \".release/lambda.tfvar\"']}),`\n`,(0,r.jsx)(e.p,{children:\"\\u200D\"}),`\n`,(0,r.jsx)(e.p,{children:\"This section tells Release where within the repository to find the code it needs for execution in the workflows. In the future, Release will support remote repositories. Now to execute this, it must be added to your workflows section\"}),`\n`,(0,r.jsx)(e.p,{children:\"# Application Template\"}),`\n`,(0,r.jsxs)(e.p,{children:[\"workflows:\",(0,r.jsx)(e.br,{}),`\n`,\"- name: setup\",(0,r.jsx)(e.br,{}),`\n`,\"order_from:\",(0,r.jsx)(e.br,{}),`\n`,\"- infrastructure.lambda\"]}),`\n`,(0,r.jsx)(e.p,{children:\"\\u200D\"}),`\n`,(0,r.jsxs)(e.p,{children:[\"This adds the creation of the Lambda function synchronously to the workflow. Workflows may be parallelized if order of execution is not important during a given step, for more information on how to parallelize your workflows, visit our \",(0,r.jsx)(e.a,{href:\"https://docs.releasehub.com/reference-documentation/application-settings/application-template/schema-definition#workflow-parallelization\",children:\"documentation\"}),\".\"]}),`\n`,(0,r.jsxs)(e.p,{children:[\"Resources created during the execution of the workflow will have their state written to the local file store. In an already planned update, Release will extend this functionality to include the additional support of remote backends, like \",(0,r.jsx)(e.a,{href:\"https://aws.amazon.com/s3/\",children:\"Amazon S3\"}),\".\"]}),`\n`,(0,r.jsxs)(e.h3,{id:\"summary\",children:[(0,r.jsx)(e.a,{className:\"anchor\",href:\"#summary\",children:(0,r.jsx)(e.span,{className:\"icon icon-link\"})}),\"Summary\"]}),`\n`,(0,r.jsxs)(e.p,{children:[\"This blog covered at a high level the concepts around Infrastructure-as-Code, some specifics related to Terraform and how to use Terraform in combination with Release Environments to extend their functionality. A repository containing the code written in this sample can be found \",(0,r.jsx)(e.a,{href:\"https://github.com/releasehub-samples/terraform-runner-example\",children:\"here\"}),\". If you have any questions, find me on \",(0,r.jsx)(e.a,{href:\"https://www.linkedin.com/in/joshdirkx/\",children:\"LinkedIn\"}),\" or reach me at \",(0,r.jsx)(e.a,{href:\"mailto:josh@release.com\",children:\"josh@release.com\"}),\".\"]})]})}function T(t={}){let{wrapper:e}=t.components||{};return e?(0,r.jsx)(e,Object.assign({},t,{children:(0,r.jsx)(h,t)})):h(t)}var x=T;return b(k);})();\n;return Component;"
        },
        "_id": "blog/posts/extending-releasehub-environments-with-terraform.mdx",
        "_raw": {
          "sourceFilePath": "blog/posts/extending-releasehub-environments-with-terraform.mdx",
          "sourceFileName": "extending-releasehub-environments-with-terraform.mdx",
          "sourceFileDir": "blog/posts",
          "contentType": "mdx",
          "flattenedPath": "blog/posts/extending-releasehub-environments-with-terraform"
        },
        "type": "BlogPost",
        "computedSlug": "extending-releasehub-environments-with-terraform"
      },
      "documentHash": "1739393595018",
      "hasWarnings": false,
      "documentTypeName": "BlogPost"
    },
    "blog/posts/feature-flags-and-ephemeral-environments.mdx": {
      "document": {
        "title": "Feature Flags and Ephemeral Environments",
        "summary": "Learn about feature flags in devops, feature flags and ephemeral environments, and more.",
        "publishDate": "Tue Jan 26 2021 22:22:08 GMT+0000 (Coordinated Universal Time)",
        "author": "erik-landerholm",
        "readingTime": 7,
        "categories": [
          "platform-engineering",
          "product"
        ],
        "mainImage": "/blog-images/6a19ca3dda8d9f11445e6062ff0936f5.jpg",
        "imageAlt": "Top view of city traffic",
        "showCTA": true,
        "ctaCopy": "Simplify managing ephemeral environments with Release's on-demand environments, enhancing feature flag testing and deployment efficiency.",
        "ctaLink": "https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=feature-flags-and-ephemeral-environments",
        "relatedPosts": [
          ""
        ],
        "ogImage": "/blog-images/6a19ca3dda8d9f11445e6062ff0936f5.jpg",
        "excerpt": "Learn about feature flags in devops, feature flags and ephemeral environments, and more.",
        "tags": [
          "platform-engineering",
          "product"
        ],
        "ctaButton": "Try Release for Free",
        "body": {
          "raw": "\n### Overview\n\n- [Feature Flags in DevOps and Software Development](#devops)\n- [Feature Flag Environments](#environments)\n- [Working with Ephemeral Environments](#ephemeral)\n- Step 1: [Create an Application in Release](#create)\n- Step 2: [Modify the Application Template](#modify)\n- Step 3: [Add Environment Variables](#add)\n- Step 4: [Deploy](#deploy)\n\n‍\n\n### Feature Flags in DevOps\n\nFeature Flags are a necessary and ubiquitous part of modern software development. As your company and the complexity of your application grows it becomes imperative to be able to control what features are available to your internal development teams, stakeholders and customers. In the long-ago, before-times, we would just have a variable that you would toggle between true and false to control behavior of your application. However, as application development transitioned to the Web we needed the same kind of control, except that hard-coded feature flags just weren't going to cut it. Enter Dynamic Feature Flags!\n\nDynamic feature flags were a big improvement over static feature flags, but also added complexity and presented challenges different from static feature flags. Gone were hard-coded flags, but they were replaced with if statements and more importantly, retrieving the appropriate flags for your application. Most people started by rolling their own, but as developing with feature flags gained popularity many different companies popped into existence looking to solve the problems of:\n\n- One interface to manage your flags\n- Easy maintenance of your flags\n- Very fast and reliable retrieval of your flags\n- Splitting traffic to one feature or another\n\nWhile companies like LaunchDarkly, Optimizely, Rollout, Split.io and others made it fairly easy to create and manage these flags this doesn't solve all of your issues. Many software orgs, especially as they grow, need lots of environments for testing. This poses a challenge to your Feature Flag setup specifically if your environments are ephemeral.\n\nEphemeral environments are like any environment except they will be removed in a relatively short amount of time unlike your staging or production environments. Good examples are:\n\n- Feature branches\n- Sales Demos\n- Load Testing\n- Refactors\n\nThese environments may not last a long time, but they are exceedingly important and can be just as complex as production. While a sales demo environment may be able to function with seed data, a load testing environment will need production or production-like data and many replicas of each service to give a valid result. These can be super complex to create and manage and their ephemeral nature can play havoc with your feature flag setup.\n\n### Feature Flag Environments to the Rescue…Sort of\n\nLaunchDarkly (and others) recognized this issue and created the concept of environments in their own applications. You can read about their implementation here. They have apis that allow you to create and manipulate these sets of feature flags on an environment by environment basis. This works great if you have a finite set of environments and the set of them doesn't change often, but with ephemeral environments the ability to spin them up and down is a feature not a bug.\n\nIn order to simplify this issue most people create two kinds of environments in their favorite Feature Flag provider: one for development (or test) and one for production. In larger organizations development teams may have a few, such as development, test, uat, staging, and production. This works fine as long as you don't want to add another one or you never take the plunge toward truly ephemeral application environments.\n\nOnce you move to ephemeral environments most people take the shortcut of assigning every ephemeral environment to a single Feature Flag environment, which is simple enough, but creates a large problem with people stepping on each other's toes.\n\nImagine you have 10 environments all pointing to a single database with writes happening from all those environments: it's the same issue here. The great thing about feature flags is the ability to toggle them and see different behavior, but if every environment is pointing to the same one you now have another resource contention problem. If you toggle Feature A 'on' what's to stop your co-worker from toggling it 'off'? Any issues you have with permanent staging environments are magnified with ephemeral environments.\n\nThe best solution would be upon the creation of an ephemeral environment you would create an environment in LaunchDarkly based on something unique about your ephemeral environment and when it comes up, you would make sure it was using the unique SDK api for that particular Feature Flag Environment. Let's implement the workflow and see how that would work with Release!\n\n![](/blog-images/40fe7f59c236984a8877850971f711ec.png)\n\nEphemeral Environment Creation Workflow\n\n### Working with Ephemeral Environments\n\nIn order to try this out with Release we need a repository with a Docker File that has implemented Feature Flags with LaunchDarkly. I'm going to use [this](https://github.com/elanderholm/rails_postgres_redis) repository on Github and you can do the same by first forking the repository so you can use it to create an application with Release.\n\nOnce you have forked the repository you can navigate to [release.com](https://github.com/elanderholm/rails_postgres_redis) and sign-in using github in order to follow along with this example.\n\nThe steps to get our ephemeral environments created in Release with support for environments in LaunchDarkly are:\n\n1.  Create our application in Release\n2.  Create a job with Release to create the environment in LaunchDarkly\n3.  Add some environment variables so the application can contact LaunchDarkly and pull in the SDK Api key from our newly created LaunchDarkly Environment\n4.  Deploy our Ephemeral Environment\n\n> _If you don't have a launch darkly account, you can create one for free for 30 days to use for this example. You will also need to create at least one feature flag. If you already have a launch darkly account with a lot of feature flags you can just skip this step._\n\n![](/blog-images/b5c97492651bfb28602db700fcac9847.png)\n\n‍\n\n### _Create_ the Application In Release\n\nOnce we are logged into Release we want to click **Create New Application** in the left-hand sidebar. After doing that we will be presented with Create New Application Workflow.\n\n![](/blog-images/d4f8f32c20255f0f69fd15650e994390.png)\n\nFirst, we will click the \"refresh\" button to find our newly forked repository. Then, we will select that repository and \"Docker\" for the \"api\" service. Finally, name your application. Once you have finished click the purple \"Generate App Template\" button.\n\n![](/blog-images/03da5feb8004cd4b44710be338ca0f14.png)\n\nLastly name your application and generate the template for your configuration.\n\n![](/blog-images/370c8324a4c8b543213337e5cc10737b.png)\n\n‍\n\n### Modify the Application Template\n\nBefore we can deploy our environment(s) we need to make a modification to our application template and add a few environment variables. We also need to create a job that will create our LaunchDarkly environment upon initial environment deployment. Jobs in Release are described in detail [here](https://docs.releasehub.com/reference-guide/application-settings/application-template#jobs). The TL;DR is that with a small amount of configuration you can run any arbitrary script or task in a container. For example, these jobs are very useful for running migrations before a deployment of your backend service. In this case we will run a rake task to set up our LaunchDarkly Environment.\n\n```yaml\njobs:\n  - name: create-launch-darkly-env\n    from_services: api\n    args:\n      - bundle\n      - exec\n      - rake\n      - launch_darkly:create_environment\n```\n\n> _The above yaml represents a job in Release_\n\nWe will place the above lines right before the \"services\" stanza in our application template.\n\n```none\n\nmemory:\n   limits: 1Gi\n   requests: 100Mi\n replicas: 1\njobs:\n  - name: create-launch-darkly-env\n    from_services: api\n    args:\n    - bundle\n    - exec\n    - rake\n    - launch_darkly:create_environment\nservices:\n  - name: api\n    image: erik-opsnuts-test-001/rails_postgres_redis/api\n    has_repo: true\n    static: false\n\n```\n\n> _Place the jobs snippet into the Application Template_\n\nIn order for Release to utilize this job as part of the workflow to deploy an environment we will need to add one line near the bottom of the file in the 'workflows\\' section. Under 'setup':order_from' we will add **jobs.create-launch-darkly-env**. Then, click \"Save and Continue.\"\n\n```none\n\nworkflows:\n- name: setup\n  order_from:\n  - jobs.create-launch-darkly-env\n  - services.all\n- name: patch\n  order_from:\n  - services.api\n  - services.sidekiq\n  - services.db\n  - services.redis\n\n```\n\n> _Place jobs.create-launch-darkly-env before services.all under the workflows stanza_\n\n‍**That's all the configuration needed, now we just need to add two environment variables before we deploy!**\n\n‍\n\n### Adding Environment Variables\n\n![](/blog-images/955467d85b9685ec1351a814fb6289b2.png)\n\nClick 'EDIT' for 'Default Environment Variables' to bring up the editor. We will add two environment variables that contain information about LaunchDarkly. They are:\n**LAUNCH_DARKLY_API_KEY**: Your LaunchDarkly Api Key which is found here. If you don't have an api token create the \"+ TOKEN\" button to make one. You will want to give it admin privileges. If you can't do that contact your administrator. **_Once you create it, make sure you copy it and paste it somewhere you can retrieve it._** LaunchDarkly will obfuscate your token and if you don't save it somewhere you will need to generate a new one.\n\n![](/blog-images/825b07bdb6f6bfb0ddcd03b4a7cf7667.png)\n**LAUNCH_DARKLY_PROJECT_NAME**: We will just use 'default' for this example, but if there is another project you would like to test with feel free.\n\n```yaml\ndefaults:\n  - key: POSTGRES_USER\n    value: postgres\n  - key: POSTGRES_PASSWORD\n    value: postgres\n  - key: LAUNCH_DARKLY_PROJECT_NAME\n    value: default\n  - key: LAUNCH_DARKLY_API_KEY\n    value: your-api-key\n    secret: true\n```\n\nClick 'Save' to save your environment variables as part of your application configuration. Then, click 'Build and Deploy'. You will be redirected to the activity dashboard for that application and a Docker build was kicked off in the background. This will be followed by the deployment of the environment for your application. You can view the build and the deployment under the 'builds' and 'deploys' sections respectively.\n\n![](/blog-images/4e9d5eb53f63754d6ebeb974fd1c3f2b.png)\n\n‍\n\n### Your Environment\n\nThis process of doing the docker build will take a few minutes the first time. Once the build and deployment have finished you can find the url for your new environment by clicking 'Environments' on the left and then by clicking into your new environment.\n\n![](/blog-images/d7ffae114d550925ce4397e67d7e1f3b.png)\n\nOnce you click on the url for your newly created ephemeral environment another window in your browser will open to the example rails site with postgres and redis. It should look something like this:\n\n![](/blog-images/5513d5ff58106e72c89f7b71551aeccf.png)\n\nIf you have a flag named 'test-flag' in your launch darkly account you can go ahead and toggle it from 'false' to 'true' and vice versa and reload your environment to see the changes. If you would like to use a different flag, you only need to make one change in: **app/views/welcome/index.html.erb**\n\n```ruby\n# app/views/welcome/index.html.erb\ntest_flag = Rails.configuration.ld_client.variation(\n  \"test-flag\",\n  {key: \"user@test.com\"},\n  false\n)\n```\n\nOnce you have changed 'test-flag' to the flag name of your choosing, you only need to commit and push the change to github. Once that happens, Release will automatically do a build and then deploy your changes. When the process finishes, you will be able to see the welcome page change based on your new flag.\n\nIn your LaunchDarkly interface you will see a newly created environment with a name of the form 'tedfe34\\`. This name is the same as your **RELEASE_ENV_ID** environment variable that Release creates automatically for your new environment. You will see this value in a few places in the Release UI besides the environment variable editor.\n\n![](/blog-images/9d8569299b8c2b955944204408f47141.png)\n\n### Conclusion - What's next?\n\nNow that you can get pristine feature flag environments dedicated to your Release environments what's next? In this example the clean-up would need to be done manually, not a huge deal, but we can do better. Release will be implementing a deeper integration with LaunchDarkly in the near future to make this seamless and handle deleting the environments in LaunchDarkly when your Release environment is removed.\n\nStay tuned for integrations with other feature flag providers in the future. If you would like to have environments for your applications that are fast, simple to define and incredibly powerful send us a note at [support@releasehub.com](mailto:support@releasehub.com) and we will help you and your team become more efficient utilizing ephemeral environments with Release.\n\nHero Image by [Denys Nevozhai](https://unsplash.com/@dnevozhai?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) on [Unsplash](https://unsplash.com/s/photos/traffic?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)\n\n### Additional Resources\n\n- [What is an Ephemeral Environment?](https://releasehub.com/ephemeral-environments)\n- [User Acceptance Testing (UAT) with Release Ephemeral Environments](https://releasehub.com/user-acceptance-testing-with-ephemeral-environments)\n- [5 Ways To Improve Developer Velocity Using Ephemeral Environments](https://releasehub.com/blog/improve-developer-velocity-with-ephemeral-environments)\n- [Agile DevOps Methodology](https://releasehub.com/blog/you-need-agile-devops-methodology)\n\n‍\n",
          "code": "var Component=(()=>{var d=Object.create;var o=Object.defineProperty;var p=Object.getOwnPropertyDescriptor;var m=Object.getOwnPropertyNames;var u=Object.getPrototypeOf,f=Object.prototype.hasOwnProperty;var g=(a,e)=>()=>(e||a((e={exports:{}}).exports,e),e.exports),y=(a,e)=>{for(var t in e)o(a,t,{get:e[t],enumerable:!0})},l=(a,e,t,r)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let i of m(e))!f.call(a,i)&&i!==t&&o(a,i,{get:()=>e[i],enumerable:!(r=p(e,i))||r.enumerable});return a};var v=(a,e,t)=>(t=a!=null?d(u(a)):{},l(e||!a||!a.__esModule?o(t,\"default\",{value:a,enumerable:!0}):t,a)),b=a=>l(o({},\"__esModule\",{value:!0}),a);var c=g((T,s)=>{s.exports=_jsx_runtime});var D={};y(D,{default:()=>_,frontmatter:()=>w});var n=v(c()),w={title:\"Feature Flags and Ephemeral Environments\",summary:\"Learn about feature flags in devops, feature flags and ephemeral environments, and more.\",publishDate:\"Tue Jan 26 2021 22:22:08 GMT+0000 (Coordinated Universal Time)\",author:\"erik-landerholm\",readingTime:7,categories:[\"platform-engineering\",\"product\"],mainImage:\"/blog-images/6a19ca3dda8d9f11445e6062ff0936f5.jpg\",imageAlt:\"Top view of city traffic\",showCTA:!0,ctaCopy:\"Simplify managing ephemeral environments with Release's on-demand environments, enhancing feature flag testing and deployment efficiency.\",ctaLink:\"https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=feature-flags-and-ephemeral-environments\",relatedPosts:[\"\"],ogImage:\"/blog-images/6a19ca3dda8d9f11445e6062ff0936f5.jpg\",excerpt:\"Learn about feature flags in devops, feature flags and ephemeral environments, and more.\",tags:[\"platform-engineering\",\"product\"],ctaButton:\"Try Release for Free\"};function h(a){let e=Object.assign({h3:\"h3\",a:\"a\",span:\"span\",ul:\"ul\",li:\"li\",p:\"p\",img:\"img\",ol:\"ol\",blockquote:\"blockquote\",em:\"em\",strong:\"strong\",pre:\"pre\",code:\"code\"},a.components);return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsxs)(e.h3,{id:\"overview\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#overview\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Overview\"]}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsx)(e.li,{children:(0,n.jsx)(e.a,{href:\"#devops\",children:\"Feature Flags in DevOps and Software Development\"})}),`\n`,(0,n.jsx)(e.li,{children:(0,n.jsx)(e.a,{href:\"#environments\",children:\"Feature Flag Environments\"})}),`\n`,(0,n.jsx)(e.li,{children:(0,n.jsx)(e.a,{href:\"#ephemeral\",children:\"Working with Ephemeral Environments\"})}),`\n`,(0,n.jsxs)(e.li,{children:[\"Step 1: \",(0,n.jsx)(e.a,{href:\"#create\",children:\"Create an Application in Release\"})]}),`\n`,(0,n.jsxs)(e.li,{children:[\"Step 2: \",(0,n.jsx)(e.a,{href:\"#modify\",children:\"Modify the Application Template\"})]}),`\n`,(0,n.jsxs)(e.li,{children:[\"Step 3: \",(0,n.jsx)(e.a,{href:\"#add\",children:\"Add Environment Variables\"})]}),`\n`,(0,n.jsxs)(e.li,{children:[\"Step 4: \",(0,n.jsx)(e.a,{href:\"#deploy\",children:\"Deploy\"})]}),`\n`]}),`\n`,(0,n.jsx)(e.p,{children:\"\\u200D\"}),`\n`,(0,n.jsxs)(e.h3,{id:\"feature-flags-in-devops\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#feature-flags-in-devops\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Feature Flags in DevOps\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Feature Flags are a necessary and ubiquitous part of modern software development. As your company and the complexity of your application grows it becomes imperative to be able to control what features are available to your internal development teams, stakeholders and customers. In the long-ago, before-times, we would just have a variable that you would toggle between true and false to control behavior of your application. However, as application development transitioned to the Web we needed the same kind of control, except that hard-coded feature flags just weren't going to cut it. Enter Dynamic Feature Flags!\"}),`\n`,(0,n.jsx)(e.p,{children:\"Dynamic feature flags were a big improvement over static feature flags, but also added complexity and presented challenges different from static feature flags. Gone were hard-coded flags, but they were replaced with if statements and more importantly, retrieving the appropriate flags for your application. Most people started by rolling their own, but as developing with feature flags gained popularity many different companies popped into existence looking to solve the problems of:\"}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsx)(e.li,{children:\"One interface to manage your flags\"}),`\n`,(0,n.jsx)(e.li,{children:\"Easy maintenance of your flags\"}),`\n`,(0,n.jsx)(e.li,{children:\"Very fast and reliable retrieval of your flags\"}),`\n`,(0,n.jsx)(e.li,{children:\"Splitting traffic to one feature or another\"}),`\n`]}),`\n`,(0,n.jsx)(e.p,{children:\"While companies like LaunchDarkly, Optimizely, Rollout, Split.io and others made it fairly easy to create and manage these flags this doesn't solve all of your issues. Many software orgs, especially as they grow, need lots of environments for testing. This poses a challenge to your Feature Flag setup specifically if your environments are ephemeral.\"}),`\n`,(0,n.jsx)(e.p,{children:\"Ephemeral environments are like any environment except they will be removed in a relatively short amount of time unlike your staging or production environments. Good examples are:\"}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsx)(e.li,{children:\"Feature branches\"}),`\n`,(0,n.jsx)(e.li,{children:\"Sales Demos\"}),`\n`,(0,n.jsx)(e.li,{children:\"Load Testing\"}),`\n`,(0,n.jsx)(e.li,{children:\"Refactors\"}),`\n`]}),`\n`,(0,n.jsx)(e.p,{children:\"These environments may not last a long time, but they are exceedingly important and can be just as complex as production. While a sales demo environment may be able to function with seed data, a load testing environment will need production or production-like data and many replicas of each service to give a valid result. These can be super complex to create and manage and their ephemeral nature can play havoc with your feature flag setup.\"}),`\n`,(0,n.jsxs)(e.h3,{id:\"feature-flag-environments-to-the-rescuesort-of\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#feature-flag-environments-to-the-rescuesort-of\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Feature Flag Environments to the Rescue\\u2026Sort of\"]}),`\n`,(0,n.jsx)(e.p,{children:\"LaunchDarkly (and others) recognized this issue and created the concept of environments in their own applications. You can read about their implementation here. They have apis that allow you to create and manipulate these sets of feature flags on an environment by environment basis. This works great if you have a finite set of environments and the set of them doesn't change often, but with ephemeral environments the ability to spin them up and down is a feature not a bug.\"}),`\n`,(0,n.jsx)(e.p,{children:\"In order to simplify this issue most people create two kinds of environments in their favorite Feature Flag provider: one for development (or test) and one for production. In larger organizations development teams may have a few, such as development, test, uat, staging, and production. This works fine as long as you don't want to add another one or you never take the plunge toward truly ephemeral application environments.\"}),`\n`,(0,n.jsx)(e.p,{children:\"Once you move to ephemeral environments most people take the shortcut of assigning every ephemeral environment to a single Feature Flag environment, which is simple enough, but creates a large problem with people stepping on each other's toes.\"}),`\n`,(0,n.jsx)(e.p,{children:\"Imagine you have 10 environments all pointing to a single database with writes happening from all those environments: it's the same issue here. The great thing about feature flags is the ability to toggle them and see different behavior, but if every environment is pointing to the same one you now have another resource contention problem. If you toggle Feature A 'on' what's to stop your co-worker from toggling it 'off'? Any issues you have with permanent staging environments are magnified with ephemeral environments.\"}),`\n`,(0,n.jsx)(e.p,{children:\"The best solution would be upon the creation of an ephemeral environment you would create an environment in LaunchDarkly based on something unique about your ephemeral environment and when it comes up, you would make sure it was using the unique SDK api for that particular Feature Flag Environment. Let's implement the workflow and see how that would work with Release!\"}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/40fe7f59c236984a8877850971f711ec.png\",alt:\"\"})}),`\n`,(0,n.jsx)(e.p,{children:\"Ephemeral Environment Creation Workflow\"}),`\n`,(0,n.jsxs)(e.h3,{id:\"working-with-ephemeral-environments\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#working-with-ephemeral-environments\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Working with Ephemeral Environments\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"In order to try this out with Release we need a repository with a Docker File that has implemented Feature Flags with LaunchDarkly. I'm going to use \",(0,n.jsx)(e.a,{href:\"https://github.com/elanderholm/rails_postgres_redis\",children:\"this\"}),\" repository on Github and you can do the same by first forking the repository so you can use it to create an application with Release.\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"Once you have forked the repository you can navigate to \",(0,n.jsx)(e.a,{href:\"https://github.com/elanderholm/rails_postgres_redis\",children:\"release.com\"}),\" and sign-in using github in order to follow along with this example.\"]}),`\n`,(0,n.jsx)(e.p,{children:\"The steps to get our ephemeral environments created in Release with support for environments in LaunchDarkly are:\"}),`\n`,(0,n.jsxs)(e.ol,{children:[`\n`,(0,n.jsx)(e.li,{children:\"Create our application in Release\"}),`\n`,(0,n.jsx)(e.li,{children:\"Create a job with Release to create the environment in LaunchDarkly\"}),`\n`,(0,n.jsx)(e.li,{children:\"Add some environment variables so the application can contact LaunchDarkly and pull in the SDK Api key from our newly created LaunchDarkly Environment\"}),`\n`,(0,n.jsx)(e.li,{children:\"Deploy our Ephemeral Environment\"}),`\n`]}),`\n`,(0,n.jsxs)(e.blockquote,{children:[`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.em,{children:\"If you don't have a launch darkly account, you can create one for free for 30 days to use for this example. You will also need to create at least one feature flag. If you already have a launch darkly account with a lot of feature flags you can just skip this step.\"})}),`\n`]}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/b5c97492651bfb28602db700fcac9847.png\",alt:\"\"})}),`\n`,(0,n.jsx)(e.p,{children:\"\\u200D\"}),`\n`,(0,n.jsxs)(e.h3,{id:\"create-the-application-in-release\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#create-the-application-in-release\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.em,{children:\"Create\"}),\" the Application In Release\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"Once we are logged into Release we want to click \",(0,n.jsx)(e.strong,{children:\"Create New Application\"}),\" in the left-hand sidebar. After doing that we will be presented with Create New Application Workflow.\"]}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/d4f8f32c20255f0f69fd15650e994390.png\",alt:\"\"})}),`\n`,(0,n.jsx)(e.p,{children:'First, we will click the \"refresh\" button to find our newly forked repository. Then, we will select that repository and \"Docker\" for the \"api\" service. Finally, name your application. Once you have finished click the purple \"Generate App Template\" button.'}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/03da5feb8004cd4b44710be338ca0f14.png\",alt:\"\"})}),`\n`,(0,n.jsx)(e.p,{children:\"Lastly name your application and generate the template for your configuration.\"}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/370c8324a4c8b543213337e5cc10737b.png\",alt:\"\"})}),`\n`,(0,n.jsx)(e.p,{children:\"\\u200D\"}),`\n`,(0,n.jsxs)(e.h3,{id:\"modify-the-application-template\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#modify-the-application-template\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Modify the Application Template\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"Before we can deploy our environment(s) we need to make a modification to our application template and add a few environment variables. We also need to create a job that will create our LaunchDarkly environment upon initial environment deployment. Jobs in Release are described in detail \",(0,n.jsx)(e.a,{href:\"https://docs.releasehub.com/reference-guide/application-settings/application-template#jobs\",children:\"here\"}),\". The TL;DR is that with a small amount of configuration you can run any arbitrary script or task in a container. For example, these jobs are very useful for running migrations before a deployment of your backend service. In this case we will run a rake task to set up our LaunchDarkly Environment.\"]}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-yaml\",children:`jobs:\n  - name: create-launch-darkly-env\n    from_services: api\n    args:\n      - bundle\n      - exec\n      - rake\n      - launch_darkly:create_environment\n`})}),`\n`,(0,n.jsxs)(e.blockquote,{children:[`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.em,{children:\"The above yaml represents a job in Release\"})}),`\n`]}),`\n`,(0,n.jsx)(e.p,{children:'We will place the above lines right before the \"services\" stanza in our application template.'}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-none\",children:`\nmemory:\n   limits: 1Gi\n   requests: 100Mi\n replicas: 1\njobs:\n  - name: create-launch-darkly-env\n    from_services: api\n    args:\n    - bundle\n    - exec\n    - rake\n    - launch_darkly:create_environment\nservices:\n  - name: api\n    image: erik-opsnuts-test-001/rails_postgres_redis/api\n    has_repo: true\n    static: false\n\n`})}),`\n`,(0,n.jsxs)(e.blockquote,{children:[`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.em,{children:\"Place the jobs snippet into the Application Template\"})}),`\n`]}),`\n`,(0,n.jsxs)(e.p,{children:[\"In order for Release to utilize this job as part of the workflow to deploy an environment we will need to add one line near the bottom of the file in the 'workflows' section. Under 'setup':order_from' we will add \",(0,n.jsx)(e.strong,{children:\"jobs.create-launch-darkly-env\"}),'. Then, click \"Save and Continue.\"']}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-none\",children:`\nworkflows:\n- name: setup\n  order_from:\n  - jobs.create-launch-darkly-env\n  - services.all\n- name: patch\n  order_from:\n  - services.api\n  - services.sidekiq\n  - services.db\n  - services.redis\n\n`})}),`\n`,(0,n.jsxs)(e.blockquote,{children:[`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.em,{children:\"Place jobs.create-launch-darkly-env before services.all under the workflows stanza\"})}),`\n`]}),`\n`,(0,n.jsxs)(e.p,{children:[\"\\u200D\",(0,n.jsx)(e.strong,{children:\"That's all the configuration needed, now we just need to add two environment variables before we deploy!\"})]}),`\n`,(0,n.jsx)(e.p,{children:\"\\u200D\"}),`\n`,(0,n.jsxs)(e.h3,{id:\"adding-environment-variables\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#adding-environment-variables\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Adding Environment Variables\"]}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/955467d85b9685ec1351a814fb6289b2.png\",alt:\"\"})}),`\n`,(0,n.jsxs)(e.p,{children:[`Click 'EDIT' for 'Default Environment Variables' to bring up the editor. We will add two environment variables that contain information about LaunchDarkly. They are:\n`,(0,n.jsx)(e.strong,{children:\"LAUNCH_DARKLY_API_KEY\"}),`: Your LaunchDarkly Api Key which is found here. If you don't have an api token create the \"+ TOKEN\" button to make one. You will want to give it admin privileges. If you can't do that contact your administrator. `,(0,n.jsx)(e.strong,{children:(0,n.jsx)(e.em,{children:\"Once you create it, make sure you copy it and paste it somewhere you can retrieve it.\"})}),\" LaunchDarkly will obfuscate your token and if you don't save it somewhere you will need to generate a new one.\"]}),`\n`,(0,n.jsxs)(e.p,{children:[(0,n.jsx)(e.img,{src:\"/blog-images/825b07bdb6f6bfb0ddcd03b4a7cf7667.png\",alt:\"\"}),`\n`,(0,n.jsx)(e.strong,{children:\"LAUNCH_DARKLY_PROJECT_NAME\"}),\": We will just use 'default' for this example, but if there is another project you would like to test with feel free.\"]}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-yaml\",children:`defaults:\n  - key: POSTGRES_USER\n    value: postgres\n  - key: POSTGRES_PASSWORD\n    value: postgres\n  - key: LAUNCH_DARKLY_PROJECT_NAME\n    value: default\n  - key: LAUNCH_DARKLY_API_KEY\n    value: your-api-key\n    secret: true\n`})}),`\n`,(0,n.jsx)(e.p,{children:\"Click 'Save' to save your environment variables as part of your application configuration. Then, click 'Build and Deploy'. You will be redirected to the activity dashboard for that application and a Docker build was kicked off in the background. This will be followed by the deployment of the environment for your application. You can view the build and the deployment under the 'builds' and 'deploys' sections respectively.\"}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/4e9d5eb53f63754d6ebeb974fd1c3f2b.png\",alt:\"\"})}),`\n`,(0,n.jsx)(e.p,{children:\"\\u200D\"}),`\n`,(0,n.jsxs)(e.h3,{id:\"your-environment\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#your-environment\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Your Environment\"]}),`\n`,(0,n.jsx)(e.p,{children:\"This process of doing the docker build will take a few minutes the first time. Once the build and deployment have finished you can find the url for your new environment by clicking 'Environments' on the left and then by clicking into your new environment.\"}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/d7ffae114d550925ce4397e67d7e1f3b.png\",alt:\"\"})}),`\n`,(0,n.jsx)(e.p,{children:\"Once you click on the url for your newly created ephemeral environment another window in your browser will open to the example rails site with postgres and redis. It should look something like this:\"}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/5513d5ff58106e72c89f7b71551aeccf.png\",alt:\"\"})}),`\n`,(0,n.jsxs)(e.p,{children:[\"If you have a flag named 'test-flag' in your launch darkly account you can go ahead and toggle it from 'false' to 'true' and vice versa and reload your environment to see the changes. If you would like to use a different flag, you only need to make one change in: \",(0,n.jsx)(e.strong,{children:\"app/views/welcome/index.html.erb\"})]}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-ruby\",children:`# app/views/welcome/index.html.erb\ntest_flag = Rails.configuration.ld_client.variation(\n  \"test-flag\",\n  {key: \"user@test.com\"},\n  false\n)\n`})}),`\n`,(0,n.jsx)(e.p,{children:\"Once you have changed 'test-flag' to the flag name of your choosing, you only need to commit and push the change to github. Once that happens, Release will automatically do a build and then deploy your changes. When the process finishes, you will be able to see the welcome page change based on your new flag.\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"In your LaunchDarkly interface you will see a newly created environment with a name of the form 'tedfe34`. This name is the same as your \",(0,n.jsx)(e.strong,{children:\"RELEASE_ENV_ID\"}),\" environment variable that Release creates automatically for your new environment. You will see this value in a few places in the Release UI besides the environment variable editor.\"]}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/9d8569299b8c2b955944204408f47141.png\",alt:\"\"})}),`\n`,(0,n.jsxs)(e.h3,{id:\"conclusion---whats-next\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#conclusion---whats-next\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Conclusion - What's next?\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Now that you can get pristine feature flag environments dedicated to your Release environments what's next? In this example the clean-up would need to be done manually, not a huge deal, but we can do better. Release will be implementing a deeper integration with LaunchDarkly in the near future to make this seamless and handle deleting the environments in LaunchDarkly when your Release environment is removed.\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"Stay tuned for integrations with other feature flag providers in the future. If you would like to have environments for your applications that are fast, simple to define and incredibly powerful send us a note at \",(0,n.jsx)(e.a,{href:\"mailto:support@releasehub.com\",children:\"support@releasehub.com\"}),\" and we will help you and your team become more efficient utilizing ephemeral environments with Release.\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"Hero Image by \",(0,n.jsx)(e.a,{href:\"https://unsplash.com/@dnevozhai?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText\",children:\"Denys Nevozhai\"}),\" on \",(0,n.jsx)(e.a,{href:\"https://unsplash.com/s/photos/traffic?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText\",children:\"Unsplash\"})]}),`\n`,(0,n.jsxs)(e.h3,{id:\"additional-resources\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#additional-resources\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Additional Resources\"]}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsx)(e.li,{children:(0,n.jsx)(e.a,{href:\"https://releasehub.com/ephemeral-environments\",children:\"What is an Ephemeral Environment?\"})}),`\n`,(0,n.jsx)(e.li,{children:(0,n.jsx)(e.a,{href:\"https://releasehub.com/user-acceptance-testing-with-ephemeral-environments\",children:\"User Acceptance Testing (UAT) with Release Ephemeral Environments\"})}),`\n`,(0,n.jsx)(e.li,{children:(0,n.jsx)(e.a,{href:\"https://releasehub.com/blog/improve-developer-velocity-with-ephemeral-environments\",children:\"5 Ways To Improve Developer Velocity Using Ephemeral Environments\"})}),`\n`,(0,n.jsx)(e.li,{children:(0,n.jsx)(e.a,{href:\"https://releasehub.com/blog/you-need-agile-devops-methodology\",children:\"Agile DevOps Methodology\"})}),`\n`]}),`\n`,(0,n.jsx)(e.p,{children:\"\\u200D\"})]})}function k(a={}){let{wrapper:e}=a.components||{};return e?(0,n.jsx)(e,Object.assign({},a,{children:(0,n.jsx)(h,a)})):h(a)}var _=k;return b(D);})();\n;return Component;"
        },
        "_id": "blog/posts/feature-flags-and-ephemeral-environments.mdx",
        "_raw": {
          "sourceFilePath": "blog/posts/feature-flags-and-ephemeral-environments.mdx",
          "sourceFileName": "feature-flags-and-ephemeral-environments.mdx",
          "sourceFileDir": "blog/posts",
          "contentType": "mdx",
          "flattenedPath": "blog/posts/feature-flags-and-ephemeral-environments"
        },
        "type": "BlogPost",
        "computedSlug": "feature-flags-and-ephemeral-environments"
      },
      "documentHash": "1739393595018",
      "hasWarnings": false,
      "documentTypeName": "BlogPost"
    },
    "blog/posts/free-minecraft-server-running-on-releaseapp-io.mdx": {
      "document": {
        "title": "How To: Get a Free Minecraft Server Running on Release",
        "summary": "Setup your own free Minecraft Server running on release.com One of the coolest things about working at Release",
        "publishDate": "Tue Jan 26 2021 22:22:08 GMT+0000 (Coordinated Universal Time)",
        "author": "tommy-mcclung",
        "readingTime": 6,
        "categories": [
          "platform-engineering",
          "product"
        ],
        "mainImage": "/blog-images/5341d2909ed85617cef2b40c9a1d025c.jpg",
        "imageAlt": "Minecraft game miniature",
        "showCTA": true,
        "ctaCopy": "Unlock seamless collaboration and streamline Minecraft server setup with Release's ephemeral environments. Try now!",
        "ctaLink": "https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=free-minecraft-server-running-on-releaseapp-io",
        "relatedPosts": [
          ""
        ],
        "ogImage": "/blog-images/5341d2909ed85617cef2b40c9a1d025c.jpg",
        "excerpt": "Setup your own free Minecraft Server running on release.com One of the coolest things about working at Release",
        "tags": [
          "platform-engineering",
          "product"
        ],
        "ctaButton": "Try Release for Free",
        "body": {
          "raw": "\n### Setup your own free Minecraft Server running on release.com\n\nOne of the coolest things about working at Release has been figuring out all of the fun stuff that we can do with the platform. While our main use case is helping people build environments for their applications, anything that runs in Docker will run easily on Release.\n\nEarly on, I found a handful of repos that helped us build out our platform. One that has been the most fun all along is the [docker-minecraft-server from itzg](https://github.com/itzg/docker-minecraft-server). I used it in the early days because it had a little complexity and a fully working docker-compose ecosystem to play around with. It's got the great side effect of when it runs, I let my kids test it out!\n\nSo while you're sipping on egg nog and enjoying a 2020 Holiday season on COVID lock-down, here's a walkthrough of how to get your very own free Minecraft server up and running on Release.\n\nI highly recommend following along on the video tutorial. I've included step by step instructions for anyone that learns better through reading or if you're confused about a step.\n\nIf you want to see a live version of this setup, we fired up our own Minecraft server that we built using these steps. So if you're bored over the Holidays, pop in and say hello! Here's our Server name if you want to say hi.\n\n> _Play Minecraft With Us on the Release Team Minecraft Server team-release-minecraft.release.com_\n\n### Full video walkthrough of this tutorial\n\n### Detailed instructions to get your Minecraft Server up and running\n\n#### Background\n\nAt the time of this writing, we have a [\"Starter\" plan](https://releasehub.com/pricing) that's free so you can give this a shot and have some Holiday fun on Release. Since we're hosting all of the environments on Release on the starter plan we have a limitation of 2Gb/container. That's sufficient for a Minecraft server for your kids and their friends.\n\nTo get started, take a look at [https://github.com/awesome-release/docker-minecraft-server](https://github.com/awesome-release/docker-minecraft-server), which we cloned from [itzg](https://github.com/itzg/docker-minecraft-server). Fork or clone this repo into your GitHub account so you've got your own version of it to play around with.\n\nOnce you've got your own repo to work with, I recommend taking a quick read through the [README](https://github.com/awesome-release/docker-minecraft-server/blob/master/README.md), there are a lot of configuration options and the documentation is extremely well done.\n\nWe're also going to use the [Rcon Web Administrative portal. Take a look at the documentation](https://github.com/rcon-web-admin/rcon-web-admin), _specifically the environment variables that can be configured._ itzg made a version of this for Docker called [docker-rcon-web-admin](https://github.com/itzg/docker-rcon-web-admin) that we are using when when we load the rcon and rcon-ws services in this tutorial.\n\nFor this walkthrough, we're going to bring up a vanilla Minecraft server with an Rcon administrative portal running in a standalone container. This will let you and your kids have full control over the Minecraft server and ban friends who can't fight off Zombie Pig Men. Here's an overview of what the system architecture looks like.\n\n![](/blog-images/0c360b5a12a9ca53a4ec6b345d3e752f.png)\n\nThe master branch of this repo is already setup to work with this docker-compose file in Release. Take a look at the .release.yaml file in the root of the repo's directory.\n\n```markdown\ncompose: examples/docker-compose-with-rcon.yml\n```\n\nThis sets the compose directive to \\`examples/docker-compose-with-rcon.yml' which tells Release that's the docker-compose file you want to use. If you want to play around with a Forge server or other examples, just point the .release.yaml file at the corresponding docker-compose.\n\n#### 1\\. Create a new application in Release\n\nOk, let's setup the server.\n\n- Fork, clone or copy this repo: [https://github.com/awesome-release/docker-minecraft-server](https://github.com/awesome-release/docker-minecraft-server) [Here are some simple instructions on how to copy this repo over to your account.](https://docs.github.com/en/free-pro-team@latest/github/creating-cloning-and-archiving-repositories/duplicating-a-repository)\n- Login or create an account on Release here: [https://release.com](https://release.com)\n- Follow the steps to create your account. Once your account is created, click the \"Create an application\" button.\n- Select your docker-minecraft-server repo. If you don't see it in the list, click Configure the Release app on Github link to assign permissions to your repo.\n\n![](/blog-images/a89dd77b4c398ceab315d9f30c4e86ca.png)\n\n- Add a name for your application. Note this name is used in your server hostname.\n- Click Generate App Template.\n\n![](/blog-images/5f870faf272b9393efc03492ebcaf3c1.png)\n\n#### 2\\. Edit the generated application template\n\nRelease automatically detects and creates an application template from the docker-compose file but there are a few edits we need to make based on how this repo works and to make sure we can fit the server into the Starter plan. If you want to dive in, [read the documentation about Release Application Templates.](https://docs.releasehub.com/reference-guide/application-settings/application-template)\n\nFor a little background, take a look at this diagram.\n\n![](/blog-images/08b2b54fa18888db6e0d9a3b65e43a7e.png)\n\nWe need to make our application reflect this networking setup.\n\nIn Release we have two different kinds of loadbalancers based on Amazon's [ELB's](https://aws.amazon.com/elasticloadbalancing/?elb-whats-new.sort-by=item.additionalFields.postDateTime&elb-whats-new.sort-order=desc) and [ALB's](https://docs.aws.amazon.com/elasticloadbalancing/latest/application/introduction.html).\n\nWe also need to make sure we're using the correct type of port for the use case. There are two types of ports container_port and node_port. In short, a node_port is exposed to the Internet and a conatiner_port is not. Because the rcon service is only internally facing, we want to set its port to a container_port. For more info on setting the correct type of port, [read about ports in Release](https://docs.releasehub.com/reference-guide/application-settings/application-template#ports).\n\nSo let's make the changes necessary to setup the Application Template correctly.\n\n##### Update memory to 2Gb\n\n![](/blog-images/abeda7a470111ccd00db09f965aac549.png)\n\n- The Minecraft server is setup to use 1Gb of max memory so we need to set the default memory limit in Release to 2Gb to leave enough room with some overhead. Edit the app template to allow the services to use up to 2Gb of memory.\n\n##### Update hostnames and ports\n\n![](/blog-images/4326ec140793b380eaf79ab9e6b1a7c4.gif)\n\n- Change the port type for 25575 to container_port and remove the target_port line.\n- Add a loadbalancer: true for port 25565.\n- Add a hostname field at the same level as ports in the file and set to `hostname: my-server-ENV_ID-domain`. You can set my-server to anything you'd like. The ENV_ID and domain values are automatically filled in by Release to customize your domain.  \n  ‍\n\n![](/blog-images/c85bb2c3e67c537ccc493d9eda22425e.gif)\n\n- Remove the ALB hostname for the minecraft service. (We only need the minecraft service exposed on port 25565 via an ELB not an ALB which is for http/https).\n- Click \"Save and Continue\".\n\n#### 3\\. Setup Environment Variables\n\nWe need to set a few passwords via environment variables and an [environment variable mapping](https://docs.releasehub.com/reference-guide/reference-examples/environment-variable-mappings) for the rcon websocket hostname. For more information about these environment variables, see the documentation/README files here:\n\n- [Rcon environment variables for the minecraft service.](https://github.com/awesome-release/docker-minecraft-server/blob/master/README.md#rcon)\n- [Environment variables for rcon-web-admin](https://github.com/rcon-web-admin/rcon-web-admin#environment-variables)\n\nIn this diagram we show the passwords that need to be set via environment variables.\n\n![](/blog-images/08b2b54fa18888db6e0d9a3b65e43a7e.png)\n\n##### Setup passwords via environment variables\n\n![](/blog-images/f022b2ff1fc09bdbf8b25239a3a912f7.gif)\n\nOn the minecraft service we need to set a password for its local rcon service on port 25575 so other containers can connect to it. RCON_PASSWORD is the environment variable that needs to be set for this and on the rcon and rcon-ws service we need to set RWA_RCON_PASSWORD to the same value so those services can control the minecraft server.\n\n![](/blog-images/433a262c803989cbfce78b2e4981c892.gif)\n\n- Click on \"Edit\" for \"Default Environment Variables\".\n- Set RCON_PASSWORD in the minecraft service and add secret: true. To encrypt this value in the database.\n- Set RWA_RCON_PASSWORD to the same value as you set in step 2 on both the rcon and rcon-ws services.\n- Set RWA_PASSWORD which will be the default password used for the RCON Web Administration tool in both the rcon and rcon-ws services. Make sure to add secret: true to encrypt this value.\n\n##### Setup mapping of environment variable RWA_WEBSOCKET_URL_SSL\n\n![](/blog-images/9f957b2242f814532caf3f6ecc6027f6.png)\n\nThe last environment variable we need to add is a mapping that tells Release to map RWA_WEBSOCKET_URL_SSL to a dynamically created environment variable for hostnames created in Release RCON_WS_INGRESS_HOST. RWA_WEBSOCKET_URL_SSL tells the Rcon Web Admin tool which container host url is running the websocket for this service which is on our rcon-ws service on port 4327.\n\nRCON_WS_INGRESS_HOST is automatically created everytime a new environment is created by Release and always conatins the correct hostname for rcon-ws. This value can change when new environments are created, thus we can't just hard set RWA_WEBSOCKET_URL_SSL. This is where an environment variable mapping comes into play. The diagram above represents the change we need to add in our Default Environment Variables.\n\n![](/blog-images/f022b2ff1fc09bdbf8b25239a3a912f7.gif)\n\n- Add a mapping: directive and map RWA_WEBSOCKET_URL_SSL to the top of the file.\n\n```yaml\nmapping:\n  RWA_WEBSOCKET_URL_SSL: wss://${RCON_WS_INGRESS_HOST}\n```\n\nWhen these chages and your env passwords have been made, your file should look like this:\n\n```yaml\n---\nmapping:\n  RWA_WEBSOCKET_URL_SSL: wss://${RCON_WS_INGRESS_HOST}\ndefaults:\n  - key: RWA_RCON_HOST\n    value: minecraft\nservices:\n  minecraft:\n    - key: EULA\n      value: \"TRUE\"\n    - key: MAX_MEMORY\n      value: 1G\n    - key: ENABLE_RCON\n      value: true\n    - key: RCON_PASSWORD\n      value: \"rcon_password\"\n      secret: true\n    - key: VIEW_DISTANCE\n      value: 15\n    - key: MAX_BUILD_HEIGHT\n      value: 256\n  rcon:\n    - key: RWA_RCON_HOST\n      value: minecraft\n    - key: RWA_RCON_PASSWORD\n      value: \"rcon_password\"\n      secret: TRUE\n    - key: RWA_PASSWORD\n      value: \"rwa_password\"\n      secret: true\n  rcon-ws:\n    - key: RWA_RCON_HOST\n      value: minecraft\n    - key: RWA_RCON_PASSWORD\n      value: \"rcon_password\"\n      secret: TRUE\n    - key: RWA_PASSWORD\n      value: \"rwa_password\"\n      secret: true\n```\n\n- Click 'Save & Deploy'\n\nYour environment is now deploying, you can click on the deploy and watch its progress. When it's done, navigate to the environment screen and inspect your created hostnames.\n\n#### 4\\. Setup the Minecraft Client to Connect to your new server and login to the RCON web admin tool.\n\n![](/blog-images/6693f0654bc562ed1607cd8ba4dee908.gif)\n\n- Using the minecraft hostname that was created by Release, create a new server within the Minecraft Client.\n\n![](/blog-images/c89a424e4158b038f9d686bf7d6c23cc.gif)\n\n- Click on the rcon hostname that was created by Release to access the RCON Web Admin user interface.\n- Login using the same password you set for the RWA_PASSWORD environment variable.\n- Add the minecraft server.\n- Add the console widget.\n- Run admin commands on your server!\n\n### What if it doesn't work???\n\nIf for any reason you made a mistake and something doesn't work. You can navigate to your App Settings and edit your Application Template and your Default Environment Variables. Double check you've made the proper settings. Once you've made these edits, navigate to your environments screen, delete your environment and create a new one. The beauty of Release is environments can be torn down and up whenever you want. Here are the links to the docs on how to edit your App Template and Default Environment Variables.\n\n- [Modify Application Settings](https://docs.releasehub.com/reference-guide/application-settings)\n\n##### Delete and Create a new Environment\n\n![](/blog-images/729f1b1ff6e959b52cb9720a570e4931.gif)\n\n### Conclusion\n\nYou now have your very own Minecraft Server running on the Release Starter Plan. This server was created in an Ephemeral Environment in Release and will destroy itself in 7 days. If you'd like your server to remain indefinitely, you'll need to delete the environment and re-create it as a permanent environment.\n\n![](/blog-images/729f1b1ff6e959b52cb9720a570e4931.gif)\n\nMake sure you choose permanent when creating the environment.\n\nWith the RCON Web Admin tool you can control and make the server your own special place. If you have any questions, please contact the Release team at [hello@release.com](mailto:hello@release.com). Jump in and say hello on our Release Team Minecraft Server here:\n\n> _team-release-minecraft.releasehub.com_\n\n[Don't forget to signup for your free Release account on our starter plan!](https://app.releasehub.com/auth/login-page)\n\nWe also have a Release Community Discord you can join if you need help or just want to say hello. Join us here: [Release Community Discord](https://discord.gg/8FKKZvBKwc)\n**Happy Holidays from the Release Team!!!**\n",
          "code": "var Component=(()=>{var d=Object.create;var o=Object.defineProperty;var m=Object.getOwnPropertyDescriptor;var p=Object.getOwnPropertyNames;var u=Object.getPrototypeOf,f=Object.prototype.hasOwnProperty;var g=(a,e)=>()=>(e||a((e={exports:{}}).exports,e),e.exports),v=(a,e)=>{for(var t in e)o(a,t,{get:e[t],enumerable:!0})},s=(a,e,t,i)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let r of p(e))!f.call(a,r)&&r!==t&&o(a,r,{get:()=>e[r],enumerable:!(i=m(e,r))||i.enumerable});return a};var b=(a,e,t)=>(t=a!=null?d(u(a)):{},s(e||!a||!a.__esModule?o(t,\"default\",{value:a,enumerable:!0}):t,a)),w=a=>s(o({},\"__esModule\",{value:!0}),a);var l=g((A,c)=>{c.exports=_jsx_runtime});var S={};v(S,{default:()=>R,frontmatter:()=>y});var n=b(l()),y={title:\"How To: Get a Free Minecraft Server Running on Release\",summary:\"Setup your own free Minecraft Server running on release.com One of the coolest things about working at Release\",publishDate:\"Tue Jan 26 2021 22:22:08 GMT+0000 (Coordinated Universal Time)\",author:\"tommy-mcclung\",readingTime:6,categories:[\"platform-engineering\",\"product\"],mainImage:\"/blog-images/5341d2909ed85617cef2b40c9a1d025c.jpg\",imageAlt:\"Minecraft game miniature\",showCTA:!0,ctaCopy:\"Unlock seamless collaboration and streamline Minecraft server setup with Release's ephemeral environments. Try now!\",ctaLink:\"https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=free-minecraft-server-running-on-releaseapp-io\",relatedPosts:[\"\"],ogImage:\"/blog-images/5341d2909ed85617cef2b40c9a1d025c.jpg\",excerpt:\"Setup your own free Minecraft Server running on release.com One of the coolest things about working at Release\",tags:[\"platform-engineering\",\"product\"],ctaButton:\"Try Release for Free\"};function h(a){let e=Object.assign({h3:\"h3\",a:\"a\",span:\"span\",p:\"p\",blockquote:\"blockquote\",em:\"em\",h4:\"h4\",img:\"img\",pre:\"pre\",code:\"code\",ul:\"ul\",li:\"li\",h5:\"h5\",br:\"br\",strong:\"strong\"},a.components);return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsxs)(e.h3,{id:\"setup-your-own-free-minecraft-server-running-on-releasecom\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#setup-your-own-free-minecraft-server-running-on-releasecom\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Setup your own free Minecraft Server running on release.com\"]}),`\n`,(0,n.jsx)(e.p,{children:\"One of the coolest things about working at Release has been figuring out all of the fun stuff that we can do with the platform. While our main use case is helping people build environments for their applications, anything that runs in Docker will run easily on Release.\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"Early on, I found a handful of repos that helped us build out our platform. One that has been the most fun all along is the \",(0,n.jsx)(e.a,{href:\"https://github.com/itzg/docker-minecraft-server\",children:\"docker-minecraft-server from itzg\"}),\". I used it in the early days because it had a little complexity and a fully working docker-compose ecosystem to play around with. It's got the great side effect of when it runs, I let my kids test it out!\"]}),`\n`,(0,n.jsx)(e.p,{children:\"So while you're sipping on egg nog and enjoying a 2020 Holiday season on COVID lock-down, here's a walkthrough of how to get your very own free Minecraft server up and running on Release.\"}),`\n`,(0,n.jsx)(e.p,{children:\"I highly recommend following along on the video tutorial. I've included step by step instructions for anyone that learns better through reading or if you're confused about a step.\"}),`\n`,(0,n.jsx)(e.p,{children:\"If you want to see a live version of this setup, we fired up our own Minecraft server that we built using these steps. So if you're bored over the Holidays, pop in and say hello! Here's our Server name if you want to say hi.\"}),`\n`,(0,n.jsxs)(e.blockquote,{children:[`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.em,{children:\"Play Minecraft With Us on the Release Team Minecraft Server team-release-minecraft.release.com\"})}),`\n`]}),`\n`,(0,n.jsxs)(e.h3,{id:\"full-video-walkthrough-of-this-tutorial\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#full-video-walkthrough-of-this-tutorial\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Full video walkthrough of this tutorial\"]}),`\n`,(0,n.jsxs)(e.h3,{id:\"detailed-instructions-to-get-your-minecraft-server-up-and-running\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#detailed-instructions-to-get-your-minecraft-server-up-and-running\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Detailed instructions to get your Minecraft Server up and running\"]}),`\n`,(0,n.jsxs)(e.h4,{id:\"background\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#background\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Background\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"At the time of this writing, we have a \",(0,n.jsx)(e.a,{href:\"https://releasehub.com/pricing\",children:'\"Starter\" plan'}),\" that's free so you can give this a shot and have some Holiday fun on Release. Since we're hosting all of the environments on Release on the starter plan we have a limitation of 2Gb/container. That's sufficient for a Minecraft server for your kids and their friends.\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"To get started, take a look at \",(0,n.jsx)(e.a,{href:\"https://github.com/awesome-release/docker-minecraft-server\",children:\"https://github.com/awesome-release/docker-minecraft-server\"}),\", which we cloned from \",(0,n.jsx)(e.a,{href:\"https://github.com/itzg/docker-minecraft-server\",children:\"itzg\"}),\". Fork or clone this repo into your GitHub account so you've got your own version of it to play around with.\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"Once you've got your own repo to work with, I recommend taking a quick read through the \",(0,n.jsx)(e.a,{href:\"https://github.com/awesome-release/docker-minecraft-server/blob/master/README.md\",children:\"README\"}),\", there are a lot of configuration options and the documentation is extremely well done.\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"We're also going to use the \",(0,n.jsx)(e.a,{href:\"https://github.com/rcon-web-admin/rcon-web-admin\",children:\"Rcon Web Administrative portal. Take a look at the documentation\"}),\", \",(0,n.jsx)(e.em,{children:\"specifically the environment variables that can be configured.\"}),\" itzg made a version of this for Docker called \",(0,n.jsx)(e.a,{href:\"https://github.com/itzg/docker-rcon-web-admin\",children:\"docker-rcon-web-admin\"}),\" that we are using when when we load the rcon and rcon-ws services in this tutorial.\"]}),`\n`,(0,n.jsx)(e.p,{children:\"For this walkthrough, we're going to bring up a vanilla Minecraft server with an Rcon administrative portal running in a standalone container. This will let you and your kids have full control over the Minecraft server and ban friends who can't fight off Zombie Pig Men. Here's an overview of what the system architecture looks like.\"}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/0c360b5a12a9ca53a4ec6b345d3e752f.png\",alt:\"\"})}),`\n`,(0,n.jsx)(e.p,{children:\"The master branch of this repo is already setup to work with this docker-compose file in Release. Take a look at the .release.yaml file in the root of the repo's directory.\"}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-markdown\",children:`compose: examples/docker-compose-with-rcon.yml\n`})}),`\n`,(0,n.jsx)(e.p,{children:\"This sets the compose directive to `examples/docker-compose-with-rcon.yml' which tells Release that's the docker-compose file you want to use. If you want to play around with a Forge server or other examples, just point the .release.yaml file at the corresponding docker-compose.\"}),`\n`,(0,n.jsxs)(e.h4,{id:\"1-create-a-new-application-in-release\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#1-create-a-new-application-in-release\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"1. Create a new application in Release\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Ok, let's setup the server.\"}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsxs)(e.li,{children:[\"Fork, clone or copy this repo: \",(0,n.jsx)(e.a,{href:\"https://github.com/awesome-release/docker-minecraft-server\",children:\"https://github.com/awesome-release/docker-minecraft-server\"}),\" \",(0,n.jsx)(e.a,{href:\"https://docs.github.com/en/free-pro-team@latest/github/creating-cloning-and-archiving-repositories/duplicating-a-repository\",children:\"Here are some simple instructions on how to copy this repo over to your account.\"})]}),`\n`,(0,n.jsxs)(e.li,{children:[\"Login or create an account on Release here: \",(0,n.jsx)(e.a,{href:\"https://release.com\",children:\"https://release.com\"})]}),`\n`,(0,n.jsx)(e.li,{children:'Follow the steps to create your account. Once your account is created, click the \"Create an application\" button.'}),`\n`,(0,n.jsx)(e.li,{children:\"Select your docker-minecraft-server repo. If you don't see it in the list, click Configure the Release app on Github link to assign permissions to your repo.\"}),`\n`]}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/a89dd77b4c398ceab315d9f30c4e86ca.png\",alt:\"\"})}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsx)(e.li,{children:\"Add a name for your application. Note this name is used in your server hostname.\"}),`\n`,(0,n.jsx)(e.li,{children:\"Click Generate App Template.\"}),`\n`]}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/5f870faf272b9393efc03492ebcaf3c1.png\",alt:\"\"})}),`\n`,(0,n.jsxs)(e.h4,{id:\"2-edit-the-generated-application-template\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#2-edit-the-generated-application-template\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"2. Edit the generated application template\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"Release automatically detects and creates an application template from the docker-compose file but there are a few edits we need to make based on how this repo works and to make sure we can fit the server into the Starter plan. If you want to dive in, \",(0,n.jsx)(e.a,{href:\"https://docs.releasehub.com/reference-guide/application-settings/application-template\",children:\"read the documentation about Release Application Templates.\"})]}),`\n`,(0,n.jsx)(e.p,{children:\"For a little background, take a look at this diagram.\"}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/08b2b54fa18888db6e0d9a3b65e43a7e.png\",alt:\"\"})}),`\n`,(0,n.jsx)(e.p,{children:\"We need to make our application reflect this networking setup.\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"In Release we have two different kinds of loadbalancers based on Amazon's \",(0,n.jsx)(e.a,{href:\"https://aws.amazon.com/elasticloadbalancing/?elb-whats-new.sort-by=item.additionalFields.postDateTime&elb-whats-new.sort-order=desc\",children:\"ELB's\"}),\" and \",(0,n.jsx)(e.a,{href:\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/introduction.html\",children:\"ALB's\"}),\".\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"We also need to make sure we're using the correct type of port for the use case. There are two types of ports container_port and node_port. In short, a node_port is exposed to the Internet and a conatiner_port is not. Because the rcon service is only internally facing, we want to set its port to a container_port. For more info on setting the correct type of port, \",(0,n.jsx)(e.a,{href:\"https://docs.releasehub.com/reference-guide/application-settings/application-template#ports\",children:\"read about ports in Release\"}),\".\"]}),`\n`,(0,n.jsx)(e.p,{children:\"So let's make the changes necessary to setup the Application Template correctly.\"}),`\n`,(0,n.jsxs)(e.h5,{id:\"update-memory-to-2gb\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#update-memory-to-2gb\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Update memory to 2Gb\"]}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/abeda7a470111ccd00db09f965aac549.png\",alt:\"\"})}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsx)(e.li,{children:\"The Minecraft server is setup to use 1Gb of max memory so we need to set the default memory limit in Release to 2Gb to leave enough room with some overhead. Edit the app template to allow the services to use up to 2Gb of memory.\"}),`\n`]}),`\n`,(0,n.jsxs)(e.h5,{id:\"update-hostnames-and-ports\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#update-hostnames-and-ports\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Update hostnames and ports\"]}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/4326ec140793b380eaf79ab9e6b1a7c4.gif\",alt:\"\"})}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsx)(e.li,{children:\"Change the port type for 25575 to container_port and remove the target_port line.\"}),`\n`,(0,n.jsx)(e.li,{children:\"Add a loadbalancer: true for port 25565.\"}),`\n`,(0,n.jsxs)(e.li,{children:[\"Add a hostname field at the same level as ports in the file and set to \",(0,n.jsx)(e.code,{children:\"hostname: my-server-ENV_ID-domain\"}),\". You can set my-server to anything you'd like. The ENV_ID and domain values are automatically filled in by Release to customize your domain.\",(0,n.jsx)(e.br,{}),`\n`,\"\\u200D\"]}),`\n`]}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/c85bb2c3e67c537ccc493d9eda22425e.gif\",alt:\"\"})}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsx)(e.li,{children:\"Remove the ALB hostname for the minecraft service. (We only need the minecraft service exposed on port 25565 via an ELB not an ALB which is for http/https).\"}),`\n`,(0,n.jsx)(e.li,{children:'Click \"Save and Continue\".'}),`\n`]}),`\n`,(0,n.jsxs)(e.h4,{id:\"3-setup-environment-variables\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#3-setup-environment-variables\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"3. Setup Environment Variables\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"We need to set a few passwords via environment variables and an \",(0,n.jsx)(e.a,{href:\"https://docs.releasehub.com/reference-guide/reference-examples/environment-variable-mappings\",children:\"environment variable mapping\"}),\" for the rcon websocket hostname. For more information about these environment variables, see the documentation/README files here:\"]}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsx)(e.li,{children:(0,n.jsx)(e.a,{href:\"https://github.com/awesome-release/docker-minecraft-server/blob/master/README.md#rcon\",children:\"Rcon environment variables for the minecraft service.\"})}),`\n`,(0,n.jsx)(e.li,{children:(0,n.jsx)(e.a,{href:\"https://github.com/rcon-web-admin/rcon-web-admin#environment-variables\",children:\"Environment variables for rcon-web-admin\"})}),`\n`]}),`\n`,(0,n.jsx)(e.p,{children:\"In this diagram we show the passwords that need to be set via environment variables.\"}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/08b2b54fa18888db6e0d9a3b65e43a7e.png\",alt:\"\"})}),`\n`,(0,n.jsxs)(e.h5,{id:\"setup-passwords-via-environment-variables\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#setup-passwords-via-environment-variables\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Setup passwords via environment variables\"]}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/f022b2ff1fc09bdbf8b25239a3a912f7.gif\",alt:\"\"})}),`\n`,(0,n.jsx)(e.p,{children:\"On the minecraft service we need to set a password for its local rcon service on port 25575 so other containers can connect to it. RCON_PASSWORD is the environment variable that needs to be set for this and on the rcon and rcon-ws service we need to set RWA_RCON_PASSWORD to the same value so those services can control the minecraft server.\"}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/433a262c803989cbfce78b2e4981c892.gif\",alt:\"\"})}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsx)(e.li,{children:'Click on \"Edit\" for \"Default Environment Variables\".'}),`\n`,(0,n.jsx)(e.li,{children:\"Set RCON_PASSWORD in the minecraft service and add secret: true. To encrypt this value in the database.\"}),`\n`,(0,n.jsx)(e.li,{children:\"Set RWA_RCON_PASSWORD to the same value as you set in step 2 on both the rcon and rcon-ws services.\"}),`\n`,(0,n.jsx)(e.li,{children:\"Set RWA_PASSWORD which will be the default password used for the RCON Web Administration tool in both the rcon and rcon-ws services. Make sure to add secret: true to encrypt this value.\"}),`\n`]}),`\n`,(0,n.jsxs)(e.h5,{id:\"setup-mapping-of-environment-variable-rwa_websocket_url_ssl\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#setup-mapping-of-environment-variable-rwa_websocket_url_ssl\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Setup mapping of environment variable RWA_WEBSOCKET_URL_SSL\"]}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/9f957b2242f814532caf3f6ecc6027f6.png\",alt:\"\"})}),`\n`,(0,n.jsx)(e.p,{children:\"The last environment variable we need to add is a mapping that tells Release to map RWA_WEBSOCKET_URL_SSL to a dynamically created environment variable for hostnames created in Release RCON_WS_INGRESS_HOST. RWA_WEBSOCKET_URL_SSL tells the Rcon Web Admin tool which container host url is running the websocket for this service which is on our rcon-ws service on port 4327.\"}),`\n`,(0,n.jsx)(e.p,{children:\"RCON_WS_INGRESS_HOST is automatically created everytime a new environment is created by Release and always conatins the correct hostname for rcon-ws. This value can change when new environments are created, thus we can't just hard set RWA_WEBSOCKET_URL_SSL. This is where an environment variable mapping comes into play. The diagram above represents the change we need to add in our Default Environment Variables.\"}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/f022b2ff1fc09bdbf8b25239a3a912f7.gif\",alt:\"\"})}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsx)(e.li,{children:\"Add a mapping: directive and map RWA_WEBSOCKET_URL_SSL to the top of the file.\"}),`\n`]}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-yaml\",children:`mapping:\n  RWA_WEBSOCKET_URL_SSL: wss://\\${RCON_WS_INGRESS_HOST}\n`})}),`\n`,(0,n.jsx)(e.p,{children:\"When these chages and your env passwords have been made, your file should look like this:\"}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-yaml\",children:`---\nmapping:\n  RWA_WEBSOCKET_URL_SSL: wss://\\${RCON_WS_INGRESS_HOST}\ndefaults:\n  - key: RWA_RCON_HOST\n    value: minecraft\nservices:\n  minecraft:\n    - key: EULA\n      value: \"TRUE\"\n    - key: MAX_MEMORY\n      value: 1G\n    - key: ENABLE_RCON\n      value: true\n    - key: RCON_PASSWORD\n      value: \"rcon_password\"\n      secret: true\n    - key: VIEW_DISTANCE\n      value: 15\n    - key: MAX_BUILD_HEIGHT\n      value: 256\n  rcon:\n    - key: RWA_RCON_HOST\n      value: minecraft\n    - key: RWA_RCON_PASSWORD\n      value: \"rcon_password\"\n      secret: TRUE\n    - key: RWA_PASSWORD\n      value: \"rwa_password\"\n      secret: true\n  rcon-ws:\n    - key: RWA_RCON_HOST\n      value: minecraft\n    - key: RWA_RCON_PASSWORD\n      value: \"rcon_password\"\n      secret: TRUE\n    - key: RWA_PASSWORD\n      value: \"rwa_password\"\n      secret: true\n`})}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsx)(e.li,{children:\"Click 'Save & Deploy'\"}),`\n`]}),`\n`,(0,n.jsx)(e.p,{children:\"Your environment is now deploying, you can click on the deploy and watch its progress. When it's done, navigate to the environment screen and inspect your created hostnames.\"}),`\n`,(0,n.jsxs)(e.h4,{id:\"4-setup-the-minecraft-client-to-connect-to-your-new-server-and-login-to-the-rcon-web-admin-tool\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#4-setup-the-minecraft-client-to-connect-to-your-new-server-and-login-to-the-rcon-web-admin-tool\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"4. Setup the Minecraft Client to Connect to your new server and login to the RCON web admin tool.\"]}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/6693f0654bc562ed1607cd8ba4dee908.gif\",alt:\"\"})}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsx)(e.li,{children:\"Using the minecraft hostname that was created by Release, create a new server within the Minecraft Client.\"}),`\n`]}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/c89a424e4158b038f9d686bf7d6c23cc.gif\",alt:\"\"})}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsx)(e.li,{children:\"Click on the rcon hostname that was created by Release to access the RCON Web Admin user interface.\"}),`\n`,(0,n.jsx)(e.li,{children:\"Login using the same password you set for the RWA_PASSWORD environment variable.\"}),`\n`,(0,n.jsx)(e.li,{children:\"Add the minecraft server.\"}),`\n`,(0,n.jsx)(e.li,{children:\"Add the console widget.\"}),`\n`,(0,n.jsx)(e.li,{children:\"Run admin commands on your server!\"}),`\n`]}),`\n`,(0,n.jsxs)(e.h3,{id:\"what-if-it-doesnt-work\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#what-if-it-doesnt-work\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"What if it doesn't work???\"]}),`\n`,(0,n.jsx)(e.p,{children:\"If for any reason you made a mistake and something doesn't work. You can navigate to your App Settings and edit your Application Template and your Default Environment Variables. Double check you've made the proper settings. Once you've made these edits, navigate to your environments screen, delete your environment and create a new one. The beauty of Release is environments can be torn down and up whenever you want. Here are the links to the docs on how to edit your App Template and Default Environment Variables.\"}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsx)(e.li,{children:(0,n.jsx)(e.a,{href:\"https://docs.releasehub.com/reference-guide/application-settings\",children:\"Modify Application Settings\"})}),`\n`]}),`\n`,(0,n.jsxs)(e.h5,{id:\"delete-and-create-a-new-environment\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#delete-and-create-a-new-environment\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Delete and Create a new Environment\"]}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/729f1b1ff6e959b52cb9720a570e4931.gif\",alt:\"\"})}),`\n`,(0,n.jsxs)(e.h3,{id:\"conclusion\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#conclusion\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Conclusion\"]}),`\n`,(0,n.jsx)(e.p,{children:\"You now have your very own Minecraft Server running on the Release Starter Plan. This server was created in an Ephemeral Environment in Release and will destroy itself in 7 days. If you'd like your server to remain indefinitely, you'll need to delete the environment and re-create it as a permanent environment.\"}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/729f1b1ff6e959b52cb9720a570e4931.gif\",alt:\"\"})}),`\n`,(0,n.jsx)(e.p,{children:\"Make sure you choose permanent when creating the environment.\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"With the RCON Web Admin tool you can control and make the server your own special place. If you have any questions, please contact the Release team at \",(0,n.jsx)(e.a,{href:\"mailto:hello@release.com\",children:\"hello@release.com\"}),\". Jump in and say hello on our Release Team Minecraft Server here:\"]}),`\n`,(0,n.jsxs)(e.blockquote,{children:[`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.em,{children:\"team-release-minecraft.releasehub.com\"})}),`\n`]}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.a,{href:\"https://app.releasehub.com/auth/login-page\",children:\"Don't forget to signup for your free Release account on our starter plan!\"})}),`\n`,(0,n.jsxs)(e.p,{children:[\"We also have a Release Community Discord you can join if you need help or just want to say hello. Join us here: \",(0,n.jsx)(e.a,{href:\"https://discord.gg/8FKKZvBKwc\",children:\"Release Community Discord\"}),`\n`,(0,n.jsx)(e.strong,{children:\"Happy Holidays from the Release Team!!!\"})]})]})}function k(a={}){let{wrapper:e}=a.components||{};return e?(0,n.jsx)(e,Object.assign({},a,{children:(0,n.jsx)(h,a)})):h(a)}var R=k;return w(S);})();\n;return Component;"
        },
        "_id": "blog/posts/free-minecraft-server-running-on-releaseapp-io.mdx",
        "_raw": {
          "sourceFilePath": "blog/posts/free-minecraft-server-running-on-releaseapp-io.mdx",
          "sourceFileName": "free-minecraft-server-running-on-releaseapp-io.mdx",
          "sourceFileDir": "blog/posts",
          "contentType": "mdx",
          "flattenedPath": "blog/posts/free-minecraft-server-running-on-releaseapp-io"
        },
        "type": "BlogPost",
        "computedSlug": "free-minecraft-server-running-on-releaseapp-io"
      },
      "documentHash": "1739393595018",
      "hasWarnings": false,
      "documentTypeName": "BlogPost"
    },
    "blog/posts/full-fidelity-data-for-ephemeral-environments.mdx": {
      "document": {
        "title": "Full Fidelity Data for Ephemeral Environments",
        "summary": "Why you should use production-like data in ephemeral environments",
        "publishDate": "Tue Mar 07 2023 21:22:09 GMT+0000 (Coordinated Universal Time)",
        "author": "regis-wilson",
        "readingTime": 8,
        "categories": [
          "platform-engineering",
          "product"
        ],
        "mainImage": "/blog-images/6edc69cd127239703621a18ee2b28558.jpg",
        "imageAlt": "Full Fidelity Data for Ephemeral Environments",
        "showCTA": true,
        "ctaCopy": "Improve testing accuracy with Release's full-stack environments mirroring production data for seamless feature validation and bug detection.",
        "ctaLink": "https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=full-fidelity-data-for-ephemeral-environments",
        "relatedPosts": [
          ""
        ],
        "ogImage": "/blog-images/6edc69cd127239703621a18ee2b28558.jpg",
        "excerpt": "Why you should use production-like data in ephemeral environments",
        "tags": [
          "platform-engineering",
          "product"
        ],
        "ctaButton": "Try Release for Free",
        "body": {
          "raw": "\nThis blog post is a followup to the wildly successful webinar on the same topic. Refer to our webinar [“Full Fidelity Data for Ephemeral Environments”](https://www.youtube.com/watch?v=TAhz_UxPWG4) for more information. In the webinar and this post, we will explore why you should use production-like data in ephemeral environments, how you can (and should) do so, and how to reduce the cost, complexity, and avoid difficulties associated with using production-like data at scale.\n\n### Overview\n\n[Ephemeral environments](https://release.com/ephemeral-environments) are an indispensable tool in almost any software development workflow. By creating environments for testing and pre-production phases of the [workflow](https://docs.release.com/reference-documentation/workflows-in-release), advanced software development teams can shift testing and verification “left” (that is, earlier in the life cycle rather than later when features or bugs reach production).\n\nThe reason that environments should be ephemeral is that they can be set up on-demand with specific features and/or branches deployed in them and then torn down when the task is completed. Each feature or branch of the development code base can be tested in an isolated environment as opposed to the legacy shared and fixed stages of QA, [Staging](https://release.com/staging-environments), [User Acceptance Testing (UAT)](https://release.com/user-acceptance-testing-with-ephemeral-environments), and so forth.\n\nEphemeral environments are always used for a specific purpose so it is clear which feature and code base is deployed. The data is ideally based on a very close approximation to production so that the feature under development or test can be deployed into production with as much confidence as possible.\n\n### Why Use Production-Like Data in Ephemeral Environments?\n\nThe best way to test new features or fix bugs that exist in production is to use the same code and data from the production environment. Version Control Systems (VCS) like git solved this problem  decades ago, at least for  code. But the data portion has long lagged behind due to complexity in access and cost, which  we will address toward the end of this post.\n\nTesting code against stale or seed data is fine for automated testing, but when developing new features or diagnosing problems in production, the data should mimic production as closely as possible. Otherwise, you are risking chasing the same fugs over and over again, or launching a faulty feature.  \n\nIt is rare that data in production is static and unchanging; if it were, you could make the database read-only! While some amount of read-only or write-rarely data exists, it almost always needs to be updated and changed at some point, the only difference is a matter of frequency of updates.\n\nBecause the data is a living and breathing thing that changes and evolves constantly  and possibly is updated based on customer live inputs or actions, the legacy strategy of fixed QA, staging, and test databases get out of date extremely quickly. In my experience, a fixed environment will stray from production data in as short a time as a few days or weeks. Many times the QA or staging databases are several years out of date from production, unless you specifically “backport” data from production.\n\nLastly, production databases and datasets are often quite large (and grow larger every day) compared to fixed QA and staging databases. Thus, testing on limited data or fake seed data when developing new features or changing the code base can introduce unexpected regressions in performance or bugs when large results are pulled out of a table.\n\n### Should I Really Use Production-like Data in Ephemeral Environments?\n\nShort answer, yes. However, you need to be mindful of the possible moral, ethical, and legal implications in using actual production data subject to HIPPA or other regulatory controls. This is why it is crucial to generate a so-called Golden Copy of the data that is scrubbed of any private or confidential data, while still maintaining the same qualities as production in terms of size, statistical parameters, and so forth. This Golden Copy is the source of truth that is  updated frequently from your actual production data. We recommend daily updates, but depending on your particular use case it can be more or less often.\n\nWith the  Golden Copy that is sufficiently production-like (each case varies in how much fidelity to production is required), it is possible to accurately portray behavior features as they would occur in production. Transactions, events, customer (faked) accounts, and so forth are all available and up-to-date with real (-ish) data. For example, if there is a bug that manifests in production, an engineer could easily reproduce the bug with a high degree of confidence, and either validate or develop fixes for the problem.\n\nTesting features with fake or fixed data is suitable for automated testing but for many use cases, especially when testing with non-technical users, real production-like data is valuable to ensure the feature not only works properly but also looks correct when it reaches production.\n\n### Isn’t that Terribly Difficult, Expensive, and Prohibitive? Or, “My Database is too Unique.”\n\nThe most common objections to using a production-like dataset are around the difficulties in creating, managing and accessing the dataset, and around the overall cost. I can try to address these in two points: the first one (creation/maintenance/access) can be  pretty difficult depending on your use case, but it can be solved; the second one (cost) can readily be handled in this modern era of cloud computing and on-demand managed services offered by cloud providers.\n\nThe first problem is that access to production data is usually kept in the hands of a small group of people, and those people are usually not the software engineers who are developing code and testing new features. Separation of concerns and responsibilities is vital for keeping production data clean and secure, but this causes problems when engineers need to test new features, verify bugs in production, or experiment with changes that need to be made.\n\nThe best practice is to generate a Golden Copy of the data that you need as mentioned above. The Golden Copy should be a cleaned, stable, up-to-date version of the data that exist in production, but without any data that could compromise confidentiality or proprietary information if it were to accidentally be exposed, (or even internally exposed).\n\nWhat I tell most people who are involved with production datasets is to create a culture of welcoming access to the Golden Copy and distributing the data internally on a self-service model so that anyone who would like the latest snapshot can access it relatively easily and without a lot of hoops to jump through. Making the data available will ensure that your cleaning and scrubbing process is actually working properly and that your test data is going to have a high degree of similarity to the data in production. This will only make your production data and operations cleaner and more efficient, I promise you.\n\nThe second problem is that allowing software engineers, QA people, testers, and even product people access to these datasets comes at a cost. Every database will typically cost a certain amount of money to run and store the data, but there are definitely some optimisations you can implement to keep costs down while still enjoying access to high quality datasets.\n\nThe best way to keep costs down is to make sure that the requested access to the production-like dataset is limited in time. For example, a software engineer might only need to run the tests for a day or a week. At the end of that time, the data should automatically be expired and the instance removed because it is no longer needed. If the data is still needed after the initial period of time has expired, you can implement a way to extend the deadline as appropriate.\n\nAnother way to reduce costs is to use Copy on Write (COW) technologies if they are available for your database engine and cloud provider. The way this works is that the Golden Copy holds most of the data in storage for use, while the clones that are handed out to engineers are sharing most of the data with the original. It is only when a change or update is made to a table or row that the data is  “copied” over for the clone to use. This is what a Copy on Write will do: it means that the only additional costs for storage on the clone are the incremental changes or writes that are executed during testing.\n\nAnother good way to reduce costs is to pause or stop a database when it is not in use. Depending on your cloud provider, you may be able to execute a pause or stop on the database instance so that you can save money during the evenings or weekends when the database is not likely to be in use. This can save 30-60% off your costs versus running the database 24/7.\n\nThe good news is that [Release](https://docs.release.com/) offers all of the features, including cloning snapshots from the Golden Copy, pausing and restarting databases on a schedule and expiring them as well, and using COW to save time, money, and storage. We support all of the above (using [AWS](https://docs.release.com/frequently-asked-questions/aws-support-faqs), GCP and soon Azure) and we can easily build a dataset pipeline that is checked out to each [ephemeral](https://www.merriam-webster.com/dictionary/ephemeral) environment where your code is deployed.\n\nYou can refresh the dataset to get new changes from the Golden Copy (which is a cleaned version of production, or directly from production as you wish), and you can also update the data in your ephemeral environment to start over from scratch with a new database. You can also set an expiration for each ephemeral environment that will last as long as the branch or feature pull request is open, and engineers can extend the environment duration as needed. Lastly, you can set a schedule for pausing the databases in the dataset so that you save additional costs when the environments are unlikely to be used.\n\nReady to learn more? Watch the on-demand webinar [“Full Fidelity Data for Ephemeral Environments.”](https://www.youtube.com/watch?v=TAhz_UxPWG4)\n\n‍\n",
          "code": "var Component=(()=>{var h=Object.create;var i=Object.defineProperty;var u=Object.getOwnPropertyDescriptor;var p=Object.getOwnPropertyNames;var m=Object.getPrototypeOf,f=Object.prototype.hasOwnProperty;var y=(a,e)=>()=>(e||a((e={exports:{}}).exports,e),e.exports),g=(a,e)=>{for(var n in e)i(a,n,{get:e[n],enumerable:!0})},r=(a,e,n,s)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let o of p(e))!f.call(a,o)&&o!==n&&i(a,o,{get:()=>e[o],enumerable:!(s=u(e,o))||s.enumerable});return a};var w=(a,e,n)=>(n=a!=null?h(m(a)):{},r(e||!a||!a.__esModule?i(n,\"default\",{value:a,enumerable:!0}):n,a)),b=a=>r(i({},\"__esModule\",{value:!0}),a);var l=y((A,d)=>{d.exports=_jsx_runtime});var T={};g(T,{default:()=>x,frontmatter:()=>v});var t=w(l()),v={title:\"Full Fidelity Data for Ephemeral Environments\",summary:\"Why you should use production-like data in ephemeral environments\",publishDate:\"Tue Mar 07 2023 21:22:09 GMT+0000 (Coordinated Universal Time)\",author:\"regis-wilson\",readingTime:8,categories:[\"platform-engineering\",\"product\"],mainImage:\"/blog-images/6edc69cd127239703621a18ee2b28558.jpg\",imageAlt:\"Full Fidelity Data for Ephemeral Environments\",showCTA:!0,ctaCopy:\"Improve testing accuracy with Release's full-stack environments mirroring production data for seamless feature validation and bug detection.\",ctaLink:\"https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=full-fidelity-data-for-ephemeral-environments\",relatedPosts:[\"\"],ogImage:\"/blog-images/6edc69cd127239703621a18ee2b28558.jpg\",excerpt:\"Why you should use production-like data in ephemeral environments\",tags:[\"platform-engineering\",\"product\"],ctaButton:\"Try Release for Free\"};function c(a){let e=Object.assign({p:\"p\",a:\"a\",h3:\"h3\",span:\"span\"},a.components);return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsxs)(e.p,{children:[\"This blog post is a followup to the wildly successful webinar on the same topic. Refer to our webinar \",(0,t.jsx)(e.a,{href:\"https://www.youtube.com/watch?v=TAhz_UxPWG4\",children:\"\\u201CFull Fidelity Data for Ephemeral Environments\\u201D\"}),\" for more information. In the webinar and this post, we will explore why you should use production-like data in ephemeral environments, how you can (and should) do so, and how to reduce the cost, complexity, and avoid difficulties associated with using production-like data at scale.\"]}),`\n`,(0,t.jsxs)(e.h3,{id:\"overview\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#overview\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Overview\"]}),`\n`,(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.a,{href:\"https://release.com/ephemeral-environments\",children:\"Ephemeral environments\"}),\" are an indispensable tool in almost any software development workflow. By creating environments for testing and pre-production phases of the \",(0,t.jsx)(e.a,{href:\"https://docs.release.com/reference-documentation/workflows-in-release\",children:\"workflow\"}),\", advanced software development teams can shift testing and verification \\u201Cleft\\u201D (that is, earlier in the life cycle rather than later when features or bugs reach production).\"]}),`\n`,(0,t.jsxs)(e.p,{children:[\"The reason that environments should be ephemeral is that they can be set up on-demand with specific features and/or branches deployed in them and then torn down when the task is completed. Each feature or branch of the development code base can be tested in an isolated environment as opposed to the legacy shared and fixed stages of QA, \",(0,t.jsx)(e.a,{href:\"https://release.com/staging-environments\",children:\"Staging\"}),\", \",(0,t.jsx)(e.a,{href:\"https://release.com/user-acceptance-testing-with-ephemeral-environments\",children:\"User Acceptance Testing (UAT)\"}),\", and so forth.\"]}),`\n`,(0,t.jsx)(e.p,{children:\"Ephemeral environments are always used for a specific purpose so it is clear which feature and code base is deployed. The data is ideally based on a very close approximation to production so that the feature under development or test can be deployed into production with as much confidence as possible.\"}),`\n`,(0,t.jsxs)(e.h3,{id:\"why-use-production-like-data-in-ephemeral-environments\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#why-use-production-like-data-in-ephemeral-environments\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Why Use Production-Like Data in Ephemeral Environments?\"]}),`\n`,(0,t.jsx)(e.p,{children:\"The best way to test new features or fix bugs that exist in production is to use the same code and data from the production environment. Version Control Systems (VCS) like git solved this problem\\xA0 decades ago, at least for\\xA0 code. But the data portion has long lagged behind due to complexity in access and cost, which\\xA0 we will address toward the end of this post.\"}),`\n`,(0,t.jsx)(e.p,{children:\"Testing code against stale or seed data is fine for automated testing, but when developing new features or diagnosing problems in production, the data should mimic production as closely as possible. Otherwise, you are risking chasing the same fugs over and over again, or launching a faulty feature.\\xA0\\xA0\"}),`\n`,(0,t.jsx)(e.p,{children:\"It is rare that data in production is static and unchanging; if it were, you could make the database read-only! While some amount of read-only or write-rarely data exists, it almost always needs to be updated and changed at some point, the only difference is a matter of frequency of updates.\"}),`\n`,(0,t.jsx)(e.p,{children:\"Because the data is a living and breathing thing that changes and evolves constantly\\xA0 and possibly is updated based on customer live inputs or actions, the legacy strategy of fixed QA, staging, and test databases get out of date extremely quickly. In my experience, a fixed environment will stray from production data in as short a time as a few days or weeks. Many times the QA or staging databases are several years out of date from production, unless you specifically \\u201Cbackport\\u201D data from production.\"}),`\n`,(0,t.jsx)(e.p,{children:\"Lastly, production databases and datasets are often quite large (and grow larger every day) compared to fixed QA and staging databases. Thus, testing on limited data or fake seed data when developing new features or changing the code base can introduce unexpected regressions in performance or bugs when large results are pulled out of a table.\"}),`\n`,(0,t.jsxs)(e.h3,{id:\"should-i-really-use-production-like-data-in-ephemeral-environments\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#should-i-really-use-production-like-data-in-ephemeral-environments\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Should I Really Use Production-like Data in Ephemeral Environments?\"]}),`\n`,(0,t.jsx)(e.p,{children:\"Short answer, yes. However, you need to be mindful of the possible moral, ethical, and legal implications in using actual production data subject to HIPPA or other regulatory controls. This is why it is crucial to generate a so-called Golden Copy of the data that is scrubbed of any private or confidential data, while still maintaining the same qualities as production in terms of size, statistical parameters, and so forth. This Golden Copy is the source of truth that is\\xA0 updated frequently from your actual production data. We recommend daily updates, but depending on your particular use case it can be more or less often.\"}),`\n`,(0,t.jsx)(e.p,{children:\"With the\\xA0 Golden Copy that is sufficiently production-like (each case varies in how much fidelity to production is required), it is possible to accurately portray behavior features as they would occur in production. Transactions, events, customer (faked) accounts, and so forth are all available and up-to-date with real (-ish) data. For example, if there is a bug that manifests in production, an engineer could easily reproduce the bug with a high degree of confidence, and either validate or develop fixes for the problem.\"}),`\n`,(0,t.jsx)(e.p,{children:\"Testing features with fake or fixed data is suitable for automated testing but for many use cases, especially when testing with non-technical users, real production-like data is valuable to ensure the feature not only works properly but also looks correct when it reaches production.\"}),`\n`,(0,t.jsxs)(e.h3,{id:\"isnt-that-terribly-difficult-expensive-and-prohibitive-or-my-database-is-too-unique\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#isnt-that-terribly-difficult-expensive-and-prohibitive-or-my-database-is-too-unique\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Isn\\u2019t that Terribly Difficult, Expensive, and Prohibitive? Or, \\u201CMy Database is too Unique.\\u201D\"]}),`\n`,(0,t.jsx)(e.p,{children:\"The most common objections to using a production-like dataset are around the difficulties in creating, managing and accessing the dataset, and around the overall cost. I can try to address these in two points: the first one (creation/maintenance/access) can be\\xA0 pretty difficult depending on your use case, but it can be solved; the second one (cost) can readily be handled in this modern era of cloud computing and on-demand managed services offered by cloud providers.\"}),`\n`,(0,t.jsx)(e.p,{children:\"The first problem is that access to production data is usually kept in the hands of a small group of people, and those people are usually not the software engineers who are developing code and testing new features. Separation of concerns and responsibilities is vital for keeping production data clean and secure, but this causes problems when engineers need to test new features, verify bugs in production, or experiment with changes that need to be made.\"}),`\n`,(0,t.jsx)(e.p,{children:\"The best practice is to generate a Golden Copy of the data that you need as mentioned above. The Golden Copy should be a cleaned, stable, up-to-date version of the data that exist in production, but without any data that could compromise confidentiality or proprietary information if it were to accidentally be exposed, (or even internally exposed).\"}),`\n`,(0,t.jsx)(e.p,{children:\"What I tell most people who are involved with production datasets is to create a culture of welcoming access to the Golden Copy and distributing the data internally on a self-service model so that anyone who would like the latest snapshot can access it relatively easily and without a lot of hoops to jump through. Making the data available will ensure that your cleaning and scrubbing process is actually working properly and that your test data is going to have a high degree of similarity to the data in production. This will only make your production data and operations cleaner and more efficient, I promise you.\"}),`\n`,(0,t.jsx)(e.p,{children:\"The second problem is that allowing software engineers, QA people, testers, and even product people access to these datasets comes at a cost. Every database will typically cost a certain amount of money to run and store the data, but there are definitely some optimisations you can implement to keep costs down while still enjoying access to high quality datasets.\"}),`\n`,(0,t.jsx)(e.p,{children:\"The best way to keep costs down is to make sure that the requested access to the production-like dataset is limited in time. For example, a software engineer might only need to run the tests for a day or a week. At the end of that time, the data should automatically be expired and the instance removed because it is no longer needed. If the data is still needed after the initial period of time has expired, you can implement a way to extend the deadline as appropriate.\"}),`\n`,(0,t.jsx)(e.p,{children:\"Another way to reduce costs is to use Copy on Write (COW) technologies if they are available for your database engine and cloud provider. The way this works is that the Golden Copy holds most of the data in storage for use, while the clones that are handed out to engineers are sharing most of the data with the original. It is only when a change or update is made to a table or row that the data is\\xA0 \\u201Ccopied\\u201D over for the clone to use. This is what a Copy on Write will do: it means that the only additional costs for storage on the clone are the incremental changes or writes that are executed during testing.\"}),`\n`,(0,t.jsx)(e.p,{children:\"Another good way to reduce costs is to pause or stop a database when it is not in use. Depending on your cloud provider, you may be able to execute a pause or stop on the database instance so that you can save money during the evenings or weekends when the database is not likely to be in use. This can save 30-60% off your costs versus running the database 24/7.\"}),`\n`,(0,t.jsxs)(e.p,{children:[\"The good news is that \",(0,t.jsx)(e.a,{href:\"https://docs.release.com/\",children:\"Release\"}),\" offers all of the features, including cloning snapshots from the Golden Copy, pausing and restarting databases on a schedule and expiring them as well, and using COW to save time, money, and storage. We support all of the above (using \",(0,t.jsx)(e.a,{href:\"https://docs.release.com/frequently-asked-questions/aws-support-faqs\",children:\"AWS\"}),\", GCP and soon Azure) and we can easily build a dataset pipeline that is checked out to each \",(0,t.jsx)(e.a,{href:\"https://www.merriam-webster.com/dictionary/ephemeral\",children:\"ephemeral\"}),\" environment where your code is deployed.\"]}),`\n`,(0,t.jsx)(e.p,{children:\"You can refresh the dataset to get new changes from the Golden Copy (which is a cleaned version of production, or directly from production as you wish), and you can also update the data in your ephemeral environment to start over from scratch with a new database. You can also set an expiration for each ephemeral environment that will last as long as the branch or feature pull request is open, and engineers can extend the environment duration as needed. Lastly, you can set a schedule for pausing the databases in the dataset so that you save additional costs when the environments are unlikely to be used.\"}),`\n`,(0,t.jsxs)(e.p,{children:[\"Ready to learn more? Watch the on-demand webinar \",(0,t.jsx)(e.a,{href:\"https://www.youtube.com/watch?v=TAhz_UxPWG4\",children:\"\\u201CFull Fidelity Data for Ephemeral Environments.\\u201D\"})]}),`\n`,(0,t.jsx)(e.p,{children:\"\\u200D\"})]})}function k(a={}){let{wrapper:e}=a.components||{};return e?(0,t.jsx)(e,Object.assign({},a,{children:(0,t.jsx)(c,a)})):c(a)}var x=k;return b(T);})();\n;return Component;"
        },
        "_id": "blog/posts/full-fidelity-data-for-ephemeral-environments.mdx",
        "_raw": {
          "sourceFilePath": "blog/posts/full-fidelity-data-for-ephemeral-environments.mdx",
          "sourceFileName": "full-fidelity-data-for-ephemeral-environments.mdx",
          "sourceFileDir": "blog/posts",
          "contentType": "mdx",
          "flattenedPath": "blog/posts/full-fidelity-data-for-ephemeral-environments"
        },
        "type": "BlogPost",
        "computedSlug": "full-fidelity-data-for-ephemeral-environments"
      },
      "documentHash": "1739393595019",
      "hasWarnings": false,
      "documentTypeName": "BlogPost"
    },
    "blog/posts/gitlab-self-managed-now-available-on-release.mdx": {
      "document": {
        "title": "GitLab self-managed now available on Release  ",
        "summary": "Release now supports self-managed versions of GitLab Enterprise and Community Editions.",
        "publishDate": "Wed Mar 22 2023 21:07:10 GMT+0000 (Coordinated Universal Time)",
        "author": "pier-olivier-thibault",
        "readingTime": 3,
        "categories": [
          "platform-engineering",
          "product"
        ],
        "mainImage": "/blog-images/5c5b5abe8612b1c738e6f0f9b3de9968.jpg",
        "imageAlt": "Release + GitLab",
        "showCTA": true,
        "ctaCopy": "Accelerate deployment cycles by testing branches with ephemeral environments using Release's platform. Connect GitLab self-managed to streamline workflows.",
        "ctaLink": "https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=gitlab-self-managed-now-available-on-release",
        "relatedPosts": [
          ""
        ],
        "ogImage": "/blog-images/5c5b5abe8612b1c738e6f0f9b3de9968.jpg",
        "excerpt": "Release now supports self-managed versions of GitLab Enterprise and Community Editions.",
        "tags": [
          "platform-engineering",
          "product"
        ],
        "ctaButton": "Try Release for Free",
        "body": {
          "raw": "\nI’m happy to announce that we now support customers using self-managed versions of [GitLab](https://about.gitlab.com/) Enterprise and Community Editions. Like with the [SaaS](https://docs.gitlab.com/ee/subscriptions/index.html#choose-between-gitlab-saas-or-gitlab-self-managed) offering, you can now integrate Release and deploy environments against repositories that live in your self hosted GitLab service.\n\nRelease is a platform that enables you to deploy all your environments and applications. As part of your development cycle, you can use Release to test branches before they hit production using our ephemeral environments. With [instant Datasets](https://docs.gitlab.com/ee/subscriptions/index.html#choose-between-gitlab-saas-or-gitlab-self-managed), it helps your team ship with confidence by testing changes with production-like data.\n\nWe will go over the GitLab setup steps today. We will:\n\n1.  create an application on GitLab,\n2.  configure your Release account to talk to GitLab, \n3.  connect your Release account to GitLab\n4.  use ephemeral environment to test your code with Release\n\n If you have any questions about Release or any new features we are working on, feel free to [drop us a line](mailto:hello@release.com).  \n\n### Create an application on GitLab\n\nFor Release to authenticate with your hosted solution, you need to configure an application on GitLab’s side to whitelist Release.\n\nIn the user’s settings, there’s a section called “Application”. If you access that section, you will be presented with a screen to create a new application. This application needs to have the proper settings defined so Release can access your GitLab repository. In essence, you’ll need to make sure two things are set up properly: **the callback URL** and **the scopes**.\n\nThe scopes is a set of permissions that will be granted to Release on your behalf. For the integration to work properly, you need to set the scope to “API”.\n\nThe callback URL is the URL that GitLab is authorized to send requests back to. If you’ve ever worked with OAuth, this will be familiar to you.\n\nHere’s how the application should look like in GitLab:\n\n![](/blog-images/3d2bc79ec7ffef9a6c65a461c15d54f3.png)\n\nOnce your application is saved, you will see additional information that will be needed on the Release side. Keep that tab open until you have successfully connected Release to GitLab.\n\nHere’s how it should look like in your GitLab account:\n\n![](/blog-images/cc113defbd618a1d8eae46fc2cba26d6.png)\n\n### Configure your Release account to talk to GitLab\n\nYou can now configure Release to talk to GitLab. Go to your [user profile](https://app.release.com/admin/user-profile) to see all the current integrations we support. We want to use GitLab self-hosted. \n\n![](/blog-images/18f4a03d7815db85a9890c329c173eff.png)\n\nClick on Configure to open up a dialog that allows you to enter the credentials you created in GitLab earlier.\n\n![](/blog-images/ffd747d7ec0423f27ed16e7236065f64.png)\n\nFill in your GitLab URL, ClientID and Client secret and Save. Note that pressing Save will only store the information and won’t validate what you have entered. Validation will occur once you attempt to connect to GitLab. The “connect” button will appear once you save your information. \n\n![](/blog-images/a2920b5c88fa90c39ed65434d678107a.png)\n\n### Connect your Release account to GitLab\n\nClicking on that Connect button will open a new tab and direct you to GitLab. If your information checks out, you will be redirected to your GitLab self-hosted service.\n\nThe first time Release will attempt to connect to GitLab, you will see a dialog that will ask if you want to authorize Release to connect to your application. Once you authorize the connection, GitLab will redirect you back to Release. At this point, GitLab and Release are connected.\n\n### Use ephemeral environment to test your code with Release\n\nNow that you are connected and ready to go, you can use Release to deploy ephemeral environments connected to any working branches in GitLab.\n\nFor every app you configure, the Pull Requests your team does will automatically deploy to Release as an Ephemeral environment. These unique Ephemeral environments are then available for QA and testing by everyone on your team (see [docs](https://docs.release.com/getting-started/prepare-to-use-release) for more details). \n\nThis is very helpful to teams that want to validate their changes before hitting production as you can deploy an application, test features and make sure there is no regression. It’s the kind of safety net you didn’t think you needed until you started using it.\n",
          "code": "var Component=(()=>{var d=Object.create;var i=Object.defineProperty;var u=Object.getOwnPropertyDescriptor;var p=Object.getOwnPropertyNames;var m=Object.getPrototypeOf,g=Object.prototype.hasOwnProperty;var b=(n,e)=>()=>(e||n((e={exports:{}}).exports,e),e.exports),y=(n,e)=>{for(var a in e)i(n,a,{get:e[a],enumerable:!0})},r=(n,e,a,s)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let o of p(e))!g.call(n,o)&&o!==a&&i(n,o,{get:()=>e[o],enumerable:!(s=u(e,o))||s.enumerable});return n};var f=(n,e,a)=>(a=n!=null?d(m(n)):{},r(e||!n||!n.__esModule?i(a,\"default\",{value:n,enumerable:!0}):a,n)),w=n=>r(i({},\"__esModule\",{value:!0}),n);var c=b((C,l)=>{l.exports=_jsx_runtime});var k={};y(k,{default:()=>R,frontmatter:()=>v});var t=f(c()),v={title:\"GitLab self-managed now available on Release  \",summary:\"Release now supports self-managed versions of GitLab Enterprise and Community Editions.\",publishDate:\"Wed Mar 22 2023 21:07:10 GMT+0000 (Coordinated Universal Time)\",author:\"pier-olivier-thibault\",readingTime:3,categories:[\"platform-engineering\",\"product\"],mainImage:\"/blog-images/5c5b5abe8612b1c738e6f0f9b3de9968.jpg\",imageAlt:\"Release + GitLab\",showCTA:!0,ctaCopy:\"Accelerate deployment cycles by testing branches with ephemeral environments using Release's platform. Connect GitLab self-managed to streamline workflows.\",ctaLink:\"https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=gitlab-self-managed-now-available-on-release\",relatedPosts:[\"\"],ogImage:\"/blog-images/5c5b5abe8612b1c738e6f0f9b3de9968.jpg\",excerpt:\"Release now supports self-managed versions of GitLab Enterprise and Community Editions.\",tags:[\"platform-engineering\",\"product\"],ctaButton:\"Try Release for Free\"};function h(n){let e=Object.assign({p:\"p\",a:\"a\",ol:\"ol\",li:\"li\",h3:\"h3\",span:\"span\",strong:\"strong\",img:\"img\"},n.components);return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsxs)(e.p,{children:[\"I\\u2019m happy to announce that we now support customers using self-managed versions of \",(0,t.jsx)(e.a,{href:\"https://about.gitlab.com/\",children:\"GitLab\"}),\" Enterprise and Community Editions. Like with the \",(0,t.jsx)(e.a,{href:\"https://docs.gitlab.com/ee/subscriptions/index.html#choose-between-gitlab-saas-or-gitlab-self-managed\",children:\"SaaS\"}),\" offering, you can now integrate Release and deploy environments against repositories that live in your self hosted GitLab service.\"]}),`\n`,(0,t.jsxs)(e.p,{children:[\"Release is a platform that enables you to deploy all your environments and applications. As part of your development cycle, you can use Release to test branches before they hit production using our ephemeral environments. With \",(0,t.jsx)(e.a,{href:\"https://docs.gitlab.com/ee/subscriptions/index.html#choose-between-gitlab-saas-or-gitlab-self-managed\",children:\"instant Datasets\"}),\", it helps your team ship with confidence by testing changes with production-like data.\"]}),`\n`,(0,t.jsx)(e.p,{children:\"We will go over the GitLab setup steps today. We will:\"}),`\n`,(0,t.jsxs)(e.ol,{children:[`\n`,(0,t.jsx)(e.li,{children:\"create an application on GitLab,\"}),`\n`,(0,t.jsx)(e.li,{children:\"configure your Release account to talk to GitLab,\\xA0\"}),`\n`,(0,t.jsx)(e.li,{children:\"connect your Release account to GitLab\"}),`\n`,(0,t.jsx)(e.li,{children:\"use ephemeral environment to test your code with Release\"}),`\n`]}),`\n`,(0,t.jsxs)(e.p,{children:[\"\\xA0If you have any questions about Release or any new features we are working on, feel free to \",(0,t.jsx)(e.a,{href:\"mailto:hello@release.com\",children:\"drop us a line\"}),\".\\xA0\\xA0\"]}),`\n`,(0,t.jsxs)(e.h3,{id:\"create-an-application-on-gitlab\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#create-an-application-on-gitlab\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Create an application on GitLab\"]}),`\n`,(0,t.jsx)(e.p,{children:\"For Release to authenticate with your hosted solution, you need to configure an application on GitLab\\u2019s side to whitelist Release.\"}),`\n`,(0,t.jsxs)(e.p,{children:[\"In the user\\u2019s settings, there\\u2019s a section called \\u201CApplication\\u201D. If you access that section, you will be presented with a screen to create a new application. This application needs to have the proper settings defined so Release can access your GitLab repository. In essence, you\\u2019ll need to make sure two things are set up properly: \",(0,t.jsx)(e.strong,{children:\"the callback URL\"}),\" and \",(0,t.jsx)(e.strong,{children:\"the scopes\"}),\".\"]}),`\n`,(0,t.jsx)(e.p,{children:\"The scopes is a set of permissions that will be granted to Release on your behalf. For the integration to work properly, you need to set the scope to \\u201CAPI\\u201D.\"}),`\n`,(0,t.jsx)(e.p,{children:\"The callback URL is the URL that GitLab is authorized to send requests back to. If you\\u2019ve ever worked with OAuth, this will be familiar to you.\"}),`\n`,(0,t.jsx)(e.p,{children:\"Here\\u2019s how the application should look like in GitLab:\"}),`\n`,(0,t.jsx)(e.p,{children:(0,t.jsx)(e.img,{src:\"/blog-images/3d2bc79ec7ffef9a6c65a461c15d54f3.png\",alt:\"\"})}),`\n`,(0,t.jsx)(e.p,{children:\"Once your application is saved, you will see additional information that will be needed on the Release side. Keep that tab open until you have successfully connected Release to GitLab.\"}),`\n`,(0,t.jsx)(e.p,{children:\"Here\\u2019s how it should look like in your GitLab account:\"}),`\n`,(0,t.jsx)(e.p,{children:(0,t.jsx)(e.img,{src:\"/blog-images/cc113defbd618a1d8eae46fc2cba26d6.png\",alt:\"\"})}),`\n`,(0,t.jsxs)(e.h3,{id:\"configure-your-release-account-to-talk-to-gitlab\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#configure-your-release-account-to-talk-to-gitlab\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Configure your Release account to talk to GitLab\"]}),`\n`,(0,t.jsxs)(e.p,{children:[\"You can now configure Release to talk to GitLab. Go to your \",(0,t.jsx)(e.a,{href:\"https://app.release.com/admin/user-profile\",children:\"user profile\"}),\" to see all the current integrations we support. We want to use GitLab self-hosted.\\xA0\"]}),`\n`,(0,t.jsx)(e.p,{children:(0,t.jsx)(e.img,{src:\"/blog-images/18f4a03d7815db85a9890c329c173eff.png\",alt:\"\"})}),`\n`,(0,t.jsx)(e.p,{children:\"Click on Configure to open up a dialog that allows you to enter the credentials you created in GitLab earlier.\"}),`\n`,(0,t.jsx)(e.p,{children:(0,t.jsx)(e.img,{src:\"/blog-images/ffd747d7ec0423f27ed16e7236065f64.png\",alt:\"\"})}),`\n`,(0,t.jsx)(e.p,{children:\"Fill in your GitLab URL, ClientID and Client secret and Save. Note that pressing Save will only store the information and won\\u2019t validate what you have entered. Validation will occur once you attempt to connect to GitLab. The \\u201Cconnect\\u201D button will appear once you save your information.\\xA0\"}),`\n`,(0,t.jsx)(e.p,{children:(0,t.jsx)(e.img,{src:\"/blog-images/a2920b5c88fa90c39ed65434d678107a.png\",alt:\"\"})}),`\n`,(0,t.jsxs)(e.h3,{id:\"connect-your-release-account-to-gitlab\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#connect-your-release-account-to-gitlab\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Connect your Release account to GitLab\"]}),`\n`,(0,t.jsx)(e.p,{children:\"Clicking on that Connect button will open a new tab and direct you to GitLab. If your information checks out, you will be redirected to your GitLab self-hosted service.\"}),`\n`,(0,t.jsx)(e.p,{children:\"The first time Release will attempt to connect to GitLab, you will see a dialog that will ask if you want to authorize Release to connect to your application. Once you authorize the connection, GitLab will redirect you back to Release. At this point, GitLab and Release are connected.\"}),`\n`,(0,t.jsxs)(e.h3,{id:\"use-ephemeral-environment-to-test-your-code-with-release\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#use-ephemeral-environment-to-test-your-code-with-release\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Use ephemeral environment to test your code with Release\"]}),`\n`,(0,t.jsx)(e.p,{children:\"Now that you are connected and ready to go, you can use Release to deploy ephemeral environments connected to any working branches in GitLab.\"}),`\n`,(0,t.jsxs)(e.p,{children:[\"For every app you configure, the Pull Requests your team does will automatically deploy to Release as an Ephemeral environment. These unique Ephemeral environments are then available for QA and testing by everyone on your team (see \",(0,t.jsx)(e.a,{href:\"https://docs.release.com/getting-started/prepare-to-use-release\",children:\"docs\"}),\" for more details).\\xA0\"]}),`\n`,(0,t.jsx)(e.p,{children:\"This is very helpful to teams that want to validate their changes before hitting production as you can deploy an application, test features and make sure there is no regression. It\\u2019s the kind of safety net you didn\\u2019t think you needed until you started using it.\"})]})}function L(n={}){let{wrapper:e}=n.components||{};return e?(0,t.jsx)(e,Object.assign({},n,{children:(0,t.jsx)(h,n)})):h(n)}var R=L;return w(k);})();\n;return Component;"
        },
        "_id": "blog/posts/gitlab-self-managed-now-available-on-release.mdx",
        "_raw": {
          "sourceFilePath": "blog/posts/gitlab-self-managed-now-available-on-release.mdx",
          "sourceFileName": "gitlab-self-managed-now-available-on-release.mdx",
          "sourceFileDir": "blog/posts",
          "contentType": "mdx",
          "flattenedPath": "blog/posts/gitlab-self-managed-now-available-on-release"
        },
        "type": "BlogPost",
        "computedSlug": "gitlab-self-managed-now-available-on-release"
      },
      "documentHash": "1739393595019",
      "hasWarnings": false,
      "documentTypeName": "BlogPost"
    },
    "blog/posts/gitops-vs-devops-understanding-the-difference.mdx": {
      "document": {
        "title": "GitOps vs DevOps: Understanding the Difference",
        "summary": "Let's look at a comparison between the classical GitOps vs DevOps. What are the key similarities and differences?",
        "publishDate": "Wed Sep 07 2022 16:05:43 GMT+0000 (Coordinated Universal Time)",
        "author": "vicky-koblinski",
        "readingTime": 3,
        "categories": [
          "platform-engineering",
          "kubernetes"
        ],
        "mainImage": "/blog-images/4752ccf6e76e8eb0b4198dbea7f80c51.jpg",
        "imageAlt": "a person working on a laptop",
        "showCTA": true,
        "ctaCopy": "Improve CD pipelines with Release's ephemeral environments for secure, automated deployments. Enhance version tracking and streamline workflows.",
        "ctaLink": "https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=gitops-vs-devops-understanding-the-difference",
        "relatedPosts": [
          ""
        ],
        "ogImage": "/blog-images/4752ccf6e76e8eb0b4198dbea7f80c51.jpg",
        "excerpt": "Let's look at a comparison between the classical GitOps vs DevOps. What are the key similarities and differences?",
        "tags": [
          "platform-engineering",
          "kubernetes"
        ],
        "ctaButton": "Try Release for Free",
        "body": {
          "raw": "\nWhen it comes to cooperative coding, DevOps is fundamentally a culture of continuous integration and deployment that aims to keep things running smoothly. GitOps is a newer, similar practice that builds on many of the same principles, only with a specifically version-tracking-based approach. It can be quite difficult to understand the differences, as both practices overlap greatly. In this article, we'll take a look at that overlap and outline some of the key differences. \n\n### What Is DevOps?\n\nDevOps is, put simply, a strategy for increasing the efficiency of coding projects by connecting development and operations, enabling them to work together in a steady feedback loop of [CI/CD](https://en.wikipedia.org/wiki/CI/CD). Instead of keeping development and operations separate, and needing to coordinate between them as two separate branches of a project, new developments are continually integrated. The product is then tested, deployed, and monitored by operations and fed back for further development. \n\n#### GitOps: Git for Automation\n\nIn many ways, GitOps can seem like a branch of classic DevOps. GitOps represents an approach to DevOps functionality and goals that specifically relies on Git, [the most commonly used version tracking software today](https://www.cyberfella.co.uk/2020/03/23/what-is-devops/). In fact, because Git is so commonly used, many DevOps pipelines also use Git! However, GitOps refers to a practice that builds on the specifics of Git. It prioritizes automation specifically tied to Git's version control systems, and it leans into those functionalities. A few examples of this are: \n\n- Rollouts and rollbacks: With version control tracking, it's easy to deploy a new feature, environment, or other aspect, then quickly revert back to an earlier, more stable version if it doesn't behave as expected.\n- Infrastructure as code: With Git, environments and infrastructure can be version tracked as well.\n- Push versus pull: Git allows operations to pull and review changes, and then the requested changes are automatically deployed.\n\n#### GitOps vs DevOps\n\nAt their most basic level, DevOps and GitOps have the same root: a continuous pipeline that connects all the steps of a stable project into one flow. However, GitOps is more specific and tied to a singular tool. This leads to a few subtle, but important, differences. \n\n### Better CD in CI/CD Pipelines\n\nA classic DevOps pipeline often starts with a repository for the developers' contributions. This is often in GitHub already, but it doesn't necessarily have to be for DevOps. Developers would then commit changes, which would be handled by a CI/CD tool (such as Azure). The CI/CD tool runs automated tests and integrates changes if they're acceptable. This then moves immediately into deployment via an automated deployment tool. \n\n![](/blog-images/6462723ca5342d6403bb4c563715a4f4.png)\n\nGitOps expands on the continuous deployment (CD) stage of the pipeline, creating a more secure way to deploy with fewer failure points. The continuous integration (CI) step remains largely the same, but GitOps gives CD its own focus. GitOps tools continuously compare the current state of the operating environment with the desired state captured in the repository. If changes are made, they're pulled to the deployment server. Deployment automation tools can now pull those changes in turn—and revert to the latest stable version if there's an issue. \n\nIt's important to note that this kind of CD implementation isn't exclusive to GitOps. Though GitOps is conducive to good deployment strategies, in part due to automated version control tracking making them easier, any DevOps pipeline can employ them. They also aren't Git exclusive! However, they're often tied to GitOps as the shift to GitOps has placed more attention and focus on the CD step using Git functionalities to refine and improve the process of deployment. \n\n### Infrastructure as Code\n\nOne thing that comes up frequently when talking about GitOps is infrastructure as code (IaC). [Infrastructure as code](https://microtica.com/blog/infrastructure-as-code-from-the-beginning-to-gitops/) is a method of managing infrastructure to ensure that projects run in a consistent environment. This is done by including infrastructure inside of your code, and in GitOps, this means Git will apply the same version control to infrastructure as it does to any other part of the source code. In fact, Git is such a useful tool for this that many DevOps pipelines today also use Git for it. \n\nThough many people regard GitOps as a direct response to the rise of the IaC concept, the relationship between GitOps and IaC has some subtle nuances. You can read about [some of these considerations in more detail here](https://microtica.com/blog/infrastructure-as-code-from-the-beginning-to-gitops/). Ultimately, while DevOps often deals with IaC as well, Git simplifies and enables this practice to the point that any DevOps techniques along these lines will either overlap significantly with GitOps or simply use Git. GitOps also places specific priority on IaC, while DevOps encompasses a greater, more generalized culture. \n\nA good example of this is Release's own [environment configuration tracking](https://docs.releasehub.com/reference-documentation/gitops). In this case, we track the environment as part of the infrastructure. Release's UI builds on Git's existing version tracking, expanding on the common aspects of GitOps, such as automation, templating, and version tracking. \n\n![](/blog-images/66d46a33d50e527bacbcfe342206ec03.jpeg)\n\n### Challenges and Best Practices of GitOps\n\nUltimately, GitOps is an approach within the DevOps mindset that helps get the most out of Git and its toolkit. A majority of its advantages over traditional DevOps come from the strategic use of its foundations to further streamline a CI/CD pipeline. As such, to get the most out of GitOps, you need to know the foremost challenges that can arise and how to best navigate them. Here are a few of the major points to keep in mind: \n\n- Security: One issue that often arises with Git is security. Sensitive data (such as security keys, passwords, or tokens) cannot be stored in areas that are committed to shared repositories. [Because these cannot be stored in plain text](https://release.com/blog/how-to-manage-gitops-secrets-a-detailed-guide), you need additional tools to either encrypt them or store them elsewhere.\n- Multi-environment configurations: Git only allows one environment per branch, and it requires that you define the environment within the repository. Any changes to environments, therefore, must be committed and pushed. Even if one environment per branch is enough, it can easily snowball. Release and environments as service (EaaS) provide solutions to manage this.\n- Scalability: This is one of the challenges that is hardest to manage. Git stores the history of all commits and changes. Over a number of versions and repositories, this can become quite a lot of data to store. While there isn't an easy fix for this, it's something that can be kept in mind. Limit the number of branches, cut down on duplicate code if possible, and keep commit permissions and protocol organized. With adequate planning and care, it's possible to keep GitOps relatively scalable.\n\n### Is GitOps an Offshoot of DevOps?\n\nIn many ways, a CI/CD pipeline developed with GitOps does much of the same things that one developed with classic DevOps does, and a good classic DevOps setup has elements in place to cover much of what GitOps does. Therefore, GitOps can be regarded both as a tool in the DevOps toolkit or as its own practice, often depending on which elements of it you prioritize. \n\nHowever, in either case, GitOps is a continuation of the culture that DevOps encompasses. It fills in blanks in areas that aren't prioritized in classical DevOps with strategies that remain in line with the core philosophy of DevOps. By knowing the most important advantages and challenges of GitOps, you can capitalize on the most important aspects of DevOps while taking it one step further. \n\n### Conclusion on GitOps vs DevOps\n\nGood CI/CD and DevOps practices can have a huge impact on how efficiently a company operates. In this article, we went over what DevOps is, how GitOps fits into DevOps, and their differences. We also mentioned some best practices for incorporating GitOps effectively into your team's DevOps mentality. You can now take this information and make the best decisions possible for your development processes.\n",
          "code": "var Component=(()=>{var p=Object.create;var o=Object.defineProperty;var d=Object.getOwnPropertyDescriptor;var m=Object.getOwnPropertyNames;var u=Object.getPrototypeOf,f=Object.prototype.hasOwnProperty;var g=(i,e)=>()=>(e||i((e={exports:{}}).exports,e),e.exports),v=(i,e)=>{for(var n in e)o(i,n,{get:e[n],enumerable:!0})},r=(i,e,n,a)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let s of m(e))!f.call(i,s)&&s!==n&&o(i,s,{get:()=>e[s],enumerable:!(a=d(e,s))||a.enumerable});return i};var y=(i,e,n)=>(n=i!=null?p(u(i)):{},r(e||!i||!i.__esModule?o(n,\"default\",{value:i,enumerable:!0}):n,i)),b=i=>r(o({},\"__esModule\",{value:!0}),i);var l=g((C,c)=>{c.exports=_jsx_runtime});var G={};v(G,{default:()=>k,frontmatter:()=>w});var t=y(l()),w={title:\"GitOps vs DevOps: Understanding the Difference\",summary:\"Let's look at a comparison between the classical GitOps vs DevOps. What are the key similarities and differences?\",publishDate:\"Wed Sep 07 2022 16:05:43 GMT+0000 (Coordinated Universal Time)\",author:\"vicky-koblinski\",readingTime:3,categories:[\"platform-engineering\",\"kubernetes\"],mainImage:\"/blog-images/4752ccf6e76e8eb0b4198dbea7f80c51.jpg\",imageAlt:\"a person working on a laptop\",showCTA:!0,ctaCopy:\"Improve CD pipelines with Release's ephemeral environments for secure, automated deployments. Enhance version tracking and streamline workflows.\",ctaLink:\"https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=gitops-vs-devops-understanding-the-difference\",relatedPosts:[\"\"],ogImage:\"/blog-images/4752ccf6e76e8eb0b4198dbea7f80c51.jpg\",excerpt:\"Let's look at a comparison between the classical GitOps vs DevOps. What are the key similarities and differences?\",tags:[\"platform-engineering\",\"kubernetes\"],ctaButton:\"Try Release for Free\"};function h(i){let e=Object.assign({p:\"p\",h3:\"h3\",a:\"a\",span:\"span\",h4:\"h4\",ul:\"ul\",li:\"li\",img:\"img\"},i.components);return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(e.p,{children:\"When it comes to cooperative coding, DevOps is fundamentally a culture of continuous integration and deployment that aims to keep things running smoothly. GitOps is a newer, similar practice that builds on many of the same principles, only with a specifically version-tracking-based approach. It can be quite difficult to understand the differences, as both practices overlap greatly. In this article, we'll take a look at that overlap and outline some of the key differences.\\xA0\"}),`\n`,(0,t.jsxs)(e.h3,{id:\"what-is-devops\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#what-is-devops\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"What Is DevOps?\"]}),`\n`,(0,t.jsxs)(e.p,{children:[\"DevOps is, put simply, a strategy for increasing the efficiency of coding projects by connecting development and operations, enabling them to work together in a steady feedback loop of \",(0,t.jsx)(e.a,{href:\"https://en.wikipedia.org/wiki/CI/CD\",children:\"CI/CD\"}),\". Instead of keeping development and operations separate, and needing to coordinate between them as two separate branches of a project, new developments are continually integrated. The product is then tested, deployed, and monitored by operations and fed back for further development.\\xA0\"]}),`\n`,(0,t.jsxs)(e.h4,{id:\"gitops-git-for-automation\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#gitops-git-for-automation\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"GitOps: Git for Automation\"]}),`\n`,(0,t.jsxs)(e.p,{children:[\"In many ways, GitOps can seem like a branch of classic DevOps. GitOps represents an approach to DevOps functionality and goals that specifically relies on Git, \",(0,t.jsx)(e.a,{href:\"https://www.cyberfella.co.uk/2020/03/23/what-is-devops/\",children:\"the most commonly used version tracking software today\"}),\". In fact, because Git is so commonly used, many DevOps pipelines also use Git! However, GitOps refers to a practice that builds on the specifics of Git. It prioritizes automation specifically tied to Git's version control systems, and it leans into those functionalities. A few examples of this are:\\xA0\"]}),`\n`,(0,t.jsxs)(e.ul,{children:[`\n`,(0,t.jsx)(e.li,{children:\"Rollouts and rollbacks: With version control tracking, it's easy to deploy a new feature, environment, or other aspect, then quickly revert back to an earlier, more stable version if it doesn't behave as expected.\"}),`\n`,(0,t.jsx)(e.li,{children:\"Infrastructure as code: With Git, environments and infrastructure can be version tracked as well.\"}),`\n`,(0,t.jsx)(e.li,{children:\"Push versus pull: Git allows operations to pull and review changes, and then the requested changes are automatically deployed.\"}),`\n`]}),`\n`,(0,t.jsxs)(e.h4,{id:\"gitops-vs-devops\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#gitops-vs-devops\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"GitOps vs DevOps\"]}),`\n`,(0,t.jsx)(e.p,{children:\"At their most basic level, DevOps and GitOps have the same root: a continuous pipeline that connects all the steps of a stable project into one flow. However, GitOps is more specific and tied to a singular tool. This leads to a few subtle, but important, differences.\\xA0\"}),`\n`,(0,t.jsxs)(e.h3,{id:\"better-cd-in-cicd-pipelines\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#better-cd-in-cicd-pipelines\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Better CD in CI/CD Pipelines\"]}),`\n`,(0,t.jsx)(e.p,{children:\"A classic DevOps pipeline often starts with a repository for the developers' contributions. This is often in GitHub already, but it doesn't necessarily have to be for DevOps. Developers would then commit changes, which would be handled by a CI/CD tool (such as Azure). The CI/CD tool runs automated tests and integrates changes if they're acceptable. This then moves immediately into deployment via an automated deployment tool.\\xA0\"}),`\n`,(0,t.jsx)(e.p,{children:(0,t.jsx)(e.img,{src:\"/blog-images/6462723ca5342d6403bb4c563715a4f4.png\",alt:\"\"})}),`\n`,(0,t.jsx)(e.p,{children:\"GitOps expands on the continuous deployment (CD) stage of the pipeline, creating a more secure way to deploy with fewer failure points. The continuous integration (CI) step remains largely the same, but GitOps gives CD its own focus. GitOps tools continuously compare the current state of the operating environment with the desired state captured in the repository. If changes are made, they're pulled to the deployment server. Deployment automation tools can now pull those changes in turn\\u2014and revert to the latest stable version if there's an issue.\\xA0\"}),`\n`,(0,t.jsx)(e.p,{children:\"It's important to note that this kind of CD implementation isn't exclusive to GitOps. Though GitOps is conducive to good deployment strategies, in part due to automated version control tracking making them easier, any DevOps pipeline can employ them. They also aren't Git exclusive! However, they're often tied to GitOps as the shift to GitOps has placed more attention and focus on the CD step using Git functionalities to refine and improve the process of deployment.\\xA0\"}),`\n`,(0,t.jsxs)(e.h3,{id:\"infrastructure-as-code\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#infrastructure-as-code\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Infrastructure as Code\"]}),`\n`,(0,t.jsxs)(e.p,{children:[\"One thing that comes up frequently when talking about GitOps is infrastructure as code (IaC). \",(0,t.jsx)(e.a,{href:\"https://microtica.com/blog/infrastructure-as-code-from-the-beginning-to-gitops/\",children:\"Infrastructure as code\"}),\" is a method of managing infrastructure to ensure that projects run in a consistent environment. This is done by including infrastructure inside of your code, and in GitOps, this means Git will apply the same version control to infrastructure as it does to any other part of the source code. In fact, Git is such a useful tool for this that many DevOps pipelines today also use Git for it.\\xA0\"]}),`\n`,(0,t.jsxs)(e.p,{children:[\"Though many people regard GitOps as a direct response to the rise of the IaC concept, the relationship between GitOps and IaC has some subtle nuances. You can read about \",(0,t.jsx)(e.a,{href:\"https://microtica.com/blog/infrastructure-as-code-from-the-beginning-to-gitops/\",children:\"some of these considerations in more detail here\"}),\". Ultimately, while DevOps often deals with IaC as well, Git simplifies and enables this practice to the point that any DevOps techniques along these lines will either overlap significantly with GitOps or simply use Git. GitOps also places specific priority on IaC, while DevOps encompasses a greater, more generalized culture.\\xA0\"]}),`\n`,(0,t.jsxs)(e.p,{children:[\"A good example of this is Release's own \",(0,t.jsx)(e.a,{href:\"https://docs.releasehub.com/reference-documentation/gitops\",children:\"environment configuration tracking\"}),\". In this case, we track the environment as part of the infrastructure. Release's UI builds on Git's existing version tracking, expanding on the common aspects of GitOps, such as automation, templating, and version tracking.\\xA0\"]}),`\n`,(0,t.jsx)(e.p,{children:(0,t.jsx)(e.img,{src:\"/blog-images/66d46a33d50e527bacbcfe342206ec03.jpeg\",alt:\"\"})}),`\n`,(0,t.jsxs)(e.h3,{id:\"challenges-and-best-practices-of-gitops\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#challenges-and-best-practices-of-gitops\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Challenges and Best Practices of GitOps\"]}),`\n`,(0,t.jsx)(e.p,{children:\"Ultimately, GitOps is an approach within the DevOps mindset that helps get the most out of Git and its toolkit. A majority of its advantages over traditional DevOps come from the strategic use of its foundations to further streamline a CI/CD pipeline. As such, to get the most out of GitOps, you need to know the foremost challenges that can arise and how to best navigate them. Here are a few of the major points to keep in mind:\\xA0\"}),`\n`,(0,t.jsxs)(e.ul,{children:[`\n`,(0,t.jsxs)(e.li,{children:[\"Security: One issue that often arises with Git is security. Sensitive data (such as security keys, passwords, or tokens) cannot be stored in areas that are committed to shared repositories. \",(0,t.jsx)(e.a,{href:\"https://release.com/blog/how-to-manage-gitops-secrets-a-detailed-guide\",children:\"Because these cannot be stored in plain text\"}),\", you need additional tools to either encrypt them or store them elsewhere.\"]}),`\n`,(0,t.jsx)(e.li,{children:\"Multi-environment configurations: Git only allows one environment per branch, and it requires that you define the environment within the repository. Any changes to environments, therefore, must be committed and pushed. Even if one environment per branch is enough, it can easily snowball. Release and environments as service (EaaS) provide solutions to manage this.\"}),`\n`,(0,t.jsx)(e.li,{children:\"Scalability: This is one of the challenges that is hardest to manage. Git stores the history of all commits and changes. Over a number of versions and repositories, this can become quite a lot of data to store. While there isn't an easy fix for this, it's something that can be kept in mind. Limit the number of branches, cut down on duplicate code if possible, and keep commit permissions and protocol organized. With adequate planning and care, it's possible to keep GitOps relatively scalable.\"}),`\n`]}),`\n`,(0,t.jsxs)(e.h3,{id:\"is-gitops-an-offshoot-of-devops\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#is-gitops-an-offshoot-of-devops\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Is GitOps an Offshoot of DevOps?\"]}),`\n`,(0,t.jsx)(e.p,{children:\"In many ways, a CI/CD pipeline developed with GitOps does much of the same things that one developed with classic DevOps does, and a good classic DevOps setup has elements in place to cover much of what GitOps does. Therefore, GitOps can be regarded both as a tool in the DevOps toolkit or as its own practice, often depending on which elements of it you prioritize.\\xA0\"}),`\n`,(0,t.jsx)(e.p,{children:\"However, in either case, GitOps is a continuation of the culture that DevOps encompasses. It fills in blanks in areas that aren't prioritized in classical DevOps with strategies that remain in line with the core philosophy of DevOps. By knowing the most important advantages and challenges of GitOps, you can capitalize on the most important aspects of DevOps while taking it one step further.\\xA0\"}),`\n`,(0,t.jsxs)(e.h3,{id:\"conclusion-on-gitops-vs-devops\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#conclusion-on-gitops-vs-devops\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Conclusion on GitOps vs DevOps\"]}),`\n`,(0,t.jsx)(e.p,{children:\"Good CI/CD and DevOps practices can have a huge impact on how efficiently a company operates. In this article, we went over what DevOps is, how GitOps fits into DevOps, and their differences. We also mentioned some best practices for incorporating GitOps effectively into your team's DevOps mentality. You can now take this information and make the best decisions possible for your development processes.\"})]})}function O(i={}){let{wrapper:e}=i.components||{};return e?(0,t.jsx)(e,Object.assign({},i,{children:(0,t.jsx)(h,i)})):h(i)}var k=O;return b(G);})();\n;return Component;"
        },
        "_id": "blog/posts/gitops-vs-devops-understanding-the-difference.mdx",
        "_raw": {
          "sourceFilePath": "blog/posts/gitops-vs-devops-understanding-the-difference.mdx",
          "sourceFileName": "gitops-vs-devops-understanding-the-difference.mdx",
          "sourceFileDir": "blog/posts",
          "contentType": "mdx",
          "flattenedPath": "blog/posts/gitops-vs-devops-understanding-the-difference"
        },
        "type": "BlogPost",
        "computedSlug": "gitops-vs-devops-understanding-the-difference"
      },
      "documentHash": "1739393595019",
      "hasWarnings": false,
      "documentTypeName": "BlogPost"
    },
    "blog/posts/great-saas-sales-demos.mdx": {
      "document": {
        "title": "Great SaaS Sales Demos - 3 Game Changers",
        "summary": "Great SaaS sales demos using these three game changers enabled by Release environments.",
        "publishDate": "Thu Mar 18 2021 20:48:40 GMT+0000 (Coordinated Universal Time)",
        "author": "regis-wilson",
        "readingTime": 7,
        "categories": [
          "platform-engineering",
          "product"
        ],
        "mainImage": "/blog-images/d86fb94eb5fdceaf27ec8fe22a1f131f.jpg",
        "imageAlt": "A man playing chess representing game changers in SaaS Sales Demos",
        "showCTA": true,
        "ctaCopy": "Improve SaaS demos with Release's on-demand environments for seamless collaboration, faster bug resolution, and consistent deployments.",
        "ctaLink": "https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=great-saas-sales-demos",
        "relatedPosts": [
          ""
        ],
        "ogImage": "/blog-images/d86fb94eb5fdceaf27ec8fe22a1f131f.jpg",
        "excerpt": "Great SaaS sales demos using these three game changers enabled by Release environments.",
        "tags": [
          "platform-engineering",
          "product"
        ],
        "ctaButton": "Try Release for Free",
        "body": {
          "raw": "\n### Great SaaS Sales Demos\n\nSales demos for Software as a Service (SaaS) products are vital to introducing your product to customers and gaining their interest or desire to use your product. The typical way that a SaaS product is demonstrated, especially in this day and age of remote work, is by presenting the application remotely, showing off features and highlighting use cases for the potential customer. A good demo can show the customer that your product is a good fit for them, but it can also do the opposite: a demonstration might reveal problems, shortcomings, or gaps that your SaaS product has that your customer might pick up on.\n\nAs a DevOps engineer, I’ve received and viewed many demonstrations for a wide variety of products over many years. I’ve never had to present a sales demonstration myself (at least, not directly), but I have had to do setup and perform internal demonstrations, and I’ve also had to participate in designing or configuring SaaS product demonstrations. There are two SaaS demonstrations that I’ve participated in that were so memorable, they always come to mind even years later and after hundreds of demonstrations I’ve witnessed. The first demonstration was an unmitigated disaster and the second was one of the best that I ever witnessed thus far. Both of these examples lead to the 3 ways that you can make your SaaS sales demo the best it can possibly be.\n\n### The Bad Example\n\nOne of the worst examples of a SaaS sales demo disaster happened quite a long time ago when a SaaS company was in the middle of releasing a new version of their software. They were also going public and the combination of these two events was a source of great fanfare in the industry and at the SaaS company. The company I worked for had tested the previous version of the product—let’s say the “1.0” product—and we were prospecting to get a very good deal on a one year implementation. The new “2.0” product was being released and our company was one of the customers that would  be a “logo” used to promote how well the product worked and that was trusted by our brand.\n\nThe problem was that internally, many of us engineers and even some of the leadership, agreed that the product wasn’t very good. We were excited to try out the new version but were a bit skeptical the product would be vastly different than the 1.0 product. In our view, the product was large, bloated, unstable, and crashed often. In preparation for the big 2.0 sales demonstration, I sat across from an executive at the SaaS company and listed a long laundry list of issues that I considered important to be fixed in the new product. The executive (who turned out to be the SaaS provider’s CTO!!!) agreed with me completely and reassured me that “all of the issues” had been resolved in the new version.\n\nThe large conference room lights dimmed and in a standing-room-only hush, the product demonstration started and was displayed on the projection screen. It was obvious the product was slow: noticeably slower than even the original. It also crashed almost immediately. The sales person started the demonstration again with a smooth cover-up. I remember the mood was still forgiving because it had crashed so suddenly and so quickly, nobody had really invested too much time yet. The sales demo started again and the product was slightly better. However, I could tell from the tics and movements on the screen that the sales person was purposely avoiding certain features and buttons that I personally knew were problematic in the old version. I knew he was avoiding them because they were likely to be buggy or cause issues.\n\nDespite such careful choreography, the product crashed yet again, in fact, it crashed numerous times to the point where the demonstration simply couldn’t continue. The executives from the SaaS company who were there to personally oversee the demonstration were flabbergasted. I personally heard one of the sales engineers speaking in hushed tones into his cell phone telling his operations folks to try to reboot the demo server, again.\n\nThese are a sample of the types of excuses we were offered:\n\n- “Oh, the new version hasn’t been deployed yet, this is a pre-release demo.”\n- “You know, the dataset is wrong, we need to load more data.”\n- “The internet connection is really laggy and we have a production network environment for the real product.”\n- “The demo server isn’t ready for production yet, we’ve been upgrading our systems as fast as we can.”\n- “We haven’t fully tested this new feature, and it wasn’t supposed to appear in this demonstration. Sorry.”\n\nBut eventually the truth was that the sales demo itself had failed. This had almost nothing to do with the product in actual practice, but that didn’t matter. The demonstration was unable to satisfactorily show what the product could do.\n\n### The Good Example\n\nOn the opposite side of the spectrum, one of the best sales SaaS demos I witnessed was nearly perfect and stood out how great the process was. The SaaS product was a monitoring tool that provided metrics and log events for a running application, and would be used to drill down and into metrics, events, logs, and so forth. This demonstration occurred many years ago and does not reflect on any present companies or products you may think of. I just want to clarify this so that you do not form an impression of any existing company or product today.\n\nThe SaaS salesperson ran the demonstration by logging into the actual live product with a special demonstration credential, and showed real, live data that were flowing from a fake application that was written specifically for the demonstration. It ran on a ten-minute cycle of generating pre-created events, metrics, logs, and so forth. The salesperson was able to look at “real,” “live,” and updated data as events happened in the application. Because the data were pregenerated and ran continuously, it seemed like the product worked perfectly and would do exactly what the salesperson seemed to make so easy.\n\nThere was one flaw that I spotted and confirmed but it was relatively minor. This flaw does demonstrate one of the key points I will discuss later on for a successful SaaS demonstration. The salesperson was unable to change or update any of the data or the layout of the screen. They were unable to demonstrate the ability to create and edit reports for the product. They were using what was essentially a “read-only” demonstration of the product and could not change anything inside the account. The reason for this is to perfectly preserve the demonstration process so that the next time the product is reviewed, it would be exactly the way it was before without any possible changes.\n\nBased on these examples, I will now detail the 3 most important ways to pull off a perfect SaaS sales demonstration.\n\n### Game Changer 1: Provide a “Real” Experience\n\nYour SaaS demonstration must be as “real” as possible. I use the word “real” in scare quotes because often you cannot provide any actual “real” customer data since the prospects aren’t customers yet. Even if you do have some customer data or can use actual real data of some sort, the customers are not interacting in any real way with the product yet, so let’s just use the scare quotes. Never-the-less, the data that you use in the sales demonstration needs to be realistic enough to provide a valid reflection of what your product can do and what the customer needs, in a way that the product actually works.\n\nThere is no other way to perform this than to use your actual live product in its actual live state. There are a lot of excuses and corners you will want to cut around this issue, but believe me when I tell you that this is critical. You must use your actual product in its actual state to perform the SaaS demonstration. When a customer test drives a car at the dealership, they are driving a real car that can be sold, not a fake or reproduction or toy model. You must do the same with your SaaS product.\n\nYou will need a stable, performant, fully functional, and fully capable product that functions exactly the way it will function when the customer logs in and starts using the product. It must function perfectly the first time, the second time, and the _nth_ time you demonstrate your product. It must not conflict with someone else when two salespeople perform a demonstration at the same time, and it must not have any leftover or incorrect settings from a previous demonstration. It also must be stable and not be updated or rebooted or crash because of normal operations of the live site.\n\nThis is where a [Release](https://releasehub.com) sales demonstration environment comes into play. A salesperson or organization that uses Release would have an environment setup and ready to deploy at a moment’s notice with pre-populated data, user accounts, and features fixed to a particular time or branch set. A few minutes before the demonstration, the salesperson would simply start a new, completely isolated and perfectly preserved application stack and dataset, ready to fully demonstrate the full, live, exact duplicate SaaS product to the customer. There is no chance for shared environments to conflict; the demonstrations can be tailored to each customer industry or feature set (if applicable); and there is no possibility of having incorrect settings or data present that can disrupt the demonstration.\n\nBecause the scale of the application can be set for one person or only one small subset of a customer, the application can be tuned to perform perfectly and for very little cost compared to the production environment. As soon as the demonstration is finished, the environment can be destroyed so that any hosting costs stop immediately.\n\n### Game Changer 2: Provide a Live Experience\n\nNotice I did not use scare quotes in this section for the word “live.” The demonstration experience can be faked to some extent in terms of pre-populated data or accounts, but it cannot be faked in terms of actual functionality. You cannot just show a static video of your product demonstration and expect customers to buy into it. The potential customers must be able to intervene, interject, ask “what if,” or (even better) take over the controls and “see what happens.”\n\nI already can hear some objections from salespeople who say that when an application is truly live then it is subject to unknown influences and problems. This is true, except that if you have a product experience that is well-tested and fixed in place at a particular time and version, then the demonstration can be well-rehearsed and practiced. This is directly at odds with the usual organization's push to release new features and update the product regularly. How is it possible to be able to have completely stable, well-populated, isolated environments at the same time that the development cycle needs to be fast, regularly spaced, and turn quickly?\n\nThis is where [Release](https://releasehub.com) sales demonstration environments come into play. An application template can be set to a stable version or branch that is well-tested and cleaned for use to show to customers. It will be prepopulated with well-known, well-tested data. When a salesperson or sales organization is ready to show off new features or product enhancements, those features can be versioned and configured for use either as a separate application or as an existing branch of the Release application so that it can be spun up on demand for a customer. Once the branch and application template are tested, that version can be used by anyone in the sales organization to demonstrate the new features or product experiences and data.\n\nIn this scenario, it doesn’t matter if the production application is running on version 0.9 or 1.1 or even 2.0, the sales demonstration environment can be configured to run exactly the same version 1.0 every single time it is deployed and demonstrated to potential customers. Also, it could be devastating if the product version were to change in the middle of the demonstration or during the customer demonstration time period. Conversely, if the production environment is running on a delayed version and a customer wants to see the new features, the new sales environments can be configured and tested for potential preview customers to give early feedback, advice, or to gain other valuable information.\n\nIn fact, all of these scenarios could be in play at any one time: some sales environments could be set behind on an extremely stable, well-tested product experience while some sales environments can be spun up on demand at exactly the same version of the production experience, while yet other environments can be ahead of production to showcase new features and product experiences. The sales person or organization could pick and choose which scenario they would like to demonstrate and create the new environment tailored to their exact requirements and customer profile.\n\nEach environment would be isolated and stable without any contention or changes interfering.\n\n### Game Changer 3: Provide a Malleable, Persistent Experience—Which Can be Recreated\n\nEven in the best SaaS sales demonstrations I have experienced, there was always a critical and nagging worry I had that the environment was not going to persist and that anything that I was working on or being demonstrated would not last for long after the demo. Strangely, the reverse is true as well: oftentimes the customer might want to revert all their changes back to a pristine state after “messing around” with things. How can you resolve this tension between two extremes?\n\nThis is where [Release](https://releasehub.com) sales demonstration environments come into play. The salesperson or organization can spin up entire environments for one or more customers and allow the customer(s) to “play” with the product without any fear of damaging or impacting anything else. If the customer wants to start over, a new environment could be created, or the exact same environment could be duplicated to start over. In fact, the salesperson could construct several accounts for the customer to use if that is appropriate, or if multiple employees or departments could be interacting with each other or the demonstration. All of this can happen in such a way that the isolated environment can be snapshot or ported to production once everything is set up the way it should be. Alternatively, the snapshot could be ported to a new sales demonstration environment for a “reset.”\n\nEven better, in some cases, the new sales demonstration environment could become a staging or testing environment for the customer to use indefinitely or on-demand when they need to test possible scenarios or possibilities. All of this can occur without any impact or conflicts with the live production product. In a few rare cases, the SaaS product could become so customized to an industry vertical or customer that it may become a standalone environment live in production for use by that customer or industry.\n\nThis brings up a great point about environments at Release: they are all identical in design and spirit, and can be used for any and for all purposes. A software engineer’s local development idea can become a testing ground for product development, which can easily be deployed for a sales demonstration, which could easily become a live production environment running live paying customers. With Release, your environments are not limited by even the sky above or the ground below!\n\nPhoto by [Carlos Esteves](https://unsplash.com/@dimage_carlos?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) on [Unsplash](/?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)\n",
          "code": "var Component=(()=>{var h=Object.create;var s=Object.defineProperty;var u=Object.getOwnPropertyDescriptor;var p=Object.getOwnPropertyNames;var m=Object.getPrototypeOf,f=Object.prototype.hasOwnProperty;var y=(a,e)=>()=>(e||a((e={exports:{}}).exports,e),e.exports),w=(a,e)=>{for(var o in e)s(a,o,{get:e[o],enumerable:!0})},i=(a,e,o,r)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let n of p(e))!f.call(a,n)&&n!==o&&s(a,n,{get:()=>e[n],enumerable:!(r=u(e,n))||r.enumerable});return a};var g=(a,e,o)=>(o=a!=null?h(m(a)):{},i(e||!a||!a.__esModule?s(o,\"default\",{value:a,enumerable:!0}):o,a)),v=a=>i(s({},\"__esModule\",{value:!0}),a);var l=y((I,d)=>{d.exports=_jsx_runtime});var T={};w(T,{default:()=>x,frontmatter:()=>b});var t=g(l()),b={title:\"Great SaaS Sales Demos - 3 Game Changers\",summary:\"Great SaaS sales demos using these three game changers enabled by Release environments.\",publishDate:\"Thu Mar 18 2021 20:48:40 GMT+0000 (Coordinated Universal Time)\",author:\"regis-wilson\",readingTime:7,categories:[\"platform-engineering\",\"product\"],mainImage:\"/blog-images/d86fb94eb5fdceaf27ec8fe22a1f131f.jpg\",imageAlt:\"A man playing chess representing game changers in SaaS Sales Demos\",showCTA:!0,ctaCopy:\"Improve SaaS demos with Release's on-demand environments for seamless collaboration, faster bug resolution, and consistent deployments.\",ctaLink:\"https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=great-saas-sales-demos\",relatedPosts:[\"\"],ogImage:\"/blog-images/d86fb94eb5fdceaf27ec8fe22a1f131f.jpg\",excerpt:\"Great SaaS sales demos using these three game changers enabled by Release environments.\",tags:[\"platform-engineering\",\"product\"],ctaButton:\"Try Release for Free\"};function c(a){let e=Object.assign({h3:\"h3\",a:\"a\",span:\"span\",p:\"p\",ul:\"ul\",li:\"li\",em:\"em\"},a.components);return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsxs)(e.h3,{id:\"great-saassales-demos\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#great-saassales-demos\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Great SaaS\\xA0Sales Demos\"]}),`\n`,(0,t.jsx)(e.p,{children:\"Sales demos for Software as a Service (SaaS) products are vital to introducing your product to customers and gaining their interest or desire to use your product. The typical way that a SaaS product is demonstrated, especially in this day and age of remote work, is by presenting the application remotely, showing off features and highlighting use cases for the potential customer. A good demo can show the customer that your product is a good fit for them, but it can also do the opposite: a demonstration might reveal problems, shortcomings, or gaps that your SaaS product has that your customer might pick up on.\"}),`\n`,(0,t.jsx)(e.p,{children:\"As a DevOps engineer, I\\u2019ve received and viewed many demonstrations for a wide variety of products over many years. I\\u2019ve never had to present a sales demonstration myself (at least, not directly), but I have had to do setup and perform internal demonstrations, and I\\u2019ve also had to participate in designing or configuring SaaS product demonstrations. There are two SaaS demonstrations that I\\u2019ve participated in that were so memorable, they always come to mind even years later and after hundreds of demonstrations I\\u2019ve witnessed. The first demonstration was an unmitigated disaster and the second was one of the best that I ever witnessed thus far. Both of these examples lead to the 3 ways that you can make your SaaS sales demo the best it can possibly be.\"}),`\n`,(0,t.jsxs)(e.h3,{id:\"the-bad-example\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#the-bad-example\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"The Bad Example\"]}),`\n`,(0,t.jsx)(e.p,{children:\"One of the worst examples of a SaaS sales demo disaster happened quite a long time ago when a SaaS company was in the middle of releasing a new version of their software. They were also going public and the combination of these two events was a source of great fanfare in the industry and at the SaaS company. The company I worked for had tested the previous version of the product\\u2014let\\u2019s say the \\u201C1.0\\u201D product\\u2014and we were prospecting to get a very good deal on a one year implementation. The new \\u201C2.0\\u201D product was being released and our company was one of the customers that would\\xA0 be a \\u201Clogo\\u201D used to promote how well the product worked and that was trusted by our brand.\"}),`\n`,(0,t.jsx)(e.p,{children:\"The problem was that internally, many of us engineers and even some of the leadership, agreed that the product wasn\\u2019t very good. We were excited to try out the new version but were a bit skeptical the product would be vastly different than the 1.0 product. In our view, the product was large, bloated, unstable, and crashed often. In preparation for the big 2.0 sales demonstration, I sat across from an executive at the SaaS company and listed a long laundry list of issues that I considered important to be fixed in the new product. The executive (who turned out to be the SaaS provider\\u2019s CTO!!!) agreed with me completely and reassured me that \\u201Call of the issues\\u201D had been resolved in the new version.\"}),`\n`,(0,t.jsx)(e.p,{children:\"The large conference room lights dimmed and in a standing-room-only hush, the product demonstration started and was displayed on the projection screen. It was obvious the product was slow: noticeably slower than even the original. It also crashed almost immediately. The sales person started the demonstration again with a smooth cover-up. I remember the mood was still forgiving because it had crashed so suddenly and so quickly, nobody had really invested too much time yet. The sales demo started again and the product was slightly better. However, I could tell from the tics and movements on the screen that the sales person was purposely avoiding certain features and buttons that I personally knew were problematic in the old version. I knew he was avoiding them because they were likely to be buggy or cause issues.\"}),`\n`,(0,t.jsx)(e.p,{children:\"Despite such careful choreography, the product crashed yet again, in fact, it crashed numerous times to the point where the demonstration simply couldn\\u2019t continue. The executives from the SaaS company who were there to personally oversee the demonstration were flabbergasted. I personally heard one of the sales engineers speaking in hushed tones into his cell phone telling his operations folks to try to reboot the demo server, again.\"}),`\n`,(0,t.jsx)(e.p,{children:\"These are a sample of the types of excuses we were offered:\"}),`\n`,(0,t.jsxs)(e.ul,{children:[`\n`,(0,t.jsx)(e.li,{children:\"\\u201COh, the new version hasn\\u2019t been deployed yet, this is a pre-release demo.\\u201D\"}),`\n`,(0,t.jsx)(e.li,{children:\"\\u201CYou know, the dataset is wrong, we need to load more data.\\u201D\"}),`\n`,(0,t.jsx)(e.li,{children:\"\\u201CThe internet connection is really laggy and we have a production network environment for the real product.\\u201D\"}),`\n`,(0,t.jsx)(e.li,{children:\"\\u201CThe demo server isn\\u2019t ready for production yet, we\\u2019ve been upgrading our systems as fast as we can.\\u201D\"}),`\n`,(0,t.jsx)(e.li,{children:\"\\u201CWe haven\\u2019t fully tested this new feature, and it wasn\\u2019t supposed to appear in this demonstration. Sorry.\\u201D\"}),`\n`]}),`\n`,(0,t.jsx)(e.p,{children:\"But eventually the truth was that the sales demo itself had failed. This had almost nothing to do with the product in actual practice, but that didn\\u2019t matter. The demonstration was unable to satisfactorily show what the product could do.\"}),`\n`,(0,t.jsxs)(e.h3,{id:\"the-good-example\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#the-good-example\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"The Good Example\"]}),`\n`,(0,t.jsx)(e.p,{children:\"On the opposite side of the spectrum, one of the best sales SaaS demos I witnessed was nearly perfect and stood out how great the process was. The SaaS product was a monitoring tool that provided metrics and log events for a running application, and would be used to drill down and into metrics, events, logs, and so forth. This demonstration occurred many years ago and does not reflect on any present companies or products you may think of. I just want to clarify this so that you do not form an impression of any existing company or product today.\"}),`\n`,(0,t.jsx)(e.p,{children:\"The SaaS salesperson ran the demonstration by logging into the actual live product with a special demonstration credential, and showed real, live data that were flowing from a fake application that was written specifically for the demonstration. It ran on a ten-minute cycle of generating pre-created events, metrics, logs, and so forth. The salesperson was able to look at \\u201Creal,\\u201D \\u201Clive,\\u201D and updated data as events happened in the application. Because the data were pregenerated and ran continuously, it seemed like the product worked perfectly and would do exactly what the salesperson seemed to make so easy.\"}),`\n`,(0,t.jsx)(e.p,{children:\"There was one flaw that I spotted and confirmed but it was relatively minor. This flaw does demonstrate one of the key points I will discuss later on for a successful SaaS demonstration. The salesperson was unable to change or update any of the data or the layout of the screen. They were unable to demonstrate the ability to create and edit reports for the product. They were using what was essentially a \\u201Cread-only\\u201D demonstration of the product and could not change anything inside the account. The reason for this is to perfectly preserve the demonstration process so that the next time the product is reviewed, it would be exactly the way it was before without any possible changes.\"}),`\n`,(0,t.jsx)(e.p,{children:\"Based on these examples, I will now detail the 3 most important ways to pull off a perfect SaaS sales demonstration.\"}),`\n`,(0,t.jsxs)(e.h3,{id:\"game-changer-1-provide-a-real-experience\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#game-changer-1-provide-a-real-experience\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Game Changer 1: Provide a \\u201CReal\\u201D Experience\"]}),`\n`,(0,t.jsx)(e.p,{children:\"Your SaaS demonstration must be as \\u201Creal\\u201D as possible. I use the word \\u201Creal\\u201D in scare quotes because often you cannot provide any actual \\u201Creal\\u201D customer data since the prospects aren\\u2019t customers yet. Even if you do have some customer data or can use actual real data of some sort, the customers are not interacting in any real way with the product yet, so let\\u2019s just use the scare quotes. Never-the-less, the data that you use in the sales demonstration needs to be realistic enough to provide a valid reflection of what your product can do and what the customer needs, in a way that the product actually works.\"}),`\n`,(0,t.jsx)(e.p,{children:\"There is no other way to perform this than to use your actual live product in its actual live state. There are a lot of excuses and corners you will want to cut around this issue, but believe me when I tell you that this is critical. You must use your actual product in its actual state to perform the SaaS demonstration. When a customer test drives a car at the dealership, they are driving a real car that can be sold, not a fake or reproduction or toy model. You must do the same with your SaaS product.\"}),`\n`,(0,t.jsxs)(e.p,{children:[\"You will need a stable, performant, fully functional, and fully capable product that functions exactly the way it will function when the customer logs in and starts using the product. It must function perfectly the first time, the second time, and the \",(0,t.jsx)(e.em,{children:\"nth\"}),\" time you demonstrate your product. It must not conflict with someone else when two salespeople perform a demonstration at the same time, and it must not have any leftover or incorrect settings from a previous demonstration. It also must be stable and not be updated or rebooted or crash because of normal operations of the live site.\"]}),`\n`,(0,t.jsxs)(e.p,{children:[\"This is where a \",(0,t.jsx)(e.a,{href:\"https://releasehub.com\",children:\"Release\"}),\" sales demonstration environment comes into play. A salesperson or organization that uses Release would have an environment setup and ready to deploy at a moment\\u2019s notice with pre-populated data, user accounts, and features fixed to a particular time or branch set. A few minutes before the demonstration, the salesperson would simply start a new, completely isolated and perfectly preserved application stack and dataset, ready to fully demonstrate the full, live, exact duplicate SaaS product to the customer. There is no chance for shared environments to conflict; the demonstrations can be tailored to each customer industry or feature set (if applicable); and there is no possibility of having incorrect settings or data present that can disrupt the demonstration.\"]}),`\n`,(0,t.jsx)(e.p,{children:\"Because the scale of the application can be set for one person or only one small subset of a customer, the application can be tuned to perform perfectly and for very little cost compared to the production environment. As soon as the demonstration is finished, the environment can be destroyed so that any hosting costs stop immediately.\"}),`\n`,(0,t.jsxs)(e.h3,{id:\"game-changer-2-provide-a-live-experience\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#game-changer-2-provide-a-live-experience\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Game Changer 2: Provide a Live Experience\"]}),`\n`,(0,t.jsx)(e.p,{children:\"Notice I did not use scare quotes in this section for the word \\u201Clive.\\u201D The demonstration experience can be faked to some extent in terms of pre-populated data or accounts, but it cannot be faked in terms of actual functionality. You cannot just show a static video of your product demonstration and expect customers to buy into it. The potential customers must be able to intervene, interject, ask \\u201Cwhat if,\\u201D or (even better) take over the controls and \\u201Csee what happens.\\u201D\"}),`\n`,(0,t.jsx)(e.p,{children:\"I already can hear some objections from salespeople who say that when an application is truly live then it is subject to unknown influences and problems. This is true, except that if you have a product experience that is well-tested and fixed in place at a particular time and version, then the demonstration can be well-rehearsed and practiced. This is directly at odds with the usual organization's push to release new features and update the product regularly. How is it possible to be able to have completely stable, well-populated, isolated environments at the same time that the development cycle needs to be fast, regularly spaced, and turn quickly?\"}),`\n`,(0,t.jsxs)(e.p,{children:[\"This is where \",(0,t.jsx)(e.a,{href:\"https://releasehub.com\",children:\"Release\"}),\" sales demonstration environments come into play. An application template can be set to a stable version or branch that is well-tested and cleaned for use to show to customers. It will be prepopulated with well-known, well-tested data. When a salesperson or sales organization is ready to show off new features or product enhancements, those features can be versioned and configured for use either as a separate application or as an existing branch of the Release application so that it can be spun up on demand for a customer. Once the branch and application template are tested, that version can be used by anyone in the sales organization to demonstrate the new features or product experiences and data.\"]}),`\n`,(0,t.jsx)(e.p,{children:\"In this scenario, it doesn\\u2019t matter if the production application is running on version 0.9 or 1.1 or even 2.0, the sales demonstration environment can be configured to run exactly the same version 1.0 every single time it is deployed and demonstrated to potential customers. Also, it could be devastating if the product version were to change in the middle of the demonstration or during the customer demonstration time period. Conversely, if the production environment is running on a delayed version and a customer wants to see the new features, the new sales environments can be configured and tested for potential preview customers to give early feedback, advice, or to gain other valuable information.\"}),`\n`,(0,t.jsx)(e.p,{children:\"In fact, all of these scenarios could be in play at any one time: some sales environments could be set behind on an extremely stable, well-tested product experience while some sales environments can be spun up on demand at exactly the same version of the production experience, while yet other environments can be ahead of production to showcase new features and product experiences. The sales person or organization could pick and choose which scenario they would like to demonstrate and create the new environment tailored to their exact requirements and customer profile.\"}),`\n`,(0,t.jsx)(e.p,{children:\"Each environment would be isolated and stable without any contention or changes interfering.\"}),`\n`,(0,t.jsxs)(e.h3,{id:\"game-changer-3-provide-a-malleable-persistent-experiencewhich-can-be-recreated\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#game-changer-3-provide-a-malleable-persistent-experiencewhich-can-be-recreated\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Game Changer 3: Provide a Malleable, Persistent Experience\\u2014Which Can be Recreated\"]}),`\n`,(0,t.jsx)(e.p,{children:\"Even in the best SaaS sales demonstrations I have experienced, there was always a critical and nagging worry I had that the environment was not going to persist and that anything that I was working on or being demonstrated would not last for long after the demo. Strangely, the reverse is true as well: oftentimes the customer might want to revert all their changes back to a pristine state after \\u201Cmessing around\\u201D with things. How can you resolve this tension between two extremes?\"}),`\n`,(0,t.jsxs)(e.p,{children:[\"This is where \",(0,t.jsx)(e.a,{href:\"https://releasehub.com\",children:\"Release\"}),\" sales demonstration environments come into play. The salesperson or organization can spin up entire environments for one or more customers and allow the customer(s) to \\u201Cplay\\u201D with the product without any fear of damaging or impacting anything else. If the customer wants to start over, a new environment could be created, or the exact same environment could be duplicated to start over. In fact, the salesperson could construct several accounts for the customer to use if that is appropriate, or if multiple employees or departments could be interacting with each other or the demonstration. All of this can happen in such a way that the isolated environment can be snapshot or ported to production once everything is set up the way it should be. Alternatively, the snapshot could be ported to a new sales demonstration environment for a \\u201Creset.\\u201D\"]}),`\n`,(0,t.jsx)(e.p,{children:\"Even better, in some cases, the new sales demonstration environment could become a staging or testing environment for the customer to use indefinitely or on-demand when they need to test possible scenarios or possibilities. All of this can occur without any impact or conflicts with the live production product. In a few rare cases, the SaaS product could become so customized to an industry vertical or customer that it may become a standalone environment live in production for use by that customer or industry.\"}),`\n`,(0,t.jsx)(e.p,{children:\"This brings up a great point about environments at Release: they are all identical in design and spirit, and can be used for any and for all purposes. A software engineer\\u2019s local development idea can become a testing ground for product development, which can easily be deployed for a sales demonstration, which could easily become a live production environment running live paying customers. With Release, your environments are not limited by even the sky above or the ground below!\"}),`\n`,(0,t.jsxs)(e.p,{children:[\"Photo by \",(0,t.jsx)(e.a,{href:\"https://unsplash.com/@dimage_carlos?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText\",children:\"Carlos Esteves\"}),\" on \",(0,t.jsx)(e.a,{href:\"/?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText\",children:\"Unsplash\"})]})]})}function S(a={}){let{wrapper:e}=a.components||{};return e?(0,t.jsx)(e,Object.assign({},a,{children:(0,t.jsx)(c,a)})):c(a)}var x=S;return v(T);})();\n;return Component;"
        },
        "_id": "blog/posts/great-saas-sales-demos.mdx",
        "_raw": {
          "sourceFilePath": "blog/posts/great-saas-sales-demos.mdx",
          "sourceFileName": "great-saas-sales-demos.mdx",
          "sourceFileDir": "blog/posts",
          "contentType": "mdx",
          "flattenedPath": "blog/posts/great-saas-sales-demos"
        },
        "type": "BlogPost",
        "computedSlug": "great-saas-sales-demos"
      },
      "documentHash": "1739393595019",
      "hasWarnings": false,
      "documentTypeName": "BlogPost"
    },
    "blog/posts/gromit-an-open-source-ai-assistant-for-your-documentation.mdx": {
      "document": {
        "title": "Gromit: an Open Source AI Assistant for your Documentation",
        "summary": "",
        "publishDate": "Wed May 10 2023 21:38:28 GMT+0000 (Coordinated Universal Time)",
        "author": "david-giffin",
        "readingTime": 4,
        "categories": [
          "ai",
          "platform-engineering"
        ],
        "mainImage": "/blog-images/7e8d7d536d69a30e608bbb547c0f46f4.jpg",
        "imageAlt": "A dog wearing a glass and the prashe: Gromit OSS project",
        "showCTA": true,
        "ctaCopy": "Integrate Gromit AI Assistant with Release for streamlined documentation search and indexing in your development workflow.",
        "ctaLink": "https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=gromit-an-open-source-ai-assistant-for-your-documentation",
        "relatedPosts": [
          "training-chatgpt-with-custom-libraries-using-extensions"
        ],
        "ogImage": "/blog-images/7e8d7d536d69a30e608bbb547c0f46f4.jpg",
        "excerpt": "Gromit: an Open Source AI Assistant for your Documentation Here at Release we have been working on tooling to integrate with OpenAI and ChatGPT using many op...",
        "tags": [
          "ai",
          "platform-engineering"
        ],
        "ctaButton": "Try Release for Free",
        "body": {
          "raw": "\n## Gromit: an Open Source AI Assistant for your Documentation\n\nHere at Release we have been working on tooling to integrate with OpenAI and ChatGPT using many open source tools. We have learned a lot from integrating these tools into our product and wanted to give back to the Open Source community with our AI Assistant Gromit project. \n\nImagine that you built an awesome app, wrote up the documentation and laid out all the relevant tidbits of information a new or tenured user would need to know. You keep on adding to your knowledge base to explain features, functionality and process in more detail, and somehow it raises more questions. Now users are pouring over even more information you generated to find answers to their questions. You could structure your documentation differently, add more tags, do more backlinks, etc., or you could have AI help you surface the right pieces, for the right user, depending on their question. This is how the idea for Gromit came about. \n\nUsing [embeddings and vector search databases](https://release.com/blog/training-chatgpt-with-custom-libraries-using-extensions) we can search all of your documentation based on a question given to the AI Assistant. The relevant documentation from the search results are used to generate a prompt for OpenAI. We have an internal project called Gromit that provides a simple way to index your documentation using OpenAI’s embeddings and stores these indices in a Redis vector search database. \n\nWe have broken out the underlying Ruby code into the [Gromit Ruby Gem](https://rubygems.org/gems/gromit) which allows developers to create their own API on top of the basic Gromit functionality. Gromit also provides a Rails engine which can be mounted in any Rails application which will give you the basic APIs out of the box within minutes. We are pleased to give back to the community and support open source projects by contributing this Gem to the open source community. \n\nHere is a quick example of how you can start using it:\n\nTo mount the Gromit Rails engine in your Rails project. First edit your `Gemfile` and add\n\n```ruby\n\ngem “gromit”\n\n```\n\nAfter executing the `bundle` command, edit your `config/routes.rb` and add the following:\n\n```ruby\n\nRails.application.routes.draw do\n  mount Gromit::Engine => \"/\"\nend\n\n```\n\nYou will need to add an environment variable with your OpenAI API access token. Edit your `.env` or `.env.local` and add the following:\n\n```ruby\n\nOPENAPI_ACCESS_TOKEN=your-openai-token\n\n```\n\nTo install the redis-stack-server do the following. Note, that if you are already running Redis you will need to stop/disable the old version of redis. The redis-stack-server provides additional search indexing functionality not provided in the default Redis server.\n\n```bash\n\nbrew tap redis-stack/redis-stack\nbrew install redis-stack\n\n```\n\nAt this point you can index all of your documentation that’s in markdown format like so:\n\n```bash\n\nbundle exec gromit-reindexer -s /path/to/your/docs\n\n```\n\nWe have also created a [Gromit Example Rails Application](https://github.com/releasehub-com/gromit-example) which also includes a [Nextjs OpenAI Documentation Search](https://github.com/supabase-community/nextjs-openai-doc-search) user interface which is hooked into the Gromit Rails Application. \n\nHere is an example of Gromit in action using the Next.js frontend [created by the folks at supabase](https://supabase.com/): \n\n![](/blog-images/5b603ede3d06f8519a2365f300f7d4c0.png)\n\nAt Release we are strong believers in the power of open source. In our drive to create a development platform that better serves modern development teams, our founding team has been using and contributing to the open source community for years. Founded in 2019 Release was and still is built entirely with open source software!\n\nTest our Gromit for yourself, tell us what you think and share it with others. We look forward to sharing more fun projects with the community in the future.\n",
          "code": "var Component=(()=>{var u=Object.create;var a=Object.defineProperty;var h=Object.getOwnPropertyDescriptor;var m=Object.getOwnPropertyNames;var p=Object.getPrototypeOf,g=Object.prototype.hasOwnProperty;var f=(t,e)=>()=>(e||t((e={exports:{}}).exports,e),e.exports),b=(t,e)=>{for(var o in e)a(t,o,{get:e[o],enumerable:!0})},s=(t,e,o,r)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let i of m(e))!g.call(t,i)&&i!==o&&a(t,i,{get:()=>e[i],enumerable:!(r=h(e,i))||r.enumerable});return t};var y=(t,e,o)=>(o=t!=null?u(p(t)):{},s(e||!t||!t.__esModule?a(o,\"default\",{value:t,enumerable:!0}):o,t)),w=t=>s(a({},\"__esModule\",{value:!0}),t);var d=f((I,c)=>{c.exports=_jsx_runtime});var k={};b(k,{default:()=>x,frontmatter:()=>v});var n=y(d()),v={title:\"Gromit: an Open Source AI Assistant for your Documentation\",summary:\"\",publishDate:\"Wed May 10 2023 21:38:28 GMT+0000 (Coordinated Universal Time)\",author:\"david-giffin\",readingTime:4,categories:[\"ai\",\"platform-engineering\"],mainImage:\"/blog-images/7e8d7d536d69a30e608bbb547c0f46f4.jpg\",imageAlt:\"A dog wearing a glass and the prashe: Gromit OSS project\",showCTA:!0,ctaCopy:\"Integrate Gromit AI Assistant with Release for streamlined documentation search and indexing in your development workflow.\",ctaLink:\"https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=gromit-an-open-source-ai-assistant-for-your-documentation\",relatedPosts:[\"training-chatgpt-with-custom-libraries-using-extensions\"],ogImage:\"/blog-images/7e8d7d536d69a30e608bbb547c0f46f4.jpg\",excerpt:\"Gromit: an Open Source AI Assistant for your Documentation Here at Release we have been working on tooling to integrate with OpenAI and ChatGPT using many op...\",tags:[\"ai\",\"platform-engineering\"],ctaButton:\"Try Release for Free\"};function l(t){let e=Object.assign({h2:\"h2\",a:\"a\",span:\"span\",p:\"p\",code:\"code\",pre:\"pre\",img:\"img\"},t.components);return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsxs)(e.h2,{id:\"gromit-an-open-source-ai-assistant-for-your-documentation\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#gromit-an-open-source-ai-assistant-for-your-documentation\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Gromit: an Open Source AI Assistant for your Documentation\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Here at Release we have been working on tooling to integrate with OpenAI and ChatGPT using many open source tools. We have learned a lot from integrating these tools into our product and wanted to give back to the Open Source community with our AI Assistant Gromit project.\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:\"Imagine that you built an awesome app, wrote up the documentation and laid out all the relevant tidbits of information a new or tenured user would need to know. You keep on adding to your knowledge base to explain features, functionality and process in more detail, and somehow it raises more questions. Now users are pouring over even more information you generated to find answers to their questions. You could structure your documentation differently, add more tags, do more backlinks, etc., or you could have AI help you surface the right pieces, for the right user, depending on their question. This is how the idea for Gromit came about.\\xA0\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"Using \",(0,n.jsx)(e.a,{href:\"https://release.com/blog/training-chatgpt-with-custom-libraries-using-extensions\",children:\"embeddings and vector search databases\"}),\" we can search all of your documentation based on a question given to the AI Assistant. The relevant documentation from the search results are used to generate a prompt for OpenAI. We have an internal project called Gromit that provides a simple way to index your documentation using OpenAI\\u2019s embeddings and stores these indices in a Redis vector search database.\\xA0\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"We have broken out the underlying Ruby code into the \",(0,n.jsx)(e.a,{href:\"https://rubygems.org/gems/gromit\",children:\"Gromit Ruby Gem\"}),\" which allows developers to create their own API on top of the basic Gromit functionality. Gromit also provides a Rails engine which can be mounted in any Rails application which will give you the basic APIs out of the box within minutes. We are pleased to give back to the community and support open source projects by contributing this Gem to the open source community.\\xA0\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Here is a quick example of how you can start using it:\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"To mount the Gromit Rails engine in your Rails project. First edit your \",(0,n.jsx)(e.code,{children:\"Gemfile\"}),\" and add\"]}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-ruby\",children:`\ngem \\u201Cgromit\\u201D\n\n`})}),`\n`,(0,n.jsxs)(e.p,{children:[\"After executing the \",(0,n.jsx)(e.code,{children:\"bundle\"}),\" command, edit your \",(0,n.jsx)(e.code,{children:\"config/routes.rb\"}),\" and add the following:\"]}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-ruby\",children:`\nRails.application.routes.draw do\n \\xA0mount Gromit::Engine => \"/\"\nend\n\n`})}),`\n`,(0,n.jsxs)(e.p,{children:[\"You will need to add an environment variable with your OpenAI API access token. Edit your \",(0,n.jsx)(e.code,{children:\".env\"}),\" or \",(0,n.jsx)(e.code,{children:\".env.local\"}),\" and add the following:\"]}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-ruby\",children:`\nOPENAPI_ACCESS_TOKEN=your-openai-token\n\n`})}),`\n`,(0,n.jsx)(e.p,{children:\"To install the redis-stack-server do the following. Note, that if you are already running Redis you will need to stop/disable the old version of redis. The redis-stack-server provides additional search indexing functionality not provided in the default Redis server.\"}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-bash\",children:`\nbrew tap redis-stack/redis-stack\nbrew install redis-stack\n\n`})}),`\n`,(0,n.jsx)(e.p,{children:\"At this point you can index all of your documentation that\\u2019s in markdown format like so:\"}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-bash\",children:`\nbundle exec gromit-reindexer -s /path/to/your/docs\n\n`})}),`\n`,(0,n.jsxs)(e.p,{children:[\"We have also created a \",(0,n.jsx)(e.a,{href:\"https://github.com/releasehub-com/gromit-example\",children:\"Gromit Example Rails Application\"}),\" which also includes a \",(0,n.jsx)(e.a,{href:\"https://github.com/supabase-community/nextjs-openai-doc-search\",children:\"Nextjs OpenAI Documentation Search\"}),\" user interface which is hooked into the Gromit Rails Application.\\xA0\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"Here is an example of Gromit in action using the Next.js frontend \",(0,n.jsx)(e.a,{href:\"https://supabase.com/\",children:\"created by the folks at supabase\"}),\":\\xA0\"]}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/5b603ede3d06f8519a2365f300f7d4c0.png\",alt:\"\"})}),`\n`,(0,n.jsx)(e.p,{children:\"At Release we are strong believers in the power of open source. In our drive to create a development platform that better serves modern development teams, our founding team has been using and contributing to the open source community for years. Founded in 2019 Release was and still is built entirely with open source software!\"}),`\n`,(0,n.jsx)(e.p,{children:\"Test our Gromit for yourself, tell us what you think and share it with others. We look forward to sharing more fun projects with the community in the future.\"})]})}function A(t={}){let{wrapper:e}=t.components||{};return e?(0,n.jsx)(e,Object.assign({},t,{children:(0,n.jsx)(l,t)})):l(t)}var x=A;return w(k);})();\n;return Component;"
        },
        "_id": "blog/posts/gromit-an-open-source-ai-assistant-for-your-documentation.mdx",
        "_raw": {
          "sourceFilePath": "blog/posts/gromit-an-open-source-ai-assistant-for-your-documentation.mdx",
          "sourceFileName": "gromit-an-open-source-ai-assistant-for-your-documentation.mdx",
          "sourceFileDir": "blog/posts",
          "contentType": "mdx",
          "flattenedPath": "blog/posts/gromit-an-open-source-ai-assistant-for-your-documentation"
        },
        "type": "BlogPost",
        "computedSlug": "gromit-an-open-source-ai-assistant-for-your-documentation"
      },
      "documentHash": "1739393595019",
      "hasWarnings": false,
      "documentTypeName": "BlogPost"
    },
    "blog/posts/hosting-mastodon-on-release-and-aws.mdx": {
      "document": {
        "title": "Learn how to host your own Mastodon instance on AWS",
        "summary": "A guide on how to self-host Mastodon on Release. You'll set up a web server, storage and everything else you need.",
        "publishDate": "Fri Dec 16 2022 12:17:26 GMT+0000 (Coordinated Universal Time)",
        "author": "",
        "readingTime": 20,
        "categories": [
          "kubernetes",
          "platform-engineering"
        ],
        "mainImage": "/blog-images/26c0878254b3e4a8eb06fe9f679beb4e.jpg",
        "imageAlt": "an elephant with a tusk",
        "showCTA": true,
        "ctaCopy": "Unlock the power of hosting and scaling Mastodon instances on AWS with Release's seamless Kubernetes clusters. Boost your DevOps workflow now!",
        "ctaLink": "https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=hosting-mastodon-on-release-and-aws",
        "relatedPosts": [
          ""
        ],
        "ogImage": "/blog-images/26c0878254b3e4a8eb06fe9f679beb4e.jpg",
        "excerpt": "A guide on how to self-host Mastodon on Release. You'll set up a web server, storage and everything else you need.",
        "tags": [
          "kubernetes",
          "platform-engineering"
        ],
        "ctaButton": "Try Release for Free",
        "body": {
          "raw": "\n### Mastodon development environments on Release and AWS\n\nTwitter users are flying the coop in droves and many are turning to Mastodon, an open-source federated social network.\n\nUnlike Twitter's centralized architecture, a federated social network consists of thousands of individual social networks, called _instances_, each with its own users.\n\nFederation is sometimes described as being similar to email in the sense that email users can send each other messages, even if they have different email hosts and use different domains.\n\nMastodon instances fetch content from other instances, which allows a user on one server to follow a user on a different server, thereby creating one big network.\n\nTo the DevOps-minded among us, the most compelling thing about the fastest-growing Twitter alternative is figuring out how to host and scale this exciting new platform.\n\nAt Release, we enable teams to create development, staging, and production environments for their apps. If you intend to add new features or otherwise modify your Mastodon instances, Release can provide the environment you need to move your project forward. Whether you want to just take Mastodon for a spin to see what it's all about, or run a production Mastodon instance that can scale up as you need, [using Release](https://release.com/book-a-demo) means you don't have to trade off simplicity for power. You can easily collaborate with your colleagues, preview the latest features, and upgrade to the latest builds.\n\nIn this post, we'll put Mastodon's welcoming community and complex federation protocols aside so that we can focus on the nuts and bolts of hosting a Mastodon instance.\n\n### What makes a Mastodon instance\n\n![](/blog-images/b500f3959bc54b57ac9455a8c5a21d0b.png)\n\nMastodon Services Diagram\n\nA Mastodon instance consists of seven services:\n\n1.  A **web server**, which runs on Ruby on Rails.\n2.  A **streaming server** that enables the instance's local users to send and receive real-time updates to the server.\n3.  **Sidekiq**, a job scheduler that runs background tasks, polls other instances' web servers, and sends data to other instances' web servers.\n4.  A **Redis server** that stores jobs for Sidekiq and caches data such as feeds for local users.\n5.  A **PostgreSQL database** that stores posts, user profiles, and instance settings.\n6.  **File storage** to store and serve media such as images and videos.\n\nAn **SMTP email server** to send messages to local users' email accounts.\n\n### How we'll run a Mastodon instance on Release and AWS\n\nRelease creates Kubernetes clusters in your AWS account.\n\nAlthough all seven of Mastodon's services could be hosted on Kubernetes, we'll use a few AWS-provided services to host supporting systems.\n\nWe'll create containers to run Mastodon's web server, streaming server, and Sidekiq in Kubernetes.\n\nFrom AWS, we'll use:\n\n- Amazon ElastiCache for Redis\n- Amazon RDS to host a PostgreSQL server\n- Amazon S3 as a file store\n- Amazon SES to send emails to local users\n\n### **Prepare your Release account**\n\nBefore we get started, you'll need to [create a Release account](https://docs.releasehub.com/getting-started/create-an-account) and upgrade to a paid account.\n\nNext, [verify your domain](https://docs.releasehub.com/guides-and-examples/domains-and-dns/external-dns) in Release, and [create a cluster](https://docs.releasehub.com/guides-and-examples/advanced-guides/create-a-cluster) using the AWS integration.\n\n#### Configuration details to save\n\nIn this guide, configuration details to note down will be pointed out with the heading: **Configuration details to save**.\n\nWhile in Release, navigate to your cluster, and note down the following:\n\n1.  The **cluster context**.\n2.  Your **cluster region**.\n\nYour **node group**.\n\n### Prepare your local machine\n\nOn your local system, you'll need to install Git and Docker.\n\nKeep a text editor handy to take notes. We'll gather configuration details as we go along, and saving these in a central file will make setting up your environment much easier later.\n\n### Fork the Mastodon repository on GitHub\n\nFork the Mastodon repository from GitHub to your GitHub, Bitbucket, or GitLab account. We'll use GitHub in this guide.\n\n1.  Log into your GitHub account.\n2.  Fork the [Mastodon](https://github.com/mastodon/mastodon/fork) repository.\n\nGitHub creates a public fork by default. Since Mastodon is AGPL-licensed, keeping your fork public is a good way to make sure you adhere to licensing requirements from the start.\n\n#### Configuration details to save\n\nMake a note of your **repository name** from GitHub.\n\n### Clone your Mastodon fork to your local machine\n\n1.  On the main page of your repository on GitHub.com, click the **Code** button\n2.  Copy the URL for the repository.\n3.  Open your terminal and navigate to where you want to clone your repository.\n4.  Clone your repository using the URL you copied\n\n```docker\n\ngit clone git@github.com:YOUR-USERNAME/mastodon.git\n\n```\n\n5.  Change your current working folder to the new repository and keep your terminal open to use again later.\n\n```docker\n\ncd mastodon\n\n```\n\n### **Generate Mastodon secret keys and VAPID keys**\n\nMastodon needs three secret keys and one public key to run. We'll generate these using the tootsuite/mastodon Docker image on our local machine.\n\n1.  Set a local environment variable SECRET_KEY_BASE. In your terminal, run:\n\n```docker\n\nSECRET_KEY_BASE=$(docker run --rm -it tootsuite/mastodon:v4.0.2 bin/rake secret)\n\n```\n\n2.  Set a local environment variable OTP_SECRET, by running the following in your terminal:\n\n```docker\n\nOTP_SECRET=$(docker run --rm -it tootsuite/mastodon:v4.0.2 bin/rake secret)\n\n```\n\n3.  Generate VAPID keys, using the SECRET_KEY_BASE and OTP_SECRET as inputs, and print our new keys to the terminal.\n\n```docker\n\ndocker run --rm \\\n   -e OTP_SECRET=$OTP_SECRET \\\n   -e SECRET_KEY_BASE=$SECRET_KEY_BASE \\\n   -it tootsuite/mastodon:v4.0.2 \\\n   bin/rake mastodon:webpush:generate_vapid_key && \\\n   echo \"SECRET_KEY_BASE=$SECRET_KEY_BASE\" && \\\n   echo \"OTP_SECRET=$OTP_SECRET\"\n```\n\n### **Configuration details to save**\n\nCopy and save VAPID_PRIVATE_KEY, VAPID_PUBLIC_KEY, SECRET_KEY_BASE, and OTP_SECRET values from the output, which should look something like this:\n\n```docker\n\nVAPID_PRIVATE_KEY=cDpok1oPz1u6jpP2fE_Vf2TWBy-VVHh0n3KqdCEz81A=\nVAPID_PUBLIC_KEY=BAb1gkLWzalGfAZq_7IeGX19T1Rp4I5aIerN_sDfon5eenIEn9DAWU1LLpFSu6VjtnhJJilbZXLBBdUSa6DL74Y=\nSECRET_KEY_BASE=bb0231c8e07870b70934a9487cd6e796bbd6f4fe086dfa9039be3743a96e18726ea168cde3cf3ea823c5214e09b8afb7696324eb024f4317cb2626e0545aac12\nOTP_SECRET=19a4d2e15d4ffa63c5ad08d716f33b2296419a6529a49908bd4f879891710d1f9efc1007efc9c18e82aeb52cba763cbd89b030ed2069be8464d9f26d167ea102\n```\n\n### **AWS Step 1: Create a PostgreSQL database using Amazon RDS**\n\nLog in to your AWS account and navigate to RDS.\n\nMake sure your AWS region is set to the same region as your Release cluster.\n\nCreate a new PostgreSQL 14 database:\n\n1.  Click on **create database**.\n\n![RDS create database](/blog-images/a8f7af65e782a9b482b6edf9e806694b.png)\n\n_RDS create database_ 2. Select **standard create**. 3. Select PostgreSQL as the engine type. 4. Pick PostgreSQL 14.5-R1 as the engine version.\n\n![RDS create database steps 2 to 4](/blog-images/657c13cc97a0c58231008fbc90e3bd39.png)\n\n_RDS create database steps 2 to 4_ 5. Select **Free tier** if you're only setting up a small/test instance. 6. Set the _DB instance identifier_ to mastodon-1. We'll need to refer to this again later.\n\n![RDS create database steps 5 to 6](/blog-images/9e77fd3763b23a49be6d22ab0586cfc1.png)\n\n_RDS create database steps 5 to 6_ 7. To make sure you don't use more space than needed, change the _allocated storage_ to 20 GiB. 8. Disable autoscaling. 9. Under the **connectivity** section, select the virtual private cloud (VPC) that Release created for your cluster. This will enable services running in the cluster to reach your new database.\n\n![RDS create database steps 7 to 8](/blog-images/aa25b0121074a8bf5953f1bf547ee4da.png)\n\n_RDS create database steps 7 to 8_ 10. Choose to allow an existing security group. 11. Under **Existing VPC security groups**, pick the security group that starts with eks-cluster-sg- followed by your cluster context, then close the dropdown.\n\n![RDS create database steps 8 to 10](/blog-images/cb7eb42499a8c3e28726ddb6b552c6cd.png)\n\n_RDS create database steps 9 to 11_ 12. Click **Create database**.\n\n![RDS create database steps 8 to 10](/blog-images/5689429d0212e384a6198ff37a1ac92a.png)\n\n_RDS create database step 12_\n\nYour database will take a few minutes to launch.\n\n#### Configuration details to save\n\nOnce the database is ready, click on **view connection details** and note down the **database endpoint** and **database password**.\n\n![Screenshot of RDS PostgreSQL configuration details](/blog-images/c0b4990b5c39223beec5652e543826a8.png)\n\n_RDS PostgreSQL configuration details_\n\n### AWS Step 2: Create a Redis cache using Amazon ElastiCache\n\nBefore we create a Redis cache, we'll need to get the VPC ID for our Release cluster (Amazon ElastiCache does not show the full VPC name when creating a Redis cluster).\n\nIn AWS, navigate to **VPC**, then note down the VPC ID for your Release cluster.\n\n![Screenshot of Amazon VPC](/blog-images/a415ad14fc62b594fce757cb13b7a55e.png)\n\n_Amazon VPC_\n\nWith the VPC ID at hand, we can create a Redis cluster. In your AWS account, navigate to ElastiCache.\n\n1.  Click on **create cluster**.\n2.  Select **create Redis cluster**.\n\n![Screenshot of ElastiCache](/blog-images/073994066b09d353fb7e1700275ab2e4.png)\n\n_ElastiCache_\n\nThis opens up the **cluster settings** page, where we'll configure our Redis cluster.\n\nUnder **cluster info**, set a name for this Redis cluster, for example, mastodon-redis.\n\n![Screenshot of Redis configuration section](/blog-images/eaba3a434d845fe7e5135f7dde97fd89.png)\n\n_Redis configuration section_\n\nIn the **connectivity** section:\n\n1.  Set a name for the Redis cluster's subnet, for example, mastodon-redis-subnet.\n2.  Select your Release cluster's **VPC ID** from the dropdown.\n\n![Screenshot of Redis connectivity section](/blog-images/c9e88c18c4248cb0849aa8eabc18332c.png)\n\n_Redis connectivity section_\n\nClick **Next**.\n\nUnder **selected security groups**, click **Manage**.\n\n1.  Filter the security groups to find one that starts with eks-cluster-sg-.\n2.  Select the security group that starts with eks-cluster-sg- followed by your Release cluster's context.\n3.  Click **choose**.\n\n![Screenshot of Redis advanced settings page](/blog-images/799f15d8d6037eb172573272b40a6f99.png)\n\n_Screenshot of Redis advanced settings page_\n\nScroll down to the bottom of the page and click **next**.\n\nScroll down the page to review your Redis settings, then click on **create**.\n\nAWS will take a moment to create your Redis cluster.\n\n#### Configuration details to save\n\nOpen the Redis cluster's details and copy the Redis cluster's endpoint (you can omit the port number).\n\n![Screenshot of Redis Cluster details](/blog-images/4646a089f3c224805a89809e571aad8e.png)\n\n_Redis Cluster details_\n\n### AWS Step 3: Create an Amazon S3 bucket for user media\n\nIn your AWS account, navigate to S3.\n\nClick on **create bucket**.\n\nEnter a unique bucket name, for example, mastodon-media-example.\n\nUncheck **Block all public access**.\n\nCheck \"I acknowledge that the current settings might result in this bucket and the objects within becoming public\".\n\nClick **create bucket**.\n\nAWS will take a moment to create your new S3 bucket.\n\n### AWS Step 4: Create an IAM user to write to S3\n\nIn your AWS account, navigate to **Identity and Access Management (IAM)**.\n\nClick on **Users**, then on **Add users**.\n\n1.  Enter a username, for example, mastodon-s3-writer.\n2.  Under **Select AWS credential type**, select only **Access key - Programmatic access**.\n3.  Click **Next: Permissions**.\n\nOn the **Set permissions** page:\n\n1.  Select **Attach existing policies directly**.\n2.  Filter for s3 policies.\n3.  Check **AmazonS3FullAccess**.\n4.  Click **Next: Tags**.\n\nClick **Next: Review**.\n\nFinally, click **Create user**.\n\n#### **Configuration details to save**\n\nNote down the access key ID and secret access key. Keep in mind that you won't be able to see this secret again, so you'll need to save it now and keep it secure.\n\n### AWS Step 5: Create an Amazon SES identity\n\nIn AWS, navigate to **Amazon Simple Email Service (SES)**.\n\nClick **Create identity**.\n\n1.  Select **Email address** as the identity type.\n2.  Enter an email address where you can receive emails.\n3.  Click **Create identity**.\n\nYou'll need to log in to the inbox for the email address you entered and verify the SES sending identity by clicking a link. Look for an email with the subject \"Amazon Web Services – Email Address Verification Request\".\n\nBack in AWS, you should now see that the identity status has changed to **verified**. If not, reload the page.\n\nSend a test email to see whether the sending identity works.\n\nClick on **SMTP settings** in the left sidebar, then on **Create SMTP credentials**.\n\nClick **Create**.\n\n#### Configuration details to save\n\nToggle **Show User SMTP Security Credentials**, then copy and save the SMTP username and password. You won't have access to the password again after this step, so keep it secure.\n\nAfter saving the credentials, click **Close**.\n\n### Create an application in Release\n\nLog in to your Release account, then click on **create new app**.\n\n1.  Enter a name for your Mastodon app, for example, mastodon.\n2.  Pick your forked repository.\n3.  Click **Next step**.\n\n![Pick your repository in Release](/blog-images/bb50a403d5b1db5f44469f63fa0a44eb.png)\n\n_Pick your repository in Release_\n\n### Pick your services in Release\n\n1.  Select **Analyze the repository**\n2.  Select the branch you would like to run.\n3.  Select the root docker-compose.yml file.\n4.  Click **Start Analysis**.\n\n![Analyze your repository in Release](/blog-images/9ec0311f450da8a01b90ac5b2a50edc0.png)\n\n_Analyze your repository in Release_\n\nRelease will now find services in your docker-compose.yml file.\n\nPick only the following services:\n\n- sidekiq\n- streaming\n- web\n\n![Pick services in Release](/blog-images/dafb0cefab0bed1f4964fb7e66c5f088.png)\n\n_Pick services in Release_\n\nClick **Next Step**.\n\n### Edit the generated template\n\nRelease generates a template from the services we selected in the previous step.\n\nThis template is a YAML file that Release uses to generate new environments. It specifies the services, ingress rules, and workflows required to set up your app.\n\n#### Double-check the template context and domain\n\nMake sure the context in your application template matches your Release cluster's context and that the domain matches the verified domain you'd like to use for this application.\n\n#### Replace hostnames with rules\n\nRelease automatically generates hostname templates for services that have node ports and adds them to a hostnames section in your Application Template.\n\nOn a Mastodon server, the public services web and streaming share a hostname but are served at different paths.\n\nRelease configures a Kubernetes ingress controller to route traffic to your applications, based on the hostnames or rules settings, but only one of these settings can be used per environment. We'll use only rules.\n\nUpdate the template to **replace** hostnames with the following rules:\n\n```docker\n\nrules:\n- service: web\n  hostnames:\n  - mastodon-${env_id}.${domain}\n  path: \"/\"\n- service: streaming\n  hostnames:\n  - mastodon-${env_id}.${domain}\n  path: \"/api/v1/streaming\"\n\n```\n\nThe changes should look like this:\n\n```docker\n- hostnames:\n-   streaming: streaming-mastodon-${env_id}.${domain}\n-   web: web-mastodon-${env_id}.${domain}\n+ rules:\n+ - service: web\n+   hostnames:\n+   - mastodon-${env_id}.${domain}\n+   path: \"/\"\n+ - service: streaming\n+   hostnames:\n+   - mastodon-${env_id}.${domain}\n+   path: \"/api/v1/streaming\"\n\n```\n\nThis instructs Release to create two Nginx location blocks on the Kubernetes ingress controller for your application, to direct requests to either web or streaming, depending on the path in the request.\n\n### Update the readiness_probe for Sidekiq\n\nIn the sidekiq service, replace the readiness_probe with the following:\n\n```docker\n\n  readiness_probe:\n    exec:\n      command:\n      - bash\n      - \"-c\"\n      - ps aux | grep '[s]idekiq\\ 6' || false\n    period_seconds: 30\n    timeout_seconds: 30\n    failure_threshold: 3\n    initial_delay_seconds: 10\n\n```\n\n### Update the readiness_probe for the webserver\n\nIn the web service, replace the readiness_probe with the following:\n\n```docker\n\n  readiness_probe:\n    http_get:\n      path: \"/health\"\n      port: 3000\n    initial_delay_seconds: 20\n    period_seconds: 30\n    timeout_seconds: 30\n    failure_threshold: 3\n\n```\n\n### Update the readiness_probe for the streaming server\n\nIn the streaming server, replace the readiness_probe with the following:\n\n```docker\n\n  readiness_probe:\n    http_get:\n      path: \"/api/v1/streaming/health\"\n      port: 4000\n    initial_delay_seconds: 20\n    period_seconds: 30\n    timeout_seconds: 30\n    failure_threshold: 3\n\n```\n\n### Add database migrations as jobs\n\nRelease can run jobs in your environments to handle once-off or repeat tasks. These can be triggered by steps in an environment's workflows.\n\nFor example, when Release first creates an environment for Mastodon, a db:setup job needs to be run to create Mastodon's database schema. Release calls this stage in the workflow setup.\n\nMastodon also needs to run database migrations during version upgrades, so that the database schema is updated along with the code. We can use Release's patch stage for this job.\n\nDuring version upgrades, Mastodon splits pre-deployment and post-deployment database migrations. We'll add two migration jobs, dbmigratepre and dbmigrate.\n\nAdd the following jobs to your template:\n\n```docker\n\njobs:\n- name: dbmigratepre\n  command:\n  - \"/usr/bin/tini\"\n  - \"--\"\n  args:\n  - bash\n  - \"-c\"\n  - rails db:version && export SKIP_POST_DEPLOYMENT_MIGRATIONS=true && rails db:migrate || rails db:setup\n  from_services: web\n- name: dbmigrate\n  command:\n  - \"/usr/bin/tini\"\n  - \"--\"\n  args:\n  - bash\n  - \"-c\"\n  - rails db:version && rails db:migrate || rails db:setup\n  from_services: web\n\n```\n\n### Add an admin user as a job\n\nAfter our services are live and the database is ready, we'll use the Mastodon CLI to create an admin user.\n\nAdd the following job to the jobs you created earlier:\n\n```docker\n- name: mastodonuser\n  command:\n  - \"/usr/bin/tini\"\n  - \"--\"\n  args:\n  - bash\n  - \"-c\"\n  - tootctl accounts create $MASTODON_OWNER_USERNAME --email $MASTODON_OWNER_EMAIL --role Owner\n  from_services: web\n\n```\n\n### Add jobs to workflows\n\nLet's update the workflows section to run these jobs during setup and patch stages.\n\nBecause the order in which our new jobs are run is important, we'll update the parallelize sections in these stages to create new steps.\n\nReplace the workflows section of the template with the following:\n\n```docker\n\nworkflows:\n- name: setup\n  parallelize:\n  - step: dbmigratepre\n    tasks:\n    - jobs.dbmigratepre\n  - step: services\n    tasks:\n    - services.sidekiq\n    - services.streaming\n    - services.web\n  - step: dbmigrate\n    tasks:\n    - jobs.dbmigrate\n  - step: mastodonuser\n    tasks:\n    - jobs.mastodonuser\n- name: patch\n  parallelize:\n  - step: dbmigratepre\n    tasks:\n    - jobs.dbmigratepre\n  - step: services\n    tasks:\n    - services.sidekiq\n    - services.streaming\n    - services.web\n  - step: dbmigrate\n    tasks:\n    - jobs.dbmigrate\n- name: teardown\n  parallelize:\n  - step: remove_environment\n    tasks:\n    - release.remove_environment\n\n```\n\n### Save your changes to the template\n\nClick **Next Step** to save your template, and move on to your environment variables.\n\n### Edit the default environment variables\n\nFor this step, you'll need to retrieve the following information that we saved previously:\n\n#### Generated on your local machine using Docker\n\n- SECRET_KEY_BASE\n- OTP_SECRET\n- VAPID_PRIVATE_KEY\n- VAPID_PUBLIC_KEY\n\n#### PostgreSQL endpoint and password from Amazon RDS\n\n- DB_HOST\n- DB_PASS\n\n#### Redis cluster endpoint from Amazon ElastiCache\n\n- REDIS_HOST\n\n#### SMTP credentials from Amazon Simple Email Service\n\n- SMTP_LOGIN\n- SMTP_PASSWORD\n- SMTP_SERVER: Get the SES endpoint for your region from [AWS docs](https://docs.aws.amazon.com/general/latest/gr/ses.html)\n- SMTP_FROM_ADDRESS: This is the verified email address Mastodon will send emails from\n\n### S3 bucket details and IAM access keys\n\n- S3_BUCKET\n- AWS_ACCESS_KEY_ID\n- AWS_SECRET_ACCESS_KEY\n- S3_REGION\n\n### Mastodon user details\n\n- MASTODON_OWNER_USERNAME: Some names, such as admin, owner, and user, are reserved. Use something unique.\n- MASTODON_OWNER_EMAIL: Use an email address you can receive your sign-up email at. Keep in mind that Amazon SES can only send messages to verified email addresses while in sandbox mode.\n\nIn Release, paste the following environment variables file in the default environment variables YAML editor, then update any values that are empty with the values we saved previously.\n\n```yaml\n---\ndefaults:\n  - key: SINGLE_USER_MODE\n    value: true\n  - key: SECRET_KEY_BASE\n    value:\n    secret: true\n  - key: OTP_SECRET\n    value:\n    secret: true\n  - key: VAPID_PRIVATE_KEY\n    value:\n    secret: true\n  - key: VAPID_PUBLIC_KEY\n    value:\n  - key: DB_HOST\n    value:\n  - key: DB_PORT\n    value: 5432\n  - key: DB_USER\n    value: postgres\n  - key: DB_PASS\n    value:\n    secret: true\n  - key: REDIS_HOST\n    value:\n  - key: REDIS_PORT\n    value: 6379\n  - key: SMTP_SERVER\n    value:\n  - key: SMTP_PORT\n    value: 587\n  - key: SMTP_LOGIN\n    value:\n  - key: SMTP_PASSWORD\n    value:\n    secret: true\n  - key: SMTP_AUTH_METHOD\n    value: plain\n  - key: SMTP_OPENSSL_VERIFY_MODE\n    value: none\n  - key: SMTP_FROM_ADDRESS\n    value:\n  - key: S3_ENABLED\n    value: true\n  - key: S3_BUCKET\n    value: mastodon-media-example\n  - key: AWS_ACCESS_KEY_ID\n    value:\n  - key: AWS_SECRET_ACCESS_KEY\n    value:\n    secret: true\n  - key: S3_REGION\n    value:\n  - key: S3_PROTOCOL\n    value: https\n  # MASTODON_OWNER_USERNAME can't be admin, owner, user\n  - key: MASTODON_OWNER_USERNAME\n    value:\n  - key: MASTODON_OWNER_EMAIL\n    value:\nservices:\n  sidekiq: []\n  streaming: []\n  web: []\nmapping:\n  LOCAL_DOMAIN: WEB_INGRESS_HOST\n  S3_HOSTNAME: s3-${S3_REGION}.amazonaws.com\n  DB_NAME: mastodon_prod_${RELEASE_RANDOMNESS}\n```\n\nClick **Next Step** to save your default environment variables.\n\n### Deploy the app\n\nYou can skip over the **build arguments** step because we're not adding any build arguments to this application.\n\n![Screenshot of app deploy button](/blog-images/3e2d9f50bb3b9f18a6acbfc354b31c4d.png)\n\nDeploy your app\n\nFinally, click **Deploy your app!**\n\n### Follow the deployment\n\nRelease will now build Docker images for our services and deploy an environment.\n\n![Screenshot of deployment](/blog-images/5dd18dad5f93baba80a1c4392e739b92.png)\n\nDeployment progress\n\nThe deployment information page shows all of the steps involved, from building the Docker images, through database migrations, and finally adding your admin user.\n\nClick the mastodonuser job step to reveal the output, and copy the password Mastodon generated for your user.\n\n![Screenshot of step showing user password](/blog-images/885332524cf51d1af59184b2bce6dc6b.png)\n\nView user password\n\nIf you miss this password, you can use the Mastodon web interface to request a password reset.\n\n### Log in and start posting\n\nWith your Mastodon instance up and running, log in and start posting.\n\n![Screenshot of environment info page, showing the web URL](/blog-images/cf41249f4d65a690662ad9910727daca.png)\n\n_environment info page, showing the web URL_\n\nYou can find the public URL for your instance by navigating to the Environment Info page in Release. Click the web URL in the right sidebar to visit your instance.\n\n![Screenshot of Mastodon](/blog-images/35a7506285674efeaf8be72abcacab96.png)\n\nMastodon in action\n\n### What to do next: create more environments\n\nRelease really shines when you create environments for different branches in your codebase.\n\nTest this by creating a feature branch with a small change, and follow Release's guide on [creating a new environment](https://docs.releasehub.com/getting-started/create-an-environment) for your new branch.\n\n### Further reading\n\nEven if Mastodon doesn't replace Twitter, it will likely only grow in popularity. If you plan to host a public instance for your organization, you may want to make sure you focus on two important aspects: moderation and scaling.\n\nRunning any community on the internet is notoriously hard to get right. Keeping users, the public, and your servers happy might seem impossible, but many have gone before us and shared their experiences.\n\nHere's a quick overview of recent writing on these topics:\n\n- [Scaling up your server](https://docs.joinmastodon.org/admin/scaling/): The official Mastodon documentation on scaling your server, which might not be as suited to a Kubernetes-hosted instance.\n- [Moderation actions](https://docs.joinmastodon.org/admin/moderation/): The official Mastodon documentation on available moderation tools. These are invaluable if you plan to allow public content on your instance.\n- [Mastodon Server Covenant](https://joinmastodon.org/covenant): A covenant which can also inform your moderation and hosting decisions.\n\n### Learn more about Release\n\nOf course, you aren't limited to hosting Mastodon. If you want to learn more about how Release can help you host your own software or other third party packages, [book a demo.](https://release.com/book-a-demo)\n",
          "code": "var Component=(()=>{var h=Object.create;var s=Object.defineProperty;var p=Object.getOwnPropertyDescriptor;var m=Object.getOwnPropertyNames;var u=Object.getPrototypeOf,g=Object.prototype.hasOwnProperty;var f=(a,e)=>()=>(e||a((e={exports:{}}).exports,e),e.exports),b=(a,e)=>{for(var i in e)s(a,i,{get:e[i],enumerable:!0})},o=(a,e,i,t)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let r of m(e))!g.call(a,r)&&r!==i&&s(a,r,{get:()=>e[r],enumerable:!(t=p(e,r))||t.enumerable});return a};var y=(a,e,i)=>(i=a!=null?h(u(a)):{},o(e||!a||!a.__esModule?s(i,\"default\",{value:a,enumerable:!0}):i,a)),k=a=>o(s({},\"__esModule\",{value:!0}),a);var c=f((R,l)=>{l.exports=_jsx_runtime});var _={};b(_,{default:()=>S,frontmatter:()=>v});var n=y(c()),v={title:\"Learn how to host your own Mastodon instance on AWS\",summary:\"A guide on how to self-host Mastodon on Release. You'll set up a web server, storage and everything else you need.\",publishDate:\"Fri Dec 16 2022 12:17:26 GMT+0000 (Coordinated Universal Time)\",author:\"\",readingTime:20,categories:[\"kubernetes\",\"platform-engineering\"],mainImage:\"/blog-images/26c0878254b3e4a8eb06fe9f679beb4e.jpg\",imageAlt:\"an elephant with a tusk\",showCTA:!0,ctaCopy:\"Unlock the power of hosting and scaling Mastodon instances on AWS with Release's seamless Kubernetes clusters. Boost your DevOps workflow now!\",ctaLink:\"https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=hosting-mastodon-on-release-and-aws\",relatedPosts:[\"\"],ogImage:\"/blog-images/26c0878254b3e4a8eb06fe9f679beb4e.jpg\",excerpt:\"A guide on how to self-host Mastodon on Release. You'll set up a web server, storage and everything else you need.\",tags:[\"kubernetes\",\"platform-engineering\"],ctaButton:\"Try Release for Free\"};function d(a){let e=Object.assign({h3:\"h3\",a:\"a\",span:\"span\",p:\"p\",em:\"em\",img:\"img\",ol:\"ol\",li:\"li\",strong:\"strong\",ul:\"ul\",h4:\"h4\",pre:\"pre\",code:\"code\"},a.components);return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsxs)(e.h3,{id:\"mastodon-development-environments-on-release-and-aws\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#mastodon-development-environments-on-release-and-aws\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Mastodon development environments on Release and AWS\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Twitter users are flying the coop in droves and many are turning to Mastodon, an open-source federated social network.\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"Unlike Twitter's centralized architecture, a federated social network consists of thousands of individual social networks, called \",(0,n.jsx)(e.em,{children:\"instances\"}),\", each with its own users.\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Federation is sometimes described as being similar to email in the sense that email users can send each other messages, even if they have different email hosts and use different domains.\"}),`\n`,(0,n.jsx)(e.p,{children:\"Mastodon instances fetch content from other instances, which allows a user on one server to follow a user on a different server, thereby creating one big network.\"}),`\n`,(0,n.jsx)(e.p,{children:\"To the DevOps-minded among us, the most compelling thing about the fastest-growing Twitter alternative is figuring out how to host and scale this exciting new platform.\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"At Release, we enable teams to create development, staging, and production environments for their apps. If you intend to add new features or otherwise modify your Mastodon instances, Release can provide the environment you need to move your project forward. Whether you want to just take Mastodon for a spin to see what it's all about, or run a production Mastodon instance that can scale up as you need, \",(0,n.jsx)(e.a,{href:\"https://release.com/book-a-demo\",children:\"using Release\"}),\" means you don't have to trade off simplicity for power. You can easily collaborate with your colleagues, preview the latest features, and upgrade to the latest builds.\"]}),`\n`,(0,n.jsx)(e.p,{children:\"In this post, we'll put Mastodon's welcoming community and complex federation protocols aside so that we can focus on the nuts and bolts of hosting a Mastodon instance.\"}),`\n`,(0,n.jsxs)(e.h3,{id:\"what-makes-a-mastodon-instance\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#what-makes-a-mastodon-instance\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"What makes a Mastodon instance\"]}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/b500f3959bc54b57ac9455a8c5a21d0b.png\",alt:\"\"})}),`\n`,(0,n.jsx)(e.p,{children:\"Mastodon Services Diagram\"}),`\n`,(0,n.jsx)(e.p,{children:\"A Mastodon instance consists of seven services:\"}),`\n`,(0,n.jsxs)(e.ol,{children:[`\n`,(0,n.jsxs)(e.li,{children:[\"A \",(0,n.jsx)(e.strong,{children:\"web server\"}),\", which runs on Ruby on Rails.\"]}),`\n`,(0,n.jsxs)(e.li,{children:[\"A \",(0,n.jsx)(e.strong,{children:\"streaming server\"}),\" that enables the instance's local users to send and receive real-time updates to the server.\"]}),`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"Sidekiq\"}),\", a job scheduler that runs background tasks, polls other instances' web servers, and sends data to other instances' web servers.\"]}),`\n`,(0,n.jsxs)(e.li,{children:[\"A \",(0,n.jsx)(e.strong,{children:\"Redis server\"}),\" that stores jobs for Sidekiq and caches data such as feeds for local users.\"]}),`\n`,(0,n.jsxs)(e.li,{children:[\"A \",(0,n.jsx)(e.strong,{children:\"PostgreSQL database\"}),\" that stores posts, user profiles, and instance settings.\"]}),`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"File storage\"}),\" to store and serve media such as images and videos.\"]}),`\n`]}),`\n`,(0,n.jsxs)(e.p,{children:[\"An \",(0,n.jsx)(e.strong,{children:\"SMTP email server\"}),\" to send messages to local users' email accounts.\"]}),`\n`,(0,n.jsxs)(e.h3,{id:\"how-well-run-a-mastodon-instance-on-release-and-aws\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#how-well-run-a-mastodon-instance-on-release-and-aws\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"How we'll run a Mastodon instance on Release and AWS\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Release creates Kubernetes clusters in your AWS account.\"}),`\n`,(0,n.jsx)(e.p,{children:\"Although all seven of Mastodon's services could be hosted on Kubernetes, we'll use a few AWS-provided services to host supporting systems.\"}),`\n`,(0,n.jsx)(e.p,{children:\"We'll create containers to run Mastodon's web server, streaming server, and Sidekiq in Kubernetes.\"}),`\n`,(0,n.jsx)(e.p,{children:\"From AWS, we'll use:\"}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsx)(e.li,{children:\"Amazon ElastiCache for Redis\"}),`\n`,(0,n.jsx)(e.li,{children:\"Amazon RDS to host a PostgreSQL server\"}),`\n`,(0,n.jsx)(e.li,{children:\"Amazon S3 as a file store\"}),`\n`,(0,n.jsx)(e.li,{children:\"Amazon SES to send emails to local users\"}),`\n`]}),`\n`,(0,n.jsxs)(e.h3,{id:\"prepare-your-release-account\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#prepare-your-release-account\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.strong,{children:\"Prepare your Release account\"})]}),`\n`,(0,n.jsxs)(e.p,{children:[\"Before we get started, you'll need to \",(0,n.jsx)(e.a,{href:\"https://docs.releasehub.com/getting-started/create-an-account\",children:\"create a Release account\"}),\" and upgrade to a paid account.\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"Next, \",(0,n.jsx)(e.a,{href:\"https://docs.releasehub.com/guides-and-examples/domains-and-dns/external-dns\",children:\"verify your domain\"}),\" in Release, and \",(0,n.jsx)(e.a,{href:\"https://docs.releasehub.com/guides-and-examples/advanced-guides/create-a-cluster\",children:\"create a cluster\"}),\" using the AWS integration.\"]}),`\n`,(0,n.jsxs)(e.h4,{id:\"configuration-details-to-save\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#configuration-details-to-save\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Configuration details to save\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"In this guide, configuration details to note down will be pointed out with the heading: \",(0,n.jsx)(e.strong,{children:\"Configuration details to save\"}),\".\"]}),`\n`,(0,n.jsx)(e.p,{children:\"While in Release, navigate to your cluster, and note down the following:\"}),`\n`,(0,n.jsxs)(e.ol,{children:[`\n`,(0,n.jsxs)(e.li,{children:[\"The \",(0,n.jsx)(e.strong,{children:\"cluster context\"}),\".\"]}),`\n`,(0,n.jsxs)(e.li,{children:[\"Your \",(0,n.jsx)(e.strong,{children:\"cluster region\"}),\".\"]}),`\n`]}),`\n`,(0,n.jsxs)(e.p,{children:[\"Your \",(0,n.jsx)(e.strong,{children:\"node group\"}),\".\"]}),`\n`,(0,n.jsxs)(e.h3,{id:\"prepare-your-local-machine\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#prepare-your-local-machine\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Prepare your local machine\"]}),`\n`,(0,n.jsx)(e.p,{children:\"On your local system, you'll need to install Git and Docker.\"}),`\n`,(0,n.jsx)(e.p,{children:\"Keep a text editor handy to take notes. We'll gather configuration details as we go along, and saving these in a central file will make setting up your environment much easier later.\"}),`\n`,(0,n.jsxs)(e.h3,{id:\"fork-the-mastodon-repository-on-github\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#fork-the-mastodon-repository-on-github\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Fork the Mastodon repository on GitHub\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Fork the Mastodon repository from GitHub to your GitHub, Bitbucket, or GitLab account. We'll use GitHub in this guide.\"}),`\n`,(0,n.jsxs)(e.ol,{children:[`\n`,(0,n.jsx)(e.li,{children:\"Log into your GitHub account.\"}),`\n`,(0,n.jsxs)(e.li,{children:[\"Fork the \",(0,n.jsx)(e.a,{href:\"https://github.com/mastodon/mastodon/fork\",children:\"Mastodon\"}),\" repository.\"]}),`\n`]}),`\n`,(0,n.jsx)(e.p,{children:\"GitHub creates a public fork by default. Since Mastodon is AGPL-licensed, keeping your fork public is a good way to make sure you adhere to licensing requirements from the start.\"}),`\n`,(0,n.jsxs)(e.h4,{id:\"configuration-details-to-save-1\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#configuration-details-to-save-1\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Configuration details to save\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"Make a note of your \",(0,n.jsx)(e.strong,{children:\"repository name\"}),\" from GitHub.\"]}),`\n`,(0,n.jsxs)(e.h3,{id:\"clone-your-mastodon-fork-to-your-local-machine\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#clone-your-mastodon-fork-to-your-local-machine\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Clone your Mastodon fork to your local machine\"]}),`\n`,(0,n.jsxs)(e.ol,{children:[`\n`,(0,n.jsxs)(e.li,{children:[\"On the main page of your repository on GitHub.com, click the \",(0,n.jsx)(e.strong,{children:\"Code\"}),\" button\"]}),`\n`,(0,n.jsx)(e.li,{children:\"Copy the URL for the repository.\"}),`\n`,(0,n.jsx)(e.li,{children:\"Open your terminal and navigate to where you want to clone your repository.\"}),`\n`,(0,n.jsx)(e.li,{children:\"Clone your repository using the URL you copied\"}),`\n`]}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-docker\",children:`\ngit clone git@github.com:YOUR-USERNAME/mastodon.git\n\n`})}),`\n`,(0,n.jsxs)(e.ol,{start:\"5\",children:[`\n`,(0,n.jsx)(e.li,{children:\"Change your current working folder to the new repository and keep your terminal open to use again later.\"}),`\n`]}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-docker\",children:`\ncd mastodon\n\n`})}),`\n`,(0,n.jsxs)(e.h3,{id:\"generate-mastodon-secret-keys-and-vapid-keys\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#generate-mastodon-secret-keys-and-vapid-keys\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.strong,{children:\"Generate Mastodon secret keys and VAPID keys\"})]}),`\n`,(0,n.jsx)(e.p,{children:\"Mastodon needs three secret keys and one public key to run. We'll generate these using the tootsuite/mastodon Docker image on our local machine.\"}),`\n`,(0,n.jsxs)(e.ol,{children:[`\n`,(0,n.jsx)(e.li,{children:\"Set a local environment variable SECRET_KEY_BASE. In your terminal, run:\"}),`\n`]}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-docker\",children:`\nSECRET_KEY_BASE=$(docker run --rm -it tootsuite/mastodon:v4.0.2 bin/rake secret)\n\n`})}),`\n`,(0,n.jsxs)(e.ol,{start:\"2\",children:[`\n`,(0,n.jsx)(e.li,{children:\"Set a local environment variable OTP_SECRET, by running the following in your terminal:\"}),`\n`]}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-docker\",children:`\nOTP_SECRET=$(docker run --rm -it tootsuite/mastodon:v4.0.2 bin/rake secret)\n\n`})}),`\n`,(0,n.jsxs)(e.ol,{start:\"3\",children:[`\n`,(0,n.jsx)(e.li,{children:\"Generate VAPID keys, using the SECRET_KEY_BASE and OTP_SECRET as inputs, and print our new keys to the terminal.\"}),`\n`]}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-docker\",children:`\ndocker run --rm \\\\\n   -e OTP_SECRET=$OTP_SECRET \\\\\n   -e SECRET_KEY_BASE=$SECRET_KEY_BASE \\\\\n   -it tootsuite/mastodon:v4.0.2 \\\\\n   bin/rake mastodon:webpush:generate_vapid_key && \\\\\n   echo \"SECRET_KEY_BASE=$SECRET_KEY_BASE\" && \\\\\n   echo \"OTP_SECRET=$OTP_SECRET\"\n`})}),`\n`,(0,n.jsxs)(e.h3,{id:\"configuration-details-to-save-2\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#configuration-details-to-save-2\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.strong,{children:\"Configuration details to save\"})]}),`\n`,(0,n.jsx)(e.p,{children:\"Copy and save VAPID_PRIVATE_KEY, VAPID_PUBLIC_KEY, SECRET_KEY_BASE, and OTP_SECRET values from the output, which should look something like this:\"}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-docker\",children:`\nVAPID_PRIVATE_KEY=cDpok1oPz1u6jpP2fE_Vf2TWBy-VVHh0n3KqdCEz81A=\nVAPID_PUBLIC_KEY=BAb1gkLWzalGfAZq_7IeGX19T1Rp4I5aIerN_sDfon5eenIEn9DAWU1LLpFSu6VjtnhJJilbZXLBBdUSa6DL74Y=\nSECRET_KEY_BASE=bb0231c8e07870b70934a9487cd6e796bbd6f4fe086dfa9039be3743a96e18726ea168cde3cf3ea823c5214e09b8afb7696324eb024f4317cb2626e0545aac12\nOTP_SECRET=19a4d2e15d4ffa63c5ad08d716f33b2296419a6529a49908bd4f879891710d1f9efc1007efc9c18e82aeb52cba763cbd89b030ed2069be8464d9f26d167ea102\n`})}),`\n`,(0,n.jsxs)(e.h3,{id:\"aws-step-1-create-a-postgresql-database-using-amazon-rds\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#aws-step-1-create-a-postgresql-database-using-amazon-rds\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.strong,{children:\"AWS Step 1: Create a PostgreSQL database using Amazon RDS\"})]}),`\n`,(0,n.jsx)(e.p,{children:\"Log in to your AWS account and navigate to RDS.\"}),`\n`,(0,n.jsx)(e.p,{children:\"Make sure your AWS region is set to the same region as your Release cluster.\"}),`\n`,(0,n.jsx)(e.p,{children:\"Create a new PostgreSQL 14 database:\"}),`\n`,(0,n.jsxs)(e.ol,{children:[`\n`,(0,n.jsxs)(e.li,{children:[\"Click on \",(0,n.jsx)(e.strong,{children:\"create database\"}),\".\"]}),`\n`]}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/a8f7af65e782a9b482b6edf9e806694b.png\",alt:\"RDS create database\"})}),`\n`,(0,n.jsxs)(e.p,{children:[(0,n.jsx)(e.em,{children:\"RDS create database\"}),\" 2. Select \",(0,n.jsx)(e.strong,{children:\"standard create\"}),\". 3. Select PostgreSQL as the engine type. 4. Pick PostgreSQL 14.5-R1 as the engine version.\"]}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/657c13cc97a0c58231008fbc90e3bd39.png\",alt:\"RDS create database steps 2 to 4\"})}),`\n`,(0,n.jsxs)(e.p,{children:[(0,n.jsx)(e.em,{children:\"RDS create database steps 2 to 4\"}),\" 5. Select \",(0,n.jsx)(e.strong,{children:\"Free tier\"}),\" if you're only setting up a small/test instance. 6. Set the \",(0,n.jsx)(e.em,{children:\"DB instance identifier\"}),\" to mastodon-1. We'll need to refer to this again later.\"]}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/9e77fd3763b23a49be6d22ab0586cfc1.png\",alt:\"RDS create database steps 5 to 6\"})}),`\n`,(0,n.jsxs)(e.p,{children:[(0,n.jsx)(e.em,{children:\"RDS create database steps 5 to 6\"}),\" 7. To make sure you don't use more space than needed, change the \",(0,n.jsx)(e.em,{children:\"allocated storage\"}),\" to 20 GiB. 8. Disable autoscaling. 9. Under the \",(0,n.jsx)(e.strong,{children:\"connectivity\"}),\" section, select the virtual private cloud (VPC) that Release created for your cluster. This will enable services running in the cluster to reach your new database.\"]}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/aa25b0121074a8bf5953f1bf547ee4da.png\",alt:\"RDS create database steps 7 to 8\"})}),`\n`,(0,n.jsxs)(e.p,{children:[(0,n.jsx)(e.em,{children:\"RDS create database steps 7 to 8\"}),\" 10. Choose to allow an existing security group. 11. Under \",(0,n.jsx)(e.strong,{children:\"Existing VPC security groups\"}),\", pick the security group that starts with eks-cluster-sg- followed by your cluster context, then close the dropdown.\"]}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/cb7eb42499a8c3e28726ddb6b552c6cd.png\",alt:\"RDS create database steps 8 to 10\"})}),`\n`,(0,n.jsxs)(e.p,{children:[(0,n.jsx)(e.em,{children:\"RDS create database steps 9 to 11\"}),\" 12. Click \",(0,n.jsx)(e.strong,{children:\"Create database\"}),\".\"]}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/5689429d0212e384a6198ff37a1ac92a.png\",alt:\"RDS create database steps 8 to 10\"})}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.em,{children:\"RDS create database step 12\"})}),`\n`,(0,n.jsx)(e.p,{children:\"Your database will take a few minutes to launch.\"}),`\n`,(0,n.jsxs)(e.h4,{id:\"configuration-details-to-save-3\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#configuration-details-to-save-3\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Configuration details to save\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"Once the database is ready, click on \",(0,n.jsx)(e.strong,{children:\"view connection details\"}),\" and note down the \",(0,n.jsx)(e.strong,{children:\"database endpoint\"}),\" and \",(0,n.jsx)(e.strong,{children:\"database password\"}),\".\"]}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/c0b4990b5c39223beec5652e543826a8.png\",alt:\"Screenshot of RDS PostgreSQL configuration details\"})}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.em,{children:\"RDS PostgreSQL configuration details\"})}),`\n`,(0,n.jsxs)(e.h3,{id:\"aws-step-2-create-a-redis-cache-using-amazon-elasticache\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#aws-step-2-create-a-redis-cache-using-amazon-elasticache\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"AWS Step 2: Create a Redis cache using Amazon ElastiCache\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Before we create a Redis cache, we'll need to get the VPC ID for our Release cluster (Amazon ElastiCache does not show the full VPC name when creating a Redis cluster).\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"In AWS, navigate to \",(0,n.jsx)(e.strong,{children:\"VPC\"}),\", then note down the VPC ID for your Release cluster.\"]}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/a415ad14fc62b594fce757cb13b7a55e.png\",alt:\"Screenshot of Amazon VPC\"})}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.em,{children:\"Amazon VPC\"})}),`\n`,(0,n.jsx)(e.p,{children:\"With the VPC ID at hand, we can create a Redis cluster. In your AWS account, navigate to ElastiCache.\"}),`\n`,(0,n.jsxs)(e.ol,{children:[`\n`,(0,n.jsxs)(e.li,{children:[\"Click on \",(0,n.jsx)(e.strong,{children:\"create cluster\"}),\".\"]}),`\n`,(0,n.jsxs)(e.li,{children:[\"Select \",(0,n.jsx)(e.strong,{children:\"create Redis cluster\"}),\".\"]}),`\n`]}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/073994066b09d353fb7e1700275ab2e4.png\",alt:\"Screenshot of ElastiCache\"})}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.em,{children:\"ElastiCache\"})}),`\n`,(0,n.jsxs)(e.p,{children:[\"This opens up the \",(0,n.jsx)(e.strong,{children:\"cluster settings\"}),\" page, where we'll configure our Redis cluster.\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"Under \",(0,n.jsx)(e.strong,{children:\"cluster info\"}),\", set a name for this Redis cluster, for example, mastodon-redis.\"]}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/eaba3a434d845fe7e5135f7dde97fd89.png\",alt:\"Screenshot of Redis configuration section\"})}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.em,{children:\"Redis configuration section\"})}),`\n`,(0,n.jsxs)(e.p,{children:[\"In the \",(0,n.jsx)(e.strong,{children:\"connectivity\"}),\" section:\"]}),`\n`,(0,n.jsxs)(e.ol,{children:[`\n`,(0,n.jsx)(e.li,{children:\"Set a name for the Redis cluster's subnet, for example, mastodon-redis-subnet.\"}),`\n`,(0,n.jsxs)(e.li,{children:[\"Select your Release cluster's \",(0,n.jsx)(e.strong,{children:\"VPC ID\"}),\" from the dropdown.\"]}),`\n`]}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/c9e88c18c4248cb0849aa8eabc18332c.png\",alt:\"Screenshot of Redis connectivity section\"})}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.em,{children:\"Redis connectivity section\"})}),`\n`,(0,n.jsxs)(e.p,{children:[\"Click \",(0,n.jsx)(e.strong,{children:\"Next\"}),\".\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"Under \",(0,n.jsx)(e.strong,{children:\"selected security groups\"}),\", click \",(0,n.jsx)(e.strong,{children:\"Manage\"}),\".\"]}),`\n`,(0,n.jsxs)(e.ol,{children:[`\n`,(0,n.jsx)(e.li,{children:\"Filter the security groups to find one that starts with eks-cluster-sg-.\"}),`\n`,(0,n.jsx)(e.li,{children:\"Select the security group that starts with eks-cluster-sg- followed by your Release cluster's context.\"}),`\n`,(0,n.jsxs)(e.li,{children:[\"Click \",(0,n.jsx)(e.strong,{children:\"choose\"}),\".\"]}),`\n`]}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/799f15d8d6037eb172573272b40a6f99.png\",alt:\"Screenshot of Redis advanced settings page\"})}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.em,{children:\"Screenshot of Redis advanced settings page\"})}),`\n`,(0,n.jsxs)(e.p,{children:[\"Scroll down to the bottom of the page and click \",(0,n.jsx)(e.strong,{children:\"next\"}),\".\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"Scroll down the page to review your Redis settings, then click on \",(0,n.jsx)(e.strong,{children:\"create\"}),\".\"]}),`\n`,(0,n.jsx)(e.p,{children:\"AWS will take a moment to create your Redis cluster.\"}),`\n`,(0,n.jsxs)(e.h4,{id:\"configuration-details-to-save-4\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#configuration-details-to-save-4\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Configuration details to save\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Open the Redis cluster's details and copy the Redis cluster's endpoint (you can omit the port number).\"}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/4646a089f3c224805a89809e571aad8e.png\",alt:\"Screenshot of Redis Cluster details\"})}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.em,{children:\"Redis Cluster details\"})}),`\n`,(0,n.jsxs)(e.h3,{id:\"aws-step-3-create-an-amazon-s3-bucket-for-user-media\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#aws-step-3-create-an-amazon-s3-bucket-for-user-media\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"AWS Step 3: Create an Amazon S3 bucket for user media\"]}),`\n`,(0,n.jsx)(e.p,{children:\"In your AWS account, navigate to S3.\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"Click on \",(0,n.jsx)(e.strong,{children:\"create bucket\"}),\".\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Enter a unique bucket name, for example, mastodon-media-example.\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"Uncheck \",(0,n.jsx)(e.strong,{children:\"Block all public access\"}),\".\"]}),`\n`,(0,n.jsx)(e.p,{children:'Check \"I acknowledge that the current settings might result in this bucket and the objects within becoming public\".'}),`\n`,(0,n.jsxs)(e.p,{children:[\"Click \",(0,n.jsx)(e.strong,{children:\"create bucket\"}),\".\"]}),`\n`,(0,n.jsx)(e.p,{children:\"AWS will take a moment to create your new S3 bucket.\"}),`\n`,(0,n.jsxs)(e.h3,{id:\"aws-step-4-create-an-iam-user-to-write-to-s3\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#aws-step-4-create-an-iam-user-to-write-to-s3\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"AWS Step 4: Create an IAM user to write to S3\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"In your AWS account, navigate to \",(0,n.jsx)(e.strong,{children:\"Identity and Access Management (IAM)\"}),\".\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"Click on \",(0,n.jsx)(e.strong,{children:\"Users\"}),\", then on \",(0,n.jsx)(e.strong,{children:\"Add users\"}),\".\"]}),`\n`,(0,n.jsxs)(e.ol,{children:[`\n`,(0,n.jsx)(e.li,{children:\"Enter a username, for example, mastodon-s3-writer.\"}),`\n`,(0,n.jsxs)(e.li,{children:[\"Under \",(0,n.jsx)(e.strong,{children:\"Select AWS credential type\"}),\", select only \",(0,n.jsx)(e.strong,{children:\"Access key - Programmatic access\"}),\".\"]}),`\n`,(0,n.jsxs)(e.li,{children:[\"Click \",(0,n.jsx)(e.strong,{children:\"Next: Permissions\"}),\".\"]}),`\n`]}),`\n`,(0,n.jsxs)(e.p,{children:[\"On the \",(0,n.jsx)(e.strong,{children:\"Set permissions\"}),\" page:\"]}),`\n`,(0,n.jsxs)(e.ol,{children:[`\n`,(0,n.jsxs)(e.li,{children:[\"Select \",(0,n.jsx)(e.strong,{children:\"Attach existing policies directly\"}),\".\"]}),`\n`,(0,n.jsx)(e.li,{children:\"Filter for s3 policies.\"}),`\n`,(0,n.jsxs)(e.li,{children:[\"Check \",(0,n.jsx)(e.strong,{children:\"AmazonS3FullAccess\"}),\".\"]}),`\n`,(0,n.jsxs)(e.li,{children:[\"Click \",(0,n.jsx)(e.strong,{children:\"Next: Tags\"}),\".\"]}),`\n`]}),`\n`,(0,n.jsxs)(e.p,{children:[\"Click \",(0,n.jsx)(e.strong,{children:\"Next: Review\"}),\".\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"Finally, click \",(0,n.jsx)(e.strong,{children:\"Create user\"}),\".\"]}),`\n`,(0,n.jsxs)(e.h4,{id:\"configuration-details-to-save-5\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#configuration-details-to-save-5\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.strong,{children:\"Configuration details to save\"})]}),`\n`,(0,n.jsx)(e.p,{children:\"Note down the access key ID and secret access key. Keep in mind that you won't be able to see this secret again, so you'll need to save it now and keep it secure.\"}),`\n`,(0,n.jsxs)(e.h3,{id:\"aws-step-5-create-an-amazon-ses-identity\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#aws-step-5-create-an-amazon-ses-identity\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"AWS Step 5: Create an Amazon SES identity\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"In AWS, navigate to \",(0,n.jsx)(e.strong,{children:\"Amazon Simple Email Service (SES)\"}),\".\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"Click \",(0,n.jsx)(e.strong,{children:\"Create identity\"}),\".\"]}),`\n`,(0,n.jsxs)(e.ol,{children:[`\n`,(0,n.jsxs)(e.li,{children:[\"Select \",(0,n.jsx)(e.strong,{children:\"Email address\"}),\" as the identity type.\"]}),`\n`,(0,n.jsx)(e.li,{children:\"Enter an email address where you can receive emails.\"}),`\n`,(0,n.jsxs)(e.li,{children:[\"Click \",(0,n.jsx)(e.strong,{children:\"Create identity\"}),\".\"]}),`\n`]}),`\n`,(0,n.jsx)(e.p,{children:`You'll need to log in to the inbox for the email address you entered and verify the SES sending identity by clicking a link. Look for an email with the subject \"Amazon Web Services \\u2013 Email Address Verification Request\".`}),`\n`,(0,n.jsxs)(e.p,{children:[\"Back in AWS, you should now see that the identity status has changed to \",(0,n.jsx)(e.strong,{children:\"verified\"}),\". If not, reload the page.\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Send a test email to see whether the sending identity works.\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"Click on \",(0,n.jsx)(e.strong,{children:\"SMTP settings\"}),\" in the left sidebar, then on \",(0,n.jsx)(e.strong,{children:\"Create SMTP credentials\"}),\".\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"Click \",(0,n.jsx)(e.strong,{children:\"Create\"}),\".\"]}),`\n`,(0,n.jsxs)(e.h4,{id:\"configuration-details-to-save-6\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#configuration-details-to-save-6\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Configuration details to save\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"Toggle \",(0,n.jsx)(e.strong,{children:\"Show User SMTP Security Credentials\"}),\", then copy and save the SMTP username and password. You won't have access to the password again after this step, so keep it secure.\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"After saving the credentials, click \",(0,n.jsx)(e.strong,{children:\"Close\"}),\".\"]}),`\n`,(0,n.jsxs)(e.h3,{id:\"create-an-application-in-release\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#create-an-application-in-release\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Create an application in Release\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"Log in to your Release account, then click on \",(0,n.jsx)(e.strong,{children:\"create new app\"}),\".\"]}),`\n`,(0,n.jsxs)(e.ol,{children:[`\n`,(0,n.jsx)(e.li,{children:\"Enter a name for your Mastodon app, for example, mastodon.\"}),`\n`,(0,n.jsx)(e.li,{children:\"Pick your forked repository.\"}),`\n`,(0,n.jsxs)(e.li,{children:[\"Click \",(0,n.jsx)(e.strong,{children:\"Next step\"}),\".\"]}),`\n`]}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/bb50a403d5b1db5f44469f63fa0a44eb.png\",alt:\"Pick your repository in Release\"})}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.em,{children:\"Pick your repository in Release\"})}),`\n`,(0,n.jsxs)(e.h3,{id:\"pick-your-services-in-release\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#pick-your-services-in-release\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Pick your services in Release\"]}),`\n`,(0,n.jsxs)(e.ol,{children:[`\n`,(0,n.jsxs)(e.li,{children:[\"Select \",(0,n.jsx)(e.strong,{children:\"Analyze the repository\"})]}),`\n`,(0,n.jsx)(e.li,{children:\"Select the branch you would like to run.\"}),`\n`,(0,n.jsx)(e.li,{children:\"Select the root docker-compose.yml file.\"}),`\n`,(0,n.jsxs)(e.li,{children:[\"Click \",(0,n.jsx)(e.strong,{children:\"Start Analysis\"}),\".\"]}),`\n`]}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/9ec0311f450da8a01b90ac5b2a50edc0.png\",alt:\"Analyze your repository in Release\"})}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.em,{children:\"Analyze your repository in Release\"})}),`\n`,(0,n.jsx)(e.p,{children:\"Release will now find services in your docker-compose.yml file.\"}),`\n`,(0,n.jsx)(e.p,{children:\"Pick only the following services:\"}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsx)(e.li,{children:\"sidekiq\"}),`\n`,(0,n.jsx)(e.li,{children:\"streaming\"}),`\n`,(0,n.jsx)(e.li,{children:\"web\"}),`\n`]}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/dafb0cefab0bed1f4964fb7e66c5f088.png\",alt:\"Pick services in Release\"})}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.em,{children:\"Pick services in Release\"})}),`\n`,(0,n.jsxs)(e.p,{children:[\"Click \",(0,n.jsx)(e.strong,{children:\"Next Step\"}),\".\"]}),`\n`,(0,n.jsxs)(e.h3,{id:\"edit-the-generated-template\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#edit-the-generated-template\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Edit the generated template\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Release generates a template from the services we selected in the previous step.\"}),`\n`,(0,n.jsx)(e.p,{children:\"This template is a YAML file that Release uses to generate new environments. It specifies the services, ingress rules, and workflows required to set up your app.\"}),`\n`,(0,n.jsxs)(e.h4,{id:\"double-check-the-template-context-and-domain\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#double-check-the-template-context-and-domain\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Double-check the template context and domain\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Make sure the context in your application template matches your Release cluster's context and that the domain matches the verified domain you'd like to use for this application.\"}),`\n`,(0,n.jsxs)(e.h4,{id:\"replace-hostnames-with-rules\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#replace-hostnames-with-rules\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Replace hostnames with rules\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Release automatically generates hostname templates for services that have node ports and adds them to a hostnames section in your Application Template.\"}),`\n`,(0,n.jsx)(e.p,{children:\"On a Mastodon server, the public services web and streaming share a hostname but are served at different paths.\"}),`\n`,(0,n.jsx)(e.p,{children:\"Release configures a Kubernetes ingress controller to route traffic to your applications, based on the hostnames or rules settings, but only one of these settings can be used per environment. We'll use only rules.\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"Update the template to \",(0,n.jsx)(e.strong,{children:\"replace\"}),\" hostnames with the following rules:\"]}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-docker\",children:`\nrules:\n- service: web\n  hostnames:\n  - mastodon-\\${env_id}.\\${domain}\n  path: \"/\"\n- service: streaming\n  hostnames:\n  - mastodon-\\${env_id}.\\${domain}\n  path: \"/api/v1/streaming\"\n\n`})}),`\n`,(0,n.jsx)(e.p,{children:\"The changes should look like this:\"}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-docker\",children:`- hostnames:\n-   streaming: streaming-mastodon-\\${env_id}.\\${domain}\n-   web: web-mastodon-\\${env_id}.\\${domain}\n+ rules:\n+ - service: web\n+   hostnames:\n+   - mastodon-\\${env_id}.\\${domain}\n+   path: \"/\"\n+ - service: streaming\n+   hostnames:\n+   - mastodon-\\${env_id}.\\${domain}\n+   path: \"/api/v1/streaming\"\n\n`})}),`\n`,(0,n.jsx)(e.p,{children:\"This instructs Release to create two Nginx location blocks on the Kubernetes ingress controller for your application, to direct requests to either web or streaming, depending on the path in the request.\"}),`\n`,(0,n.jsxs)(e.h3,{id:\"update-the-readiness_probe-for-sidekiq\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#update-the-readiness_probe-for-sidekiq\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Update the readiness_probe for Sidekiq\"]}),`\n`,(0,n.jsx)(e.p,{children:\"In the sidekiq service, replace the readiness_probe with the following:\"}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-docker\",children:`\n  readiness_probe:\n    exec:\n      command:\n      - bash\n      - \"-c\"\n      - ps aux | grep '[s]idekiq\\\\ 6' || false\n    period_seconds: 30\n    timeout_seconds: 30\n    failure_threshold: 3\n    initial_delay_seconds: 10\n\n`})}),`\n`,(0,n.jsxs)(e.h3,{id:\"update-the-readiness_probe-for-the-webserver\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#update-the-readiness_probe-for-the-webserver\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Update the readiness_probe for the webserver\"]}),`\n`,(0,n.jsx)(e.p,{children:\"In the web service, replace the readiness_probe with the following:\"}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-docker\",children:`\n  readiness_probe:\n    http_get:\n      path: \"/health\"\n      port: 3000\n    initial_delay_seconds: 20\n    period_seconds: 30\n    timeout_seconds: 30\n    failure_threshold: 3\n\n`})}),`\n`,(0,n.jsxs)(e.h3,{id:\"update-the-readiness_probe-for-the-streaming-server\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#update-the-readiness_probe-for-the-streaming-server\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Update the readiness_probe for the streaming server\"]}),`\n`,(0,n.jsx)(e.p,{children:\"In the streaming server, replace the readiness_probe with the following:\"}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-docker\",children:`\n  readiness_probe:\n    http_get:\n      path: \"/api/v1/streaming/health\"\n      port: 4000\n    initial_delay_seconds: 20\n    period_seconds: 30\n    timeout_seconds: 30\n    failure_threshold: 3\n\n`})}),`\n`,(0,n.jsxs)(e.h3,{id:\"add-database-migrations-as-jobs\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#add-database-migrations-as-jobs\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Add database migrations as jobs\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Release can run jobs in your environments to handle once-off or repeat tasks. These can be triggered by steps in an environment's workflows.\"}),`\n`,(0,n.jsx)(e.p,{children:\"For example, when Release first creates an environment for Mastodon, a db:setup job needs to be run to create Mastodon's database schema. Release calls this stage in the workflow setup.\"}),`\n`,(0,n.jsx)(e.p,{children:\"Mastodon also needs to run database migrations during version upgrades, so that the database schema is updated along with the code. We can use Release's patch stage for this job.\"}),`\n`,(0,n.jsx)(e.p,{children:\"During version upgrades, Mastodon splits pre-deployment and post-deployment database migrations. We'll add two migration jobs, dbmigratepre and dbmigrate.\"}),`\n`,(0,n.jsx)(e.p,{children:\"Add the following jobs to your template:\"}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-docker\",children:`\njobs:\n- name: dbmigratepre\n  command:\n  - \"/usr/bin/tini\"\n  - \"--\"\n  args:\n  - bash\n  - \"-c\"\n  - rails db:version && export SKIP_POST_DEPLOYMENT_MIGRATIONS=true && rails db:migrate || rails db:setup\n  from_services: web\n- name: dbmigrate\n  command:\n  - \"/usr/bin/tini\"\n  - \"--\"\n  args:\n  - bash\n  - \"-c\"\n  - rails db:version && rails db:migrate || rails db:setup\n  from_services: web\n\n`})}),`\n`,(0,n.jsxs)(e.h3,{id:\"add-an-admin-user-as-a-job\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#add-an-admin-user-as-a-job\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Add an admin user as a job\"]}),`\n`,(0,n.jsx)(e.p,{children:\"After our services are live and the database is ready, we'll use the Mastodon CLI to create an admin user.\"}),`\n`,(0,n.jsx)(e.p,{children:\"Add the following job to the jobs you created earlier:\"}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-docker\",children:`- name: mastodonuser\n  command:\n  - \"/usr/bin/tini\"\n  - \"--\"\n  args:\n  - bash\n  - \"-c\"\n  - tootctl accounts create $MASTODON_OWNER_USERNAME --email $MASTODON_OWNER_EMAIL --role Owner\n  from_services: web\n\n`})}),`\n`,(0,n.jsxs)(e.h3,{id:\"add-jobs-to-workflows\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#add-jobs-to-workflows\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Add jobs to workflows\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Let's update the workflows section to run these jobs during setup and patch stages.\"}),`\n`,(0,n.jsx)(e.p,{children:\"Because the order in which our new jobs are run is important, we'll update the parallelize sections in these stages to create new steps.\"}),`\n`,(0,n.jsx)(e.p,{children:\"Replace the workflows section of the template with the following:\"}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-docker\",children:`\nworkflows:\n- name: setup\n  parallelize:\n  - step: dbmigratepre\n    tasks:\n    - jobs.dbmigratepre\n  - step: services\n    tasks:\n    - services.sidekiq\n    - services.streaming\n    - services.web\n  - step: dbmigrate\n    tasks:\n    - jobs.dbmigrate\n  - step: mastodonuser\n    tasks:\n    - jobs.mastodonuser\n- name: patch\n  parallelize:\n  - step: dbmigratepre\n    tasks:\n    - jobs.dbmigratepre\n  - step: services\n    tasks:\n    - services.sidekiq\n    - services.streaming\n    - services.web\n  - step: dbmigrate\n    tasks:\n    - jobs.dbmigrate\n- name: teardown\n  parallelize:\n  - step: remove_environment\n    tasks:\n    - release.remove_environment\n\n`})}),`\n`,(0,n.jsxs)(e.h3,{id:\"save-your-changes-to-the-template\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#save-your-changes-to-the-template\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Save your changes to the template\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"Click \",(0,n.jsx)(e.strong,{children:\"Next Step\"}),\" to save your template, and move on to your environment variables.\"]}),`\n`,(0,n.jsxs)(e.h3,{id:\"edit-the-default-environment-variables\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#edit-the-default-environment-variables\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Edit the default environment variables\"]}),`\n`,(0,n.jsx)(e.p,{children:\"For this step, you'll need to retrieve the following information that we saved previously:\"}),`\n`,(0,n.jsxs)(e.h4,{id:\"generated-on-your-local-machine-using-docker\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#generated-on-your-local-machine-using-docker\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Generated on your local machine using Docker\"]}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsx)(e.li,{children:\"SECRET_KEY_BASE\"}),`\n`,(0,n.jsx)(e.li,{children:\"OTP_SECRET\"}),`\n`,(0,n.jsx)(e.li,{children:\"VAPID_PRIVATE_KEY\"}),`\n`,(0,n.jsx)(e.li,{children:\"VAPID_PUBLIC_KEY\"}),`\n`]}),`\n`,(0,n.jsxs)(e.h4,{id:\"postgresql-endpoint-and-password-from-amazon-rds\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#postgresql-endpoint-and-password-from-amazon-rds\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"PostgreSQL endpoint and password from Amazon RDS\"]}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsx)(e.li,{children:\"DB_HOST\"}),`\n`,(0,n.jsx)(e.li,{children:\"DB_PASS\"}),`\n`]}),`\n`,(0,n.jsxs)(e.h4,{id:\"redis-cluster-endpoint-from-amazon-elasticache\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#redis-cluster-endpoint-from-amazon-elasticache\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Redis cluster endpoint from Amazon ElastiCache\"]}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsx)(e.li,{children:\"REDIS_HOST\"}),`\n`]}),`\n`,(0,n.jsxs)(e.h4,{id:\"smtp-credentials-from-amazon-simple-email-service\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#smtp-credentials-from-amazon-simple-email-service\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"SMTP credentials from Amazon Simple Email Service\"]}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsx)(e.li,{children:\"SMTP_LOGIN\"}),`\n`,(0,n.jsx)(e.li,{children:\"SMTP_PASSWORD\"}),`\n`,(0,n.jsxs)(e.li,{children:[\"SMTP_SERVER: Get the SES endpoint for your region from \",(0,n.jsx)(e.a,{href:\"https://docs.aws.amazon.com/general/latest/gr/ses.html\",children:\"AWS docs\"})]}),`\n`,(0,n.jsx)(e.li,{children:\"SMTP_FROM_ADDRESS: This is the verified email address Mastodon will send emails from\"}),`\n`]}),`\n`,(0,n.jsxs)(e.h3,{id:\"s3-bucket-details-and-iam-access-keys\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#s3-bucket-details-and-iam-access-keys\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"S3 bucket details and IAM access keys\"]}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsx)(e.li,{children:\"S3_BUCKET\"}),`\n`,(0,n.jsx)(e.li,{children:\"AWS_ACCESS_KEY_ID\"}),`\n`,(0,n.jsx)(e.li,{children:\"AWS_SECRET_ACCESS_KEY\"}),`\n`,(0,n.jsx)(e.li,{children:\"S3_REGION\"}),`\n`]}),`\n`,(0,n.jsxs)(e.h3,{id:\"mastodon-user-details\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#mastodon-user-details\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Mastodon user details\"]}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsx)(e.li,{children:\"MASTODON_OWNER_USERNAME: Some names, such as admin, owner, and user, are reserved. Use something unique.\"}),`\n`,(0,n.jsx)(e.li,{children:\"MASTODON_OWNER_EMAIL: Use an email address you can receive your sign-up email at. Keep in mind that Amazon SES can only send messages to verified email addresses while in sandbox mode.\"}),`\n`]}),`\n`,(0,n.jsx)(e.p,{children:\"In Release, paste the following environment variables file in the default environment variables YAML editor, then update any values that are empty with the values we saved previously.\"}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-yaml\",children:`---\ndefaults:\n  - key: SINGLE_USER_MODE\n    value: true\n  - key: SECRET_KEY_BASE\n    value:\n    secret: true\n  - key: OTP_SECRET\n    value:\n    secret: true\n  - key: VAPID_PRIVATE_KEY\n    value:\n    secret: true\n  - key: VAPID_PUBLIC_KEY\n    value:\n  - key: DB_HOST\n    value:\n  - key: DB_PORT\n    value: 5432\n  - key: DB_USER\n    value: postgres\n  - key: DB_PASS\n    value:\n    secret: true\n  - key: REDIS_HOST\n    value:\n  - key: REDIS_PORT\n    value: 6379\n  - key: SMTP_SERVER\n    value:\n  - key: SMTP_PORT\n    value: 587\n  - key: SMTP_LOGIN\n    value:\n  - key: SMTP_PASSWORD\n    value:\n    secret: true\n  - key: SMTP_AUTH_METHOD\n    value: plain\n  - key: SMTP_OPENSSL_VERIFY_MODE\n    value: none\n  - key: SMTP_FROM_ADDRESS\n    value:\n  - key: S3_ENABLED\n    value: true\n  - key: S3_BUCKET\n    value: mastodon-media-example\n  - key: AWS_ACCESS_KEY_ID\n    value:\n  - key: AWS_SECRET_ACCESS_KEY\n    value:\n    secret: true\n  - key: S3_REGION\n    value:\n  - key: S3_PROTOCOL\n    value: https\n  # MASTODON_OWNER_USERNAME can't be admin, owner, user\n  - key: MASTODON_OWNER_USERNAME\n    value:\n  - key: MASTODON_OWNER_EMAIL\n    value:\nservices:\n  sidekiq: []\n  streaming: []\n  web: []\nmapping:\n  LOCAL_DOMAIN: WEB_INGRESS_HOST\n  S3_HOSTNAME: s3-\\${S3_REGION}.amazonaws.com\n  DB_NAME: mastodon_prod_\\${RELEASE_RANDOMNESS}\n`})}),`\n`,(0,n.jsxs)(e.p,{children:[\"Click \",(0,n.jsx)(e.strong,{children:\"Next Step\"}),\" to save your default environment variables.\"]}),`\n`,(0,n.jsxs)(e.h3,{id:\"deploy-the-app\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#deploy-the-app\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Deploy the app\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"You can skip over the \",(0,n.jsx)(e.strong,{children:\"build arguments\"}),\" step because we're not adding any build arguments to this application.\"]}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/3e2d9f50bb3b9f18a6acbfc354b31c4d.png\",alt:\"Screenshot of app deploy button\"})}),`\n`,(0,n.jsx)(e.p,{children:\"Deploy your app\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"Finally, click \",(0,n.jsx)(e.strong,{children:\"Deploy your app!\"})]}),`\n`,(0,n.jsxs)(e.h3,{id:\"follow-the-deployment\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#follow-the-deployment\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Follow the deployment\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Release will now build Docker images for our services and deploy an environment.\"}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/5dd18dad5f93baba80a1c4392e739b92.png\",alt:\"Screenshot of deployment\"})}),`\n`,(0,n.jsx)(e.p,{children:\"Deployment progress\"}),`\n`,(0,n.jsx)(e.p,{children:\"The deployment information page shows all of the steps involved, from building the Docker images, through database migrations, and finally adding your admin user.\"}),`\n`,(0,n.jsx)(e.p,{children:\"Click the mastodonuser job step to reveal the output, and copy the password Mastodon generated for your user.\"}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/885332524cf51d1af59184b2bce6dc6b.png\",alt:\"Screenshot of step showing user password\"})}),`\n`,(0,n.jsx)(e.p,{children:\"View user password\"}),`\n`,(0,n.jsx)(e.p,{children:\"If you miss this password, you can use the Mastodon web interface to request a password reset.\"}),`\n`,(0,n.jsxs)(e.h3,{id:\"log-in-and-start-posting\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#log-in-and-start-posting\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Log in and start posting\"]}),`\n`,(0,n.jsx)(e.p,{children:\"With your Mastodon instance up and running, log in and start posting.\"}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/cf41249f4d65a690662ad9910727daca.png\",alt:\"Screenshot of environment info page, showing the web URL\"})}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.em,{children:\"environment info page, showing the web URL\"})}),`\n`,(0,n.jsx)(e.p,{children:\"You can find the public URL for your instance by navigating to the Environment Info page in Release. Click the web URL in the right sidebar to visit your instance.\"}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/35a7506285674efeaf8be72abcacab96.png\",alt:\"Screenshot of Mastodon\"})}),`\n`,(0,n.jsx)(e.p,{children:\"Mastodon in action\"}),`\n`,(0,n.jsxs)(e.h3,{id:\"what-to-do-next-create-more-environments\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#what-to-do-next-create-more-environments\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"What to do next: create more environments\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Release really shines when you create environments for different branches in your codebase.\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"Test this by creating a feature branch with a small change, and follow Release's guide on \",(0,n.jsx)(e.a,{href:\"https://docs.releasehub.com/getting-started/create-an-environment\",children:\"creating a new environment\"}),\" for your new branch.\"]}),`\n`,(0,n.jsxs)(e.h3,{id:\"further-reading\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#further-reading\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Further reading\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Even if Mastodon doesn't replace Twitter, it will likely only grow in popularity. If you plan to host a public instance for your organization, you may want to make sure you focus on two important aspects: moderation and scaling.\"}),`\n`,(0,n.jsx)(e.p,{children:\"Running any community on the internet is notoriously hard to get right. Keeping users, the public, and your servers happy might seem impossible, but many have gone before us and shared their experiences.\"}),`\n`,(0,n.jsx)(e.p,{children:\"Here's a quick overview of recent writing on these topics:\"}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.a,{href:\"https://docs.joinmastodon.org/admin/scaling/\",children:\"Scaling up your server\"}),\": The official Mastodon documentation on scaling your server, which might not be as suited to a Kubernetes-hosted instance.\"]}),`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.a,{href:\"https://docs.joinmastodon.org/admin/moderation/\",children:\"Moderation actions\"}),\": The official Mastodon documentation on available moderation tools. These are invaluable if you plan to allow public content on your instance.\"]}),`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.a,{href:\"https://joinmastodon.org/covenant\",children:\"Mastodon Server Covenant\"}),\": A covenant which can also inform your moderation and hosting decisions.\"]}),`\n`]}),`\n`,(0,n.jsxs)(e.h3,{id:\"learn-more-about-release\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#learn-more-about-release\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Learn more about Release\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"Of course, you aren't limited to hosting Mastodon. If you want to learn more about how Release can help you host your own software or other third party packages, \",(0,n.jsx)(e.a,{href:\"https://release.com/book-a-demo\",children:\"book a demo.\"})]})]})}function w(a={}){let{wrapper:e}=a.components||{};return e?(0,n.jsx)(e,Object.assign({},a,{children:(0,n.jsx)(d,a)})):d(a)}var S=w;return k(_);})();\n;return Component;"
        },
        "_id": "blog/posts/hosting-mastodon-on-release-and-aws.mdx",
        "_raw": {
          "sourceFilePath": "blog/posts/hosting-mastodon-on-release-and-aws.mdx",
          "sourceFileName": "hosting-mastodon-on-release-and-aws.mdx",
          "sourceFileDir": "blog/posts",
          "contentType": "mdx",
          "flattenedPath": "blog/posts/hosting-mastodon-on-release-and-aws"
        },
        "type": "BlogPost",
        "computedSlug": "hosting-mastodon-on-release-and-aws"
      },
      "documentHash": "1739393595019",
      "hasWarnings": false,
      "documentTypeName": "BlogPost"
    },
    "blog/posts/how-debtbook-ships-6x-faster-with-release.mdx": {
      "document": {
        "title": "How DebtBook Ships 6x Faster with Release  ",
        "summary": "Every customer story is different. See how DebtBook implemented Release and never looked back.",
        "publishDate": "Mon Mar 04 2024 17:22:39 GMT+0000 (Coordinated Universal Time)",
        "author": "ira-casteel",
        "readingTime": 5,
        "categories": [
          "platform-engineering",
          "product"
        ],
        "mainImage": "/blog-images/f6bc253d1412664f69bde44eb7760fe8.jpg",
        "imageAlt": "Release DebtBook",
        "showCTA": true,
        "ctaCopy": "Accelerate your release cycles like DebtBook by leveraging Release's on-demand environments for seamless collaboration and faster deployments.",
        "ctaLink": "https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=how-debtbook-ships-6x-faster-with-release",
        "relatedPosts": [
          "components-of-a-successful-idp-build-a-product-your-developers-actually-want-to-use; lessons-learned-from-maintaining-the-soc-2-type-2-certification-over-the-years"
        ],
        "ogImage": "/blog-images/f6bc253d1412664f69bde44eb7760fe8.jpg",
        "excerpt": "Every customer story is different. See how DebtBook implemented Release and never looked back.",
        "tags": [
          "platform-engineering",
          "product"
        ],
        "ctaButton": "Try Release for Free",
        "body": {
          "raw": "\nWe sat down with Michael Gorsuch, Director of Infrastructure at DebtBook, a rapidly growing financial software company to talk about ways they are improving developer experience and accelerating product delivery in a highly regulated field. Throughout the conversation we learned about the unexpected cultural shifts implementing Release brought to their team, discussed process improvements, and explored future collaboration plans.\n\nQ: Michael, tell us about DebtBook.\n\nA: DebtBook builds accounting software for public sector finance teams. Regulatory landscape around public spending is rapidly changing and becoming more complex with the introduction of the Financial Data Transparency Act, GASB 87, GASB 96, and more. All these regulations are great for the taxpayers but add layers of complexity for the resource-strapped finance teams, making it that much more difficult to keep up. DebtBook founders spent years in the public sector regulatory space and saw how no one was addressing these challenges well, leaving the finance teams buried in disparate spreadsheets with outdated figures. With DebtBook, teams at your local hospitals, universities and city governments have a unified view of their debt, lease, subscription management, and compliance reporting, making sure tax-payers funds are spent wisely and transparently.\n\nQ: Sounds like DebtBook has an important mission to fulfill. Tell us more about your particular team and the role you have in the company.\n\nA: I lead the Infrastructure team inside the Product Engineering org at DebtBook, and our primary role is to help our engineering team win. Aside form the day-to-day duties, my team has two main objectives:\n\n1. Ensure that the infrastructure is well aligned to how our engineering team wants to function, so that it is easy for them to stay on task.\n2. Make sure everything [infra] is running smoothly and securely, so our engineering team is not bogged down with unnecessary toil.\n\nI believe that our work is hard enough, and the challenges we are solving for our customers are extremely complex, so we do not need infrastructure to slow us down. We need to work on the hard, unsolved problems, and make everything that's already been solved by someone else easy.\n\nQ: What development challenges were top of mind for you?\n\nA: We relied on a number of shared environments to work through our deployment process. As the team grew and our product became more complex, the shared environments became more complex as well and our commit-to-customer cycle expanded to go well over a month. As you might expect, such long release cycles caused friction. Developers would spend a significant amount of energy perfecting their code, only to learn a few weeks later that they built an awesome feature that was not actually addressing the customer's needs. It was frustrating to context switch to old issues all because our shared environments setup was too slow and cumbersome to accommodate timely feedback. We knew we could ease these frustrations, shorten the feedback loop and speed up our deployment process, we just needed the right tools.\n\nQ: Why did you think an ephemeral environments solution like Release could be the answer?\n\nA: We knew we needed a preview or on-demand environments option, and that our shared environments process was not going to cut it as the company grew and our product became more complex. In the past I worked with Heroku and came to rely on the preview functionality it offered. It made feedback faster and more actionable, and made collaboration easier overall. For various reasons Heroku was not a good fit at Debtbook, so we needed to find a reliable, secure, scalable and functional solution that our engineers could incorporate into their workflow quickly.\n\nQ: Did you consider building a preview or an ephemeral environment solution in-house?\nA: Building an ephemeral environment platform ourselves was never a serious consideration at DebtBook. We knew it would take at least two senior engineers to build something usable, and it would take time. Time we did not have and time we did not want to spend on a project outside of our core competencies. As an engineer you always want to tackle the problem yourself, but as a leader you need to allocate your resources to the most impactful work, and building a platform someone already perfected made no sense for our growth.\n\nQ: Release is not the only environments-as-a-service offering (although we'd argue it's the best), how did you evaluate all the options and make your decision?\nA: Surprisingly there are not that many options for true environments-as-service platforms out there. Many claim they offer ephemeral environments, but are not a fit for complex, large applications that require high levels of reliability, security and scalability. That said, we did find a couple that seemed to fit the bill. We timeboxed the evaluation to a week, and set out to build a working prototype for engineers to play with. Release was the only one that actually worked as described. I was able to stand something up in one afternoon, shared it with the engineers and they got excited. It started a feedback loop to bring more things into Release and refine it, and it worked, it simply worked. I got good use of the free trial and once we were happy with the results, we engaged Release to start rolling it out to a wider team. The team at Release was on-point from day one, and continues to be a trusted partner.\n\nQ: As you onboarded with Release, what changes did you start seeing in your organization?\nA: Release team was tremendously helpful during the onboarding process. It took us about a month to get everything up and running at the level we needed it, and roll it out to the wider team. From the start, we got amazing adoption among engineers. They all wanted a simpler, more seamless way to preview, test and deploy their changes and Release did just that from day one. Now within 10 minutes on average of pushing up a commit they have a full-stack production-like environment they can share with product or other engineers and show their work. They can do whatever they want in it without any risk of messing up production, or stepping on each other's toes.\n\nWith ephemeral environments we expected to see the commit-to-customer cycle to go down, and it did. We went from 40 days to less than a week to ship changes. What we did not expect was the massive culture change that followed. Giving engineers self-service, on-demand, isolated environments that spin up in minutes, with production-like data in tow, allowed them to take more chances, satisfy their curiosity and start digging into the issues that they simply could not have thought about before. Release allowed them to try out completely different patterns and effectively deploy something and show it to their peers or even to the customer right then and there, without breaking anything. We could not do these kinds of experiments with the static environments we had before. This has a tangible impact for our customers too. The other day we had a ticket go from creation to completely reviewed, tested and merged, and it took us less than 2 hours. A year ago before several changes (including using Release as the latest one) that would have taken days. Things like this make our engineers happy to come to work each day.\n\nQ: What are the key benefits of implementing Release at DebtBook?\nA: There are three main benefits:\n\n1. **Development velocity** - we 6x our development velocity. Going from over 40 days to ship new features, to shipping multiple times a week. And that's still slow. We are working on moving even faster as we speak.\n2. **Developer experience** - our feedback loops shrank significantly. Our engineers are less frustrated, they collaborate and share ideas more freely, and get features in the hands of our customers faster. All this gives our team excitement and energy to tackle the next challenge.\n3. **Lean operations** - as a small infrastructure team, we are able to deliver functionality well above our size. We're small but mighty, leveraging our resources in a smart way.\n\nQ: How would your team react if you stopped using Release today?\nA: Funny you ask that. At our recent offsite I had multiple engineers come up and comment on how happy they were with Release and how it made their life easier, and if I made it go away they would not stand for it. At this point Release is an essential part of our development workflow. It allows engineers to experiment and get relevant feedback in minutes, not days, which makes them that much more excited to work on our product.\n\nQ: It's great to hear that everyone is happy with Release. But I'm sure you have suggestions for improvement. Are there features or functionality Release is missing?\nA: We've been vocal about our feature requests from early days, and so far Release either implemented the requests, found workarounds, or helped us rethink processes on our side to make things flow smoother. It's been a great partnership. Every time we have questions, Release team responds within minutes with genuine curiosity and willingness to help. I'd say Release meets our requirements at the moment and I'm confident they will continue to grow and improve alongside us.\n\nQ: Who would you recommend Release to?\nA: Who wouldn't I recommend Release to? If you are building software you should be using Release. Unless you're building an environments-as-a-service or infrastructure-as-a-service platform, and that's your core business, or you're Google, Facebook or Netflix with hundreds of infrastructure engineers, you should not be building this functionality yourself. Inevitably it will consume too much of your time, effort and money, and you'll still get something that's only a fraction as good as Release. So who should use Release? Any company with complex cloud-first, containerized applications (internal or external) who wants to move faster and have a competitive advantage in their industry. [Give it a try](https://release.com/signup), and see for yourself.\n",
          "code": "var Component=(()=>{var u=Object.create;var r=Object.defineProperty;var c=Object.getOwnPropertyDescriptor;var m=Object.getOwnPropertyNames;var p=Object.getPrototypeOf,g=Object.prototype.hasOwnProperty;var y=(o,e)=>()=>(e||o((e={exports:{}}).exports,e),e.exports),f=(o,e)=>{for(var n in e)r(o,n,{get:e[n],enumerable:!0})},s=(o,e,n,i)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let a of m(e))!g.call(o,a)&&a!==n&&r(o,a,{get:()=>e[a],enumerable:!(i=c(e,a))||i.enumerable});return o};var w=(o,e,n)=>(n=o!=null?u(p(o)):{},s(e||!o||!o.__esModule?r(n,\"default\",{value:o,enumerable:!0}):n,o)),b=o=>s(r({},\"__esModule\",{value:!0}),o);var d=y((I,l)=>{l.exports=_jsx_runtime});var R={};f(R,{default:()=>x,frontmatter:()=>v});var t=w(d()),v={title:\"How DebtBook Ships 6x Faster with Release  \",summary:\"Every customer story is different. See how DebtBook implemented Release and never looked back.\",publishDate:\"Mon Mar 04 2024 17:22:39 GMT+0000 (Coordinated Universal Time)\",author:\"ira-casteel\",readingTime:5,categories:[\"platform-engineering\",\"product\"],mainImage:\"/blog-images/f6bc253d1412664f69bde44eb7760fe8.jpg\",imageAlt:\"Release DebtBook\",showCTA:!0,ctaCopy:\"Accelerate your release cycles like DebtBook by leveraging Release's on-demand environments for seamless collaboration and faster deployments.\",ctaLink:\"https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=how-debtbook-ships-6x-faster-with-release\",relatedPosts:[\"components-of-a-successful-idp-build-a-product-your-developers-actually-want-to-use; lessons-learned-from-maintaining-the-soc-2-type-2-certification-over-the-years\"],ogImage:\"/blog-images/f6bc253d1412664f69bde44eb7760fe8.jpg\",excerpt:\"Every customer story is different. See how DebtBook implemented Release and never looked back.\",tags:[\"platform-engineering\",\"product\"],ctaButton:\"Try Release for Free\"};function h(o){let e=Object.assign({p:\"p\",ol:\"ol\",li:\"li\",strong:\"strong\",a:\"a\"},o.components);return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(e.p,{children:\"We sat down with Michael Gorsuch, Director of Infrastructure at DebtBook, a rapidly growing financial software company to talk about ways they are improving developer experience and accelerating product delivery in a highly regulated field. Throughout the conversation we learned about the unexpected cultural shifts implementing Release brought to their team, discussed process improvements, and explored future collaboration plans.\"}),`\n`,(0,t.jsx)(e.p,{children:\"Q: Michael, tell us about DebtBook.\"}),`\n`,(0,t.jsx)(e.p,{children:\"A: DebtBook builds accounting software for public sector finance teams. Regulatory landscape around public spending is rapidly changing and becoming more complex with the introduction of the Financial Data Transparency Act, GASB 87, GASB 96, and more. All these regulations are great for the taxpayers but add layers of complexity for the resource-strapped finance teams, making it that much more difficult to keep up. DebtBook founders spent years in the public sector regulatory space and saw how no one was addressing these challenges well, leaving the finance teams buried in disparate spreadsheets with outdated figures. With DebtBook, teams at your local hospitals, universities and city governments have a unified view of their debt, lease, subscription management, and compliance reporting, making sure tax-payers funds are spent wisely and transparently.\"}),`\n`,(0,t.jsx)(e.p,{children:\"Q: Sounds like DebtBook has an important mission to fulfill. Tell us more about your particular team and the role you have in the company.\"}),`\n`,(0,t.jsx)(e.p,{children:\"A: I lead the Infrastructure team inside the Product Engineering org at DebtBook, and our primary role is to help our engineering team win. Aside form the day-to-day duties, my team has two main objectives:\"}),`\n`,(0,t.jsxs)(e.ol,{children:[`\n`,(0,t.jsx)(e.li,{children:\"Ensure that the infrastructure is well aligned to how our engineering team wants to function, so that it is easy for them to stay on task.\"}),`\n`,(0,t.jsx)(e.li,{children:\"Make sure everything [infra] is running smoothly and securely, so our engineering team is not bogged down with unnecessary toil.\"}),`\n`]}),`\n`,(0,t.jsx)(e.p,{children:\"I believe that our work is hard enough, and the challenges we are solving for our customers are extremely complex, so we do not need infrastructure to slow us down. We need to work on the hard, unsolved problems, and make everything that's already been solved by someone else easy.\"}),`\n`,(0,t.jsx)(e.p,{children:\"Q: What development challenges were top of mind for you?\"}),`\n`,(0,t.jsx)(e.p,{children:\"A: We relied on a number of shared environments to work through our deployment process. As the team grew and our product became more complex, the shared environments became more complex as well and our commit-to-customer cycle expanded to go well over a month. As you might expect, such long release cycles caused friction. Developers would spend a significant amount of energy perfecting their code, only to learn a few weeks later that they built an awesome feature that was not actually addressing the customer's needs. It was frustrating to context switch to old issues all because our shared environments setup was too slow and cumbersome to accommodate timely feedback. We knew we could ease these frustrations, shorten the feedback loop and speed up our deployment process, we just needed the right tools.\"}),`\n`,(0,t.jsx)(e.p,{children:\"Q: Why did you think an ephemeral environments solution like Release could be the answer?\"}),`\n`,(0,t.jsx)(e.p,{children:\"A: We knew we needed a preview or on-demand environments option, and that our shared environments process was not going to cut it as the company grew and our product became more complex. In the past I worked with Heroku and came to rely on the preview functionality it offered. It made feedback faster and more actionable, and made collaboration easier overall. For various reasons Heroku was not a good fit at Debtbook, so we needed to find a reliable, secure, scalable and functional solution that our engineers could incorporate into their workflow quickly.\"}),`\n`,(0,t.jsx)(e.p,{children:`Q: Did you consider building a preview or an ephemeral environment solution in-house?\nA: Building an ephemeral environment platform ourselves was never a serious consideration at DebtBook. We knew it would take at least two senior engineers to build something usable, and it would take time. Time we did not have and time we did not want to spend on a project outside of our core competencies. As an engineer you always want to tackle the problem yourself, but as a leader you need to allocate your resources to the most impactful work, and building a platform someone already perfected made no sense for our growth.`}),`\n`,(0,t.jsx)(e.p,{children:`Q: Release is not the only environments-as-a-service offering (although we'd argue it's the best), how did you evaluate all the options and make your decision?\nA: Surprisingly there are not that many options for true environments-as-service platforms out there. Many claim they offer ephemeral environments, but are not a fit for complex, large applications that require high levels of reliability, security and scalability. That said, we did find a couple that seemed to fit the bill. We timeboxed the evaluation to a week, and set out to build a working prototype for engineers to play with. Release was the only one that actually worked as described. I was able to stand something up in one afternoon, shared it with the engineers and they got excited. It started a feedback loop to bring more things into Release and refine it, and it worked, it simply worked. I got good use of the free trial and once we were happy with the results, we engaged Release to start rolling it out to a wider team. The team at Release was on-point from day one, and continues to be a trusted partner.`}),`\n`,(0,t.jsx)(e.p,{children:`Q: As you onboarded with Release, what changes did you start seeing in your organization?\nA: Release team was tremendously helpful during the onboarding process. It took us about a month to get everything up and running at the level we needed it, and roll it out to the wider team. From the start, we got amazing adoption among engineers. They all wanted a simpler, more seamless way to preview, test and deploy their changes and Release did just that from day one. Now within 10 minutes on average of pushing up a commit they have a full-stack production-like environment they can share with product or other engineers and show their work. They can do whatever they want in it without any risk of messing up production, or stepping on each other's toes.`}),`\n`,(0,t.jsx)(e.p,{children:\"With ephemeral environments we expected to see the commit-to-customer cycle to go down, and it did. We went from 40 days to less than a week to ship changes. What we did not expect was the massive culture change that followed. Giving engineers self-service, on-demand, isolated environments that spin up in minutes, with production-like data in tow, allowed them to take more chances, satisfy their curiosity and start digging into the issues that they simply could not have thought about before. Release allowed them to try out completely different patterns and effectively deploy something and show it to their peers or even to the customer right then and there, without breaking anything. We could not do these kinds of experiments with the static environments we had before. This has a tangible impact for our customers too. The other day we had a ticket go from creation to completely reviewed, tested and merged, and it took us less than 2 hours. A year ago before several changes (including using Release as the latest one) that would have taken days. Things like this make our engineers happy to come to work each day.\"}),`\n`,(0,t.jsx)(e.p,{children:`Q: What are the key benefits of implementing Release at DebtBook?\nA: There are three main benefits:`}),`\n`,(0,t.jsxs)(e.ol,{children:[`\n`,(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:\"Development velocity\"}),\" - we 6x our development velocity. Going from over 40 days to ship new features, to shipping multiple times a week. And that's still slow. We are working on moving even faster as we speak.\"]}),`\n`,(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:\"Developer experience\"}),\" - our feedback loops shrank significantly. Our engineers are less frustrated, they collaborate and share ideas more freely, and get features in the hands of our customers faster. All this gives our team excitement and energy to tackle the next challenge.\"]}),`\n`,(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:\"Lean operations\"}),\" - as a small infrastructure team, we are able to deliver functionality well above our size. We're small but mighty, leveraging our resources in a smart way.\"]}),`\n`]}),`\n`,(0,t.jsx)(e.p,{children:`Q: How would your team react if you stopped using Release today?\nA: Funny you ask that. At our recent offsite I had multiple engineers come up and comment on how happy they were with Release and how it made their life easier, and if I made it go away they would not stand for it. At this point Release is an essential part of our development workflow. It allows engineers to experiment and get relevant feedback in minutes, not days, which makes them that much more excited to work on our product.`}),`\n`,(0,t.jsx)(e.p,{children:`Q: It's great to hear that everyone is happy with Release. But I'm sure you have suggestions for improvement. Are there features or functionality Release is missing?\nA: We've been vocal about our feature requests from early days, and so far Release either implemented the requests, found workarounds, or helped us rethink processes on our side to make things flow smoother. It's been a great partnership. Every time we have questions, Release team responds within minutes with genuine curiosity and willingness to help. I'd say Release meets our requirements at the moment and I'm confident they will continue to grow and improve alongside us.`}),`\n`,(0,t.jsxs)(e.p,{children:[`Q: Who would you recommend Release to?\nA: Who wouldn't I recommend Release to? If you are building software you should be using Release. Unless you're building an environments-as-a-service or infrastructure-as-a-service platform, and that's your core business, or you're Google, Facebook or Netflix with hundreds of infrastructure engineers, you should not be building this functionality yourself. Inevitably it will consume too much of your time, effort and money, and you'll still get something that's only a fraction as good as Release. So who should use Release? Any company with complex cloud-first, containerized applications (internal or external) who wants to move faster and have a competitive advantage in their industry. `,(0,t.jsx)(e.a,{href:\"https://release.com/signup\",children:\"Give it a try\"}),\", and see for yourself.\"]})]})}function k(o={}){let{wrapper:e}=o.components||{};return e?(0,t.jsx)(e,Object.assign({},o,{children:(0,t.jsx)(h,o)})):h(o)}var x=k;return b(R);})();\n;return Component;"
        },
        "_id": "blog/posts/how-debtbook-ships-6x-faster-with-release.mdx",
        "_raw": {
          "sourceFilePath": "blog/posts/how-debtbook-ships-6x-faster-with-release.mdx",
          "sourceFileName": "how-debtbook-ships-6x-faster-with-release.mdx",
          "sourceFileDir": "blog/posts",
          "contentType": "mdx",
          "flattenedPath": "blog/posts/how-debtbook-ships-6x-faster-with-release"
        },
        "type": "BlogPost",
        "computedSlug": "how-debtbook-ships-6x-faster-with-release"
      },
      "documentHash": "1739658100191",
      "hasWarnings": false,
      "documentTypeName": "BlogPost"
    },
    "blog/posts/how-do-you-prepare-for-user-acceptance-testing.mdx": {
      "document": {
        "title": "How to Prepare For User Acceptance Testing?",
        "summary": "In this post you'll learn about various stages of user acceptance testing and tips while preparing for UAT testing.",
        "publishDate": "Tue Jan 04 2022 23:31:35 GMT+0000 (Coordinated Universal Time)",
        "author": "tommy-mcclung",
        "readingTime": 4,
        "categories": [
          "platform-engineering",
          "product"
        ],
        "mainImage": "/blog-images/86ed2c9b00081db1d144e22bd38b9006.jpg",
        "imageAlt": "a person holding a phone",
        "showCTA": true,
        "ctaCopy": "Improve UAT success with Release's production-like test environments for accurate testing and seamless collaboration.",
        "ctaLink": "https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=how-do-you-prepare-for-user-acceptance-testing",
        "relatedPosts": [
          ""
        ],
        "ogImage": "/blog-images/86ed2c9b00081db1d144e22bd38b9006.jpg",
        "excerpt": "In this post you'll learn about various stages of user acceptance testing and tips while preparing for UAT testing.",
        "tags": [
          "platform-engineering",
          "product"
        ],
        "ctaButton": "Try Release for Free",
        "body": {
          "raw": "\nThis is a 4-part series on User Acceptance Testing (UAT)\n\n- Part 1: [What is User Acceptance Testing and its Best Practices](https://release.com/blog/user-acceptance-testing-best-practices)\n- **Part 2: How to Prepare for User Acceptance Testing?**\n- Part 3: [User Acceptance Testing Challenges & UAT Environment Examples](https://release.com//blog/what-are-the-challenges-faced-during-uat-testing)\n- Part 4: [UAT Checklist](https://release.com/blog/user-acceptance-testing-checklist)\n\nEnsuring that the appropriate solution is delivered to the users is the ultimate goal of every test engineer. In this post, we are going to discuss various stages of user acceptance testing and some tips you can use while preparing for UAT.\n\n### Tips to Prepare for User Acceptance Testing\n\n- **Prepare and set up the right environment:** To to carry out accurate performance testing, the first and foremost requirement is to have a production-like test environment.\n- **Plan your test:** Once done, it is crucial to design a clear test acceptance plan during the design and the requirement analysis phase as it helps to reduce pressure to meet deadlines.\n- **Train the UAT staff adequately:** Another important way to prepare for UAT is to train the testers adequately on the developed business requirements as it can help increase the success of UAT significantly.\n- **Set up the right communication channel:** UAT is a process that involves seamless collaboration between various teams, including the development team, the QA team, and the UAT team. It is, therefore, important to have a proper communication channel between these teams to ensure the success of UAT, especially when all these teams are working remotely.**‍**\n- **Do not involve the functional testing team:** Functional testers are not equipped enough to conduct UAT, and they may not test all real-world scenarios. This can less to end-users finding several issues when the software is in final production.\n\n### What are the Stages of Acceptance Testing?\n\nAmong the key stages of user acceptance testing are-\n\n#### Planning phase\n\nThe stage involves assigning a dedicated UAT test manager to oversee the end-to-end process of UAT. The main objective of this stage is to outline proper planning and execution strategy along with identification of important resources and preparation of a powerful resource plan.\n\n#### Preparation of UAT test data and test environment\n\nThis stage ensures UAT readiness as the UAT test environment is set up along with preparation of test management with test data, interfaces, authorization, and scenario readiness.\n\n#### UAT test scheduling and management\n\nThe phase marks preparing proper action plans along with UAT priorities. During this stage, a triage process is also kept in place to prioritize the assessments of defects blocking, if any. Apart from this, an effective mechanism to effortlessly track test scenarios/ test scripts based on the requirements defined is also taken up at this stage.\n\n#### Testing execution and defect management\n\nAn important phase of UAT, here the key goal is to take up proper identification of priority defects with more focus being placed on performing root cause analysis assessments. Apart from this, the stage also marks a trial run of UAT processes to validate execution and defects assignment.\n\n#### UAT sign-off and reporting\n\nThis is the last stage of UAT that involves testing of accurate defect, status reports as well as defect report generation from the test management system. Once done, a sign-off when all bugs have been fixed indicates the acceptance of the software. The idea here is to validate that the application being developed meets the user requirements and is ready for production.\n\nNow that you are familiar with [what is user acceptance testing, UAT best practices](https://releasehub.com/blog/user-acceptance-testing-best-practices), and how to prepare for user acceptance testing let's explore some of the typical challenges faced during user acceptance testing.\n",
          "code": "var Component=(()=>{var p=Object.create;var i=Object.defineProperty;var d=Object.getOwnPropertyDescriptor;var g=Object.getOwnPropertyNames;var u=Object.getPrototypeOf,m=Object.prototype.hasOwnProperty;var f=(n,e)=>()=>(e||n((e={exports:{}}).exports,e),e.exports),T=(n,e)=>{for(var a in e)i(n,a,{get:e[a],enumerable:!0})},o=(n,e,a,r)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let s of g(e))!m.call(n,s)&&s!==a&&i(n,s,{get:()=>e[s],enumerable:!(r=d(e,s))||r.enumerable});return n};var A=(n,e,a)=>(a=n!=null?p(u(n)):{},o(e||!n||!n.__esModule?i(a,\"default\",{value:n,enumerable:!0}):a,n)),b=n=>o(i({},\"__esModule\",{value:!0}),n);var l=f((x,c)=>{c.exports=_jsx_runtime});var y={};T(y,{default:()=>U,frontmatter:()=>v});var t=A(l()),v={title:\"How to Prepare For User Acceptance Testing?\",summary:\"In this post you'll learn about various stages of user acceptance testing and tips while preparing for UAT testing.\",publishDate:\"Tue Jan 04 2022 23:31:35 GMT+0000 (Coordinated Universal Time)\",author:\"tommy-mcclung\",readingTime:4,categories:[\"platform-engineering\",\"product\"],mainImage:\"/blog-images/86ed2c9b00081db1d144e22bd38b9006.jpg\",imageAlt:\"a person holding a phone\",showCTA:!0,ctaCopy:\"Improve UAT success with Release's production-like test environments for accurate testing and seamless collaboration.\",ctaLink:\"https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=how-do-you-prepare-for-user-acceptance-testing\",relatedPosts:[\"\"],ogImage:\"/blog-images/86ed2c9b00081db1d144e22bd38b9006.jpg\",excerpt:\"In this post you'll learn about various stages of user acceptance testing and tips while preparing for UAT testing.\",tags:[\"platform-engineering\",\"product\"],ctaButton:\"Try Release for Free\"};function h(n){let e=Object.assign({p:\"p\",ul:\"ul\",li:\"li\",a:\"a\",strong:\"strong\",h3:\"h3\",span:\"span\",h4:\"h4\"},n.components);return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(e.p,{children:\"This is a 4-part series on User Acceptance Testing (UAT)\"}),`\n`,(0,t.jsxs)(e.ul,{children:[`\n`,(0,t.jsxs)(e.li,{children:[\"Part 1: \",(0,t.jsx)(e.a,{href:\"https://release.com/blog/user-acceptance-testing-best-practices\",children:\"What is User Acceptance Testing and its Best Practices\"})]}),`\n`,(0,t.jsx)(e.li,{children:(0,t.jsx)(e.strong,{children:\"Part 2: How to Prepare for User Acceptance Testing?\"})}),`\n`,(0,t.jsxs)(e.li,{children:[\"Part 3: \",(0,t.jsx)(e.a,{href:\"https://release.com//blog/what-are-the-challenges-faced-during-uat-testing\",children:\"User Acceptance Testing Challenges & UAT Environment Examples\"})]}),`\n`,(0,t.jsxs)(e.li,{children:[\"Part 4: \",(0,t.jsx)(e.a,{href:\"https://release.com/blog/user-acceptance-testing-checklist\",children:\"UAT Checklist\"})]}),`\n`]}),`\n`,(0,t.jsx)(e.p,{children:\"Ensuring that the appropriate solution is delivered to the users is the ultimate goal of every test engineer. In this post, we are going to discuss various stages of user acceptance testing and some tips you can use while preparing for UAT.\"}),`\n`,(0,t.jsxs)(e.h3,{id:\"tips-to-prepare-for-user-acceptance-testing\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#tips-to-prepare-for-user-acceptance-testing\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Tips to Prepare for User Acceptance Testing\"]}),`\n`,(0,t.jsxs)(e.ul,{children:[`\n`,(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:\"Prepare and set up the right environment:\"}),\" To to carry out accurate performance testing, the first and foremost requirement is to have a production-like test environment.\"]}),`\n`,(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:\"Plan your test:\"}),\" Once done, it is crucial to design a clear test acceptance plan during the design and the requirement analysis phase as it helps to reduce pressure to meet deadlines.\"]}),`\n`,(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:\"Train the UAT staff adequately:\"}),\" Another important way to prepare for UAT is to train the testers adequately on the developed business requirements as it can help increase the success of UAT significantly.\"]}),`\n`,(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:\"Set up the right communication channel:\"}),\" UAT is a process that involves seamless collaboration between various teams, including the development team, the QA team, and the UAT team. It is, therefore, important to have a proper communication channel between these teams to ensure the success of UAT, especially when all these teams are working remotely.\",(0,t.jsx)(e.strong,{children:\"\\u200D\"})]}),`\n`,(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:\"Do not involve the functional testing team:\"}),\" Functional testers are not equipped enough to conduct UAT, and they may not test all real-world scenarios. This can less to end-users finding several issues when the software is in final production.\"]}),`\n`]}),`\n`,(0,t.jsxs)(e.h3,{id:\"what-are-the-stages-of-acceptance-testing\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#what-are-the-stages-of-acceptance-testing\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"What are the Stages of Acceptance Testing?\"]}),`\n`,(0,t.jsx)(e.p,{children:\"Among the key stages of user acceptance testing are-\"}),`\n`,(0,t.jsxs)(e.h4,{id:\"planning-phase\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#planning-phase\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Planning phase\"]}),`\n`,(0,t.jsx)(e.p,{children:\"The stage involves assigning a dedicated UAT test manager to oversee the end-to-end process of UAT. The main objective of this stage is to outline proper planning and execution strategy along with identification of important resources and preparation of a powerful resource plan.\"}),`\n`,(0,t.jsxs)(e.h4,{id:\"preparation-of-uat-test-data-and-test-environment\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#preparation-of-uat-test-data-and-test-environment\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Preparation of UAT test data and test environment\"]}),`\n`,(0,t.jsx)(e.p,{children:\"This stage ensures UAT readiness as the UAT test environment is set up along with preparation of test management with test data, interfaces, authorization, and scenario readiness.\"}),`\n`,(0,t.jsxs)(e.h4,{id:\"uat-test-scheduling-and-management\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#uat-test-scheduling-and-management\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"UAT test scheduling and management\"]}),`\n`,(0,t.jsx)(e.p,{children:\"The phase marks preparing proper action plans along with UAT priorities. During this stage, a triage process is also kept in place to prioritize the assessments of defects blocking, if any. Apart from this, an effective mechanism to effortlessly track test scenarios/ test scripts based on the requirements defined is also taken up at this stage.\"}),`\n`,(0,t.jsxs)(e.h4,{id:\"testing-execution-and-defect-management\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#testing-execution-and-defect-management\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Testing execution and defect management\"]}),`\n`,(0,t.jsx)(e.p,{children:\"An important phase of UAT, here the key goal is to take up proper identification of priority defects with more focus being placed on performing root cause analysis assessments. Apart from this, the stage also marks a trial run of UAT processes to validate execution and defects assignment.\"}),`\n`,(0,t.jsxs)(e.h4,{id:\"uat-sign-off-and-reporting\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#uat-sign-off-and-reporting\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"UAT sign-off and reporting\"]}),`\n`,(0,t.jsx)(e.p,{children:\"This is the last stage of UAT that involves testing of accurate defect, status reports as well as defect report generation from the test management system. Once done, a sign-off when all bugs have been fixed indicates the acceptance of the software. The idea here is to validate that the application being developed meets the user requirements and is ready for production.\"}),`\n`,(0,t.jsxs)(e.p,{children:[\"Now that you are familiar with \",(0,t.jsx)(e.a,{href:\"https://releasehub.com/blog/user-acceptance-testing-best-practices\",children:\"what is user acceptance testing, UAT best practices\"}),\", and how to prepare for user acceptance testing let's explore some of the typical challenges faced during user acceptance testing.\"]})]})}function w(n={}){let{wrapper:e}=n.components||{};return e?(0,t.jsx)(e,Object.assign({},n,{children:(0,t.jsx)(h,n)})):h(n)}var U=w;return b(y);})();\n;return Component;"
        },
        "_id": "blog/posts/how-do-you-prepare-for-user-acceptance-testing.mdx",
        "_raw": {
          "sourceFilePath": "blog/posts/how-do-you-prepare-for-user-acceptance-testing.mdx",
          "sourceFileName": "how-do-you-prepare-for-user-acceptance-testing.mdx",
          "sourceFileDir": "blog/posts",
          "contentType": "mdx",
          "flattenedPath": "blog/posts/how-do-you-prepare-for-user-acceptance-testing"
        },
        "type": "BlogPost",
        "computedSlug": "how-do-you-prepare-for-user-acceptance-testing"
      },
      "documentHash": "1739393595019",
      "hasWarnings": false,
      "documentTypeName": "BlogPost"
    },
    "blog/posts/how-does-terraform-enable-infrastructure-as-code.mdx": {
      "document": {
        "title": "How Does Terraform Enable Infrastructure as Code?",
        "summary": " In this post, learn about infrastructure as code (IaC) and how Terraform enables infrastructure as code.",
        "publishDate": "Tue Jan 17 2023 09:57:02 GMT+0000 (Coordinated Universal Time)",
        "author": "",
        "readingTime": 6,
        "categories": [
          "platform-engineering",
          "product"
        ],
        "mainImage": "/blog-images/65dfbbec321f004011cd90b5d5a9c45e.jpeg",
        "imageAlt": "How Does Terraform Enable Infrastructure as Code?",
        "showCTA": true,
        "ctaCopy": "Automate infrastructure provisioning with Release for efficient IaC implementation and reliable environment management.",
        "ctaLink": "https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=how-does-terraform-enable-infrastructure-as-code",
        "relatedPosts": [
          ""
        ],
        "ogImage": "/blog-images/65dfbbec321f004011cd90b5d5a9c45e.jpeg",
        "excerpt": " In this post, learn about infrastructure as code (IaC) and how Terraform enables infrastructure as code.",
        "tags": [
          "platform-engineering",
          "product"
        ],
        "ctaButton": "Try Release for Free",
        "body": {
          "raw": "\nA good deployment pipeline requires a well-balanced use of tools and resources. But finding that balance can be challenging. With infrastructure as code (IaC), you don't need to worry about the configuration-management side of the equation because you're creating your infrastructure based on how it works and what it should look like. In addition, as your infrastructure scales, it does so without human intervention. This eliminates the need for manual operations such as redeployment or rollback when changes go awry. \n\nThis post will shed more light on what infrastructure as code is. Then, we'll explain how Terraform enables it. \n\n## **What is Infrastructure as Code?**\n\nInfrastructure as code is the art of using software tools to manage and provision physical, virtual, and cloud-based resources. \n\nIaC allows infrastructure engineers and application developers to design and build systems in an automated fashion with high levels of reliability. We can achieve this by defining the infrastructure in text-based files that a specification language interpreter then processes. \n\nThe concept of IaC has its roots in configuration management. Configuration management refers to managing and maintaining the configuration of systems and devices in a consistent and controlled way. \n\n![](/blog-images/52bdb97660d371af2876002fdafd6e37.png)\n\nIaC builds on this concept by extending it to include infrastructure resources such as servers, storage devices, and networking equipment. This enables organizations to manage their infrastructure using configuration files rather than manual processes. \n\nDeveloping tools like [Terraform](https://www.terraform.io/) and CloudFormation have enabled IaC by providing a way to define and manage infrastructure using configuration files. These tools have made it easier for organizations to automate the provisioning and management of infrastructure and have helped drive the adoption of IaC. \n\n## **What Are the Benefits of IaC?**\n\nThese are some of the main benefits of IaC. \n\n- Simpler and more consistent provisioning: IaC helps you collaborate on infrastructure management and share infrastructure configurations with other teams and organizations.\n- Easy and efficient infrastructure replication: By using IaC tools like Terraform, Ansible, and Puppet, we can define how we want our infrastructure to look using code. We can also deploy the configurations to any environment.\n- Repeatable and auditable changes: You can track changes using version control, which helps keep the infrastructure up to date and reduces the risk of security threats.\n\n## **How do you Write IaC?**\n\nThere are several ways to write IaC depending on your project's specific needs and requirements. Some common approaches include: \n\n- Using a declarative language such as YAML or JSON to define the desired state of your infrastructure. This approach allows you to specify your infrastructure and the IaC tool will automatically provision and configure the resources to meet those specifications.\n- Using a procedural language such as Python or Ruby to write scripts that provision and configure your infrastructure. This approach allows you to specify the steps that need to be taken to provision and configure your infrastructure. The IaC tool will execute those steps to create the desired infrastructure.\n- Using a configuration management tool such as Ansible or Puppet to manage your infrastructure. These tools provide a set of predefined modules to provision and configure your infrastructure in a repeatable and predictable way.\n\n## **What is Terraform?**\n\nTerraform is a tool for building, changing, and versioning infrastructure safely and efficiently. It can manage existing popular service providers and custom in-house solutions. \n\nTerraform manages the complete state of your infrastructure with declarative configuration files, which are treated like code. The system keeps knowledge of rebuilding or changing any resource or infrastructure component. This makes it safer than a traditional on-site human expert who needs to guide every action. \n\n![](/blog-images/f8651fabc7d45f59142db03eb005c51c.png)\n\nWhen applied with IaC practices such as continuous integration (CI), [Terraform](https://releasehub.com/blog/terraform-kubernetes-deployment-a-detailed-walkthrough) can become an increasingly powerful tool for rapidly provisioning new environments and then scaling them out safely while minimizing disruption to existing customers. \n\nTerraform's IaC feature keeps your infrastructure up to date with the latest versions of Terraform modules through updates and releases performed by other users. \n\n## **What Code is Terraform Written in?**\n\nTerraform is primarily written in the [Go](https://go.dev/) programming language. HashiCorp initially developed Go and it's now an open-source project with contributions from a community of users and developers. \n\nBesides Go, Terraform uses the [HashiCorp Configuration Language](https://developer.hashicorp.com/terraform/language) (HCL) as its primary configuration language. \n\nHCL is a declarative language that is human-readable and easy to write and understand. It's used to define the desired state of infrastructure, which Terraform uses to provision and configure the resources. \n\n## **What Are the Benefits of Using Terraform?**\n\n- Easy to use: You can codify existing system architecture into declarative configuration files. No more brittle shell scripts and manual operations.\n- Fast: With Terraform, you can plan, create, and destroy infrastructure for hundreds of services in a matter of seconds. It doesn't require a central database, so it's also easy to install, no matter how big your environment is.\n- Secure: Terraform provides locking mechanisms to prevent multiple teams from accidentally creating overlapping resources, which could easily happen with manual setups. Terraform also uses standard modules, so it's difficult to expose sensitive data like API keys or passwords accidentally.\n\n![](/blog-images/371353e8ac1d8b4e6c63f7fe811b457e.jpeg)\n\n## **How Does Terraform Enable IaC?**\n\nTerraform enables IaC by providing a declarative language and tools that allow organizations to define and manage their infrastructure using code. \n\nHere's an example of how Terraform can enable IaC: \n\n```hcl\nresource \"aws_instance\" \"example\" {\n  ami           = \"ami-0c55b159cbfafe1f0\"\n  instance_type = \"t2.micro\"\n}\n```\n\nIn this example, you define the **aws_instance** resource using HCL. This code specifies the desired state of the infrastructure: A virtual machine with a specific AMI and instance type. \n\nWhen you execute this code using the Terraform command-line interface (CLI), Terraform will provision and configure the resources in AWS to meet these specifications. The virtual machine will be created and configured automatically without requiring manual intervention. \n\n### **Key Features of Terraform That Support IaC**\n\nBelow are some features of Terraform that support and enable IaC: \n\n- You can use the HCL declarative language to define the desired state of your infrastructure. This allows you to specify how your infrastructure should look.\n- Support for version control enables organizations to track changes to their infrastructure over time and collaborate with other developers on infrastructure management. This makes it easier to review and test changes to infrastructure and to roll back changes if necessary.\n- The resource graph allows you to visualize and understand your entire infrastructure.\n- Execution plans created in Terraform show you the actions that will be required to reach your desired state.\n- Terraform can also orchestrate multiple services and providers, allowing you to manage your resources across different cloud platforms.\n\n## **Is Terraform an IaC?**\n\nNo, but it's an IaC _tool_. IaC is a practice. Terraform is an IaC tool that enables organizations to manage their infrastructure using code. \n\n## **What Other Tools can you use to Implement IaC?**\n\nOther tools that can implement IaC include: \n\n1\\. Ansible \n\n2\\. Puppet \n\n3\\. Progress Chef \n\n4\\. SaltStack \n\n5\\. CloudFormation \n\n6\\. Vagrant \n\n## **Conclusion**\n\nWith Terraform, your team can easily provision and manage their IaC. They'll scale their resources and deploy applications across different environments. Terraform also allows you to take advantage of different environments, like ephemeral environments. Maybe you'd like to spin up an environment—for example, to show a certain feature or product—without worrying that someone else will mess it up. Follow [this guide](https://releasehub.com/ephemeral-environments) to get started with ephemeral environments. \n\n_Discover the benefits of using infrastructure as code (IaC) with Terraform! IaC enables organizations to manage their infrastructure in a more predictable and repeatable way, which can improve the reliability and maintainability of their infrastructure. Learn more with Release. #IaC #Terraform #Ephemeral Environment_\n",
          "code": "var Component=(()=>{var d=Object.create;var t=Object.defineProperty;var u=Object.getOwnPropertyDescriptor;var f=Object.getOwnPropertyNames;var m=Object.getPrototypeOf,p=Object.prototype.hasOwnProperty;var g=(a,e)=>()=>(e||a((e={exports:{}}).exports,e),e.exports),w=(a,e)=>{for(var n in e)t(a,n,{get:e[n],enumerable:!0})},s=(a,e,n,o)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let i of f(e))!p.call(a,i)&&i!==n&&t(a,i,{get:()=>e[i],enumerable:!(o=u(e,i))||o.enumerable});return a};var y=(a,e,n)=>(n=a!=null?d(m(a)):{},s(e||!a||!a.__esModule?t(n,\"default\",{value:a,enumerable:!0}):n,a)),b=a=>s(t({},\"__esModule\",{value:!0}),a);var l=g((N,c)=>{c.exports=_jsx_runtime});var I={};w(I,{default:()=>C,frontmatter:()=>v});var r=y(l()),v={title:\"How Does Terraform Enable Infrastructure as Code?\",summary:\" In this post, learn about infrastructure as code (IaC) and how Terraform enables infrastructure as code.\",publishDate:\"Tue Jan 17 2023 09:57:02 GMT+0000 (Coordinated Universal Time)\",author:\"\",readingTime:6,categories:[\"platform-engineering\",\"product\"],mainImage:\"/blog-images/65dfbbec321f004011cd90b5d5a9c45e.jpeg\",imageAlt:\"How Does Terraform Enable Infrastructure as Code?\",showCTA:!0,ctaCopy:\"Automate infrastructure provisioning with Release for efficient IaC implementation and reliable environment management.\",ctaLink:\"https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=how-does-terraform-enable-infrastructure-as-code\",relatedPosts:[\"\"],ogImage:\"/blog-images/65dfbbec321f004011cd90b5d5a9c45e.jpeg\",excerpt:\" In this post, learn about infrastructure as code (IaC) and how Terraform enables infrastructure as code.\",tags:[\"platform-engineering\",\"product\"],ctaButton:\"Try Release for Free\"};function h(a){let e=Object.assign({p:\"p\",h2:\"h2\",a:\"a\",span:\"span\",strong:\"strong\",img:\"img\",ul:\"ul\",li:\"li\",pre:\"pre\",code:\"code\",h3:\"h3\",em:\"em\"},a.components);return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(e.p,{children:\"A good deployment pipeline requires a well-balanced use of tools and resources. But finding that balance can be challenging. With infrastructure as code (IaC), you don't need to worry about the configuration-management side of the equation because you're creating your infrastructure based on how it works and what it should look like. In addition, as your infrastructure scales, it does so without human intervention. This eliminates the need for manual operations such as redeployment or rollback when changes go awry.\\xA0\"}),`\n`,(0,r.jsx)(e.p,{children:\"This post will shed more light on what infrastructure as code is. Then, we'll explain how Terraform enables it.\\xA0\"}),`\n`,(0,r.jsxs)(e.h2,{id:\"what-is-infrastructure-as-code\",children:[(0,r.jsx)(e.a,{className:\"anchor\",href:\"#what-is-infrastructure-as-code\",children:(0,r.jsx)(e.span,{className:\"icon icon-link\"})}),(0,r.jsx)(e.strong,{children:\"What is Infrastructure as Code?\"})]}),`\n`,(0,r.jsx)(e.p,{children:\"Infrastructure as code is the art of using software tools to manage and provision physical, virtual, and cloud-based resources.\\xA0\"}),`\n`,(0,r.jsx)(e.p,{children:\"IaC allows infrastructure engineers and application developers to design and build systems in an automated fashion with high levels of reliability. We can achieve this by defining the infrastructure in text-based files that a specification language interpreter then processes.\\xA0\"}),`\n`,(0,r.jsx)(e.p,{children:\"The concept of IaC has its roots in configuration management. Configuration management refers to managing and maintaining the configuration of systems and devices in a consistent and controlled way.\\xA0\"}),`\n`,(0,r.jsx)(e.p,{children:(0,r.jsx)(e.img,{src:\"/blog-images/52bdb97660d371af2876002fdafd6e37.png\",alt:\"\"})}),`\n`,(0,r.jsx)(e.p,{children:\"IaC builds on this concept by extending it to include infrastructure resources such as servers, storage devices, and networking equipment. This enables organizations to manage their infrastructure using configuration files rather than manual processes.\\xA0\"}),`\n`,(0,r.jsxs)(e.p,{children:[\"Developing tools like \",(0,r.jsx)(e.a,{href:\"https://www.terraform.io/\",children:\"Terraform\"}),\" and CloudFormation have enabled IaC by providing a way to define and manage infrastructure using configuration files. These tools have made it easier for organizations to automate the provisioning and management of infrastructure and have helped drive the adoption of IaC.\\xA0\"]}),`\n`,(0,r.jsxs)(e.h2,{id:\"what-are-the-benefits-of-iac\",children:[(0,r.jsx)(e.a,{className:\"anchor\",href:\"#what-are-the-benefits-of-iac\",children:(0,r.jsx)(e.span,{className:\"icon icon-link\"})}),(0,r.jsx)(e.strong,{children:\"What Are the Benefits of IaC?\"})]}),`\n`,(0,r.jsx)(e.p,{children:\"These are some of the main benefits of IaC.\\xA0\"}),`\n`,(0,r.jsxs)(e.ul,{children:[`\n`,(0,r.jsx)(e.li,{children:\"Simpler and more consistent provisioning: IaC helps you collaborate on infrastructure management and share infrastructure configurations with other teams and organizations.\"}),`\n`,(0,r.jsx)(e.li,{children:\"Easy and efficient infrastructure replication: By using IaC tools like Terraform, Ansible, and Puppet, we can define how we want our infrastructure to look using code. We can also deploy the configurations to any environment.\"}),`\n`,(0,r.jsx)(e.li,{children:\"Repeatable and auditable changes: You can track changes using version control, which helps keep the infrastructure up to date and reduces the risk of security threats.\"}),`\n`]}),`\n`,(0,r.jsxs)(e.h2,{id:\"how-do-you-write-iac\",children:[(0,r.jsx)(e.a,{className:\"anchor\",href:\"#how-do-you-write-iac\",children:(0,r.jsx)(e.span,{className:\"icon icon-link\"})}),(0,r.jsx)(e.strong,{children:\"How do you Write IaC?\"})]}),`\n`,(0,r.jsx)(e.p,{children:\"There are several ways to write IaC depending on your project's specific needs and requirements. Some common approaches include:\\xA0\"}),`\n`,(0,r.jsxs)(e.ul,{children:[`\n`,(0,r.jsx)(e.li,{children:\"Using a declarative language such as YAML or JSON to define the desired state of your infrastructure. This approach allows you to specify your infrastructure and the IaC tool will automatically provision and configure the resources to meet those specifications.\"}),`\n`,(0,r.jsx)(e.li,{children:\"Using a procedural language such as Python or Ruby to write scripts that provision and configure your infrastructure. This approach allows you to specify the steps that need to be taken to provision and configure your infrastructure. The IaC tool will execute those steps to create the desired infrastructure.\"}),`\n`,(0,r.jsx)(e.li,{children:\"Using a configuration management tool such as Ansible or Puppet to manage your infrastructure. These tools provide a set of predefined modules to provision and configure your infrastructure in a repeatable and predictable way.\"}),`\n`]}),`\n`,(0,r.jsxs)(e.h2,{id:\"what-is-terraform\",children:[(0,r.jsx)(e.a,{className:\"anchor\",href:\"#what-is-terraform\",children:(0,r.jsx)(e.span,{className:\"icon icon-link\"})}),(0,r.jsx)(e.strong,{children:\"What is Terraform?\"})]}),`\n`,(0,r.jsx)(e.p,{children:\"Terraform is a tool for building, changing, and versioning infrastructure safely and efficiently. It can manage existing popular service providers and custom in-house solutions.\\xA0\"}),`\n`,(0,r.jsx)(e.p,{children:\"Terraform manages the complete state of your infrastructure with declarative configuration files, which are treated like code. The system keeps knowledge of rebuilding or changing any resource or infrastructure component. This makes it safer than a traditional on-site human expert who needs to guide every action.\\xA0\"}),`\n`,(0,r.jsx)(e.p,{children:(0,r.jsx)(e.img,{src:\"/blog-images/f8651fabc7d45f59142db03eb005c51c.png\",alt:\"\"})}),`\n`,(0,r.jsxs)(e.p,{children:[\"When applied with IaC practices such as continuous integration (CI), \",(0,r.jsx)(e.a,{href:\"https://releasehub.com/blog/terraform-kubernetes-deployment-a-detailed-walkthrough\",children:\"Terraform\"}),\" can become an increasingly powerful tool for rapidly provisioning new environments and then scaling them out safely while minimizing disruption to existing customers.\\xA0\"]}),`\n`,(0,r.jsx)(e.p,{children:\"Terraform's IaC feature keeps your infrastructure up to date with the latest versions of Terraform modules through updates and releases performed by other users.\\xA0\"}),`\n`,(0,r.jsxs)(e.h2,{id:\"what-code-is-terraform-written-in\",children:[(0,r.jsx)(e.a,{className:\"anchor\",href:\"#what-code-is-terraform-written-in\",children:(0,r.jsx)(e.span,{className:\"icon icon-link\"})}),(0,r.jsx)(e.strong,{children:\"What Code is Terraform Written in?\"})]}),`\n`,(0,r.jsxs)(e.p,{children:[\"Terraform is primarily written in the \",(0,r.jsx)(e.a,{href:\"https://go.dev/\",children:\"Go\"}),\" programming language. HashiCorp initially developed Go and it's now an open-source project with contributions from a community of users and developers.\\xA0\"]}),`\n`,(0,r.jsxs)(e.p,{children:[\"Besides Go, Terraform uses the \",(0,r.jsx)(e.a,{href:\"https://developer.hashicorp.com/terraform/language\",children:\"HashiCorp Configuration Language\"}),\" (HCL) as its primary configuration language.\\xA0\"]}),`\n`,(0,r.jsx)(e.p,{children:\"HCL is a declarative language that is human-readable and easy to write and understand. It's used to define the desired state of infrastructure, which Terraform uses to provision and configure the resources.\\xA0\"}),`\n`,(0,r.jsxs)(e.h2,{id:\"what-are-the-benefits-of-using-terraform\",children:[(0,r.jsx)(e.a,{className:\"anchor\",href:\"#what-are-the-benefits-of-using-terraform\",children:(0,r.jsx)(e.span,{className:\"icon icon-link\"})}),(0,r.jsx)(e.strong,{children:\"What Are the Benefits of Using Terraform?\"})]}),`\n`,(0,r.jsxs)(e.ul,{children:[`\n`,(0,r.jsx)(e.li,{children:\"Easy to use: You can codify existing system architecture into declarative configuration files. No more brittle shell scripts and manual operations.\"}),`\n`,(0,r.jsx)(e.li,{children:\"Fast: With Terraform, you can plan, create, and destroy infrastructure for hundreds of services in a matter of seconds. It doesn't require a central database, so it's also easy to install, no matter how big your environment is.\"}),`\n`,(0,r.jsx)(e.li,{children:\"Secure: Terraform provides locking mechanisms to prevent multiple teams from accidentally creating overlapping resources, which could easily happen with manual setups. Terraform also uses standard modules, so it's difficult to expose sensitive data like API keys or passwords accidentally.\"}),`\n`]}),`\n`,(0,r.jsx)(e.p,{children:(0,r.jsx)(e.img,{src:\"/blog-images/371353e8ac1d8b4e6c63f7fe811b457e.jpeg\",alt:\"\"})}),`\n`,(0,r.jsxs)(e.h2,{id:\"how-does-terraform-enable-iac\",children:[(0,r.jsx)(e.a,{className:\"anchor\",href:\"#how-does-terraform-enable-iac\",children:(0,r.jsx)(e.span,{className:\"icon icon-link\"})}),(0,r.jsx)(e.strong,{children:\"How Does Terraform Enable IaC?\"})]}),`\n`,(0,r.jsx)(e.p,{children:\"Terraform enables IaC by providing a declarative language and tools that allow organizations to define and manage their infrastructure using code.\\xA0\"}),`\n`,(0,r.jsx)(e.p,{children:\"Here's an example of how Terraform can enable IaC:\\xA0\"}),`\n`,(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:\"language-hcl\",children:`resource \"aws_instance\" \"example\" {\n  ami           = \"ami-0c55b159cbfafe1f0\"\n  instance_type = \"t2.micro\"\n}\n`})}),`\n`,(0,r.jsxs)(e.p,{children:[\"In this example, you define the \",(0,r.jsx)(e.strong,{children:\"aws_instance\"}),\" resource using HCL. This code specifies the desired state of the infrastructure: A virtual machine with a specific AMI and instance type.\\xA0\"]}),`\n`,(0,r.jsx)(e.p,{children:\"When you execute this code using the Terraform command-line interface (CLI), Terraform will provision and configure the resources in AWS to meet these specifications. The virtual machine will be created and configured automatically without requiring manual intervention.\\xA0\"}),`\n`,(0,r.jsxs)(e.h3,{id:\"key-features-of-terraform-that-support-iac\",children:[(0,r.jsx)(e.a,{className:\"anchor\",href:\"#key-features-of-terraform-that-support-iac\",children:(0,r.jsx)(e.span,{className:\"icon icon-link\"})}),(0,r.jsx)(e.strong,{children:\"Key Features of Terraform That Support IaC\"})]}),`\n`,(0,r.jsx)(e.p,{children:\"Below are some features of Terraform that support and enable IaC:\\xA0\"}),`\n`,(0,r.jsxs)(e.ul,{children:[`\n`,(0,r.jsx)(e.li,{children:\"You can use the HCL declarative language to define the desired state of your infrastructure. This allows you to specify how your infrastructure should look.\"}),`\n`,(0,r.jsx)(e.li,{children:\"Support for version control enables organizations to track changes to their infrastructure over time and collaborate with other developers on infrastructure management. This makes it easier to review and test changes to infrastructure and to roll back changes if necessary.\"}),`\n`,(0,r.jsx)(e.li,{children:\"The resource graph allows you to visualize and understand your entire infrastructure.\"}),`\n`,(0,r.jsx)(e.li,{children:\"Execution plans created in Terraform show you the actions that will be required to reach your desired state.\"}),`\n`,(0,r.jsx)(e.li,{children:\"Terraform can also orchestrate multiple services and providers, allowing you to manage your resources across different cloud platforms.\"}),`\n`]}),`\n`,(0,r.jsxs)(e.h2,{id:\"is-terraform-an-iac\",children:[(0,r.jsx)(e.a,{className:\"anchor\",href:\"#is-terraform-an-iac\",children:(0,r.jsx)(e.span,{className:\"icon icon-link\"})}),(0,r.jsx)(e.strong,{children:\"Is Terraform an IaC?\"})]}),`\n`,(0,r.jsxs)(e.p,{children:[\"No, but it's an IaC \",(0,r.jsx)(e.em,{children:\"tool\"}),\". IaC is a practice. Terraform is an IaC tool that enables organizations to manage their infrastructure using code.\\xA0\"]}),`\n`,(0,r.jsxs)(e.h2,{id:\"what-other-tools-can-you-use-to-implement-iac\",children:[(0,r.jsx)(e.a,{className:\"anchor\",href:\"#what-other-tools-can-you-use-to-implement-iac\",children:(0,r.jsx)(e.span,{className:\"icon icon-link\"})}),(0,r.jsx)(e.strong,{children:\"What Other Tools can you use to Implement IaC?\"})]}),`\n`,(0,r.jsx)(e.p,{children:\"Other tools that can implement IaC include:\\xA0\"}),`\n`,(0,r.jsx)(e.p,{children:\"1. Ansible\\xA0\"}),`\n`,(0,r.jsx)(e.p,{children:\"2. Puppet\\xA0\"}),`\n`,(0,r.jsx)(e.p,{children:\"3. Progress Chef\\xA0\"}),`\n`,(0,r.jsx)(e.p,{children:\"4. SaltStack\\xA0\"}),`\n`,(0,r.jsx)(e.p,{children:\"5. CloudFormation\\xA0\"}),`\n`,(0,r.jsx)(e.p,{children:\"6. Vagrant\\xA0\"}),`\n`,(0,r.jsxs)(e.h2,{id:\"conclusion\",children:[(0,r.jsx)(e.a,{className:\"anchor\",href:\"#conclusion\",children:(0,r.jsx)(e.span,{className:\"icon icon-link\"})}),(0,r.jsx)(e.strong,{children:\"Conclusion\"})]}),`\n`,(0,r.jsxs)(e.p,{children:[\"With Terraform, your team can easily provision and manage their IaC. They'll scale their resources and deploy applications across different environments. Terraform also allows you to take advantage of different environments, like ephemeral environments. Maybe you'd like to spin up an environment\\u2014for example, to show a certain feature or product\\u2014without worrying that someone else will mess it up. Follow \",(0,r.jsx)(e.a,{href:\"https://releasehub.com/ephemeral-environments\",children:\"this guide\"}),\" to get started with ephemeral environments.\\xA0\"]}),`\n`,(0,r.jsx)(e.p,{children:(0,r.jsx)(e.em,{children:\"Discover the benefits of using infrastructure as code (IaC) with Terraform! IaC enables organizations to manage their infrastructure in a more predictable and repeatable way, which can improve the reliability and maintainability of their infrastructure. Learn more with Release. #IaC #Terraform #Ephemeral Environment\"})})]})}function T(a={}){let{wrapper:e}=a.components||{};return e?(0,r.jsx)(e,Object.assign({},a,{children:(0,r.jsx)(h,a)})):h(a)}var C=T;return b(I);})();\n;return Component;"
        },
        "_id": "blog/posts/how-does-terraform-enable-infrastructure-as-code.mdx",
        "_raw": {
          "sourceFilePath": "blog/posts/how-does-terraform-enable-infrastructure-as-code.mdx",
          "sourceFileName": "how-does-terraform-enable-infrastructure-as-code.mdx",
          "sourceFileDir": "blog/posts",
          "contentType": "mdx",
          "flattenedPath": "blog/posts/how-does-terraform-enable-infrastructure-as-code"
        },
        "type": "BlogPost",
        "computedSlug": "how-does-terraform-enable-infrastructure-as-code"
      },
      "documentHash": "1739393595020",
      "hasWarnings": false,
      "documentTypeName": "BlogPost"
    },
    "blog/posts/how-release-uses-action-cable-redux-toolkit.mdx": {
      "document": {
        "title": "How Release Uses Action Cable and Redux Toolkit",
        "summary": "Walk through setting up Action Cable messages that are received by a React Component hooked up to Redux Toolkit.",
        "publishDate": "Wed Jul 14 2021 14:45:06 GMT+0000 (Coordinated Universal Time)",
        "author": "jeremy-kreutzbender",
        "readingTime": 8,
        "categories": [
          "platform-engineering",
          "product"
        ],
        "mainImage": "/blog-images/4a62d2a6629b891fdae6ce6926d1fb05.jpg",
        "imageAlt": "A toolkit that includes several tools such as hammer, saw and file",
        "showCTA": true,
        "ctaCopy": "Enhance your Redux setup with Release's ephemeral environments for seamless collaboration and efficient testing.",
        "ctaLink": "https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=how-release-uses-action-cable-redux-toolkit",
        "relatedPosts": [
          ""
        ],
        "ogImage": "/blog-images/4a62d2a6629b891fdae6ce6926d1fb05.jpg",
        "excerpt": "Walk through setting up Action Cable messages that are received by a React Component hooked up to Redux Toolkit.",
        "tags": [
          "platform-engineering",
          "product"
        ],
        "ctaButton": "Try Release for Free",
        "body": {
          "raw": "\nOver the past few weeks at [Release](https://releasehub.com) the Frontend Engineering team has started working on adding Redux to Release. We had been making use of [React Context](https://reactjs.org/docs/context.html) but felt that we were starting to stretch its capabilities. In some places we were having to add multiple providers to implement new features. After some research on the current state of Redux, we decided to go with [Redux Toolkit](https://redux-toolkit.js.org/) and [Redux Saga](https://redux-saga.js.org/). Moving all our data into the Redux store and out of local state meant that we were going to have to change our approach with [Action Cable](https://guides.rubyonrails.org/action_cable_overview.html) and how we were going to receive the messages, store them, and display changes for the user.\n\n### Action Cable, Redux, and Release\n\nRelease uses Action Cable in a single direction, which is from the backend to the frontend. The frontend is a separate React application running as a [Static Service Application](https://docs.releasehub.com/reference-guide/static-service-deployment), not a part of Rails. The backend will send messages to the frontend when the state of objects change or to stream logs of deployments and builds. Today we're going to go through the thought process, including code snippets, of how we set up our Redux implementation for Action Cable when Release builds a Docker image. If you’re curious about how Release builds Docker images, read about we [Cut Build Time In Half with Docker’s Buildx Kubernetes Driver](https://releasehub.com/blog/cutting-build-time-in-half-docker-buildx-kubernetes).\n\n### Action Cable Setup\n\nLet’s start off with how we set up the backend to send updates as a **_Build_** object progresses. We have two **_ActiveRecord_** models to consider in this scenario, **_Build_**, and **_Log_**. The **_Build_** class includes the [aasm](https://github.com/aasm/aasm) gem functionality to progress it through the lifecycle of actually creating a Docker build. The following is an extremely pared down version of our **_Build_** class, but has enough information to explain how we’re sending the Action Cable messages.\n\n```ruby line-numbers\n\nclass Build < ApplicationRecord\n  include AASM\n  include Logging\n\n  has_many :logs\n\n  aasm use_transactions: false do\n    state :ready, initial: true\n    state :running, after_enter: Proc.new { update_started_at; log_start }\n    state :done, after_enter: Proc.new { set_duration; log_done }\n    state :errored, after_enter: Proc.new { set_duration; log_error }\n\n    event :start do\n      transitions from: [:ready], to: :running\n    end\n\n    event :finish do\n      transitions from: [:running], to: :done\n    end\n\n    event :error do\n      transitions from: [:running], to: :errored\n    end\n\n  def log_start\n    message = \"Build starting for #{repository.name}!\"\n    log_it(:info, message, metadata: log_metadata)\n  end\n\n  def log_done\n    message = \"Build finished for #{repository.name}!\"\n    log_it(:info, message, metadata: log_metadata)\n  end\n\n  def log_error\n    message = \"Build errored for #{repository.name}!\"\n    log_it(:error, message, metadata: log_metadata)\n  end\n\n  def log_metadata\n    {\n      build_id: self.id,\n      aasm_state: self.aasm_state,\n      started_at: self.started_at,\n      duration: self.total_duration\n    }\n  end\n\n  def logs_channel\n    \"build_channel_#{self.id}\"\n  end\nend\n\n```\n\nWhenever the **_Build_** transitions its state, we create a **_Log_** record through the _log_it_ method. A log level is supplied, along with the message, and metadata about the **_Build_** itself. That metadata is used by the frontend to make changes for the user as you’ll see when we go through the Redux code. _log_it_ also sends the message to the _logs_channel_ through Action Cable. Since that wasn’t defined above, let’s look at that now.\n\n```ruby\n\nmodule Logging\n  module Log\n    def log_it(level, message, metadata: {})\n      log_hash = {\n        level: level,\n        message: message.dup.force_encoding('UTF-8')\n      }\n\n      self.logs << ::Log.new(log_hash)\n\n      payload = log_hash.merge(metadata)\n      ActionCable.server.broadcast(logs_channel, payload)\n    end\n  end\nend\n\n```\n\nThere is not too much to it. We create the **_Log_** record and ensure the message is properly encoded. Then we combine the level, message, and supplied metadata to Action Cable and broadcast it. We use the _log_it_ method with more classes than just **_Build_** and have found it makes for an easy and reliable way to store and send messages.\n\nThat takes care of our state transitions. The last piece needed to wrap up our backend setup is to create the **_BuildChannel_**.\n\n```ruby\n\nclass BuildChannel < ApplicationCable::Channel\n  def subscribed\n    Rails.logger.info \"Subscribing to: build_channel_#{params['room']}\"\n    stream_from \"build_channel_#{params['room']}\"\n  end\nend\n\n```\n\nThe method receives a room parameter to ensure we are sending messages about a specific **_Build_** and does not go to everyone. I like to have the logging message in there so that it is easy to tell in the Rails logs if the frontend has successfully connected to the channel. With all that covered, we’re ready to dive into the setup on the frontend to receive those messages!\n\n### Redux Setup\n\n![](/blog-images/68dcb6fe704aa06c5c855ffbdcf25ff8.png)\n\nRelease Build Screen with Logs\n\nAs you’ll recall we’re using Redux Toolkit and we’re not going to cover our entire setup with Toolkit, instead focusing only on the portions relevant to updating the **_Build_** when we receive an Action Cable message. From there we’ll go over a small wrapper component we made to handle receiving the Action Cable messages and tie it all together with a small demo component.\n\nWe’ll start off with the **_BuildsSlice_**.\n\n```javascript\nimport { createSlice } from \"@reduxjs/toolkit\";\n\nimport { handleBuildMessageReceived } from \"./helpers/actionCable/builds\";\n\nconst initialState = {\n  activeBuild: undefined, // object\n};\n\nexport const buildsSlice = createSlice({\n  updateBuildFromMessage(state, action) {\n    const message = action.payload;\n\n    const build = state.activeBuild;\n    const newBuild = handleBuildMessageReceived(build, message);\n\n    return {\n      ...state,\n      activeBuild: newBuild,\n    };\n  },\n});\n\nexport const { updateBuildFromMessage } = buildsSlice.actions;\n\nexport default buildsSlice.reducer;\n```\n\nYou’ll notice that we import _handleBuildMessageReceived_ from a file under _helpers/actionCable_. We wanted to separate out the code for the logic of updating the build from the slice itself so that our slice file does not grow too enormous. Other than that, the slice itself follows the suggested setup of a slice from the [createSlice](https://redux-toolkit.js.org/api/createslice) documentation.\n\nNow we need to look at our _handleBuildMessageReceived_ function.\n\n```javascript\nconst handleBuildMessageReceived = (build, message) => {\n  const buildId = message[\"build_id\"];\n  const aasmState = message[\"aasm_state\"];\n  const duration = message[\"duration\"];\n  const startedAt = message[\"started_at\"];\n  const level = message[\"level\"];\n  const messageLog = message[\"message\"];\n  const logs = build.logs;\n\n  if (build.id !== buildId) {\n    return build;\n  } else {\n    const newLogLine = { level: level, message: messageLog };\n    const newBuild = {\n      ...build,\n      logs: [...logs, newLogLine],\n      aasm_state: aasmState || build.aasm_state,\n      total_duration: duration || build.total_duration,\n      started_at: startedAt || build.started_at,\n    };\n    return newBuild;\n  }\n};\n\nexport { handleBuildMessageReceived };\n```\n\nFirst a sanity check is done to ensure we didn’t somehow receive a message for a **_Build_** that we aren’t viewing. This shouldn’t happen because we open and close our Action Cable subscriptions when we enter and leave a page, but an extra check never hurts. Then we construct a new **_Build_** object by appending the new log line and adding the metadata. If the metadata fields are _undefined_, we’ll retain what the _build_ variable already had.\n\nWe’re ready to receive messages so we need a component that will handle that for us. The **_ActionCableWrapper_** component is just that.\n\n```javascript\nimport React, { useEffect, useState } from \"react\";\nimport actionCable from \"actioncable\";\n\nexport default function ActionCableWrapper({ channel, room, onReceived }) {\n  const [actionCableConsumer, setActionCableConsumer] = useState(undefined);\n\n  useEffect(() => {\n    if (!actionCableConsumer) {\n      setActionCableConsumer(\n        actionCable.createConsumer(\"ws://localhost:3000/cable\"),\n      );\n    } else {\n      actionCableConsumer.subscriptions.create(\n        { channel, room },\n        {\n          received: onReceived,\n        },\n      );\n    }\n\n    return () => {\n      if (actionCableConsumer) {\n        actionCableConsumer.disconnect();\n      }\n    };\n  }, [actionCableConsumer]);\n\n  return <></>;\n}\n```\n\nThis component will mount and check to see if _actionCableConsumer_ is not _undefined_. However, if it is _undefined_, which it will be on the first pass through the _useEffect_, we will create a consumer through _actionCable.createConsumer_ connecting to a _/cable_ endpoint. _\"ws://localhost:3000/cable\"_ is hard coded but the URL should come from an environment variable so the component works locally or in production. That consumer is set into the local state _actionCableConsumer_ and the _useEffect_ will trigger a second time.\n\nIn the second pass through, the _else_ block is entered and a subscription is created with the passed in _channel_, _room_, and _onReceived_ properties. The _return_ function is set to call _disconnect()_ if we have an _actionCableConsumer_ set and will ensure that no web socket connections are left open if a user navigates away from the page. With that, we have a reusable component that will take care of our Action Cable needs throughout the application.\n\nPulling it all together, we can create a demo component that will display the state and logs and update whenever it receives a message.\n\n```javascript\nimport React from \"react\";\nimport { useDispatch, useSelector } from \"react-redux\";\n\nimport { Grid } from \"@material-ui/core\";\n\nimport ActionCableWrapper from \"../ActionCableWrapper\";\n\nimport { updateBuildFromMessage } from \"redux/slices/builds\";\n\nexport default function BuildDetailsCard(props) {\n  const dispatch = useDispatch();\n  const build = useSelector((state) => state.builds.activeBuild);\n\n  const handleMessageReceived = (message) =>\n    dispatch(updateBuildFromMessage(message));\n\n  return (\n    <>\n           {\" \"}\n      <ActionCableWrapper\n        channel=\"BuildChannel\"\n        room={build.id}\n        onReceived={handleMessageReceived}\n      />\n      >      {\" \"}\n      <Grid container>\n               {\" \"}\n        <Grid item xs={3}>\n                   {\" \"}\n          <div>\n                        <b>Repository Name: </b> {build.repository.name}       \n             {\" \"}\n          </div>\n                   {\" \"}\n          <div>\n                        <b>Commit Message: </b> {build.commit_message}         {\" \"}\n          </div>\n                   {\" \"}\n          <div>\n                        <b>Commit SHA: </b> {build.commit_short}         {\" \"}\n          </div>\n                   {\" \"}\n          <div>\n                        <b>State: </b> {build.aasm_state}         {\" \"}\n          </div>\n                 {\" \"}\n        </Grid>\n               {\" \"}\n        <Grid\n          item\n          xs={9}\n          style={{\n            border: \"2px\",\n            backgroundColor: \"#343a40\",\n            fontSize: \"0.9rem\",\n            fontFamily: \"Monaco\",\n            color: \"white\",\n            padding: 10,\n          }}\n        >\n                   \n          {build.logs.map((log) => (\n            <div>{log.message} </div>\n          ))}\n                 {\" \"}\n        </Grid>\n             {\" \"}\n      </Grid>\n         \n    </>\n  );\n}\n```\n\nFor demo purposes I probably went a little overboard with the styling, but I wanted to create something that resembles our actual application which you saw at the start of this post. The two things needed to power the page are the _build_, which is retrieved with _useSelector_ and the _handleMessageReceived_ function, which dispatches _updateBuildFromMessage_ every time we receive a message through Action Cable. We supply the _”BuildChannel”_ and _build.id_ as the channel and room to _ActionCableWrapper_ along with _handleMessageReceived_ as the _onReceived_ function.\n\nIn the video below I’ll move the build through its different states and we’ll be able to see the frontend receive the messages, update the state, and add the logs to the screen.\n\n![](/blog-images/57d547202f306ddf41f2b8df93ed8650.gif)\n\n### Conclusion\n\nThat's a wrap on my adventure into how we set up our Action Cable integration with Redux Toolkit. There are tons of places in the application we’re going to be adding live updates too so that our users will always be up to date on the state of their application. I hope you enjoyed taking a peek inside some development work at Release. If you're interested in having an ephemeral environment created whenever we receive a Pull Request webhook from your Repository, head on over to the [homepage](https://release.com) and sign up! If you’d like to join our awesome team, check out our [job listings](https://releasehub.com/company).\n",
          "code": "var Component=(()=>{var h=Object.create;var i=Object.defineProperty;var u=Object.getOwnPropertyDescriptor;var m=Object.getOwnPropertyNames;var g=Object.getPrototypeOf,p=Object.prototype.hasOwnProperty;var b=(t,e)=>()=>(e||t((e={exports:{}}).exports,e),e.exports),f=(t,e)=>{for(var a in e)i(t,a,{get:e[a],enumerable:!0})},r=(t,e,a,s)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let o of m(e))!p.call(t,o)&&o!==a&&i(t,o,{get:()=>e[o],enumerable:!(s=u(e,o))||s.enumerable});return t};var w=(t,e,a)=>(a=t!=null?h(g(t)):{},r(e||!t||!t.__esModule?i(a,\"default\",{value:t,enumerable:!0}):a,t)),v=t=>r(i({},\"__esModule\",{value:!0}),t);var d=b((x,l)=>{l.exports=_jsx_runtime});var R={};f(R,{default:()=>y,frontmatter:()=>C});var n=w(d()),C={title:\"How Release Uses Action Cable and Redux Toolkit\",summary:\"Walk through setting up Action Cable messages that are received by a React Component hooked up to Redux Toolkit.\",publishDate:\"Wed Jul 14 2021 14:45:06 GMT+0000 (Coordinated Universal Time)\",author:\"jeremy-kreutzbender\",readingTime:8,categories:[\"platform-engineering\",\"product\"],mainImage:\"/blog-images/4a62d2a6629b891fdae6ce6926d1fb05.jpg\",imageAlt:\"A toolkit that includes several tools such as hammer, saw and file\",showCTA:!0,ctaCopy:\"Enhance your Redux setup with Release's ephemeral environments for seamless collaboration and efficient testing.\",ctaLink:\"https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=how-release-uses-action-cable-redux-toolkit\",relatedPosts:[\"\"],ogImage:\"/blog-images/4a62d2a6629b891fdae6ce6926d1fb05.jpg\",excerpt:\"Walk through setting up Action Cable messages that are received by a React Component hooked up to Redux Toolkit.\",tags:[\"platform-engineering\",\"product\"],ctaButton:\"Try Release for Free\"};function c(t){let e=Object.assign({p:\"p\",a:\"a\",h3:\"h3\",span:\"span\",strong:\"strong\",em:\"em\",pre:\"pre\",code:\"code\",img:\"img\"},t.components);return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsxs)(e.p,{children:[\"Over the past few weeks at \",(0,n.jsx)(e.a,{href:\"https://releasehub.com\",children:\"Release\"}),\" the Frontend Engineering team has started working on adding Redux to Release. We had been making use of \",(0,n.jsx)(e.a,{href:\"https://reactjs.org/docs/context.html\",children:\"React Context\"}),\" but felt that we were starting to stretch its capabilities. In some places we were having to add multiple providers to implement new features. After some research on the current state of Redux, we decided to go with \",(0,n.jsx)(e.a,{href:\"https://redux-toolkit.js.org/\",children:\"Redux Toolkit\"}),\" and \",(0,n.jsx)(e.a,{href:\"https://redux-saga.js.org/\",children:\"Redux Saga\"}),\". Moving all our data into the Redux store and out of local state meant that we were going to have to change our approach with \",(0,n.jsx)(e.a,{href:\"https://guides.rubyonrails.org/action_cable_overview.html\",children:\"Action Cable\"}),\" and how we were going to receive the messages, store them, and display changes for the user.\"]}),`\n`,(0,n.jsxs)(e.h3,{id:\"action-cable-redux-and-release\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#action-cable-redux-and-release\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Action Cable, Redux, and Release\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"Release uses Action Cable in a single direction, which is from the backend to the frontend. The frontend is a separate React application running as a \",(0,n.jsx)(e.a,{href:\"https://docs.releasehub.com/reference-guide/static-service-deployment\",children:\"Static Service Application\"}),\", not a part of Rails. The backend will send messages to the frontend when the state of objects change or to stream logs of deployments and builds. Today we're going to go through the thought process, including code snippets, of how we set up our Redux implementation for Action Cable when Release builds a Docker image. If you\\u2019re curious about how Release builds Docker images, read about we \",(0,n.jsx)(e.a,{href:\"https://releasehub.com/blog/cutting-build-time-in-half-docker-buildx-kubernetes\",children:\"Cut Build Time In Half with Docker\\u2019s Buildx Kubernetes Driver\"}),\".\"]}),`\n`,(0,n.jsxs)(e.h3,{id:\"action-cable-setup\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#action-cable-setup\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Action Cable Setup\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"Let\\u2019s start off with how we set up the backend to send updates as a \",(0,n.jsx)(e.strong,{children:(0,n.jsx)(e.em,{children:\"Build\"})}),\" object progresses. We have two \",(0,n.jsx)(e.strong,{children:(0,n.jsx)(e.em,{children:\"ActiveRecord\"})}),\" models to consider in this scenario, \",(0,n.jsx)(e.strong,{children:(0,n.jsx)(e.em,{children:\"Build\"})}),\", and \",(0,n.jsx)(e.strong,{children:(0,n.jsx)(e.em,{children:\"Log\"})}),\". The \",(0,n.jsx)(e.strong,{children:(0,n.jsx)(e.em,{children:\"Build\"})}),\" class includes the \",(0,n.jsx)(e.a,{href:\"https://github.com/aasm/aasm\",children:\"aasm\"}),\" gem functionality to progress it through the lifecycle of actually creating a Docker build. The following is an extremely pared down version of our \",(0,n.jsx)(e.strong,{children:(0,n.jsx)(e.em,{children:\"Build\"})}),\" class, but has enough information to explain how we\\u2019re sending the Action Cable messages.\"]}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-ruby\",children:`\nclass Build < ApplicationRecord\n \\xA0include AASM\n \\xA0include Logging\n\n \\xA0has_many :logs\n\n \\xA0aasm use_transactions: false do\n \\xA0 \\xA0state :ready, initial: true\n \\xA0 \\xA0state :running, after_enter: Proc.new { update_started_at; log_start }\n \\xA0 \\xA0state :done, after_enter: Proc.new { set_duration; log_done }\n \\xA0 \\xA0state :errored, after_enter: Proc.new { set_duration; log_error }\n\n \\xA0 \\xA0event :start do\n \\xA0 \\xA0 \\xA0transitions from: [:ready], to: :running\n \\xA0 \\xA0end\n\n \\xA0 \\xA0event :finish do\n \\xA0 \\xA0 \\xA0transitions from: [:running], to: :done\n \\xA0 \\xA0end\n\n \\xA0 \\xA0event :error do\n \\xA0 \\xA0 \\xA0transitions from: [:running], to: :errored\n \\xA0 \\xA0end\n\n \\xA0def log_start\n \\xA0 \\xA0message = \"Build starting for #{repository.name}!\"\n \\xA0 \\xA0log_it(:info, message, metadata: log_metadata)\n \\xA0end\n\n \\xA0def log_done\n \\xA0 \\xA0message = \"Build finished for #{repository.name}!\"\n \\xA0 \\xA0log_it(:info, message, metadata: log_metadata)\n \\xA0end\n\n \\xA0def log_error\n \\xA0 \\xA0message = \"Build errored for #{repository.name}!\"\n \\xA0 \\xA0log_it(:error, message, metadata: log_metadata)\n \\xA0end\n\n \\xA0def log_metadata\n \\xA0 \\xA0{\n \\xA0 \\xA0 \\xA0build_id: self.id,\n \\xA0 \\xA0 \\xA0aasm_state: self.aasm_state,\n \\xA0 \\xA0 \\xA0started_at: self.started_at,\n \\xA0 \\xA0 \\xA0duration: self.total_duration\n \\xA0 \\xA0}\n \\xA0end\n\n \\xA0def logs_channel\n \\xA0 \\xA0\"build_channel_#{self.id}\"\n \\xA0end\nend\n\n`})}),`\n`,(0,n.jsxs)(e.p,{children:[\"Whenever the \",(0,n.jsx)(e.strong,{children:(0,n.jsx)(e.em,{children:\"Build\"})}),\" transitions its state, we create a \",(0,n.jsx)(e.strong,{children:(0,n.jsx)(e.em,{children:\"Log\"})}),\" record through the \",(0,n.jsx)(e.em,{children:\"log_it\"}),\" method. A log level is supplied, along with the message, and metadata about the \",(0,n.jsx)(e.strong,{children:(0,n.jsx)(e.em,{children:\"Build\"})}),\" itself. That metadata is used by the frontend to make changes for the user as you\\u2019ll see when we go through the Redux code. \",(0,n.jsx)(e.em,{children:\"log_it\"}),\" also sends the message to the \",(0,n.jsx)(e.em,{children:\"logs_channel\"}),\" through Action Cable. Since that wasn\\u2019t defined above, let\\u2019s look at that now.\"]}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-ruby\",children:`\nmodule Logging\n \\xA0module Log\n \\xA0 \\xA0def log_it(level, message, metadata: {})\n \\xA0 \\xA0 \\xA0log_hash = {\n \\xA0 \\xA0 \\xA0 \\xA0level: level,\n \\xA0 \\xA0 \\xA0 \\xA0message: message.dup.force_encoding('UTF-8')\n \\xA0 \\xA0 \\xA0}\n\n \\xA0 \\xA0 \\xA0self.logs << ::Log.new(log_hash)\n\n \\xA0 \\xA0 \\xA0payload = log_hash.merge(metadata)\n \\xA0 \\xA0 \\xA0ActionCable.server.broadcast(logs_channel, payload)\n \\xA0 \\xA0end\n \\xA0end\nend\n\n`})}),`\n`,(0,n.jsxs)(e.p,{children:[\"There is not too much to it. We create the \",(0,n.jsx)(e.strong,{children:(0,n.jsx)(e.em,{children:\"Log\"})}),\" record and ensure the message is properly encoded. Then we combine the level, message, and supplied metadata to Action Cable and broadcast it. We use the \",(0,n.jsx)(e.em,{children:\"log_it\"}),\" method with more classes than just \",(0,n.jsx)(e.strong,{children:(0,n.jsx)(e.em,{children:\"Build\"})}),\" and have found it makes for an easy and reliable way to store and send messages.\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"That takes care of our state transitions. The last piece needed to wrap up our backend setup is to create the \",(0,n.jsx)(e.strong,{children:(0,n.jsx)(e.em,{children:\"BuildChannel\"})}),\".\"]}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-ruby\",children:`\nclass BuildChannel < ApplicationCable::Channel\n \\xA0def subscribed\n \\xA0 \\xA0Rails.logger.info \"Subscribing to: build_channel_#{params['room']}\"\n \\xA0 \\xA0stream_from \"build_channel_#{params['room']}\"\n \\xA0end\nend\n\n`})}),`\n`,(0,n.jsxs)(e.p,{children:[\"The method receives a room parameter to ensure we are sending messages about a specific \",(0,n.jsx)(e.strong,{children:(0,n.jsx)(e.em,{children:\"Build\"})}),\" and does not go to everyone. I like to have the logging message in there so that it is easy to tell in the Rails logs if the frontend has successfully connected to the channel. With all that covered, we\\u2019re ready to dive into the setup on the frontend to receive those messages!\"]}),`\n`,(0,n.jsxs)(e.h3,{id:\"redux-setup\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#redux-setup\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Redux Setup\"]}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/68dcb6fe704aa06c5c855ffbdcf25ff8.png\",alt:\"\"})}),`\n`,(0,n.jsx)(e.p,{children:\"Release Build Screen with Logs\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"As you\\u2019ll recall we\\u2019re using Redux Toolkit and we\\u2019re not going to cover our entire setup with Toolkit, instead focusing only on the portions relevant to updating the \",(0,n.jsx)(e.strong,{children:(0,n.jsx)(e.em,{children:\"Build\"})}),\" when we receive an Action Cable message. From there we\\u2019ll go over a small wrapper component we made to handle receiving the Action Cable messages and tie it all together with a small demo component.\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"We\\u2019ll start off with the \",(0,n.jsx)(e.strong,{children:(0,n.jsx)(e.em,{children:\"BuildsSlice\"})}),\".\"]}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-javascript\",children:`import { createSlice } from \"@reduxjs/toolkit\";\n\nimport { handleBuildMessageReceived } from \"./helpers/actionCable/builds\";\n\nconst initialState = {\n  activeBuild: undefined, // object\n};\n\nexport const buildsSlice = createSlice({\n  updateBuildFromMessage(state, action) {\n    const message = action.payload;\n\n    const build = state.activeBuild;\n    const newBuild = handleBuildMessageReceived(build, message);\n\n    return {\n      ...state,\n      activeBuild: newBuild,\n    };\n  },\n});\n\nexport const { updateBuildFromMessage } = buildsSlice.actions;\n\nexport default buildsSlice.reducer;\n`})}),`\n`,(0,n.jsxs)(e.p,{children:[\"You\\u2019ll notice that we import \",(0,n.jsx)(e.em,{children:\"handleBuildMessageReceived\"}),\" from a file under \",(0,n.jsx)(e.em,{children:\"helpers/actionCable\"}),\". We wanted to separate out the code for the logic of updating the build from the slice itself so that our slice file does not grow too enormous. Other than that, the slice itself follows the suggested setup of a slice from the \",(0,n.jsx)(e.a,{href:\"https://redux-toolkit.js.org/api/createslice\",children:\"createSlice\"}),\" documentation.\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"Now we need to look at our \",(0,n.jsx)(e.em,{children:\"handleBuildMessageReceived\"}),\" function.\"]}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-javascript\",children:`const handleBuildMessageReceived = (build, message) => {\n  const buildId = message[\"build_id\"];\n  const aasmState = message[\"aasm_state\"];\n  const duration = message[\"duration\"];\n  const startedAt = message[\"started_at\"];\n  const level = message[\"level\"];\n  const messageLog = message[\"message\"];\n  const logs = build.logs;\n\n  if (build.id !== buildId) {\n    return build;\n  } else {\n    const newLogLine = { level: level, message: messageLog };\n    const newBuild = {\n      ...build,\n      logs: [...logs, newLogLine],\n      aasm_state: aasmState || build.aasm_state,\n      total_duration: duration || build.total_duration,\n      started_at: startedAt || build.started_at,\n    };\n    return newBuild;\n  }\n};\n\nexport { handleBuildMessageReceived };\n`})}),`\n`,(0,n.jsxs)(e.p,{children:[\"First a sanity check is done to ensure we didn\\u2019t somehow receive a message for a \",(0,n.jsx)(e.strong,{children:(0,n.jsx)(e.em,{children:\"Build\"})}),\" that we aren\\u2019t viewing. This shouldn\\u2019t happen because we open and close our Action Cable subscriptions when we enter and leave a page, but an extra check never hurts. Then we construct a new \",(0,n.jsx)(e.strong,{children:(0,n.jsx)(e.em,{children:\"Build\"})}),\" object by appending the new log line and adding the metadata. If the metadata fields are \",(0,n.jsx)(e.em,{children:\"undefined\"}),\", we\\u2019ll retain what the \",(0,n.jsx)(e.em,{children:\"build\"}),\" variable already had.\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"We\\u2019re ready to receive messages so we need a component that will handle that for us. The \",(0,n.jsx)(e.strong,{children:(0,n.jsx)(e.em,{children:\"ActionCableWrapper\"})}),\" component is just that.\"]}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-javascript\",children:`import React, { useEffect, useState } from \"react\";\nimport actionCable from \"actioncable\";\n\nexport default function ActionCableWrapper({ channel, room, onReceived }) {\n  const [actionCableConsumer, setActionCableConsumer] = useState(undefined);\n\n  useEffect(() => {\n    if (!actionCableConsumer) {\n      setActionCableConsumer(\n        actionCable.createConsumer(\"ws://localhost:3000/cable\"),\n      );\n    } else {\n      actionCableConsumer.subscriptions.create(\n        { channel, room },\n        {\n          received: onReceived,\n        },\n      );\n    }\n\n    return () => {\n      if (actionCableConsumer) {\n        actionCableConsumer.disconnect();\n      }\n    };\n  }, [actionCableConsumer]);\n\n  return <></>;\n}\n`})}),`\n`,(0,n.jsxs)(e.p,{children:[\"This component will mount and check to see if \",(0,n.jsx)(e.em,{children:\"actionCableConsumer\"}),\" is not \",(0,n.jsx)(e.em,{children:\"undefined\"}),\". However, if it is \",(0,n.jsx)(e.em,{children:\"undefined\"}),\", which it will be on the first pass through the \",(0,n.jsx)(e.em,{children:\"useEffect\"}),\", we will create a consumer through \",(0,n.jsx)(e.em,{children:\"actionCable.createConsumer\"}),\" connecting to a \",(0,n.jsx)(e.em,{children:\"/cable\"}),\" endpoint. \",(0,n.jsx)(e.em,{children:'\"ws://localhost:3000/cable\"'}),\" is hard coded but the URL should come from an environment variable so the component works locally or in production. That consumer is set into the local state \",(0,n.jsx)(e.em,{children:\"actionCableConsumer\"}),\" and the \",(0,n.jsx)(e.em,{children:\"useEffect\"}),\" will trigger a second time.\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"In the second pass through, the \",(0,n.jsx)(e.em,{children:\"else\"}),\" block is entered and a subscription is created with the passed in \",(0,n.jsx)(e.em,{children:\"channel\"}),\", \",(0,n.jsx)(e.em,{children:\"room\"}),\", and \",(0,n.jsx)(e.em,{children:\"onReceived\"}),\" properties. The \",(0,n.jsx)(e.em,{children:\"return\"}),\" function is set to call \",(0,n.jsx)(e.em,{children:\"disconnect()\"}),\" if we have an \",(0,n.jsx)(e.em,{children:\"actionCableConsumer\"}),\" set and will ensure that no web socket connections are left open if a user navigates away from the page. With that, we have a reusable component that will take care of our Action Cable needs throughout the application.\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Pulling it all together, we can create a demo component that will display the state and logs and update whenever it receives a message.\"}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-javascript\",children:`import React from \"react\";\nimport { useDispatch, useSelector } from \"react-redux\";\n\nimport { Grid } from \"@material-ui/core\";\n\nimport ActionCableWrapper from \"../ActionCableWrapper\";\n\nimport { updateBuildFromMessage } from \"redux/slices/builds\";\n\nexport default function BuildDetailsCard(props) {\n  const dispatch = useDispatch();\n  const build = useSelector((state) => state.builds.activeBuild);\n\n  const handleMessageReceived = (message) =>\n    dispatch(updateBuildFromMessage(message));\n\n  return (\n    <>\n      \\xA0 \\xA0 \\xA0{\" \"}\n      <ActionCableWrapper\n        channel=\"BuildChannel\"\n        room={build.id}\n        onReceived={handleMessageReceived}\n      />\n      > \\xA0 \\xA0 \\xA0{\" \"}\n      <Grid container>\n        \\xA0 \\xA0 \\xA0 \\xA0{\" \"}\n        <Grid item xs={3}>\n          \\xA0 \\xA0 \\xA0 \\xA0 \\xA0{\" \"}\n          <div>\n            \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 <b>Repository Name: </b> {build.repository.name}\\xA0 \\xA0 \\xA0 \\xA0\n            \\xA0{\" \"}\n          </div>\n          \\xA0 \\xA0 \\xA0 \\xA0 \\xA0{\" \"}\n          <div>\n            \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 <b>Commit Message: </b> {build.commit_message}\\xA0 \\xA0 \\xA0 \\xA0 \\xA0{\" \"}\n          </div>\n          \\xA0 \\xA0 \\xA0 \\xA0 \\xA0{\" \"}\n          <div>\n            \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 <b>Commit SHA: </b> {build.commit_short}\\xA0 \\xA0 \\xA0 \\xA0 \\xA0{\" \"}\n          </div>\n          \\xA0 \\xA0 \\xA0 \\xA0 \\xA0{\" \"}\n          <div>\n            \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 <b>State: </b> {build.aasm_state}\\xA0 \\xA0 \\xA0 \\xA0 \\xA0{\" \"}\n          </div>\n          \\xA0 \\xA0 \\xA0 \\xA0{\" \"}\n        </Grid>\n        \\xA0 \\xA0 \\xA0 \\xA0{\" \"}\n        <Grid\n          item\n          xs={9}\n          style={{\n            border: \"2px\",\n            backgroundColor: \"#343a40\",\n            fontSize: \"0.9rem\",\n            fontFamily: \"Monaco\",\n            color: \"white\",\n            padding: 10,\n          }}\n        >\n          \\xA0 \\xA0 \\xA0 \\xA0 \\xA0\n          {build.logs.map((log) => (\n            <div>{log.message} </div>\n          ))}\n          \\xA0 \\xA0 \\xA0 \\xA0{\" \"}\n        </Grid>\n        \\xA0 \\xA0 \\xA0{\" \"}\n      </Grid>\n      \\xA0 \\xA0\n    </>\n  );\n}\n`})}),`\n`,(0,n.jsxs)(e.p,{children:[\"For demo purposes I probably went a little overboard with the styling, but I wanted to create something that resembles our actual application which you saw at the start of this post. The two things needed to power the page are the \",(0,n.jsx)(e.em,{children:\"build\"}),\", which is retrieved with \",(0,n.jsx)(e.em,{children:\"useSelector\"}),\" and the \",(0,n.jsx)(e.em,{children:\"handleMessageReceived\"}),\" function, which dispatches \",(0,n.jsx)(e.em,{children:\"updateBuildFromMessage\"}),\" every time we receive a message through Action Cable. We supply the \",(0,n.jsx)(e.em,{children:\"\\u201DBuildChannel\\u201D\"}),\" and \",(0,n.jsx)(e.em,{children:\"build.id\"}),\" as the channel and room to \",(0,n.jsx)(e.em,{children:\"ActionCableWrapper\"}),\" along with \",(0,n.jsx)(e.em,{children:\"handleMessageReceived\"}),\" as the \",(0,n.jsx)(e.em,{children:\"onReceived\"}),\" function.\"]}),`\n`,(0,n.jsx)(e.p,{children:\"In the video below I\\u2019ll move the build through its different states and we\\u2019ll be able to see the frontend receive the messages, update the state, and add the logs to the screen.\"}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/57d547202f306ddf41f2b8df93ed8650.gif\",alt:\"\"})}),`\n`,(0,n.jsxs)(e.h3,{id:\"conclusion\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#conclusion\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Conclusion\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"That's a wrap on my adventure into how we set up our Action Cable integration with Redux Toolkit. There are tons of places in the application we\\u2019re going to be adding live updates too so that our users will always be up to date on the state of their application. I hope you enjoyed taking a peek inside some development work at Release. If you're interested in having an ephemeral environment created whenever we receive a Pull Request webhook from your Repository, head on over to the \",(0,n.jsx)(e.a,{href:\"https://release.com\",children:\"homepage\"}),\" and sign up! If you\\u2019d like to join our awesome team, check out our \",(0,n.jsx)(e.a,{href:\"https://releasehub.com/company\",children:\"job listings\"}),\".\"]})]})}function _(t={}){let{wrapper:e}=t.components||{};return e?(0,n.jsx)(e,Object.assign({},t,{children:(0,n.jsx)(c,t)})):c(t)}var y=_;return v(R);})();\n;return Component;"
        },
        "_id": "blog/posts/how-release-uses-action-cable-redux-toolkit.mdx",
        "_raw": {
          "sourceFilePath": "blog/posts/how-release-uses-action-cable-redux-toolkit.mdx",
          "sourceFileName": "how-release-uses-action-cable-redux-toolkit.mdx",
          "sourceFileDir": "blog/posts",
          "contentType": "mdx",
          "flattenedPath": "blog/posts/how-release-uses-action-cable-redux-toolkit"
        },
        "type": "BlogPost",
        "computedSlug": "how-release-uses-action-cable-redux-toolkit"
      },
      "documentHash": "1739393595020",
      "hasWarnings": false,
      "documentTypeName": "BlogPost"
    },
    "blog/posts/how-to-create-and-configure-your-kubernetes-service-account.mdx": {
      "document": {
        "title": "How to Create and Configure Your Kubernetes Service Account",
        "summary": "How to access Kubernetes API from inside the pod? Learn what Service Accounts are and how to use them.",
        "publishDate": "Tue Sep 06 2022 16:54:48 GMT+0000 (Coordinated Universal Time)",
        "author": "kevin-luu",
        "readingTime": 4,
        "categories": [
          "kubernetes",
          "platform-engineering"
        ],
        "mainImage": "/blog-images/5a4cea651659c159e68b1c51f033d546.jpg",
        "imageAlt": "a laptop with a plant on the screen",
        "showCTA": true,
        "ctaCopy": "Looking to streamline Kubernetes service account management? Try Release.com for automated environment provisioning and RBAC configuration.",
        "ctaLink": "https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=how-to-create-and-configure-your-kubernetes-service-account",
        "relatedPosts": [
          ""
        ],
        "ogImage": "/blog-images/5a4cea651659c159e68b1c51f033d546.jpg",
        "excerpt": "How to access Kubernetes API from inside the pod? Learn what Service Accounts are and how to use them.",
        "tags": [
          "kubernetes",
          "platform-engineering"
        ],
        "ctaButton": "Try Release for Free",
        "body": {
          "raw": "\nAccessing Kubernetes clusters has always been straightforward. You only need to download a kubeconfig file and place it in a specific place for your kubectl tool to read. This works well for human access, but there are use cases when you'd like some tools to access your Kubernetes API server. For example, your CI/CD pipeline somehow needs to authenticate to your cluster in order to deploy your applications there. For non-human access, Kubernetes offers what it calls service accounts. In this post, you'll learn what they are and how to use them. \n\n### What Are Kubernetes Service Accounts?\n\nLet's start with the basics. In order to understand what a Kubernetes service account is, you first need to know how the authentication mechanism works. \n\nWhen you access your Kubernetes cluster, you authenticate to the Kubernetes API as a human user via a user account. This is just an ordinary user account like in any other system. It distinguishes one user from another (however, by default, Kubernetes uses the same user account for all users). \n\nNormally, you should connect your Kubernetes cluster to an external user management solution like [Active Directory](https://en.wikipedia.org/wiki/Active_Directory) or [LDAP](https://en.wikipedia.org/wiki/Lightweight_Directory_Access_Protocol). When you do that, users will authenticate to Kubernetes using their company email address. So, for each request to its API server, Kubernetes will be able to see who made the request. In most organizations, this will follow the typical firstname.lastname@company.com format. \n\nThis model works perfectly fine for human users. But what about non-human users? They can't authenticate using user accounts because they're not human. They won't have a firstname.lastname@company.com email address. Which brings us to the point of this post. For these use cases, instead of user accounts, Kubernetes offers service accounts. And again, as the name suggests, these are special accounts that are meant to be used by non-humans or services. \n\n### How To Create a Service Account\n\nNow that you know the theory, let's get into the nuts and bolts. As with any other resource on Kubernetes, you can create a service account by using the **kubectl create** command. In the case of service accounts, it's as simple as specifying **serviceaccount** as the resource to be created, followed by its name. \n\n```yaml\n$ kubectl create serviceaccount my-service-account\nserviceaccount/my-service-account created\n```\n\nThat's it. You just created a new service account. But don't get too excited yet. This service account won't be very useful because, by default, it won't have any permissions associated with it. In other words, it won't be able to do anything. In order to change that, you can use the same Kubernetes RBAC mechanism as with user accounts. Therefore, you need to create a role binding for your new service account to an existing Kubernetes role or create a new custom role. Here's an example. \n\n```yaml\n$ kubectl create rolebinding my-service-account-rolebinding \\\n--clusterrole=view \\\n--serviceaccount=default:my-service-account \\\n--namespace=default\nrolebinding.rbac.authorization.k8s.io/my-service-account-rolebinding created\n```\n\nIn the code above, I created a Kubernetes role binding that associates build in the \"view\" role with my new service account. By doing so, my service principal will now be able to contact the Kubernetes API and perform read-only operations. So, how do you actually use a service principal? \n\n![Graphical user interface, text, applicationDescription automatically generated](/blog-images/44be52d4c48e7e433b05406cdab5215e.png)\n\n### How to Use a Service Account\n\nUsing Kubernetes as a human user in most cases means downloading [kubeconfig](https://kubernetes.io/docs/concepts/configuration/organize-cluster-access-kubeconfig/) and interacting with the cluster using the kubectl command. And as we already established, service accounts are used by non-humans. You already know how to create a service account, so now it's time to discuss how non-humans actually use them. \n\nFirst of all, what is non-human? In most cases, it just means pods on your cluster, be it your CI/CD agent that needs to be able to deploy other pods on the same cluster, a monitoring solution that needs to be able to get metrics from Kubernetes, or a security scanning tool that needs to get details about all pods on the cluster. \n\n### Assigning Service Accounts to Pods\n\nThese are just a few examples. The point is that anytime an application running in a pod on your cluster will need to get some information about other pods or the cluster itself, it will need a service account. You already know how to create a service account, but your pods won't magically start using it. Especially since you may have a few different service accounts with different permissions assigned to them. \n\nTherefore, you need to somehow tell a pod which service account to use. The good news is that it's pretty simple. All it takes is one extra line in the **spec** section of your deployment YAML definition. \n\n```yaml\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: example-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: example-deployment-app\n  template:\n    metadata:\n      labels:\n        app: example-deployment-app\n    spec:\n      serviceAccountName: my-service-account\n      containers:\n        - name: busybox\n          image: busybox\n          command:\n            - sleep\n            - \"3600\"\n\n```\n\nBy specifying **serviceAccountName** in your deployment (or any other object that creates pods), you'll tell Kubernetes which service account to assign to the underlying pods. It's worth remembering that service accounts are assigned to pods themselves, not higher-level resources like deployments. \n\nWhy did we specify **serviceAccountName** in the deployment definition then? Simple. Because you normally don't create pods directly. You usually use these higher-level resources that create pods for you. And Kubernetes is smart enough and won't complain. It will just apply specified service accounts on the pods directly. \n\n![Graphical user interface, text, applicationDescription automatically generated](/blog-images/e994fc809b6ae4bedc65a51ed4cccb87.png)\n\n### How to Validate If It Works\n\nNow you know how to create and apply a service account to your pods. But how can you be sure that everything works and that your pod is, in fact, using a specified service account? \n\nIt's quite straightforward. You can get the details of the pod with **kubectl get pod** and pass the **\\-o yaml** parameter. One of the lines in the **spec** section of the output will tell you which service account the pod is using. \n\n```yaml\n\n$ kubectl get pod nginx-deployment-c486548df-4spkw -oyaml\n\napiVersion: v1\nkind: Pod\nmetadata:\n  creationTimestamp: \"2022-08-27T15:36:29Z\"\n  generateName: nginx-deployment-c486548df-\n(...)\nspec:\n  containers:\n  - image: nginx:1.14.2\n    imagePullPolicy: IfNotPresent\n    name: nginx\n    (...)\n  serviceAccount: my-service-account\n  serviceAccountName: my-service-account\n(...)\n\n```\n\n![](/blog-images/5ccafd4c54de3ee29292963307b7df87.jpeg)\n\n### Adjusting Permissions\n\nOK, now you have a running pod with a custom service account attached to it that allows the application running in the pod to view resources on the cluster. What if you want to add read-write permissions You have two options. You can delete the existing role binding for your service account and create a new one, or you can start from scratch and create a separate service account altogether. \n\nLet's look at the first option. For that, you first need to execute the **kubectl delete** **rolebinding my-service-account-rolebinding** command to delete the existing role binding. You need to do that because Kubernetes doesn't allow you to change role bindings. \n\nNow you can create a new role binding, this time binding your service account to the **edit** role instead of **view.** Previously you did it with an inline kubectl command. This time I'll show you how to do it using the YAML file. The definition for role bindings looks like this: \n\n```yaml\n\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: my-service-account-rolebinding\n  namespace: default\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: edit\nsubjects:\n- kind: ServiceAccount\n  name: my-service-account\n  namespace: default\n\n```\n\nSave the above snippet in a YAML file and apply it to the cluster just like with any other YAML definition using **kubectl apply**. \n\n```yaml\n$ kubectl apply -f rolebinding.yaml\nrolebinding.rbac.authorization.k8s.io/my-service-account-rolebinding created\n```\n\nAnd just like with any other Kubernetes resource, you can always list existing role bindings using the **kubectl get** command. \n\n```yaml\n$ kubectl get rolebindings\nNAME                             ROLE               AGE\nmy-service-account-rolebinding   ClusterRole/edit   34s\n```\n\nNow, after restarting your pod, it will have read-write permissions. \n\n### Summary\n\nAs you can see, creating and configuring a service account is not that difficult. It is, however, a useful thing to know since most Kubernetes-based tools these days use service accounts. On top of that, it's a good security practice to have the least privileged service accounts for your pods. Misconfigured service accounts with too many permissions and no control over which pod gets which service principal could easily lead to an attacker taking control over your cluster. \n\nIf you want to learn more about Kubernetes, take a look at our other posts on [our blog](https://release.com/blog).\n",
          "code": "var Component=(()=>{var h=Object.create;var s=Object.defineProperty;var d=Object.getOwnPropertyDescriptor;var m=Object.getOwnPropertyNames;var p=Object.getPrototypeOf,g=Object.prototype.hasOwnProperty;var y=(t,e)=>()=>(e||t((e={exports:{}}).exports,e),e.exports),b=(t,e)=>{for(var o in e)s(t,o,{get:e[o],enumerable:!0})},c=(t,e,o,i)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let a of m(e))!g.call(t,a)&&a!==o&&s(t,a,{get:()=>e[a],enumerable:!(i=d(e,a))||i.enumerable});return t};var w=(t,e,o)=>(o=t!=null?h(p(t)):{},c(e||!t||!t.__esModule?s(o,\"default\",{value:t,enumerable:!0}):o,t)),f=t=>c(s({},\"__esModule\",{value:!0}),t);var l=y((K,r)=>{r.exports=_jsx_runtime});var N={};b(N,{default:()=>A,frontmatter:()=>v});var n=w(l()),v={title:\"How to Create and Configure Your Kubernetes Service Account\",summary:\"How to access Kubernetes API from inside the pod? Learn what Service Accounts are and how to use them.\",publishDate:\"Tue Sep 06 2022 16:54:48 GMT+0000 (Coordinated Universal Time)\",author:\"kevin-luu\",readingTime:4,categories:[\"kubernetes\",\"platform-engineering\"],mainImage:\"/blog-images/5a4cea651659c159e68b1c51f033d546.jpg\",imageAlt:\"a laptop with a plant on the screen\",showCTA:!0,ctaCopy:\"Looking to streamline Kubernetes service account management? Try Release.com for automated environment provisioning and RBAC configuration.\",ctaLink:\"https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=how-to-create-and-configure-your-kubernetes-service-account\",relatedPosts:[\"\"],ogImage:\"/blog-images/5a4cea651659c159e68b1c51f033d546.jpg\",excerpt:\"How to access Kubernetes API from inside the pod? Learn what Service Accounts are and how to use them.\",tags:[\"kubernetes\",\"platform-engineering\"],ctaButton:\"Try Release for Free\"};function u(t){let e=Object.assign({p:\"p\",h3:\"h3\",a:\"a\",span:\"span\",strong:\"strong\",pre:\"pre\",code:\"code\",img:\"img\"},t.components);return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(e.p,{children:\"Accessing Kubernetes clusters has always been straightforward. You only need to download a kubeconfig file and place it in a specific place for your kubectl tool to read. This works well for human access, but there are use cases when you'd like some tools to access your Kubernetes API server. For example, your CI/CD pipeline somehow needs to authenticate to your cluster in order to deploy your applications there. For non-human access, Kubernetes offers what it calls service accounts. In this post, you'll learn what they are and how to use them.\\xA0\"}),`\n`,(0,n.jsxs)(e.h3,{id:\"what-are-kubernetes-service-accounts\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#what-are-kubernetes-service-accounts\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"What Are Kubernetes Service Accounts?\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Let's start with the basics. In order to understand what a Kubernetes service account is, you first need to know how the authentication mechanism works.\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:\"When you access your Kubernetes cluster, you authenticate to the Kubernetes API as a human user via a user account. This is just an ordinary user account like in any other system. It distinguishes one user from another (however, by default, Kubernetes uses the same user account for all users).\\xA0\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"Normally, you should connect your Kubernetes cluster to an external user management solution like \",(0,n.jsx)(e.a,{href:\"https://en.wikipedia.org/wiki/Active_Directory\",children:\"Active Directory\"}),\" or \",(0,n.jsx)(e.a,{href:\"https://en.wikipedia.org/wiki/Lightweight_Directory_Access_Protocol\",children:\"LDAP\"}),\". When you do that, users will authenticate to Kubernetes using their company email address. So, for each request to its API server, Kubernetes will be able to see who made the request. In most organizations, this will follow the typical \",(0,n.jsx)(e.a,{href:\"mailto:firstname.lastname@company.com\",children:\"firstname.lastname@company.com\"}),\" format.\\xA0\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"This model works perfectly fine for human users. But what about non-human users? They can't authenticate using user accounts because they're not human. They won't have a \",(0,n.jsx)(e.a,{href:\"mailto:firstname.lastname@company.com\",children:\"firstname.lastname@company.com\"}),\" email address. Which brings us to the point of this post. For these use cases, instead of user accounts, Kubernetes offers service accounts. And again, as the name suggests, these are special accounts that are meant to be used by non-humans or services.\\xA0\"]}),`\n`,(0,n.jsxs)(e.h3,{id:\"how-to-create-a-service-account\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#how-to-create-a-service-account\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"How To Create a Service Account\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"Now that you know the theory, let's get into the nuts and bolts. As with any other resource on Kubernetes, you can create a service account by using the \",(0,n.jsx)(e.strong,{children:\"kubectl create\"}),\" command. In the case of service accounts, it's as simple as specifying \",(0,n.jsx)(e.strong,{children:\"serviceaccount\"}),\" as the resource to be created, followed by its name.\\xA0\"]}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-yaml\",children:`$ kubectl create serviceaccount my-service-account\nserviceaccount/my-service-account created\n`})}),`\n`,(0,n.jsx)(e.p,{children:\"That's it. You just created a new service account. But don't get too excited yet. This service account won't be very useful because, by default, it won't have any permissions associated with it. In other words, it won't be able to do anything. In order to change that, you can use the same Kubernetes RBAC mechanism as with user accounts. Therefore, you need to create a role binding for your new service account to an existing Kubernetes role or create a new custom role. Here's an example.\\xA0\"}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-yaml\",children:`$ kubectl create rolebinding my-service-account-rolebinding \\\\\n--clusterrole=view \\\\\n--serviceaccount=default:my-service-account \\\\\n--namespace=default\nrolebinding.rbac.authorization.k8s.io/my-service-account-rolebinding created\n`})}),`\n`,(0,n.jsx)(e.p,{children:'In the code above, I created a Kubernetes role binding that associates build in the \"view\" role with my new service account. By doing so, my service principal will now be able to contact the Kubernetes API and perform read-only operations. So, how do you actually use a service principal?\\xA0'}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/44be52d4c48e7e433b05406cdab5215e.png\",alt:\"Graphical user interface, text, applicationDescription automatically generated\"})}),`\n`,(0,n.jsxs)(e.h3,{id:\"how-to-use-a-service-account\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#how-to-use-a-service-account\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"How to Use a Service Account\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"Using Kubernetes as a human user in most cases means downloading \",(0,n.jsx)(e.a,{href:\"https://kubernetes.io/docs/concepts/configuration/organize-cluster-access-kubeconfig/\",children:\"kubeconfig\"}),\" and interacting with the cluster using the kubectl command. And as we already established, service accounts are used by non-humans. You already know how to create a service account, so now it's time to discuss how non-humans actually use them.\\xA0\"]}),`\n`,(0,n.jsx)(e.p,{children:\"First of all, what is non-human? In most cases, it just means pods on your cluster, be it your CI/CD agent that needs to be able to deploy other pods on the same cluster, a monitoring solution that needs to be able to get metrics from Kubernetes, or a security scanning tool that needs to get details about all pods on the cluster.\\xA0\"}),`\n`,(0,n.jsxs)(e.h3,{id:\"assigning-service-accounts-to-pods\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#assigning-service-accounts-to-pods\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Assigning Service Accounts to Pods\"]}),`\n`,(0,n.jsx)(e.p,{children:\"These are just a few examples. The point is that anytime an application running in a pod on your cluster will need to get some information about other pods or the cluster itself, it will need a service account. You already know how to create a service account, but your pods won't magically start using it. Especially since you may have a few different service accounts with different permissions assigned to them.\\xA0\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"Therefore, you need to somehow tell a pod which service account to use. The good news is that it's pretty simple. All it takes is one extra line in the \",(0,n.jsx)(e.strong,{children:\"spec\"}),\" section of your deployment YAML definition.\\xA0\"]}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-yaml\",children:`---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n \\xA0name: example-deployment\nspec:\n \\xA0replicas: 1\n \\xA0selector:\n \\xA0 \\xA0matchLabels:\n \\xA0 \\xA0 \\xA0app: example-deployment-app\n \\xA0template:\n \\xA0 \\xA0metadata:\n \\xA0 \\xA0 \\xA0labels:\n \\xA0 \\xA0 \\xA0 \\xA0app: example-deployment-app\n \\xA0 \\xA0spec:\n \\xA0 \\xA0 \\xA0serviceAccountName: my-service-account\n \\xA0 \\xA0 \\xA0containers:\n \\xA0 \\xA0 \\xA0 \\xA0- name: busybox\n \\xA0 \\xA0 \\xA0 \\xA0 \\xA0image: busybox\n \\xA0 \\xA0 \\xA0 \\xA0 \\xA0command:\n \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0- sleep\n \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0- \"3600\"\n\n`})}),`\n`,(0,n.jsxs)(e.p,{children:[\"By specifying \",(0,n.jsx)(e.strong,{children:\"serviceAccountName\"}),\" in your deployment (or any other object that creates pods), you'll tell Kubernetes which service account to assign to the underlying pods. It's worth remembering that service accounts are assigned to pods themselves, not higher-level resources like deployments.\\xA0\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"Why did we specify \",(0,n.jsx)(e.strong,{children:\"serviceAccountName\"}),\" in the deployment definition then? Simple. Because you normally don't create pods directly. You usually use these higher-level resources that create pods for you. And Kubernetes is smart enough and won't complain. It will just apply specified service accounts on the pods directly.\\xA0\"]}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/e994fc809b6ae4bedc65a51ed4cccb87.png\",alt:\"Graphical user interface, text, applicationDescription automatically generated\"})}),`\n`,(0,n.jsxs)(e.h3,{id:\"how-to-validate-if-it-works\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#how-to-validate-if-it-works\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"How to Validate If It Works\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Now you know how to create and apply a service account to your pods. But how can you be sure that everything works and that your pod is, in fact, using a specified service account?\\xA0\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"It's quite straightforward. You can get the details of the pod with \",(0,n.jsx)(e.strong,{children:\"kubectl get pod\"}),\" and pass the \",(0,n.jsx)(e.strong,{children:\"-o yaml\"}),\" parameter. One of the lines in the \",(0,n.jsx)(e.strong,{children:\"spec\"}),\" section of the output will tell you which service account the pod is using.\\xA0\"]}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-yaml\",children:`\n$ kubectl get pod nginx-deployment-c486548df-4spkw -oyaml\n\napiVersion: v1\nkind: Pod\nmetadata:\n \\xA0creationTimestamp: \"2022-08-27T15:36:29Z\"\n \\xA0generateName: nginx-deployment-c486548df-\n(...)\nspec:\n \\xA0containers:\n \\xA0- image: nginx:1.14.2\n \\xA0 \\xA0imagePullPolicy: IfNotPresent\n \\xA0 \\xA0name: nginx\n \\xA0 \\xA0(...)\n \\xA0serviceAccount: my-service-account\n \\xA0serviceAccountName: my-service-account\n(...)\n\n`})}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/5ccafd4c54de3ee29292963307b7df87.jpeg\",alt:\"\"})}),`\n`,(0,n.jsxs)(e.h3,{id:\"adjusting-permissions\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#adjusting-permissions\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Adjusting Permissions\"]}),`\n`,(0,n.jsx)(e.p,{children:\"OK, now you have a running pod with a custom service account attached to it that allows the application running in the pod to view resources on the cluster. What if you want to add read-write permissions You have two options. You can delete the existing role binding for your service account and create a new one, or you can start from scratch and create a separate service account altogether.\\xA0\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"Let's look at the first option. For that, you first need to execute the \",(0,n.jsx)(e.strong,{children:\"kubectl delete\"}),\" \",(0,n.jsx)(e.strong,{children:\"rolebinding my-service-account-rolebinding\"}),\" command to delete the existing role binding. You need to do that because Kubernetes doesn't allow you to change role bindings.\\xA0\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"Now you can create a new role binding, this time binding your service account to the \",(0,n.jsx)(e.strong,{children:\"edit\"}),\" role instead of \",(0,n.jsx)(e.strong,{children:\"view.\"}),\" Previously you did it with an inline kubectl command. This time I'll show you how to do it using the YAML file. The definition for role bindings looks like this:\\xA0\"]}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-yaml\",children:`\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n \\xA0name: my-service-account-rolebinding\n \\xA0namespace: default\nroleRef:\n \\xA0apiGroup: rbac.authorization.k8s.io\n \\xA0kind: ClusterRole\n \\xA0name: edit\nsubjects:\n- kind: ServiceAccount\n \\xA0name: my-service-account\n \\xA0namespace: default\n\n`})}),`\n`,(0,n.jsxs)(e.p,{children:[\"Save the above snippet in a YAML file and apply it to the cluster just like with any other YAML definition using \",(0,n.jsx)(e.strong,{children:\"kubectl apply\"}),\".\\xA0\"]}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-yaml\",children:`$ kubectl apply -f rolebinding.yaml\nrolebinding.rbac.authorization.k8s.io/my-service-account-rolebinding created\n`})}),`\n`,(0,n.jsxs)(e.p,{children:[\"And just like with any other Kubernetes resource, you can always list existing role bindings using the \",(0,n.jsx)(e.strong,{children:\"kubectl get\"}),\" command.\\xA0\"]}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-yaml\",children:`$ kubectl get rolebindings\nNAME \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 ROLE \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 AGE\nmy-service-account-rolebinding \\xA0 ClusterRole/edit \\xA0 34s\n`})}),`\n`,(0,n.jsx)(e.p,{children:\"Now, after restarting your pod, it will have read-write permissions.\\xA0\"}),`\n`,(0,n.jsxs)(e.h3,{id:\"summary\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#summary\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Summary\"]}),`\n`,(0,n.jsx)(e.p,{children:\"As you can see, creating and configuring a service account is not that difficult. It is, however, a useful thing to know since most Kubernetes-based tools these days use service accounts. On top of that, it's a good security practice to have the least privileged service accounts for your pods. Misconfigured service accounts with too many permissions and no control over which pod gets which service principal could easily lead to an attacker taking control over your cluster.\\xA0\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"If you want to learn more about Kubernetes, take a look at our other posts on \",(0,n.jsx)(e.a,{href:\"https://release.com/blog\",children:\"our blog\"}),\".\"]})]})}function k(t={}){let{wrapper:e}=t.components||{};return e?(0,n.jsx)(e,Object.assign({},t,{children:(0,n.jsx)(u,t)})):u(t)}var A=k;return f(N);})();\n;return Component;"
        },
        "_id": "blog/posts/how-to-create-and-configure-your-kubernetes-service-account.mdx",
        "_raw": {
          "sourceFilePath": "blog/posts/how-to-create-and-configure-your-kubernetes-service-account.mdx",
          "sourceFileName": "how-to-create-and-configure-your-kubernetes-service-account.mdx",
          "sourceFileDir": "blog/posts",
          "contentType": "mdx",
          "flattenedPath": "blog/posts/how-to-create-and-configure-your-kubernetes-service-account"
        },
        "type": "BlogPost",
        "computedSlug": "how-to-create-and-configure-your-kubernetes-service-account"
      },
      "documentHash": "1739393595020",
      "hasWarnings": false,
      "documentTypeName": "BlogPost"
    },
    "blog/posts/how-to-delete-hundreds-or-thousands-of-route53-dns-entries.mdx": {
      "document": {
        "title": "How to Delete Hundreds or Thousands of Route53 DNS Entries",
        "summary": "How do Delete Hundreds, or Possibly Thousands of Route53 DNS Entries Quickly and Easily",
        "publishDate": "Wed Feb 03 2021 05:21:12 GMT+0000 (Coordinated Universal Time)",
        "author": "regis-wilson",
        "readingTime": 6,
        "categories": [
          "platform-engineering",
          "product"
        ],
        "mainImage": "/blog-images/a40c6571d6ad435527d1213a49721a90.jpg",
        "imageAlt": "Hundreds of bright candy representing Route53 DNS Entries",
        "showCTA": true,
        "ctaCopy": "Simplify DNS management with Release: automate bulk Route53 operations and streamline environment setup effortlessly.",
        "ctaLink": "https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=how-to-delete-hundreds-or-thousands-of-route53-dns-entries",
        "relatedPosts": [
          ""
        ],
        "ogImage": "/blog-images/a40c6571d6ad435527d1213a49721a90.jpg",
        "excerpt": "How do Delete Hundreds, or Possibly Thousands of Route53 DNS Entries Quickly and Easily",
        "tags": [
          "platform-engineering",
          "product"
        ],
        "ctaButton": "Try Release for Free",
        "body": {
          "raw": "\n### How to Delete Hundreds (or Thousands) of Route53 Entries Quickly on the Command Line\n\n### Overview\n\nAt Release, we make Staging environments easy by quickly creating and updating environments to run, test, and share your application code in full fledged, isolated environments. In previous versions of our product, we were able to quickly roll out new environments and features by creating tons of AWS Route53 DNS entries for each new application and environment. Unfortunately, that meant that we were quickly creating over 5,000 Route53 entries.\n\nThe maximum number of Route53 entries you can have by default in one hosted zone is 10,000, so we needed to fix this before we ran out. Luckily, we added new features to create smart wildcard entries and a routing system to drastically reduce the number of entries we needed to create for ourselves and our customers.\n\nBut then we were stuck with a legacy of over 5,000 entries that needed to be deleted (carefully!) in a reasonable timeframe and preferably automatically, rather than by hand. This article will show you how we accomplished the task and how this relatively obscure and niche problem (we hope!) can be solved relatively quickly and painlessly.\n\n### Investigation\n\nThe initial approach is to simply come up with a command-line query to list Route53 entries and then parse them one by one to delete them. Unfortunately, the documentation quickly shows this to be the wrong method, since Route53 entry “upserts” (additions or changes) or “deletes” (as you would expect) need to be batched and uploaded in a transaction. There is no simple “delete one Route53 entry” command on the CLI as of the time of this writing. In point of fact, this naive approach is actually not a good way to do this type of bulk update anyway. Route53 will correctly handle each batch of operations as a transaction; so that if one entry fails to update or delete for some reason, the whole batch will be rolled back to preserve the integrity of your records.\n\nI therefore started with one of my favourite Stack Overflow answers that I turn to way more often than I should: [How to Export Route53 Zone File](https://stackoverflow.com/a/48498598). This was one of those copy-paste answers I would blindly use when approaching a Route53 use-case and it happily contained enough of a starting solution to building out the entire point of this blog post.\n\n### A Slight Tangent on JQ\n\n[JQ](https://stedolan.github.io/jq/) is a [JSON query language](https://stackoverflow.com/a/56114895) and is billed as “[sed](https://en.wikipedia.org/wiki/Sed) for JSON”. For me, jq has always been a bit opaque and I usually just copy-paste whatever a Stack Overflow answer has provided. In the case of the problem presented in this blog post, I needed to really dive in and learn about the power jq offers to help me solve this problem. It is, indeed, part of the core solution the above Stack Overflow answer is based on.\n\nThe first thing to note is that jq can be used to extract, transform, output, rollup, and filter JSON objects or text in a programmatic fashion. In this way, I have started changing my pitch to be that jq is “[awk](https://en.wikipedia.org/wiki/AWK) for JSON”. I have found jq syntax and structure to be a bit difficult to test or grasp, so I was delighted to find [jq Play](https://jqplay.org/), an online resource for testing and visualising jq syntax and test inputs. I will use the screenshots from jq play to display my steps as we go along.\n\n#### Step 1, Gather Test Data\n\nThe first step, as exactly described above in Stack Overflow, is to run the AWS CLI to output a JSON list of all of your Route53 entries (in our case, over 5,000+!!!). Take a few lines of the first part of the output to play with. You can grab a few entries by limiting the --max-items option in the list-resource-record-sets command. Take those and paste them into the jq play screen on the left, then select “.” as the operator to output everything. You can follow along with this [snippet](https://jqplay.org/s/0EWB_zhjvG). Here is what it looks like initially:\n\n![](/blog-images/d9b85c1e4277baadf503ebec1565f804.png)\n\nThis is a good starting point but we need to first start with unwrapping the outer layer of the “ResourceRecords” key to find the list of entries as [follows](https://jqplay.org/s/PM3uM757DE) (turn on the “Compact View” to make it easier to understand and see more of what’s happening:\n\n![](/blog-images/8e36c46777593174cef0867a6c03390b.png)\n\n#### Step 2, Filter Records\n\nNow we need to filter out records so that only the “AliasTarget -> DNSName” keys matching a particular endpoint get deleted. That is relatively easy to do by filtering results with the pipe (“|”) character and using the “Select” operator as [follows](https://jqplay.org/s/cKEL8vTJHr):\n\n![](/blog-images/8059997113e53a00e0f627bd481eb3bb.png)\n\nKeep in mind that in practice, we will be filtering way more than 2 records from 4 records, but this is just a test before we run the full solution. Also, keep in mind that you could use any number of filters and selector operators (for example on the “Type” field) to choose which entries to act upon. The world is your oyster!\n\nWhich is a silly saying, of course. If the world is your oyster, then that is a salty, squishy, goey, messy, muscly world. And where is the pearl in your world? Some hard round misshapen thing rolling around in your bedroom so you can’t sleep comfortably? I suppose it’s better than sand everywhere, but really. “The world is your oyster”?\n\n#### Step 3, Manipulate Rows for Delete\n\nThe next step is to manipulate the rows we’re filtering/selecting to create the individual records that will become the batch delete operation. To do this, we will need to construct the output record form for each individual delete action using the schema that Route53 is going to expect. In this case, we extract several fields from each record and wrap them inside an “Action: DELETE” key as [follows](https://jqplay.org/s/hsZ65XuciY):\n\n![](/blog-images/1b31c9ae6788a3eeae8ce3da27bed79f.png)\n\nNotice how the JSON in the right hand pane is looking like the output that we will be able to pipe back into an AWS CLI call to delete entries. We’re coming along nicely!\n\nAlso keep in mind that you could extend this example to manipulate entries in any way you like, for example, changing record types, or bulk-changing TTLs or some other field.\n\nNotice how the outputs are newline-separated? This initially confused me, but you can easily create a list or map by wrapping the whole query inside either square brackets (for a list) or curly brackets for a map:\n\n![](/blog-images/d20d696ffc43a5907c4edc33838c3613.png)\n\n#### Step 4, Mind the Max Batch Size\n\nWe’re almost done, but since we’re deleting multiple hundreds (thousands, actually) of records, we want to set a batch size that is reasonable and that Route53 will accept. According to the documentation, the maximum batch size is 1000. I arbitrarily chose a batch size of 100 that is more reasonable and manageable for the CLI, so layer on the \\_nwise() operator as [follows](https://jqplay.org/s/uu649cb-BM):\n\n![](/blog-images/d5376d45a40fa7c9a2a0caa01234e4fd.png)\n\nKeep in mind that in this example, I’m playing with 4 records, filtered to 2, and then batched into sizes of 1. In reality, we’re going to apply this to 5,000+ records, filtered to ~4,000 records, and batched at 100. The question you will want to ask yourself is, “How many records do I want to hassle with (possibly manually) if something explodes in the middle? Or if some intervention is required in a batch to add/remove/massage form one or more batches?” I settled on about 100.\n\nHopefully you’re not as crazy as I am and in a similar predicament. You should be smart enough to avoid this situation in the first place. But if you are as crazy as I am, welcome to the club; we’re very sympathetic to your problems around here. I also really appreciate you reading all the way through to this spot, you crazy, wonderful, patient soul.\n\n#### Step 5, Wrap It Up\n\nEach batch is ready to be delivered on one line as shown above, however, before we’re done we need to add a “Changes” key at the top level for Route53 to accept. This is easy to accomplish by just piping the results into a map with one key and using the period (“.”) to select “everything” as [follows](https://jqplay.org/s/lYveGGoUwC):\n\n![](/blog-images/871187d03e4505b812bc462a9a0d1a5e.png)\n\n#### Step 6, Apply\n\nNow we are ready to actually apply the records and see how much damage we can do! Take the entire output of your records with this kind of query:\n\n```yaml\naws route53 list-resource-record-sets \\\n--hosted-zone-id ${hostedzoneid} \\\n--max-items 10000 \\\n--output json\n```\n\nAnd pipe it into the handy command line options provided at the bottom of your screen in the jq player application:\n\n```yaml\n\njq --compact-output '[.ResourceRecordSets[] |\n  select(.AliasTarget.DNSName == \"something.us-west-2.elb.amazonaws.com.\") |\n  {Action: \"DELETE\", ResourceRecordSet: {Name: .Name, Type: .Type, AliasTarget: .AliasTarget}}] |\n  _nwise(1) |\n  {Changes: .}'\n  \n```\n\nAnd use split to create a bunch of individual files:\n\n```yaml\nsplit -l 1\n```\n\nThen loop over all your files to apply them in Route53:\n\n```yaml\nfor file in x*; do\naws route53 change-resource-record-sets \\\n--hosted-zone-id=${hostedzoneid} \\\n--cange-batch=file://${file}\ndone\n```\n\n#### Step 7, Profit\n\nI hope you enjoyed this exploration and how quickly you can manipulate JSON data with jq to produce a fast, efficient, and automated method of clearing out a bunch of old Route53 entries in your zones!\n\nhero image: [Sharon McCutcheon via Unsplash.com](https://unsplash.com/photos/1wz7cN1XTmk)‍\n",
          "code": "var Component=(()=>{var d=Object.create;var i=Object.defineProperty;var p=Object.getOwnPropertyDescriptor;var u=Object.getOwnPropertyNames;var m=Object.getPrototypeOf,f=Object.prototype.hasOwnProperty;var y=(a,e)=>()=>(e||a((e={exports:{}}).exports,e),e.exports),g=(a,e)=>{for(var o in e)i(a,o,{get:e[o],enumerable:!0})},s=(a,e,o,r)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let n of u(e))!f.call(a,n)&&n!==o&&i(a,n,{get:()=>e[n],enumerable:!(r=p(e,n))||r.enumerable});return a};var w=(a,e,o)=>(o=a!=null?d(m(a)):{},s(e||!a||!a.__esModule?i(o,\"default\",{value:a,enumerable:!0}):o,a)),b=a=>s(i({},\"__esModule\",{value:!0}),a);var h=y((S,l)=>{l.exports=_jsx_runtime});var T={};g(T,{default:()=>N,frontmatter:()=>v});var t=w(h()),v={title:\"How to Delete Hundreds or Thousands of Route53 DNS Entries\",summary:\"How do Delete Hundreds, or Possibly Thousands of Route53 DNS Entries Quickly and Easily\",publishDate:\"Wed Feb 03 2021 05:21:12 GMT+0000 (Coordinated Universal Time)\",author:\"regis-wilson\",readingTime:6,categories:[\"platform-engineering\",\"product\"],mainImage:\"/blog-images/a40c6571d6ad435527d1213a49721a90.jpg\",imageAlt:\"Hundreds of bright candy representing Route53 DNS Entries\",showCTA:!0,ctaCopy:\"Simplify DNS management with Release: automate bulk Route53 operations and streamline environment setup effortlessly.\",ctaLink:\"https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=how-to-delete-hundreds-or-thousands-of-route53-dns-entries\",relatedPosts:[\"\"],ogImage:\"/blog-images/a40c6571d6ad435527d1213a49721a90.jpg\",excerpt:\"How do Delete Hundreds, or Possibly Thousands of Route53 DNS Entries Quickly and Easily\",tags:[\"platform-engineering\",\"product\"],ctaButton:\"Try Release for Free\"};function c(a){let e=Object.assign({h3:\"h3\",a:\"a\",span:\"span\",p:\"p\",h4:\"h4\",img:\"img\",pre:\"pre\",code:\"code\"},a.components);return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsxs)(e.h3,{id:\"how-to-delete-hundreds-or-thousands-of-route53-entries-quickly-on-the-command-line\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#how-to-delete-hundreds-or-thousands-of-route53-entries-quickly-on-the-command-line\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"How to Delete Hundreds (or Thousands) of Route53 Entries Quickly on the Command Line\"]}),`\n`,(0,t.jsxs)(e.h3,{id:\"overview\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#overview\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Overview\"]}),`\n`,(0,t.jsx)(e.p,{children:\"At Release, we make Staging environments easy by quickly creating and updating environments to run, test, and share your application code in full fledged, isolated environments. In previous versions of our product, we were able to quickly roll out new environments and features by creating tons of AWS Route53 DNS entries for each new application and environment. Unfortunately, that meant that we were quickly creating over 5,000 Route53 entries.\"}),`\n`,(0,t.jsx)(e.p,{children:\"The maximum number of Route53 entries you can have by default in one hosted zone is 10,000, so we needed to fix this before we ran out. Luckily, we added new features to create smart wildcard entries and a routing system to drastically reduce the number of entries we needed to create for ourselves and our customers.\"}),`\n`,(0,t.jsx)(e.p,{children:\"But then we were stuck with a legacy of over 5,000 entries that needed to be deleted (carefully!) in a reasonable timeframe and preferably automatically, rather than by hand. This article will show you how we accomplished the task and how this relatively obscure and niche problem (we hope!) can be solved relatively quickly and painlessly.\"}),`\n`,(0,t.jsxs)(e.h3,{id:\"investigation\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#investigation\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Investigation\"]}),`\n`,(0,t.jsx)(e.p,{children:\"The initial approach is to simply come up with a command-line query to list Route53 entries and then parse them one by one to delete them. Unfortunately, the documentation quickly shows this to be the wrong method, since Route53 entry \\u201Cupserts\\u201D (additions or changes) or \\u201Cdeletes\\u201D (as you would expect) need to be batched and uploaded in a transaction. There is no simple \\u201Cdelete one Route53 entry\\u201D command on the CLI as of the time of this writing. In point of fact, this naive approach is actually not a good way to do this type of bulk update anyway. Route53 will correctly handle each batch of operations as a transaction; so that if one entry fails to update or delete for some reason, the whole batch will be rolled back to preserve the integrity of your records.\"}),`\n`,(0,t.jsxs)(e.p,{children:[\"I therefore started with one of my favourite Stack Overflow answers that I turn to way more often than I should: \",(0,t.jsx)(e.a,{href:\"https://stackoverflow.com/a/48498598\",children:\"How to Export Route53 Zone File\"}),\". This was one of those copy-paste answers I would blindly use when approaching a Route53 use-case and it happily contained enough of a starting solution to building out the entire point of this blog post.\"]}),`\n`,(0,t.jsxs)(e.h3,{id:\"a-slight-tangent-on-jq\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#a-slight-tangent-on-jq\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"A Slight Tangent on JQ\"]}),`\n`,(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.a,{href:\"https://stedolan.github.io/jq/\",children:\"JQ\"}),\" is a \",(0,t.jsx)(e.a,{href:\"https://stackoverflow.com/a/56114895\",children:\"JSON query language\"}),\" and is billed as \\u201C\",(0,t.jsx)(e.a,{href:\"https://en.wikipedia.org/wiki/Sed\",children:\"sed\"}),\" for JSON\\u201D. For me, jq has always been a bit opaque and I usually just copy-paste whatever a Stack Overflow answer has provided. In the case of the problem presented in this blog post, I needed to really dive in and learn about the power jq offers to help me solve this problem. It is, indeed, part of the core solution the above Stack Overflow answer is based on.\"]}),`\n`,(0,t.jsxs)(e.p,{children:[\"The first thing to note is that jq can be used to extract, transform, output, rollup, and filter JSON objects or text in a programmatic fashion. In this way, I have started changing my pitch to be that jq is \\u201C\",(0,t.jsx)(e.a,{href:\"https://en.wikipedia.org/wiki/AWK\",children:\"awk\"}),\" for JSON\\u201D. I have found jq syntax and structure to be a bit difficult to test or grasp, so I was delighted to find \",(0,t.jsx)(e.a,{href:\"https://jqplay.org/\",children:\"jq Play\"}),\", an online resource for testing and visualising jq syntax and test inputs. I will use the screenshots from jq play to display my steps as we go along.\"]}),`\n`,(0,t.jsxs)(e.h4,{id:\"step-1-gather-test-data\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#step-1-gather-test-data\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Step 1, Gather Test Data\"]}),`\n`,(0,t.jsxs)(e.p,{children:[\"The first step, as exactly described above in Stack Overflow, is to run the AWS CLI to output a JSON list of all of your Route53 entries (in our case, over 5,000+!!!). Take a few lines of the first part of the output to play with. You can grab a few entries by limiting the --max-items option in the list-resource-record-sets command. Take those and paste them into the jq play screen on the left, then select \\u201C.\\u201D as the operator to output everything. You can follow along with this \",(0,t.jsx)(e.a,{href:\"https://jqplay.org/s/0EWB_zhjvG\",children:\"snippet\"}),\". Here is what it looks like initially:\"]}),`\n`,(0,t.jsx)(e.p,{children:(0,t.jsx)(e.img,{src:\"/blog-images/d9b85c1e4277baadf503ebec1565f804.png\",alt:\"\"})}),`\n`,(0,t.jsxs)(e.p,{children:[\"This is a good starting point but we need to first start with unwrapping the outer layer of the \\u201CResourceRecords\\u201D key to find the list of entries as \",(0,t.jsx)(e.a,{href:\"https://jqplay.org/s/PM3uM757DE\",children:\"follows\"}),\" (turn on the \\u201CCompact View\\u201D to make it easier to understand and see more of what\\u2019s happening:\"]}),`\n`,(0,t.jsx)(e.p,{children:(0,t.jsx)(e.img,{src:\"/blog-images/8e36c46777593174cef0867a6c03390b.png\",alt:\"\"})}),`\n`,(0,t.jsxs)(e.h4,{id:\"step-2-filter-records\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#step-2-filter-records\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Step 2, Filter Records\"]}),`\n`,(0,t.jsxs)(e.p,{children:[\"Now we need to filter out records so that only the \\u201CAliasTarget -> DNSName\\u201D keys matching a particular endpoint get deleted. That is relatively easy to do by filtering results with the pipe (\\u201C|\\u201D) character and using the \\u201CSelect\\u201D operator as \",(0,t.jsx)(e.a,{href:\"https://jqplay.org/s/cKEL8vTJHr\",children:\"follows\"}),\":\"]}),`\n`,(0,t.jsx)(e.p,{children:(0,t.jsx)(e.img,{src:\"/blog-images/8059997113e53a00e0f627bd481eb3bb.png\",alt:\"\"})}),`\n`,(0,t.jsx)(e.p,{children:\"Keep in mind that in practice, we will be filtering way more than 2 records from 4 records, but this is just a test before we run the full solution. Also, keep in mind that you could use any number of filters and selector operators (for example on the \\u201CType\\u201D field) to choose which entries to act upon. The world is your oyster!\"}),`\n`,(0,t.jsx)(e.p,{children:\"Which is a silly saying, of course. If the world is your oyster, then that is a salty, squishy, goey, messy, muscly world. And where is the pearl in your world? Some hard round misshapen thing rolling around in your bedroom so you can\\u2019t sleep comfortably? I suppose it\\u2019s better than sand everywhere, but really. \\u201CThe world is your oyster\\u201D?\"}),`\n`,(0,t.jsxs)(e.h4,{id:\"step-3-manipulate-rows-for-delete\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#step-3-manipulate-rows-for-delete\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Step 3, Manipulate Rows for Delete\"]}),`\n`,(0,t.jsxs)(e.p,{children:[\"The next step is to manipulate the rows we\\u2019re filtering/selecting to create the individual records that will become the batch delete operation. To do this, we will need to construct the output record form for each individual delete action using the schema that Route53 is going to expect. In this case, we extract several fields from each record and wrap them inside an \\u201CAction: DELETE\\u201D key as \",(0,t.jsx)(e.a,{href:\"https://jqplay.org/s/hsZ65XuciY\",children:\"follows\"}),\":\"]}),`\n`,(0,t.jsx)(e.p,{children:(0,t.jsx)(e.img,{src:\"/blog-images/1b31c9ae6788a3eeae8ce3da27bed79f.png\",alt:\"\"})}),`\n`,(0,t.jsx)(e.p,{children:\"Notice how the JSON in the right hand pane is looking like the output that we will be able to pipe back into an AWS CLI call to delete entries. We\\u2019re coming along nicely!\"}),`\n`,(0,t.jsx)(e.p,{children:\"Also keep in mind that you could extend this example to manipulate entries in any way you like, for example, changing record types, or bulk-changing TTLs or some other field.\"}),`\n`,(0,t.jsx)(e.p,{children:\"Notice how the outputs are newline-separated? This initially confused me, but you can easily create a list or map by wrapping the whole query inside either square brackets (for a list) or curly brackets for a map:\"}),`\n`,(0,t.jsx)(e.p,{children:(0,t.jsx)(e.img,{src:\"/blog-images/d20d696ffc43a5907c4edc33838c3613.png\",alt:\"\"})}),`\n`,(0,t.jsxs)(e.h4,{id:\"step-4-mind-the-max-batch-size\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#step-4-mind-the-max-batch-size\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Step 4, Mind the Max Batch Size\"]}),`\n`,(0,t.jsxs)(e.p,{children:[\"We\\u2019re almost done, but since we\\u2019re deleting multiple hundreds (thousands, actually) of records, we want to set a batch size that is reasonable and that Route53 will accept. According to the documentation, the maximum batch size is 1000. I arbitrarily chose a batch size of 100 that is more reasonable and manageable for the CLI, so layer on the _nwise() operator as \",(0,t.jsx)(e.a,{href:\"https://jqplay.org/s/uu649cb-BM\",children:\"follows\"}),\":\"]}),`\n`,(0,t.jsx)(e.p,{children:(0,t.jsx)(e.img,{src:\"/blog-images/d5376d45a40fa7c9a2a0caa01234e4fd.png\",alt:\"\"})}),`\n`,(0,t.jsx)(e.p,{children:\"Keep in mind that in this example, I\\u2019m playing with 4 records, filtered to 2, and then batched into sizes of 1. In reality, we\\u2019re going to apply this to 5,000+ records, filtered to ~4,000 records, and batched at 100. The question you will want to ask yourself is, \\u201CHow many records do I want to hassle with (possibly manually) if something explodes in the middle? Or if some intervention is required in a batch to add/remove/massage form one or more batches?\\u201D I settled on about 100.\"}),`\n`,(0,t.jsx)(e.p,{children:\"Hopefully you\\u2019re not as crazy as I am and in a similar predicament. You should be smart enough to avoid this situation in the first place. But if you are as crazy as I am, welcome to the club; we\\u2019re very sympathetic to your problems around here. I also really appreciate you reading all the way through to this spot, you crazy, wonderful, patient soul.\"}),`\n`,(0,t.jsxs)(e.h4,{id:\"step-5-wrap-it-up\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#step-5-wrap-it-up\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Step 5, Wrap It Up\"]}),`\n`,(0,t.jsxs)(e.p,{children:[\"Each batch is ready to be delivered on one line as shown above, however, before we\\u2019re done we need to add a \\u201CChanges\\u201D key at the top level for Route53 to accept. This is easy to accomplish by just piping the results into a map with one key and using the period (\\u201C.\\u201D) to select \\u201Ceverything\\u201D as \",(0,t.jsx)(e.a,{href:\"https://jqplay.org/s/lYveGGoUwC\",children:\"follows\"}),\":\"]}),`\n`,(0,t.jsx)(e.p,{children:(0,t.jsx)(e.img,{src:\"/blog-images/871187d03e4505b812bc462a9a0d1a5e.png\",alt:\"\"})}),`\n`,(0,t.jsxs)(e.h4,{id:\"step-6-apply\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#step-6-apply\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Step 6, Apply\"]}),`\n`,(0,t.jsx)(e.p,{children:\"Now we are ready to actually apply the records and see how much damage we can do! Take the entire output of your records with this kind of query:\"}),`\n`,(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:\"language-yaml\",children:`aws route53 list-resource-record-sets \\\\\n--hosted-zone-id \\${hostedzoneid} \\\\\n--max-items 10000 \\\\\n--output json\n`})}),`\n`,(0,t.jsx)(e.p,{children:\"And pipe it into the handy command line options provided at the bottom of your screen in the jq player application:\"}),`\n`,(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:\"language-yaml\",children:`\njq --compact-output '[.ResourceRecordSets[] |\n \\xA0select(.AliasTarget.DNSName == \"something.us-west-2.elb.amazonaws.com.\") |\n \\xA0{Action: \"DELETE\", ResourceRecordSet: {Name: .Name, Type: .Type, AliasTarget: .AliasTarget}}] |\n \\xA0_nwise(1) |\n \\xA0{Changes: .}'\n \\xA0\n`})}),`\n`,(0,t.jsx)(e.p,{children:\"And use split to create a bunch of individual files:\"}),`\n`,(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:\"language-yaml\",children:`split -l 1\n`})}),`\n`,(0,t.jsx)(e.p,{children:\"Then loop over all your files to apply them in Route53:\"}),`\n`,(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:\"language-yaml\",children:`for file in x*; do\naws route53 change-resource-record-sets \\\\\n--hosted-zone-id=\\${hostedzoneid} \\\\\n--cange-batch=file://\\${file}\ndone\n`})}),`\n`,(0,t.jsxs)(e.h4,{id:\"step-7-profit\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#step-7-profit\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Step 7, Profit\"]}),`\n`,(0,t.jsx)(e.p,{children:\"I hope you enjoyed this exploration and how quickly you can manipulate JSON data with jq to produce a fast, efficient, and automated method of clearing out a bunch of old Route53 entries in your zones!\"}),`\n`,(0,t.jsxs)(e.p,{children:[\"hero image: \",(0,t.jsx)(e.a,{href:\"https://unsplash.com/photos/1wz7cN1XTmk\",children:\"Sharon McCutcheon via Unsplash.com\"}),\"\\u200D\"]})]})}function k(a={}){let{wrapper:e}=a.components||{};return e?(0,t.jsx)(e,Object.assign({},a,{children:(0,t.jsx)(c,a)})):c(a)}var N=k;return b(T);})();\n;return Component;"
        },
        "_id": "blog/posts/how-to-delete-hundreds-or-thousands-of-route53-dns-entries.mdx",
        "_raw": {
          "sourceFilePath": "blog/posts/how-to-delete-hundreds-or-thousands-of-route53-dns-entries.mdx",
          "sourceFileName": "how-to-delete-hundreds-or-thousands-of-route53-dns-entries.mdx",
          "sourceFileDir": "blog/posts",
          "contentType": "mdx",
          "flattenedPath": "blog/posts/how-to-delete-hundreds-or-thousands-of-route53-dns-entries"
        },
        "type": "BlogPost",
        "computedSlug": "how-to-delete-hundreds-or-thousands-of-route53-dns-entries"
      },
      "documentHash": "1739393595020",
      "hasWarnings": false,
      "documentTypeName": "BlogPost"
    },
    "blog/posts/how-to-edit-a-file-in-a-docker-container.mdx": {
      "document": {
        "title": "How to edit a file in a Docker container",
        "summary": "See how to edit files in Docker containers with command line editors or through connecting VS Code.",
        "publishDate": "Tue Jul 19 2022 07:37:33 GMT+0000 (Coordinated Universal Time)",
        "author": "regis-wilson",
        "readingTime": 3,
        "categories": [
          "platform-engineering",
          "product"
        ],
        "mainImage": "/blog-images/9ad01e7b63d384efa30ad9fe35b9e74b.jpg",
        "imageAlt": "Some files on a table",
        "showCTA": true,
        "ctaCopy": "Looking to edit files in Docker containers hassle-free? Try Release.com for ephemeral environments that simplify setup and teardown.",
        "ctaLink": "https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=how-to-edit-a-file-in-a-docker-container",
        "relatedPosts": [
          ""
        ],
        "ogImage": "/blog-images/9ad01e7b63d384efa30ad9fe35b9e74b.jpg",
        "excerpt": "See how to edit files in Docker containers with command line editors or through connecting VS Code.",
        "tags": [
          "platform-engineering",
          "product"
        ],
        "ctaButton": "Try Release for Free",
        "body": {
          "raw": "\nYou want to edit a file in your Docker container, but you've run into an error that leaves you with none of the tools you need to make your changes. Now what?\n\nDocker intentionally keeps containers as lean as possible with no unnecessary packages installed to maximize performance and stability. Unfortunately, this also means Docker containers don't have a file editor like Vim or Nano preinstalled.\n\nIn this guide, we'll show you how to install an editor, make the changes you need to, and return the container to its original state, both from the command line and using the Docker extension inside VS Code.\n\nFirst, though, some housekeeping. It's considered bad practice to edit Docker files currently running in a production environment, and, once you've made your change, you should remove any packages you installed to do so (the editor, for example).\n\nHere's our step-by-step guide to editing a file in Docker.\n\n### Option 1: Edit from the command line\n\n#### #1 Log in to your container\n\nIf your container is not already running, run the container with the following:\n\n```yaml\ndocker run --name  -d -t\n```\n\nTo check all your running containers, you can use the command:\n\n```yaml\ndocker ps\n```\n\nYou should be met with something like this:\n\n![The console showing the output of docker ps listing the container ID and other information.](/blog-images/5269f2732d8885273f30405faaabebd6.png)\n\nThis list indicates your target container is up and running. Note that every container has a discrete ID, which we'll need to gain root access to the container.\n\nTo gain root access to the container, run:\n\n```yaml\ndocker exec -it\n```\n\nYou should see something like this:\n\n![A prompt showing that root login has succeeded.](/blog-images/449be2be0b420fbd7378193201c1564d.png)\n\nAs you can see, **root@CONTAINER_ID:/#** indicates we now have root access to the container.\n\n#### #2 Install the editor\n\nIt's a good idea to update your package manager before you install the editor. This ensures that you install the latest stable release of the editor. On Ubuntu, that command is:\n\n```yaml\napt-get update\n```\n\nTo install your preferred editor, such as Vim, Nano or GNU Emacs:\n\n```yaml\napt-get install\n```\n\nFor example, to install Vim:\n\n```yaml\napt-get install vim\n```\n\n#### #3 Edit the File\n\nTo edit the file, ensure you are in the appropriate directory and use the command:\n\n```yaml\nvim yourfilename.yaml\n```\n\nOnce you've made the edit to the file, you can remove the editor (in our case, Vim) like this:\n\n```yaml\napt-get remove vim\n```\n\nOr like this:\n\n```yaml\napt-get purge vim\n```\n\nThe command \"remove\" will remove only Vim, and no other config files or dependencies involved in the initial install. The command \"purge\" will remove all config files associated with Vim. In the interest of leaving no trace, the purge command is probably appropriate in this case.\n\nYour package manager may change depending on your OS. These commands are associated with Ubuntu and Vim.\n\n### Persisting an editor for regular changes\n\nThe above steps are useful for one-off changes, but if you need to make changes often – in a development environment, for example – it's best to add your editor to your Dockerfile. This will ensure your chosen editor is always available whenever you spin up another instance of your container.\n\nAdd your editor to the Dockerfile like this:\n\n```yaml\nRUN[\"apt-get\", \"update\"]\nRUN[\"apt-get\", \"install\", \"vim\"]\n```\n\nEvery image created with that Dockerfile will have Vim pre-installed and ready to go.\n\nYou can replace \"Vim\" with your editor of choice, such as Nano or GNU Emacs. Keep in mind that the commands in the square brackets are specific to Ubuntu Linux. You may need to adapt these to the operating system you are running in your Docker container.\n\n### Option 2: Edit from VS Code\n\nIf you prefer to use a GUI editor (for example, if you'd like to use your mouse to navigate through large files, or cut and paste text), you can use VS Code.\n\nThis option requires both the Visual Studio Code IDE and the Docker extension from Microsoft. To install the extension, navigate to the extensions tab in VS Code and type in \"Docker\".\n\n![Search results in VS Code extensions for docker.](/blog-images/c65ee7d20b5528430ddb2927aaac44c2.png)\n\nBe sure to select the Docker extension from Microsoft. This extension allows you to easily manage any containers on your system directly from its UI.\n\n![VS Code displaying a file inside the docker container.](/blog-images/70ac9ac2bf3cda7f1eba36a594a7cc7e.png)\n\nFrom here, treating a container like any file directory, you can navigate to and open files in that container, and make your changes right in VS Code.\n\n### Closing remarks\n\nNow that you know how to edit files in a Docker file, it's important to take note of the best practice for it.\n\nEditing files in a running Docker container is only recommended when working in a development environment during conceptualisation and when building proof-of-concepts.\n\nOnce you've made changes to your project in Docker containers, save a new image with those changes in place. This leaves flexibility for testing two containers comparatively while ensuring stability and consistency across containers.\n\n‍\n\n‍\n",
          "code": "var Component=(()=>{var h=Object.create;var t=Object.defineProperty;var m=Object.getOwnPropertyDescriptor;var u=Object.getOwnPropertyNames;var p=Object.getPrototypeOf,g=Object.prototype.hasOwnProperty;var f=(i,e)=>()=>(e||i((e={exports:{}}).exports,e),e.exports),y=(i,e)=>{for(var o in e)t(i,o,{get:e[o],enumerable:!0})},c=(i,e,o,r)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let a of u(e))!g.call(i,a)&&a!==o&&t(i,a,{get:()=>e[a],enumerable:!(r=m(e,a))||r.enumerable});return i};var k=(i,e,o)=>(o=i!=null?h(p(i)):{},c(e||!i||!i.__esModule?t(o,\"default\",{value:i,enumerable:!0}):o,i)),b=i=>c(t({},\"__esModule\",{value:!0}),i);var l=f((T,s)=>{s.exports=_jsx_runtime});var D={};y(D,{default:()=>N,frontmatter:()=>w});var n=k(l()),w={title:\"How to edit a file in a Docker container\",summary:\"See how to edit files in Docker containers with command line editors or through connecting VS Code.\",publishDate:\"Tue Jul 19 2022 07:37:33 GMT+0000 (Coordinated Universal Time)\",author:\"regis-wilson\",readingTime:3,categories:[\"platform-engineering\",\"product\"],mainImage:\"/blog-images/9ad01e7b63d384efa30ad9fe35b9e74b.jpg\",imageAlt:\"Some files on a table\",showCTA:!0,ctaCopy:\"Looking to edit files in Docker containers hassle-free? Try Release.com for ephemeral environments that simplify setup and teardown.\",ctaLink:\"https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=how-to-edit-a-file-in-a-docker-container\",relatedPosts:[\"\"],ogImage:\"/blog-images/9ad01e7b63d384efa30ad9fe35b9e74b.jpg\",excerpt:\"See how to edit files in Docker containers with command line editors or through connecting VS Code.\",tags:[\"platform-engineering\",\"product\"],ctaButton:\"Try Release for Free\"};function d(i){let e=Object.assign({p:\"p\",h3:\"h3\",a:\"a\",span:\"span\",h4:\"h4\",pre:\"pre\",code:\"code\",img:\"img\",strong:\"strong\"},i.components);return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(e.p,{children:\"You want to edit a file in your Docker container, but you've run into an error that leaves you with none of the tools you need to make your changes. Now what?\"}),`\n`,(0,n.jsx)(e.p,{children:\"Docker intentionally keeps containers as lean as possible with no unnecessary packages installed to maximize performance and stability. Unfortunately, this also means Docker containers don't have a file editor like Vim or Nano preinstalled.\"}),`\n`,(0,n.jsx)(e.p,{children:\"In this guide, we'll show you how to install an editor, make the changes you need to, and return the container to its original state, both from the command line and using the Docker extension inside VS Code.\"}),`\n`,(0,n.jsx)(e.p,{children:\"First, though, some housekeeping. It's considered bad practice to edit Docker files currently running in a production environment, and, once you've made your change, you should remove any packages you installed to do so (the editor, for example).\"}),`\n`,(0,n.jsx)(e.p,{children:\"Here's our step-by-step guide to editing a file in Docker.\"}),`\n`,(0,n.jsxs)(e.h3,{id:\"option-1-edit-from-the-command-line\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#option-1-edit-from-the-command-line\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Option 1: Edit from the command line\"]}),`\n`,(0,n.jsxs)(e.h4,{id:\"1-log-in-to-your-container\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#1-log-in-to-your-container\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"#1 Log in to your container\"]}),`\n`,(0,n.jsx)(e.p,{children:\"If your container is not already running, run the container with the following:\"}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-yaml\",children:`docker run --name  -d -t\n`})}),`\n`,(0,n.jsx)(e.p,{children:\"To check all your running containers, you can use the command:\"}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-yaml\",children:`docker ps\n`})}),`\n`,(0,n.jsx)(e.p,{children:\"You should be met with something like this:\"}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/5269f2732d8885273f30405faaabebd6.png\",alt:\"The console showing the output of docker ps listing the container ID and other information.\"})}),`\n`,(0,n.jsx)(e.p,{children:\"This list indicates your target container is up and running. Note that every container has a discrete ID, which we'll need to gain root access to the container.\"}),`\n`,(0,n.jsx)(e.p,{children:\"To gain root access to the container, run:\"}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-yaml\",children:`docker exec -it\n`})}),`\n`,(0,n.jsx)(e.p,{children:\"You should see something like this:\"}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/449be2be0b420fbd7378193201c1564d.png\",alt:\"A prompt showing that root login has succeeded.\"})}),`\n`,(0,n.jsxs)(e.p,{children:[\"As you can see, \",(0,n.jsx)(e.strong,{children:\"root@CONTAINER_ID:/#\"}),\" indicates we now have root access to the container.\"]}),`\n`,(0,n.jsxs)(e.h4,{id:\"2-install-the-editor\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#2-install-the-editor\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"#2 Install the editor\"]}),`\n`,(0,n.jsx)(e.p,{children:\"It's a good idea to update your package manager before you install the editor. This ensures that you install the latest stable release of the editor. On Ubuntu, that command is:\"}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-yaml\",children:`apt-get update\n`})}),`\n`,(0,n.jsx)(e.p,{children:\"To install your preferred editor, such as Vim, Nano or GNU Emacs:\"}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-yaml\",children:`apt-get install\n`})}),`\n`,(0,n.jsx)(e.p,{children:\"For example, to install Vim:\"}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-yaml\",children:`apt-get install vim\n`})}),`\n`,(0,n.jsxs)(e.h4,{id:\"3-edit-the-file\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#3-edit-the-file\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"#3 Edit the File\"]}),`\n`,(0,n.jsx)(e.p,{children:\"To edit the file, ensure you are in the appropriate directory and use the command:\"}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-yaml\",children:`vim yourfilename.yaml\n`})}),`\n`,(0,n.jsx)(e.p,{children:\"Once you've made the edit to the file, you can remove the editor (in our case, Vim) like this:\"}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-yaml\",children:`apt-get remove vim\n`})}),`\n`,(0,n.jsx)(e.p,{children:\"Or like this:\"}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-yaml\",children:`apt-get purge vim\n`})}),`\n`,(0,n.jsx)(e.p,{children:'The command \"remove\" will remove only Vim, and no other config files or dependencies involved in the initial install. The command \"purge\" will remove all config files associated with Vim. In the interest of leaving no trace, the purge command is probably appropriate in this case.'}),`\n`,(0,n.jsx)(e.p,{children:\"Your package manager may change depending on your OS. These commands are associated with Ubuntu and Vim.\"}),`\n`,(0,n.jsxs)(e.h3,{id:\"persisting-an-editor-for-regular-changes\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#persisting-an-editor-for-regular-changes\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Persisting an editor for regular changes\"]}),`\n`,(0,n.jsx)(e.p,{children:\"The above steps are useful for one-off changes, but if you need to make changes often \\u2013 in a development environment, for example \\u2013 it's best to add your editor to your Dockerfile. This will ensure your chosen editor is always available whenever you spin up another instance of your container.\"}),`\n`,(0,n.jsx)(e.p,{children:\"Add your editor to the Dockerfile like this:\"}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-yaml\",children:`RUN[\"apt-get\", \"update\"]\nRUN[\"apt-get\", \"install\", \"vim\"]\n`})}),`\n`,(0,n.jsx)(e.p,{children:\"Every image created with that Dockerfile will have Vim pre-installed and ready to go.\"}),`\n`,(0,n.jsx)(e.p,{children:'You can replace \"Vim\" with your editor of choice, such as Nano or GNU Emacs. Keep in mind that the commands in the square brackets are specific to Ubuntu Linux. You may need to adapt these to the operating system you are running in your Docker container.'}),`\n`,(0,n.jsxs)(e.h3,{id:\"option-2-edit-from-vs-code\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#option-2-edit-from-vs-code\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Option 2: Edit from VS Code\"]}),`\n`,(0,n.jsx)(e.p,{children:\"If you prefer to use a GUI editor (for example, if you'd like to use your mouse to navigate through large files, or cut and paste text), you can use VS Code.\"}),`\n`,(0,n.jsx)(e.p,{children:'This option requires both the Visual Studio Code IDE and the Docker extension from Microsoft. To install the extension, navigate to the extensions tab in VS Code and type in \"Docker\".'}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/c65ee7d20b5528430ddb2927aaac44c2.png\",alt:\"Search results in VS Code extensions for docker.\"})}),`\n`,(0,n.jsx)(e.p,{children:\"Be sure to select the Docker extension from Microsoft. This extension allows you to easily manage any containers on your system directly from its UI.\"}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/70ac9ac2bf3cda7f1eba36a594a7cc7e.png\",alt:\"VS Code displaying a file inside the docker container.\"})}),`\n`,(0,n.jsx)(e.p,{children:\"From here, treating a container like any file directory, you can navigate to and open files in that container, and make your changes right in VS Code.\"}),`\n`,(0,n.jsxs)(e.h3,{id:\"closing-remarks\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#closing-remarks\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Closing remarks\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Now that you know how to edit files in a Docker file, it's important to take note of the best practice for it.\"}),`\n`,(0,n.jsx)(e.p,{children:\"Editing files in a running Docker container is only recommended when working in a development environment during conceptualisation and when building proof-of-concepts.\"}),`\n`,(0,n.jsx)(e.p,{children:\"Once you've made changes to your project in Docker containers, save a new image with those changes in place. This leaves flexibility for testing two containers comparatively while ensuring stability and consistency across containers.\"}),`\n`,(0,n.jsx)(e.p,{children:\"\\u200D\"}),`\n`,(0,n.jsx)(e.p,{children:\"\\u200D\"})]})}function v(i={}){let{wrapper:e}=i.components||{};return e?(0,n.jsx)(e,Object.assign({},i,{children:(0,n.jsx)(d,i)})):d(i)}var N=v;return b(D);})();\n;return Component;"
        },
        "_id": "blog/posts/how-to-edit-a-file-in-a-docker-container.mdx",
        "_raw": {
          "sourceFilePath": "blog/posts/how-to-edit-a-file-in-a-docker-container.mdx",
          "sourceFileName": "how-to-edit-a-file-in-a-docker-container.mdx",
          "sourceFileDir": "blog/posts",
          "contentType": "mdx",
          "flattenedPath": "blog/posts/how-to-edit-a-file-in-a-docker-container"
        },
        "type": "BlogPost",
        "computedSlug": "how-to-edit-a-file-in-a-docker-container"
      },
      "documentHash": "1739393595020",
      "hasWarnings": false,
      "documentTypeName": "BlogPost"
    },
    "blog/posts/how-to-evolve-your-development-and-deployment-workflow.mdx": {
      "document": {
        "title": "How to Evolve Your Development and Deployment Workflow",
        "summary": "See how on-demand environments can provide an effective development experience for your team.",
        "publishDate": "Tue Jun 20 2023 22:06:15 GMT+0000 (Coordinated Universal Time)",
        "author": "nick-busey",
        "readingTime": 8,
        "categories": [
          "platform-engineering",
          "product"
        ],
        "mainImage": "/blog-images/b5626524d4eecf94addcab5f6b464918.jpg",
        "imageAlt": "How to Evolve Your Development and Deployment Workflow",
        "showCTA": true,
        "ctaCopy": "Reduce bottlenecks with Release's on-demand environments for faster testing and deployment. Optimize your workflow today.",
        "ctaLink": "https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=how-to-evolve-your-development-and-deployment-workflow",
        "relatedPosts": [
          ""
        ],
        "ogImage": "/blog-images/b5626524d4eecf94addcab5f6b464918.jpg",
        "excerpt": "See how on-demand environments can provide an effective development experience for your team.",
        "tags": [
          "platform-engineering",
          "product"
        ],
        "ctaButton": "Try Release for Free",
        "body": {
          "raw": "\nIn my recent PlatformCon talk [“From skateboard to car: How to evolve your developer experience”](https://www.youtube.com/watch?v=9BDfebNQTGM&list=PLR74Ng-6aEfBOEdyuk3OMZu2cLFma4r6b&index=13&t=1s) I talked about different ways teams can evolve their development and deployment workflows. As your team grows and your application becomes more complex, it's important to upgrade your development and deployment workflow to keep up with the needs of the team. In this article I will recap the main points of the talk, and show how on-demand environments can provide an effective development experience for your team.\n\n### What are Environments?\n\nAn environment is any machine or group of machines that serves an application, API, or service. From your laptop to an entire data center, everything in between can be an environment.\n\nEnvironments can be categorized into different types, depending on the purpose and level of access. For instance, a development environment is where developers write and test their code. A testing environment is where the code is tested in a close-to-production environment. A staging environment is a replica of the production environment, where the code is tested before deployment.\n\n### The Problem\n\nAs your team and application grow, you may experience bottlenecks and coordination overhead in your shared \"sandbox\" environment. Waiting for test environments to be created or updated can slow down development and cause frustration.\n\nThe traditional model of testing in a single environment can lead to problems. For example, if multiple developers are working on the same code and trying to test it in the same environment, it can lead to conflicts. Also, if developers are waiting for an environment to be created, they are not able to work on other things during that time. This can lead to a lot of downtime and lower productivity.\n\n### The Solution\n\nOne way to solve this problem is to use ephemeral environments. These are temporary environments that are created on demand and then destroyed when they are no longer needed.\n\nEphemeral environments can reduce developer burnout by removing bottlenecks and reducing the time it takes to ship features and bug fixes. When developers have to wait hours, days, or sometimes even longer for test environments to be created or updated, it can slow down the development process and create frustration.\n\nEphemeral environments remove this bottleneck by allowing developers to create their own environments on demand, without having to wait for someone else to do it for them. This means that developers can test their code in an environment that is specific to their needs without having to worry about conflicts with other developers.\n\nAlso, ephemeral environments are cost-effective and can lead to a more efficient workflow. They allow developers to work more independently and focus on their work instead of coordinating with others to get access to an environment.\n\n### Implementing On-Demand Environments\n\nThere are many different tools available to implement on-demand environments, with varying levels of complexity and functionality. We'll start with an example \"skateboard\" approach, and discuss more advanced tooling until we get to the \"car\" level.\n\n#### 🛹 Skateboard\n\nThe simplest option is to just build your environments by hand. This requires a lot of manual effort, but it does work. Typically done on an on-premise server or a cloud VPS, this is usually the first stop for most teams. Make a \"QA\" or \"staging\" environment, maybe both if you're feeling fancy, and call it good.\n\nData issues crop up often at this level. Seed files are notoriously difficult to wrangle into actually representing anything close to what production data would look like. This often leads to bugs slipping through the cracks, because the data just isn't there to expose the bug in the first place.  \n\nThis approach is suitable for small teams, but it can quickly become unmanageable as the team grows, and it is not on-demand.\n**🎯 When to use:** Small team; Very early stage\n**🧩 How to build it:** On-prem server or Cloud server; Lots of manual work\n**👍 Pros:** Simplest to set up initially; Easiest to understand and debug\n**👎 Cons:** Creates bottlenecks; Coordination and maintenance overhead\n\n#### 🚲 Bicycle\n\nTools like GitLab and Rancher can automate environment creation on a per-branch basis or through a button push. This approach is more automated and can save time and effort by reducing the toil of coordinating who/what gets to use which environment when, while still giving you a lot of control over the environment.  \nGitHub Codespaces integrates tightly with your existing workflow, but you'll still need to know enough about how your infrastructure is designed to be able to configure everything correctly.\n\nThis approach reduces coordination overhead but still requires significant maintenance, and the data issues often still persist at this level.\n**🎯 When to use:** Growing team; Encountering environment bottlenecks\n**🧩 How to build it:** [GitLab](https://about.gitlab.com/) + [Rancher](https://www.rancher.com/) or [GitHub Codespaces](https://github.com/features/codespaces)\n**👍 Pros**: Low costs; Low maintenance overhead\n**👎 Cons:** Limited flexibility; Still decent bit of initial set up\n\n#### 🛵 Motorcycle\n\nCloud-based platforms like Heroku automate environment deployments, but can lead to high vendor lock-in and cost. This stage is generally most useful for teams without much or any dedicated infrastructure / DevOps / SRE resources, and while requiring less maintenance than a home baked deployment system, it can lead to some pretty high bills.\n\n🎯 **When to use:** Medium to large dev team(s); Limited ops talent\n**🧩 How to build it:** [Heroku](https://www.heroku.com/home); [Fly.io](https://fly.io/); [Render](https://render.com/); [Vercel](https://vercel.com/)\n**👍 Pros:** Easier setup than manual options; Offload operational overhead\n\n👎 **Cons:** Cost can be higher; More reliant on 3rd party services\n\n#### 🏎️ Car\n\nEnterprise-ready Environment-as-a-Service companies like Okteto, Ergomake, and Release provide the most fully-featured environment solution, but can be costly.  \nEach of these options has its own advantages and disadvantages. The best option for your team will depend on your specific needs and budget.\n\nThis stage may seem like overkill for smaller teams, but I would argue the benefits outweigh the drawbacks. Having as close to production environments as possible will increase the velocity of teams of any size, and give you confidence when deploying new code that it's already been tested in a production-like environment. Depending on the provider, you can use helm charts with your own kubernetes cluster to automatically spin up and tear down environments that mirror production at the individual component level.\n\nSome allow you to pause environments when not in use, reducing the costs. And some have features like Instant Datasets that mirror production data (scrubbed of PII) and avoid data integrity issues. Many services also offer Remote Development options, where developers can edit code on their local machine, and those changes are synced directly and immediately to the cloud environment. \\\\It really is the best of both worlds as far as speed of iteration and environment correctness.\n\nIt really is the best of both worlds as far as speed of iteration and environment correctness.\n**🎯 When to use:** Need enterprise features; On-prem requirement; Advanced use cases\n**🧩 How to build it:** [Release.com](https://release.com/); [Okteto](https://www.okteto.com/); [Ergomake](https://ergomake.dev/); [BunnyShell](https://www.bunnyshell.com/); [Massdriver](https://blog.massdriver.cloud/changelog/2023-01-19-preview-environments/); [Shipyard](https://shipyard.build/); [Acorn](https://docs.acorn.io/#what-is-acorn); [Tugboat](https://www.tugboatqa.com/)\n**👍 Pros:** Professional level support; Easy onboarding; Power-user features\n**👎 Cons:** Switching costs can be high; Many features leads to more footguns\n\n### Conclusion\n\nWhether you choose to go with a skateboard, a car, or something in between, the most important thing is to find the approach that works best for your team and your product. Continually reevaluating your needs as your team and application grow is crucial. By providing fast, efficient, and fulfilling environments, you can increase productivity, avoid burnout, and provide better outcomes for your product and users.\n\nThe benefits of using on-demand environments are clear. They can help you reduce coordination overhead, increase productivity, and improve the quality of your code. By implementing the right tools and processes, you can create an environment that is tailored to the needs of your team and your product. And by adopting new technologies and best practices, you can ensure that your team is always working at their best.\n\n‍\n",
          "code": "var Component=(()=>{var h=Object.create;var a=Object.defineProperty;var m=Object.getOwnPropertyDescriptor;var p=Object.getOwnPropertyNames;var u=Object.getPrototypeOf,f=Object.prototype.hasOwnProperty;var v=(t,e)=>()=>(e||t((e={exports:{}}).exports,e),e.exports),g=(t,e)=>{for(var o in e)a(t,o,{get:e[o],enumerable:!0})},s=(t,e,o,i)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let r of p(e))!f.call(t,r)&&r!==o&&a(t,r,{get:()=>e[r],enumerable:!(i=m(e,r))||i.enumerable});return t};var y=(t,e,o)=>(o=t!=null?h(u(t)):{},s(e||!t||!t.__esModule?a(o,\"default\",{value:t,enumerable:!0}):o,t)),w=t=>s(a({},\"__esModule\",{value:!0}),t);var c=v((C,l)=>{l.exports=_jsx_runtime});var x={};g(x,{default:()=>T,frontmatter:()=>b});var n=y(c()),b={title:\"How to Evolve Your Development and Deployment Workflow\",summary:\"See how on-demand environments can provide an effective development experience for your team.\",publishDate:\"Tue Jun 20 2023 22:06:15 GMT+0000 (Coordinated Universal Time)\",author:\"nick-busey\",readingTime:8,categories:[\"platform-engineering\",\"product\"],mainImage:\"/blog-images/b5626524d4eecf94addcab5f6b464918.jpg\",imageAlt:\"How to Evolve Your Development and Deployment Workflow\",showCTA:!0,ctaCopy:\"Reduce bottlenecks with Release's on-demand environments for faster testing and deployment. Optimize your workflow today.\",ctaLink:\"https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=how-to-evolve-your-development-and-deployment-workflow\",relatedPosts:[\"\"],ogImage:\"/blog-images/b5626524d4eecf94addcab5f6b464918.jpg\",excerpt:\"See how on-demand environments can provide an effective development experience for your team.\",tags:[\"platform-engineering\",\"product\"],ctaButton:\"Try Release for Free\"};function d(t){let e=Object.assign({p:\"p\",a:\"a\",h3:\"h3\",span:\"span\",h4:\"h4\",strong:\"strong\",br:\"br\"},t.components);return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsxs)(e.p,{children:[\"In my recent PlatformCon talk \",(0,n.jsx)(e.a,{href:\"https://www.youtube.com/watch?v=9BDfebNQTGM&list=PLR74Ng-6aEfBOEdyuk3OMZu2cLFma4r6b&index=13&t=1s\",children:\"\\u201CFrom skateboard to car: How to evolve your developer experience\\u201D\"}),\" I talked about different ways teams can evolve their development and deployment workflows. As your team grows and your application becomes more complex, it's important to upgrade your development and deployment workflow to keep up with the needs of the team. In this article I will recap the main points of the talk, and show how on-demand environments can provide an effective development experience for your team.\"]}),`\n`,(0,n.jsxs)(e.h3,{id:\"what-are-environments\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#what-are-environments\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"What are Environments?\"]}),`\n`,(0,n.jsx)(e.p,{children:\"An environment is any machine or group of machines that serves an application, API, or service. From your laptop to an entire data center, everything in between can be an environment.\"}),`\n`,(0,n.jsx)(e.p,{children:\"Environments can be categorized into different types, depending on the purpose and level of access. For instance, a development environment is where developers write and test their code. A testing environment is where the code is tested in a close-to-production environment. A staging environment is a replica of the production environment, where the code is tested before deployment.\"}),`\n`,(0,n.jsxs)(e.h3,{id:\"the-problem\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#the-problem\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"The Problem\"]}),`\n`,(0,n.jsx)(e.p,{children:'As your team and application grow, you may experience bottlenecks and coordination overhead in your shared \"sandbox\" environment. Waiting for test environments to be created or updated can slow down development and cause frustration.'}),`\n`,(0,n.jsx)(e.p,{children:\"The traditional model of testing in a single environment can lead to problems. For example, if multiple developers are working on the same code and trying to test it in the same environment, it can lead to conflicts. Also, if developers are waiting for an environment to be created, they are not able to work on other things during that time. This can lead to a lot of downtime and lower productivity.\"}),`\n`,(0,n.jsxs)(e.h3,{id:\"the-solution\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#the-solution\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"The Solution\"]}),`\n`,(0,n.jsx)(e.p,{children:\"One way to solve this problem is to use ephemeral environments. These are temporary environments that are created on demand and then destroyed when they are no longer needed.\"}),`\n`,(0,n.jsx)(e.p,{children:\"Ephemeral environments can reduce developer burnout by removing bottlenecks and reducing the time it takes to ship features and bug fixes. When developers have to wait hours, days, or sometimes even longer for test environments to be created or updated, it can slow down the development process and create frustration.\"}),`\n`,(0,n.jsx)(e.p,{children:\"Ephemeral environments remove this bottleneck by allowing developers to create their own environments on demand, without having to wait for someone else to do it for them. This means that developers can test their code in an environment that is specific to their needs without having to worry about conflicts with other developers.\"}),`\n`,(0,n.jsx)(e.p,{children:\"Also, ephemeral environments are cost-effective and can lead to a more efficient workflow. They allow developers to work more independently and focus on their work instead of coordinating with others to get access to an environment.\"}),`\n`,(0,n.jsxs)(e.h3,{id:\"implementing-on-demand-environments\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#implementing-on-demand-environments\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Implementing On-Demand Environments\"]}),`\n`,(0,n.jsx)(e.p,{children:`There are many different tools available to implement on-demand environments, with varying levels of complexity and functionality. We'll start with an example \"skateboard\" approach, and discuss more advanced tooling until we get to the \"car\" level.`}),`\n`,(0,n.jsxs)(e.h4,{id:\"-skateboard\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#-skateboard\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"\\u{1F6F9} Skateboard\"]}),`\n`,(0,n.jsx)(e.p,{children:`The simplest option is to just build your environments by hand. This requires a lot of manual effort, but it does work. Typically done on an on-premise server or a cloud VPS, this is usually the first stop for most teams. Make a \"QA\" or \"staging\" environment, maybe both if you're feeling fancy, and call it good.`}),`\n`,(0,n.jsx)(e.p,{children:\"Data issues crop up often at this level. Seed files are notoriously difficult to wrangle into actually representing anything close to what production data would look like. This often leads to bugs slipping through the cracks, because the data just isn't there to expose the bug in the first place. \\xA0\"}),`\n`,(0,n.jsxs)(e.p,{children:[`This approach is suitable for small teams, but it can quickly become unmanageable as the team grows, and it is not on-demand.\n`,(0,n.jsx)(e.strong,{children:\"\\u{1F3AF} When to use:\"}),` Small team; Very early stage\n`,(0,n.jsx)(e.strong,{children:\"\\u{1F9E9} How to build it:\"}),` On-prem server or Cloud server; Lots of manual work\n`,(0,n.jsx)(e.strong,{children:\"\\u{1F44D} Pros:\"}),` Simplest to set up initially; Easiest to understand and debug\n`,(0,n.jsx)(e.strong,{children:\"\\u{1F44E} Cons:\"}),\" Creates bottlenecks; Coordination and maintenance overhead\"]}),`\n`,(0,n.jsxs)(e.h4,{id:\"-bicycle\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#-bicycle\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"\\u{1F6B2} Bicycle\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"Tools like GitLab and Rancher can automate environment creation on a per-branch basis or through a button push. This approach is more automated and can save time and effort by reducing the toil of coordinating who/what gets to use which environment when, while still giving you a lot of control over the environment.\",(0,n.jsx)(e.br,{}),`\n`,\"GitHub Codespaces integrates tightly with your existing workflow, but you'll still need to know enough about how your infrastructure is designed to be able to configure everything correctly.\"]}),`\n`,(0,n.jsxs)(e.p,{children:[`This approach reduces coordination overhead but still requires significant maintenance, and the data issues often still persist at this level.\n`,(0,n.jsx)(e.strong,{children:\"\\u{1F3AF} When to use:\"}),` Growing team; Encountering environment bottlenecks\n`,(0,n.jsx)(e.strong,{children:\"\\u{1F9E9} How to build it:\"}),\" \",(0,n.jsx)(e.a,{href:\"https://about.gitlab.com/\",children:\"GitLab\"}),\" + \",(0,n.jsx)(e.a,{href:\"https://www.rancher.com/\",children:\"Rancher\"}),\" or \",(0,n.jsx)(e.a,{href:\"https://github.com/features/codespaces\",children:\"GitHub Codespaces\"}),`\n`,(0,n.jsx)(e.strong,{children:\"\\u{1F44D} Pros\"}),`: Low costs; Low maintenance overhead\n`,(0,n.jsx)(e.strong,{children:\"\\u{1F44E} Cons:\"}),\" Limited flexibility; Still decent bit of initial set up\"]}),`\n`,(0,n.jsxs)(e.h4,{id:\"-motorcycle\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#-motorcycle\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"\\u{1F6F5} Motorcycle\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Cloud-based platforms like Heroku automate environment deployments, but can lead to high vendor lock-in and cost. This stage is generally most useful for teams without much or any dedicated infrastructure / DevOps / SRE resources, and while requiring less maintenance than a home baked deployment system, it can lead to some pretty high bills.\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"\\u{1F3AF} \",(0,n.jsx)(e.strong,{children:\"When to use:\"}),` Medium to large dev team(s); Limited ops talent\n`,(0,n.jsx)(e.strong,{children:\"\\u{1F9E9} How to build it:\"}),\" \",(0,n.jsx)(e.a,{href:\"https://www.heroku.com/home\",children:\"Heroku\"}),\"; \",(0,n.jsx)(e.a,{href:\"https://fly.io/\",children:\"Fly.io\"}),\"; \",(0,n.jsx)(e.a,{href:\"https://render.com/\",children:\"Render\"}),\"; \",(0,n.jsx)(e.a,{href:\"https://vercel.com/\",children:\"Vercel\"}),`\n`,(0,n.jsx)(e.strong,{children:\"\\u{1F44D} Pros:\"}),\" Easier setup than manual options; Offload operational overhead\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"\\u{1F44E} \",(0,n.jsx)(e.strong,{children:\"Cons:\"}),\" Cost can be higher; More reliant on 3rd party services\"]}),`\n`,(0,n.jsxs)(e.h4,{id:\"\\uFE0F-car\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#\\uFE0F-car\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"\\u{1F3CE}\\uFE0F Car\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"Enterprise-ready Environment-as-a-Service companies like Okteto, Ergomake, and Release provide the most fully-featured environment solution, but can be costly.\",(0,n.jsx)(e.br,{}),`\n`,\"Each of these options has its own advantages and disadvantages. The best option for your team will depend on your specific needs and budget.\"]}),`\n`,(0,n.jsx)(e.p,{children:\"This stage may seem like overkill for smaller teams, but I would argue the benefits outweigh the drawbacks. Having as close to production environments as possible will increase the velocity of teams of any size, and give you confidence when deploying new code that it's already been tested in a production-like environment. Depending on the provider, you can use helm charts with your own kubernetes cluster to automatically spin up and tear down environments that mirror production at the individual component level.\"}),`\n`,(0,n.jsx)(e.p,{children:\"Some allow you to pause environments when not in use, reducing the costs. And some have features like Instant Datasets that mirror production data (scrubbed of PII) and avoid data integrity issues. Many services also offer Remote Development options, where developers can edit code on their local machine, and those changes are synced directly and immediately to the cloud environment. \\\\It really is the best of both worlds as far as speed of iteration and environment correctness.\"}),`\n`,(0,n.jsxs)(e.p,{children:[`It really is the best of both worlds as far as speed of iteration and environment correctness.\n`,(0,n.jsx)(e.strong,{children:\"\\u{1F3AF} When to use:\"}),` Need enterprise features; On-prem requirement; Advanced use cases\n`,(0,n.jsx)(e.strong,{children:\"\\u{1F9E9} How to build it:\"}),\" \",(0,n.jsx)(e.a,{href:\"https://release.com/\",children:\"Release.com\"}),\"; \",(0,n.jsx)(e.a,{href:\"https://www.okteto.com/\",children:\"Okteto\"}),\"; \",(0,n.jsx)(e.a,{href:\"https://ergomake.dev/\",children:\"Ergomake\"}),\"; \",(0,n.jsx)(e.a,{href:\"https://www.bunnyshell.com/\",children:\"BunnyShell\"}),\"; \",(0,n.jsx)(e.a,{href:\"https://blog.massdriver.cloud/changelog/2023-01-19-preview-environments/\",children:\"Massdriver\"}),\"; \",(0,n.jsx)(e.a,{href:\"https://shipyard.build/\",children:\"Shipyard\"}),\"; \",(0,n.jsx)(e.a,{href:\"https://docs.acorn.io/#what-is-acorn\",children:\"Acorn\"}),\"; \",(0,n.jsx)(e.a,{href:\"https://www.tugboatqa.com/\",children:\"Tugboat\"}),`\n`,(0,n.jsx)(e.strong,{children:\"\\u{1F44D} Pros:\"}),` Professional level support; Easy onboarding; Power-user features\n`,(0,n.jsx)(e.strong,{children:\"\\u{1F44E} Cons:\"}),\" Switching costs can be high; Many features leads to more footguns\"]}),`\n`,(0,n.jsxs)(e.h3,{id:\"conclusion\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#conclusion\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Conclusion\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Whether you choose to go with a skateboard, a car, or something in between, the most important thing is to find the approach that works best for your team and your product. Continually reevaluating your needs as your team and application grow is crucial. By providing fast, efficient, and fulfilling environments, you can increase productivity, avoid burnout, and provide better outcomes for your product and users.\"}),`\n`,(0,n.jsx)(e.p,{children:\"The benefits of using on-demand environments are clear. They can help you reduce coordination overhead, increase productivity, and improve the quality of your code. By implementing the right tools and processes, you can create an environment that is tailored to the needs of your team and your product. And by adopting new technologies and best practices, you can ensure that your team is always working at their best.\"}),`\n`,(0,n.jsx)(e.p,{children:\"\\u200D\"})]})}function k(t={}){let{wrapper:e}=t.components||{};return e?(0,n.jsx)(e,Object.assign({},t,{children:(0,n.jsx)(d,t)})):d(t)}var T=k;return w(x);})();\n;return Component;"
        },
        "_id": "blog/posts/how-to-evolve-your-development-and-deployment-workflow.mdx",
        "_raw": {
          "sourceFilePath": "blog/posts/how-to-evolve-your-development-and-deployment-workflow.mdx",
          "sourceFileName": "how-to-evolve-your-development-and-deployment-workflow.mdx",
          "sourceFileDir": "blog/posts",
          "contentType": "mdx",
          "flattenedPath": "blog/posts/how-to-evolve-your-development-and-deployment-workflow"
        },
        "type": "BlogPost",
        "computedSlug": "how-to-evolve-your-development-and-deployment-workflow"
      },
      "documentHash": "1739393595020",
      "hasWarnings": false,
      "documentTypeName": "BlogPost"
    },
    "blog/posts/how-to-get-started-with-infrastructure-as-code-on-aws.mdx": {
      "document": {
        "title": "How to Get Started With Infrastructure As Code on AWS",
        "summary": "A quick overview of getting started with Infrastructure as Code (IaC) on AWS (Cloudform and Terraform).",
        "publishDate": "Wed Sep 14 2022 16:15:36 GMT+0000 (Coordinated Universal Time)",
        "author": "kelsey-degeorge",
        "readingTime": 3,
        "categories": [
          "platform-engineering",
          "product"
        ],
        "mainImage": "/blog-images/edd60d391195d9581fc3cd171ba51ff1.jpg",
        "imageAlt": "a close up of a keyboard",
        "showCTA": true,
        "ctaCopy": "[Automate infrastructure setup like AWS with Release's on-demand environments. Streamline workflows, reduce costs, and accelerate deployments.]",
        "ctaLink": "https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=how-to-get-started-with-infrastructure-as-code-on-aws",
        "relatedPosts": [
          ""
        ],
        "ogImage": "/blog-images/edd60d391195d9581fc3cd171ba51ff1.jpg",
        "excerpt": "A quick overview of getting started with Infrastructure as Code (IaC) on AWS (Cloudform and Terraform).",
        "tags": [
          "platform-engineering",
          "product"
        ],
        "ctaButton": "Try Release for Free",
        "body": {
          "raw": "\nInfrastructure as code (IaC) is a crucial component of the DevOps playbook. It approaches infrastructure by including it in the code, as code, ensuring consistent configurations and environments across many stages of a project.\n\nAWS is an excellent tool to use to facilitate this, as it offers many ways to create and operate infrastructure as code.\n\nIn this post, we'll take a look at how to get started with this approach and what it should look like. We'll also look at two tools AWS uses for building infrastructure—CloudFormation and Terraform—as well as best practices for infrastructure as code on AWS.\n\n### What Is Infrastructure as Code?\n\nThe key goal of infrastructure as code is to automate infrastructure. Rather than having a system administrator (or several, as is usually the case) devote valuable work hours to manually setting up configurations on every single server, configurations can be stored as code.\n\nThis configuration then provisions and maintains servers with well-documented and version-tracked code. This comes with several advantages, including:\n\n- **Speed**—By having a standard code to run for provisioning, deployment, and production, the entire pipeline becomes faster and simpler. It cuts down on decision-making and implementation time needed to run environments significantly.\n- **Less manual effort** — System administrators don't need to handle every facet of infrastructure, meaning less work and lower cost.\n- **Prevents environment drift**—When system administrators handle infrastructure on a server-by-server basis, each configuration is ultimately handled in different ways. This causes the configurations to \"drift\" further and further apart until the configuration active on one server is impossible to repeat on another. IaC provides a consistent configuration for each environment.\n\n![](/blog-images/95832d4a6cd1f353b3c85efc94bf4b23.jpeg)\n\n#### Infrastructure As Code on AWS\n\nAWS is the lead cloud-computing platform in today's market.  As such, many infrastructure as code tools are built for or to integrate smoothly with AWS.\n\nHowever, AWS isn't the go-to infrastructure as a service (IaaS) platform only because it's the most common.\n\nIt also comes with a number of advantages. The first and foremost benefit of AWS—and the one we'll focus on here—is that it only charges for computational usage. This goes hand-in-hand with infrastructure as code, as it streamlines provisioning even further.\n\nYou allocate servers through AWS, initialize them immediately with an already-coded configuration (aided by AWS' tools for exactly that), and you have a consistent environment ready to go.\n\nBut enough about how easy infrastructure as code is. Let's take a look at how to actually set it up and what it should look like!\n\nIn this post, we'll look at two tools: CloudFormation and Terraform. If you're ready to get started with your infrastructure setup, you've likely already chosen a tool to work with. However, as a quick recap, [CloudFormation](https://docs.aws.amazon.com/cloudformation/index.html) is an AWS service for setting up AWS resources, and [Terraform](https://www.terraform.io/intro) is an open-source infrastructure as code tool by HashiCorp.\n\n#### CloudFormation\n\n##### Templates\n\nBoth CloudFormation and Terraform are declarative tools. This means that rather than defining what steps to take while provisioning infrastructure, you'll be defining a desired _end_ state. CloudFormation then decides the path to take to achieve that state.\n\nYou'll use a template to tell CloudFormation what configurations you want created in the stack. The template is a text file written in JSON or YAML formats. It doesn't really matter which format you use. It can be slightly more advantageous to use YAML, as parsers can read JSON and not vice versa. It's also the default format for other tools, such as Kubernetes and [Release's build files,](https://docs.release.com/reference-documentation/release.yaml) so for consistency's sake, if you're using Release for your project, it might be easier to keep everything in YAML.\n\nThe important part is that you have all your desired configuration settings and resources matched to string keys that explain what they do to both the parser and anyone who might be reading the code—for example, \"WebServer\" followed by specifications for desired web server properties.\n\nReadability is important for template files, as anyone working with this configuration should be able to check the specifics of it.\n\n##### Stacks\n\nOnce you have a template file, this is used to create a stack. CloudFormation checks if the resources listed in the template are available and creates the stack based on the uploaded template. Use the Events tab to monitor the creation process.\n\n![events tab showing progress creating stacks](/blog-images/868973b67f4e87560d5e0597da00262a.png)\n\n_Source: aws.amazon.com_\n\nOnce the Events tab shows \"CREATE_COMPLETE\" for a new stack, that stack is ready to go! You can start using those resources right away.\n\n[For more details, read through AWS' CloudFormation documentation.](https://docs.aws.amazon.com/cloudformation/index.html)\n\n#### Terraform\n\nTerraform is an independent program, meaning you'll have to link it to AWS first. Do this by creating an access key for your AWS account and setting the access key environment variables in Terraform to your access key.\n\nOnce you've done this, you can get to work on defining an AWS-based infrastructure in Terraform. Terraform has its own native configuration language, and while Terraform also includes functions for encoding and decoding both JSON and YAML, the YAML functions are experimental, so it's best to keep your configuration files in Terraform's native language.\n\nSimilar to CloudFormation's template files, the configuration files declare the desired configuration and settings. For a detailed overview of these steps and how to write these configuration files, [make sure to read Terraform's AWS-specific documentation.](https://learn.hashicorp.com/collections/terraform/aws-get-started)\n\nWe then have to implement the configuration files. Terraform does this via the CLI. The configuration is initialized, formatted, validated, and applied using the following Terraform commands:\n\n```yaml\nterraform init\nterraform fmt\nterraform validate\nterraform apply\n```\n\nOnce we reach the \"apply\" stage, we see one of the notable differences between CloudFormation and Terraform. While both are declarative and create a plan to reach the defined end state, Terraform prints this execution plan into the CLI for user approval, detailing the changes made to the infrastructure to achieve the desired configuration. This gives you a bit more control over what exactly happens to your infrastructure.\n\nAfter approving this execution plan, Terraform applies the steps. This stage is, again, quite similar to the CloudFormation process, though it uses a CLI to monitor instead of a GUI tab.\n\nThe end result can be reviewed with the command\n\n```yaml\nterraform show\n```\n\n#### Best Practices for IaC on AWS\n\nAWS' focus lies in providing infrastructure and infrastructure as code. As such, they have extensive support and documentation, and it's worth reading their own [best practices for infrastructure](https://docs.aws.amazon.com/cdk/v2/guide/best-practices.html) section. However, let's look at a few of the most important considerations.\n\n- **Streamline**—Because this infrastructure as code _is_ code, it behaves like code. It's version-tracked, duplicated, and used often. By eliminating duplicates and discrepancies, you can avoid overusing resources to store and integrate the code. Try to avoid unnecessary duplicates of code or code segments, and avoid unnecessary code segments overall.\n- **Test**—Infrastructure as code is also a form of automation and, as such, unit testing and other fail-safes are crucial to ensure that your code is actually doing what it's supposed to be doing.\n- **Clarify**—Infrastructure as code is meant to be used often and will likely be used by several members of your team. Make sure your code is readable, well-documented, and able to be quickly interpret by others. YAML and Terraform's native language are conducive to easy-to-read code, so with a little bit of attention, this is easy enough to achieve.\n\nThere's also a lot to be said for using [environments as a service (EaaS)](https://release.com/blog/environments-as-a-service-eaas-top-3-benefits) to make sure you're getting the most out of AWS with streamlined and tested environments. Release also [has existing AWS support.](https://docs.aws.amazon.com/cdk/v2/guide/best-practices.html)\n\n### Conclusion\n\nAWS is the leading Infrastructure-as-a-Service provider, with countless resources for infrastructure as code.\n\nAWS documentation has a wealth of tutorials, best practices, and FAQs for infrastructure as code. Additionally, because AWS is so common and robust, the most commonly used IaC tools—such as CloudFormation and Terraform—work with it seamlessly. This means comprehensive support and resources.\n\nWith some attention, you can set up a stable foundation for your infrastructure, making your workflow simpler and more efficient.\n",
          "code": "var Component=(()=>{var h=Object.create;var r=Object.defineProperty;var u=Object.getOwnPropertyDescriptor;var m=Object.getOwnPropertyNames;var f=Object.getPrototypeOf,p=Object.prototype.hasOwnProperty;var g=(a,e)=>()=>(e||a((e={exports:{}}).exports,e),e.exports),y=(a,e)=>{for(var n in e)r(a,n,{get:e[n],enumerable:!0})},i=(a,e,n,s)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let o of m(e))!p.call(a,o)&&o!==n&&r(a,o,{get:()=>e[o],enumerable:!(s=u(e,o))||s.enumerable});return a};var w=(a,e,n)=>(n=a!=null?h(f(a)):{},i(e||!a||!a.__esModule?r(n,\"default\",{value:a,enumerable:!0}):n,a)),v=a=>i(r({},\"__esModule\",{value:!0}),a);var d=g((C,c)=>{c.exports=_jsx_runtime});var T={};y(T,{default:()=>A,frontmatter:()=>b});var t=w(d()),b={title:\"How to Get Started With Infrastructure As Code on AWS\",summary:\"A quick overview of getting started with Infrastructure as Code (IaC) on AWS (Cloudform and Terraform).\",publishDate:\"Wed Sep 14 2022 16:15:36 GMT+0000 (Coordinated Universal Time)\",author:\"kelsey-degeorge\",readingTime:3,categories:[\"platform-engineering\",\"product\"],mainImage:\"/blog-images/edd60d391195d9581fc3cd171ba51ff1.jpg\",imageAlt:\"a close up of a keyboard\",showCTA:!0,ctaCopy:\"[Automate infrastructure setup like AWS with Release's on-demand environments. Streamline workflows, reduce costs, and accelerate deployments.]\",ctaLink:\"https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=how-to-get-started-with-infrastructure-as-code-on-aws\",relatedPosts:[\"\"],ogImage:\"/blog-images/edd60d391195d9581fc3cd171ba51ff1.jpg\",excerpt:\"A quick overview of getting started with Infrastructure as Code (IaC) on AWS (Cloudform and Terraform).\",tags:[\"platform-engineering\",\"product\"],ctaButton:\"Try Release for Free\"};function l(a){let e=Object.assign({p:\"p\",h3:\"h3\",a:\"a\",span:\"span\",ul:\"ul\",li:\"li\",strong:\"strong\",img:\"img\",h4:\"h4\",h5:\"h5\",em:\"em\",pre:\"pre\",code:\"code\"},a.components);return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(e.p,{children:\"Infrastructure as code (IaC) is a crucial component of the DevOps playbook. It approaches infrastructure by including it in the code, as code, ensuring consistent configurations and environments across many stages of a project.\"}),`\n`,(0,t.jsx)(e.p,{children:\"AWS is an excellent tool to use to facilitate this, as it offers many ways to create and operate infrastructure as code.\"}),`\n`,(0,t.jsx)(e.p,{children:\"In this post, we'll take a look at how to get started with this approach and what it should look like. We'll also look at two tools AWS uses for building infrastructure\\u2014CloudFormation and Terraform\\u2014as well as best practices for infrastructure as code on AWS.\"}),`\n`,(0,t.jsxs)(e.h3,{id:\"what-is-infrastructure-as-code\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#what-is-infrastructure-as-code\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"What Is Infrastructure as Code?\"]}),`\n`,(0,t.jsx)(e.p,{children:\"The key goal of infrastructure as code is to automate infrastructure. Rather than having a system administrator (or several, as is usually the case) devote valuable work hours to manually setting up configurations on every single server, configurations can be stored as code.\"}),`\n`,(0,t.jsx)(e.p,{children:\"This configuration then provisions and maintains servers with well-documented and version-tracked code. This comes with several advantages, including:\"}),`\n`,(0,t.jsxs)(e.ul,{children:[`\n`,(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:\"Speed\"}),\"\\u2014By having a standard code to run for provisioning, deployment, and production, the entire pipeline becomes faster and simpler. It cuts down on decision-making and implementation time needed to run environments significantly.\"]}),`\n`,(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:\"Less manual effort\"}),\" \\u2014 System administrators don't need to handle every facet of infrastructure, meaning less work and lower cost.\"]}),`\n`,(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:\"Prevents environment drift\"}),'\\u2014When system administrators handle infrastructure on a server-by-server basis, each configuration is ultimately handled in different ways. This causes the configurations to \"drift\" further and further apart until the configuration active on one server is impossible to repeat on another. IaC provides a consistent configuration for each environment.']}),`\n`]}),`\n`,(0,t.jsx)(e.p,{children:(0,t.jsx)(e.img,{src:\"/blog-images/95832d4a6cd1f353b3c85efc94bf4b23.jpeg\",alt:\"\"})}),`\n`,(0,t.jsxs)(e.h4,{id:\"infrastructure-as-code-on-aws\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#infrastructure-as-code-on-aws\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Infrastructure As Code on AWS\"]}),`\n`,(0,t.jsx)(e.p,{children:\"AWS is the lead cloud-computing platform in today's market. \\xA0As such, many infrastructure as code tools are built for or to integrate smoothly with AWS.\"}),`\n`,(0,t.jsx)(e.p,{children:\"However, AWS isn't the go-to infrastructure as a service (IaaS) platform only because it's the most common.\"}),`\n`,(0,t.jsx)(e.p,{children:\"It also comes with a number of advantages. The first and foremost benefit of AWS\\u2014and the one we'll focus on here\\u2014is that it only charges for computational usage. This goes hand-in-hand with infrastructure as code, as it streamlines provisioning even further.\"}),`\n`,(0,t.jsx)(e.p,{children:\"You allocate servers through AWS, initialize them immediately with an already-coded configuration (aided by AWS' tools for exactly that), and you have a consistent environment ready to go.\"}),`\n`,(0,t.jsx)(e.p,{children:\"But enough about how easy infrastructure as code is. Let's take a look at how to actually set it up and what it should look like!\"}),`\n`,(0,t.jsxs)(e.p,{children:[\"In this post, we'll look at two tools: CloudFormation and Terraform. If you're ready to get started with your infrastructure setup, you've likely already chosen a tool to work with. However, as a quick recap, \",(0,t.jsx)(e.a,{href:\"https://docs.aws.amazon.com/cloudformation/index.html\",children:\"CloudFormation\"}),\" is an AWS service for setting up AWS resources, and \",(0,t.jsx)(e.a,{href:\"https://www.terraform.io/intro\",children:\"Terraform\"}),\" is an open-source infrastructure as code tool by HashiCorp.\"]}),`\n`,(0,t.jsxs)(e.h4,{id:\"cloudformation\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#cloudformation\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"CloudFormation\"]}),`\n`,(0,t.jsxs)(e.h5,{id:\"templates\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#templates\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Templates\"]}),`\n`,(0,t.jsxs)(e.p,{children:[\"Both CloudFormation and Terraform are declarative tools. This means that rather than defining what steps to take while provisioning infrastructure, you'll be defining a desired \",(0,t.jsx)(e.em,{children:\"end\"}),\" state. CloudFormation then decides the path to take to achieve that state.\"]}),`\n`,(0,t.jsxs)(e.p,{children:[\"You'll use a template to tell CloudFormation what configurations you want created in the stack. The template is a text file written in JSON or YAML formats. It doesn't really matter which format you use. It can be slightly more advantageous to use YAML, as parsers can read JSON and not vice versa. It's also the default format for other tools, such as Kubernetes and \",(0,t.jsx)(e.a,{href:\"https://docs.release.com/reference-documentation/release.yaml\",children:\"Release's build files,\"}),\" so for consistency's sake, if you're using Release for your project, it might be easier to keep everything in YAML.\"]}),`\n`,(0,t.jsx)(e.p,{children:'The important part is that you have all your desired configuration settings and resources matched to string keys that explain what they do to both the parser and anyone who might be reading the code\\u2014for example, \"WebServer\" followed by specifications for desired web server properties.'}),`\n`,(0,t.jsx)(e.p,{children:\"Readability is important for template files, as anyone working with this configuration should be able to check the specifics of it.\"}),`\n`,(0,t.jsxs)(e.h5,{id:\"stacks\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#stacks\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Stacks\"]}),`\n`,(0,t.jsx)(e.p,{children:\"Once you have a template file, this is used to create a stack. CloudFormation checks if the resources listed in the template are available and creates the stack based on the uploaded template. Use the Events tab to monitor the creation process.\"}),`\n`,(0,t.jsx)(e.p,{children:(0,t.jsx)(e.img,{src:\"/blog-images/868973b67f4e87560d5e0597da00262a.png\",alt:\"events tab showing progress creating stacks\"})}),`\n`,(0,t.jsx)(e.p,{children:(0,t.jsx)(e.em,{children:\"Source: aws.amazon.com\"})}),`\n`,(0,t.jsx)(e.p,{children:'Once the Events tab shows \"CREATE_COMPLETE\" for a new stack, that stack is ready to go! You can start using those resources right away.'}),`\n`,(0,t.jsx)(e.p,{children:(0,t.jsx)(e.a,{href:\"https://docs.aws.amazon.com/cloudformation/index.html\",children:\"For more details, read through AWS' CloudFormation documentation.\"})}),`\n`,(0,t.jsxs)(e.h4,{id:\"terraform\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#terraform\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Terraform\"]}),`\n`,(0,t.jsx)(e.p,{children:\"Terraform is an independent program, meaning you'll have to link it to AWS first. Do this by creating an access key for your AWS account and setting the access key environment variables in Terraform to your access key.\"}),`\n`,(0,t.jsx)(e.p,{children:\"Once you've done this, you can get to work on defining an AWS-based infrastructure in Terraform. Terraform has its own native configuration language, and while Terraform also includes functions for encoding and decoding both JSON and YAML, the YAML functions are experimental, so it's best to keep your configuration files in Terraform's native language.\"}),`\n`,(0,t.jsxs)(e.p,{children:[\"Similar to CloudFormation's template files, the configuration files declare the desired configuration and settings. For a detailed overview of these steps and how to write these configuration files, \",(0,t.jsx)(e.a,{href:\"https://learn.hashicorp.com/collections/terraform/aws-get-started\",children:\"make sure to read Terraform's AWS-specific documentation.\"})]}),`\n`,(0,t.jsx)(e.p,{children:\"We then have to implement the configuration files. Terraform does this via the CLI. The configuration is initialized, formatted, validated, and applied using the following Terraform commands:\"}),`\n`,(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:\"language-yaml\",children:`terraform init\nterraform fmt\nterraform validate\nterraform apply\n`})}),`\n`,(0,t.jsx)(e.p,{children:'Once we reach the \"apply\" stage, we see one of the notable differences between CloudFormation and Terraform. While both are declarative and create a plan to reach the defined end state, Terraform prints this execution plan into the CLI for user approval, detailing the changes made to the infrastructure to achieve the desired configuration. This gives you a bit more control over what exactly happens to your infrastructure.'}),`\n`,(0,t.jsx)(e.p,{children:\"After approving this execution plan, Terraform applies the steps. This stage is, again, quite similar to the CloudFormation process, though it uses a CLI to monitor instead of a GUI tab.\"}),`\n`,(0,t.jsx)(e.p,{children:\"The end result can be reviewed with the command\"}),`\n`,(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:\"language-yaml\",children:`terraform show\n`})}),`\n`,(0,t.jsxs)(e.h4,{id:\"best-practices-for-iac-on-aws\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#best-practices-for-iac-on-aws\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Best Practices for IaC on AWS\"]}),`\n`,(0,t.jsxs)(e.p,{children:[\"AWS' focus lies in providing infrastructure and infrastructure as code. As such, they have extensive support and documentation, and it's worth reading their own \",(0,t.jsx)(e.a,{href:\"https://docs.aws.amazon.com/cdk/v2/guide/best-practices.html\",children:\"best practices for infrastructure\"}),\" section. However, let's look at a few of the most important considerations.\"]}),`\n`,(0,t.jsxs)(e.ul,{children:[`\n`,(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:\"Streamline\"}),\"\\u2014Because this infrastructure as code \",(0,t.jsx)(e.em,{children:\"is\"}),\" code, it behaves like code. It's version-tracked, duplicated, and used often. By eliminating duplicates and discrepancies, you can avoid overusing resources to store and integrate the code. Try to avoid unnecessary duplicates of code or code segments, and avoid unnecessary code segments overall.\"]}),`\n`,(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:\"Test\"}),\"\\u2014Infrastructure as code is also a form of automation and, as such, unit testing and other fail-safes are crucial to ensure that your code is actually doing what it's supposed to be doing.\"]}),`\n`,(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:\"Clarify\"}),\"\\u2014Infrastructure as code is meant to be used often and will likely be used by several members of your team. Make sure your code is readable, well-documented, and able to be quickly interpret by others. YAML and Terraform's native language are conducive to easy-to-read code, so with a little bit of attention, this is easy enough to achieve.\"]}),`\n`]}),`\n`,(0,t.jsxs)(e.p,{children:[\"There's also a lot to be said for using \",(0,t.jsx)(e.a,{href:\"https://release.com/blog/environments-as-a-service-eaas-top-3-benefits\",children:\"environments as a service (EaaS)\"}),\" to make sure you're getting the most out of AWS with streamlined and tested environments. Release also \",(0,t.jsx)(e.a,{href:\"https://docs.aws.amazon.com/cdk/v2/guide/best-practices.html\",children:\"has existing AWS support.\"})]}),`\n`,(0,t.jsxs)(e.h3,{id:\"conclusion\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#conclusion\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Conclusion\"]}),`\n`,(0,t.jsx)(e.p,{children:\"AWS is the leading Infrastructure-as-a-Service provider, with countless resources for infrastructure as code.\"}),`\n`,(0,t.jsx)(e.p,{children:\"AWS documentation has a wealth of tutorials, best practices, and FAQs for infrastructure as code. Additionally, because AWS is so common and robust, the most commonly used IaC tools\\u2014such as CloudFormation and Terraform\\u2014work with it seamlessly. This means comprehensive support and resources.\"}),`\n`,(0,t.jsx)(e.p,{children:\"With some attention, you can set up a stable foundation for your infrastructure, making your workflow simpler and more efficient.\"})]})}function k(a={}){let{wrapper:e}=a.components||{};return e?(0,t.jsx)(e,Object.assign({},a,{children:(0,t.jsx)(l,a)})):l(a)}var A=k;return v(T);})();\n;return Component;"
        },
        "_id": "blog/posts/how-to-get-started-with-infrastructure-as-code-on-aws.mdx",
        "_raw": {
          "sourceFilePath": "blog/posts/how-to-get-started-with-infrastructure-as-code-on-aws.mdx",
          "sourceFileName": "how-to-get-started-with-infrastructure-as-code-on-aws.mdx",
          "sourceFileDir": "blog/posts",
          "contentType": "mdx",
          "flattenedPath": "blog/posts/how-to-get-started-with-infrastructure-as-code-on-aws"
        },
        "type": "BlogPost",
        "computedSlug": "how-to-get-started-with-infrastructure-as-code-on-aws"
      },
      "documentHash": "1739393595020",
      "hasWarnings": false,
      "documentTypeName": "BlogPost"
    },
    "blog/posts/how-to-make-kubernetes-config-files-not-suck.mdx": {
      "document": {
        "title": "How Do You Make Kubernetes Config Files Not Suck?",
        "summary": "I know that when I try out a new product if it’s hard to see what it does quickly I usually move on.",
        "publishDate": "Wed Feb 03 2021 05:22:59 GMT+0000 (Coordinated Universal Time)",
        "author": "regis-wilson",
        "readingTime": 4,
        "categories": [
          "kubernetes",
          "platform-engineering"
        ],
        "mainImage": "/blog-images/ade0049406c93db8b827edd0f4966435.jpg",
        "imageAlt": "Pile of trash representing bad Kubernetes Config Files",
        "showCTA": true,
        "ctaCopy": "Simplify Kubernetes config management with Release's ephemeral environments, preventing production mishaps and streamlining cluster connections.",
        "ctaLink": "https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=how-to-make-kubernetes-config-files-not-suck",
        "relatedPosts": [
          ""
        ],
        "ogImage": "/blog-images/ade0049406c93db8b827edd0f4966435.jpg",
        "excerpt": "I know that when I try out a new product if it’s hard to see what it does quickly I usually move on.",
        "tags": [
          "kubernetes",
          "platform-engineering"
        ],
        "ctaButton": "Try Release for Free",
        "body": {
          "raw": "\n### How Do You Make Kubernetes Config Files Not Suck?\n\nNothing makes me break out in a panic and cold sweat faster than someone saying, “Edit the YAML config files and push it to production.” I have so many welts and scars on my backside from years of YAML file mishaps in production. I have also personally witnessed and had to try to fix many more such production outages due to YAML files being edited and pushed to production. In some cases, just figuring out what was wrong with the YAML file, much less how to fix it took seemingly endless minutes of frantic searching and scrambling to save a production website that was down and losing money.\n\nPlease note, this is not a JSON vs. YAML or Yet Another Data Language vs. YAML(because YAML Ain’t Markup Language) religious war! You can actually use JSON for Kubernetes configuration files _if you want to_. The real issue is that _there are so many_ of them and _they repeat so often_ and _I don’t know what to put where_ or even _where to find out where to start_.\n\nThere is a very good page of [best practices](https://kubernetes.io/docs/concepts/configuration/overview/) and the documentation for Kubernetes does tend to be surprisingly useful. There are tons of useful videos on Youtube that are helpful, so I am not even complaining about that.\n\nThe problem begins with just trying to connect to a cluster the very first time! The mysteries of the ~/.kube/ directory arise swiftly from the depths and bottom out the boat on your Kubernetes journey before you’ve even begun. Fortunately, there are a lot of ways you can avoid editing or creating the configuration files with a few steps that were enlightening to me; hopefully they will be useful for you.\n\nI like to keep my configuration files separated and specify them explicitly. This prevents me from, say, deploying or sending commands to a production environment by accident. I also tend to have a few pre-production or even developer environments laying about and I want to choose which one I interact with each time. I also don’t want to overwrite any important credentials I may have stored in a default location so I like to keep all my files separated away from the default file names if possible.\n\n### Prerequisites\n\nWe use Amazon Web Services (AWS) managed Kubernetes service called EKS and so my configuration setup is pretty AWS-centric, but by no means unusual. I also run a local Ubuntu 20.20 instance on Windows 10, so even though I have Windows, I’m not a Powershell or Command Prompt user. This will be a Linux/AWS configuration example but it should be usable on a Macintosh, or with proper translation, a native Windows environment. Similarly, you can use the same approach for other cloud providers or on-premise clusters.\n\nYou will need an AWS account, AWS credentials (preferably an Admin, but if your cluster is already created, then just a user), the AWS CLI, EKSCTL, Kubectl commands installed.\n\n#### Your AWS credentials\n\nThe first step is to set up your AWS credentials. You can setup default credentials just by typing\n\n```yaml\n\n$ aws configure\nAWS Access Key ID [None]: accesskey\nAWS Secret Access Key [None]: secretkey\nDefault region name [None]: us-west-2\nDefault output format [None]:\n\n$ aws ec2 describe-instances\n\n```\n\nThis works well for defaults or if you’ve never setup AWS credentials on your computer before. However, I will always move these credentials into a profile that I can access only when needed. Edit your ~/.aws/credentials file with an editor and move your credentials from \\[default\\] to some other named profile, for example if you have a production and development account, your file might look like the following.\n\n```yaml\n\n[default]\n\n[production]\naws_access_key_id = SOMETHING\naws_secret_access_key = SOMETHINGELSE\n\n[development]\naws_access_key_id = SOMETHING\naws_secret_access_key = SOMETHINGELSE\n\n```\n\nThe great thing about this setup is you can choose which environment you want to deploy into, and you won’t accidentally deploy to production if you switch terminal windows or pick up where you left off after a break. On the downside, you will have to remember to always specify your profile in one of several ways, for example:\n\n```yaml\n$ AWS_PROFILE=production aws ec2 describe-instances\n$ aws ec2 describe-instances --profile=production\n```\n\nYou may find that less than convenient, but I enjoy it. I even go so far as not specifying a default region, so that I have to specify both profile and region in my commands (but it prevents me from making a lot of mistakes I would otherwise make):\n\n```yaml\n$ AWS_DEFAULT_REGION=us-west-2 AWS_PROFILE=production aws ec2 describe-instances\n$ aws ec2 describe-instances --profile=production --region=us-west-2\n```\n\nAnd finally, in [Terraform](https://registry.terraform.io/providers/hashicorp/aws/latest/docs#shared-credentials-file), you can easily switch environments by using this existing setup and specifying an input variable for the provider profile.\n\n```yaml\nprovider {\nregion = var.aws_region\nprofile = var.credentials_profile\n}\n```\n\n### The EKS Cluster Configuration\n\nNow, you need to connect to an EKS cluster by generating a file which is known as a kubeconfig. By default, the kubeconfig files will be merged or written into your ~/.kube/config file, or if you have a $KUBECONFIG variable set, into the first file in that list (more on the $KUBECONFIG variable later).\n\nAgain, I break out in hives around anything to do with YAML files and merging multiple configurations into one default file sounds like an easy recipe for disaster or rolling out the wrong changes to production late at night. Ideally, I’d like to keep all my configurations separate and specify them when I need them. I also want to avoid editing files or updating labels in dense, hard to read YAML.\n\nIf you have more than one EKS cluster, and perhaps in separate AWS accounts, I want to make sure I keep them straight. The first step is to create a cluster configuration file and save it to a specific file:\n\n```yaml\n$ AWS_PROFILE=production AWS_DEFAULT_REGION=us-west-2 \\\naws eks update-kubeconfig --name=prodEKS --alias=production \\\n--kubeconfig=~/.kube/config-prod-us-west-2\n```\n\nOne of the great tools you should check out is EKSCTL which has a similar use case:\n\n```yaml\n$ eksctl utils write-kubeconfig --cluster=prodEKS \\\n--kubeconfig=~/.kube/config-prod-us-west-2 \\\n--set-kubeconfig-context --profile=production \\\n--region=us-west-2\n```\n\nI also like to use the --auto-kubeconfig option instead of --kubeconfig because it will save the file in ~/.kube/clusters/<clustername> by default.</clustername>\n\nNow, you can access your cluster by name, for example:\n\n```yaml\n$ AWS_PROFILE=prod kubectl get pods -A -o wide \\\n--kubeconfig=~/.kube/config-prod-us-west-2\n```\n\nSo it gets a bit hairy to keep listing the file to specify which cluster you want to connect with. There must be a better way to do this, and there is luckily a way to specify a [context and merge files](https://kubernetes.io/docs/concepts/configuration/organize-cluster-access-kubeconfig/#the-kubeconfig-environment-variable) to get this to work.\n\nSo let’s say that all your cluster configurations are stored in separate files (which I like) and they all have a convention of starting with ~/.kube/config-\\* or exist in a subdirectory like ~/.kube/clusters/\\*. Now you can create a KUBECONFIG colon-separated list of the files like so:\n\n```\n\nFILES=(~/.kube/config-*); IFS=: eval 'export KUBECONFIG=\"${FILES[*]}\"'\n\n```\n\nAdd the above snippet to your ~/.bash_aliases (or whatever bashrc script you prefer) and then start a new shell and you’ll be able to select a cluster by context:\n\n```\n\n$ exec bash -l # This just loads my exports if I have updated anything\n$ AWS_PROFILE=production kubectl get pods -A -o wide --context=production\n\n```\n\nSo you will need to specify your AWS profile (to gain access credentials to your assumed role for EKS) and also specify the context in order to choose a cluster to connect to. But I find it fairly usable and keeps all my configuration files in separate locations while still being relatively easy to maintain and manage. I also remove the accidental possibility of accessing the incorrect cluster or environment and wreaking havoc.\n\n### Conclusion\n\nIt is possible to keep YAML file configuration and management of EKS kubernetes clusters separate and yet accessible with some flags that switch into the correct environment. Adding and removing clusters and credentials is easily managed in the filesystem and without editing files directly. Also, defaults are removed so that a command does not get executed on the wrong cluster inadvertently.\n",
          "code": "var Component=(()=>{var d=Object.create;var a=Object.defineProperty;var h=Object.getOwnPropertyDescriptor;var p=Object.getOwnPropertyNames;var f=Object.getPrototypeOf,m=Object.prototype.hasOwnProperty;var g=(t,e)=>()=>(e||t((e={exports:{}}).exports,e),e.exports),y=(t,e)=>{for(var o in e)a(t,o,{get:e[o],enumerable:!0})},r=(t,e,o,s)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let i of p(e))!m.call(t,i)&&i!==o&&a(t,i,{get:()=>e[i],enumerable:!(s=h(e,i))||s.enumerable});return t};var w=(t,e,o)=>(o=t!=null?d(f(t)):{},r(e||!t||!t.__esModule?a(o,\"default\",{value:t,enumerable:!0}):o,t)),b=t=>r(a({},\"__esModule\",{value:!0}),t);var l=g((N,c)=>{c.exports=_jsx_runtime});var S={};y(S,{default:()=>I,frontmatter:()=>k});var n=w(l()),k={title:\"How Do You Make Kubernetes Config Files Not Suck?\",summary:\"I know that when I try out a new product if it\\u2019s hard to see what it does quickly I usually move on.\",publishDate:\"Wed Feb 03 2021 05:22:59 GMT+0000 (Coordinated Universal Time)\",author:\"regis-wilson\",readingTime:4,categories:[\"kubernetes\",\"platform-engineering\"],mainImage:\"/blog-images/ade0049406c93db8b827edd0f4966435.jpg\",imageAlt:\"Pile of trash representing bad Kubernetes Config Files\",showCTA:!0,ctaCopy:\"Simplify Kubernetes config management with Release's ephemeral environments, preventing production mishaps and streamlining cluster connections.\",ctaLink:\"https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=how-to-make-kubernetes-config-files-not-suck\",relatedPosts:[\"\"],ogImage:\"/blog-images/ade0049406c93db8b827edd0f4966435.jpg\",excerpt:\"I know that when I try out a new product if it\\u2019s hard to see what it does quickly I usually move on.\",tags:[\"kubernetes\",\"platform-engineering\"],ctaButton:\"Try Release for Free\"};function u(t){let e=Object.assign({h3:\"h3\",a:\"a\",span:\"span\",p:\"p\",em:\"em\",h4:\"h4\",pre:\"pre\",code:\"code\"},t.components);return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsxs)(e.h3,{id:\"how-do-you-make-kubernetes-config-files-not-suck\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#how-do-you-make-kubernetes-config-files-not-suck\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"How Do You Make Kubernetes Config Files Not Suck?\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Nothing makes me break out in a panic and cold sweat faster than someone saying, \\u201CEdit the YAML config files and push it to production.\\u201D I have so many welts and scars on my backside from years of YAML file mishaps in production. I have also personally witnessed and had to try to fix many more such production outages due to YAML files being edited and pushed to production. In some cases, just figuring out what was wrong with the YAML file, much less how to fix it took seemingly endless minutes of frantic searching and scrambling to save a production website that was down and losing money.\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"Please note, this is not a JSON vs. YAML or Yet Another Data Language vs. YAML(because YAML Ain\\u2019t Markup Language) religious war! You can actually use JSON for Kubernetes configuration files \",(0,n.jsx)(e.em,{children:\"if you want to\"}),\". The real issue is that \",(0,n.jsx)(e.em,{children:\"there are so many\"}),\" of them and \",(0,n.jsx)(e.em,{children:\"they repeat so often\"}),\" and \",(0,n.jsx)(e.em,{children:\"I don\\u2019t know what to put where\"}),\" or even \",(0,n.jsx)(e.em,{children:\"where to find out where to start\"}),\".\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"There is a very good page of \",(0,n.jsx)(e.a,{href:\"https://kubernetes.io/docs/concepts/configuration/overview/\",children:\"best practices\"}),\" and the documentation for Kubernetes does tend to be surprisingly useful. There are tons of useful videos on Youtube that are helpful, so I am not even complaining about that.\"]}),`\n`,(0,n.jsx)(e.p,{children:\"The problem begins with just trying to connect to a cluster the very first time! The mysteries of the ~/.kube/ directory arise swiftly from the depths and bottom out the boat on your Kubernetes journey before you\\u2019ve even begun. Fortunately, there are a lot of ways you can avoid editing or creating the configuration files with a few steps that were enlightening to me; hopefully they will be useful for you.\"}),`\n`,(0,n.jsx)(e.p,{children:\"I like to keep my configuration files separated and specify them explicitly. This prevents me from, say, deploying or sending commands to a production environment by accident. I also tend to have a few pre-production or even developer environments laying about and I want to choose which one I interact with each time. I also don\\u2019t want to overwrite any important credentials I may have stored in a default location so I like to keep all my files separated away from the default file names if possible.\"}),`\n`,(0,n.jsxs)(e.h3,{id:\"prerequisites\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#prerequisites\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Prerequisites\"]}),`\n`,(0,n.jsx)(e.p,{children:\"We use Amazon Web Services (AWS) managed Kubernetes service called EKS and so my configuration setup is pretty AWS-centric, but by no means unusual. I also run a local Ubuntu 20.20 instance on Windows 10, so even though I have Windows, I\\u2019m not a Powershell or Command Prompt user. This will be a Linux/AWS configuration example but it should be usable on a Macintosh, or with proper translation, a native Windows environment. Similarly, you can use the same approach for other cloud providers or on-premise clusters.\"}),`\n`,(0,n.jsx)(e.p,{children:\"You will need an AWS account, AWS credentials (preferably an Admin, but if your cluster is already created, then just a user), the AWS CLI, EKSCTL, Kubectl commands installed.\"}),`\n`,(0,n.jsxs)(e.h4,{id:\"your-aws-credentials\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#your-aws-credentials\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Your AWS credentials\"]}),`\n`,(0,n.jsx)(e.p,{children:\"The first step is to set up your AWS credentials. You can setup default credentials just by typing\"}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-yaml\",children:`\n$ aws configure\nAWS Access Key ID [None]: accesskey\nAWS Secret Access Key [None]: secretkey\nDefault region name [None]: us-west-2\nDefault output format [None]:\n\n$ aws ec2 describe-instances\n\n`})}),`\n`,(0,n.jsx)(e.p,{children:\"This works well for defaults or if you\\u2019ve never setup AWS credentials on your computer before. However, I will always move these credentials into a profile that I can access only when needed. Edit your ~/.aws/credentials file with an editor and move your credentials from [default] to some other named profile, for example if you have a production and development account, your file might look like the following.\"}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-yaml\",children:`\n[default]\n\n[production]\naws_access_key_id = SOMETHING\naws_secret_access_key = SOMETHINGELSE\n\n[development]\naws_access_key_id = SOMETHING\naws_secret_access_key = SOMETHINGELSE\n\n`})}),`\n`,(0,n.jsx)(e.p,{children:\"The great thing about this setup is you can choose which environment you want to deploy into, and you won\\u2019t accidentally deploy to production if you switch terminal windows or pick up where you left off after a break. On the downside, you will have to remember to always specify your profile in one of several ways, for example:\"}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-yaml\",children:`$ AWS_PROFILE=production aws ec2 describe-instances\n$ aws ec2 describe-instances --profile=production\n`})}),`\n`,(0,n.jsx)(e.p,{children:\"You may find that less than convenient, but I enjoy it. I even go so far as not specifying a default region, so that I have to specify both profile and region in my commands (but it prevents me from making a lot of mistakes I would otherwise make):\"}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-yaml\",children:`$ AWS_DEFAULT_REGION=us-west-2 AWS_PROFILE=production aws ec2 describe-instances\n$ aws ec2 describe-instances --profile=production --region=us-west-2\n`})}),`\n`,(0,n.jsxs)(e.p,{children:[\"And finally, in \",(0,n.jsx)(e.a,{href:\"https://registry.terraform.io/providers/hashicorp/aws/latest/docs#shared-credentials-file\",children:\"Terraform\"}),\", you can easily switch environments by using this existing setup and specifying an input variable for the provider profile.\"]}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-yaml\",children:`provider {\nregion = var.aws_region\nprofile = var.credentials_profile\n}\n`})}),`\n`,(0,n.jsxs)(e.h3,{id:\"the-eks-cluster-configuration\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#the-eks-cluster-configuration\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"The EKS Cluster Configuration\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Now, you need to connect to an EKS cluster by generating a file which is known as a kubeconfig. By default, the kubeconfig files will be merged or written into your ~/.kube/config file, or if you have a $KUBECONFIG variable set, into the first file in that list (more on the $KUBECONFIG variable later).\"}),`\n`,(0,n.jsx)(e.p,{children:\"Again, I break out in hives around anything to do with YAML files and merging multiple configurations into one default file sounds like an easy recipe for disaster or rolling out the wrong changes to production late at night. Ideally, I\\u2019d like to keep all my configurations separate and specify them when I need them. I also want to avoid editing files or updating labels in dense, hard to read YAML.\"}),`\n`,(0,n.jsx)(e.p,{children:\"If you have more than one EKS cluster, and perhaps in separate AWS accounts, I want to make sure I keep them straight. The first step is to create a cluster configuration file and save it to a specific file:\"}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-yaml\",children:`$ AWS_PROFILE=production AWS_DEFAULT_REGION=us-west-2 \\\\\naws eks update-kubeconfig --name=prodEKS --alias=production \\\\\n--kubeconfig=~/.kube/config-prod-us-west-2\n`})}),`\n`,(0,n.jsx)(e.p,{children:\"One of the great tools you should check out is EKSCTL which has a similar use case:\"}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-yaml\",children:`$ eksctl utils write-kubeconfig --cluster=prodEKS \\\\\n--kubeconfig=~/.kube/config-prod-us-west-2 \\\\\n--set-kubeconfig-context --profile=production \\\\\n--region=us-west-2\n`})}),`\n`,(0,n.jsxs)(e.p,{children:[\"I also like to use the --auto-kubeconfig option instead of --kubeconfig because it will save the file in ~/.kube/clusters/\",(0,n.jsx)(\"clustername\",{children:\" by default.\"})]}),`\n`,(0,n.jsx)(e.p,{children:\"Now, you can access your cluster by name, for example:\"}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-yaml\",children:`$ AWS_PROFILE=prod kubectl get pods -A -o wide \\\\\n--kubeconfig=~/.kube/config-prod-us-west-2\n`})}),`\n`,(0,n.jsxs)(e.p,{children:[\"So it gets a bit hairy to keep listing the file to specify which cluster you want to connect with. There must be a better way to do this, and there is luckily a way to specify a \",(0,n.jsx)(e.a,{href:\"https://kubernetes.io/docs/concepts/configuration/organize-cluster-access-kubeconfig/#the-kubeconfig-environment-variable\",children:\"context and merge files\"}),\" to get this to work.\"]}),`\n`,(0,n.jsx)(e.p,{children:\"So let\\u2019s say that all your cluster configurations are stored in separate files (which I like) and they all have a convention of starting with ~/.kube/config-* or exist in a subdirectory like ~/.kube/clusters/*. Now you can create a KUBECONFIG colon-separated list of the files like so:\"}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{children:`\nFILES=(~/.kube/config-*); IFS=: eval 'export KUBECONFIG=\"\\${FILES[*]}\"'\n\n`})}),`\n`,(0,n.jsx)(e.p,{children:\"Add the above snippet to your ~/.bash_aliases (or whatever bashrc script you prefer) and then start a new shell and you\\u2019ll be able to select a cluster by context:\"}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{children:`\n$ exec bash -l # This just loads my exports if I have updated anything\n$ AWS_PROFILE=production kubectl get pods -A -o wide --context=production\n\n`})}),`\n`,(0,n.jsx)(e.p,{children:\"So you will need to specify your AWS profile (to gain access credentials to your assumed role for EKS) and also specify the context in order to choose a cluster to connect to. But I find it fairly usable and keeps all my configuration files in separate locations while still being relatively easy to maintain and manage. I also remove the accidental possibility of accessing the incorrect cluster or environment and wreaking havoc.\"}),`\n`,(0,n.jsxs)(e.h3,{id:\"conclusion\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#conclusion\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Conclusion\"]}),`\n`,(0,n.jsx)(e.p,{children:\"It is possible to keep YAML file configuration and management of EKS kubernetes clusters separate and yet accessible with some flags that switch into the correct environment. Adding and removing clusters and credentials is easily managed in the filesystem and without editing files directly. Also, defaults are removed so that a command does not get executed on the wrong cluster inadvertently.\"})]})}function v(t={}){let{wrapper:e}=t.components||{};return e?(0,n.jsx)(e,Object.assign({},t,{children:(0,n.jsx)(u,t)})):u(t)}var I=v;return b(S);})();\n;return Component;"
        },
        "_id": "blog/posts/how-to-make-kubernetes-config-files-not-suck.mdx",
        "_raw": {
          "sourceFilePath": "blog/posts/how-to-make-kubernetes-config-files-not-suck.mdx",
          "sourceFileName": "how-to-make-kubernetes-config-files-not-suck.mdx",
          "sourceFileDir": "blog/posts",
          "contentType": "mdx",
          "flattenedPath": "blog/posts/how-to-make-kubernetes-config-files-not-suck"
        },
        "type": "BlogPost",
        "computedSlug": "how-to-make-kubernetes-config-files-not-suck"
      },
      "documentHash": "1739393595020",
      "hasWarnings": false,
      "documentTypeName": "BlogPost"
    },
    "blog/posts/how-to-manage-gitops-secrets-a-detailed-guide.mdx": {
      "document": {
        "title": "How to Manage GitOps Secrets: A Detailed Guide",
        "summary": "GitOps practices promote storing all your configs in git repositories. Learn how to store your secrets in plain text.",
        "publishDate": "Tue Aug 23 2022 20:41:42 GMT+0000 (Coordinated Universal Time)",
        "author": "ashley-penney",
        "readingTime": 6,
        "categories": [
          "kubernetes",
          "platform-engineering"
        ],
        "mainImage": "/blog-images/96e5afb078f22a510c88413dcb278756.jpg",
        "imageAlt": "a small black toy",
        "showCTA": true,
        "ctaCopy": "Enhance GitOps security with Release's ephemeral environments for secure secret management and streamlined collaboration.",
        "ctaLink": "https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=how-to-manage-gitops-secrets-a-detailed-guide",
        "relatedPosts": [
          ""
        ],
        "ogImage": "/blog-images/96e5afb078f22a510c88413dcb278756.jpg",
        "excerpt": "GitOps practices promote storing all your configs in git repositories. Learn how to store your secrets in plain text.",
        "tags": [
          "kubernetes",
          "platform-engineering"
        ],
        "ctaButton": "Try Release for Free",
        "body": {
          "raw": "\nGitOps is becoming increasingly popular. More and more companies have started using Git as the source of truth for their infrastructure and application configuration. However, with its advantages comes also challenges. For example, if all your configuration is stored in Git, how do you manage secrets? You can't simply commit your passwords and tokens in clear text to the Git repository even if that repository is private and only a few people have access to it. In this post, you'll learn how to manage GitOps secrets securely. Stay tuned.\n\n![](/blog-images/f808540d87cebe56d7ee1d06ba115259.png)\n\n### GitOps vs Secrets\n\nIf you've never used GitOps before, here's a short introduction for you. GitOps is a way of managing infrastructure and application configuration purely via Git repositories in a declarative manner. Here's how it works: You store all the configuration in Git, and then you have a GitOps tool installed somewhere that constantly monitors changes to that Git repository and applies infrastructure and application changes once it detects that something changed in the repository. The whole point of GitOps is that you have one, centralized, single point of truth for all your infrastructure and application configuration. GitOps is most commonly used with Kubernetes.\n\nBut as mentioned in the beginning of this post, there are some challenges when using GitOps. And the biggest one is secret management. There will be many secrets that your infrastructure will require. Your application configuration is probably full of secrets too. And it ought to go without saying that storing secrets in the Git repository in plain text is a security vulnerability. That's true even if that repository is private. You need a different solution for that, but ideally something that still works in a GitOps manner. This means it would be great not to have a separate process to define secrets. I'll show you how it can be done.\n\n### Secrets the GitOps Way\n\nThere are two popular ways of solving this problem. They work quite differently, but both achieve the same outcome: the ability to store secrets or their references in a Git repository. Which one you choose will depend on your company's needs. Let's discuss both of them.\n\n#### SealedSecrets\n\nWe already established that you can't store secrets in plain text in a Git repository. But how about storing them in a non-plain-text version? That's precisely what the [SealedSecrets](https://github.com/bitnami-labs/sealed-secrets) tool does. It allows you to encrypt your secrets and only store their encrypted version in your Git repository. Simple as that.\n\nHow does SealedSecrets work, you ask? You install a SealedSecrets controller on your Kubernetes cluster and the **kubeseal** binary on your local machine. SealedSecrets will generate private and public keys for encrypting the secrets. Before committing a secret to a Git repository, you'll use **kubeseal** binary to encrypt it. Then, in an encrypted form, it's totally safe to store it in a repository, and only the SealedSecrets controller running in your Kubernetes cluster will be able to decrypt it. Quite smart, if you ask me.\n\n#### How to Use SealedSecrets\n\nFirst, follow the installation instructions for SealedSecrets [here](https://github.com/bitnami-labs/sealed-secrets#installation). Once you have it up and running, you can try to seal your first secret with kubeseal. Let's create a simple Kubernetes secret definition YAML file and use kubeseal to seal it.\n\n```yaml\n\napiVersion: v1\nkind: Secret\nmetadata:  \n  name: example-secret\ntype: Opaque\ndata:\n  username: my-username\n  password: super-secret-password\n\n```\n\nOnce you have the file, you can pipe its content to the kubeseal binary.\n\n```yaml\ncat secret.yaml| kubeseal --controller-name=sealed-secrets-controller --format yaml > sealed-secret.yaml\n```\n\nAnd if you now take a look at the created sealed-secret.yaml file, you'll see that the actual username and password values are encrypted.\n\n```yaml\n\n$ cat sealed-secret.yaml\napiVersion: bitnami.com/v1alpha1\nkind: SealedSecret\nmetadata:\n  creationTimestamp: null\n  name: example-secret\n  namespace: default\nspec:\n  encryptedData:\n    password: AgC7jlVk(...)eb+XOk5/99fKHk=\n    username: AgAHbCU7(...)hIgv5D6LDYopF4n\n  template:\n    data: null\n    metadata:\n      creationTimestamp: null\n      name: example-secret\n      namespace: default\n    type: Opaque\n\n```\n\nThis file is now safe to be stored in a Git repository since only a SealedSecrets controller that was used to encrypt this file will be able to decrypt it.\n\nBut how do you consume that secret in your cluster? It's very straightforward. You can directly apply that sealed file to your cluster, and the SealedSecrets controller running on it will automatically unseal it and create a standard Kubernetes secret resource from it. Let's take a look.\n\n```yaml\n$ kubectl apply -f sealed-secret.yaml\nsealedsecret.bitnami.com/example-secret created\n\n$ kubectl get secret\nNAME                                 TYPE                 DATA   AGE\nexample-secret                       Opaque               2      7s\n```\n\nFrom now on, you can use **example-secret** just like any standard Kubernetes secret.\n\n#### ExternalSecrets\n\nAnother way to store secrets for your GitOps needs is using [ExternalSecrets](https://external-secrets.io/). It works differently than SealedSecrets but also solves the problem of storing plain text secrets in a Git repository. ExternalSecrets does this by removing the need to store the actual secret in your repository. Instead, your secret can be safely stored in a secret vault, and you only need to store a reference to a secret in your repository.\n\nSo instead of having, for example, the actual username and password in a file in your Git, you'll instead have a file that says something like \"this username is password is stored in that secret vault, under this key.\" And then it's the external secret operator's job to go and fetch the actual value for you when you need it. Let's try that.\n\n![](/blog-images/51c0cf27ba7ead6a04410e970bd6ce75.png)\n\n#### Using ESO\n\nThe external secrets operator can be installed just like any other tool using Helm. You can follow the installation and initial configuration steps [here](https://external-secrets.io/v0.5.8/guides-getting-started/). Once you have ExternalSecrets up and running, using it is quite simple. You first need to add your secrets to the secret vault that you want to use and then create an ExternalSecrets reference file. This file will be a replacement for your typical Kubernetes secret definition file.\n\nAs explained before, working with ESO means referencing the actual secrets from an external secret vault. So, you create an external secret resource, and the external secret operator will fetch the actual secret from an external vault in the background and create an actual Kubernetes secret for you. Here's an example:\n\n```yaml\n\napiVersion: external-secrets.io/v1beta1\nkind: ExternalSecret\nmetadata:\n  name: database-externalsecret\nspec:\n  refreshInterval: 3h\n  secretStoreRef:\n    name: azure-keyvault\n    kind: SecretStore\n  target:\n    name: database-secret\n    creationPolicy: Owner\n  data:\n  - secretKey: database-secret-dev\n    remoteRef:\n      key: database-secret-dev\n\n```\n\nThat is an ExternalSecrets definition file that tells ESO to fetch the value of the **database-secret-dev** key from the Azure Key Vault and create a Kubernetes secret called **database-secret** from it. As you can see, we don't have actual secret values in this file, so storing it in a Git repository is perfectly fine.\n\nIt's the same when it comes to consuming secrets. You simply apply that ExternalSecrets definition file to your cluster, and the ESO operator will automatically fetch the secret from the defined secret vault and create an actual Kubernetes secret from it.\n\n```yaml\n$ kubectl apply -f external-secret.yaml\nexternalsecret.external-secrets.io/database-externalsecret created\n\n$ kubectl get secret\nNAME                                 TYPE                 DATA   AGE\nexample-secret                       Opaque               2      12m\ndatabase-secret                      Opaque               2      4s\n```\n\n### Summary\n\nAs you can see, the GitOps secrets problem can be solved. It's not even that difficult. However, it does include some extra steps and tools. But once the initial setup is done, it doesn't take much more daily effort to manage secrets securely in your GitOps practices.\n\nIf you want to learn more about secrets or GitOps, you can find more content on [our blog](https://release.com/blog).\n",
          "code": "var Component=(()=>{var d=Object.create;var s=Object.defineProperty;var u=Object.getOwnPropertyDescriptor;var p=Object.getOwnPropertyNames;var m=Object.getPrototypeOf,y=Object.prototype.hasOwnProperty;var g=(a,e)=>()=>(e||a((e={exports:{}}).exports,e),e.exports),f=(a,e)=>{for(var n in e)s(a,n,{get:e[n],enumerable:!0})},i=(a,e,n,o)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let r of p(e))!y.call(a,r)&&r!==n&&s(a,r,{get:()=>e[r],enumerable:!(o=u(e,r))||o.enumerable});return a};var b=(a,e,n)=>(n=a!=null?d(m(a)):{},i(e||!a||!a.__esModule?s(n,\"default\",{value:a,enumerable:!0}):n,a)),w=a=>i(s({},\"__esModule\",{value:!0}),a);var c=g((O,l)=>{l.exports=_jsx_runtime});var S={};f(S,{default:()=>k,frontmatter:()=>v});var t=b(c()),v={title:\"How to Manage GitOps Secrets: A Detailed Guide\",summary:\"GitOps practices promote storing all your configs in git repositories. Learn how to store your secrets in plain text.\",publishDate:\"Tue Aug 23 2022 20:41:42 GMT+0000 (Coordinated Universal Time)\",author:\"ashley-penney\",readingTime:6,categories:[\"kubernetes\",\"platform-engineering\"],mainImage:\"/blog-images/96e5afb078f22a510c88413dcb278756.jpg\",imageAlt:\"a small black toy\",showCTA:!0,ctaCopy:\"Enhance GitOps security with Release's ephemeral environments for secure secret management and streamlined collaboration.\",ctaLink:\"https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=how-to-manage-gitops-secrets-a-detailed-guide\",relatedPosts:[\"\"],ogImage:\"/blog-images/96e5afb078f22a510c88413dcb278756.jpg\",excerpt:\"GitOps practices promote storing all your configs in git repositories. Learn how to store your secrets in plain text.\",tags:[\"kubernetes\",\"platform-engineering\"],ctaButton:\"Try Release for Free\"};function h(a){let e=Object.assign({p:\"p\",img:\"img\",h3:\"h3\",a:\"a\",span:\"span\",h4:\"h4\",strong:\"strong\",pre:\"pre\",code:\"code\"},a.components);return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(e.p,{children:\"GitOps is becoming increasingly popular. More and more companies have started using Git as the source of truth for their infrastructure and application configuration. However, with its advantages comes also challenges. For example, if all your configuration is stored in Git, how do you manage secrets? You can't simply commit your passwords and tokens in clear text to the Git repository even if that repository is private and only a few people have access to it. In this post, you'll learn how to manage GitOps secrets securely. Stay tuned.\"}),`\n`,(0,t.jsx)(e.p,{children:(0,t.jsx)(e.img,{src:\"/blog-images/f808540d87cebe56d7ee1d06ba115259.png\",alt:\"\"})}),`\n`,(0,t.jsxs)(e.h3,{id:\"gitops-vs-secrets\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#gitops-vs-secrets\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"GitOps vs Secrets\"]}),`\n`,(0,t.jsx)(e.p,{children:\"If you've never used GitOps before, here's a short introduction for you. GitOps is a way of managing infrastructure and application configuration purely via Git repositories in a declarative manner. Here's how it works: You store all the configuration in Git, and then you have a GitOps tool installed somewhere that constantly monitors changes to that Git repository and applies infrastructure and application changes once it detects that something changed in the repository. The whole point of GitOps is that you have one, centralized, single point of truth for all your infrastructure and application configuration. GitOps is most commonly used with Kubernetes.\"}),`\n`,(0,t.jsx)(e.p,{children:\"But as mentioned in the beginning of this post, there are some challenges when using GitOps. And the biggest one is secret management. There will be many secrets that your infrastructure will require. Your application configuration is probably full of secrets too. And it ought to go without saying that storing secrets in the Git repository in plain text is a security vulnerability. That's true even if that repository is private. You need a different solution for that, but ideally something that still works in a GitOps manner. This means it would be great not to have a separate process to define secrets. I'll show you how it can be done.\"}),`\n`,(0,t.jsxs)(e.h3,{id:\"secrets-the-gitops-way\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#secrets-the-gitops-way\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Secrets the GitOps Way\"]}),`\n`,(0,t.jsx)(e.p,{children:\"There are two popular ways of solving this problem. They work quite differently, but both achieve the same outcome: the ability to store secrets or their references in a Git repository. Which one you choose will depend on your company's needs. Let's discuss both of them.\"}),`\n`,(0,t.jsxs)(e.h4,{id:\"sealedsecrets\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#sealedsecrets\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"SealedSecrets\"]}),`\n`,(0,t.jsxs)(e.p,{children:[\"We already established that you can't store secrets in plain text in a Git repository. But how about storing them in a non-plain-text version? That's precisely what the \",(0,t.jsx)(e.a,{href:\"https://github.com/bitnami-labs/sealed-secrets\",children:\"SealedSecrets\"}),\" tool does. It allows you to encrypt your secrets and only store their encrypted version in your Git repository. Simple as that.\"]}),`\n`,(0,t.jsxs)(e.p,{children:[\"How does SealedSecrets work, you ask? You install a SealedSecrets controller on your Kubernetes cluster and the \",(0,t.jsx)(e.strong,{children:\"kubeseal\"}),\" binary on your local machine. SealedSecrets will generate private and public keys for encrypting the secrets. Before committing a secret to a Git repository, you'll use \",(0,t.jsx)(e.strong,{children:\"kubeseal\"}),\" binary to encrypt it. Then, in an encrypted form, it's totally safe to store it in a repository, and only the SealedSecrets controller running in your Kubernetes cluster will be able to decrypt it. Quite smart, if you ask me.\"]}),`\n`,(0,t.jsxs)(e.h4,{id:\"how-to-use-sealedsecrets\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#how-to-use-sealedsecrets\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"How to Use SealedSecrets\"]}),`\n`,(0,t.jsxs)(e.p,{children:[\"First, follow the installation instructions for SealedSecrets \",(0,t.jsx)(e.a,{href:\"https://github.com/bitnami-labs/sealed-secrets#installation\",children:\"here\"}),\". Once you have it up and running, you can try to seal your first secret with kubeseal. Let's create a simple Kubernetes secret definition YAML file and use kubeseal to seal it.\"]}),`\n`,(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:\"language-yaml\",children:`\napiVersion: v1\nkind: Secret\nmetadata: \\xA0\n \\xA0name: example-secret\ntype: Opaque\ndata:\n \\xA0username: my-username\n \\xA0password: super-secret-password\n\n`})}),`\n`,(0,t.jsx)(e.p,{children:\"Once you have the file, you can pipe its content to the kubeseal binary.\"}),`\n`,(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:\"language-yaml\",children:`cat secret.yaml| kubeseal --controller-name=sealed-secrets-controller --format yaml > sealed-secret.yaml\n`})}),`\n`,(0,t.jsx)(e.p,{children:\"And if you now take a look at the created sealed-secret.yaml file, you'll see that the actual username and password values are encrypted.\"}),`\n`,(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:\"language-yaml\",children:`\n$ cat sealed-secret.yaml\napiVersion: bitnami.com/v1alpha1\nkind: SealedSecret\nmetadata:\n \\xA0creationTimestamp: null\n \\xA0name: example-secret\n \\xA0namespace: default\nspec:\n \\xA0encryptedData:\n \\xA0 \\xA0password: AgC7jlVk(...)eb+XOk5/99fKHk=\n \\xA0 \\xA0username: AgAHbCU7(...)hIgv5D6LDYopF4n\n \\xA0template:\n \\xA0 \\xA0data: null\n \\xA0 \\xA0metadata:\n \\xA0 \\xA0 \\xA0creationTimestamp: null\n \\xA0 \\xA0 \\xA0name: example-secret\n \\xA0 \\xA0 \\xA0namespace: default\n \\xA0 \\xA0type: Opaque\n\n`})}),`\n`,(0,t.jsx)(e.p,{children:\"This file is now safe to be stored in a Git repository since only a SealedSecrets controller that was used to encrypt this file will be able to decrypt it.\"}),`\n`,(0,t.jsx)(e.p,{children:\"But how do you consume that secret in your cluster? It's very straightforward. You can directly apply that sealed file to your cluster, and the SealedSecrets controller running on it will automatically unseal it and create a standard Kubernetes secret resource from it. Let's take a look.\"}),`\n`,(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:\"language-yaml\",children:`$ kubectl apply -f sealed-secret.yaml\nsealedsecret.bitnami.com/example-secret created\n\n$ kubectl get secret\nNAME \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 TYPE \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 DATA \\xA0 AGE\nexample-secret \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 Opaque \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 2 \\xA0 \\xA0 \\xA07s\n`})}),`\n`,(0,t.jsxs)(e.p,{children:[\"From now on, you can use \",(0,t.jsx)(e.strong,{children:\"example-secret\"}),\" just like any standard Kubernetes secret.\"]}),`\n`,(0,t.jsxs)(e.h4,{id:\"externalsecrets\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#externalsecrets\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"ExternalSecrets\"]}),`\n`,(0,t.jsxs)(e.p,{children:[\"Another way to store secrets for your GitOps needs is using \",(0,t.jsx)(e.a,{href:\"https://external-secrets.io/\",children:\"ExternalSecrets\"}),\". It works differently than SealedSecrets but also solves the problem of storing plain text secrets in a Git repository. ExternalSecrets does this by removing the need to store the actual secret in your repository. Instead, your secret can be safely stored in a secret vault, and you only need to store a reference to a secret in your repository.\"]}),`\n`,(0,t.jsx)(e.p,{children:`So instead of having, for example, the actual username and password in a file in your Git, you'll instead have a file that says something like \"this username is password is stored in that secret vault, under this key.\" And then it's the external secret operator's job to go and fetch the actual value for you when you need it. Let's try that.`}),`\n`,(0,t.jsx)(e.p,{children:(0,t.jsx)(e.img,{src:\"/blog-images/51c0cf27ba7ead6a04410e970bd6ce75.png\",alt:\"\"})}),`\n`,(0,t.jsxs)(e.h4,{id:\"using-eso\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#using-eso\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Using ESO\"]}),`\n`,(0,t.jsxs)(e.p,{children:[\"The external secrets operator can be installed just like any other tool using Helm. You can follow the installation and initial configuration steps \",(0,t.jsx)(e.a,{href:\"https://external-secrets.io/v0.5.8/guides-getting-started/\",children:\"here\"}),\". Once you have ExternalSecrets up and running, using it is quite simple. You first need to add your secrets to the secret vault that you want to use and then create an ExternalSecrets reference file. This file will be a replacement for your typical Kubernetes secret definition file.\"]}),`\n`,(0,t.jsx)(e.p,{children:\"As explained before, working with ESO means referencing the actual secrets from an external secret vault. So, you create an external secret resource, and the external secret operator will fetch the actual secret from an external vault in the background and create an actual Kubernetes secret for you. Here's an example:\"}),`\n`,(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:\"language-yaml\",children:`\napiVersion: external-secrets.io/v1beta1\nkind: ExternalSecret\nmetadata:\n \\xA0name: database-externalsecret\nspec:\n \\xA0refreshInterval: 3h\n \\xA0secretStoreRef:\n \\xA0 \\xA0name: azure-keyvault\n \\xA0 \\xA0kind: SecretStore\n \\xA0target:\n \\xA0 \\xA0name: database-secret\n \\xA0 \\xA0creationPolicy: Owner\n \\xA0data:\n \\xA0- secretKey: database-secret-dev\n \\xA0 \\xA0remoteRef:\n \\xA0 \\xA0 \\xA0key: database-secret-dev\n\n`})}),`\n`,(0,t.jsxs)(e.p,{children:[\"That is an ExternalSecrets definition file that tells ESO to fetch the value of the \",(0,t.jsx)(e.strong,{children:\"database-secret-dev\"}),\" key from the Azure Key Vault and create a Kubernetes secret called \",(0,t.jsx)(e.strong,{children:\"database-secret\"}),\" from it. As you can see, we don't have actual secret values in this file, so storing it in a Git repository is perfectly fine.\"]}),`\n`,(0,t.jsx)(e.p,{children:\"It's the same when it comes to consuming secrets. You simply apply that ExternalSecrets definition file to your cluster, and the ESO operator will automatically fetch the secret from the defined secret vault and create an actual Kubernetes secret from it.\"}),`\n`,(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:\"language-yaml\",children:`$ kubectl apply -f external-secret.yaml\nexternalsecret.external-secrets.io/database-externalsecret created\n\n$ kubectl get secret\nNAME \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 TYPE \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 DATA \\xA0 AGE\nexample-secret \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 Opaque \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 2 \\xA0 \\xA0 \\xA012m\ndatabase-secret \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0Opaque \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 2 \\xA0 \\xA0 \\xA04s\n`})}),`\n`,(0,t.jsxs)(e.h3,{id:\"summary\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#summary\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Summary\"]}),`\n`,(0,t.jsx)(e.p,{children:\"As you can see, the GitOps secrets problem can be solved. It's not even that difficult. However, it does include some extra steps and tools. But once the initial setup is done, it doesn't take much more daily effort to manage secrets securely in your GitOps practices.\"}),`\n`,(0,t.jsxs)(e.p,{children:[\"If you want to learn more about secrets or GitOps, you can find more content on \",(0,t.jsx)(e.a,{href:\"https://release.com/blog\",children:\"our blog\"}),\".\"]})]})}function x(a={}){let{wrapper:e}=a.components||{};return e?(0,t.jsx)(e,Object.assign({},a,{children:(0,t.jsx)(h,a)})):h(a)}var k=x;return w(S);})();\n;return Component;"
        },
        "_id": "blog/posts/how-to-manage-gitops-secrets-a-detailed-guide.mdx",
        "_raw": {
          "sourceFilePath": "blog/posts/how-to-manage-gitops-secrets-a-detailed-guide.mdx",
          "sourceFileName": "how-to-manage-gitops-secrets-a-detailed-guide.mdx",
          "sourceFileDir": "blog/posts",
          "contentType": "mdx",
          "flattenedPath": "blog/posts/how-to-manage-gitops-secrets-a-detailed-guide"
        },
        "type": "BlogPost",
        "computedSlug": "how-to-manage-gitops-secrets-a-detailed-guide"
      },
      "documentHash": "1739393595020",
      "hasWarnings": false,
      "documentTypeName": "BlogPost"
    },
    "blog/posts/how-to-self-host-posthog-using-release-part-one.mdx": {
      "document": {
        "title": "How to Self-Host PostHog Using Release Part One",
        "summary": "Learn how to run a hobby version of PostHog on your own cloud infrastructure using Release",
        "publishDate": "Tue Feb 14 2023 23:50:39 GMT+0000 (Coordinated Universal Time)",
        "author": "regis-wilson",
        "readingTime": 7,
        "categories": [
          "platform-engineering",
          "product"
        ],
        "mainImage": "/blog-images/0fabd6b01cf2c47c90de069cad6705bb.jpg",
        "imageAlt": "Image of two women sitting in front of a computer",
        "showCTA": true,
        "ctaCopy": "Deploy PostHog on your cloud with Release for seamless testing and development environments. Simplify self-hosting today!",
        "ctaLink": "https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=how-to-self-host-posthog-using-release-part-one",
        "relatedPosts": [
          ""
        ],
        "ogImage": "/blog-images/0fabd6b01cf2c47c90de069cad6705bb.jpg",
        "excerpt": "Learn how to run a hobby version of PostHog on your own cloud infrastructure using Release",
        "tags": [
          "platform-engineering",
          "product"
        ],
        "ctaButton": "Try Release for Free",
        "body": {
          "raw": "\nThis first part of a series explains how to run a hobby version of [PostHog](https://github.com/PostHog/posthog) on your own cloud infrastructure using Release. Check back later to read how to perform the self-hosted enterprise version. You can read the [PostHog FAQ](https://posthog.com/faq) for more details on the software and self-hosting options for hobby and enterprise options.\n\n### Introduction\n\nPostHog is an open source tool for collecting and analyzing behavior metrics and events from your application without sending data to a third party hosting provider. As the GitHub repository says, “_…third-party analytics tools do not work in a world of cookie deprecation, GDPR, HIPAA, CCPA, and many other four-letter acronyms. PostHog is the alternative to sending all of your customers' personal information and usage data to third-parties._”\n\nThis goes together with the Release philosophy of deploying high-quality, full-stack environments in your own cloud accounts. Deploying PostHog next to your development environments for complete testing and development, or as a shared staging location for integration testing is a valuable component for developing and testing user interactions and metrics collection that would be much harder to support without Release.\n\nThis article will walk you through the simple steps for configuring a PostHog application template that you can deploy to your own cloud infrastructure using Release in about 30 minutes or so. The hobby version is a full functionality installation of PostHog but will not be configured with redundant services and long-term storage and backup solutions like the enterprise version would. The most common use-case for wanting to install the hobby version is to support developer environments for testing in isolation or for QA environments for complete end-to-end testing of product analytics.\n\nWe will cover the enterprise version for permanent installations (like staging or production) in a followup post. The most common use-case to install the enterprise version is to self-host and scale your own analytics engine and data from your product customers _without sending the data outside your organization or to a third-party Software-as-a-Service (SAAS)_. Release allows you to self-host applications in your own cloud environments keeping your and your customers’ data safe and secure.\n\n### Prerequisites\n\nTo get started, open a [Release account](https://app.release.com/) which allows you to have unlimited applications and pay only for the environments you are running. You can test out Release by creating a trial account in a shared environment; or if you already have a paid plan, you can use your Release account to host these environments in your own cloud infrastructure next to your existing applications. See our [quickstart documentation](https://docs.release.com/getting-started/quickstart) for more details.\n\nNext, fork the [PostHog GitHub repo](https://github.com/PostHog/posthog) using either [our fork with the configuration options already built in](https://github.com/releasehub-samples/posthog), or the original upstream repository. You can also use one of our integrations with GitLab or BitBucket but you will need to clone and push the repository separately before you get started.\n\n### Configure the PostHog Application\n\nThe first step is to import the application into Release by analyzing the repository and reading in the docker-compose and package.json files to get a running version of the repository in your account. If you are using the fork from our repository, then you will have a head start by using our configuration YAML already integrated into the repository. These instructions will work for the plain upstream version and they show how the process works.\n\nFirst, click the “Create New App” button and select your forked repository (note: if the repository does not show up, go to your profile page and make sure you have configured the correct permissions and scope for the version control system integration you are using). Give the application a name and go to the next step.\n\n![](/blog-images/10ec97ac01beb4f2710783919ee94f48.png)\n\nNext, analyze the repository using a branch and the common hint files we search for to configure this application. Select at least one docker-compose file and then select the services to your preference as shown in the following image. The hobby version will not include any helm charts or cloud native service integrations, but this is a good example and low-cost way to test out the functionality of the application.\n\n![](/blog-images/b8e462a3651cb4737f55c299509cd1b9.png)\n\nWhen you’ve selected the services to import and analyze, you will fine tune the application template so that it has the specific customizations and workflows you would like to use for testing purposes. You can start by editing the hostnames for each service (you will only need a service hostname for **web**, **maildev**, and **clickhouse** for example). You can also customize the domain you will deploy the test application URLs to.\n\n![](/blog-images/541eb5a9942fa0e6db4d0bd129d9be47.png)\n\nYou should pay close attention to the workflow steps as the order of the services is important. Reading up on the architecture diagram and understanding the **depends_on** fields in the docker compose will help you understand the correct workflow as well. This is all organized for you in the [forked version we host and maintain](https://github.com/releasehub-samples/posthog).\n\n![](/blog-images/622b9013a73cb1dfc968bcf0be779250.png)\n\nOnce you are happy with the application template and service definitions, you can proceed with fine-tuning the environment variables for the application. Here, the [PostHog documentation](https://posthog.com/docs/self-host/configure/environment-variables) is excellent in helping you compare the defaults and necessary customization you will need side-by-side.\n\n![](/blog-images/a769c73048831a444eee0f9d7d5d409f.png)\n\nYou can see that I’ve set some environment variable mappings which allow interpolation of Release-injected variables so that the application URLs and internal configuration items work correctly for each environment that will be created. Since we are using a hobby (or ephemeral) version of this application, we are not going to hardcode values or secrets as we might do in a permanent or production environment.\n\nThe next steps involve creating build arguments. We always recommend using the **production** value for **NODE_ENV** since Release ephemeral environments run in production mode, rather than development mode (except when using our [Remote Development](https://docs.release.com/cli/remote-dev) feature).\n\n![](/blog-images/712366d3afe51b9063964486a9c082d5.png)\n\nWhen those steps are done, you can go back to review all your work and confirm it looks good or you can immediately deploy your environment and see the results of your hard work! The application template and environment variables will be deployed into your cloud environment in a completely isolated environment that you can test and play with. When you are done with testing the environment, it can be expired, removed, or you can redeploy it with any changes and tweaks you need to make until you are satisfied with the results.\n\n![](/blog-images/cb365819d78882a5f18edf44d4fea1a2.png)\n\n### Deploy the Application as an Environment\n\nYou should see your deploy starting up and an ephemeral environment is immediately spun up to begin playing with!\n\n![](/blog-images/24557fb18ce0c2ab42185d775f7006e2.jpg)\n\n‍  \nOnce  complete, you can visit the environment status page which shows you a list of clickable URL links for your application (we recommend editing those down to services that you actually need, for example, **redis** and **postgres** are not going to be reachable on the internet and you should remove those from your configuration).\n\n![](/blog-images/4eee429f2d8daec7fb395cffeb83c1be.jpg)\n\n### Debug and Test the Environment\n\nYou can inspect the services and statuses of each underlying instance that is running in your kubernetes namespace. This gives you the up-to-date picture of what is started and what is running properly, what needs to be investigated for errors, logs, and so-forth. Alternatively, you can jump into a terminal session on a service for immediate debugging and testing.\n\n![](/blog-images/b40b177ea3e84beda653ba1118ae0525.jpg)\n\nClicking on each service link, you can see Clickhouse works\n\n![](/blog-images/5decc8a2d4443f5481f0d105d30df86a.png)\n\nAnd the same with Maildev:\n\n![](/blog-images/c1e2f7df42f3529ccde1b981f55afe6a.png)\n\n### Login and Configure the Application\n\nBut the most important thing to look at is the PostHog application itself. We will click on “Just experimenting” for now.\n\n![](/blog-images/bf9f16972343f2acf6d63595d524143a.png)\n\nWe can verify that all the services and prerequisites are running:\n\n![](/blog-images/50f0aae29aca94afc38ff10ce41b7199.png)\n\nThen get started by creating an account:\n\n![](/blog-images/81dd405ea02fb45c47387a53e51aa374.png)\n\nAnd finally you can start configuring your application and settings to use in your new isolated hobby experience:\n\n![](/blog-images/629fc0d4e252b78520be21c7655226aa.png)\n\n### Conclusion\n\nUsing Release to deploy a hobby version of PostHog allows you to run an isolated version of analytics and metrics collection during the development and testing of your own application code. PostHog can be deployed alongside every application deployment you use for each developer or for each branch/feature/pull request that needs to integrate with PostHog, depending on your workflow and requirements.\n\nWith Release, it is easy to spin up environments for almost any conceivable use case and this empowers developer teams to deploy applications without relying on a devops or an infrastructure team and minimizing costs associated with provisioning and maintaining the application. You can deploy dependencies or third-party tools that need to be tested or verified when performing application changes or when rolling out new features. Verifying this functionality with third party tools and services is a valuable way to ensure that features reaching staging or production will work even during the earliest phases of development.\n\nReady to get started? Open a [Release account](https://app.release.com/) and start your free trial today or check out our [quickstart documentation](https://docs.release.com/getting-started/quickstart) for additional information.\n",
          "code": "var Component=(()=>{var d=Object.create;var a=Object.defineProperty;var p=Object.getOwnPropertyDescriptor;var u=Object.getOwnPropertyNames;var g=Object.getPrototypeOf,m=Object.prototype.hasOwnProperty;var f=(n,e)=>()=>(e||n((e={exports:{}}).exports,e),e.exports),y=(n,e)=>{for(var t in e)a(n,t,{get:e[t],enumerable:!0})},s=(n,e,t,r)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let i of u(e))!m.call(n,i)&&i!==t&&a(n,i,{get:()=>e[i],enumerable:!(r=p(e,i))||r.enumerable});return n};var b=(n,e,t)=>(t=n!=null?d(g(n)):{},s(e||!n||!n.__esModule?a(t,\"default\",{value:n,enumerable:!0}):t,n)),w=n=>s(a({},\"__esModule\",{value:!0}),n);var c=f((R,l)=>{l.exports=_jsx_runtime});var H={};y(H,{default:()=>x,frontmatter:()=>v});var o=b(c()),v={title:\"How to Self-Host PostHog Using Release Part One\",summary:\"Learn how to run a hobby version of PostHog on your own cloud infrastructure using Release\",publishDate:\"Tue Feb 14 2023 23:50:39 GMT+0000 (Coordinated Universal Time)\",author:\"regis-wilson\",readingTime:7,categories:[\"platform-engineering\",\"product\"],mainImage:\"/blog-images/0fabd6b01cf2c47c90de069cad6705bb.jpg\",imageAlt:\"Image of two women sitting in front of a computer\",showCTA:!0,ctaCopy:\"Deploy PostHog on your cloud with Release for seamless testing and development environments. Simplify self-hosting today!\",ctaLink:\"https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=how-to-self-host-posthog-using-release-part-one\",relatedPosts:[\"\"],ogImage:\"/blog-images/0fabd6b01cf2c47c90de069cad6705bb.jpg\",excerpt:\"Learn how to run a hobby version of PostHog on your own cloud infrastructure using Release\",tags:[\"platform-engineering\",\"product\"],ctaButton:\"Try Release for Free\"};function h(n){let e=Object.assign({p:\"p\",a:\"a\",h3:\"h3\",span:\"span\",em:\"em\",img:\"img\",strong:\"strong\",br:\"br\"},n.components);return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsxs)(e.p,{children:[\"This first part of a series explains how to run a hobby version of \",(0,o.jsx)(e.a,{href:\"https://github.com/PostHog/posthog\",children:\"PostHog\"}),\" on your own cloud infrastructure using Release. Check back later to read how to perform the self-hosted enterprise version. You can read the \",(0,o.jsx)(e.a,{href:\"https://posthog.com/faq\",children:\"PostHog FAQ\"}),\" for more details on the software and self-hosting options for hobby and enterprise options.\"]}),`\n`,(0,o.jsxs)(e.h3,{id:\"introduction\",children:[(0,o.jsx)(e.a,{className:\"anchor\",href:\"#introduction\",children:(0,o.jsx)(e.span,{className:\"icon icon-link\"})}),\"Introduction\"]}),`\n`,(0,o.jsxs)(e.p,{children:[\"PostHog is an open source tool for collecting and analyzing behavior metrics and events from your application without sending data to a third party hosting provider. As the GitHub repository says, \\u201C\",(0,o.jsx)(e.em,{children:\"\\u2026third-party analytics tools do not work in a world of cookie deprecation, GDPR, HIPAA, CCPA, and many other four-letter acronyms. PostHog is the alternative to sending all of your customers' personal information and usage data to third-parties.\"}),\"\\u201D\"]}),`\n`,(0,o.jsx)(e.p,{children:\"This goes together with the Release philosophy of deploying high-quality, full-stack environments in your own cloud accounts. Deploying PostHog next to your development environments for complete testing and development, or as a shared staging location for integration testing is a valuable component for developing and testing user interactions and metrics collection that would be much harder to support without Release.\"}),`\n`,(0,o.jsx)(e.p,{children:\"This article will walk you through the simple steps for configuring a PostHog application template that you can deploy to your own cloud infrastructure using Release in about 30 minutes or so. The hobby version is a full functionality installation of PostHog but will not be configured with redundant services and long-term storage and backup solutions like the enterprise version would. The most common use-case for wanting to install the hobby version is to support developer environments for testing in isolation or for QA environments for complete end-to-end testing of product analytics.\"}),`\n`,(0,o.jsxs)(e.p,{children:[\"We will cover the enterprise version for permanent installations (like staging or production) in a followup post. The most common use-case to install the enterprise version is to self-host and scale your own analytics engine and data from your product customers \",(0,o.jsx)(e.em,{children:\"without sending the data outside your organization or to a third-party Software-as-a-Service (SAAS)\"}),\". Release allows you to self-host applications in your own cloud environments keeping your and your customers\\u2019 data safe and secure.\"]}),`\n`,(0,o.jsxs)(e.h3,{id:\"prerequisites\",children:[(0,o.jsx)(e.a,{className:\"anchor\",href:\"#prerequisites\",children:(0,o.jsx)(e.span,{className:\"icon icon-link\"})}),\"Prerequisites\"]}),`\n`,(0,o.jsxs)(e.p,{children:[\"To get started, open a \",(0,o.jsx)(e.a,{href:\"https://app.release.com/\",children:\"Release account\"}),\" which allows you to have unlimited applications and pay only for the environments you are running. You can test out Release by creating a trial account in a shared environment; or if you already have a paid plan, you can use your Release account to host these environments in your own cloud infrastructure next to your existing applications. See our \",(0,o.jsx)(e.a,{href:\"https://docs.release.com/getting-started/quickstart\",children:\"quickstart documentation\"}),\" for more details.\"]}),`\n`,(0,o.jsxs)(e.p,{children:[\"Next, fork the \",(0,o.jsx)(e.a,{href:\"https://github.com/PostHog/posthog\",children:\"PostHog GitHub repo\"}),\" using either \",(0,o.jsx)(e.a,{href:\"https://github.com/releasehub-samples/posthog\",children:\"our fork with the configuration options already built in\"}),\", or the original upstream repository. You can also use one of our integrations with GitLab or BitBucket but you will need to clone and push the repository separately before you get started.\"]}),`\n`,(0,o.jsxs)(e.h3,{id:\"configure-the-posthog-application\",children:[(0,o.jsx)(e.a,{className:\"anchor\",href:\"#configure-the-posthog-application\",children:(0,o.jsx)(e.span,{className:\"icon icon-link\"})}),\"Configure the PostHog Application\"]}),`\n`,(0,o.jsx)(e.p,{children:\"The first step is to import the application into Release by analyzing the repository and reading in the docker-compose and package.json files to get a running version of the repository in your account. If you are using the fork from our repository, then you will have a head start by using our configuration YAML already integrated into the repository. These instructions will work for the plain upstream version and they show how the process works.\"}),`\n`,(0,o.jsx)(e.p,{children:\"First, click the \\u201CCreate New App\\u201D button and select your forked repository (note: if the repository does not show up, go to your profile page and make sure you have configured the correct permissions and scope for the version control system integration you are using). Give the application a name and go to the next step.\"}),`\n`,(0,o.jsx)(e.p,{children:(0,o.jsx)(e.img,{src:\"/blog-images/10ec97ac01beb4f2710783919ee94f48.png\",alt:\"\"})}),`\n`,(0,o.jsx)(e.p,{children:\"Next, analyze the repository using a branch and the common hint files we search for to configure this application. Select at least one docker-compose file and then select the services to your preference as shown in the following image. The hobby version will not include any helm charts or cloud native service integrations, but this is a good example and low-cost way to test out the functionality of the application.\"}),`\n`,(0,o.jsx)(e.p,{children:(0,o.jsx)(e.img,{src:\"/blog-images/b8e462a3651cb4737f55c299509cd1b9.png\",alt:\"\"})}),`\n`,(0,o.jsxs)(e.p,{children:[\"When you\\u2019ve selected the services to import and analyze, you will fine tune the application template so that it has the specific customizations and workflows you would like to use for testing purposes. You can start by editing the hostnames for each service (you will only need a service hostname for \",(0,o.jsx)(e.strong,{children:\"web\"}),\", \",(0,o.jsx)(e.strong,{children:\"maildev\"}),\", and \",(0,o.jsx)(e.strong,{children:\"clickhouse\"}),\" for example). You can also customize the domain you will deploy the test application URLs to.\"]}),`\n`,(0,o.jsx)(e.p,{children:(0,o.jsx)(e.img,{src:\"/blog-images/541eb5a9942fa0e6db4d0bd129d9be47.png\",alt:\"\"})}),`\n`,(0,o.jsxs)(e.p,{children:[\"You should pay close attention to the workflow steps as the order of the services is important. Reading up on the architecture diagram and understanding the \",(0,o.jsx)(e.strong,{children:\"depends_on\"}),\" fields in the docker compose will help you understand the correct workflow as well. This is all organized for you in the \",(0,o.jsx)(e.a,{href:\"https://github.com/releasehub-samples/posthog\",children:\"forked version we host and maintain\"}),\".\"]}),`\n`,(0,o.jsx)(e.p,{children:(0,o.jsx)(e.img,{src:\"/blog-images/622b9013a73cb1dfc968bcf0be779250.png\",alt:\"\"})}),`\n`,(0,o.jsxs)(e.p,{children:[\"Once you are happy with the application template and service definitions, you can proceed with fine-tuning the environment variables for the application. Here, the \",(0,o.jsx)(e.a,{href:\"https://posthog.com/docs/self-host/configure/environment-variables\",children:\"PostHog documentation\"}),\" is excellent in helping you compare the defaults and necessary customization you will need side-by-side.\"]}),`\n`,(0,o.jsx)(e.p,{children:(0,o.jsx)(e.img,{src:\"/blog-images/a769c73048831a444eee0f9d7d5d409f.png\",alt:\"\"})}),`\n`,(0,o.jsx)(e.p,{children:\"You can see that I\\u2019ve set some environment variable mappings which allow interpolation of Release-injected variables so that the application URLs and internal configuration items work correctly for each environment that will be created. Since we are using a hobby (or ephemeral) version of this application, we are not going to hardcode values or secrets as we might do in a permanent or production environment.\"}),`\n`,(0,o.jsxs)(e.p,{children:[\"The next steps involve creating build arguments. We always recommend using the \",(0,o.jsx)(e.strong,{children:\"production\"}),\" value for \",(0,o.jsx)(e.strong,{children:\"NODE_ENV\"}),\" since Release ephemeral environments run in production mode, rather than development mode (except when using our \",(0,o.jsx)(e.a,{href:\"https://docs.release.com/cli/remote-dev\",children:\"Remote Development\"}),\" feature).\"]}),`\n`,(0,o.jsx)(e.p,{children:(0,o.jsx)(e.img,{src:\"/blog-images/712366d3afe51b9063964486a9c082d5.png\",alt:\"\"})}),`\n`,(0,o.jsx)(e.p,{children:\"When those steps are done, you can go back to review all your work and confirm it looks good or you can immediately deploy your environment and see the results of your hard work! The application template and environment variables will be deployed into your cloud environment in a completely isolated environment that you can test and play with. When you are done with testing the environment, it can be expired, removed, or you can redeploy it with any changes and tweaks you need to make until you are satisfied with the results.\"}),`\n`,(0,o.jsx)(e.p,{children:(0,o.jsx)(e.img,{src:\"/blog-images/cb365819d78882a5f18edf44d4fea1a2.png\",alt:\"\"})}),`\n`,(0,o.jsxs)(e.h3,{id:\"deploy-the-application-as-an-environment\",children:[(0,o.jsx)(e.a,{className:\"anchor\",href:\"#deploy-the-application-as-an-environment\",children:(0,o.jsx)(e.span,{className:\"icon icon-link\"})}),\"Deploy the Application as an Environment\"]}),`\n`,(0,o.jsx)(e.p,{children:\"You should see your deploy starting up and an ephemeral environment is immediately spun up to begin playing with!\"}),`\n`,(0,o.jsx)(e.p,{children:(0,o.jsx)(e.img,{src:\"/blog-images/24557fb18ce0c2ab42185d775f7006e2.jpg\",alt:\"\"})}),`\n`,(0,o.jsxs)(e.p,{children:[\"\\u200D\",(0,o.jsx)(e.br,{}),`\n`,\"Once\\xA0 complete, you can visit the environment status page which shows you a list of clickable URL links for your application (we recommend editing those down to services that you actually need, for example, \",(0,o.jsx)(e.strong,{children:\"redis\"}),\" and \",(0,o.jsx)(e.strong,{children:\"postgres\"}),\" are not going to be reachable on the internet and you should remove those from your configuration).\"]}),`\n`,(0,o.jsx)(e.p,{children:(0,o.jsx)(e.img,{src:\"/blog-images/4eee429f2d8daec7fb395cffeb83c1be.jpg\",alt:\"\"})}),`\n`,(0,o.jsxs)(e.h3,{id:\"debug-and-test-the-environment\",children:[(0,o.jsx)(e.a,{className:\"anchor\",href:\"#debug-and-test-the-environment\",children:(0,o.jsx)(e.span,{className:\"icon icon-link\"})}),\"Debug and Test the Environment\"]}),`\n`,(0,o.jsx)(e.p,{children:\"You can inspect the services and statuses of each underlying instance that is running in your kubernetes namespace. This gives you the up-to-date picture of what is started and what is running properly, what needs to be investigated for errors, logs, and so-forth. Alternatively, you can jump into a terminal session on a service for immediate debugging and testing.\"}),`\n`,(0,o.jsx)(e.p,{children:(0,o.jsx)(e.img,{src:\"/blog-images/b40b177ea3e84beda653ba1118ae0525.jpg\",alt:\"\"})}),`\n`,(0,o.jsx)(e.p,{children:\"Clicking on each service link, you can see Clickhouse works\"}),`\n`,(0,o.jsx)(e.p,{children:(0,o.jsx)(e.img,{src:\"/blog-images/5decc8a2d4443f5481f0d105d30df86a.png\",alt:\"\"})}),`\n`,(0,o.jsx)(e.p,{children:\"And the same with Maildev:\"}),`\n`,(0,o.jsx)(e.p,{children:(0,o.jsx)(e.img,{src:\"/blog-images/c1e2f7df42f3529ccde1b981f55afe6a.png\",alt:\"\"})}),`\n`,(0,o.jsxs)(e.h3,{id:\"login-and-configure-the-application\",children:[(0,o.jsx)(e.a,{className:\"anchor\",href:\"#login-and-configure-the-application\",children:(0,o.jsx)(e.span,{className:\"icon icon-link\"})}),\"Login and Configure the Application\"]}),`\n`,(0,o.jsx)(e.p,{children:\"But the most important thing to look at is the PostHog application itself. We will click on \\u201CJust experimenting\\u201D for now.\"}),`\n`,(0,o.jsx)(e.p,{children:(0,o.jsx)(e.img,{src:\"/blog-images/bf9f16972343f2acf6d63595d524143a.png\",alt:\"\"})}),`\n`,(0,o.jsx)(e.p,{children:\"We can verify that all the services and prerequisites are running:\"}),`\n`,(0,o.jsx)(e.p,{children:(0,o.jsx)(e.img,{src:\"/blog-images/50f0aae29aca94afc38ff10ce41b7199.png\",alt:\"\"})}),`\n`,(0,o.jsx)(e.p,{children:\"Then get started by creating an account:\"}),`\n`,(0,o.jsx)(e.p,{children:(0,o.jsx)(e.img,{src:\"/blog-images/81dd405ea02fb45c47387a53e51aa374.png\",alt:\"\"})}),`\n`,(0,o.jsx)(e.p,{children:\"And finally you can start configuring your application and settings to use in your new isolated hobby experience:\"}),`\n`,(0,o.jsx)(e.p,{children:(0,o.jsx)(e.img,{src:\"/blog-images/629fc0d4e252b78520be21c7655226aa.png\",alt:\"\"})}),`\n`,(0,o.jsxs)(e.h3,{id:\"conclusion\",children:[(0,o.jsx)(e.a,{className:\"anchor\",href:\"#conclusion\",children:(0,o.jsx)(e.span,{className:\"icon icon-link\"})}),\"Conclusion\"]}),`\n`,(0,o.jsx)(e.p,{children:\"Using Release to deploy a hobby version of PostHog allows you to run an isolated version of analytics and metrics collection during the development and testing of your own application code. PostHog can be deployed alongside every application deployment you use for each developer or for each branch/feature/pull request that needs to integrate with PostHog, depending on your workflow and requirements.\"}),`\n`,(0,o.jsx)(e.p,{children:\"With Release, it is easy to spin up environments for almost any conceivable use case and this empowers developer teams to deploy applications without relying on a devops or an infrastructure team and minimizing costs associated with provisioning and maintaining the application. You can deploy dependencies or third-party tools that need to be tested or verified when performing application changes or when rolling out new features. Verifying this functionality with third party tools and services is a valuable way to ensure that features reaching staging or production will work even during the earliest phases of development.\"}),`\n`,(0,o.jsxs)(e.p,{children:[\"Ready to get started? Open a \",(0,o.jsx)(e.a,{href:\"https://app.release.com/\",children:\"Release account\"}),\" and start your free trial today or check out our \",(0,o.jsx)(e.a,{href:\"https://docs.release.com/getting-started/quickstart\",children:\"quickstart documentation\"}),\" for additional information.\"]})]})}function k(n={}){let{wrapper:e}=n.components||{};return e?(0,o.jsx)(e,Object.assign({},n,{children:(0,o.jsx)(h,n)})):h(n)}var x=k;return w(H);})();\n;return Component;"
        },
        "_id": "blog/posts/how-to-self-host-posthog-using-release-part-one.mdx",
        "_raw": {
          "sourceFilePath": "blog/posts/how-to-self-host-posthog-using-release-part-one.mdx",
          "sourceFileName": "how-to-self-host-posthog-using-release-part-one.mdx",
          "sourceFileDir": "blog/posts",
          "contentType": "mdx",
          "flattenedPath": "blog/posts/how-to-self-host-posthog-using-release-part-one"
        },
        "type": "BlogPost",
        "computedSlug": "how-to-self-host-posthog-using-release-part-one"
      },
      "documentHash": "1739393595021",
      "hasWarnings": false,
      "documentTypeName": "BlogPost"
    },
    "blog/posts/how-to-set-docker-compose-environment-variables.mdx": {
      "document": {
        "title": "How to set Docker Compose Environment Variables",
        "summary": "How to define environment variables directly in a Docker Compose file, or copy tnem from the host's environment.",
        "publishDate": "Thu Aug 11 2022 14:12:30 GMT+0000 (Coordinated Universal Time)",
        "author": "regis-wilson",
        "readingTime": 4,
        "categories": [
          "platform-engineering",
          "product"
        ],
        "mainImage": "/blog-images/4c0d3d08781f13bb6a55f737ccdd49d2.jpg",
        "imageAlt": "Cog wheels",
        "showCTA": true,
        "ctaCopy": "Looking to streamline environment setup like Docker Compose? Try Release for efficient, on-demand environments management.",
        "ctaLink": "https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=how-to-set-docker-compose-environment-variables",
        "relatedPosts": [
          ""
        ],
        "ogImage": "/blog-images/4c0d3d08781f13bb6a55f737ccdd49d2.jpg",
        "excerpt": "How to define environment variables directly in a Docker Compose file, or copy tnem from the host's environment.",
        "tags": [
          "platform-engineering",
          "product"
        ],
        "ctaButton": "Try Release for Free",
        "body": {
          "raw": "\nDocker Compose lets you run multi-container Docker applications. If you want to use Docker Compose with environment variables, you have several options. The [official documentation](https://docs.docker.com/compose/environment-variables/) is a good reference, but it's hard to understand and very abstract!\n\nLet's take a practical look at how to pass environment variables to your Docker containers using Docker Compose.\n\nWe'll use a simplified version of the [official getting started](https://docs.docker.com/compose/gettingstarted/) Docker Compose app, which is a basic web application built with Python and Flask, but you'll be able to use the same method to access environment variables using any other programming language.\n\n### Setting up the example project\n\nIf you want to follow along, clone our [Example Docker Compose Project](https://github.com/ritza-co/docker-compose-environment-variables-demo). You'll also need Docker and Docker Compose installed on your machine.\n\nWe'll assume that you want to pass an environment variable called _SECRET_ENV_ to your application.\n\nThe project consists of:\n\n- An _app.py_ file that returns a string containing the _SECRET_ENV_ value.\n- A Dockerfile that starts up the Flask app.\n- A Docker Compose that defines the variable and starts the Dockerfile.\n\nA _requirements.txt_ file that installs the Flask framework.\n\n### Defining the environment variable in the Docker Compose file\n\nThe easiest way to send an environment variable to your Docker app is by defining it using an _environment:_ section in your _docker-compose.yml_ file as in the example below:\n\n```yml\n\nversion: \"3.9\"\nservices:\n  web:\n    environment:\n      - SECRET_ENV=my_secret_defined_in_docker_compose_yml\n    build: .\n    ports:\n      - \"8000:5000\"\n  redis:\n    image: \"redis:alpine\"\n\n```\n\nIf you do this, run _docker compose build && docker compose up_, and visit [http://localhost:8000](http://localhost:8000/) in your web browser. You'll see the variable displayed.\n\n![](/blog-images/d3ceded73b3651835e9c323cd8bb9279.png)\n\nThis is easy, but not very useful. If you don't want to have the variable in your application code, you probably don't want it in your _docker-compose.yml_ file either.\n\n### Passing an environment variable from the host\n\nPress Ctrl + C twice to stop the Docker server and edit the _docker-compose.yml_ file to look as follows:\n\n```yml\n\nversion: \"3.9\"\nservices:\n  web:\n    environment:\n      - SECRET_ENV=${HOST_SECRET}\n    build: .\n    ports:\n      - \"8000:5000\"\n  redis:\n    image: \"redis:alpine\"\n\n```\n\nSave the file and run _export HOST_SECRET=my_secret_from_host_ in your shell. This sets the environment variable on your host machine, which will be read by the _docker-compose.yml_ file after you rebuild with Compose.\n\nNow run _docker compose build && docker compose up_ again.\n\nVisit [http://localhost:8000](http://localhost:8000/) in your browser and you'll see that the app now displays the new secret variable.\n\n![](/blog-images/a5a9a53631c94d3cdf9609560003edd3.png)\n\n### Passing an environment variable from a _.env_ file\n\nIf you have a lot of environment variables or you want to be sure you don't lose them, you can define them in a file called _.env_ in the same directory as your _docker-compose.yml_ file.\n\nQuit your shell to clear the _HOST_SECRET_ variable that you set before and open a new shell. Create a file called _.env_ containing the following:\n\nHOST_SECRET=my_secret_from_dot_env_file\n\nSave the file and run _docker compose build && docker compose up_ again.\n\nNow you'll see the variable as you defined it in your _.env_ file. Usually, you would add this file to _.gitignore_ and recreate it on each machine you run the code on, which means that your (sensitive) environment variables are not stored with your codebase.\n\n![](/blog-images/234d1b8930c3fd22ef37cdf72aa6346e.png)\n\n### Understanding the priority of environment variables\n\nIf you define the same variable in a _.env_ file and directly in your environment (for example, using _export HOST_SECRET=my_secret_from_host_), Docker Compose will give priority to your environment. The variable in your _.env_ file will be ignored.\n\nIf you want to check exactly what is being used by your _docker-compose.yml_ file without building the whole project, you can run:\n\n_docker compose convert_\n\nThis will output the model it uses, based on your _docker-compose.yml_ file but with any environment variables replaced with their real values. If you run it now, you'll see it still picks up the value from the _.env_ file.\n\n```yml\n---\n   environment:\n     SECRET_ENV: my_secret_from_dot_env_file\n```\n\nIf you run _export HOST_SECRET=my_secret_from_host_ and _docker compose convert_ again, you'll see it prefers the variable you set explicitly over the one in the _.env_ file.\n\n```yml\n---\n   environment:\n     SECRET_ENV: my_secret_from_host\n```\n\n### The best way to handle environment variables in your Docker Compose projects\n\nTo recap, you can:\n\n- Hard code your variables directly in your _docker-compose.yml_.\n- Pull the variables from the host environment.\n- Define the variables in a _.env_ file.\n\nFor non-sensitive variables that don't change very often, it's easiest to simply put the values directly in the _docker-compose.yml_ file. This means you don't have to spend extra time and effort tracking down the values and messing around with multiple files.\n\nFor sensitive values, such as database passwords and API tokens, you should ideally only define these directly in secure environments (for example, your production server). However, it's inconvenient to lose all of these values every time you need to restart your server or change to a new machine.\n\nTherefore a good tradeoff between security and convenience is to use a _.env_ file containing your sensitive environment variables. It's important to keep any copies of this file in a secure place, such as a secrets manager or vault, and to not check in this file as part of your code base.\n\n### Managing environment variables and secrets with ReleaseHub\n\nIf you're looking for a simple and powerful platform to manage all of your environments for you, take a look at [Release](https://release.com/). Using our custom application template file, you can easily set environment variables and map them to specific environments, ensuring that your secret management is both secure and convenient.\n\n‍\n",
          "code": "var Component=(()=>{var d=Object.create;var a=Object.defineProperty;var m=Object.getOwnPropertyDescriptor;var p=Object.getOwnPropertyNames;var u=Object.getPrototypeOf,v=Object.prototype.hasOwnProperty;var f=(o,e)=>()=>(e||o((e={exports:{}}).exports,e),e.exports),y=(o,e)=>{for(var i in e)a(o,i,{get:e[i],enumerable:!0})},l=(o,e,i,t)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let r of p(e))!v.call(o,r)&&r!==i&&a(o,r,{get:()=>e[r],enumerable:!(t=m(e,r))||t.enumerable});return o};var g=(o,e,i)=>(i=o!=null?d(u(o)):{},l(e||!o||!o.__esModule?a(i,\"default\",{value:o,enumerable:!0}):i,o)),b=o=>l(a({},\"__esModule\",{value:!0}),o);var c=f((E,s)=>{s.exports=_jsx_runtime});var C={};y(C,{default:()=>_,frontmatter:()=>k});var n=g(c()),k={title:\"How to set Docker Compose Environment Variables\",summary:\"How to define environment variables directly in a Docker Compose file, or copy tnem from the host's environment.\",publishDate:\"Thu Aug 11 2022 14:12:30 GMT+0000 (Coordinated Universal Time)\",author:\"regis-wilson\",readingTime:4,categories:[\"platform-engineering\",\"product\"],mainImage:\"/blog-images/4c0d3d08781f13bb6a55f737ccdd49d2.jpg\",imageAlt:\"Cog wheels\",showCTA:!0,ctaCopy:\"Looking to streamline environment setup like Docker Compose? Try Release for efficient, on-demand environments management.\",ctaLink:\"https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=how-to-set-docker-compose-environment-variables\",relatedPosts:[\"\"],ogImage:\"/blog-images/4c0d3d08781f13bb6a55f737ccdd49d2.jpg\",excerpt:\"How to define environment variables directly in a Docker Compose file, or copy tnem from the host's environment.\",tags:[\"platform-engineering\",\"product\"],ctaButton:\"Try Release for Free\"};function h(o){let e=Object.assign({p:\"p\",a:\"a\",h3:\"h3\",span:\"span\",em:\"em\",ul:\"ul\",li:\"li\",pre:\"pre\",code:\"code\",img:\"img\"},o.components);return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsxs)(e.p,{children:[\"Docker Compose lets you run multi-container Docker applications. If you want to use Docker Compose with environment variables, you have several options. The \",(0,n.jsx)(e.a,{href:\"https://docs.docker.com/compose/environment-variables/\",children:\"official documentation\"}),\" is a good reference, but it's hard to understand and very abstract!\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Let's take a practical look at how to pass environment variables to your Docker containers using Docker Compose.\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"We'll use a simplified version of the \",(0,n.jsx)(e.a,{href:\"https://docs.docker.com/compose/gettingstarted/\",children:\"official getting started\"}),\" Docker Compose app, which is a basic web application built with Python and Flask, but you'll be able to use the same method to access environment variables using any other programming language.\"]}),`\n`,(0,n.jsxs)(e.h3,{id:\"setting-up-the-example-project\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#setting-up-the-example-project\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Setting up the example project\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"If you want to follow along, clone our \",(0,n.jsx)(e.a,{href:\"https://github.com/ritza-co/docker-compose-environment-variables-demo\",children:\"Example Docker Compose Project\"}),\". You'll also need Docker and Docker Compose installed on your machine.\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"We'll assume that you want to pass an environment variable called \",(0,n.jsx)(e.em,{children:\"SECRET_ENV\"}),\" to your application.\"]}),`\n`,(0,n.jsx)(e.p,{children:\"The project consists of:\"}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsxs)(e.li,{children:[\"An \",(0,n.jsx)(e.em,{children:\"app.py\"}),\" file that returns a string containing the \",(0,n.jsx)(e.em,{children:\"SECRET_ENV\"}),\" value.\"]}),`\n`,(0,n.jsx)(e.li,{children:\"A Dockerfile that starts up the Flask app.\"}),`\n`,(0,n.jsx)(e.li,{children:\"A Docker Compose that defines the variable and starts the Dockerfile.\"}),`\n`]}),`\n`,(0,n.jsxs)(e.p,{children:[\"A \",(0,n.jsx)(e.em,{children:\"requirements.txt\"}),\" file that installs the Flask framework.\"]}),`\n`,(0,n.jsxs)(e.h3,{id:\"defining-the-environment-variable-in-the-docker-compose-file\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#defining-the-environment-variable-in-the-docker-compose-file\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Defining the environment variable in the Docker Compose file\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"The easiest way to send an environment variable to your Docker app is by defining it using an \",(0,n.jsx)(e.em,{children:\"environment:\"}),\" section in your \",(0,n.jsx)(e.em,{children:\"docker-compose.yml\"}),\" file as in the example below:\"]}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-yml\",children:`\nversion: \"3.9\"\nservices:\n \\xA0web:\n \\xA0 \\xA0environment:\n \\xA0 \\xA0 \\xA0- SECRET_ENV=my_secret_defined_in_docker_compose_yml\n \\xA0 \\xA0build: .\n \\xA0 \\xA0ports:\n \\xA0 \\xA0 \\xA0- \"8000:5000\"\n \\xA0redis:\n \\xA0 \\xA0image: \"redis:alpine\"\n\n`})}),`\n`,(0,n.jsxs)(e.p,{children:[\"If you do this, run \",(0,n.jsx)(e.em,{children:\"docker compose build && docker compose up\"}),\", and visit \",(0,n.jsx)(e.a,{href:\"http://localhost:8000/\",children:\"http://localhost:8000\"}),\" in your web browser. You'll see the variable displayed.\"]}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/d3ceded73b3651835e9c323cd8bb9279.png\",alt:\"\"})}),`\n`,(0,n.jsxs)(e.p,{children:[\"This is easy, but not very useful. If you don't want to have the variable in your application code, you probably don't want it in your \",(0,n.jsx)(e.em,{children:\"docker-compose.yml\"}),\" file either.\"]}),`\n`,(0,n.jsxs)(e.h3,{id:\"passing-an-environment-variable-from-the-host\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#passing-an-environment-variable-from-the-host\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Passing an environment variable from the host\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"Press Ctrl + C twice to stop the Docker server and edit the \",(0,n.jsx)(e.em,{children:\"docker-compose.yml\"}),\" file to look as follows:\"]}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-yml\",children:`\nversion: \"3.9\"\nservices:\n \\xA0web:\n \\xA0 \\xA0environment:\n \\xA0 \\xA0 \\xA0- SECRET_ENV=\\${HOST_SECRET}\n \\xA0 \\xA0build: .\n \\xA0 \\xA0ports:\n \\xA0 \\xA0 \\xA0- \"8000:5000\"\n \\xA0redis:\n \\xA0 \\xA0image: \"redis:alpine\"\n\n`})}),`\n`,(0,n.jsxs)(e.p,{children:[\"Save the file and run \",(0,n.jsx)(e.em,{children:\"export HOST_SECRET=my_secret_from_host\"}),\" in your shell. This sets the environment variable on your host machine, which will be read by the \",(0,n.jsx)(e.em,{children:\"docker-compose.yml\"}),\" file after you rebuild with Compose.\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"Now run \",(0,n.jsx)(e.em,{children:\"docker compose build && docker compose up\"}),\" again.\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"Visit \",(0,n.jsx)(e.a,{href:\"http://localhost:8000/\",children:\"http://localhost:8000\"}),\" in your browser and you'll see that the app now displays the new secret variable.\"]}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/a5a9a53631c94d3cdf9609560003edd3.png\",alt:\"\"})}),`\n`,(0,n.jsxs)(e.h3,{id:\"passing-an-environment-variable-from-a-env-file\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#passing-an-environment-variable-from-a-env-file\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Passing an environment variable from a \",(0,n.jsx)(e.em,{children:\".env\"}),\" file\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"If you have a lot of environment variables or you want to be sure you don't lose them, you can define them in a file called \",(0,n.jsx)(e.em,{children:\".env\"}),\" in the same directory as your \",(0,n.jsx)(e.em,{children:\"docker-compose.yml\"}),\" file.\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"Quit your shell to clear the \",(0,n.jsx)(e.em,{children:\"HOST_SECRET\"}),\" variable that you set before and open a new shell. Create a file called \",(0,n.jsx)(e.em,{children:\".env\"}),\" containing the following:\"]}),`\n`,(0,n.jsx)(e.p,{children:\"HOST_SECRET=my_secret_from_dot_env_file\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"Save the file and run \",(0,n.jsx)(e.em,{children:\"docker compose build && docker compose up\"}),\" again.\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"Now you'll see the variable as you defined it in your \",(0,n.jsx)(e.em,{children:\".env\"}),\" file. Usually, you would add this file to \",(0,n.jsx)(e.em,{children:\".gitignore\"}),\" and recreate it on each machine you run the code on, which means that your (sensitive) environment variables are not stored with your codebase.\"]}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/234d1b8930c3fd22ef37cdf72aa6346e.png\",alt:\"\"})}),`\n`,(0,n.jsxs)(e.h3,{id:\"understanding-the-priority-of-environment-variables\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#understanding-the-priority-of-environment-variables\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Understanding the priority of environment variables\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"If you define the same variable in a \",(0,n.jsx)(e.em,{children:\".env\"}),\" file and directly in your environment (for example, using \",(0,n.jsx)(e.em,{children:\"export HOST_SECRET=my_secret_from_host\"}),\"), Docker Compose will give priority to your environment. The variable in your \",(0,n.jsx)(e.em,{children:\".env\"}),\" file will be ignored.\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"If you want to check exactly what is being used by your \",(0,n.jsx)(e.em,{children:\"docker-compose.yml\"}),\" file without building the whole project, you can run:\"]}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.em,{children:\"docker compose convert\"})}),`\n`,(0,n.jsxs)(e.p,{children:[\"This will output the model it uses, based on your \",(0,n.jsx)(e.em,{children:\"docker-compose.yml\"}),\" file but with any environment variables replaced with their real values. If you run it now, you'll see it still picks up the value from the \",(0,n.jsx)(e.em,{children:\".env\"}),\" file.\"]}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-yml\",children:`---\n\\xA0 \\xA0environment:\n\\xA0 \\xA0 \\xA0SECRET_ENV: my_secret_from_dot_env_file\n`})}),`\n`,(0,n.jsxs)(e.p,{children:[\"If you run \",(0,n.jsx)(e.em,{children:\"export HOST_SECRET=my_secret_from_host\"}),\" and \",(0,n.jsx)(e.em,{children:\"docker compose convert\"}),\" again, you'll see it prefers the variable you set explicitly over the one in the \",(0,n.jsx)(e.em,{children:\".env\"}),\" file.\"]}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-yml\",children:`---\n\\xA0 \\xA0environment:\n\\xA0 \\xA0 \\xA0SECRET_ENV: my_secret_from_host\n`})}),`\n`,(0,n.jsxs)(e.h3,{id:\"the-best-way-to-handle-environment-variables-in-your-docker-compose-projects\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#the-best-way-to-handle-environment-variables-in-your-docker-compose-projects\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"The best way to handle environment variables in your Docker Compose projects\"]}),`\n`,(0,n.jsx)(e.p,{children:\"To recap, you can:\"}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsxs)(e.li,{children:[\"Hard code your variables directly in your \",(0,n.jsx)(e.em,{children:\"docker-compose.yml\"}),\".\"]}),`\n`,(0,n.jsx)(e.li,{children:\"Pull the variables from the host environment.\"}),`\n`,(0,n.jsxs)(e.li,{children:[\"Define the variables in a \",(0,n.jsx)(e.em,{children:\".env\"}),\" file.\"]}),`\n`]}),`\n`,(0,n.jsxs)(e.p,{children:[\"For non-sensitive variables that don't change very often, it's easiest to simply put the values directly in the \",(0,n.jsx)(e.em,{children:\"docker-compose.yml\"}),\" file. This means you don't have to spend extra time and effort tracking down the values and messing around with multiple files.\"]}),`\n`,(0,n.jsx)(e.p,{children:\"For sensitive values, such as database passwords and API tokens, you should ideally only define these directly in secure environments (for example, your production server). However, it's inconvenient to lose all of these values every time you need to restart your server or change to a new machine.\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"Therefore a good tradeoff between security and convenience is to use a \",(0,n.jsx)(e.em,{children:\".env\"}),\" file containing your sensitive environment variables. It's important to keep any copies of this file in a secure place, such as a secrets manager or vault, and to not check in this file as part of your code base.\"]}),`\n`,(0,n.jsxs)(e.h3,{id:\"managing-environment-variables-and-secrets-with-releasehub\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#managing-environment-variables-and-secrets-with-releasehub\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Managing environment variables and secrets with ReleaseHub\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"If you're looking for a simple and powerful platform to manage all of your environments for you, take a look at \",(0,n.jsx)(e.a,{href:\"https://release.com/\",children:\"Release\"}),\". Using our custom application template file, you can easily set environment variables and map them to specific environments, ensuring that your secret management is both secure and convenient.\"]}),`\n`,(0,n.jsx)(e.p,{children:\"\\u200D\"})]})}function w(o={}){let{wrapper:e}=o.components||{};return e?(0,n.jsx)(e,Object.assign({},o,{children:(0,n.jsx)(h,o)})):h(o)}var _=w;return b(C);})();\n;return Component;"
        },
        "_id": "blog/posts/how-to-set-docker-compose-environment-variables.mdx",
        "_raw": {
          "sourceFilePath": "blog/posts/how-to-set-docker-compose-environment-variables.mdx",
          "sourceFileName": "how-to-set-docker-compose-environment-variables.mdx",
          "sourceFileDir": "blog/posts",
          "contentType": "mdx",
          "flattenedPath": "blog/posts/how-to-set-docker-compose-environment-variables"
        },
        "type": "BlogPost",
        "computedSlug": "how-to-set-docker-compose-environment-variables"
      },
      "documentHash": "1739393595021",
      "hasWarnings": false,
      "documentTypeName": "BlogPost"
    },
    "blog/posts/how-to-solve-aws-efs-operation-not-permitted-errors-in-eks.mdx": {
      "document": {
        "title": "How to Solve AWS EFS “Operation Not Permitted” Errors in EKS",
        "summary": "Learn how we have helped our customers maintain strong deployment without overloading the workload and application stack",
        "publishDate": "Sat Jul 16 2022 20:59:54 GMT+0000 (Coordinated Universal Time)",
        "author": "regis-wilson",
        "readingTime": 7,
        "categories": [
          "kubernetes",
          "platform-engineering"
        ],
        "mainImage": "/blog-images/5b34326d4b07234018b56444f8235b98.jpg",
        "imageAlt": "A block of code in a computer screen",
        "showCTA": true,
        "ctaCopy": "By automating environment setup like EFS in EKS, Release simplifies storage management for Kubernetes workloads. Try it now!",
        "ctaLink": "https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=how-to-solve-aws-efs-operation-not-permitted-errors-in-eks",
        "relatedPosts": [
          ""
        ],
        "ogImage": "/blog-images/5b34326d4b07234018b56444f8235b98.jpg",
        "excerpt": "Learn how we have helped our customers maintain strong deployment without overloading the workload and application stack",
        "tags": [
          "kubernetes",
          "platform-engineering"
        ],
        "ctaButton": "Try Release for Free",
        "body": {
          "raw": "\nAt ReleaseHub, we operate dozens of Amazon Elastic Kubernetes Service (EKS) clusters on behalf of our customers. The various workloads and application stacks we have to support are practically as diverse as the number of engineers who use our product. One very common use case is a permanent storage space for the workloads that are deployed in each environment.\n\nThe most common general solution for storage in AWS for compute workloads is the Elastic Block Service (EBS), which has the advantage of being relatively performant and easy to set up. However, it has the drawback that EBS volumes are tied to a specific Availability Zone (AZ). Therefore, using Kubernetes workloads running in multiple Availability Zones (AZs), it turns out that ensuring pod workloads correctly map to the correct AZ is actually difficult to do properly and has caused numerous issues for our customers who use EBS storage in their clusters. We also discovered that EBS storage costs can add up quickly and over-provisioning volume sizes (which is a necessary evil) can add to this problem.\n\nWithout going too far down the pros and cons of each storage system, we found that most customers were well satisfied with using Elastic FileSystem (EFS) mount points to provide persistent storage volumes backing the application workloads deployed to their clusters. EFS provides a good balance of performance, reliability, price (pay-for-what-you-store), and AZ diversification. As such, we made an early decision to move almost all customer workloads off EBS to EFS and only allowed the EBS option for customer workloads who specifically opt-in to it. This solution worked well for us since EKS version 1.14 all the way up until recently when we started moving customers to 1.21 and beyond.\n\n### The Problem\n\nIn our original implementation of EFS workloads in EKS, we started out using the (now retired) [EFS provisioner](https://github.com/kubernetes-retired/external-storage/tree/master/aws/efs). This solution allowed our customers to specify a volume for persistent storage and the provisioner would create a filesystem mount from an existing EFS infrastructure point (which we create automatically upon cluster creation). The customer pods would then mount this filesystem and have unlimited storage that would persist until the workload expired or was deleted, at which point the volume space would be removed. We literally experienced zero issues with this configuration from the first time we tested it.\n\nIn recent months, we have been tirelessly upgrading to the latest version(s) of EKS to keep customers up to date with the latest features and deprecations in the never ending Kubernetes versions. Upon reviewing the various addons and plugins, we realised that the EFS provisioner was replaced by the modern [EFS CSI driver](https://github.com/kubernetes-sigs/aws-efs-csi-driver). You can read more about the two projects in [this stack overflow article](https://github.com/kubernetes-sigs/aws-efs-csi-driver).\n\nThe upgrade process was not terribly difficult for us since we could easily run both provisioners side by side and then switch over workloads using the [Kubernetes Storage Class](https://kubernetes.io/docs/concepts/storage/storage-classes/) objects. As one example, Customer A would be using the legacy _provisioner: releasehub.com/aws-efs_ storage class and then we could upgrade any subsequent workloads to _provisioner: efs.csi.aws.com_ and then test until we were satisfied with the results. Rolling back was easy to revert the workloads back to the original storage class.\n\nEventually, after demonstrating that the process worked seamlessly and nearly flawlessly with the new driver and the same infrastructure in a variety of scenarios, we were able to confidently roll out the changes to more and more customers in a planned migration.\n\nThat was when we ran into two major stumbling blocks with customer workloads that use persistent volumes: postgres and rabbitmq containers. Here are the horrible details we discovered for each:\n\n_initdb: could not change permissions of directory \"/var/lib/postgresql/data/pgdata\": Operation not permitted_\n\n‍*chown: /var/lib/rabbitmq: Operation not permitted*\n\nIt is important to note that this could happen to any workloads that use the chown command, but these were the most common complaints we got from customers.\n\n### Diagnosis\n\nAt first, we did what every engineer does: we searched Google and confirmed the problems were widespread, finding stack overflow and server fault questions [here](https://stackoverflow.com/questions/51801220/postgres-on-kubernetes-volume-permission-error) and [here](https://serverfault.com/questions/993907/rabbitmq-kubernetes-with-nfs-mount) respectively. Unfortunately, and most frustratingly, there were no good solutions to the problem(s) and even worse, many of the solutions posited by people were highly complex, tightly tied to a particular implementation, or technically brittle. There seemed to be no particularly elegant, easy solution especially for our wide diversity of customer user cases.\n\nWe tried using the latest versions of the drivers to no avail. We tried even older versions of the CSI driver to see if this might have been a regression (to no avail). Digging in even deeper to EKS and EFS specifically, we discovered that dynamic provisioning (which is what we rely on to provide a seamless, fast, efficient service for workloads) was [recently added to the new CSI driver](https://aws.amazon.com/blogs/containers/introducing-efs-csi-dynamic-provisioning/). This [GitHub issue](https://github.com/kubernetes-sigs/aws-efs-csi-driver/issues/300) (unsolved to this day) indicates that the problem has actually been in place from the beginning of the driver’s use cases.\n\nReading through the various use cases affected was like reading a long-lost diary of all our horrible secrets and failures laid bare: including some horrific harbingers of doom we had nearly inflicted on the rest of our customers who were yet to be migrated. We quickly reviewed our test cases and made the stunning discovery that we had been testing all kinds of workloads that read and write to NFS volumes, but hadn’t tested the ones that use chown. That was the only use case we hadn’t considered, and it was the one use case that failed.\n\nThe root cause of the issue is that an EFS mount point that is dynamically created for a pod workload is given a set of mapped numerical User IDs (UIDs), but the UID that is stored inside the pod workload typically will not match the UID assigned to the EFS mount point. In most use cases, the operating system will not necessarily care what UID is in use on the mounted filesystem; it will typically just blindly read and/or write to the filesystem and assume that if the operation is a success that the permissions are correct. There are a number of good reasons not to be that trusting however. For example, in a database scenario, the permissions related to reading and writing data for the storage of important information is not left to chance and the application will attempt to ensure the UID (and maybe even Group IDs \\[GIDs\\]) match.\n\nThis did not answer the question of why the legacy deprecated provisioner seems to work flawlessly, but we will dig into that on another blog post.\n\nTo date, there does not seem to be any way to match the UIDs so that the operating system inside the container can set or even pretend to set the UID of a directory the application needs for reading and writing so that it matches the physical infrastructure underlying Kubernetes. This is not just an academic legacy issue, it is a real concern for security and privacy reasons that affect modern applications running in modern Cloud Native environments.\n\n### A Few Solutions\n\nFinally we present a few solutions, in chronological order of ones that we tried. We gradually settled on the last option as you will see the rationale behind this decision unfold.\n\nOption 1: Find every occurrence of Waldo and fix it for each customer and application workload. This option sounds as bad as you imagine it would be. Worse, it could make an easy and simple solution (pull a standard container and run it) unusable under normal circumstances. Even worse, our work would never be done: any new customers we onboard would have a new set of changes or fixes or workarounds to find and implement.\n\nFor example, we could easily identify the lines affecting us in the [postgresql image entrypoint](https://github.com/docker-library/postgres/issues/361#issuecomment-468391845) and create our own version. Which you would then need to create a separate dockerfile and modify it to your tastes…for each customer and each version of postgres and operating system that is in use times the number of applications each customer uses. Or, we could try to force the UID and GID numbers to match the CSI provisioner’s UID and GID to match (again, with a [splinter version of the dockerfile](https://github.com/docker-library/postgres/issues/361#issuecomment-508303459)). Now that we have quote-unquote, allegedly, supposedly, air quotes “solved” the problem, do the exact same thing for the next application (like rabbitmq, or Jenkins, or _whatever_) and all the application and operating system versions. Not just now, but also moving forward into the future forever.\n\nOption 2: Try to boil the ocean to find every single species of fish and identify them. Taking a step back, it is clear that we cannot hope to ever solve every use case of chown that is out there in the wild today, not to mention new ones that are being born every year. We were able to identify that most docker images use a specific UID and GID combination and the numbers of these are fairly limited. Examining two use cases in question, we found that postgresql images tended to use 999:999 and several others used 99 or 100, perhaps 1000 and 1001. This seemed like a promising lead to a solution because you can specify the [UID in the CSI provisioner](https://github.com/kubernetes-sigs/aws-efs-csi-driver#storage-class-parameters-for-dynamic-provisioning).\n\nThis elegant solution would result in creating several StorageClasses in Kubernetes, like say, “postgresql-999”, “rabbitmq-1001”, and so forth. Or maybe just “efs-uid-999” to be more generic. Then we would teach each customer who enjoyed a failed build or deploy stack trace to change their settings to use the appropriate StorageClass. Even better, there are only about 2^16 possible unique UIDs in Linux, so we could programmatically create all of them in advance and apply them to our cluster to be stored in etcd, ready for retrieval whenever a customer wanted a UID-specific storage class. Or to limit choices in an opinionated but friendly way, we could require all containers to use a fixed UID, like 42, in order to use the storage volumes on our platform. If a customer wanted to use a different UID, like 43, we could charge $1 for every UID above and beyond the original one.\n\nIf you did not detect any sarcasm in the preceding paragraph, you may want to call a crisis hotline to discuss obtaining a sense of humour. Amazon does not sell any upon last check; although you might find a used version on Etsy or eBay. I once ordered a sense of humour and it was stolen by a porch pirate before I could bring it in. Once I had obtained a suitable one, I would occasionally rent mine out on the joke version of Uber or Lyft, and sometimes you can even spend the night in my sense of humour on AirBNB, but due to abuse and lack of adequate tipping I have had to scale my activities down lately.\n\nOption 3: When in doubt, rollback to when it worked. We ultimately had to decide that we would be unable to support the new CSI driver until an adequate solution for dynamic deployments of EFS volumes was found for EKS. In the world of open source, there is always someone who comes up with a clever solution to a common problem and that becomes the de facto implementation recommendation. Currently, we were satisfied with the original functionality of the deprecated provisioner.\n\nBut this raises another issue, how do we square using a deprecated and potentially unsupported solution on a platform our customers depend and rely upon? The answer is that we can make small adjustments and updates to the yaml and source code since the original solution code is still available and can be updated by Releasehub to support our customers.\n\n### Conclusion\n\nSometimes we must accept that we live in an imperfect world and accept the fact that we are as imperfect as the imperfect world we live in which means that we should accept the imperfection as the correct way that things should be and thus, the imperfection we see in the world merely reflects the imperfections in ourselves, which makes us perfect in every way.\n",
          "code": "var Component=(()=>{var h=Object.create;var n=Object.defineProperty;var u=Object.getOwnPropertyDescriptor;var m=Object.getOwnPropertyNames;var p=Object.getPrototypeOf,w=Object.prototype.hasOwnProperty;var f=(o,e)=>()=>(e||o((e={exports:{}}).exports,e),e.exports),g=(o,e)=>{for(var s in e)n(o,s,{get:e[s],enumerable:!0})},r=(o,e,s,i)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let a of m(e))!w.call(o,a)&&a!==s&&n(o,a,{get:()=>e[a],enumerable:!(i=u(e,a))||i.enumerable});return o};var y=(o,e,s)=>(s=o!=null?h(p(o)):{},r(e||!o||!o.__esModule?n(s,\"default\",{value:o,enumerable:!0}):s,o)),b=o=>r(n({},\"__esModule\",{value:!0}),o);var d=f((T,l)=>{l.exports=_jsx_runtime});var I={};g(I,{default:()=>S,frontmatter:()=>v});var t=y(d()),v={title:\"How to Solve AWS EFS \\u201COperation Not Permitted\\u201D Errors in EKS\",summary:\"Learn how we have helped our customers maintain strong deployment without overloading the workload and application stack\",publishDate:\"Sat Jul 16 2022 20:59:54 GMT+0000 (Coordinated Universal Time)\",author:\"regis-wilson\",readingTime:7,categories:[\"kubernetes\",\"platform-engineering\"],mainImage:\"/blog-images/5b34326d4b07234018b56444f8235b98.jpg\",imageAlt:\"A block of code in a computer screen\",showCTA:!0,ctaCopy:\"By automating environment setup like EFS in EKS, Release simplifies storage management for Kubernetes workloads. Try it now!\",ctaLink:\"https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=how-to-solve-aws-efs-operation-not-permitted-errors-in-eks\",relatedPosts:[\"\"],ogImage:\"/blog-images/5b34326d4b07234018b56444f8235b98.jpg\",excerpt:\"Learn how we have helped our customers maintain strong deployment without overloading the workload and application stack\",tags:[\"kubernetes\",\"platform-engineering\"],ctaButton:\"Try Release for Free\"};function c(o){let e=Object.assign({p:\"p\",h3:\"h3\",a:\"a\",span:\"span\",em:\"em\"},o.components);return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(e.p,{children:\"At ReleaseHub, we operate dozens of Amazon Elastic Kubernetes Service (EKS) clusters on behalf of our customers. The various workloads and application stacks we have to support are practically as diverse as the number of engineers who use our product. One very common use case is a permanent storage space for the workloads that are deployed in each environment.\"}),`\n`,(0,t.jsx)(e.p,{children:\"The most common general solution for storage in AWS for compute workloads is the Elastic Block Service (EBS), which has the advantage of being relatively performant and easy to set up. However, it has the drawback that EBS volumes are tied to a specific Availability Zone (AZ). Therefore, using Kubernetes workloads running in multiple Availability Zones (AZs), it turns out that ensuring pod workloads correctly map to the correct AZ is actually difficult to do properly and has caused numerous issues for our customers who use EBS storage in their clusters. We also discovered that EBS storage costs can add up quickly and over-provisioning volume sizes (which is a necessary evil) can add to this problem.\"}),`\n`,(0,t.jsx)(e.p,{children:\"Without going too far down the pros and cons of each storage system, we found that most customers were well satisfied with using Elastic FileSystem (EFS) mount points to provide persistent storage volumes backing the application workloads deployed to their clusters. EFS provides a good balance of performance, reliability, price (pay-for-what-you-store), and AZ diversification. As such, we made an early decision to move almost all customer workloads off EBS to EFS and only allowed the EBS option for customer workloads who specifically opt-in to it. This solution worked well for us since EKS version 1.14 all the way up until recently when we started moving customers to 1.21 and beyond.\"}),`\n`,(0,t.jsxs)(e.h3,{id:\"the-problem\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#the-problem\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"The Problem\"]}),`\n`,(0,t.jsxs)(e.p,{children:[\"In our original implementation of EFS workloads in EKS, we started out using the (now retired) \",(0,t.jsx)(e.a,{href:\"https://github.com/kubernetes-retired/external-storage/tree/master/aws/efs\",children:\"EFS provisioner\"}),\". This solution allowed our customers to specify a volume for persistent storage and the provisioner would create a filesystem mount from an existing EFS infrastructure point (which we create automatically upon cluster creation). The customer pods would then mount this filesystem and have unlimited storage that would persist until the workload expired or was deleted, at which point the volume space would be removed. We literally experienced zero issues with this configuration from the first time we tested it.\"]}),`\n`,(0,t.jsxs)(e.p,{children:[\"In recent months, we have been tirelessly upgrading to the latest version(s) of EKS to keep customers up to date with the latest features and deprecations in the never ending Kubernetes versions. Upon reviewing the various addons and plugins, we realised that the EFS provisioner was replaced by the modern \",(0,t.jsx)(e.a,{href:\"https://github.com/kubernetes-sigs/aws-efs-csi-driver\",children:\"EFS CSI driver\"}),\". You can read more about the two projects in \",(0,t.jsx)(e.a,{href:\"https://github.com/kubernetes-sigs/aws-efs-csi-driver\",children:\"this stack overflow article\"}),\".\"]}),`\n`,(0,t.jsxs)(e.p,{children:[\"The upgrade process was not terribly difficult for us since we could easily run both provisioners side by side and then switch over workloads using the \",(0,t.jsx)(e.a,{href:\"https://kubernetes.io/docs/concepts/storage/storage-classes/\",children:\"Kubernetes Storage Class\"}),\" objects. As one example, Customer A would be using the legacy \",(0,t.jsx)(e.em,{children:\"provisioner: releasehub.com/aws-efs\"}),\" storage class and then we could upgrade any subsequent workloads to \",(0,t.jsx)(e.em,{children:\"provisioner: efs.csi.aws.com\"}),\" and then test until we were satisfied with the results. Rolling back was easy to revert the workloads back to the original storage class.\"]}),`\n`,(0,t.jsx)(e.p,{children:\"Eventually, after demonstrating that the process worked seamlessly and nearly flawlessly with the new driver and the same infrastructure in a variety of scenarios, we were able to confidently roll out the changes to more and more customers in a planned migration.\"}),`\n`,(0,t.jsx)(e.p,{children:\"That was when we ran into two major stumbling blocks with customer workloads that use persistent volumes: postgres and rabbitmq containers. Here are the horrible details we discovered for each:\"}),`\n`,(0,t.jsx)(e.p,{children:(0,t.jsx)(e.em,{children:'initdb: could not change permissions of directory \"/var/lib/postgresql/data/pgdata\": Operation not permitted'})}),`\n`,(0,t.jsxs)(e.p,{children:[\"\\u200D\",(0,t.jsx)(e.em,{children:\"chown: /var/lib/rabbitmq: Operation not permitted\"})]}),`\n`,(0,t.jsx)(e.p,{children:\"It is important to note that this could happen to any workloads that use the chown command, but these were the most common complaints we got from customers.\"}),`\n`,(0,t.jsxs)(e.h3,{id:\"diagnosis\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#diagnosis\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Diagnosis\"]}),`\n`,(0,t.jsxs)(e.p,{children:[\"At first, we did what every engineer does: we searched Google and confirmed the problems were widespread, finding stack overflow and server fault questions \",(0,t.jsx)(e.a,{href:\"https://stackoverflow.com/questions/51801220/postgres-on-kubernetes-volume-permission-error\",children:\"here\"}),\" and \",(0,t.jsx)(e.a,{href:\"https://serverfault.com/questions/993907/rabbitmq-kubernetes-with-nfs-mount\",children:\"here\"}),\" respectively. Unfortunately, and most frustratingly, there were no good solutions to the problem(s) and even worse, many of the solutions posited by people were highly complex, tightly tied to a particular implementation, or technically brittle. There seemed to be no particularly elegant, easy solution especially for our wide diversity of customer user cases.\"]}),`\n`,(0,t.jsxs)(e.p,{children:[\"We tried using the latest versions of the drivers to no avail. We tried even older versions of the CSI driver to see if this might have been a regression (to no avail). Digging in even deeper to EKS and EFS specifically, we discovered that dynamic provisioning (which is what we rely on to provide a seamless, fast, efficient service for workloads) was \",(0,t.jsx)(e.a,{href:\"https://aws.amazon.com/blogs/containers/introducing-efs-csi-dynamic-provisioning/\",children:\"recently added to the new CSI driver\"}),\". This \",(0,t.jsx)(e.a,{href:\"https://github.com/kubernetes-sigs/aws-efs-csi-driver/issues/300\",children:\"GitHub issue\"}),\" (unsolved to this day) indicates that the problem has actually been in place from the beginning of the driver\\u2019s use cases.\"]}),`\n`,(0,t.jsx)(e.p,{children:\"Reading through the various use cases affected was like reading a long-lost diary of all our horrible secrets and failures laid bare: including some horrific harbingers of doom we had nearly inflicted on the rest of our customers who were yet to be migrated. We quickly reviewed our test cases and made the stunning discovery that we had been testing all kinds of workloads that read and write to NFS volumes, but hadn\\u2019t tested the ones that use chown. That was the only use case we hadn\\u2019t considered, and it was the one use case that failed.\"}),`\n`,(0,t.jsx)(e.p,{children:\"The root cause of the issue is that an EFS mount point that is dynamically created for a pod workload is given a set of mapped numerical User IDs (UIDs), but the UID that is stored inside the pod workload typically will not match the UID assigned to the EFS mount point. In most use cases, the operating system will not necessarily care what UID is in use on the mounted filesystem; it will typically just blindly read and/or write to the filesystem and assume that if the operation is a success that the permissions are correct. There are a number of good reasons not to be that trusting however. For example, in a database scenario, the permissions related to reading and writing data for the storage of important information is not left to chance and the application will attempt to ensure the UID (and maybe even Group IDs [GIDs]) match.\"}),`\n`,(0,t.jsx)(e.p,{children:\"This did not answer the question of why the legacy deprecated provisioner seems to work flawlessly, but we will dig into that on another blog post.\"}),`\n`,(0,t.jsx)(e.p,{children:\"To date, there does not seem to be any way to match the UIDs so that the operating system inside the container can set or even pretend to set the UID of a directory the application needs for reading and writing so that it matches the physical infrastructure underlying Kubernetes. This is not just an academic legacy issue, it is a real concern for security and privacy reasons that affect modern applications running in modern Cloud Native environments.\"}),`\n`,(0,t.jsxs)(e.h3,{id:\"a-few-solutions\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#a-few-solutions\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"A Few Solutions\"]}),`\n`,(0,t.jsx)(e.p,{children:\"Finally we present a few solutions, in chronological order of ones that we tried. We gradually settled on the last option as you will see the rationale behind this decision unfold.\"}),`\n`,(0,t.jsx)(e.p,{children:\"Option 1: Find every occurrence of Waldo and fix it for each customer and application workload. This option sounds as bad as you imagine it would be. Worse, it could make an easy and simple solution (pull a standard container and run it) unusable under normal circumstances. Even worse, our work would never be done: any new customers we onboard would have a new set of changes or fixes or workarounds to find and implement.\"}),`\n`,(0,t.jsxs)(e.p,{children:[\"For example, we could easily identify the lines affecting us in the \",(0,t.jsx)(e.a,{href:\"https://github.com/docker-library/postgres/issues/361#issuecomment-468391845\",children:\"postgresql image entrypoint\"}),\" and create our own version. Which you would then need to create a separate dockerfile and modify it to your tastes\\u2026for each customer and each version of postgres and operating system that is in use times the number of applications each customer uses. Or, we could try to force the UID and GID numbers to match the CSI provisioner\\u2019s UID and GID to match (again, with a \",(0,t.jsx)(e.a,{href:\"https://github.com/docker-library/postgres/issues/361#issuecomment-508303459\",children:\"splinter version of the dockerfile\"}),\"). Now that we have quote-unquote, allegedly, supposedly, air quotes \\u201Csolved\\u201D the problem, do the exact same thing for the next application (like rabbitmq, or Jenkins, or \",(0,t.jsx)(e.em,{children:\"whatever\"}),\") and all the application and operating system versions. Not just now, but also moving forward into the future forever.\"]}),`\n`,(0,t.jsxs)(e.p,{children:[\"Option 2: Try to boil the ocean to find every single species of fish and identify them. Taking a step back, it is clear that we cannot hope to ever solve every use case of chown that is out there in the wild today, not to mention new ones that are being born every year. We were able to identify that most docker images use a specific UID and GID combination and the numbers of these are fairly limited. Examining two use cases in question, we found that postgresql images tended to use 999:999 and several others used 99 or 100, perhaps 1000 and 1001. This seemed like a promising lead to a solution because you can specify the \",(0,t.jsx)(e.a,{href:\"https://github.com/kubernetes-sigs/aws-efs-csi-driver#storage-class-parameters-for-dynamic-provisioning\",children:\"UID in the CSI provisioner\"}),\".\"]}),`\n`,(0,t.jsx)(e.p,{children:\"This elegant solution would result in creating several StorageClasses in Kubernetes, like say, \\u201Cpostgresql-999\\u201D, \\u201Crabbitmq-1001\\u201D, and so forth. Or maybe just \\u201Cefs-uid-999\\u201D to be more generic. Then we would teach each customer who enjoyed a failed build or deploy stack trace to change their settings to use the appropriate StorageClass. Even better, there are only about 2^16 possible unique UIDs in Linux, so we could programmatically create all of them in advance and apply them to our cluster to be stored in etcd, ready for retrieval whenever a customer wanted a UID-specific storage class. Or to limit choices in an opinionated but friendly way, we could require all containers to use a fixed UID, like 42, in order to use the storage volumes on our platform. If a customer wanted to use a different UID, like 43, we could charge $1 for every UID above and beyond the original one.\"}),`\n`,(0,t.jsx)(e.p,{children:\"If you did not detect any sarcasm in the preceding paragraph, you may want to call a crisis hotline to discuss obtaining a sense of humour. Amazon does not sell any upon last check; although you might find a used version on Etsy or eBay. I once ordered a sense of humour and it was stolen by a porch pirate before I could bring it in. Once I had obtained a suitable one, I would occasionally rent mine out on the joke version of Uber or Lyft, and sometimes you can even spend the night in my sense of humour on AirBNB, but due to abuse and lack of adequate tipping I have had to scale my activities down lately.\"}),`\n`,(0,t.jsx)(e.p,{children:\"Option 3: When in doubt, rollback to when it worked. We ultimately had to decide that we would be unable to support the new CSI driver until an adequate solution for dynamic deployments of EFS volumes was found for EKS. In the world of open source, there is always someone who comes up with a clever solution to a common problem and that becomes the de facto implementation recommendation. Currently, we were satisfied with the original functionality of the deprecated provisioner.\"}),`\n`,(0,t.jsx)(e.p,{children:\"But this raises another issue, how do we square using a deprecated and potentially unsupported solution on a platform our customers depend and rely upon? The answer is that we can make small adjustments and updates to the yaml and source code since the original solution code is still available and can be updated by Releasehub to support our customers.\"}),`\n`,(0,t.jsxs)(e.h3,{id:\"conclusion\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#conclusion\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Conclusion\"]}),`\n`,(0,t.jsx)(e.p,{children:\"Sometimes we must accept that we live in an imperfect world and accept the fact that we are as imperfect as the imperfect world we live in which means that we should accept the imperfection as the correct way that things should be and thus, the imperfection we see in the world merely reflects the imperfections in ourselves, which makes us perfect in every way.\"})]})}function k(o={}){let{wrapper:e}=o.components||{};return e?(0,t.jsx)(e,Object.assign({},o,{children:(0,t.jsx)(c,o)})):c(o)}var S=k;return b(I);})();\n;return Component;"
        },
        "_id": "blog/posts/how-to-solve-aws-efs-operation-not-permitted-errors-in-eks.mdx",
        "_raw": {
          "sourceFilePath": "blog/posts/how-to-solve-aws-efs-operation-not-permitted-errors-in-eks.mdx",
          "sourceFileName": "how-to-solve-aws-efs-operation-not-permitted-errors-in-eks.mdx",
          "sourceFileDir": "blog/posts",
          "contentType": "mdx",
          "flattenedPath": "blog/posts/how-to-solve-aws-efs-operation-not-permitted-errors-in-eks"
        },
        "type": "BlogPost",
        "computedSlug": "how-to-solve-aws-efs-operation-not-permitted-errors-in-eks"
      },
      "documentHash": "1739393595021",
      "hasWarnings": false,
      "documentTypeName": "BlogPost"
    },
    "blog/posts/how-to-use-github-actions-with-environment-variables.mdx": {
      "document": {
        "title": "How to Use GitHub Actions With Environment Variables",
        "summary": "We'll cover GitHub Actions & show you how to use it to automate your deployment workflow & save secrets",
        "publishDate": "Thu Aug 18 2022 17:07:54 GMT+0000 (Coordinated Universal Time)",
        "author": "ashley-penney",
        "readingTime": 8,
        "categories": [
          "platform-engineering",
          "product"
        ],
        "mainImage": "/blog-images/f1faa433780d4d232b9bb49a72ccb7bf.jpg",
        "imageAlt": "How to Use GitHub Actions With Environment Variables",
        "showCTA": true,
        "ctaCopy": "Learn how Release.com's ephemeral environments automate deployment workflows, simplifying setup of environment variables and enabling seamless collaboration.",
        "ctaLink": "https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=how-to-use-github-actions-with-environment-variables",
        "relatedPosts": [
          ""
        ],
        "ogImage": "/blog-images/f1faa433780d4d232b9bb49a72ccb7bf.jpg",
        "excerpt": "We'll cover GitHub Actions & show you how to use it to automate your deployment workflow & save secrets",
        "tags": [
          "platform-engineering",
          "product"
        ],
        "ctaButton": "Try Release for Free",
        "body": {
          "raw": "\nIn this post we'll cover GitHub Actions. We'll show you how to use it to automate your deployment workflow and how to use environment variables to save secrets.\n\nWe will deploy a simple Nest app using GitHub Actions. We'll start with a simple NestJS app. You don't need to know NestJS to get started, and you can use your own app if you wish. The starting point of the project can be taken from [here](https://drive.google.com/file/d/1CJFwKAXWL8lZ0ujf3YPpG5dpdOLQsHpF/view?usp=sharing).\n\n### GitHub Actions Environment Variables\n\nWe're going to store Docker secrets later in the post using secret variables from GitHub Actions. They can be easily found in the Settings tab and will help you during the automation process.\n\n### The Tasks\n\nLike in any production app, we'll create a feature branch from the main branch. After the changes are made, we'll create a pull request to the main branch.\n\nBefore merging, you need to ensure that this feature branch will not cause any breaking changes. For each pull request, you want to ensure that the build is successful and that all tests passed.\n\nIf you don’t use GitHub Actions, this will require a manual process. Suppose you’re happy with your pull request and want to merge it into the main branch. The next set of tasks is to build Docker image and push it into Docker hub. You can also automate these manual tasks with GitHub Actions.\n\n### The Setup\n\nYou need to extract the **Nest-App** from the link provided earlier in your local system and run **npm install** in it.\n\n![](/blog-images/61de738636b74a4216e630ff1024635f.jpeg)\n\nAfter that, create a new repository in GitHub and push your project.\n\n![](/blog-images/76ded41a7c03b2f5afb5abb7d123c616.jpeg)\n\nClick on the Actions tab. There you will find a lot of snippets for different projects.\n\n![](/blog-images/b9ec68025691256ab9cba2614a385b74.jpeg)\n\nSearch for **Node** and then click on the Configure button of a Node.js project.\n\n![](/blog-images/45d3290c126861a09d429bc2582bd944.jpeg)\n\nCopy all the code from this page, but don't save it. You are going to create it from VS Code in the next step.\n\n![](/blog-images/b82f17f22e01ffa6472fdc081c3495cc.jpeg)\n\nBack in your app, you need to create a **.github** folder and a **workflows** folder inside it. So inside it, create an **integrate.yml** file and paste the code from earlier. The .yml file basically says that on push or pull to the main branch, you need to run jobs. The first job name is **build**, and it runs-on an Ubuntu machine provided by GitHub. It also specifies the node version it will run.\n\nIn the steps below, you are using GitHub's Checkout Actions to check out into the build agent's directory. After that, you are setting up Node in that directory.\n\nLastly, the main command of **npm i** and **npm run build** are run.\n\n![](/blog-images/7aa0ea5d28b8b100aee5be0f64b6f28d.jpeg)\n\nYou will also create the workflow for your tests. Below, you are creating another job called **unit-tests** and doing the same setup again.\n\n![](/blog-images/98b73339a487e1bb24cb6bd6194ac428.jpeg)\n\n### The Failed Run\n\nNow create and push a remote branch that you'll use for your changes.\n\n![](/blog-images/fa1f72626c6431c4352d9f1b0ddec471.jpeg)\n\nBack in GitHub, you'll see a compare and pull request button that you need to click.\n\n![](/blog-images/dd9fcd4428c1e06c04a7ca4a3646b869.jpeg)\n\nIn the next screen, you just need to write a description and click the **Create pull request** button.\n\n![](/blog-images/98b160ae8870934a7cd483ca622baaab.jpeg)\n\nThe next screen says that a new branch got created called **new-actions**. Click on the **Actions** tab.\n\n![](/blog-images/7f19256ca772c9d9678d56bc8d3d0936.jpeg)\n\nOn the Actions page, you can see that your action failed. You need to click on the same to know the exact issue.\n\n![](/blog-images/63e3309115da67b74b5a6d779f9eab51.jpeg)\n\nThe next page shows that there is an issue in **unit-tests** in the **integrate.yml** file.\n\n![](/blog-images/a9829e0056013f757f67c1f13d22e684.jpeg)\n\n### The Successful Run\n\nUpon inspection, it's clear that there is no space in the unit tests. So, we're adding the space and again pushing the changes to new-actions.\n\n![](/blog-images/156ce8c10431fe1426a009fab5432af5.jpeg)\n\nBack in GitHub, you can see that a new action has been running.\n\n![](/blog-images/bb891d268168446141db309c475e2881.jpeg)\n\nThis time the build completed successfully.\n\nAlso notice that the build and unit-tests jobs were run on three versions of Node.js.\n\n![](/blog-images/26b26e641ffb845367e548b35dc7ba73.jpeg)\n\nNow you can Merge this pull request and will get a confirmation.\n\n![](/blog-images/292e6e4263ee046fa40b786a031f6e5e.jpeg)\n\n### The Docker Setup\n\nYou'll need to go to [https://hub.docker.com/](https://hub.docker.com/) and register if you don't have an account.\n\n![](/blog-images/6b410e32beebed28daf6bea9d6490921.jpeg)\n\nOnce you're logged in, click on the **Create Repository** button.\n\n![](/blog-images/a99532a999716f3bde75b620aa354502.jpeg)\n\nOn the next page, you need to give the app a name and a description. Then click the **Create** button.\n\n![](/blog-images/85961c4d417b3e66423920d66cde69f9.jpeg)\n\nOn the next page, you should see the success message. Then click on the user profile in the top-right corner and then click the **Account Settings** link.\n\n![](/blog-images/8ac4ab239e5e298f18683a64a93ba1c4.jpeg)\n\nOn the next screen, click on the **Security** tab and then the **New Access Token** button.\n\n![](/blog-images/ade3e8ac580bfad73508eae59e679b29.jpeg)\n\nA pop-up will open, and you need to give a description and permissions. After that, click the **Generate** button.\n\n![](/blog-images/52a843287e5ae1987ee99e9a7033ceed.jpeg)\n\nOn the next screen, you will get your username and the token. You'll need to copy those.\n\n![](/blog-images/d4bec72be3d4b04e38779e69e396406d.jpeg)\n\n### Environment Variables in GitHub\n\nYou cannot expose the Docker username and password, so you'll save them in GitHub environment variables.\n\nIn the GitHub repository, click **Settings**, **Secrets**, then **Actions**. You should see a button for **New repository secret**, and you'll need to click on it.\n\n![](/blog-images/6353eefbb121932e8ab2a8453666d341.jpeg)\n\nOn the next page, you will give the new secret a name of **DOCKER_PASSWORD**. Add the access token from the earlier section. After that, click on the **Add secret** button.\n\n![](/blog-images/6584dab387bacd7a8884bbc995b3ac87.jpeg)\n\nYou'll also create another variable of DOCKER_USERNAME and store the username here.\n\n![](/blog-images/7754383385a826feb41943a24da64a52.jpeg)\n\nNow you should see two Repository secrets in the Actions tab.\n\n![](/blog-images/a2bf3b669a7c118fd697a9211ad7daff.jpeg)\n\n### The Release Flow\n\nNext, you'll create the Docker release flow. Go to the main branch. Then get the updates and create a new branch of **release-flow**.\n\n![](/blog-images/36da4467aba4283357fc61ddfed95f5a.jpeg)\n\nNext, create a file **release.yml** in the **workflows** folder. Place the code below in it. Here, you're running the job **deploy** only when merging to the main branch. You just need to check out to the branch. After that, you will be running the Docker build command.\n\nNext, you'll use the username and password to log in to Docker using the secrets stored in GitHub in the previous section.\n\nLastly, you're running the push command to push to Docker.\n\n![](/blog-images/832271f36ff927eb9bbb671f38e243b5.jpeg)\n\n### The Docker Run\n\nFirst, you'll add these changes and then commit them to your new **release-flow** branch. After that, you'll push it to GitHub.\n\n![](/blog-images/d5590fff294bb7b3d89fba8b444062dd.jpeg)\n\nBack in GitHub, you should see a new **Compare & pull request** on the home page of your app.\n\nClick on it.\n\n![](/blog-images/1b55c04f275301aba88419232e4fd989.jpeg)\n\nOn the next page, you need to provide some descriptions and click the **Create pull request** button.\n\n![](/blog-images/329aa27963728a7dbdfdb7c519fcc060.jpeg)\n\nSince you've created a new branch, the integrate flow, the build, and the unit test jobs should run successfully.\n\n![](/blog-images/8405ed8cda6741f5000d54a8bf55d9d3.jpeg)\n\nNow you can merge your pull request in the main branch by clicking the **Merge pull request** button.\n\n![](/blog-images/73e3af43c41f41a1f3f441e33533148a.jpeg)\n\nYou will be asked to confirm this merge on the next page. Just click the **Confirm merge** button.\n\n![](/blog-images/d7f2b2cab41cae10bb97ba925b9fbb9f.jpeg)\n\nYour new actions of release-flow should run now, and you can see it in the Actions tab.\n\n![](/blog-images/29cf0c74c3a35e4b8cffbaaf1ff67d6c.jpeg)\n\nAfter clicking any of them, you should find that your Docker commands also ran.\n\n![](/blog-images/1d6be7566e7a5977a38bcdfb7c3d15f8.jpeg)\n\nAfter two or three minutes, your workflows should complete successfully.\n\n![](/blog-images/94954f42d12405ba1b7f217fbc0e8ee8.jpeg)\n\nYou can also confirm that the new Docker image was added in Docker hub.\n\n![](/blog-images/257d71668c191a3e7682cbf079802232.jpeg)\n\n### Conclusion\n\nIn this post, you pushed a simple Nest app to GitHub. Then you created two automated workflows on GitHub through GitHub Actions.\n\nFirst, you created an integrate workflow that ran when a pull request was made to main branch from a feature branch. It ran the build and test jobs. Next, you created integrate workflow for your Docker workflow. It ran after a push was made to the main branch. The job created a Docker build and a Docker image in Docker hub.\n\nDid you know you can easily spin up an environment on release directly from your docker-compose file? Give it a [shot](https://release.com/).\n",
          "code": "var Component=(()=>{var d=Object.create;var o=Object.defineProperty;var u=Object.getOwnPropertyDescriptor;var g=Object.getOwnPropertyNames;var p=Object.getPrototypeOf,b=Object.prototype.hasOwnProperty;var m=(t,e)=>()=>(e||t((e={exports:{}}).exports,e),e.exports),f=(t,e)=>{for(var a in e)o(t,a,{get:e[a],enumerable:!0})},c=(t,e,a,r)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let i of g(e))!b.call(t,i)&&i!==a&&o(t,i,{get:()=>e[i],enumerable:!(r=u(e,i))||r.enumerable});return t};var y=(t,e,a)=>(a=t!=null?d(p(t)):{},c(e||!t||!t.__esModule?o(a,\"default\",{value:t,enumerable:!0}):a,t)),w=t=>c(o({},\"__esModule\",{value:!0}),t);var l=m((x,s)=>{s.exports=_jsx_runtime});var A={};f(A,{default:()=>v,frontmatter:()=>k});var n=y(l()),k={title:\"How to Use GitHub Actions With Environment Variables\",summary:\"We'll cover GitHub Actions & show you how to use it to automate your deployment workflow & save secrets\",publishDate:\"Thu Aug 18 2022 17:07:54 GMT+0000 (Coordinated Universal Time)\",author:\"ashley-penney\",readingTime:8,categories:[\"platform-engineering\",\"product\"],mainImage:\"/blog-images/f1faa433780d4d232b9bb49a72ccb7bf.jpg\",imageAlt:\"How to Use GitHub Actions With Environment Variables\",showCTA:!0,ctaCopy:\"Learn how Release.com's ephemeral environments automate deployment workflows, simplifying setup of environment variables and enabling seamless collaboration.\",ctaLink:\"https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=how-to-use-github-actions-with-environment-variables\",relatedPosts:[\"\"],ogImage:\"/blog-images/f1faa433780d4d232b9bb49a72ccb7bf.jpg\",excerpt:\"We'll cover GitHub Actions & show you how to use it to automate your deployment workflow & save secrets\",tags:[\"platform-engineering\",\"product\"],ctaButton:\"Try Release for Free\"};function h(t){let e=Object.assign({p:\"p\",a:\"a\",h3:\"h3\",span:\"span\",strong:\"strong\",img:\"img\"},t.components);return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(e.p,{children:\"In this post we'll cover GitHub Actions. We'll show you how to use it to automate your deployment workflow and how to use environment variables to save secrets.\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"We will deploy a simple Nest app using GitHub Actions. We'll start with a simple NestJS app. You don't need to know NestJS to get started, and you can use your own app if you wish. The starting point of the project can be taken from \",(0,n.jsx)(e.a,{href:\"https://drive.google.com/file/d/1CJFwKAXWL8lZ0ujf3YPpG5dpdOLQsHpF/view?usp=sharing\",children:\"here\"}),\".\"]}),`\n`,(0,n.jsxs)(e.h3,{id:\"github-actions-environment-variables\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#github-actions-environment-variables\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"GitHub Actions Environment Variables\"]}),`\n`,(0,n.jsx)(e.p,{children:\"We're going to store Docker secrets later in the post using secret variables from GitHub Actions. They can be easily found in the Settings tab and will help you during the automation process.\"}),`\n`,(0,n.jsxs)(e.h3,{id:\"the-tasks\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#the-tasks\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"The Tasks\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Like in any production app, we'll create a feature branch from the main branch. After the changes are made, we'll create a pull request to the main branch.\"}),`\n`,(0,n.jsx)(e.p,{children:\"Before merging, you need to ensure that this feature branch will not cause any breaking changes. For each pull request, you want to ensure that the build is successful and that all tests passed.\"}),`\n`,(0,n.jsx)(e.p,{children:\"If you don\\u2019t use GitHub Actions, this will require a manual process. Suppose you\\u2019re happy with your pull request and want to merge it into the main branch. The next set of tasks is to build Docker image and push it into Docker hub. You can also automate these manual tasks with GitHub Actions.\"}),`\n`,(0,n.jsxs)(e.h3,{id:\"the-setup\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#the-setup\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"The Setup\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"You need to extract the \",(0,n.jsx)(e.strong,{children:\"Nest-App\"}),\" from the link provided earlier in your local system and run \",(0,n.jsx)(e.strong,{children:\"npm install\"}),\" in it.\"]}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/61de738636b74a4216e630ff1024635f.jpeg\",alt:\"\"})}),`\n`,(0,n.jsx)(e.p,{children:\"After that, create a new repository in GitHub and push your project.\"}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/76ded41a7c03b2f5afb5abb7d123c616.jpeg\",alt:\"\"})}),`\n`,(0,n.jsx)(e.p,{children:\"Click on the Actions tab. There you will find a lot of snippets for different projects.\"}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/b9ec68025691256ab9cba2614a385b74.jpeg\",alt:\"\"})}),`\n`,(0,n.jsxs)(e.p,{children:[\"Search for \",(0,n.jsx)(e.strong,{children:\"Node\"}),\" and then click on the Configure button of a Node.js project.\"]}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/45d3290c126861a09d429bc2582bd944.jpeg\",alt:\"\"})}),`\n`,(0,n.jsx)(e.p,{children:\"Copy all the code from this page, but don't save it. You are going to create it from VS Code in the next step.\"}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/b82f17f22e01ffa6472fdc081c3495cc.jpeg\",alt:\"\"})}),`\n`,(0,n.jsxs)(e.p,{children:[\"Back in your app, you need to create a \",(0,n.jsx)(e.strong,{children:\".github\"}),\" folder and a \",(0,n.jsx)(e.strong,{children:\"workflows\"}),\" folder inside it. So inside it, create an \",(0,n.jsx)(e.strong,{children:\"integrate.yml\"}),\" file and paste the code from earlier. The .yml file basically says that on push or pull to the main branch, you need to run jobs. The first job name is \",(0,n.jsx)(e.strong,{children:\"build\"}),\", and it runs-on an Ubuntu machine provided by GitHub. It also specifies the node version it will run.\"]}),`\n`,(0,n.jsx)(e.p,{children:\"In the steps below, you are using GitHub's Checkout Actions to check out into the build agent's directory. After that, you are setting up Node in that directory.\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"Lastly, the main command of \",(0,n.jsx)(e.strong,{children:\"npm i\"}),\" and \",(0,n.jsx)(e.strong,{children:\"npm run build\"}),\" are run.\"]}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/7aa0ea5d28b8b100aee5be0f64b6f28d.jpeg\",alt:\"\"})}),`\n`,(0,n.jsxs)(e.p,{children:[\"You will also create the workflow for your tests. Below, you are creating another job called \",(0,n.jsx)(e.strong,{children:\"unit-tests\"}),\" and doing the same setup again.\"]}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/98b73339a487e1bb24cb6bd6194ac428.jpeg\",alt:\"\"})}),`\n`,(0,n.jsxs)(e.h3,{id:\"the-failed-run\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#the-failed-run\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"The Failed Run\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Now create and push a remote branch that you'll use for your changes.\"}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/fa1f72626c6431c4352d9f1b0ddec471.jpeg\",alt:\"\"})}),`\n`,(0,n.jsx)(e.p,{children:\"Back in GitHub, you'll see a compare and pull request button that you need to click.\"}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/dd9fcd4428c1e06c04a7ca4a3646b869.jpeg\",alt:\"\"})}),`\n`,(0,n.jsxs)(e.p,{children:[\"In the next screen, you just need to write a description and click the \",(0,n.jsx)(e.strong,{children:\"Create pull request\"}),\" button.\"]}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/98b160ae8870934a7cd483ca622baaab.jpeg\",alt:\"\"})}),`\n`,(0,n.jsxs)(e.p,{children:[\"The next screen says that a new branch got created called \",(0,n.jsx)(e.strong,{children:\"new-actions\"}),\". Click on the \",(0,n.jsx)(e.strong,{children:\"Actions\"}),\" tab.\"]}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/7f19256ca772c9d9678d56bc8d3d0936.jpeg\",alt:\"\"})}),`\n`,(0,n.jsx)(e.p,{children:\"On the Actions page, you can see that your action failed. You need to click on the same to know the exact issue.\"}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/63e3309115da67b74b5a6d779f9eab51.jpeg\",alt:\"\"})}),`\n`,(0,n.jsxs)(e.p,{children:[\"The next page shows that there is an issue in \",(0,n.jsx)(e.strong,{children:\"unit-tests\"}),\" in the \",(0,n.jsx)(e.strong,{children:\"integrate.yml\"}),\" file.\"]}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/a9829e0056013f757f67c1f13d22e684.jpeg\",alt:\"\"})}),`\n`,(0,n.jsxs)(e.h3,{id:\"the-successful-run\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#the-successful-run\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"The Successful Run\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Upon inspection, it's clear that there is no space in the unit tests. So, we're adding the space and again pushing the changes to new-actions.\"}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/156ce8c10431fe1426a009fab5432af5.jpeg\",alt:\"\"})}),`\n`,(0,n.jsx)(e.p,{children:\"Back in GitHub, you can see that a new action has been running.\"}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/bb891d268168446141db309c475e2881.jpeg\",alt:\"\"})}),`\n`,(0,n.jsx)(e.p,{children:\"This time the build completed successfully.\"}),`\n`,(0,n.jsx)(e.p,{children:\"Also notice that the build and unit-tests jobs were run on three versions of Node.js.\"}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/26b26e641ffb845367e548b35dc7ba73.jpeg\",alt:\"\"})}),`\n`,(0,n.jsx)(e.p,{children:\"Now you can Merge this pull request and will get a confirmation.\"}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/292e6e4263ee046fa40b786a031f6e5e.jpeg\",alt:\"\"})}),`\n`,(0,n.jsxs)(e.h3,{id:\"the-docker-setup\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#the-docker-setup\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"The Docker Setup\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"You'll need to go to \",(0,n.jsx)(e.a,{href:\"https://hub.docker.com/\",children:\"https://hub.docker.com/\"}),\" and register if you don't have an account.\"]}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/6b410e32beebed28daf6bea9d6490921.jpeg\",alt:\"\"})}),`\n`,(0,n.jsxs)(e.p,{children:[\"Once you're logged in, click on the \",(0,n.jsx)(e.strong,{children:\"Create Repository\"}),\" button.\"]}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/a99532a999716f3bde75b620aa354502.jpeg\",alt:\"\"})}),`\n`,(0,n.jsxs)(e.p,{children:[\"On the next page, you need to give the app a name and a description. Then click the \",(0,n.jsx)(e.strong,{children:\"Create\"}),\" button.\"]}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/85961c4d417b3e66423920d66cde69f9.jpeg\",alt:\"\"})}),`\n`,(0,n.jsxs)(e.p,{children:[\"On the next page, you should see the success message. Then click on the user profile in the top-right corner and then click the \",(0,n.jsx)(e.strong,{children:\"Account Settings\"}),\" link.\"]}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/8ac4ab239e5e298f18683a64a93ba1c4.jpeg\",alt:\"\"})}),`\n`,(0,n.jsxs)(e.p,{children:[\"On the next screen, click on the \",(0,n.jsx)(e.strong,{children:\"Security\"}),\" tab and then the \",(0,n.jsx)(e.strong,{children:\"New Access Token\"}),\" button.\"]}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/ade3e8ac580bfad73508eae59e679b29.jpeg\",alt:\"\"})}),`\n`,(0,n.jsxs)(e.p,{children:[\"A pop-up will open, and you need to give a description and permissions. After that, click the \",(0,n.jsx)(e.strong,{children:\"Generate\"}),\" button.\"]}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/52a843287e5ae1987ee99e9a7033ceed.jpeg\",alt:\"\"})}),`\n`,(0,n.jsx)(e.p,{children:\"On the next screen, you will get your username and the token. You'll need to copy those.\"}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/d4bec72be3d4b04e38779e69e396406d.jpeg\",alt:\"\"})}),`\n`,(0,n.jsxs)(e.h3,{id:\"environment-variables-in-github\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#environment-variables-in-github\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Environment Variables in GitHub\"]}),`\n`,(0,n.jsx)(e.p,{children:\"You cannot expose the Docker username and password, so you'll save them in GitHub environment variables.\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"In the GitHub repository, click \",(0,n.jsx)(e.strong,{children:\"Settings\"}),\", \",(0,n.jsx)(e.strong,{children:\"Secrets\"}),\", then \",(0,n.jsx)(e.strong,{children:\"Actions\"}),\". You should see a button for \",(0,n.jsx)(e.strong,{children:\"New repository secret\"}),\", and you'll need to click on it.\"]}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/6353eefbb121932e8ab2a8453666d341.jpeg\",alt:\"\"})}),`\n`,(0,n.jsxs)(e.p,{children:[\"On the next page, you will give the new secret a name of \",(0,n.jsx)(e.strong,{children:\"DOCKER_PASSWORD\"}),\". Add the access token from the earlier section. After that, click on the \",(0,n.jsx)(e.strong,{children:\"Add secret\"}),\" button.\"]}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/6584dab387bacd7a8884bbc995b3ac87.jpeg\",alt:\"\"})}),`\n`,(0,n.jsx)(e.p,{children:\"You'll also create another variable of DOCKER_USERNAME and store the username here.\"}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/7754383385a826feb41943a24da64a52.jpeg\",alt:\"\"})}),`\n`,(0,n.jsx)(e.p,{children:\"Now you should see two Repository secrets in the Actions tab.\"}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/a2bf3b669a7c118fd697a9211ad7daff.jpeg\",alt:\"\"})}),`\n`,(0,n.jsxs)(e.h3,{id:\"the-release-flow\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#the-release-flow\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"The Release Flow\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"Next, you'll create the Docker release flow. Go to the main branch. Then get the updates and create a new branch of \",(0,n.jsx)(e.strong,{children:\"release-flow\"}),\".\"]}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/36da4467aba4283357fc61ddfed95f5a.jpeg\",alt:\"\"})}),`\n`,(0,n.jsxs)(e.p,{children:[\"Next, create a file \",(0,n.jsx)(e.strong,{children:\"release.yml\"}),\" in the \",(0,n.jsx)(e.strong,{children:\"workflows\"}),\" folder. Place the code below in it. Here, you're running the job \",(0,n.jsx)(e.strong,{children:\"deploy\"}),\" only when merging to the main branch. You just need to check out to the branch. After that, you will be running the Docker build command.\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Next, you'll use the username and password to log in to Docker using the secrets stored in GitHub in the previous section.\"}),`\n`,(0,n.jsx)(e.p,{children:\"Lastly, you're running the push command to push to Docker.\"}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/832271f36ff927eb9bbb671f38e243b5.jpeg\",alt:\"\"})}),`\n`,(0,n.jsxs)(e.h3,{id:\"the-docker-run\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#the-docker-run\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"The Docker Run\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"First, you'll add these changes and then commit them to your new \",(0,n.jsx)(e.strong,{children:\"release-flow\"}),\" branch. After that, you'll push it to GitHub.\"]}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/d5590fff294bb7b3d89fba8b444062dd.jpeg\",alt:\"\"})}),`\n`,(0,n.jsxs)(e.p,{children:[\"Back in GitHub, you should see a new \",(0,n.jsx)(e.strong,{children:\"Compare & pull request\"}),\" on the home page of your app.\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Click on it.\"}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/1b55c04f275301aba88419232e4fd989.jpeg\",alt:\"\"})}),`\n`,(0,n.jsxs)(e.p,{children:[\"On the next page, you need to provide some descriptions and click the \",(0,n.jsx)(e.strong,{children:\"Create pull request\"}),\" button.\"]}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/329aa27963728a7dbdfdb7c519fcc060.jpeg\",alt:\"\"})}),`\n`,(0,n.jsx)(e.p,{children:\"Since you've created a new branch, the integrate flow, the build, and the unit test jobs should run successfully.\"}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/8405ed8cda6741f5000d54a8bf55d9d3.jpeg\",alt:\"\"})}),`\n`,(0,n.jsxs)(e.p,{children:[\"Now you can merge your pull request in the main branch by clicking the \",(0,n.jsx)(e.strong,{children:\"Merge pull request\"}),\" button.\"]}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/73e3af43c41f41a1f3f441e33533148a.jpeg\",alt:\"\"})}),`\n`,(0,n.jsxs)(e.p,{children:[\"You will be asked to confirm this merge on the next page. Just click the \",(0,n.jsx)(e.strong,{children:\"Confirm merge\"}),\" button.\"]}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/d7f2b2cab41cae10bb97ba925b9fbb9f.jpeg\",alt:\"\"})}),`\n`,(0,n.jsx)(e.p,{children:\"Your new actions of release-flow should run now, and you can see it in the Actions tab.\"}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/29cf0c74c3a35e4b8cffbaaf1ff67d6c.jpeg\",alt:\"\"})}),`\n`,(0,n.jsx)(e.p,{children:\"After clicking any of them, you should find that your Docker commands also ran.\"}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/1d6be7566e7a5977a38bcdfb7c3d15f8.jpeg\",alt:\"\"})}),`\n`,(0,n.jsx)(e.p,{children:\"After two or three minutes, your workflows should complete successfully.\"}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/94954f42d12405ba1b7f217fbc0e8ee8.jpeg\",alt:\"\"})}),`\n`,(0,n.jsx)(e.p,{children:\"You can also confirm that the new Docker image was added in Docker hub.\"}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/257d71668c191a3e7682cbf079802232.jpeg\",alt:\"\"})}),`\n`,(0,n.jsxs)(e.h3,{id:\"conclusion\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#conclusion\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Conclusion\"]}),`\n`,(0,n.jsx)(e.p,{children:\"In this post, you pushed a simple Nest app to GitHub. Then you created two automated workflows on GitHub through GitHub Actions.\"}),`\n`,(0,n.jsx)(e.p,{children:\"First, you created an integrate workflow that ran when a pull request was made to main branch from a feature branch. It ran the build and test jobs. Next, you created integrate workflow for your Docker workflow. It ran after a push was made to the main branch. The job created a Docker build and a Docker image in Docker hub.\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"Did you know you can easily spin up an environment on release directly from your docker-compose file? Give it a \",(0,n.jsx)(e.a,{href:\"https://release.com/\",children:\"shot\"}),\".\"]})]})}function j(t={}){let{wrapper:e}=t.components||{};return e?(0,n.jsx)(e,Object.assign({},t,{children:(0,n.jsx)(h,t)})):h(t)}var v=j;return w(A);})();\n;return Component;"
        },
        "_id": "blog/posts/how-to-use-github-actions-with-environment-variables.mdx",
        "_raw": {
          "sourceFilePath": "blog/posts/how-to-use-github-actions-with-environment-variables.mdx",
          "sourceFileName": "how-to-use-github-actions-with-environment-variables.mdx",
          "sourceFileDir": "blog/posts",
          "contentType": "mdx",
          "flattenedPath": "blog/posts/how-to-use-github-actions-with-environment-variables"
        },
        "type": "BlogPost",
        "computedSlug": "how-to-use-github-actions-with-environment-variables"
      },
      "documentHash": "1739393595021",
      "hasWarnings": false,
      "documentTypeName": "BlogPost"
    },
    "blog/posts/how-to-use-the-kubernetes-rbac-api-for-more-secure-apps.mdx": {
      "document": {
        "title": "How to Use the Kubernetes RBAC API for More Secure Apps",
        "summary": "RBAC API gives permissions to an object in a Kubernetes cluster to improve governance & strengthen secuity.",
        "publishDate": "Fri Sep 30 2022 18:55:19 GMT+0000 (Coordinated Universal Time)",
        "author": "tommy-mcclung",
        "readingTime": 4,
        "categories": [
          "kubernetes",
          "platform-engineering"
        ],
        "mainImage": "/blog-images/514418c7a457bea6c774f627b909a699.jpg",
        "imageAlt": "Close-up of people shaking hands",
        "showCTA": true,
        "ctaCopy": "Unlock secure, on-demand Kubernetes environments with RBAC controls for streamlined app development and testing.",
        "ctaLink": "https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=how-to-use-the-kubernetes-rbac-api-for-more-secure-apps",
        "relatedPosts": [
          ""
        ],
        "ogImage": "/blog-images/514418c7a457bea6c774f627b909a699.jpg",
        "excerpt": "RBAC API gives permissions to an object in a Kubernetes cluster to improve governance & strengthen secuity.",
        "tags": [
          "kubernetes",
          "platform-engineering"
        ],
        "ctaButton": "Try Release for Free",
        "body": {
          "raw": "\nIf you manage a Kubernetes cluster, sooner or later, you'll need to assign roles and permissions to users so that everyone has visibility and access only to the resources they need. For example, some users must have read-only access, while others must only be able to write to certain resources or APIs. In addition, there must be users with unlimited access to all available resources. \n\nSuppose you have a team of four users: \n\n- Patricia: leader of the dev team\n- James: part of the dev team\n- Oliver: leader of the QA team\n- Amelia: part of the QA team\n\nJames works on one of the many projects within the company's cluster and therefore needs access to your namespace. Patricia, being the dev leader, needs full access to all projects. Oliver, likewise, needs full read access to all objects in the cluster. However, Amelia is working on the same project as James and can only have read access in this namespace. \n\nRole-based access control (RBAC) is an authorization mechanism built to handle such cases. In this article, we will discuss what RBAC API is and how you can use Kubernetes RBAC API to develop secure applications. \n\n![Close-up of people shaking handsDescription automatically generated](/blog-images/c86b1152882a95adcd1ae1ebf5677e4d.jpeg)\n\n### What is RBAC?\n\nThe concept of role-based access control is not something new. RBAC is based on the concepts of roles, permissions, and user groups, and it's one of the most widespread access control models being used in organizations today.  \n\nIn organizational use, RBAC allows you to create secure access models based on the real functions that people have within the organization rather than on the actions they must be able to perform. \n\n### What is the RBAC API, and how is it used in Kubernetes?\n\nKubernetes (as of version 1.6) introduced the concept of role-based access control as a system for distributing access rights to various objects in a Kubernetes cluster. \n\nObjects in a Kubernetes cluster are YAML manifests, and permissions determine which user can only view the manifests and who can create, modify, or even delete them. \n\nBefore we go into how RBAC works in Kubernetes, it's important to understand what a user is in Kubernetes. Everyone who sends requests to the API server is a user in a Kubernetes cluster. This means that not only administrators and developers are users, but also various CI/CD scripts and control plane, **kubelet**, and **kube-proxy** components on nodes are considered users.  \n\nThe RBAC model includes five entities:  \n\n- Role\n- RoleBinding\n- ClusterRole\n- ClusterRoleBinding\n- ServiceAccount\n\nLet's explore each entity in more detail. **‍**\n\n### Role\n\nThe **role** is a YAML manifest that describes a set of rights on Kubernetes cluster objects. \n\nHere it's important to understand that cluster objects are YAML manifests stored in **etcd**. The API server checks all rights as they relate to the requests that the API server receives.  \n\nIf you restrict the user to execute **kubectl exec**, but that user has access to the worker node, then they will not be able to block it from entering the worker node and doing docker exec in the RBAC container. \n\nWe can go to the cluster, and in **ns ingress-nginx**, look at the **role: ingress-nginx**: \n\n```yaml\nkubectl get role -n ingress-nginx ingress-nginx -o yaml\n```\n\nHere we are interested in the rules section. This is a list of rules that describe access rights. \n\nIn each rule, we have three parameters. Let's look at an example: \n\n```yaml\n\napiGroups:\n  - extensions\n  - networking.k8s.io\n resources:\n  - ingresses\n verbs:\n  - get\n  - list\n  - watch\n\n```\n\nHere **apiGroups:** describes the manifest API group. If only the version is specified in apiVersion—without a group, for example, as in the Pod manifest—then this manifest is considered to have the so-called root group (core-group). In the role, the root group is specified as an empty string. \n\nThe **resources:** parameter refers to a list of resources to which we describe access. You can view the list of resources in your cluster with the command **kubectl api-resources**. Some sub-resources describe specific actions. For example, the **pods/log** sub-resource allows you to view container logs in a pod.  \n\nThe **verbs:** parameter is a list of actions that you can perform on the resources described above: get, view the list, monitor changes, edit, delete, etc. \n\n### RoleBinding\n\nLet's now look at the **RoleBinding** manifest: \n\n```yaml\nkubectl get rolebinding ingress-nginx -n ingress-nginx -o yaml\n```\n\nIt has two types of fields: **roleRef** and **subjects:** \n\n```yaml\n\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: Role\n  name: ingress-nginx\nsubjects:\n- kind: ServiceAccount\n  name: ingress-nginx\n  namespace: ingress-nginx\n- kind: User\n  name: jane              # \"name\" is case sensitive\n  apiGroup: rbac.authorization.k8s.io\n- kind: Group\n  name: developer      \n  apiGroup: rbac.authorization.k8s.io\n\n```\n\nHere's what we have: \n\n- **roleRef** specifies the role.\n- **subjects** specifies who will be assigned this role.\n- **kind** specifies permissions for requests not authenticated through a token from the service account.\n\n### ClusterRole\n\nThe **role** entity is namespace dependent and we can create roles with the same name in different namespaces. While **ClusterRole** is a cluster object, this entity describes the rights to objects in the entire cluster. \n\nKubernetes has many preconfigured cluster roles. These include the **admin**, **edit**, and **view** roles, which describe the rights that allow administrating, editing, or only viewing entities. If you have administrator rights, you can view the role in your cluster with the following command: \n\n```yaml\nkubectl get clusterrole edit -o yaml\n```\n\n### ClusterRoleBinding\n\nRoleBinding only gives access to entities in the same namespace as the RoleBinding manifest. **ClusterRoleBinding** allows you to grant access to entities in all cluster namespaces simultaneously. \n\n### Service Account\n\nKubernetes knows nothing about users in the form we are used to seeing them in when it comes to other access restriction systems, where users have a login or a password. Still, it has mechanisms for calling external password verification services, such as **oidc**, a user certificate verification option, or even the usual HTTP basic auth with the classic Apache file **htpasswd**. \n\n‍**ServiceAccount** was created primarily to limit the rights of applications that run in a cluster. All communication between cluster components goes through requests to the API server, and a special JWT token just authorizes each such request. This token is automatically generated when an object of the **ServiceAccount** type is created and placed in [secret.](https://release.com/blog/kubernetes-secrets-management-a-practical-guide)  \n\n### How to use RBAC API in Kubernetes\n\n[RBAC authorization](https://kubernetes.io/docs/reference/access-authn-authz/rbac/) is one way to assign roles to users in a Kubernetes cluster. Here are the steps: \n\n1\\. Connect the [service account token](https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/). Without this token, you will need to re-download **kubeconfig** after any change in roles. \n\n2\\. Assign roles. Here's an example manifest that creates two namespaces and two users, each of which will only be able to [manage pods](https://release.com/blog/kubernetes-pod-a-beginners-guide-to-an-essential-resource) in their own namespace: \n\n```yaml\n\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: test-one\n---\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: test-two\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: read-pods-one\n  namespace: test-one\nsubjects:\n- kind: ServiceAccount\n  name: test-sa-one\n  apiGroup: \"\"\nroleRef:\n  kind: Role\n  name: pod-reader-one\n  apiGroup: rbac.authorization.k8s.io\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: read-pods-two\n  namespace: test-two\nsubjects:\n- kind: ServiceAccount\n  name: test-sa-two\n  apiGroup: \"\"\nroleRef:\n  kind: Role\n  name: pod-reader-two\n  apiGroup: rbac.authorization.k8s.io\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  namespace: test-one\n  name: pod-reader-one\nrules:\n- apiGroups: [\"\"] # \"\" indicates the core API group\n  resources: [\"pods\"]\n  verbs: [\"get\", \"watch\", \"list\"]\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  namespace: test-two\n  name: pod-reader-two\nrules:\n- apiGroups: [\"\"] # \"\" indicates the core API group\n  resources: [\"pods\"]\n  verbs: [\"get\", \"watch\", \"list\"]\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  namespace: test-one\n  name: test-sa-one\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  namespace: test-two\n  name: test-sa-twoRun the manifest.\n\n```\n\n3\\. Run the manifest.\n\n4\\. Create tokens: \n\n```yaml\nkubectl get secret $(kubectl get serviceaccount test-sa-one -o jsonpath='{.secrets[0].name}' --namespace test-one) -o jsonpath='{.data.token}' --namespace test-one | base64 -d\n\nkubectl get secret $(kubectl get serviceaccount test-sa-two -o jsonpath='{.secrets[0].name}' --namespace test-two) -o jsonpath='{.data.token}' --namespace test-two | base64 -d\n```\n\n5\\. Manually add tokens to users in the **kubeconfig.yaml** file for authorization without a password: \n\n```yaml\n\nusers:\n...\n- name: test-sa-one\nuser:\n  token:\n- name: test-sa-two\nuser:\n  token:\n...\n\n```\n\n6\\. Check the distribution of roles: \n\n```yaml\n\nkubectl config set-context --current --user=test-sa-two\nContext \"admin@kubernetes\" modified.\n\nkubectl get pods --namespace test-two\nNo resources found in test-two namespace.\n\nkubectl get pods --namespace test-one\nError from server (Forbidden): pods is forbidden: User \"system:serviceaccount:test-two:test-sa-two\" cannot list resource \"pods\" in API group \"\" in the namespace \"test-one\"\n________\n\nkubectl config set-context --current --user=test-sa-one\nContext \"admin@kubernetes\" modified.\n\nkubectl get pods --namespace test-two\nError from server (Forbidden): pods is forbidden: User \"system:serviceaccount:test-one:test-sa-one\" cannot list resource \"pods\" in API group \"\" in the namespace \"test-two\"\n\nkubectl get pods --namespace test-one\nNo resources found in test-one namespace.\n\n```\n\nThe test-sa-two user now has access to pods in the **test-two** namespace and no access to the **test-one** namespace. Similarly, the \"test-sa-one\" user has access to pods in the **test-one** namespace but not those in the **test-two** namespace.\n\n### Kubernetes RBAC as a Security Strategy\n\nThe RBAC model has proved particularly effective for managing roles and is now considered a Kubernetes security best practice.  \n\nRBAC reduces the risk of unwanted access to critical resources and allows you to implement the principle of least privilege by giving access to only needed resources, which makes your cluster more secure.  \n\nMoreover, if you are using RBAC, you don't have to check the individual permissions assigned to each user. You can monitor who has access to resources simply by consulting roles, which makes auditing easier. \n\n### Conclusion\n\nIn summary, RBAC API is a role-based approach to giving permissions to an object in a Kubernetes cluster. By making it part of the Kubernetes pipeline, you can improve governance and significantly strengthen security.\n",
          "code": "var Component=(()=>{var d=Object.create;var i=Object.defineProperty;var u=Object.getOwnPropertyDescriptor;var p=Object.getOwnPropertyNames;var m=Object.getPrototypeOf,g=Object.prototype.hasOwnProperty;var b=(s,e)=>()=>(e||s((e={exports:{}}).exports,e),e.exports),f=(s,e)=>{for(var t in e)i(s,t,{get:e[t],enumerable:!0})},o=(s,e,t,a)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let r of p(e))!g.call(s,r)&&r!==t&&i(s,r,{get:()=>e[r],enumerable:!(a=u(e,r))||a.enumerable});return s};var w=(s,e,t)=>(t=s!=null?d(m(s)):{},o(e||!s||!s.__esModule?i(t,\"default\",{value:s,enumerable:!0}):t,s)),k=s=>o(i({},\"__esModule\",{value:!0}),s);var l=b((j,c)=>{c.exports=_jsx_runtime});var R={};f(R,{default:()=>A,frontmatter:()=>v});var n=w(l()),v={title:\"How to Use the Kubernetes RBAC API for More Secure Apps\",summary:\"RBAC API gives permissions to an object in a Kubernetes cluster to improve governance & strengthen secuity.\",publishDate:\"Fri Sep 30 2022 18:55:19 GMT+0000 (Coordinated Universal Time)\",author:\"tommy-mcclung\",readingTime:4,categories:[\"kubernetes\",\"platform-engineering\"],mainImage:\"/blog-images/514418c7a457bea6c774f627b909a699.jpg\",imageAlt:\"Close-up of people shaking hands\",showCTA:!0,ctaCopy:\"Unlock secure, on-demand Kubernetes environments with RBAC controls for streamlined app development and testing.\",ctaLink:\"https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=how-to-use-the-kubernetes-rbac-api-for-more-secure-apps\",relatedPosts:[\"\"],ogImage:\"/blog-images/514418c7a457bea6c774f627b909a699.jpg\",excerpt:\"RBAC API gives permissions to an object in a Kubernetes cluster to improve governance & strengthen secuity.\",tags:[\"kubernetes\",\"platform-engineering\"],ctaButton:\"Try Release for Free\"};function h(s){let e=Object.assign({p:\"p\",ul:\"ul\",li:\"li\",img:\"img\",h3:\"h3\",a:\"a\",span:\"span\",strong:\"strong\",pre:\"pre\",code:\"code\"},s.components);return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(e.p,{children:\"If you manage a Kubernetes cluster, sooner or later, you'll need to assign roles and permissions to users so that everyone has visibility and access only to the resources they need. For example, some users must have read-only access, while others must only be able to write to certain resources or APIs. In addition, there must be users with unlimited access to all available resources.\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:\"Suppose you have a team of four users:\\xA0\"}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsx)(e.li,{children:\"Patricia: leader of the dev team\"}),`\n`,(0,n.jsx)(e.li,{children:\"James: part of the dev team\"}),`\n`,(0,n.jsx)(e.li,{children:\"Oliver: leader of the QA team\"}),`\n`,(0,n.jsx)(e.li,{children:\"Amelia: part of the QA team\"}),`\n`]}),`\n`,(0,n.jsx)(e.p,{children:\"James works on one of the many projects within the company's cluster and therefore needs access to your namespace. Patricia, being the dev leader, needs full access to all projects. Oliver, likewise, needs full read access to all objects in the cluster. However, Amelia is working on the same project as James and can only have read access in this namespace.\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:\"Role-based access control (RBAC) is an authorization mechanism built to handle such cases. In this article, we will discuss what RBAC API is and how you can use Kubernetes RBAC API to develop secure applications.\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/c86b1152882a95adcd1ae1ebf5677e4d.jpeg\",alt:\"Close-up of people shaking handsDescription automatically generated\"})}),`\n`,(0,n.jsxs)(e.h3,{id:\"what-is-rbac\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#what-is-rbac\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"What is RBAC?\"]}),`\n`,(0,n.jsx)(e.p,{children:\"The concept of role-based access control is not something new. RBAC is based on the concepts of roles, permissions, and user groups, and it's one of the most widespread access control models being used in organizations today. \\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:\"In organizational use, RBAC allows you to create secure access models based on the real functions that people have within the organization rather than on the actions they must be able to perform.\\xA0\"}),`\n`,(0,n.jsxs)(e.h3,{id:\"what-is-the-rbac-api-and-how-is-it-used-in-kubernetes\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#what-is-the-rbac-api-and-how-is-it-used-in-kubernetes\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"What is the RBAC API, and how is it used in Kubernetes?\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Kubernetes (as of version 1.6) introduced the concept of role-based access control as a system for distributing access rights to various objects in a Kubernetes cluster.\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:\"Objects in a Kubernetes cluster are YAML manifests, and permissions determine which user can only view the manifests and who can create, modify, or even delete them.\\xA0\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"Before we go into how RBAC works in Kubernetes, it's important to understand what a user is in Kubernetes. Everyone who sends requests to the API server is a user in a Kubernetes cluster. This means that not only administrators and developers are users, but also various CI/CD scripts and control plane, \",(0,n.jsx)(e.strong,{children:\"kubelet\"}),\", and \",(0,n.jsx)(e.strong,{children:\"kube-proxy\"}),\" components on nodes are considered users. \\xA0\"]}),`\n`,(0,n.jsx)(e.p,{children:\"The RBAC model includes five entities: \\xA0\"}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsx)(e.li,{children:\"Role\"}),`\n`,(0,n.jsx)(e.li,{children:\"RoleBinding\"}),`\n`,(0,n.jsx)(e.li,{children:\"ClusterRole\"}),`\n`,(0,n.jsx)(e.li,{children:\"ClusterRoleBinding\"}),`\n`,(0,n.jsx)(e.li,{children:\"ServiceAccount\"}),`\n`]}),`\n`,(0,n.jsxs)(e.p,{children:[\"Let's explore each entity in more detail.\\xA0\",(0,n.jsx)(e.strong,{children:\"\\u200D\"})]}),`\n`,(0,n.jsxs)(e.h3,{id:\"role\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#role\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Role\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"The \",(0,n.jsx)(e.strong,{children:\"role\"}),\" is a YAML manifest that describes a set of rights on Kubernetes cluster objects.\\xA0\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"Here it's important to understand that cluster objects are YAML manifests stored in \",(0,n.jsx)(e.strong,{children:\"etcd\"}),\". The API server checks all rights as they relate to the requests that the API server receives. \\xA0\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"If you restrict the user to execute \",(0,n.jsx)(e.strong,{children:\"kubectl exec\"}),\", but that user has access to the worker node, then they will not be able to block it from entering the worker node and doing docker exec in the RBAC container.\\xA0\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"We can go to the cluster, and in \",(0,n.jsx)(e.strong,{children:\"ns ingress-nginx\"}),\", look at the \",(0,n.jsx)(e.strong,{children:\"role: ingress-nginx\"}),\":\\xA0\"]}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-yaml\",children:`kubectl get role -n ingress-nginx ingress-nginx -o yaml\n`})}),`\n`,(0,n.jsx)(e.p,{children:\"Here we are interested in the rules section. This is a list of rules that describe access rights.\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:\"In each rule, we have three parameters. Let's look at an example:\\xA0\"}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-yaml\",children:`\napiGroups:\n \\xA0- extensions\n \\xA0- networking.k8s.io\n resources:\n \\xA0- ingresses\n verbs:\n \\xA0- get\n \\xA0- list\n \\xA0- watch\n\n`})}),`\n`,(0,n.jsxs)(e.p,{children:[\"Here \",(0,n.jsx)(e.strong,{children:\"apiGroups:\"}),\" describes the manifest API group. If only the version is specified in apiVersion\\u2014without a group, for example, as in the Pod manifest\\u2014then this manifest is considered to have the so-called root group (core-group). In the role, the root group is specified as an empty string.\\xA0\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"The \",(0,n.jsx)(e.strong,{children:\"resources:\"}),\" parameter refers to a list of resources to which we describe access. You can view the list of resources in your cluster with the command \",(0,n.jsx)(e.strong,{children:\"kubectl api-resources\"}),\". Some sub-resources describe specific actions. For example, the \",(0,n.jsx)(e.strong,{children:\"pods/log\"}),\" sub-resource allows you to view container logs in a pod. \\xA0\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"The \",(0,n.jsx)(e.strong,{children:\"verbs:\"}),\" parameter is a list of actions that you can perform on the resources described above: get, view the list, monitor changes, edit, delete, etc.\\xA0\"]}),`\n`,(0,n.jsxs)(e.h3,{id:\"rolebinding\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#rolebinding\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"RoleBinding\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"Let's now look at the \",(0,n.jsx)(e.strong,{children:\"RoleBinding\"}),\" manifest:\\xA0\"]}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-yaml\",children:`kubectl get rolebinding ingress-nginx -n ingress-nginx -o yaml\n`})}),`\n`,(0,n.jsxs)(e.p,{children:[\"It has two types of fields: \",(0,n.jsx)(e.strong,{children:\"roleRef\"}),\" and \",(0,n.jsx)(e.strong,{children:\"subjects:\"}),\"\\xA0\"]}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-yaml\",children:`\nroleRef:\n \\xA0apiGroup: rbac.authorization.k8s.io\n \\xA0kind: Role\n \\xA0name: ingress-nginx\nsubjects:\n- kind: ServiceAccount\n \\xA0name: ingress-nginx\n \\xA0namespace: ingress-nginx\n- kind: User\n \\xA0name: jane \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0# \"name\" is case sensitive\n \\xA0apiGroup: rbac.authorization.k8s.io\n- kind: Group\n \\xA0name: developer \\xA0 \\xA0 \\xA0\n \\xA0apiGroup: rbac.authorization.k8s.io\n\n`})}),`\n`,(0,n.jsx)(e.p,{children:\"Here's what we have:\\xA0\"}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"roleRef\"}),\" specifies the role.\"]}),`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"subjects\"}),\" specifies who will be assigned this role.\"]}),`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"kind\"}),\" specifies permissions for requests not authenticated through a token from the service account.\"]}),`\n`]}),`\n`,(0,n.jsxs)(e.h3,{id:\"clusterrole\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#clusterrole\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"ClusterRole\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"The \",(0,n.jsx)(e.strong,{children:\"role\"}),\" entity is namespace dependent and we can create roles with the same name in different namespaces. While \",(0,n.jsx)(e.strong,{children:\"ClusterRole\"}),\" is a cluster object, this entity describes the rights to objects in the entire cluster.\\xA0\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"Kubernetes has many preconfigured cluster roles. These include the \",(0,n.jsx)(e.strong,{children:\"admin\"}),\", \",(0,n.jsx)(e.strong,{children:\"edit\"}),\", and \",(0,n.jsx)(e.strong,{children:\"view\"}),\" roles, which describe the rights that allow administrating, editing, or only viewing entities. If you have administrator rights, you can view the role in your cluster with the following command:\\xA0\"]}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-yaml\",children:`kubectl get clusterrole edit -o yaml\n`})}),`\n`,(0,n.jsxs)(e.h3,{id:\"clusterrolebinding\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#clusterrolebinding\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"ClusterRoleBinding\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"RoleBinding only gives access to entities in the same namespace as the RoleBinding manifest. \",(0,n.jsx)(e.strong,{children:\"ClusterRoleBinding\"}),\" allows you to grant access to entities in all cluster namespaces simultaneously.\\xA0\"]}),`\n`,(0,n.jsxs)(e.h3,{id:\"service-account\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#service-account\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Service Account\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"Kubernetes knows nothing about users in the form we are used to seeing them in when it comes to other access restriction systems, where users have a login or a password. Still, it has mechanisms for calling external password verification services, such as \",(0,n.jsx)(e.strong,{children:\"oidc\"}),\", a user certificate verification option, or even the usual HTTP basic auth with the classic Apache file \",(0,n.jsx)(e.strong,{children:\"htpasswd\"}),\".\\xA0\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"\\u200D\",(0,n.jsx)(e.strong,{children:\"ServiceAccount\"}),\" was created primarily to limit the rights of applications that run in a cluster. All communication between cluster components goes through requests to the API server, and a special JWT token just authorizes each such request. This token is automatically generated when an object of the \",(0,n.jsx)(e.strong,{children:\"ServiceAccount\"}),\" type is created and placed in \",(0,n.jsx)(e.a,{href:\"https://release.com/blog/kubernetes-secrets-management-a-practical-guide\",children:\"secret.\"}),\" \\xA0\"]}),`\n`,(0,n.jsxs)(e.h3,{id:\"how-to-use-rbac-api-in-kubernetes\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#how-to-use-rbac-api-in-kubernetes\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"How to use RBAC API in Kubernetes\"]}),`\n`,(0,n.jsxs)(e.p,{children:[(0,n.jsx)(e.a,{href:\"https://kubernetes.io/docs/reference/access-authn-authz/rbac/\",children:\"RBAC authorization\"}),\" is one way to assign roles to users in a Kubernetes cluster. Here are the steps:\\xA0\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"1. Connect the \",(0,n.jsx)(e.a,{href:\"https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/\",children:\"service account token\"}),\". Without this token, you will need to re-download \",(0,n.jsx)(e.strong,{children:\"kubeconfig\"}),\" after any change in roles.\\xA0\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"2. Assign roles. Here's an example manifest that creates two namespaces and two users, each of which will only be able to \",(0,n.jsx)(e.a,{href:\"https://release.com/blog/kubernetes-pod-a-beginners-guide-to-an-essential-resource\",children:\"manage pods\"}),\" in their own namespace:\\xA0\"]}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-yaml\",children:`\napiVersion: v1\nkind: Namespace\nmetadata:\n \\xA0name: test-one\n---\napiVersion: v1\nkind: Namespace\nmetadata:\n \\xA0name: test-two\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n \\xA0name: read-pods-one\n \\xA0namespace: test-one\nsubjects:\n- kind: ServiceAccount\n \\xA0name: test-sa-one\n \\xA0apiGroup: \"\"\nroleRef:\n \\xA0kind: Role\n \\xA0name: pod-reader-one\n \\xA0apiGroup: rbac.authorization.k8s.io\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n \\xA0name: read-pods-two\n \\xA0namespace: test-two\nsubjects:\n- kind: ServiceAccount\n \\xA0name: test-sa-two\n \\xA0apiGroup: \"\"\nroleRef:\n \\xA0kind: Role\n \\xA0name: pod-reader-two\n \\xA0apiGroup: rbac.authorization.k8s.io\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n \\xA0namespace: test-one\n \\xA0name: pod-reader-one\nrules:\n- apiGroups: [\"\"] # \"\" indicates the core API group\n \\xA0resources: [\"pods\"]\n \\xA0verbs: [\"get\", \"watch\", \"list\"]\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n \\xA0namespace: test-two\n \\xA0name: pod-reader-two\nrules:\n- apiGroups: [\"\"] # \"\" indicates the core API group\n \\xA0resources: [\"pods\"]\n \\xA0verbs: [\"get\", \"watch\", \"list\"]\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n \\xA0namespace: test-one\n \\xA0name: test-sa-one\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n \\xA0namespace: test-two\n \\xA0name: test-sa-twoRun the manifest.\n\n`})}),`\n`,(0,n.jsx)(e.p,{children:\"3. Run the manifest.\"}),`\n`,(0,n.jsx)(e.p,{children:\"4. Create tokens:\\xA0\"}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-yaml\",children:`kubectl get secret $(kubectl get serviceaccount test-sa-one -o jsonpath='{.secrets[0].name}' --namespace test-one) -o jsonpath='{.data.token}' --namespace test-one | base64 -d\n\nkubectl get secret $(kubectl get serviceaccount test-sa-two -o jsonpath='{.secrets[0].name}' --namespace test-two) -o jsonpath='{.data.token}' --namespace test-two | base64 -d\n`})}),`\n`,(0,n.jsxs)(e.p,{children:[\"5. Manually add tokens to users in the \",(0,n.jsx)(e.strong,{children:\"kubeconfig.yaml\"}),\" file for authorization without a password:\\xA0\"]}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-yaml\",children:`\nusers:\n...\n- name: test-sa-one\nuser:\n \\xA0token:\n- name: test-sa-two\nuser:\n \\xA0token:\n...\n\n`})}),`\n`,(0,n.jsx)(e.p,{children:\"6. Check the distribution of roles:\\xA0\"}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-yaml\",children:`\nkubectl config set-context --current --user=test-sa-two\nContext \"admin@kubernetes\" modified.\n\nkubectl get pods --namespace test-two\nNo resources found in test-two namespace.\n\nkubectl get pods --namespace test-one\nError from server (Forbidden): pods is forbidden: User \"system:serviceaccount:test-two:test-sa-two\" cannot list resource \"pods\" in API group \"\" in the namespace \"test-one\"\n________\n\nkubectl config set-context --current --user=test-sa-one\nContext \"admin@kubernetes\" modified.\n\nkubectl get pods --namespace test-two\nError from server (Forbidden): pods is forbidden: User \"system:serviceaccount:test-one:test-sa-one\" cannot list resource \"pods\" in API group \"\" in the namespace \"test-two\"\n\nkubectl get pods --namespace test-one\nNo resources found in test-one namespace.\n\n`})}),`\n`,(0,n.jsxs)(e.p,{children:[\"The test-sa-two user now has access to pods in the \",(0,n.jsx)(e.strong,{children:\"test-two\"}),\" namespace and no access to the \",(0,n.jsx)(e.strong,{children:\"test-one\"}),' namespace. Similarly, the \"test-sa-one\" user has access to pods in the ',(0,n.jsx)(e.strong,{children:\"test-one\"}),\" namespace but not those in the \",(0,n.jsx)(e.strong,{children:\"test-two\"}),\" namespace.\"]}),`\n`,(0,n.jsxs)(e.h3,{id:\"kubernetes-rbac-as-a-security-strategy\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#kubernetes-rbac-as-a-security-strategy\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Kubernetes RBAC as a Security Strategy\"]}),`\n`,(0,n.jsx)(e.p,{children:\"The RBAC model has proved particularly effective for managing roles and is now considered a Kubernetes security best practice. \\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:\"RBAC reduces the risk of unwanted access to critical resources and allows you to implement the principle of least privilege by giving access to only needed resources, which makes your cluster more secure. \\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:\"Moreover, if you are using RBAC, you don't have to check the individual permissions assigned to each user. You can monitor who has access to resources simply by consulting roles, which makes auditing easier.\\xA0\"}),`\n`,(0,n.jsxs)(e.h3,{id:\"conclusion\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#conclusion\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Conclusion\"]}),`\n`,(0,n.jsx)(e.p,{children:\"In summary, RBAC API is a role-based approach to giving permissions to an object in a Kubernetes cluster. By making it part of the Kubernetes pipeline, you can improve governance and significantly strengthen security.\"})]})}function y(s={}){let{wrapper:e}=s.components||{};return e?(0,n.jsx)(e,Object.assign({},s,{children:(0,n.jsx)(h,s)})):h(s)}var A=y;return k(R);})();\n;return Component;"
        },
        "_id": "blog/posts/how-to-use-the-kubernetes-rbac-api-for-more-secure-apps.mdx",
        "_raw": {
          "sourceFilePath": "blog/posts/how-to-use-the-kubernetes-rbac-api-for-more-secure-apps.mdx",
          "sourceFileName": "how-to-use-the-kubernetes-rbac-api-for-more-secure-apps.mdx",
          "sourceFileDir": "blog/posts",
          "contentType": "mdx",
          "flattenedPath": "blog/posts/how-to-use-the-kubernetes-rbac-api-for-more-secure-apps"
        },
        "type": "BlogPost",
        "computedSlug": "how-to-use-the-kubernetes-rbac-api-for-more-secure-apps"
      },
      "documentHash": "1739393595021",
      "hasWarnings": false,
      "documentTypeName": "BlogPost"
    },
    "blog/posts/how-to-write-route53-stubbed-responses-for-rspec-tests.mdx": {
      "document": {
        "title": "How To Write Route53 Stubbed Responses For Rspec Tests",
        "summary": "Short Example for writing AWS Ruby SDK unit tests for Route53 API calls with stubbed responses",
        "publishDate": "Thu Sep 09 2021 02:28:14 GMT+0000 (Coordinated Universal Time)",
        "author": "regis-wilson",
        "readingTime": 6,
        "categories": [
          "platform-engineering",
          "product"
        ],
        "mainImage": "/blog-images/be577b12dad97ecf46a4f0ff2cdd1d35.jpg",
        "imageAlt": "A person hands holding wires and making tests on a circuit board",
        "showCTA": true,
        "ctaCopy": "Automate your environment setup like testing AWS Route53 responses with Release's on-demand environments.",
        "ctaLink": "https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=how-to-write-route53-stubbed-responses-for-rspec-tests",
        "relatedPosts": [
          ""
        ],
        "ogImage": "/blog-images/be577b12dad97ecf46a4f0ff2cdd1d35.jpg",
        "excerpt": "Short Example for writing AWS Ruby SDK unit tests for Route53 API calls with stubbed responses",
        "tags": [
          "platform-engineering",
          "product"
        ],
        "ctaButton": "Try Release for Free",
        "body": {
          "raw": "\n### How to Write Route53 Stubbed Responses For Rspec Tests\n\nIn this blog post, I will go over a recent exercise to fix some bugs, refactor, and write tests for some of our code related to Route53. Route53 is an AWS service that creates, updates, and provides Domain Name Service (DNS) for the internet. The reason that code unit tests are so important is because it helps reveal bugs, creates supportable and high quality code, and allows restructuring and refactoring with confidence. The downside to writing unit tests is that it can be time consuming, difficult at times, and bloating to the normal code base. It is not uncommon for unit tests’ \"lines of code\" (LOC) count to far exceed the LOC for the actual codebase. You would not be crazy to have nearly an order of magnitude difference in LOC for actual codebase versus LOC for unit test cases.\n\nIn this case, interacting with the AWS Route53 API was daunting to test and stubbing responses seemed incredibly difficult until I found some examples written by another one of our engineers that showed how the rspec and API SDKs could be made to work in a fairly straightforward and (dare I say) downright fun method for unit testing Ruby code.\n\n### The Code Under Examination\n\nThis straightforward code snippet was my first target for unit testing. It is very simple and only does one thing. It is ripe for refactoring for readability and reusability for other sections of the code. This should be the best way to begin the project and get familiar with the rspec templates I’d be using later. Before I start refactoring and fixing bugs, I wanted to write tests. Other than the fairly “inliney” and hard to follow syntax and “magical” code, can you spot any bugs?\n\n```ruby line-numbers\n\ndef route53_hosted_zone_id(subdomain)\n  route53.list_hosted_zones_by_name.map do |response|\n    response.hosted_zones.detect{|zone| zone.name == \"#{subdomain}.\" }&.id&.gsub(/.*\\//, '')\n  end.flatten.compact.first\nend\n\n```\n\n### Write Helpers Before the Refactor\n\nI am already itching to remove the magical subdomain rewriting and gsub deleting into separate methods that can be reused and are easier to read:\n\n```ruby line-numbers\n\ndef cannonicalise(hostname)\n  hostname = domain_parts(hostname).join('.')\n\n  \"#{hostname}.\"\nend\n\ndef parse_hosted_zone_id(hosted_zone_id)\n  return nil if hosted_zone_id.blank?\n\n  hosted_zone_id.gsub(%r{.*/+}, '')\nend\n\n```\n\n### Stub and Test the New Methods\n\nFirst things first, we need to do a little bit of boilerplate to get the API calls mocked and stubbed, then add a few very simple tests to get started.\n\n```ruby line-numbers\n\n# frozen_string_literal: true\n\nrequire 'rails_helper'\n\nRSpec.describe Cloud::Aws::Route53 do\n  let(:route53) { Aws::Route53::Client.new(stub_responses: true) }\n\n  subject { FactoryBot.create(:v2_cloud_integration) }\n\n  before do\n    allow(subject).to receive(:route53).and_return(route53)\n  end\n\n  describe '#parse_hosted_zone_id' do\n    context 'with a valid hostedzone identifier' do\n      it 'returns just the zoneid' do\n        expect(subject.parse_hosted_zone_id('/hostedzone/Z1234ABC')).to eq('Z1234ABC')\n      end\n    end\n  end\n  describe '#cannonicalise' do\n    context 'without a dot' do\n      it 'returns the zone with a dot' do\n        expect(subject.cannonicalise('some.host')).to eq('some.host.')\n      end\n    end\n    context 'with a dot' do\n      it 'returns the zone with a dot' do\n        expect(subject.cannonicalise('some.host.')).to eq('some.host.')\n      end\n    end\n  end\nend\n\n```\n\n### Write A Fixture\n\nPerfect, now we can test our new **cannonicalise** and **parse_hosted_zone_id** methods and we have a stubbed response coming from the Route53 API calls. Let’s write a simple new test to uncover some bugs by testing the api responses we get. The first step is to write some fixtures we can test with. Here we generate two faked stubbed responses for a very [common domain](https://example.com).\n\n```ruby line-numbers\n\n  context 'an AWS cloud integration' do\n    before do\n      route53.stub_responses(:list_hosted_zones_by_name, {\n                               is_truncated: false,\n                               max_items: 100,\n                               hosted_zones: [\n                                 {\n                                   id: '/hostedzone/Z321EXAMPLE',\n                                   name: 'example.com.',\n                                   config: {\n                                     comment: 'Some comment 1',\n                                     private_zone: true\n                                   },\n                                   caller_reference: SecureRandom.hex\n                                 },\n                                 {\n                                   id: '/hostedzone/Z123EXAMPLE',\n                                   name: 'example.com.',\n                                   config: {\n                                     comment: 'Some comment 2',\n                                     private_zone: false\n                                   },\n                                   caller_reference: SecureRandom.hex\n                                 }\n                               ]\n                             })\n    end\nend\n\n```\n\nIf you’re wondering how to make these fixtures, you can easily read the [AWS Ruby SDK V3 documentation](https://docs.aws.amazon.com/sdk-for-ruby/v3/api/Aws/Route53/Client.html#list_hosted_zones_by_name-instance_method) for sample inputs and outputs, or you can make API calls via the AWS CLI and inspect the responses, or you can even just put in some values and see what happens when you run rspec. For example, if I remove, say, the \\`caller_reference\\` parameter, I’ll get an error that helpfully identifies the problem.\n\n![](/blog-images/1533e9e3859e67cfef95708dd3aadb80.png)\n\nRemoving required parameters gives a helpful error message to correct the problem.\n\nYou really can’t go wrong with the SDK validation and stubbed responses taken from the examples or from live requests you make with the CLI! This is already a tremendous benefit and we’re not even testing our own code yet.\n\n### Write a Test Case with the Stubbed Responses\n\nNow we can write some unit test cases and loop through several responses that we expect to find the hosted zone. Voilá we’ve uncovered some bugs just by being a little creative with our inputs! Do you see why?\n\n```ruby line-numbers\n\ndescribe '#route53_hosted_zone_id' do\n  %w[\n    example.com\n    example.com.\n    www.example.com\n    www.example.com.\n    test.www.example.com\n    test.www.example.com.\n    deep.test.www.example.com\n  ].each do |hostname|\n    context 'for hosts that exist in the parent zone' do\n      it \"returns the hosted_zone_id for #{hostname}\" do\n        expect(route53).to receive(:list_hosted_zones_by_name).with(no_args).and_call_original\n        hosted_zone_id = subject.route53_hosted_zone_id(hostname)\n        expect(hosted_zone_id).to eq('Z123EXAMPLE')\n      end\n    end\n  end\nend\n\n```\n\n![](/blog-images/62c5d2015dd19e4280f0680a300995af.png)\n\nWith some creativity in test inputs and stubbed responses from the API, we can uncover some edge cases and bugs to fix!\n\nWhat these failed test cases are telling us is that the code worked under perfect conditions but in strange scenarios that may not be uncommon (for example, having an internal private zone and public zone with the same name, or selecting a two-level-deep name in a zone) could cause unpredictable behaviours.\n\n### The Solution is an Exercise for the Reader\n\nNow we merely need to write or refactor the code from our original snippet to pass all of our new test cases. One of the issues that our test cases revealed was that two-level-deep names (say, test.www.example.com in the zone example.com) would be missed. We also needed a way to ensure that zones are not private, perhaps with an optional parameter to specify private zones. Here is an example that passes all the existing tests and welcome feedback on any other bugs or optimisations you find.\n\n```ruby line-numbers\n\ndef route53_hosted_zone_ids_by_name(is_private_zone: false)\n  # TODO: danger, does not handle duplicate zone names!!!\n  hosted_zone_ids_by_name = {}\n  route53.list_hosted_zones_by_name.each do |response|\n    response.hosted_zones.each do |zone|\n      if !!zone.config.private_zone == is_private_zone\n        hosted_zone_ids_by_name[zone.name] = parse_hosted_zone_id(zone.id)\n      end\n    end\n  end\n  hosted_zone_ids_by_name\nend\n\ndef route53_hosted_zone_id(hostname)\n  # Recursively look for the zone id of the nearest parent (host, subdomain, or apex)\n  hosted_zone_ids_by_name = route53_hosted_zone_ids_by_name\n\n  loop do\n    hostname = cannonicalise(hostname)\n    break if hosted_zone_ids_by_name[hostname].present?\n\n    # Strip off one level and try again\n    hostname = domain_parts(hostname).drop(1).join('.')\n    break if hostname.blank?\n  end\n  hosted_zone_ids_by_name[hostname]\nend\n\n```\n\n### Congratulations\n\nAll test cases now pass! Keep writing tests until you get nearly 100% coverage!\n\n> Hero Image by [Jeswin Thomas](https://unsplash.com/@jeswinthomas?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) on [Unsplash](https://unsplash.com/s/photos/testing?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)\n",
          "code": "var Component=(()=>{var l=Object.create;var a=Object.defineProperty;var u=Object.getOwnPropertyDescriptor;var m=Object.getOwnPropertyNames;var p=Object.getPrototypeOf,f=Object.prototype.hasOwnProperty;var b=(t,e)=>()=>(e||t((e={exports:{}}).exports,e),e.exports),g=(t,e)=>{for(var o in e)a(t,o,{get:e[o],enumerable:!0})},i=(t,e,o,r)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let s of m(e))!f.call(t,s)&&s!==o&&a(t,s,{get:()=>e[s],enumerable:!(r=u(e,s))||r.enumerable});return t};var _=(t,e,o)=>(o=t!=null?l(p(t)):{},i(e||!t||!t.__esModule?a(o,\"default\",{value:t,enumerable:!0}):o,t)),w=t=>i(a({},\"__esModule\",{value:!0}),t);var c=b((R,d)=>{d.exports=_jsx_runtime});var v={};g(v,{default:()=>z,frontmatter:()=>y});var n=_(c()),y={title:\"How To Write Route53 Stubbed Responses For Rspec Tests\",summary:\"Short Example for writing AWS Ruby SDK unit tests for Route53 API calls with stubbed responses\",publishDate:\"Thu Sep 09 2021 02:28:14 GMT+0000 (Coordinated Universal Time)\",author:\"regis-wilson\",readingTime:6,categories:[\"platform-engineering\",\"product\"],mainImage:\"/blog-images/be577b12dad97ecf46a4f0ff2cdd1d35.jpg\",imageAlt:\"A person hands holding wires and making tests on a circuit board\",showCTA:!0,ctaCopy:\"Automate your environment setup like testing AWS Route53 responses with Release's on-demand environments.\",ctaLink:\"https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=how-to-write-route53-stubbed-responses-for-rspec-tests\",relatedPosts:[\"\"],ogImage:\"/blog-images/be577b12dad97ecf46a4f0ff2cdd1d35.jpg\",excerpt:\"Short Example for writing AWS Ruby SDK unit tests for Route53 API calls with stubbed responses\",tags:[\"platform-engineering\",\"product\"],ctaButton:\"Try Release for Free\"};function h(t){let e=Object.assign({h3:\"h3\",a:\"a\",span:\"span\",p:\"p\",pre:\"pre\",code:\"code\",strong:\"strong\",img:\"img\",blockquote:\"blockquote\"},t.components);return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsxs)(e.h3,{id:\"how-to-write-route53-stubbed-responses-for-rspec-tests\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#how-to-write-route53-stubbed-responses-for-rspec-tests\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"How to Write Route53 Stubbed Responses For Rspec Tests\"]}),`\n`,(0,n.jsx)(e.p,{children:'In this blog post, I will go over a recent exercise to fix some bugs, refactor, and write tests for some of our code related to Route53. Route53 is an AWS service that creates, updates, and provides Domain Name Service (DNS) for the internet. The reason that code unit tests are so important is because it helps reveal bugs, creates supportable and high quality code, and allows restructuring and refactoring with confidence. The downside to writing unit tests is that it can be time consuming, difficult at times, and bloating to the normal code base. It is not uncommon for unit tests\\u2019 \"lines of code\" (LOC) count to far exceed the LOC for the actual codebase. You would not be crazy to have nearly an order of magnitude difference in LOC for actual codebase versus LOC\\xA0for unit test cases.'}),`\n`,(0,n.jsx)(e.p,{children:\"In this case, interacting with the AWS Route53 API was daunting to test and stubbing responses seemed incredibly difficult until I found some examples written by another one of our engineers that showed how the rspec and API SDKs could be made to work in a fairly straightforward and (dare I say) downright fun method for unit testing Ruby code.\"}),`\n`,(0,n.jsxs)(e.h3,{id:\"the-code-under-examination\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#the-code-under-examination\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"The Code Under Examination\"]}),`\n`,(0,n.jsx)(e.p,{children:\"This straightforward code snippet was my first target for unit testing. It is very simple and only does one thing. It is ripe for refactoring for readability and reusability for other sections of the code. This should be the best way to begin the project and get familiar with the rspec templates I\\u2019d be using later. Before I start refactoring and fixing bugs, I wanted to write tests. Other than the fairly \\u201Cinliney\\u201D and hard to follow syntax and \\u201Cmagical\\u201D code, can you spot any bugs?\"}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-ruby\",children:`\ndef route53_hosted_zone_id(subdomain)\n \\xA0route53.list_hosted_zones_by_name.map do |response|\n \\xA0 \\xA0response.hosted_zones.detect{|zone| zone.name == \"#{subdomain}.\" }&.id&.gsub(/.*\\\\//, '')\n \\xA0end.flatten.compact.first\nend\n\n`})}),`\n`,(0,n.jsxs)(e.h3,{id:\"write-helpers-before-the-refactor\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#write-helpers-before-the-refactor\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Write Helpers Before the Refactor\"]}),`\n`,(0,n.jsx)(e.p,{children:\"I am already itching to remove the magical subdomain rewriting and gsub deleting into separate methods that can be reused and are easier to read:\"}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-ruby\",children:`\ndef cannonicalise(hostname)\n \\xA0hostname = domain_parts(hostname).join('.')\n\n \\xA0\"#{hostname}.\"\nend\n\ndef parse_hosted_zone_id(hosted_zone_id)\n \\xA0return nil if hosted_zone_id.blank?\n\n \\xA0hosted_zone_id.gsub(%r{.*/+}, '')\nend\n\n`})}),`\n`,(0,n.jsxs)(e.h3,{id:\"stub-and-test-the-new-methods\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#stub-and-test-the-new-methods\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Stub and Test the New Methods\"]}),`\n`,(0,n.jsx)(e.p,{children:\"First things first, we need to do a little bit of boilerplate to get the API calls mocked and stubbed, then add a few very simple tests to get started.\"}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-ruby\",children:`\n# frozen_string_literal: true\n\nrequire 'rails_helper'\n\nRSpec.describe Cloud::Aws::Route53 do\n \\xA0let(:route53) { Aws::Route53::Client.new(stub_responses: true) }\n\n \\xA0subject { FactoryBot.create(:v2_cloud_integration) }\n\n \\xA0before do\n \\xA0 \\xA0allow(subject).to receive(:route53).and_return(route53)\n \\xA0end\n\n \\xA0describe '#parse_hosted_zone_id' do\n \\xA0 \\xA0context 'with a valid hostedzone identifier' do\n \\xA0 \\xA0 \\xA0it 'returns just the zoneid' do\n \\xA0 \\xA0 \\xA0 \\xA0expect(subject.parse_hosted_zone_id('/hostedzone/Z1234ABC')).to eq('Z1234ABC')\n \\xA0 \\xA0 \\xA0end\n \\xA0 \\xA0end\n \\xA0end\n \\xA0describe '#cannonicalise' do\n \\xA0 \\xA0context 'without a dot' do\n \\xA0 \\xA0 \\xA0it 'returns the zone with a dot' do\n \\xA0 \\xA0 \\xA0 \\xA0expect(subject.cannonicalise('some.host')).to eq('some.host.')\n \\xA0 \\xA0 \\xA0end\n \\xA0 \\xA0end\n \\xA0 \\xA0context 'with a dot' do\n \\xA0 \\xA0 \\xA0it 'returns the zone with a dot' do\n \\xA0 \\xA0 \\xA0 \\xA0expect(subject.cannonicalise('some.host.')).to eq('some.host.')\n \\xA0 \\xA0 \\xA0end\n \\xA0 \\xA0end\n \\xA0end\nend\n\n`})}),`\n`,(0,n.jsxs)(e.h3,{id:\"write-a-fixture\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#write-a-fixture\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Write A Fixture\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"Perfect, now we can test our new \",(0,n.jsx)(e.strong,{children:\"cannonicalise\"}),\" and \",(0,n.jsx)(e.strong,{children:\"parse_hosted_zone_id\"}),\" methods and we have a stubbed response coming from the Route53 API calls. Let\\u2019s write a simple new test to uncover some bugs by testing the api responses we get. The first step is to write some fixtures we can test with. Here we generate two faked stubbed responses for a very \",(0,n.jsx)(e.a,{href:\"https://example.com\",children:\"common domain\"}),\".\"]}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-ruby\",children:`\n \\xA0context 'an AWS cloud integration' do\n \\xA0 \\xA0before do\n \\xA0 \\xA0 \\xA0route53.stub_responses(:list_hosted_zones_by_name, {\n \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 is_truncated: false,\n \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 max_items: 100,\n \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 hosted_zones: [\n \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 {\n \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 id: '/hostedzone/Z321EXAMPLE',\n \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 name: 'example.com.',\n \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 config: {\n \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 comment: 'Some comment 1',\n \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 private_zone: true\n \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 },\n \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 caller_reference: SecureRandom.hex\n \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 },\n \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 {\n \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 id: '/hostedzone/Z123EXAMPLE',\n \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 name: 'example.com.',\n \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 config: {\n \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 comment: 'Some comment 2',\n \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 private_zone: false\n \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 },\n \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 caller_reference: SecureRandom.hex\n \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 }\n \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 ]\n \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 })\n \\xA0 \\xA0end\nend\n\n`})}),`\n`,(0,n.jsxs)(e.p,{children:[\"If you\\u2019re wondering how to make these fixtures, you can easily read the \",(0,n.jsx)(e.a,{href:\"https://docs.aws.amazon.com/sdk-for-ruby/v3/api/Aws/Route53/Client.html#list_hosted_zones_by_name-instance_method\",children:\"AWS Ruby SDK V3 documentation\"}),\" for sample inputs and outputs, or you can make API calls via the AWS CLI and inspect the responses, or you can even just put in some values and see what happens when you run rspec. For example, if I remove, say, the `caller_reference` parameter, I\\u2019ll get an error that helpfully identifies the problem.\"]}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/1533e9e3859e67cfef95708dd3aadb80.png\",alt:\"\"})}),`\n`,(0,n.jsx)(e.p,{children:\"Removing required parameters gives a helpful error message to correct the problem.\"}),`\n`,(0,n.jsx)(e.p,{children:\"You really can\\u2019t go wrong with the SDK validation and stubbed responses taken from the examples or from live requests you make with the CLI! This is already a tremendous benefit and we\\u2019re not even testing our own code yet.\"}),`\n`,(0,n.jsxs)(e.h3,{id:\"write-a-test-case-with-the-stubbed-responses\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#write-a-test-case-with-the-stubbed-responses\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Write a Test Case with the Stubbed Responses\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Now we can write some unit test cases and loop through several responses that we expect to find the hosted zone. Voil\\xE1 we\\u2019ve uncovered some bugs just by being a little creative with our inputs! Do you see why?\"}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-ruby\",children:`\ndescribe '#route53_hosted_zone_id' do\n \\xA0%w[\n \\xA0 \\xA0example.com\n \\xA0 \\xA0example.com.\n \\xA0 \\xA0www.example.com\n \\xA0 \\xA0www.example.com.\n \\xA0 \\xA0test.www.example.com\n \\xA0 \\xA0test.www.example.com.\n \\xA0 \\xA0deep.test.www.example.com\n \\xA0].each do |hostname|\n \\xA0 \\xA0context 'for hosts that exist in the parent zone' do\n \\xA0 \\xA0 \\xA0it \"returns the hosted_zone_id for #{hostname}\" do\n \\xA0 \\xA0 \\xA0 \\xA0expect(route53).to receive(:list_hosted_zones_by_name).with(no_args).and_call_original\n \\xA0 \\xA0 \\xA0 \\xA0hosted_zone_id = subject.route53_hosted_zone_id(hostname)\n \\xA0 \\xA0 \\xA0 \\xA0expect(hosted_zone_id).to eq('Z123EXAMPLE')\n \\xA0 \\xA0 \\xA0end\n \\xA0 \\xA0end\n \\xA0end\nend\n\n`})}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/62c5d2015dd19e4280f0680a300995af.png\",alt:\"\"})}),`\n`,(0,n.jsx)(e.p,{children:\"With some creativity in test inputs and stubbed responses from the API, we can uncover some edge cases and bugs to fix!\"}),`\n`,(0,n.jsx)(e.p,{children:\"What these failed test cases are telling us is that the code worked under perfect conditions but in strange scenarios that may not be uncommon (for example, having an internal private zone and public zone with the same name, or selecting a two-level-deep name in a zone) could cause unpredictable behaviours.\"}),`\n`,(0,n.jsxs)(e.h3,{id:\"the-solution-is-an-exercise-for-the-reader\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#the-solution-is-an-exercise-for-the-reader\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"The Solution is an Exercise for the Reader\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"Now we merely need to write or refactor the code from our original snippet to pass all of our new test cases. One of the issues that our test cases revealed was that two-level-deep names (say, test.\",(0,n.jsx)(e.a,{href:\"http://www.example.com\",children:\"www.example.com\"}),\" in the zone example.com) would be missed. We also needed a way to ensure that zones are not private, perhaps with an optional parameter to specify private zones. Here is an example that passes all the existing tests and welcome feedback on any other bugs or optimisations you find.\"]}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-ruby\",children:`\ndef route53_hosted_zone_ids_by_name(is_private_zone: false)\n \\xA0# TODO: danger, does not handle duplicate zone names!!!\n \\xA0hosted_zone_ids_by_name = {}\n \\xA0route53.list_hosted_zones_by_name.each do |response|\n \\xA0 \\xA0response.hosted_zones.each do |zone|\n \\xA0 \\xA0 \\xA0if !!zone.config.private_zone == is_private_zone\n \\xA0 \\xA0 \\xA0 \\xA0hosted_zone_ids_by_name[zone.name] = parse_hosted_zone_id(zone.id)\n \\xA0 \\xA0 \\xA0end\n \\xA0 \\xA0end\n \\xA0end\n \\xA0hosted_zone_ids_by_name\nend\n\ndef route53_hosted_zone_id(hostname)\n \\xA0# Recursively look for the zone id of the nearest parent (host, subdomain, or apex)\n \\xA0hosted_zone_ids_by_name = route53_hosted_zone_ids_by_name\n\n \\xA0loop do\n \\xA0 \\xA0hostname = cannonicalise(hostname)\n \\xA0 \\xA0break if hosted_zone_ids_by_name[hostname].present?\n\n \\xA0 \\xA0# Strip off one level and try again\n \\xA0 \\xA0hostname = domain_parts(hostname).drop(1).join('.')\n \\xA0 \\xA0break if hostname.blank?\n \\xA0end\n \\xA0hosted_zone_ids_by_name[hostname]\nend\n\n`})}),`\n`,(0,n.jsxs)(e.h3,{id:\"congratulations\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#congratulations\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Congratulations\"]}),`\n`,(0,n.jsx)(e.p,{children:\"All test cases now pass! Keep writing tests until you get nearly 100% coverage!\"}),`\n`,(0,n.jsxs)(e.blockquote,{children:[`\n`,(0,n.jsxs)(e.p,{children:[\"Hero Image by \",(0,n.jsx)(e.a,{href:\"https://unsplash.com/@jeswinthomas?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText\",children:\"Jeswin\\xA0Thomas\"}),\" on \",(0,n.jsx)(e.a,{href:\"https://unsplash.com/s/photos/testing?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText\",children:\"Unsplash\"})]}),`\n`]})]})}function x(t={}){let{wrapper:e}=t.components||{};return e?(0,n.jsx)(e,Object.assign({},t,{children:(0,n.jsx)(h,t)})):h(t)}var z=x;return w(v);})();\n;return Component;"
        },
        "_id": "blog/posts/how-to-write-route53-stubbed-responses-for-rspec-tests.mdx",
        "_raw": {
          "sourceFilePath": "blog/posts/how-to-write-route53-stubbed-responses-for-rspec-tests.mdx",
          "sourceFileName": "how-to-write-route53-stubbed-responses-for-rspec-tests.mdx",
          "sourceFileDir": "blog/posts",
          "contentType": "mdx",
          "flattenedPath": "blog/posts/how-to-write-route53-stubbed-responses-for-rspec-tests"
        },
        "type": "BlogPost",
        "computedSlug": "how-to-write-route53-stubbed-responses-for-rspec-tests"
      },
      "documentHash": "1739393595021",
      "hasWarnings": false,
      "documentTypeName": "BlogPost"
    },
    "blog/posts/idp-trends-where-do-we-go-from-here.mdx": {
      "document": {
        "title": "IDP Trends: Where Do We Go From Here",
        "summary": "Explore how IDPs will shape the future of software development by looking at current and anticipating future trends.",
        "publishDate": "Tue Oct 31 2023 20:50:09 GMT+0000 (Coordinated Universal Time)",
        "author": "sylvia-fronczak",
        "readingTime": 10,
        "categories": [
          "platform-engineering",
          "ai",
          "product"
        ],
        "mainImage": "/blog-images/841354692fb08046d7d58b3b71d7cb1d.jpg",
        "imageAlt": "DALL-E genrated image",
        "showCTA": true,
        "ctaCopy": "Empower your dev teams with AI-driven automation and streamlined workflows using Release's environment management platform.",
        "ctaLink": "https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=idp-trends-where-do-we-go-from-here",
        "relatedPosts": [
          ""
        ],
        "ogImage": "/blog-images/841354692fb08046d7d58b3b71d7cb1d.jpg",
        "excerpt": "Explore how IDPs will shape the future of software development by looking at current and anticipating future trends.",
        "tags": [
          "platform-engineering",
          "ai",
          "product"
        ],
        "ctaButton": "Try Release for Free",
        "body": {
          "raw": "\nWe’ve shared a lot about [Internal Developer Platforms (IDPs)](https://release.com/blog/what-is-an-internal-developer-platform-and-why-should-i-have-one), [what they can provide](https://release.com/blog/components-of-a-successful-idp-build-a-product-your-developers-actually-want-to-use), and [how we should build them](https://release.com/blog/use-product-thinking-to-establish-your-idp). As they exist today, simple IDPs focus on improving dev workflows to make our development teams more efficient.\n\nBut what should we expect in the future? Will further developments and patterns in IDPs provide even more efficiencies? Will new opportunities open up to integrate additional tools that simplify product development?\n\nIn short, yes!\n\nThe more organizations start building and using IDPs to enhance workflows, the more development teams will expect IDPs to automate the toil, and the more benefits we’ll gain.\n\nIn order for you to stay on top of trends and continue to provide your developers with a competitive advantage, creating effective IDPs will become more critical than ever.\n\nIn this post, we’ll explore how IDPs will shape the future of software development by looking at existing trends and envisioning what else IDPs can bring to our teams.\n\nTo kick it off, let’s look at some of the biggest trends in software development today.\n\n### What Existing Software Trends Will Continue?\n\nTo determine what will happen with IDPs, we must first consider the environment our developers work in. By following development trends, we can shape how IDPs need to expand to meet future needs.\n\n#### ⚙️ Increased Complexity in Distributed Systems\n\nFirst, we can expect software development to continue to grow in complexity. Distributed systems will continue to grow and become even more distributed.\n\nWhy is that?\n\nTo start, organizations will continue to utilize microservices and serverless systems as they attempt to compartmentalize their products’ complexities within well-defined boundaries. Even monolithic tech stacks don’t provide much value within a vacuum and require other systems and services to function.\n\nWith more services to manage and integrate, dev teams face more and more complexity in developing, testing, debugging, and understanding their systems. To simplify that complexity through automation, IDPs will continue to streamline their day-to-day activities.\n\n#### 🤖Reliance on AI\n\nNext, let’s talk about our growing dependence and reliance on AI. We should consider this reliance to be a positive trend, and not one that takes the purity of software development away from our developers. Organizations increasingly leverage AI to enhance their software products, from automating repetitive tasks to optimizing performance. IDPs can harness the power of AI to augment the developer experience significantly.\n\nUsing AI as part of software development is the next step in taking tedious or basic tasks and simplifying software development. Similar to when developers all moved to IDEs, organizations are embracing tools to keep their development orgs efficient and productive.\n\nTools like [Copilot](https://github.com/features/copilot), [Tabnine](https://www.tabnine.com/), and [CodeWhisperer](https://aws.amazon.com/codewhisperer/) will grow in functionality, and new competitors will join the market. New tools like [release.ai](https://release.ai/) will take those capabilities a step further, allowing development teams to interact with not just their code but also their infrastructure in a smooth and natural fashion. Teams already use tools like ChatGPT to automate the writing of documentation. Forward-thinking orgs will integrate these functionalities throughout the developer workflows through the IDP. Handwritten wikis and instructions will be replaced by AI-driven tools that provide the latest information with considerably less searching.\n\n#### 🚧 Establishment of Platform Teams\n\nFinally, more organizations recognize the importance of dedicated platform teams responsible for delivering IDPs and integrating the various components.\n\nThese internal product teams build, test, market, and sell your IDP to your development organization. They play a vital role in providing developers with the tools they need to excel in their work.\n\nThe trend of building these platform teams will continue as their work leads to more efficient and productive development workflows.\n\n‍\n\n![](/blog-images/93831edeae61e83d317056347566c418.png)\n\n### What IDP Trends Will We See?\n\nKeeping in mind the development trends mentioned above, where should we expect IDPs to grow next?\n\n#### 🦾 More AI Integrations\n\nAs we already hinted, incorporating AI within IDPs will become more pervasive, improving the developer experience in several ways. Let’s take a look at a few of those:\n\n##### **:: AI-Written Documentation**\n\nAI will generate comprehensive documentation for internal services, development practices, and standards. This documentation will always be up to date, ensuring developers can access the latest information without hours of searching wikis, docs, and internal organization sites.\n\nThese docs, integrated throughout the developer workflow, will give our development teams the context they need when they need it.\n\nEventually, this AI-written documentation will extend to externally facing docs. At first, individuals will still review these docs, ensuring that the information is appropriate for external audiences. But eventually, with the right prompts and guardrails, you won’t need to spend human time and resources on documentation at all.\n\n##### **:: Automated Code Reviews**\n\nWe already have some automated code checking through our IDEs and code linters. But is that it? Can we expect more?\n\nOf course we can! AI-driven code review tools will further our capabilities, using context and \\`information about the entire system to point out potential flaws or to improve readability.\n\nThis will also help identify bugs faster and suggest potential fixes.\n\n#### 🌎 Improved Environments\n\nNext, let’s talk about environments. The future of IDPs will drive improvements in developer and production environments, making development more efficient and responsive to real-world scenarios.\n\n##### **:: Emulating External Dependencies**\n\nDue to our growing complexities, our systems depend more on other tools, data stores, and APIs.\n\nRight now, developers who require complex external dependencies to validate changes experience significant setup and toil to mock those dependencies. Whether mocking interactions between microservices or mocking APIs to infrastructure tools like storage and queues, it’s time consuming and difficult to create realistic test and developer environments that have all the working interactions that you’d expect of production.\n\nIn the future, IDPs will improve our environments to make them efficient and reliable. We’ll evolve beyond basic emulation of our dependencies and instead seamlessly mock external dependencies with minimal configuration.\n\nThere won’t be a question of whether something will work in production, because when it works on my machine, you’ll know it’ll also work in production.\n\n##### **:: Scalability and Provisioning**\n\nThough we don’t like to admit it, many organizations still manually scale environments to accommodate changes in load—either the upcoming expected load around a new feature launch or surprise loads coming from unexpected use or nefarious attacks.\n\nFuture IDPs will fully automate this process, integrating cloud tools, load forecasts, and real-time monitoring.\n\nFor example, in the future, we can expect IDPs to use predictive algorithms to anticipate increased (or decreased) load and scale environments accordingly. As another example, ephemeral environments can start up with the right amount of resourcing needed for their use. Environments used for simple testing will have minimal resources, while load test environments will have enough oomph to replicate what we’d see on a node in production.\n\nDo you want to tie this into expected cost calculators and better reporting? Sure, why not! Teams will be able to track their infrastructure spend and receive alerts when trends indicate that they’ll exceed their budgeted amounts. Did your team forget to scale down environments after some load testing? No problem. They’ll soon get an email or a Slack message to their team channel so they can make changes as needed.\n\nUltimately, anything we’re still thinking through as engineers can be automated with the right context and expectations.\n\n##### **:: Production Bug Replication**\n\nProduction bugs can be difficult to fix. Sometimes it’s hard to understand what’s really going on in our distributed systems. Also, replicating the bug locally can be close to impossible if we don’t fully know why it’s occurring.\n\nWith the right IDP integrations, we won’t just have tools like [Sentry](https://sentry.io/welcome/) or [BugSnag](https://www.bugsnag.com/) notifying teams of production exceptions. We’ll integrate AI components to provide context and potential fixes.\n\nAnd if that doesn’t do it, the IDP can stand up an environment emulating that particular defect. In fact, if you’re using Release in production and can use that data to seed other environments, you could do this today.\n\nTo make it even smoother for the development team, they’ll have that environment hooked up to their IDE, so they can quickly debug and test potential fixes, all with little overhead.\n\n‍\n\n![](/blog-images/b6c5189463e3f8b268d37a82beb19edd.png)\n\n##### **:: Performance Optimization**\n\nNext, let’s talk about performance.\n\nWe’re still in a distributed environment where fully understanding performance bottlenecks and constraints can be difficult.\n\nIn the future, IDPs can integrate components that provide performance analysis throughout the dev workflow. We’ll be able to empower developers with tools to optimize performance during development and before we get to production.\n\nUsing ephemeral environments, you’ll be able to quickly generate production-like traffic and use cases, enabling rapid testing and validation of functionality.\n\nImagine a scenario where an IDP not only measures code performance but also simulates realistic user interactions and data loads. Developers can proactively identify and address performance bottlenecks early in the development process.\n\n#### 🤝 Increased Collaboration Between Disciplines\n\nOne difficulty in almost any organization involves creating highly coupled collaboration between disciplines. A lot of this comes from different workflows, ways of working, and tools.\n\nIn the future, IDPs will bridge the gap between disciplines, fostering seamless collaboration by integrating more than just developer tools. IDPs will enable communication across the organization of the right data at the right time, similar to our body’s complex nervous system.\n\n##### **:: Integration with UX**\n\nTo improve collaboration with UX, IDPs will soon integrate with UX tools like Figma and Miro, providing developers with easy access to validate workflows and UX for their work without hunting for flows and images.\n\nOnce deployed, UX can receive automated notifications and links to ephemeral environments to validate the customer experience.\n\n##### **:: Integration With Product Management**\n\nIDPs will integrate more closely with product management tools as well, providing real-time status updates that encompass not only pull requests but also projections on work completion. This integration will enable better estimations based on historical data.\n\nProduct managers can make more informed decisions based on real-time data.\n\n##### **:: Data Science and Data Warehouse Integration**\n\nRight now, whenever production schemas are updated, there’s often a separate task for either developers or data engineers to sync up the schemas between production and the data warehouse.\n\nIf this isn’t automated in any way within the workflow, we end up with missing data in our data warehouse, and our models fall out of sync.\n\nFuture IDPs will automate data engineering and data warehouse tasks by efficiently funneling changes from developers to the data warehouse. They’ll completely automate some tasks while also signaling for help when automation isn’t enough.\n\nThis automation will reduce the manual effort required to update schemas in response to code changes.\n\n‍\n\n![](/blog-images/feebcbb9804b731e8c323d0500297237.png)\n\n### Where Do We Go From Here?\n\nFor developers, complexity will continue to increase in our environments. At the same time, developers will need to expand the knowledge they need to work with that complexity.\n\nIDPs that follow the current trends will continue to simplify development workflows either through AI capabilities or through adding spokes on the IDP wheel of components. These IDPs will develop into an all-encompassing and ever-changing platform based on development needs.\n\nWhat does that mean for us?\n\nIf your organization already has a robust internal developer platform in place, you can begin expanding into future trends by experimenting with the ideas above.\n\nOn the other hand, if your organization is just beginning its IDP journey, it’s not too late. In fact, it’s a great time to get started. In order to capitalize on the changes headed our way, we need to begin building a solid base of automation today.\n\nEither way, our priority should direct our efforts toward improving our IDPs to solve our most pressing problems today while leaving room for future integrations.\n\n‍*This post was written by Sylvia Fronczak.* [_Sylvia_](https://sylviafronczak.com/) _is a software developer who has worked in various industries with various software methodologies. She’s currently focused on design practices that the whole team can own, understand, and evolve over time._\n",
          "code": "var Component=(()=>{var h=Object.create;var a=Object.defineProperty;var m=Object.getOwnPropertyDescriptor;var p=Object.getOwnPropertyNames;var u=Object.getPrototypeOf,g=Object.prototype.hasOwnProperty;var w=(t,e)=>()=>(e||t((e={exports:{}}).exports,e),e.exports),f=(t,e)=>{for(var i in e)a(t,i,{get:e[i],enumerable:!0})},s=(t,e,i,r)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let o of p(e))!g.call(t,o)&&o!==i&&a(t,o,{get:()=>e[o],enumerable:!(r=m(e,o))||r.enumerable});return t};var v=(t,e,i)=>(i=t!=null?h(u(t)):{},s(e||!t||!t.__esModule?a(i,\"default\",{value:t,enumerable:!0}):i,t)),y=t=>s(a({},\"__esModule\",{value:!0}),t);var d=w((P,l)=>{l.exports=_jsx_runtime});var x={};f(x,{default:()=>I,frontmatter:()=>b});var n=v(d()),b={title:\"IDP Trends: Where Do We Go From Here\",summary:\"Explore how IDPs will shape the future of software development by looking at current and anticipating future trends.\",publishDate:\"Tue Oct 31 2023 20:50:09 GMT+0000 (Coordinated Universal Time)\",author:\"sylvia-fronczak\",readingTime:10,categories:[\"platform-engineering\",\"ai\",\"product\"],mainImage:\"/blog-images/841354692fb08046d7d58b3b71d7cb1d.jpg\",imageAlt:\"DALL-E genrated image\",showCTA:!0,ctaCopy:\"Empower your dev teams with AI-driven automation and streamlined workflows using Release's environment management platform.\",ctaLink:\"https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=idp-trends-where-do-we-go-from-here\",relatedPosts:[\"\"],ogImage:\"/blog-images/841354692fb08046d7d58b3b71d7cb1d.jpg\",excerpt:\"Explore how IDPs will shape the future of software development by looking at current and anticipating future trends.\",tags:[\"platform-engineering\",\"ai\",\"product\"],ctaButton:\"Try Release for Free\"};function c(t){let e=Object.assign({p:\"p\",a:\"a\",h3:\"h3\",span:\"span\",h4:\"h4\",img:\"img\",h5:\"h5\",strong:\"strong\",em:\"em\"},t.components);return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsxs)(e.p,{children:[\"We\\u2019ve shared a lot about \",(0,n.jsx)(e.a,{href:\"https://release.com/blog/what-is-an-internal-developer-platform-and-why-should-i-have-one\",children:\"Internal Developer Platforms (IDPs)\"}),\", \",(0,n.jsx)(e.a,{href:\"https://release.com/blog/components-of-a-successful-idp-build-a-product-your-developers-actually-want-to-use\",children:\"what they can provide\"}),\", and \",(0,n.jsx)(e.a,{href:\"https://release.com/blog/use-product-thinking-to-establish-your-idp\",children:\"how we should build them\"}),\". As they exist today, simple IDPs focus on improving dev workflows to make our development teams more efficient.\"]}),`\n`,(0,n.jsx)(e.p,{children:\"But what should we expect in the future? Will further developments and patterns in IDPs provide even more efficiencies? Will new opportunities open up to integrate additional tools that simplify product development?\"}),`\n`,(0,n.jsx)(e.p,{children:\"In short, yes!\"}),`\n`,(0,n.jsx)(e.p,{children:\"The more organizations start building and using IDPs to enhance workflows, the more development teams will expect IDPs to automate the toil, and the more benefits we\\u2019ll gain.\"}),`\n`,(0,n.jsx)(e.p,{children:\"In order for you to stay on top of trends and continue to provide your developers with a competitive advantage, creating effective IDPs will become more critical than ever.\"}),`\n`,(0,n.jsx)(e.p,{children:\"In this post, we\\u2019ll explore how IDPs will shape the future of software development by looking at existing trends and envisioning what else IDPs can bring to our teams.\"}),`\n`,(0,n.jsx)(e.p,{children:\"To kick it off, let\\u2019s look at some of the biggest trends in software development today.\"}),`\n`,(0,n.jsxs)(e.h3,{id:\"what-existing-software-trends-will-continue\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#what-existing-software-trends-will-continue\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"What Existing Software Trends Will Continue?\"]}),`\n`,(0,n.jsx)(e.p,{children:\"To determine what will happen with IDPs, we must first consider the environment our developers work in. By following development trends, we can shape how IDPs need to expand to meet future needs.\"}),`\n`,(0,n.jsxs)(e.h4,{id:\"\\uFE0F-increased-complexity-in-distributed-systems\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#\\uFE0F-increased-complexity-in-distributed-systems\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"\\u2699\\uFE0F Increased Complexity in Distributed Systems\"]}),`\n`,(0,n.jsx)(e.p,{children:\"First, we can expect software development to continue to grow in complexity. Distributed systems will continue to grow and become even more distributed.\"}),`\n`,(0,n.jsx)(e.p,{children:\"Why is that?\"}),`\n`,(0,n.jsx)(e.p,{children:\"To start, organizations will continue to utilize microservices and serverless systems as they attempt to compartmentalize their products\\u2019 complexities within well-defined boundaries. Even monolithic tech stacks don\\u2019t provide much value within a vacuum and require other systems and services to function.\"}),`\n`,(0,n.jsx)(e.p,{children:\"With more services to manage and integrate, dev teams face more and more complexity in developing, testing, debugging, and understanding their systems. To simplify that complexity through automation, IDPs will continue to streamline their day-to-day activities.\"}),`\n`,(0,n.jsxs)(e.h4,{id:\"reliance-on-ai\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#reliance-on-ai\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"\\u{1F916}Reliance on AI\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Next, let\\u2019s talk about our growing dependence and reliance on AI. We should consider this reliance to be a positive trend, and not one that takes the purity of software development away from our developers. Organizations increasingly leverage AI to enhance their software products, from automating repetitive tasks to optimizing performance. IDPs can harness the power of AI to augment the developer experience significantly.\"}),`\n`,(0,n.jsx)(e.p,{children:\"Using AI as part of software development is the next step in taking tedious or basic tasks and simplifying software development. Similar to when developers all moved to IDEs, organizations are embracing tools to keep their development orgs efficient and productive.\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"Tools like \",(0,n.jsx)(e.a,{href:\"https://github.com/features/copilot\",children:\"Copilot\"}),\", \",(0,n.jsx)(e.a,{href:\"https://www.tabnine.com/\",children:\"Tabnine\"}),\", and \",(0,n.jsx)(e.a,{href:\"https://aws.amazon.com/codewhisperer/\",children:\"CodeWhisperer\"}),\" will grow in functionality, and new competitors will join the market. New tools like \",(0,n.jsx)(e.a,{href:\"https://release.ai/\",children:\"release.ai\"}),\" will take those capabilities a step further, allowing development teams to interact with not just their code but also their infrastructure in a smooth and natural fashion. Teams already use tools like ChatGPT to automate the writing of documentation. Forward-thinking orgs will integrate these functionalities throughout the developer workflows through the IDP. Handwritten wikis and instructions will be replaced by AI-driven tools that provide the latest information with considerably less searching.\"]}),`\n`,(0,n.jsxs)(e.h4,{id:\"-establishment-of-platform-teams\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#-establishment-of-platform-teams\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"\\u{1F6A7} Establishment of Platform Teams\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Finally, more organizations recognize the importance of dedicated platform teams responsible for delivering IDPs and integrating the various components.\"}),`\n`,(0,n.jsx)(e.p,{children:\"These internal product teams build, test, market, and sell your IDP to your development organization. They play a vital role in providing developers with the tools they need to excel in their work.\"}),`\n`,(0,n.jsx)(e.p,{children:\"The trend of building these platform teams will continue as their work leads to more efficient and productive development workflows.\"}),`\n`,(0,n.jsx)(e.p,{children:\"\\u200D\"}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/93831edeae61e83d317056347566c418.png\",alt:\"\"})}),`\n`,(0,n.jsxs)(e.h3,{id:\"what-idp-trends-will-we-see\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#what-idp-trends-will-we-see\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"What IDP Trends Will We See?\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Keeping in mind the development trends mentioned above, where should we expect IDPs to grow next?\"}),`\n`,(0,n.jsxs)(e.h4,{id:\"-more-ai-integrations\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#-more-ai-integrations\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"\\u{1F9BE} More AI Integrations\"]}),`\n`,(0,n.jsx)(e.p,{children:\"As we already hinted, incorporating AI within IDPs will become more pervasive, improving the developer experience in several ways. Let\\u2019s take a look at a few of those:\"}),`\n`,(0,n.jsxs)(e.h5,{id:\"-ai-written-documentation\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#-ai-written-documentation\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.strong,{children:\":: AI-Written Documentation\"})]}),`\n`,(0,n.jsx)(e.p,{children:\"AI will generate comprehensive documentation for internal services, development practices, and standards. This documentation will always be up to date, ensuring developers can access the latest information without hours of searching wikis, docs, and internal organization sites.\"}),`\n`,(0,n.jsx)(e.p,{children:\"These docs, integrated throughout the developer workflow, will give our development teams the context they need when they need it.\"}),`\n`,(0,n.jsx)(e.p,{children:\"Eventually, this AI-written documentation will extend to externally facing docs. At first, individuals will still review these docs, ensuring that the information is appropriate for external audiences. But eventually, with the right prompts and guardrails, you won\\u2019t need to spend human time and resources on documentation at all.\"}),`\n`,(0,n.jsxs)(e.h5,{id:\"-automated-code-reviews\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#-automated-code-reviews\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.strong,{children:\":: Automated Code Reviews\"})]}),`\n`,(0,n.jsx)(e.p,{children:\"We already have some automated code checking through our IDEs and code linters. But is that it? Can we expect more?\"}),`\n`,(0,n.jsx)(e.p,{children:\"Of course we can! AI-driven code review tools will further our capabilities, using context and `information about the entire system to point out potential flaws or to improve readability.\"}),`\n`,(0,n.jsx)(e.p,{children:\"This will also help identify bugs faster and suggest potential fixes.\"}),`\n`,(0,n.jsxs)(e.h4,{id:\"-improved-environments\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#-improved-environments\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"\\u{1F30E} Improved Environments\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Next, let\\u2019s talk about environments. The future of IDPs will drive improvements in developer and production environments, making development more efficient and responsive to real-world scenarios.\"}),`\n`,(0,n.jsxs)(e.h5,{id:\"-emulating-external-dependencies\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#-emulating-external-dependencies\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.strong,{children:\":: Emulating External Dependencies\"})]}),`\n`,(0,n.jsx)(e.p,{children:\"Due to our growing complexities, our systems depend more on other tools, data stores, and APIs.\"}),`\n`,(0,n.jsx)(e.p,{children:\"Right now, developers who require complex external dependencies to validate changes experience significant setup and toil to mock those dependencies. Whether mocking interactions between microservices or mocking APIs to infrastructure tools like storage and queues, it\\u2019s time consuming and difficult to create realistic test and developer environments that have all the working interactions that you\\u2019d expect of production.\"}),`\n`,(0,n.jsx)(e.p,{children:\"In the future, IDPs will improve our environments to make them efficient and reliable. We\\u2019ll evolve beyond basic emulation of our dependencies and instead seamlessly mock external dependencies with minimal configuration.\"}),`\n`,(0,n.jsx)(e.p,{children:\"There won\\u2019t be a question of whether something will work in production, because when it works on my machine, you\\u2019ll know it\\u2019ll also work in production.\"}),`\n`,(0,n.jsxs)(e.h5,{id:\"-scalability-and-provisioning\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#-scalability-and-provisioning\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.strong,{children:\":: Scalability and Provisioning\"})]}),`\n`,(0,n.jsx)(e.p,{children:\"Though we don\\u2019t like to admit it, many organizations still manually scale environments to accommodate changes in load\\u2014either the upcoming expected load around a new feature launch or surprise loads coming from unexpected use or nefarious attacks.\"}),`\n`,(0,n.jsx)(e.p,{children:\"Future IDPs will fully automate this process, integrating cloud tools, load forecasts, and real-time monitoring.\"}),`\n`,(0,n.jsx)(e.p,{children:\"For example, in the future, we can expect IDPs to use predictive algorithms to anticipate increased (or decreased) load and scale environments accordingly. As another example, ephemeral environments can start up with the right amount of resourcing needed for their use. Environments used for simple testing will have minimal resources, while load test environments will have enough oomph to replicate what we\\u2019d see on a node in production.\"}),`\n`,(0,n.jsx)(e.p,{children:\"Do you want to tie this into expected cost calculators and better reporting? Sure, why not! Teams will be able to track their infrastructure spend and receive alerts when trends indicate that they\\u2019ll exceed their budgeted amounts. Did your team forget to scale down environments after some load testing? No problem. They\\u2019ll soon get an email or a Slack message to their team channel so they can make changes as needed.\"}),`\n`,(0,n.jsx)(e.p,{children:\"Ultimately, anything we\\u2019re still thinking through as engineers can be automated with the right context and expectations.\"}),`\n`,(0,n.jsxs)(e.h5,{id:\"-production-bug-replication\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#-production-bug-replication\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.strong,{children:\":: Production Bug Replication\"})]}),`\n`,(0,n.jsx)(e.p,{children:\"Production bugs can be difficult to fix. Sometimes it\\u2019s hard to understand what\\u2019s really going on in our distributed systems. Also, replicating the bug locally can be close to impossible if we don\\u2019t fully know why it\\u2019s occurring.\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"With the right IDP integrations, we won\\u2019t just have tools like \",(0,n.jsx)(e.a,{href:\"https://sentry.io/welcome/\",children:\"Sentry\"}),\" or \",(0,n.jsx)(e.a,{href:\"https://www.bugsnag.com/\",children:\"BugSnag\"}),\" notifying teams of production exceptions. We\\u2019ll integrate AI components to provide context and potential fixes.\"]}),`\n`,(0,n.jsx)(e.p,{children:\"And if that doesn\\u2019t do it, the IDP can stand up an environment emulating that particular defect. In fact, if you\\u2019re using Release in production and can use that data to seed other environments, you could do this today.\"}),`\n`,(0,n.jsx)(e.p,{children:\"To make it even smoother for the development team, they\\u2019ll have that environment hooked up to their IDE, so they can quickly debug and test potential fixes, all with little overhead.\"}),`\n`,(0,n.jsx)(e.p,{children:\"\\u200D\"}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/b6c5189463e3f8b268d37a82beb19edd.png\",alt:\"\"})}),`\n`,(0,n.jsxs)(e.h5,{id:\"-performance-optimization\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#-performance-optimization\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.strong,{children:\":: Performance Optimization\"})]}),`\n`,(0,n.jsx)(e.p,{children:\"Next, let\\u2019s talk about performance.\"}),`\n`,(0,n.jsx)(e.p,{children:\"We\\u2019re still in a distributed environment where fully understanding performance bottlenecks and constraints can be difficult.\"}),`\n`,(0,n.jsx)(e.p,{children:\"In the future, IDPs can integrate components that provide performance analysis throughout the dev workflow. We\\u2019ll be able to empower developers with tools to optimize performance during development and before we get to production.\"}),`\n`,(0,n.jsx)(e.p,{children:\"Using ephemeral environments, you\\u2019ll be able to quickly generate production-like traffic and use cases, enabling rapid testing and validation of functionality.\"}),`\n`,(0,n.jsx)(e.p,{children:\"Imagine a scenario where an IDP not only measures code performance but also simulates realistic user interactions and data loads. Developers can proactively identify and address performance bottlenecks early in the development process.\"}),`\n`,(0,n.jsxs)(e.h4,{id:\"-increased-collaboration-between-disciplines\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#-increased-collaboration-between-disciplines\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"\\u{1F91D} Increased Collaboration Between Disciplines\"]}),`\n`,(0,n.jsx)(e.p,{children:\"One difficulty in almost any organization involves creating highly coupled collaboration between disciplines. A lot of this comes from different workflows, ways of working, and tools.\"}),`\n`,(0,n.jsx)(e.p,{children:\"In the future, IDPs will bridge the gap between disciplines, fostering seamless collaboration by integrating more than just developer tools. IDPs will enable communication across the organization of the right data at the right time, similar to our body\\u2019s complex nervous system.\"}),`\n`,(0,n.jsxs)(e.h5,{id:\"-integration-with-ux\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#-integration-with-ux\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.strong,{children:\":: Integration with UX\"})]}),`\n`,(0,n.jsx)(e.p,{children:\"To improve collaboration with UX, IDPs will soon integrate with UX tools like Figma and Miro, providing developers with easy access to validate workflows and UX for their work without hunting for flows and images.\"}),`\n`,(0,n.jsx)(e.p,{children:\"Once deployed, UX can receive automated notifications and links to ephemeral environments to validate the customer experience.\"}),`\n`,(0,n.jsxs)(e.h5,{id:\"integration-with-product-management\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#integration-with-product-management\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.strong,{children:\"::\\xA0Integration With Product Management\"})]}),`\n`,(0,n.jsx)(e.p,{children:\"IDPs will integrate more closely with product management tools as well, providing real-time status updates that encompass not only pull requests but also projections on work completion. This integration will enable better estimations based on historical data.\"}),`\n`,(0,n.jsx)(e.p,{children:\"Product managers can make more informed decisions based on real-time data.\"}),`\n`,(0,n.jsxs)(e.h5,{id:\"-data-science-and-data-warehouse-integration\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#-data-science-and-data-warehouse-integration\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.strong,{children:\":: Data Science and Data Warehouse Integration\"})]}),`\n`,(0,n.jsx)(e.p,{children:\"Right now, whenever production schemas are updated, there\\u2019s often a separate task for either developers or data engineers to sync up the schemas between production and the data warehouse.\"}),`\n`,(0,n.jsx)(e.p,{children:\"If this isn\\u2019t automated in any way within the workflow, we end up with missing data in our data warehouse, and our models fall out of sync.\"}),`\n`,(0,n.jsx)(e.p,{children:\"Future IDPs will automate data engineering and data warehouse tasks by efficiently funneling changes from developers to the data warehouse. They\\u2019ll completely automate some tasks while also signaling for help when automation isn\\u2019t enough.\"}),`\n`,(0,n.jsx)(e.p,{children:\"This automation will reduce the manual effort required to update schemas in response to code changes.\"}),`\n`,(0,n.jsx)(e.p,{children:\"\\u200D\"}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/feebcbb9804b731e8c323d0500297237.png\",alt:\"\"})}),`\n`,(0,n.jsxs)(e.h3,{id:\"where-do-we-go-from-here\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#where-do-we-go-from-here\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Where Do We Go From Here?\"]}),`\n`,(0,n.jsx)(e.p,{children:\"For developers, complexity will continue to increase in our environments. At the same time, developers will need to expand the knowledge they need to work with that complexity.\"}),`\n`,(0,n.jsx)(e.p,{children:\"IDPs that follow the current trends will continue to simplify development workflows either through AI capabilities or through adding spokes on the IDP wheel of components. These IDPs will develop into an all-encompassing and ever-changing platform based on development needs.\"}),`\n`,(0,n.jsx)(e.p,{children:\"What does that mean for us?\"}),`\n`,(0,n.jsx)(e.p,{children:\"If your organization already has a robust internal developer platform in place, you can begin expanding into future trends by experimenting with the ideas above.\"}),`\n`,(0,n.jsx)(e.p,{children:\"On the other hand, if your organization is just beginning its IDP journey, it\\u2019s not too late. In fact, it\\u2019s a great time to get started. In order to capitalize on the changes headed our way, we need to begin building a solid base of automation today.\"}),`\n`,(0,n.jsx)(e.p,{children:\"Either way, our priority should direct our efforts toward improving our IDPs to solve our most pressing problems today while leaving room for future integrations.\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"\\u200D\",(0,n.jsx)(e.em,{children:\"This post was written by Sylvia Fronczak.\"}),\" \",(0,n.jsx)(e.a,{href:\"https://sylviafronczak.com/\",children:(0,n.jsx)(e.em,{children:\"Sylvia\"})}),\" \",(0,n.jsx)(e.em,{children:\"is a software developer who has worked in various industries with various software methodologies. She\\u2019s currently focused on design practices that the whole team can own, understand, and evolve over time.\"})]})]})}function k(t={}){let{wrapper:e}=t.components||{};return e?(0,n.jsx)(e,Object.assign({},t,{children:(0,n.jsx)(c,t)})):c(t)}var I=k;return y(x);})();\n;return Component;"
        },
        "_id": "blog/posts/idp-trends-where-do-we-go-from-here.mdx",
        "_raw": {
          "sourceFilePath": "blog/posts/idp-trends-where-do-we-go-from-here.mdx",
          "sourceFileName": "idp-trends-where-do-we-go-from-here.mdx",
          "sourceFileDir": "blog/posts",
          "contentType": "mdx",
          "flattenedPath": "blog/posts/idp-trends-where-do-we-go-from-here"
        },
        "type": "BlogPost",
        "computedSlug": "idp-trends-where-do-we-go-from-here"
      },
      "documentHash": "1739393595021",
      "hasWarnings": false,
      "documentTypeName": "BlogPost"
    },
    "blog/posts/improve-developer-velocity-with-ephemeral-environments.mdx": {
      "document": {
        "title": "5 Ways To Improve Developer Velocity Using Ephemeral Environments",
        "summary": "Learn how to measure/improve developer velocity",
        "publishDate": "Thu Feb 25 2021 01:44:25 GMT+0000 (Coordinated Universal Time)",
        "author": "tommy-mcclung",
        "readingTime": 6,
        "categories": [
          "platform-engineering",
          "product"
        ],
        "mainImage": "/blog-images/134546b8e70e75351244e2451cfd891d.jpg",
        "imageAlt": "Two athletes cycling in high speed representing the developer velocity",
        "showCTA": true,
        "ctaCopy": "Improve developer velocity with Release's ephemeral environments to eliminate staging bottlenecks and boost collaboration and testing efficiency.",
        "ctaLink": "https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=improve-developer-velocity-with-ephemeral-environments",
        "relatedPosts": [
          ""
        ],
        "ogImage": "/blog-images/134546b8e70e75351244e2451cfd891d.jpg",
        "excerpt": "Learn how to measure/improve developer velocity",
        "tags": [
          "platform-engineering",
          "product"
        ],
        "ctaButton": "Try Release for Free",
        "body": {
          "raw": "\n###\n\n### What is Developer/Development Velocity?\n\nVelocity is a measurement of how many story points a software development team can finish within a sprint (usually one or two weeks). These points are set by the software development team when they review a ticket and estimate how complex the ticket is. When a team measures this output over a period of time, generally they have a consistent amount of story points they can deliver in a sprint and their velocity is known.\n\nImproving developer velocity is directly correlated with performance. [McKinsey published an article in April 2020](https://www.mckinsey.com/industries/technology-media-and-telecommunications/our-insights/developer-velocity-how-software-excellence-fuels-business-performance), where they cite that companies in the top 25% on their Developer Velocity Index grow up to twice as fast as companies in their same industries. Intuitively this makes sense since delivering more allows the development team to learn through iterating and improving.\n\nOne might argue that velocity alone doesn’t make for great software, but assuming a development team is aware that quality is important, one can see how velocity usually helps. The ability to deliver quickly also allows a development team to address quality issues quickly. It’s easy to argue that development teams with high velocity have the ability to deliver better quality software because they can address issues quickly.\n\nIn the same study, McKinsey highlighted several factors that allow a software development team to move quickly. Specifically they highlight that Technology Tools are an incredibly important dimension to velocity and business outcomes. And the most important tools are: Planning, Collaboration, Development and DevOps tools.\n\nIn this post I’m going to discuss the **top 5 ways Ephemeral Environments can improve developer velocity** by touching on how they are a _Collaboration_, _Development_ and _DevOps_ tool. As we’ve spoken about in our article [“What is an Ephemeral Environment?”](https://releasehub.com/ephemeral-environments), ephemeral environments are spun up on demand and contain the code and data that approximates production closely. These environments are used by development teams in the software development process to test, debug and ensure features are built correctly before code is pushed to production.\n\n### Here are the top 5 ways ephemeral environments can be used to improve developer velocity\n\n#### 1\\. Ephemeral environments are a DevOps tool designed to remove the staging or QA environment bottleneck\n\nTraditional pre-production ecosystems usually have a limited amount of environments for developers. The staging or QA environment is generally used as a step before production where all code is merged and tested. Most organizations have one or very few of these environments, so as the organization grows these environments become a bottleneck in the process as all code must be tested here before production.\n\n![Example of ephemeral environments for each branch](/blog-images/5e902c6fb781bd57f662c7a4d0133f69.png)\n\nWith ephemeral environments, the traditional idea of “staging” is gone. Every feature branch is contained in its own isolated environment and becomes its own integration environment. There is no longer a need to have a single testing and integration environment where all code must merge before going to production. With ephemeral environments you have a limitless supply of environments for any purpose.\n\n#### 2\\. Ephemeral environments are a collaboration tool designed to allow for “early and often” feedback\n\nFeedback is the lifeblood of great products. If you’ve ever read [Andy Grove’s book on high output management](https://www.amazon.com/High-Output-Management-Andrew-Grove/dp/0679762884/), you know he does an amazing job of discussing how rework is so costly. If you haven’t read this book, I highly recommend it, even if all you read are the first few chapters where he discusses trying to cook a high quality egg repeatedly, in under three minutes. In summary, Andy suggested through this analogy that finding issues/defects early in the egg cooking process is the most important part of consistently cooking a high quality egg in under three minutes.\n\nLikewise in software development, getting feedback and finding quality issues early in the development cycle reduces costly rework and improves velocity. If a product is delivered to a customer that doesn’t work or has bugs, it has to be reworked and go through the entire process again. Or if a product manager or designer doesn’t have a way to see changes until an engineer is finished with development, there is a high likelihood they will spot something wrong and rework the solution. These are all examples of rotten eggs in the process that hamper developer velocity.\n\nWith ephemeral environments, rework can be minimized because stakeholders become a part of the development process. When an ephemeral environment is created, URLs to the environment are created so stakeholders can see progress while code is being developed.\n\n![Links in the PR](/blog-images/8fd67650916dd6a7aade9567716cc61b.png)\n\nAt [Release](https://releasehub.com/), we highly recommend to our customers that they create a PR as soon as a developer starts working on a feature so a Release Ephemeral Environment is automatically created. When the developer pushes code to their source control system, the environment is updated making it a live reflection of the feature during development. Product managers, designers and QA are automatically notified when changes are live and they can preview those changes and give feedback immediately.\n\nAt Release, we will also share our own ephemeral environments with our customers as we’re building a feature so we can get feedback directly from the people we’re making the software for before we release it to production.\n\n#### 3\\. Ephemeral environments can limit rework and thus increase developer velocity.\n\nEphemeral environments are a developer tool that allows for full integration and smoke testing on isolated features\n\nTraditional continuous integration (CI) is the idea that your developer process should constantly be testing as a developer pushes code. What this leaves out many times is that most CI systems only perform unit tests continuously. Unit tests are meant to test small units of code and not the entire system as a whole. Integration and Smoke tests are where full paths of user experience can be tested. Usually Integration and Smoke tests are left to be tested only when the code makes its way via a merge to the mainline code branch and a traditional staging environment.\n\nAgain, if we refer back to Andy Grove’s three minute egg analogy, this step of running Integration and Smoke tests only when the code branch is merged to the mainline is extremely late in the process. If issues are found during Integration and/or Smoke tests, the developer has to start the development cycle again from the beginning after finding this issue too late in the process.\n\nTo add to the issue, if a team only has a single staging environment, the bottleneck around this staging environment is exacerbated with developers waiting for Integration and Smoke tests to be run on this single environment. On top of this, many code changes/features/branches may have been a part of the mainline merge making finding the cause of failed Integration/Smoke tests difficult and time consuming.\n\nWith ephemeral environments, Integration and Smoke tests can be run when the ephemeral environment is created for a feature branch. This ensures that Integration and Smoke tests are run as frequently as unit tests so developers can find issues early in the process. Additionally, Integration and Smoke tests run against a single feature change/branch will isolate changes against the mainline and make finding the root cause much easier.\n\n#### 4\\. Ephemeral environments are a DevOps tool that allow for experimentation with infrastructure\n\nMaking changes to infrastructure is hard and when a developer introduces the need for an infrastructure change it’s costly in time across the board. In a traditional environment setup (without ephemeral environments) this will result in an overall slow down in developer velocity as the shared staging environments must be updated by the DevOps team so the developer has some place to test their changes and new infrastructure.\n\n![Experiment with environment configuration](/blog-images/be638a1797eb4614f6894a2aa9c6e102.png)\n\nWith ephemeral environments, this testing can be done in isolation and does not impact any other developer. For instance, with Release Ephemeral Environments, a developer can add services, environment variables, new infrastructure dependencies, new datasets/databases on their own through use of environment templates (environments as code) to experiment and develop without interfering with any other developers work or environments. This results in higher developer velocity again through minimization of rework and bottlenecks on shared resources.\n\n#### 5\\. Ephemeral environments are a collaboration tool designed to be an agile/scrum catalyst\n\nMany organizations have made the move to Agile/Scrum but their infrastructure and technology haven’t adapted to support a more iterative approach to building software. The entire premise of Agile/Scrum is for teams to be empowered and driven by early and often feedback. If your organization is on Agile/Scrum and you’re still using a single or few staging environments, you’re technologically hampering your process improvements. Ephemeral environments are the homes and office buildings where agile teams live, work, build, and play.\n\nEphemeral environments are a catalyst to the Agile/Scrum methodology. When a developer does a pull request the ephemeral environment is created and collaboration on the feature can begin. The team is free to iterate, share, nand solicit feedback all while keeping the rest of the organization freely moving with their own ephemeral environments. Stakeholders are a part of the development process and true customer driven development, which is the heart of the Agile/Scrum methodology, can occur.\n\n### Conclusion\n\nEphemeral environments turbo charge development velocity by eliminating bottlenecks in the process (DevOps Tool), including stakeholders in the process (Collaboration Tool) and improving product quality (Developer Tool). All of these factors were highlighted in the McKinsey report on developer velocity as critical and ephemeral environments are _an investment that will put your organization in the top 25%_.\n\nPhoto by [Maico Amorim](https://unsplash.com/@maicoamorim?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) on [Unsplash](https://unsplash.com/@maicoamorim?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText).\n\n### Additional Resources\n\n- [Increase Developer Velocity by Removing Environment Bottlenecks](https://releasehub.com/blog/increase-developer-velocity-by-removing-environment-bottlenecks)\n- [What is an Ephemeral Environment?](https://releasehub.com/ephemeral-environments)\n- [What is a staging Environment?](https://releasehub.com/staging-environments)\n\n‍\n",
          "code": "var Component=(()=>{var d=Object.create;var i=Object.defineProperty;var m=Object.getOwnPropertyDescriptor;var p=Object.getOwnPropertyNames;var u=Object.getPrototypeOf,g=Object.prototype.hasOwnProperty;var v=(t,e)=>()=>(e||t((e={exports:{}}).exports,e),e.exports),f=(t,e)=>{for(var o in e)i(t,o,{get:e[o],enumerable:!0})},s=(t,e,o,r)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let a of p(e))!g.call(t,a)&&a!==o&&i(t,a,{get:()=>e[a],enumerable:!(r=m(e,a))||r.enumerable});return t};var y=(t,e,o)=>(o=t!=null?d(u(t)):{},s(e||!t||!t.__esModule?i(o,\"default\",{value:t,enumerable:!0}):o,t)),w=t=>s(i({},\"__esModule\",{value:!0}),t);var h=v((E,l)=>{l.exports=_jsx_runtime});var T={};f(T,{default:()=>I,frontmatter:()=>b});var n=y(h()),b={title:\"5 Ways To Improve Developer Velocity Using Ephemeral Environments\",summary:\"Learn how to measure/improve developer velocity\",publishDate:\"Thu Feb 25 2021 01:44:25 GMT+0000 (Coordinated Universal Time)\",author:\"tommy-mcclung\",readingTime:6,categories:[\"platform-engineering\",\"product\"],mainImage:\"/blog-images/134546b8e70e75351244e2451cfd891d.jpg\",imageAlt:\"Two athletes cycling in high speed representing the developer velocity\",showCTA:!0,ctaCopy:\"Improve developer velocity with Release's ephemeral environments to eliminate staging bottlenecks and boost collaboration and testing efficiency.\",ctaLink:\"https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=improve-developer-velocity-with-ephemeral-environments\",relatedPosts:[\"\"],ogImage:\"/blog-images/134546b8e70e75351244e2451cfd891d.jpg\",excerpt:\"Learn how to measure/improve developer velocity\",tags:[\"platform-engineering\",\"product\"],ctaButton:\"Try Release for Free\"};function c(t){let e=Object.assign({h3:\"h3\",a:\"a\",span:\"span\",p:\"p\",strong:\"strong\",em:\"em\",h4:\"h4\",img:\"img\",ul:\"ul\",li:\"li\"},t.components);return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(e.h3,{id:\"\"}),`\n`,(0,n.jsxs)(e.h3,{id:\"what-is-developerdevelopment-velocity\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#what-is-developerdevelopment-velocity\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"What is Developer/Development Velocity?\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Velocity is a measurement of how many story points a software development team can finish within a sprint (usually one or two weeks). These points are set by the software development team when they review a ticket and estimate how complex the ticket is. When a team measures this output over a period of time, generally they have a consistent amount of story points they can deliver in a sprint and their velocity is known.\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"Improving developer velocity is directly correlated with performance. \",(0,n.jsx)(e.a,{href:\"https://www.mckinsey.com/industries/technology-media-and-telecommunications/our-insights/developer-velocity-how-software-excellence-fuels-business-performance\",children:\"McKinsey published an article in April 2020\"}),\", where they cite that companies in the top 25% on their Developer Velocity Index grow up to twice as fast as companies in their same industries. Intuitively this makes sense since delivering more allows the development team to learn through iterating and improving.\"]}),`\n`,(0,n.jsx)(e.p,{children:\"One might argue that velocity alone doesn\\u2019t make for great software, but assuming a development team is aware that quality is important, one can see how velocity usually helps. The ability to deliver quickly also allows a development team to address quality issues quickly. It\\u2019s easy to argue that development teams with high velocity have the ability to deliver better quality software because they can address issues quickly.\"}),`\n`,(0,n.jsx)(e.p,{children:\"In the same study, McKinsey highlighted several factors that allow a software development team to move quickly. Specifically they highlight that Technology Tools are an incredibly important dimension to velocity and business outcomes. And the most important tools are: Planning, Collaboration, Development and DevOps tools.\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"In this post I\\u2019m going to discuss the \",(0,n.jsx)(e.strong,{children:\"top 5 ways Ephemeral Environments can improve developer velocity\"}),\" by touching on how they are a \",(0,n.jsx)(e.em,{children:\"Collaboration\"}),\", \",(0,n.jsx)(e.em,{children:\"Development\"}),\" and \",(0,n.jsx)(e.em,{children:\"DevOps\"}),\" tool. As we\\u2019ve spoken about in our article \",(0,n.jsx)(e.a,{href:\"https://releasehub.com/ephemeral-environments\",children:\"\\u201CWhat is an Ephemeral Environment?\\u201D\"}),\", ephemeral environments are spun up on demand and contain the code and data that approximates production closely. These environments are used by development teams in the software development process to test, debug and ensure features are built correctly before code is pushed to production.\"]}),`\n`,(0,n.jsxs)(e.h3,{id:\"here-are-the-top-5-ways-ephemeral-environments-can-be-used-to-improve-developer-velocity\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#here-are-the-top-5-ways-ephemeral-environments-can-be-used-to-improve-developer-velocity\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Here are the top 5 ways ephemeral environments can be used to improve developer velocity\"]}),`\n`,(0,n.jsxs)(e.h4,{id:\"1-ephemeral-environments-are-a-devops-tool-designed-to-remove-the-staging-or-qa-environment-bottleneck\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#1-ephemeral-environments-are-a-devops-tool-designed-to-remove-the-staging-or-qa-environment-bottleneck\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"1. Ephemeral environments are a DevOps tool designed to remove the staging or QA environment bottleneck\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Traditional pre-production ecosystems usually have a limited amount of environments for developers. The staging or QA environment is generally used as a step before production where all code is merged and tested. Most organizations have one or very few of these environments, so as the organization grows these environments become a bottleneck in the process as all code must be tested here before production.\"}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/5e902c6fb781bd57f662c7a4d0133f69.png\",alt:\"Example of ephemeral environments for each branch\"})}),`\n`,(0,n.jsx)(e.p,{children:\"With ephemeral environments, the traditional idea of \\u201Cstaging\\u201D is gone. Every feature branch is contained in its own isolated environment and becomes its own integration environment. There is no longer a need to have a single testing and integration environment where all code must merge before going to production. With ephemeral environments you have a limitless supply of environments for any purpose.\"}),`\n`,(0,n.jsxs)(e.h4,{id:\"2-ephemeral-environments-are-a-collaboration-tool-designed-to-allow-for-early-and-often-feedback\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#2-ephemeral-environments-are-a-collaboration-tool-designed-to-allow-for-early-and-often-feedback\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"2. Ephemeral environments are a collaboration tool designed to allow for \\u201Cearly and often\\u201D feedback\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"Feedback is the lifeblood of great products. If you\\u2019ve ever read \",(0,n.jsx)(e.a,{href:\"https://www.amazon.com/High-Output-Management-Andrew-Grove/dp/0679762884/\",children:\"Andy Grove\\u2019s book on high output management\"}),\", you know he does an amazing job of discussing how rework is so costly. If you haven\\u2019t read this book, I highly recommend it, even if all you read are the first few chapters where he discusses trying to cook a high quality egg repeatedly, in under three minutes. In summary, Andy suggested through this analogy that finding issues/defects early in the egg cooking process is the most important part of consistently cooking a high quality egg in under three minutes.\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Likewise in software development, getting feedback and finding quality issues early in the development cycle reduces costly rework and improves velocity. If a product is delivered to a customer that doesn\\u2019t work or has bugs, it has to be reworked and go through the entire process again. Or if a product manager or designer doesn\\u2019t have a way to see changes until an engineer is finished with development, there is a high likelihood they will spot something wrong and rework the solution. These are all examples of rotten eggs in the process that hamper developer velocity.\"}),`\n`,(0,n.jsx)(e.p,{children:\"With ephemeral environments, rework can be minimized because stakeholders become a part of the development process. When an ephemeral environment is created, URLs to the environment are created so stakeholders can see progress while code is being developed.\"}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/8fd67650916dd6a7aade9567716cc61b.png\",alt:\"Links in the PR\"})}),`\n`,(0,n.jsxs)(e.p,{children:[\"At \",(0,n.jsx)(e.a,{href:\"https://releasehub.com/\",children:\"Release\"}),\", we highly recommend to our customers that they create a PR as soon as a developer starts working on a feature so a Release Ephemeral Environment is automatically created. When the developer pushes code to their source control system, the environment is updated making it a live reflection of the feature during development. Product managers, designers and QA are automatically notified when changes are live and they can preview those changes and give feedback immediately.\"]}),`\n`,(0,n.jsx)(e.p,{children:\"At Release, we will also share our own ephemeral environments with our customers as we\\u2019re building a feature so we can get feedback directly from the people we\\u2019re making the software for before we release it to production.\"}),`\n`,(0,n.jsxs)(e.h4,{id:\"3-ephemeral-environments-can-limit-rework-and-thus-increase-developer-velocity\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#3-ephemeral-environments-can-limit-rework-and-thus-increase-developer-velocity\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"3. Ephemeral environments can limit rework and thus increase developer velocity.\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Ephemeral environments are a developer tool that allows for full integration and smoke testing on isolated features\"}),`\n`,(0,n.jsx)(e.p,{children:\"Traditional continuous integration (CI) is the idea that your developer process should constantly be testing as a developer pushes code. What this leaves out many times is that most CI systems only perform unit tests continuously. Unit tests are meant to test small units of code and not the entire system as a whole. Integration and Smoke tests are where full paths of user experience can be tested. Usually Integration and Smoke tests are left to be tested only when the code makes its way via a merge to the mainline code branch and a traditional staging environment.\"}),`\n`,(0,n.jsx)(e.p,{children:\"Again, if we refer back to Andy Grove\\u2019s three minute egg analogy, this step of running Integration and Smoke tests only when the code branch is merged to the mainline is extremely late in the process. If issues are found during Integration and/or Smoke tests, the developer has to start the development cycle again from the beginning after finding this issue too late in the process.\"}),`\n`,(0,n.jsx)(e.p,{children:\"To add to the issue, if a team only has a single staging environment, the bottleneck around this staging environment is exacerbated with developers waiting for Integration and Smoke tests to be run on this single environment. On top of this, many code changes/features/branches may have been a part of the mainline merge making finding the cause of failed Integration/Smoke tests difficult and time consuming.\"}),`\n`,(0,n.jsx)(e.p,{children:\"With ephemeral environments, Integration and Smoke tests can be run when the ephemeral environment is created for a feature branch. This ensures that Integration and Smoke tests are run as frequently as unit tests so developers can find issues early in the process. Additionally, Integration and Smoke tests run against a single feature change/branch will isolate changes against the mainline and make finding the root cause much easier.\"}),`\n`,(0,n.jsxs)(e.h4,{id:\"4-ephemeral-environments-are-a-devops-tool-that-allow-for-experimentation-with-infrastructure\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#4-ephemeral-environments-are-a-devops-tool-that-allow-for-experimentation-with-infrastructure\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"4. Ephemeral environments are a DevOps tool that allow for experimentation with infrastructure\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Making changes to infrastructure is hard and when a developer introduces the need for an infrastructure change it\\u2019s costly in time across the board. In a traditional environment setup (without ephemeral environments) this will result in an overall slow down in developer velocity as the shared staging environments must be updated by the DevOps team so the developer has some place to test their changes and new infrastructure.\"}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/be638a1797eb4614f6894a2aa9c6e102.png\",alt:\"Experiment with environment configuration\"})}),`\n`,(0,n.jsx)(e.p,{children:\"With ephemeral environments, this testing can be done in isolation and does not impact any other developer. For instance, with Release Ephemeral Environments, a developer can add services, environment variables, new infrastructure dependencies, new datasets/databases on their own through use of environment templates (environments as code) to experiment and develop without interfering with any other developers work or environments. This results in higher developer velocity again through minimization of rework and bottlenecks on shared resources.\"}),`\n`,(0,n.jsxs)(e.h4,{id:\"5-ephemeral-environments-are-a-collaboration-tool-designed-to-be-an-agilescrum-catalyst\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#5-ephemeral-environments-are-a-collaboration-tool-designed-to-be-an-agilescrum-catalyst\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"5. Ephemeral environments are a collaboration tool designed to be an agile/scrum catalyst\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Many organizations have made the move to Agile/Scrum but their infrastructure and technology haven\\u2019t adapted to support a more iterative approach to building software. The entire premise of Agile/Scrum is for teams to be empowered and driven by early and often feedback. If your organization is on Agile/Scrum and you\\u2019re still using a single or few staging environments, you\\u2019re technologically hampering your process improvements. Ephemeral environments are the homes and office buildings where agile teams live, work, build, and play.\"}),`\n`,(0,n.jsx)(e.p,{children:\"Ephemeral environments are a catalyst to the Agile/Scrum methodology. When a developer does a pull request the ephemeral environment is created and collaboration on the feature can begin. The team is free to iterate, share, nand solicit feedback all while keeping the rest of the organization freely moving with their own ephemeral environments. Stakeholders are a part of the development process and true customer driven development, which is the heart of the Agile/Scrum methodology, can occur.\"}),`\n`,(0,n.jsxs)(e.h3,{id:\"conclusion\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#conclusion\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Conclusion\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"Ephemeral environments turbo charge development velocity by eliminating bottlenecks in the process (DevOps Tool), including stakeholders in the process (Collaboration Tool) and improving product quality (Developer Tool). All of these factors were highlighted in the McKinsey report on developer velocity as critical and ephemeral environments are \",(0,n.jsx)(e.em,{children:\"an investment that will put your organization in the top 25%\"}),\".\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"Photo by \",(0,n.jsx)(e.a,{href:\"https://unsplash.com/@maicoamorim?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText\",children:\"Maico Amorim\"}),\" on \",(0,n.jsx)(e.a,{href:\"https://unsplash.com/@maicoamorim?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText\",children:\"Unsplash\"}),\".\"]}),`\n`,(0,n.jsxs)(e.h3,{id:\"additional-resources\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#additional-resources\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Additional Resources\"]}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsx)(e.li,{children:(0,n.jsx)(e.a,{href:\"https://releasehub.com/blog/increase-developer-velocity-by-removing-environment-bottlenecks\",children:\"Increase Developer Velocity by Removing Environment Bottlenecks\"})}),`\n`,(0,n.jsx)(e.li,{children:(0,n.jsx)(e.a,{href:\"https://releasehub.com/ephemeral-environments\",children:\"What is an Ephemeral Environment?\"})}),`\n`,(0,n.jsx)(e.li,{children:(0,n.jsx)(e.a,{href:\"https://releasehub.com/staging-environments\",children:\"What is a staging Environment?\"})}),`\n`]}),`\n`,(0,n.jsx)(e.p,{children:\"\\u200D\"})]})}function k(t={}){let{wrapper:e}=t.components||{};return e?(0,n.jsx)(e,Object.assign({},t,{children:(0,n.jsx)(c,t)})):c(t)}var I=k;return w(T);})();\n;return Component;"
        },
        "_id": "blog/posts/improve-developer-velocity-with-ephemeral-environments.mdx",
        "_raw": {
          "sourceFilePath": "blog/posts/improve-developer-velocity-with-ephemeral-environments.mdx",
          "sourceFileName": "improve-developer-velocity-with-ephemeral-environments.mdx",
          "sourceFileDir": "blog/posts",
          "contentType": "mdx",
          "flattenedPath": "blog/posts/improve-developer-velocity-with-ephemeral-environments"
        },
        "type": "BlogPost",
        "computedSlug": "improve-developer-velocity-with-ephemeral-environments"
      },
      "documentHash": "1739393595021",
      "hasWarnings": false,
      "documentTypeName": "BlogPost"
    },
    "blog/posts/improving-developer-productivity-with-ephemeral-environments.mdx": {
      "document": {
        "title": "Improving Developer Productivity with Ephemeral Environments",
        "summary": "Release Environments can help improve Developer productivity.",
        "publishDate": "Thu Nov 11 2021 00:52:43 GMT+0000 (Coordinated Universal Time)",
        "author": "sam-allen",
        "readingTime": 2,
        "categories": [
          "platform-engineering",
          "product"
        ],
        "mainImage": "/blog-images/5d511bd72f482e77dce37dd7c48e36a7.jpg",
        "imageAlt": "A car production line gaining productivity creating a ephemeral environment",
        "showCTA": true,
        "ctaCopy": "Improve developer productivity with on-demand environments like Lean Manufacturing principles. Streamline workflows with Release.com.",
        "ctaLink": "https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=improving-developer-productivity-with-ephemeral-environments",
        "relatedPosts": [
          ""
        ],
        "ogImage": "/blog-images/5d511bd72f482e77dce37dd7c48e36a7.jpg",
        "excerpt": "Release Environments can help improve Developer productivity.",
        "tags": [
          "platform-engineering",
          "product"
        ],
        "ctaButton": "Try Release for Free",
        "body": {
          "raw": "\n### Introduction\n\nThere seems to be an insatiable demand for software; indeed there is an app for everything these days. In many ways, modern software development is more akin to a continuous manufacturing process, where products are frequently produced and shipped. Software companies are constantly iterating on new product features. User expectations demand constant improvements ranging from bug fixes to expanded product offerings.\n\nThe Developer process for _HOW_ software is shipped is vital for maintaining a competitive edge. For example, having access to rapid-prototyping test environments is a critical process that can greatly enhance the overall Developer Experience. **_At Release, our mission is to enable companies to get their best ideas to the world quickly by helping them produce consistent, reliable, and plentiful Environments on demand._**\n\nAdditionally, having an overall improvement strategy for streamlining the Developer Experience is crucial for eliminating any internal friction that may arise. There are many different approaches for conducting process improvement, ranging from formal methodologies to informal ad hoc projects. All approaches are valid and can add value depending on your company culture. In recent years, the Lean Manufacturing philosophy has become a popular approach for analyzing and improving internal work processes.\n\n### Brief Lean Manufacturing History\n\nPrior to pivoting my career toward software development, I lived another life as an industrial engineer facilitating Lean Manufacturing projects and workshops where the goal was to leverage employee ingenuity to identify and creatively eliminate any waste in a process.\n\nThe Lean Manufacturing philosophy has a rich history that can be traced back to Walter Shewart’s statistical quality control methods at Bell Laboratories in the early 20th century. W. Edward Deming learned and enhanced these techniques from Shewart. After WWII, Deming was called upon to help rebuild Japanese industry and he championed this thought-process of continuous improvement through statistical analysis. Companies like Toyota embraced this approach and continued to enhance it where it gradually morphed into the Lean philosophy that many of us are familiar with today.\n\nLean Manufacturing can now be found in other non-manufacturing sectors such as healthcare, banking, government, software development, etc. Essentially, all work consists of a “process” that can be continually improved upon, regardless of the product or work environment. Continuous improvement enables the employee to be more successful by standardizing or automating routines, eliminating blockers, and harvesting worker ideas for improving their own work. Many software companies do this naturally and they may or may not call it “kaizen”, which is the Japanese word for continuous improvement. There are agile scrum retrospectives where teams debrief on how their last Sprint went to see whether they can improve the overall developer experience. However a team approaches it, continuous improvement cannot be avoided.\n\n### How Release Can Help\n\nAt many tech organizations, there is still the common bottleneck problem of generating test environments. Software companies typically employ a DevOps team to produce such environments which can be costly to maintain and frequently break down. Any shared resource can be a pain point for both large and small companies that are trying to rapidly ship new features. Developer teams must wait around for testing environments to become available or argue over priorities and who can utilize the environment.\n**_Our entire mission at Release is to enable companies to get their best ideas to the world quickly by helping them produce consistent, reliable, and plentiful Environments on demand._** This is a huge kaizen improvement opportunity that can greatly improve the overall Developer Experience by eliminating frustrating downtime and improving quality, throughput, and morale. If you wish to get your organization set up with automated Environments, let us know how we can help you accomplish your mission faster and easier.\n\nPhoto by [Lenny Kuhne](https://unsplash.com/@lennykuhne?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) on [Unsplash](https://unsplash.com/s/photos/manufacturing?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)\n\n‍\n",
          "code": "var Component=(()=>{var p=Object.create;var r=Object.defineProperty;var m=Object.getOwnPropertyDescriptor;var d=Object.getOwnPropertyNames;var u=Object.getPrototypeOf,g=Object.prototype.hasOwnProperty;var f=(t,e)=>()=>(e||t((e={exports:{}}).exports,e),e.exports),y=(t,e)=>{for(var a in e)r(t,a,{get:e[a],enumerable:!0})},s=(t,e,a,i)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let o of d(e))!g.call(t,o)&&o!==a&&r(t,o,{get:()=>e[o],enumerable:!(i=m(e,o))||i.enumerable});return t};var v=(t,e,a)=>(a=t!=null?p(u(t)):{},s(e||!t||!t.__esModule?r(a,\"default\",{value:t,enumerable:!0}):a,t)),w=t=>s(r({},\"__esModule\",{value:!0}),t);var l=f((D,c)=>{c.exports=_jsx_runtime});var _={};y(_,{default:()=>x,frontmatter:()=>b});var n=v(l()),b={title:\"Improving Developer Productivity with Ephemeral Environments\",summary:\"Release Environments can help improve Developer productivity.\",publishDate:\"Thu Nov 11 2021 00:52:43 GMT+0000 (Coordinated Universal Time)\",author:\"sam-allen\",readingTime:2,categories:[\"platform-engineering\",\"product\"],mainImage:\"/blog-images/5d511bd72f482e77dce37dd7c48e36a7.jpg\",imageAlt:\"A car production line gaining productivity creating a ephemeral environment\",showCTA:!0,ctaCopy:\"Improve developer productivity with on-demand environments like Lean Manufacturing principles. Streamline workflows with Release.com.\",ctaLink:\"https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=improving-developer-productivity-with-ephemeral-environments\",relatedPosts:[\"\"],ogImage:\"/blog-images/5d511bd72f482e77dce37dd7c48e36a7.jpg\",excerpt:\"Release Environments can help improve Developer productivity.\",tags:[\"platform-engineering\",\"product\"],ctaButton:\"Try Release for Free\"};function h(t){let e=Object.assign({h3:\"h3\",a:\"a\",span:\"span\",p:\"p\",em:\"em\",strong:\"strong\"},t.components);return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsxs)(e.h3,{id:\"introduction\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#introduction\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Introduction\"]}),`\n`,(0,n.jsx)(e.p,{children:\"There seems to be an insatiable demand for software; indeed there is an app for everything these days. In many ways, modern software development is more akin to a continuous manufacturing process, where products are frequently produced and shipped. Software companies are constantly iterating on new product features. User expectations demand constant improvements ranging from bug fixes to expanded product offerings.\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"The Developer process for \",(0,n.jsx)(e.em,{children:\"HOW\"}),\" software is shipped is vital for maintaining a competitive edge. For example, having access to rapid-prototyping test environments is a critical process that can greatly enhance the overall Developer Experience. \",(0,n.jsx)(e.strong,{children:(0,n.jsx)(e.em,{children:\"At Release, our mission is to enable companies to get their best ideas to the world quickly by helping them produce consistent, reliable, and plentiful Environments on demand.\"})})]}),`\n`,(0,n.jsx)(e.p,{children:\"Additionally, having an overall improvement strategy for streamlining the Developer Experience is crucial for eliminating any internal friction that may arise. There are many different approaches for conducting process improvement, ranging from formal methodologies to informal ad hoc projects. All approaches are valid and can add value depending on your company culture. In recent years, the Lean Manufacturing philosophy has become a popular approach for analyzing and improving internal work processes.\"}),`\n`,(0,n.jsxs)(e.h3,{id:\"brief-lean-manufacturing-history\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#brief-lean-manufacturing-history\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Brief Lean Manufacturing History\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Prior to pivoting my career toward software development, I lived another life as an industrial engineer facilitating Lean Manufacturing projects and workshops where the goal was to leverage employee ingenuity to identify and creatively eliminate any waste in a process.\"}),`\n`,(0,n.jsx)(e.p,{children:\"The Lean Manufacturing philosophy has a rich history that can be traced back to Walter Shewart\\u2019s statistical quality control methods at Bell Laboratories in the early 20th century. W. Edward Deming learned and enhanced these techniques from Shewart. After WWII, Deming was called upon to help rebuild Japanese industry and he championed this thought-process of continuous improvement through statistical analysis. Companies like Toyota embraced this approach and continued to enhance it where it gradually morphed into the Lean philosophy that many of us are familiar with today.\"}),`\n`,(0,n.jsx)(e.p,{children:\"Lean Manufacturing can now be found in other non-manufacturing sectors such as healthcare, banking, government, software development, etc. Essentially, all work consists of a \\u201Cprocess\\u201D that can be continually improved upon, regardless of the product or work environment. Continuous improvement enables the employee to be more successful by standardizing or automating routines, eliminating blockers, and harvesting worker ideas for improving their own work. Many software companies do this naturally and they may or may not call it \\u201Ckaizen\\u201D, which is the Japanese word for continuous improvement. There are agile scrum retrospectives where teams debrief on how their last Sprint went to see whether they can improve the overall developer experience. However a team approaches it, continuous improvement cannot be avoided.\"}),`\n`,(0,n.jsxs)(e.h3,{id:\"how-release-can-help\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#how-release-can-help\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"How Release Can Help\"]}),`\n`,(0,n.jsxs)(e.p,{children:[`At many tech organizations, there is still the common bottleneck problem of generating test environments. Software companies typically employ a DevOps team to produce such environments which can be costly to maintain and frequently break down. Any shared resource can be a pain point for both large and small companies that are trying to rapidly ship new features. Developer teams must wait around for testing environments to become available or argue over priorities and who can utilize the environment.\n`,(0,n.jsx)(e.strong,{children:(0,n.jsx)(e.em,{children:\"Our entire mission at Release is to enable companies to get their best ideas to the world quickly by helping them produce consistent, reliable, and plentiful Environments on demand.\"})}),\" This is a huge kaizen improvement opportunity that can greatly improve the overall Developer Experience by eliminating frustrating downtime and improving quality, throughput, and morale. If you wish to get your organization set up with automated Environments, let us know how we can help you accomplish your mission faster and easier.\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"Photo by \",(0,n.jsx)(e.a,{href:\"https://unsplash.com/@lennykuhne?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText\",children:\"Lenny Kuhne\"}),\" on \",(0,n.jsx)(e.a,{href:\"https://unsplash.com/s/photos/manufacturing?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText\",children:\"Unsplash\"})]}),`\n`,(0,n.jsx)(e.p,{children:\"\\u200D\"})]})}function k(t={}){let{wrapper:e}=t.components||{};return e?(0,n.jsx)(e,Object.assign({},t,{children:(0,n.jsx)(h,t)})):h(t)}var x=k;return w(_);})();\n;return Component;"
        },
        "_id": "blog/posts/improving-developer-productivity-with-ephemeral-environments.mdx",
        "_raw": {
          "sourceFilePath": "blog/posts/improving-developer-productivity-with-ephemeral-environments.mdx",
          "sourceFileName": "improving-developer-productivity-with-ephemeral-environments.mdx",
          "sourceFileDir": "blog/posts",
          "contentType": "mdx",
          "flattenedPath": "blog/posts/improving-developer-productivity-with-ephemeral-environments"
        },
        "type": "BlogPost",
        "computedSlug": "improving-developer-productivity-with-ephemeral-environments"
      },
      "documentHash": "1739393595022",
      "hasWarnings": false,
      "documentTypeName": "BlogPost"
    },
    "blog/posts/increase-developer-velocity-by-removing-environment-bottlenecks.mdx": {
      "document": {
        "title": "Increase Developer Velocity by Removing Environment Bottlenecks",
        "summary": "In our latest whitepaper, we show how to increase developer velocity by 35% using Environments as a Service.",
        "publishDate": "Wed May 05 2021 23:48:15 GMT+0000 (Coordinated Universal Time)",
        "author": "sam-allen",
        "readingTime": 9,
        "categories": [
          "platform-engineering",
          "product"
        ],
        "mainImage": "/blog-images/9819c0cd0c1e2104e9678b4ff5ffe5ec.jpg",
        "imageAlt": "Workflow of development environment with Increase on Developer Velocity by Removing Environment Bottlenecks",
        "showCTA": true,
        "ctaCopy": "Unlock developer velocity by instantly creating isolated environments for precise testing and faster bug resolution.",
        "ctaLink": "https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=increase-developer-velocity-by-removing-environment-bottlenecks",
        "relatedPosts": [
          ""
        ],
        "ogImage": "/blog-images/9819c0cd0c1e2104e9678b4ff5ffe5ec.jpg",
        "excerpt": "In our latest whitepaper, we show how to increase developer velocity by 35% using Environments as a Service.",
        "tags": [
          "platform-engineering",
          "product"
        ],
        "ctaButton": "Try Release for Free",
        "body": {
          "raw": "\n### Remove Environment Bottlenecks\n\nWe’ve all heard the phrase “time is money” and we intuitively know this statement to be true, but understanding just _how_ much money is spent on labor can be a tricky thing to estimate. This is especially true with complex operations like software development.  Before I learned how to write code in React/Node JS, I was an industrial engineer for many years and spent time studying this topic at university.\n\nIndustrial engineering is a systems-thinking discipline that is obsessed with figuring out how to optimize resources and improve processes to get the most out of a system. It borrows from other fields like economics, project management, mechanical engineering, and statistics, to name a few, and lies at the intersection between business operations and engineering.  Quality, Cost, Schedule, and Safety can all be measured and quantified with incremental improvements made across each category.\n\nThese topics can be easy to grapple with when dealing with a consistent, repeatable process like a manufacturing assembly line, hospital queue, or restaurant. However, wrestling with non-standard operations like software development can be nebulous, abstract, and difficult to shove into a one-type-fits-all solution. But that doesn’t mean that we shouldn’t attempt to understand it.  Any attempt at understanding and gathering data is still incrementally better than remaining ignorant and relying on gut-intuition alone.\n\n### The Problem\n\nAs I started to learn how to write software applications a couple years ago, I had high hopes of perhaps crossing my industrial engineering and project management skills into the realm of software.  Gradually, as I began to understand the Agile/Scrum approach, I realized it is challenging to estimate computer programming labor resources and it’s not a very good planning approach, especially in a start-up culture.\n\nYou don’t have a blueprint, there are no bills of materials, there is no work breakdown structure or sequence of operations.  Instead, it’s better to deal with chunks of hazy ranges, like “well, it could take a day or two, but less than a week” and then iterate toward a solution, biting off smaller chunks at a time.  Precedence is still knowable in many cases and you can break the problem into smaller pieces, but estimating _how_ long it will take is not really worth figuring out because it doesn’t help you gain any ground toward solving the problem. Time estimation is purely an administrative task that will need to be repeated _ad infinitum_ because no two tickets are ever the same.\n\nSoftware development can have many unknowns which further complicates any attempts at labor estimation.  The ‘Johari Window’ is a method for identifying known or unknown knowledge that a person and their surrounding organization may possess. Some things fall into the ‘known-unknown’ category which means you need to research something that you don’t know yet. But even worse, the ‘unknown-unknown’ realm often crops up, which is to say that you have no idea what is going on until you dive in and start to uncover hidden things.\n\nIn software development, especially when trying something new and novel—like in a startup—there are many unknowns.  Pioneering into uncharted areas takes an extra amount of time and effort when building a greenfield product, fixing bugs, doing user research, discovering go-to-market fit, and so on.  As a company matures, some software development and ticket refinement might approach a stable steady-state, but in many cases, if the company continues to innovate, there will always be many unknowns.\n\n### The Solution\n\nSo what are we supposed to do?  We know that software development labor is costly, and in fact, is often the top operating cost for a tech company. It’s important to investigate and attempt to understand _how_ labor is allocated so we can begin to feel more confident about what we are willing to build or not.  There are important decisions that many managers face: should we build something in-house using our own labor resources?  Or can we get something off-the-shelf that can be customized to fit our needs?  It would also be good to know whether a manager’s most precious resource is blocked with bottlenecks and being under- or over-utilized.  Having a rough understanding of your labor resources can help make this type of decision much easier.\n\nOne of the best ways to understand a complex system is to model it.  We see this all the time when we watch the weather report on the news when the reporter stands in front of a weather map and gives a rough forecast using a computer simulation.  Statistical modeling is now used in a variety of complex industries to make planning forecasts with many different input variables.\n\nIt’s important to know that a model is just that: a simulation, a mock-up, an imaginary scenario.  It’s not _real_, just ask Morpheus in the Matrix.  Every statistical model relies heavily upon baseline assumptions and measured, knowable, controlled inputs that can be adjusted for a range of possible outcomes.  The more data you have, the more reliable the model becomes, but it has to start somewhere with a simplified version of reality broken up into discrete events built upon statistical averages.  \n\nSo what are some of the safe assumptions we can make about a typical tech startup?  First, we would want to add boundaries to our system.  We can fix the number of employees and their typical working hours.  Tickets tend to vary widely, but it is possible to make different ticket types broken down into difficulty levels.  Let’s say the easy ones are half a day, while the tougher ones might take a week.  We also know the number of environments we have available for testing our code.  These might be custom built, maybe there are 2 or 3.\n\nSome other knowable assumptions might be how long it takes to deploy our code to production and whether a certain amount of tickets will need rework after QA testing, let’s say 25%.  It’s rather arbitrary, but in the absence of solid data, we can plug in some intuitive anecdotal numbers to start with.  If you hold all things consistent, but only adjust one variable at a time, then you can begin to compare the results to uncover any major bottlenecks in the system.  Models are a simplified version of reality, so we start really simply.  To use a crude example, we could simulate 5 farmers working 8 hrs/day in a 500 acre field using 3 tractors, record what happens, then run it again with only 2 tractors instead for comparison.\n\n### Simulation Results\n\nIn this blog post, we will share the end results of our analysis. For a full detail of the simulation setup and results, you can download our free [whitepaper](https://release.com/whitepaper).\n\n> _Key takeaway: If we increase the number of available environments to 5 while keeping all other variables consistent, we saw the simulated throughput go from 12 to 39 tickets, more than a threefold increase._\n\nAfter running the baseline setup with only a single staging environment, we can see the team can only complete 12 out of 42 tickets, but more importantly, we can identify a major bottleneck as 23 tickets are piled up waiting for an environment resource and waiting to pass thru the testing process. If we increase the number of available environments to 5 while keeping all other variables consistent, we saw the simulated throughput went up to 39 tickets, more than a threefold increase. The bottleneck has also been eliminated and there are even a few surplus environments available.\n\nAgain, to read the full results and analysis, be sure to download the free [whitepaper](https://release.com/whitepaper).\n",
          "code": "var Component=(()=>{var d=Object.create;var i=Object.defineProperty;var m=Object.getOwnPropertyDescriptor;var u=Object.getOwnPropertyNames;var p=Object.getPrototypeOf,g=Object.prototype.hasOwnProperty;var w=(n,e)=>()=>(e||n((e={exports:{}}).exports,e),e.exports),f=(n,e)=>{for(var a in e)i(n,a,{get:e[a],enumerable:!0})},r=(n,e,a,s)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let o of u(e))!g.call(n,o)&&o!==a&&i(n,o,{get:()=>e[o],enumerable:!(s=m(e,o))||s.enumerable});return n};var b=(n,e,a)=>(a=n!=null?d(p(n)):{},r(e||!n||!n.__esModule?i(a,\"default\",{value:n,enumerable:!0}):a,n)),y=n=>r(i({},\"__esModule\",{value:!0}),n);var h=w((T,l)=>{l.exports=_jsx_runtime});var j={};f(j,{default:()=>I,frontmatter:()=>v});var t=b(h()),v={title:\"Increase Developer Velocity by Removing Environment Bottlenecks\",summary:\"In our latest whitepaper, we show how to increase developer velocity by 35% using Environments as a Service.\",publishDate:\"Wed May 05 2021 23:48:15 GMT+0000 (Coordinated Universal Time)\",author:\"sam-allen\",readingTime:9,categories:[\"platform-engineering\",\"product\"],mainImage:\"/blog-images/9819c0cd0c1e2104e9678b4ff5ffe5ec.jpg\",imageAlt:\"Workflow of development environment with Increase on Developer Velocity by Removing Environment Bottlenecks\",showCTA:!0,ctaCopy:\"Unlock developer velocity by instantly creating isolated environments for precise testing and faster bug resolution.\",ctaLink:\"https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=increase-developer-velocity-by-removing-environment-bottlenecks\",relatedPosts:[\"\"],ogImage:\"/blog-images/9819c0cd0c1e2104e9678b4ff5ffe5ec.jpg\",excerpt:\"In our latest whitepaper, we show how to increase developer velocity by 35% using Environments as a Service.\",tags:[\"platform-engineering\",\"product\"],ctaButton:\"Try Release for Free\"};function c(n){let e=Object.assign({h3:\"h3\",a:\"a\",span:\"span\",p:\"p\",em:\"em\",blockquote:\"blockquote\"},n.components);return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsxs)(e.h3,{id:\"remove-environment-bottlenecks\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#remove-environment-bottlenecks\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Remove Environment Bottlenecks\"]}),`\n`,(0,t.jsxs)(e.p,{children:[\"We\\u2019ve all heard the phrase \\u201Ctime is money\\u201D and we intuitively know this statement to be true, but understanding just \",(0,t.jsx)(e.em,{children:\"how\"}),\" much money is spent on labor can be a tricky thing to estimate. This is especially true with complex operations like software development.\\xA0 Before I learned how to write code in React/Node JS, I was an industrial engineer for many years and spent time studying this topic at university.\"]}),`\n`,(0,t.jsx)(e.p,{children:\"Industrial engineering is a systems-thinking discipline that is obsessed with figuring out how to optimize resources and improve processes to get the most out of a system. It borrows from other fields like economics, project management, mechanical engineering, and statistics, to name a few, and lies at the intersection between business operations and engineering.\\xA0 Quality, Cost, Schedule, and Safety can all be measured and quantified with incremental improvements made across each category.\"}),`\n`,(0,t.jsx)(e.p,{children:\"These topics can be easy to grapple with when dealing with a consistent, repeatable process like a manufacturing assembly line, hospital queue, or restaurant. However, wrestling with non-standard operations like software development can be nebulous, abstract, and difficult to shove into a one-type-fits-all solution. But that doesn\\u2019t mean that we shouldn\\u2019t attempt to understand it.\\xA0 Any attempt at understanding and gathering data is still incrementally better than remaining ignorant and relying on gut-intuition alone.\"}),`\n`,(0,t.jsxs)(e.h3,{id:\"the-problem\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#the-problem\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"The Problem\"]}),`\n`,(0,t.jsx)(e.p,{children:\"As I started to learn how to write software applications a couple years ago, I had high hopes of perhaps crossing my industrial engineering and project management skills into the realm of software.\\xA0 Gradually, as I began to understand the Agile/Scrum approach, I realized it is challenging to estimate computer programming labor resources and it\\u2019s not a very good planning approach, especially in a start-up culture.\"}),`\n`,(0,t.jsxs)(e.p,{children:[\"You don\\u2019t have a blueprint, there are no bills of materials, there is no work breakdown structure or sequence of operations.\\xA0 Instead, it\\u2019s better to deal with chunks of hazy ranges, like \\u201Cwell, it could take a day or two, but less than a week\\u201D and then iterate toward a solution, biting off smaller chunks at a time.\\xA0 Precedence is still knowable in many cases and you can break the problem into smaller pieces, but estimating \",(0,t.jsx)(e.em,{children:\"how\"}),\" long it will take is not really worth figuring out because it doesn\\u2019t help you gain any ground toward solving the problem. Time estimation is purely an administrative task that will need to be repeated \",(0,t.jsx)(e.em,{children:\"ad infinitum\"}),\" because no two tickets are ever the same.\"]}),`\n`,(0,t.jsx)(e.p,{children:\"Software development can have many unknowns which further complicates any attempts at labor estimation.\\xA0 The \\u2018Johari Window\\u2019 is a method for identifying known or unknown knowledge that a person and their surrounding organization may possess. Some things fall into the \\u2018known-unknown\\u2019 category which means you need to research something that you don\\u2019t know yet. But even worse, the \\u2018unknown-unknown\\u2019 realm often crops up, which is to say that you have no idea what is going on until you dive in and start to uncover hidden things.\"}),`\n`,(0,t.jsx)(e.p,{children:\"In software development, especially when trying something new and novel\\u2014like in a startup\\u2014there are many unknowns.\\xA0 Pioneering into uncharted areas takes an extra amount of time and effort when building a greenfield product, fixing bugs, doing user research, discovering go-to-market fit, and so on.\\xA0 As a company matures, some software development and ticket refinement might approach a stable steady-state, but in many cases, if the company continues to innovate, there will always be many unknowns.\"}),`\n`,(0,t.jsxs)(e.h3,{id:\"the-solution\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#the-solution\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"The Solution\"]}),`\n`,(0,t.jsxs)(e.p,{children:[\"So what are we supposed to do?\\xA0 We know that software development labor is costly, and in fact, is often the top operating cost for a tech company. It\\u2019s important to investigate and attempt to understand \",(0,t.jsx)(e.em,{children:\"how\"}),\" labor is allocated so we can begin to feel more confident about what we are willing to build or not.\\xA0 There are important decisions that many managers face: should we build something in-house using our own labor resources?\\xA0 Or can we get something off-the-shelf that can be customized to fit our needs?\\xA0 It would also be good to know whether a manager\\u2019s most precious resource is blocked with bottlenecks and being under- or over-utilized.\\xA0 Having a rough understanding of your labor resources can help make this type of decision much easier.\"]}),`\n`,(0,t.jsx)(e.p,{children:\"One of the best ways to understand a complex system is to model it.\\xA0 We see this all the time when we watch the weather report on the news when the reporter stands in front of a weather map and gives a rough forecast using a computer simulation.\\xA0 Statistical modeling is now used in a variety of complex industries to make planning forecasts with many different input variables.\"}),`\n`,(0,t.jsxs)(e.p,{children:[\"It\\u2019s important to know that a model is just that: a simulation, a mock-up, an imaginary scenario.\\xA0 It\\u2019s not \",(0,t.jsx)(e.em,{children:\"real\"}),\", just ask Morpheus in the Matrix.\\xA0 Every statistical model relies heavily upon baseline assumptions and measured, knowable, controlled inputs that can be adjusted for a range of possible outcomes.\\xA0 The more data you have, the more reliable the model becomes, but it has to start somewhere with a simplified version of reality broken up into discrete events built upon statistical averages.\\xA0\\xA0\"]}),`\n`,(0,t.jsx)(e.p,{children:\"So what are some of the safe assumptions we can make about a typical tech startup?\\xA0 First, we would want to add boundaries to our system.\\xA0 We can fix the number of employees and their typical working hours.\\xA0 Tickets tend to vary widely, but it is possible to make different ticket types broken down into difficulty levels.\\xA0 Let\\u2019s say the easy ones are half a day, while the tougher ones might take a week.\\xA0 We also know the number of environments we have available for testing our code.\\xA0 These might be custom built, maybe there are 2 or 3.\"}),`\n`,(0,t.jsx)(e.p,{children:\"Some other knowable assumptions might be how long it takes to deploy our code to production and whether a certain amount of tickets will need rework after QA testing, let\\u2019s say 25%.\\xA0 It\\u2019s rather arbitrary, but in the absence of solid data, we can plug in some intuitive anecdotal numbers to start with.\\xA0 If you hold all things consistent, but only adjust one variable at a time, then you can begin to compare the results to uncover any major bottlenecks in the system.\\xA0 Models are a simplified version of reality, so we start really simply.\\xA0 To use a crude example, we could simulate 5 farmers working 8 hrs/day in a 500 acre field using 3 tractors, record what happens, then run it again with only 2 tractors instead for comparison.\"}),`\n`,(0,t.jsxs)(e.h3,{id:\"simulation-results\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#simulation-results\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Simulation Results\"]}),`\n`,(0,t.jsxs)(e.p,{children:[\"In this blog post, we will share the end results of our analysis. For a full detail of the simulation setup and results, you can download our free \",(0,t.jsx)(e.a,{href:\"https://release.com/whitepaper\",children:\"whitepaper\"}),\".\"]}),`\n`,(0,t.jsxs)(e.blockquote,{children:[`\n`,(0,t.jsx)(e.p,{children:(0,t.jsx)(e.em,{children:\"Key takeaway: If we increase the number of available environments to 5 while keeping all other variables consistent, we saw the simulated throughput go from 12 to 39 tickets, more than a threefold increase.\"})}),`\n`]}),`\n`,(0,t.jsx)(e.p,{children:\"After running the baseline setup with only a single staging environment, we can see the team can only complete 12 out of 42 tickets, but more importantly, we can identify a major bottleneck as 23 tickets are piled up waiting for an environment resource and waiting to pass thru the testing process. If we increase the number of available environments to 5 while keeping all other variables consistent, we saw the simulated throughput went up to 39 tickets, more than a threefold increase. The bottleneck has also been eliminated and there are even a few surplus environments available.\"}),`\n`,(0,t.jsxs)(e.p,{children:[\"Again, to read the full results and analysis, be sure to download the free \",(0,t.jsx)(e.a,{href:\"https://release.com/whitepaper\",children:\"whitepaper\"}),\".\"]})]})}function k(n={}){let{wrapper:e}=n.components||{};return e?(0,t.jsx)(e,Object.assign({},n,{children:(0,t.jsx)(c,n)})):c(n)}var I=k;return y(j);})();\n;return Component;"
        },
        "_id": "blog/posts/increase-developer-velocity-by-removing-environment-bottlenecks.mdx",
        "_raw": {
          "sourceFilePath": "blog/posts/increase-developer-velocity-by-removing-environment-bottlenecks.mdx",
          "sourceFileName": "increase-developer-velocity-by-removing-environment-bottlenecks.mdx",
          "sourceFileDir": "blog/posts",
          "contentType": "mdx",
          "flattenedPath": "blog/posts/increase-developer-velocity-by-removing-environment-bottlenecks"
        },
        "type": "BlogPost",
        "computedSlug": "increase-developer-velocity-by-removing-environment-bottlenecks"
      },
      "documentHash": "1739393595022",
      "hasWarnings": false,
      "documentTypeName": "BlogPost"
    },
    "blog/posts/introducing-release-share-a-docker-desktop-extension.mdx": {
      "document": {
        "title": "Introducing Release Share, a Docker Desktop Extension",
        "summary": "Easily share your containers and apps with collaborators and reviewers using Release Share Extension in Docker Desktop",
        "publishDate": "Mon Oct 02 2023 20:30:34 GMT+0000 (Coordinated Universal Time)",
        "author": "ira-casteel",
        "readingTime": 5,
        "categories": [
          "platform-engineering",
          "product"
        ],
        "mainImage": "/blog-images/ad4f0d9a6a9336170e2e4fc9145f03d9.jpg",
        "imageAlt": "Introducing Release Share, a Docker Desktop Extension",
        "showCTA": true,
        "ctaCopy": "Empower faster testing and sharing with Release's ephemeral environments for seamless collaboration and efficient deployment cycles.",
        "ctaLink": "https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=introducing-release-share-a-docker-desktop-extension",
        "relatedPosts": [
          "release-is-going-to-dockercon-2023"
        ],
        "ogImage": "/blog-images/ad4f0d9a6a9336170e2e4fc9145f03d9.jpg",
        "excerpt": "Easily share your containers and apps with collaborators and reviewers using Release Share Extension in Docker Desktop",
        "tags": [
          "platform-engineering",
          "product"
        ],
        "ctaButton": "Try Release for Free",
        "body": {
          "raw": "\nAt Release, we like to move fast and break fewer things. So we use ephemeral environments a lot. They're like playgrounds where we build and test things before moving them to the real world. This way, we can quickly see if something works (or doesn't) and fix it early. And since sharing what we make is a big part of this process, we thought: why not make sharing easier?\n\nSo, we made [Release Share](https://hub.docker.com/extensions/releasecom/docker-extension) for Docker Desktop. It's a simple tool to help you share your containers. Just one click and you get a public URL, which you can even customize if you like.\n\n#### **What Release Share does:**\n\n🔗 **Make Your Own URL**: Change the link name to whatever you like.\n\n▶️ **List Live Containers**: If it's running and has exposed ports, you can share it.\n\n✅ **Simple Sharing**: Send the link to anyone, and they can see your app in their browser\n\n#### **How to Use It:**\n\n**1.** Start Docker Desktop (or download the latest version [here](https://www.docker.com/products/docker-desktop/)).\n\n![](/blog-images/3587b81a0c040cd7e3d82d94f9b6e26e.png)\n**2.** Go to Extensions Marketplace, find [Release Share](https://open.docker.com/extensions/marketplace?extensionId=releasecom/docker-extension), and install it.\n\n![](/blog-images/0b4e8659a9fe8f4c0dc6093012b137bc.png)\n**3.** Click on “Get Started” on the welcome screen (tip: you can always get back to this screen by clicking “About” in the top right corner).\n\n![](/blog-images/d0de679cd4d410b8393ea838ebc4f772.png)\n**4\\.** Release Share looks for your active containers and lists all that have exposed ports and can be shared externally. Choose one, click “Connect” and Release Share will create a private tunnel for your container with a shareable link.\n\nNote that you can also see recent inactive containers by toggling the “Show containers without exposed ports” switch.\n\n![](/blog-images/fb34e83aa126d1490fa4bc4c05112e71.png)\n**5.** The standard URL looks like [https://random-name.rshare.io/](https://random-name.rshare.io/). Copy and share the URL and the recipient can preview your container or app in any browser.\n\n![](/blog-images/84869eefc978f982bf40d7eabf911df5.png)\n\nLike this:\n\n![](/blog-images/5154644d9cab9603ebd8cc818e3d2970.png)\n**6.** Want a different name? You can change it by clicking “Edit”.\n\n![](/blog-images/e4786837bd61e77bca78d40c9251c567.png)\n\nThe system will check if the name is available and assign the new name to your public URL. At this point the old URL becomes disabled and whoever had it will not be able to see the contents of your container or app any more. This comes handy for version control or tagging specific changes or audiences.\n\n![](/blog-images/11ac264b2481ea2e68f091d81f95895e.png)\n\nYour new URL will go live as soon as you hit save.\n\n![](/blog-images/5bd29fbc9332e87699811568a2212cee.png)\n**7.** When you're done sharing, click “Disconnect” to take your container offline. Note, the custom URL is saved for future use, so if you want to share the same container later, it will spin up with the last name used.\n\nThis concludes our quick rundown of Release Share. Give it a try. If you've got suggestions or feedback, we're all ears. Find us at [DockerCon2023](https://www.dockercon.com) or drop us a line.\n",
          "code": "var Component=(()=>{var d=Object.create;var r=Object.defineProperty;var g=Object.getOwnPropertyDescriptor;var p=Object.getOwnPropertyNames;var m=Object.getPrototypeOf,u=Object.prototype.hasOwnProperty;var f=(a,e)=>()=>(e||a((e={exports:{}}).exports,e),e.exports),b=(a,e)=>{for(var o in e)r(a,o,{get:e[o],enumerable:!0})},i=(a,e,o,s)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let t of p(e))!u.call(a,t)&&t!==o&&r(a,t,{get:()=>e[t],enumerable:!(s=g(e,t))||s.enumerable});return a};var w=(a,e,o)=>(o=a!=null?d(m(a)):{},i(e||!a||!a.__esModule?r(o,\"default\",{value:a,enumerable:!0}):o,a)),k=a=>i(r({},\"__esModule\",{value:!0}),a);var l=f((D,c)=>{c.exports=_jsx_runtime});var R={};b(R,{default:()=>v,frontmatter:()=>y});var n=w(l()),y={title:\"Introducing Release Share, a Docker Desktop Extension\",summary:\"Easily share your containers and apps with collaborators and reviewers using Release Share Extension in Docker Desktop\",publishDate:\"Mon Oct 02 2023 20:30:34 GMT+0000 (Coordinated Universal Time)\",author:\"ira-casteel\",readingTime:5,categories:[\"platform-engineering\",\"product\"],mainImage:\"/blog-images/ad4f0d9a6a9336170e2e4fc9145f03d9.jpg\",imageAlt:\"Introducing Release Share, a Docker Desktop Extension\",showCTA:!0,ctaCopy:\"Empower faster testing and sharing with Release's ephemeral environments for seamless collaboration and efficient deployment cycles.\",ctaLink:\"https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=introducing-release-share-a-docker-desktop-extension\",relatedPosts:[\"release-is-going-to-dockercon-2023\"],ogImage:\"/blog-images/ad4f0d9a6a9336170e2e4fc9145f03d9.jpg\",excerpt:\"Easily share your containers and apps with collaborators and reviewers using Release Share Extension in Docker Desktop\",tags:[\"platform-engineering\",\"product\"],ctaButton:\"Try Release for Free\"};function h(a){let e=Object.assign({p:\"p\",a:\"a\",h4:\"h4\",span:\"span\",strong:\"strong\",img:\"img\"},a.components);return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(e.p,{children:\"At Release, we like to move fast and break fewer things. So we use ephemeral environments a lot. They're like playgrounds where we build and test things before moving them to the real world. This way, we can quickly see if something works (or doesn't) and fix it early. And since sharing what we make is a big part of this process, we thought: why not make sharing easier?\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"So, we made \",(0,n.jsx)(e.a,{href:\"https://hub.docker.com/extensions/releasecom/docker-extension\",children:\"Release Share\"}),\" for Docker Desktop. It's a simple tool to help you share your containers. Just one click and you get a public URL, which you can even customize if you like.\"]}),`\n`,(0,n.jsxs)(e.h4,{id:\"what-release-share-does\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#what-release-share-does\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.strong,{children:\"What Release Share does:\"})]}),`\n`,(0,n.jsxs)(e.p,{children:[\"\\u{1F517} \",(0,n.jsx)(e.strong,{children:\"Make Your Own URL\"}),\": Change the link name to whatever you like.\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"\\u25B6\\uFE0F \",(0,n.jsx)(e.strong,{children:\"List Live Containers\"}),\": If it's running and has exposed ports, you can share it.\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"\\u2705 \",(0,n.jsx)(e.strong,{children:\"Simple Sharing\"}),\": Send the link to anyone, and they can see your app in their browser\"]}),`\n`,(0,n.jsxs)(e.h4,{id:\"how-to-use-it\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#how-to-use-it\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.strong,{children:\"How to Use It:\"})]}),`\n`,(0,n.jsxs)(e.p,{children:[(0,n.jsx)(e.strong,{children:\"1.\"}),\" Start Docker Desktop (or download the latest version \",(0,n.jsx)(e.a,{href:\"https://www.docker.com/products/docker-desktop/\",children:\"here\"}),\").\"]}),`\n`,(0,n.jsxs)(e.p,{children:[(0,n.jsx)(e.img,{src:\"/blog-images/3587b81a0c040cd7e3d82d94f9b6e26e.png\",alt:\"\"}),`\n`,(0,n.jsx)(e.strong,{children:\"2.\"}),\" Go to Extensions Marketplace, find \",(0,n.jsx)(e.a,{href:\"https://open.docker.com/extensions/marketplace?extensionId=releasecom/docker-extension\",children:\"Release Share\"}),\", and install it.\"]}),`\n`,(0,n.jsxs)(e.p,{children:[(0,n.jsx)(e.img,{src:\"/blog-images/0b4e8659a9fe8f4c0dc6093012b137bc.png\",alt:\"\"}),`\n`,(0,n.jsx)(e.strong,{children:\"3.\"}),\" Click on \\u201CGet Started\\u201D on the welcome screen (tip: you can always get back to this screen by clicking \\u201CAbout\\u201D in the top right corner).\"]}),`\n`,(0,n.jsxs)(e.p,{children:[(0,n.jsx)(e.img,{src:\"/blog-images/d0de679cd4d410b8393ea838ebc4f772.png\",alt:\"\"}),`\n`,(0,n.jsx)(e.strong,{children:\"4.\"}),\" Release Share looks for your active containers and lists all that have exposed ports and can be shared externally. Choose one, click \\u201CConnect\\u201D and Release Share will create a private tunnel for your container with a shareable link.\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Note that you can also see recent inactive containers by toggling the \\u201CShow containers without exposed ports\\u201D switch.\"}),`\n`,(0,n.jsxs)(e.p,{children:[(0,n.jsx)(e.img,{src:\"/blog-images/fb34e83aa126d1490fa4bc4c05112e71.png\",alt:\"\"}),`\n`,(0,n.jsx)(e.strong,{children:\"5.\"}),\" The standard URL looks like \",(0,n.jsx)(e.a,{href:\"https://random-name.rshare.io/\",children:\"https://random-name.rshare.io/\"}),\". Copy and share the URL and the recipient can preview your container or app in any browser.\"]}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/84869eefc978f982bf40d7eabf911df5.png\",alt:\"\"})}),`\n`,(0,n.jsx)(e.p,{children:\"Like this:\"}),`\n`,(0,n.jsxs)(e.p,{children:[(0,n.jsx)(e.img,{src:\"/blog-images/5154644d9cab9603ebd8cc818e3d2970.png\",alt:\"\"}),`\n`,(0,n.jsx)(e.strong,{children:\"6.\"}),\" Want a different name? You can change it by clicking \\u201CEdit\\u201D.\"]}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/e4786837bd61e77bca78d40c9251c567.png\",alt:\"\"})}),`\n`,(0,n.jsx)(e.p,{children:\"The system will check if the name is available and assign the new name to your public URL. At this point the old URL becomes disabled and whoever had it will not be able to see the contents of your container or app any more. This comes handy for version control or tagging specific changes or audiences.\"}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/11ac264b2481ea2e68f091d81f95895e.png\",alt:\"\"})}),`\n`,(0,n.jsx)(e.p,{children:\"Your new URL will go live as soon as you hit save.\"}),`\n`,(0,n.jsxs)(e.p,{children:[(0,n.jsx)(e.img,{src:\"/blog-images/5bd29fbc9332e87699811568a2212cee.png\",alt:\"\"}),`\n`,(0,n.jsx)(e.strong,{children:\"7.\"}),\" When you're done sharing, click \\u201CDisconnect\\u201D to take your container offline. Note, the custom URL is saved for future use, so if you want to share the same container later, it will spin up with the last name used.\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"This concludes our quick rundown of Release Share. Give it a try. If you've got suggestions or feedback, we're all ears. Find us at \",(0,n.jsx)(e.a,{href:\"https://www.dockercon.com\",children:\"DockerCon2023\"}),\" or drop us a line.\"]})]})}function x(a={}){let{wrapper:e}=a.components||{};return e?(0,n.jsx)(e,Object.assign({},a,{children:(0,n.jsx)(h,a)})):h(a)}var v=x;return k(R);})();\n;return Component;"
        },
        "_id": "blog/posts/introducing-release-share-a-docker-desktop-extension.mdx",
        "_raw": {
          "sourceFilePath": "blog/posts/introducing-release-share-a-docker-desktop-extension.mdx",
          "sourceFileName": "introducing-release-share-a-docker-desktop-extension.mdx",
          "sourceFileDir": "blog/posts",
          "contentType": "mdx",
          "flattenedPath": "blog/posts/introducing-release-share-a-docker-desktop-extension"
        },
        "type": "BlogPost",
        "computedSlug": "introducing-release-share-a-docker-desktop-extension"
      },
      "documentHash": "1739393595022",
      "hasWarnings": false,
      "documentTypeName": "BlogPost"
    },
    "blog/posts/introducing-standalone-instant-datasets-build-and-test-with-realistic-production-like-data-with-ease.mdx": {
      "document": {
        "title": "Introducing Standalone Instant Datasets: Build and Test with Realistic Production-like Data with Ease",
        "summary": "Release Instant Datasets in now a standalone product allowing everyone to build and test with production-like data.",
        "publishDate": "Tue Jul 25 2023 21:04:31 GMT+0000 (Coordinated Universal Time)",
        "author": "erik-landerholm",
        "readingTime": 5,
        "categories": [
          "platform-engineering",
          "product"
        ],
        "mainImage": "/blog-images/4db8f892b62cded336a7aacff59eb100.jpg",
        "imageAlt": "photo credit: vox.athena",
        "showCTA": true,
        "ctaCopy": "Build and test with realistic data instantly using Release's platform for seamless collaboration and efficient testing.",
        "ctaLink": "https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=introducing-standalone-instant-datasets-build-and-test-with-realistic-production-like-data-with-ease",
        "relatedPosts": [
          "syncing-databases-how-to-do-it-and-best-practices"
        ],
        "ogImage": "/blog-images/4db8f892b62cded336a7aacff59eb100.jpg",
        "excerpt": "Release Instant Datasets in now a standalone product allowing everyone to build and test with production-like data.",
        "tags": [
          "platform-engineering",
          "product"
        ],
        "ctaButton": "Try Release for Free",
        "body": {
          "raw": "\nAt Release, we are strong advocates of using production-like data for all feature development. We build and test with production-like data daily and think you should too, that’s why we built the dataset replication capability directly into our platform.\n\nUnderstanding how your database's data integrity impacts your application's behavior and performance, as well as challenging your UI assumptions against production data, can significantly enhance every aspect of software development. This approach not only streamlines the development process but also brings potential issues and defects to light as early as possible, fostering high performance within your team and company.\n\nToday, we are introducing [Instant Datasets](https://release.com/product/instant-datasets) as a standalone product. Now everyone can build and test with production-like data instantaneously, no exceptions! Whether you already use the Release platform for ephemeral environments, share environments, manage complex applications with multiple data stores, or run a simpler setup, Instant Datasets is now accessible to you. Anyone in need of realistic, up-to-date data for building and testing applications can take advantage of Instant Datasets within their own cloud accounts.\n\nNow, you may wonder why you would need Instant Datasets when you can manually create a copy of your snapshot and add it to your application. While that is a viable solution for obtaining production-like data, Instant Datasets offers a more comprehensive and efficient approach by managing the entire process for you. It generates a pool of instances that can be easily checked out and utilized by individual developers, product managers, test environments, staging environments, and any other user or use case you can imagine. With Instant Datasets, you can effortlessly create and manage multiple datasets based on various cloud databases.\n\nBut what about the potential cost of maintaining all those available instances on your cloud account? Fear not! Instant Datasets has implemented several measures to protect you from the dreaded \"surprise cloud bill\" we've all experienced:\n\n- Each database comes with a user-defined default Time-to-Live (TTL) when created.\n- Databases can be paused manually or on a schedule, helping you control costs effectively.\n- The \"check-in/check-out\" process ensures that Instant Datasets cleans up after itself when you no longer need the data, further managing costs.\n\nAlthough the ability to instantly use production-like data comes with some associated costs, the benefits in terms of application performance and experience make it well worth it.\n\nNow, you might wonder if you can share the databases with the rest of your team and whether they will see the changes you've made to your working copy. Rest assured, every checked-out database is exclusively yours to use and modify for as long as you need it. The moment you check one out, a replacement copy is instantly generated; and once you check your database back in, it is deleted to help you manage costs and maintain integrity of the data. Note that currently Instant Datasets works with AWS RDS and Aurora and we are adding new services soon.\n\nBut what if you don't have a budget for another subscription? We've got you covered! For our initial user cohort, we are making standalone Instant Datasets absolutely free. We understand the pain of dealing with inaccurate seed data, and our aim is to make access to production-like data as easy as possible for everyone.\n\nNow, let's address the crucial matter of security. Your data is YOURS, and it remains securely stored within your cloud account. Release orchestrates and manages access to instant replicas without interacting with the actual data itself, ensuring a safe choice even for the most stringent security environments. Datasets are password-protected and you can further [limit access](https://docs.release.com/release-instant-datasets/security/aws-instant-dataset-security) based on your specific needs. Additionally, we integrate with services like [Tonic](https://www.tonic.ai/) to obfuscate data and create \"fake\" datasets that closely mimic production data without risking exposure to sensitive values.\n\nFinally, we built Instant Datasets for ourselves, using it as a scratchpad of sorts to gain early insights into real applications. By doing so, we've saved countless hours of rework and bug hunting, making our entire team happier and more productive. Now, we invite you to join us on our mission to embrace production-like data and give Instant Datasets a try for yourself. Check out the [Quickstart Guide](https://docs.release.com/release-instant-datasets/quickstart) in our documentation and snag a [free account](https://beta.release.com/instantdatasets/register) while they last, and let's revolutionize the way we build and test applications with realistic data!\n",
          "code": "var Component=(()=>{var u=Object.create;var o=Object.defineProperty;var h=Object.getOwnPropertyDescriptor;var p=Object.getOwnPropertyNames;var m=Object.getPrototypeOf,y=Object.prototype.hasOwnProperty;var g=(a,e)=>()=>(e||a((e={exports:{}}).exports,e),e.exports),f=(a,e)=>{for(var n in e)o(a,n,{get:e[n],enumerable:!0})},r=(a,e,n,i)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let s of p(e))!y.call(a,s)&&s!==n&&o(a,s,{get:()=>e[s],enumerable:!(i=h(e,s))||i.enumerable});return a};var w=(a,e,n)=>(n=a!=null?u(m(a)):{},r(e||!a||!a.__esModule?o(n,\"default\",{value:a,enumerable:!0}):n,a)),b=a=>r(o({},\"__esModule\",{value:!0}),a);var c=g((T,d)=>{d.exports=_jsx_runtime});var I={};f(I,{default:()=>D,frontmatter:()=>v});var t=w(c()),v={title:\"Introducing Standalone Instant Datasets: Build and Test with Realistic Production-like Data with Ease\",summary:\"Release Instant Datasets in now a standalone product allowing everyone to build and test with production-like data.\",publishDate:\"Tue Jul 25 2023 21:04:31 GMT+0000 (Coordinated Universal Time)\",author:\"erik-landerholm\",readingTime:5,categories:[\"platform-engineering\",\"product\"],mainImage:\"/blog-images/4db8f892b62cded336a7aacff59eb100.jpg\",imageAlt:\"photo credit: vox.athena\",showCTA:!0,ctaCopy:\"Build and test with realistic data instantly using Release's platform for seamless collaboration and efficient testing.\",ctaLink:\"https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=introducing-standalone-instant-datasets-build-and-test-with-realistic-production-like-data-with-ease\",relatedPosts:[\"syncing-databases-how-to-do-it-and-best-practices\"],ogImage:\"/blog-images/4db8f892b62cded336a7aacff59eb100.jpg\",excerpt:\"Release Instant Datasets in now a standalone product allowing everyone to build and test with production-like data.\",tags:[\"platform-engineering\",\"product\"],ctaButton:\"Try Release for Free\"};function l(a){let e=Object.assign({p:\"p\",a:\"a\",ul:\"ul\",li:\"li\"},a.components);return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(e.p,{children:\"At Release, we are strong advocates of using production-like data for all feature development. We build and test with production-like data daily and think you should too, that\\u2019s why we built the dataset replication capability directly into our platform.\"}),`\n`,(0,t.jsx)(e.p,{children:\"Understanding how your database's data integrity impacts your application's behavior and performance, as well as challenging your UI assumptions against production data, can significantly enhance every aspect of software development. This approach not only streamlines the development process but also brings potential issues and defects to light as early as possible, fostering high performance within your team and company.\"}),`\n`,(0,t.jsxs)(e.p,{children:[\"Today, we are introducing \",(0,t.jsx)(e.a,{href:\"https://release.com/product/instant-datasets\",children:\"Instant Datasets\"}),\" as a standalone product. Now everyone can build and test with production-like data instantaneously, no exceptions! Whether you already use the Release platform for ephemeral environments, share environments, manage complex applications with multiple data stores, or run a simpler setup, Instant Datasets is now accessible to you. Anyone in need of realistic, up-to-date data for building and testing applications can take advantage of Instant Datasets within their own cloud accounts.\"]}),`\n`,(0,t.jsx)(e.p,{children:\"Now, you may wonder why you would need Instant Datasets when you can manually create a copy of your snapshot and add it to your application. While that is a viable solution for obtaining production-like data, Instant Datasets offers a more comprehensive and efficient approach by managing the entire process for you. It generates a pool of instances that can be easily checked out and utilized by individual developers, product managers, test environments, staging environments, and any other user or use case you can imagine. With Instant Datasets, you can effortlessly create and manage multiple datasets based on various cloud databases.\"}),`\n`,(0,t.jsx)(e.p,{children:`But what about the potential cost of maintaining all those available instances on your cloud account? Fear not! Instant Datasets has implemented several measures to protect you from the dreaded \"surprise cloud bill\" we've all experienced:`}),`\n`,(0,t.jsxs)(e.ul,{children:[`\n`,(0,t.jsx)(e.li,{children:\"Each database comes with a user-defined default Time-to-Live (TTL) when created.\"}),`\n`,(0,t.jsx)(e.li,{children:\"Databases can be paused manually or on a schedule, helping you control costs effectively.\"}),`\n`,(0,t.jsx)(e.li,{children:'The \"check-in/check-out\" process ensures that Instant Datasets cleans up after itself when you no longer need the data, further managing costs.'}),`\n`]}),`\n`,(0,t.jsx)(e.p,{children:\"Although the ability to instantly use production-like data comes with some associated costs, the benefits in terms of application performance and experience make it well worth it.\"}),`\n`,(0,t.jsx)(e.p,{children:\"Now, you might wonder if you can share the databases with the rest of your team and whether they will see the changes you've made to your working copy. Rest assured, every checked-out database is exclusively yours to use and modify for as long as you need it. The moment you check one out, a replacement copy is instantly generated; and once you check your database back in, it is deleted to help you manage costs and maintain integrity of the data. Note that currently Instant Datasets works with AWS RDS and Aurora and we are adding new services soon.\"}),`\n`,(0,t.jsx)(e.p,{children:\"But what if you don't have a budget for another subscription? We've got you covered! For our initial user cohort, we are making standalone Instant Datasets absolutely free. We understand the pain of dealing with inaccurate seed data, and our aim is to make access to production-like data as easy as possible for everyone.\"}),`\n`,(0,t.jsxs)(e.p,{children:[\"Now, let's address the crucial matter of security. Your data is YOURS, and it remains securely stored within your cloud account. Release orchestrates and manages access to instant replicas without interacting with the actual data itself, ensuring a safe choice even for the most stringent security environments. Datasets are password-protected and you can further \",(0,t.jsx)(e.a,{href:\"https://docs.release.com/release-instant-datasets/security/aws-instant-dataset-security\",children:\"limit access\"}),\" based on your specific needs. Additionally, we integrate with services like \",(0,t.jsx)(e.a,{href:\"https://www.tonic.ai/\",children:\"Tonic\"}),' to obfuscate data and create \"fake\" datasets that closely mimic production data without risking exposure to sensitive values.']}),`\n`,(0,t.jsxs)(e.p,{children:[\"Finally, we built Instant Datasets for ourselves, using it as a scratchpad of sorts to gain early insights into real applications. By doing so, we've saved countless hours of rework and bug hunting, making our entire team happier and more productive. Now, we invite you to join us on our mission to embrace production-like data and give Instant Datasets a try for yourself. Check out the \",(0,t.jsx)(e.a,{href:\"https://docs.release.com/release-instant-datasets/quickstart\",children:\"Quickstart Guide\"}),\" in our documentation and snag a \",(0,t.jsx)(e.a,{href:\"https://beta.release.com/instantdatasets/register\",children:\"free account\"}),\" while they last, and let's revolutionize the way we build and test applications with realistic data!\"]})]})}function k(a={}){let{wrapper:e}=a.components||{};return e?(0,t.jsx)(e,Object.assign({},a,{children:(0,t.jsx)(l,a)})):l(a)}var D=k;return b(I);})();\n;return Component;"
        },
        "_id": "blog/posts/introducing-standalone-instant-datasets-build-and-test-with-realistic-production-like-data-with-ease.mdx",
        "_raw": {
          "sourceFilePath": "blog/posts/introducing-standalone-instant-datasets-build-and-test-with-realistic-production-like-data-with-ease.mdx",
          "sourceFileName": "introducing-standalone-instant-datasets-build-and-test-with-realistic-production-like-data-with-ease.mdx",
          "sourceFileDir": "blog/posts",
          "contentType": "mdx",
          "flattenedPath": "blog/posts/introducing-standalone-instant-datasets-build-and-test-with-realistic-production-like-data-with-ease"
        },
        "type": "BlogPost",
        "computedSlug": "introducing-standalone-instant-datasets-build-and-test-with-realistic-production-like-data-with-ease"
      },
      "documentHash": "1739393595022",
      "hasWarnings": false,
      "documentTypeName": "BlogPost"
    },
    "blog/posts/join-the-release-ai-technical-preview.mdx": {
      "document": {
        "title": "Build AI, Automate the Infrastructure. Join the Release.ai Technical Preview",
        "summary": "Learn about Release.ai, a cloud-agnostic, framework-agnostic, and compute-agnostic platform for AI infrastructure.",
        "publishDate": "Mon Mar 18 2024 20:47:23 GMT+0000 (Coordinated Universal Time)",
        "author": "michael-poon",
        "readingTime": 3,
        "categories": [
          "ai",
          "kubernetes",
          "nvidia",
          "platform-engineering"
        ],
        "mainImage": "/blog-images/69e8ac9e4966689046f1a95353e74484.png",
        "imageAlt": "Join the ReleaseAI early access program",
        "showCTA": true,
        "ctaCopy": "Simplify AI infrastructure like in the Release.ai technical preview. Automate environments for seamless AI model deployment.",
        "ctaLink": "https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=join-the-release-ai-technical-preview",
        "relatedPosts": [
          ""
        ],
        "ogImage": "/blog-images/69e8ac9e4966689046f1a95353e74484.png",
        "excerpt": "Learn about Release.ai, a cloud-agnostic, framework-agnostic, and compute-agnostic platform for AI infrastructure.",
        "tags": [
          "ai",
          "kubernetes",
          "nvidia",
          "platform-engineering"
        ],
        "ctaButton": "Try Release for Free",
        "body": {
          "raw": "\nToday we are excited to launch the technical preview for [Release.ai](http://release.ai) – a cloud-agnostic, framework-agnostic, and compute-agnostic platform for AI infrastructure. As the team here at Release was working on our own AI DevOps assistant, and comparing notes with teams who build, fine-tune, and integrate AI models into products, we saw a consistent challenge: the infrastructure is burdensome. \n\nAll across the industry we see amazing innovations pop up daily, and underneath the surface to make it all happen are infrastructure teams toiling away at accumulating GPUs, managing finicky clusters, investigating the latest timeouts, grappling with latest framework peculiarities, and making their finance folks bug-eyed at the bills being generated. It does not have to be this hard.\n\n[Release](http://release.com) has been abstracting away infrastructure complexity for conventional SDLC processes for years. Helping engineering teams focus on shipping high-quality products, faster. Now we bring our expertise to support AI engineering teams. To begin, we partnered with NVIDIA to simplify infrastructure setup and management of the [NVIDIA NeMo framework](https://www.nvidia.com/en-us/ai-data-science/generative-ai/nemo-framework/). Now you can use NeMo and Release to build and deploy models in a portable and cloud-agnostic way, on top of the Kubernetes clusters that we manage for you. This automates resource management, speeds up time-to-insight and gives AI development teams greater control over their infrastructure. \n\nWhile we are building Release.ai to simplify AI infrastructure, you don’t have to choose between simplicity and control - everything on Release.ai is orchestrated in your cloud account, on the hardware that you own, simplifying toil while maintaining control. Currently we support training, fine-tuning, and inference workflows with more workflows and frameworks to come. \n\nThis is your opportunity to get in on the ground floor. [Join our technical preview today](https://release.ai/) to experience our solution firsthand, provide invaluable feedback, and become a key design partner helping to shape the future of AI infrastructure tooling. Together, we can untether AI teams from infrastructure complexities so they can pour their energy into innovation.\n",
          "code": "var Component=(()=>{var m=Object.create;var r=Object.defineProperty;var h=Object.getOwnPropertyDescriptor;var g=Object.getOwnPropertyNames;var p=Object.getPrototypeOf,d=Object.prototype.hasOwnProperty;var f=(t,e)=>()=>(e||t((e={exports:{}}).exports,e),e.exports),w=(t,e)=>{for(var n in e)r(t,n,{get:e[n],enumerable:!0})},s=(t,e,n,o)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let i of g(e))!d.call(t,i)&&i!==n&&r(t,i,{get:()=>e[i],enumerable:!(o=h(e,i))||o.enumerable});return t};var y=(t,e,n)=>(n=t!=null?m(p(t)):{},s(e||!t||!t.__esModule?r(n,\"default\",{value:t,enumerable:!0}):n,t)),b=t=>s(r({},\"__esModule\",{value:!0}),t);var l=f((R,c)=>{c.exports=_jsx_runtime});var k={};w(k,{default:()=>I,frontmatter:()=>v});var a=y(l()),v={title:\"Build AI, Automate the Infrastructure. Join the Release.ai Technical Preview\",summary:\"Learn about Release.ai, a cloud-agnostic, framework-agnostic, and compute-agnostic platform for AI infrastructure.\",publishDate:\"Mon Mar 18 2024 20:47:23 GMT+0000 (Coordinated Universal Time)\",author:\"michael-poon\",readingTime:3,categories:[\"ai\",\"kubernetes\",\"nvidia\",\"platform-engineering\"],mainImage:\"/blog-images/69e8ac9e4966689046f1a95353e74484.png\",imageAlt:\"Join the ReleaseAI early access program\",showCTA:!0,ctaCopy:\"Simplify AI infrastructure like in the Release.ai technical preview. Automate environments for seamless AI model deployment.\",ctaLink:\"https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=join-the-release-ai-technical-preview\",relatedPosts:[\"\"],ogImage:\"/blog-images/69e8ac9e4966689046f1a95353e74484.png\",excerpt:\"Learn about Release.ai, a cloud-agnostic, framework-agnostic, and compute-agnostic platform for AI infrastructure.\",tags:[\"ai\",\"kubernetes\",\"nvidia\",\"platform-engineering\"],ctaButton:\"Try Release for Free\"};function u(t){let e=Object.assign({p:\"p\",a:\"a\"},t.components);return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsxs)(e.p,{children:[\"Today we are excited to launch the technical preview for \",(0,a.jsx)(e.a,{href:\"http://release.ai\",children:\"Release.ai\"}),\" \\u2013 a cloud-agnostic, framework-agnostic, and compute-agnostic platform for AI infrastructure. As the team here at Release was working on our own AI DevOps assistant, and comparing notes with teams who build, fine-tune, and integrate AI models into products, we saw a consistent challenge: the infrastructure is burdensome.\\xA0\"]}),`\n`,(0,a.jsx)(e.p,{children:\"All across the industry we see amazing innovations pop up daily, and underneath the surface to make it all happen are infrastructure teams toiling away at accumulating GPUs, managing finicky clusters, investigating the latest timeouts, grappling with latest framework peculiarities, and making their finance folks bug-eyed at the bills being generated. It does not have to be this hard.\"}),`\n`,(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.a,{href:\"http://release.com\",children:\"Release\"}),\" has been abstracting away infrastructure complexity for conventional SDLC processes for years. Helping engineering teams focus on shipping high-quality products, faster. Now we bring our expertise to support AI engineering teams. To begin, we partnered with NVIDIA to simplify infrastructure setup and management of the \",(0,a.jsx)(e.a,{href:\"https://www.nvidia.com/en-us/ai-data-science/generative-ai/nemo-framework/\",children:\"NVIDIA NeMo framework\"}),\". Now you can use NeMo and Release to build and deploy models in a portable and cloud-agnostic way, on top of the Kubernetes clusters that we manage for you. This automates resource management, speeds up time-to-insight and gives AI development teams greater control over their infrastructure.\\xA0\"]}),`\n`,(0,a.jsx)(e.p,{children:\"While we are building Release.ai to simplify AI infrastructure, you don\\u2019t have to choose between simplicity and control - everything on Release.ai is orchestrated in your cloud account, on the hardware that you own, simplifying toil while maintaining control. Currently we support training, fine-tuning, and inference workflows with more workflows and frameworks to come.\\xA0\"}),`\n`,(0,a.jsxs)(e.p,{children:[\"This is your opportunity to get in on the ground floor. \",(0,a.jsx)(e.a,{href:\"https://release.ai/\",children:\"Join our technical preview today\"}),\" to experience our solution firsthand, provide invaluable feedback, and become a key design partner helping to shape the future of AI infrastructure tooling. Together, we can untether AI teams from infrastructure complexities so they can pour their energy into innovation.\"]})]})}function A(t={}){let{wrapper:e}=t.components||{};return e?(0,a.jsx)(e,Object.assign({},t,{children:(0,a.jsx)(u,t)})):u(t)}var I=A;return b(k);})();\n;return Component;"
        },
        "_id": "blog/posts/join-the-release-ai-technical-preview.mdx",
        "_raw": {
          "sourceFilePath": "blog/posts/join-the-release-ai-technical-preview.mdx",
          "sourceFileName": "join-the-release-ai-technical-preview.mdx",
          "sourceFileDir": "blog/posts",
          "contentType": "mdx",
          "flattenedPath": "blog/posts/join-the-release-ai-technical-preview"
        },
        "type": "BlogPost",
        "computedSlug": "join-the-release-ai-technical-preview"
      },
      "documentHash": "1739393595022",
      "hasWarnings": false,
      "documentTypeName": "BlogPost"
    },
    "blog/posts/kelsey-degeorge-why-i-joined-release.mdx": {
      "document": {
        "title": "Why I joined Release",
        "summary": "Exciting update on why CRO Kelsey DeGeorge decided to join Release",
        "publishDate": "Thu Feb 24 2022 03:14:43 GMT+0000 (Coordinated Universal Time)",
        "author": "kelsey-degeorge",
        "readingTime": 3,
        "categories": [
          "platform-engineering",
          "product"
        ],
        "mainImage": "/blog-images/c5e739ff8f8a86e7e7e911833ce9e434.jpg",
        "imageAlt": "Why I joined Release",
        "showCTA": true,
        "ctaCopy": "Simplify cloud management with Release's environments-as-a-service platform. Focus on core business, not infrastructure. Accelerate deployment cycles.",
        "ctaLink": "https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=kelsey-degeorge-why-i-joined-release",
        "relatedPosts": [
          ""
        ],
        "ogImage": "/blog-images/c5e739ff8f8a86e7e7e911833ce9e434.jpg",
        "excerpt": "Exciting update on why CRO Kelsey DeGeorge decided to join Release",
        "tags": [
          "platform-engineering",
          "product"
        ],
        "ctaButton": "Try Release for Free",
        "body": {
          "raw": "\nDuring my 5 year tenure at AWS, leading teams who supported B2B software companies, I had the opportunity to observe an extremely broad cohort of customers, challenges they faced, and opportunities for technology disruption. Companies were making substantial investments in the cloud - all at various phases. Some were in their infancy and needed guidance formulating a migration strategy, others were all-in seeking support to modernize their stack, and every customer in between. One commonality across these customers in various phases of the cloud journey is that they were all looking to achieve a similar outcome: offload undifferentiated heavy lifting of maintaining infrastructure to focus on what’s core to their business, their “special sauce.”\n\nWhile the cloud offers building block services to ultimately achieve this ideal state, there is still a substantial burden on the teams responsible for managing cloud services. The burden wasn’t eliminated - it simply transferred. Teams previously in place to manage hardware became teams in place to manage cloud infrastructure. In many instances, companies invested **even more** in internal cloud teams (SRE, DevOps, etc.) than they did in the data center days. This is by no means a slight to the cloud. It is the only way companies can innovate at the pace necessary in today’s demanding market. However, I instantly recognized Release’s ability to address these challenges and maximize the value of the cloud’s agility, ease of experimentation, and cost efficiency.\n\nWith Release, instead of allocating massive amounts of resources to DevOps, engineers can focus on the core of the business. How, though? Release offers environments-as-a-service, containing the manifestation and execution of all of the code, data, infrastructure, settings and services needed to run any application. Environments can be ephemeral, short-lived for testing and branch-based development or permanent for environment needs such as staging and production.\n\nEnvironments are critical to organizations because every member of product development is dependent on them in every step of building and delivering products. Oh and my fellow sales and GTM folks, I’m talking to you too. You might not know it, but you rely on environments every time you demonstrate your product offering to a customer (arguably the most critical stage of the sales cycle). We have on-demand environments for you too, so you never have to worry about the tech failing mid-demo (defeating Murphy’s law!). With Release, customers get environments for virtually every use case, best practice DevOps out of the box, instant data sets, development isolation, uniformity, and more.\n\nNot only do I firmly believe in the company’s mission and their novel technology. I strongly believe in the founding team and have had quite a poetic journey with the three of them. It started back in 2017 when I supported Tommy, David, and Erik on a journey to migrate from several data centers to AWS. This was a multi-year effort requiring expertise from dozens of AWS resources, substantial SOW dollars, and months of training and enablement. I wish I could say this multi-year effort gave them everything we promised (agility, scale, innovation), but the migration was only the beginning. They spent years building a homegrown solution in an effort to become less reliant on DevOps resources and remain product-oriented. They recognized the need to give developers the independence to build, test, iterate, and re-iterate in their own isolated environments and to share those changes with others. Their mission to bring ideas to the world faster ultimately led to the inception of Release.\n\nIt has come an extraordinary way since then, but we’re still early. If you’re excited about joining a mission-driven company backed by the most credible investors in the world, you know where to find us. Spoiler alert: We’re hiring.\n",
          "code": "var Component=(()=>{var m=Object.create;var i=Object.defineProperty;var u=Object.getOwnPropertyDescriptor;var h=Object.getOwnPropertyNames;var g=Object.getPrototypeOf,y=Object.prototype.hasOwnProperty;var f=(t,e)=>()=>(e||t((e={exports:{}}).exports,e),e.exports),p=(t,e)=>{for(var o in e)i(t,o,{get:e[o],enumerable:!0})},r=(t,e,o,s)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let a of h(e))!y.call(t,a)&&a!==o&&i(t,a,{get:()=>e[a],enumerable:!(s=u(e,a))||s.enumerable});return t};var v=(t,e,o)=>(o=t!=null?m(g(t)):{},r(e||!t||!t.__esModule?i(o,\"default\",{value:t,enumerable:!0}):o,t)),b=t=>r(i({},\"__esModule\",{value:!0}),t);var c=f((T,l)=>{l.exports=_jsx_runtime});var I={};p(I,{default:()=>j,frontmatter:()=>w});var n=v(c()),w={title:\"Why I joined Release\",summary:\"Exciting update on why CRO Kelsey DeGeorge decided to join Release\",publishDate:\"Thu Feb 24 2022 03:14:43 GMT+0000 (Coordinated Universal Time)\",author:\"kelsey-degeorge\",readingTime:3,categories:[\"platform-engineering\",\"product\"],mainImage:\"/blog-images/c5e739ff8f8a86e7e7e911833ce9e434.jpg\",imageAlt:\"Why I joined Release\",showCTA:!0,ctaCopy:\"Simplify cloud management with Release's environments-as-a-service platform. Focus on core business, not infrastructure. Accelerate deployment cycles.\",ctaLink:\"https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=kelsey-degeorge-why-i-joined-release\",relatedPosts:[\"\"],ogImage:\"/blog-images/c5e739ff8f8a86e7e7e911833ce9e434.jpg\",excerpt:\"Exciting update on why CRO Kelsey DeGeorge decided to join Release\",tags:[\"platform-engineering\",\"product\"],ctaButton:\"Try Release for Free\"};function d(t){let e=Object.assign({p:\"p\",strong:\"strong\"},t.components);return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(e.p,{children:\"During my 5 year tenure at AWS, leading teams who supported B2B software companies, I had the opportunity to observe an extremely broad cohort of customers, challenges they faced, and opportunities for technology disruption. Companies were making substantial investments in the cloud - all at various phases. Some were in their infancy and needed guidance formulating a migration strategy, others were all-in seeking support to modernize their stack, and every customer in between. One commonality across these customers in various phases of the cloud journey is that they were all looking to achieve a similar outcome: offload undifferentiated heavy lifting of maintaining infrastructure to focus on what\\u2019s core to their business, their \\u201Cspecial sauce.\\u201D\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"While the cloud offers building block services to ultimately achieve this ideal state, there is still a substantial burden on the teams responsible for managing cloud services. The burden wasn\\u2019t eliminated - it simply transferred. Teams previously in place to manage hardware became teams in place to manage cloud infrastructure. In many instances, companies invested \",(0,n.jsx)(e.strong,{children:\"even more\"}),\" in internal cloud teams (SRE, DevOps, etc.) than they did in the data center days. This is by no means a slight to the cloud. It is the only way companies can innovate at the pace necessary in today\\u2019s demanding market. However, I instantly recognized Release\\u2019s ability to address these challenges and maximize the value of the cloud\\u2019s agility, ease of experimentation, and cost efficiency.\"]}),`\n`,(0,n.jsx)(e.p,{children:\"With Release, instead of allocating massive amounts of resources to DevOps, engineers can focus on the core of the business. How, though? Release offers environments-as-a-service, containing the manifestation and execution of all of the code, data, infrastructure, settings and services needed to run any application. Environments can be ephemeral, short-lived for testing and branch-based development or permanent for environment needs such as staging and production.\"}),`\n`,(0,n.jsx)(e.p,{children:\"Environments are critical to organizations because every member of product development is dependent on them in every step of building and delivering products. Oh and my fellow sales and GTM folks, I\\u2019m talking to you too. You might not know it, but you rely on environments every time you demonstrate your product offering to a customer (arguably the most critical stage of the sales cycle). We have on-demand environments for you too, so you never have to worry about the tech failing mid-demo (defeating Murphy\\u2019s law!). With Release, customers get environments for virtually every use case, best practice DevOps out of the box, instant data sets, development isolation, uniformity, and more.\"}),`\n`,(0,n.jsx)(e.p,{children:\"Not only do I firmly believe in the company\\u2019s mission and their novel technology. I strongly believe in the founding team and have had quite a poetic journey with the three of them. It started back in 2017 when I supported Tommy, David, and Erik on a journey to migrate from several data centers to AWS. This was a multi-year effort requiring expertise from dozens of AWS resources, substantial SOW dollars, and months of training and enablement. I wish I could say this multi-year effort gave them everything we promised (agility, scale, innovation), but the migration was only the beginning. They spent years building a homegrown solution in an effort to become less reliant on DevOps resources and remain product-oriented. They recognized the need to give developers the independence to build, test, iterate, and re-iterate in their own isolated environments and to share those changes with others. Their mission to bring ideas to the world faster ultimately led to the inception of Release.\"}),`\n`,(0,n.jsx)(e.p,{children:\"It has come an extraordinary way since then, but we\\u2019re still early. If you\\u2019re excited about joining a mission-driven company backed by the most credible investors in the world, you know where to find us. Spoiler alert: We\\u2019re hiring.\"})]})}function x(t={}){let{wrapper:e}=t.components||{};return e?(0,n.jsx)(e,Object.assign({},t,{children:(0,n.jsx)(d,t)})):d(t)}var j=x;return b(I);})();\n;return Component;"
        },
        "_id": "blog/posts/kelsey-degeorge-why-i-joined-release.mdx",
        "_raw": {
          "sourceFilePath": "blog/posts/kelsey-degeorge-why-i-joined-release.mdx",
          "sourceFileName": "kelsey-degeorge-why-i-joined-release.mdx",
          "sourceFileDir": "blog/posts",
          "contentType": "mdx",
          "flattenedPath": "blog/posts/kelsey-degeorge-why-i-joined-release"
        },
        "type": "BlogPost",
        "computedSlug": "kelsey-degeorge-why-i-joined-release"
      },
      "documentHash": "1739393595022",
      "hasWarnings": false,
      "documentTypeName": "BlogPost"
    },
    "blog/posts/kubernetes-crds.mdx": {
      "document": {
        "title": "Kubernetes CRDs: What They Are and Why They Are Useful",
        "summary": "What are Kubernetes Custom Resource Definitions? What are they useful for and how to create your own? Read all about it",
        "publishDate": "Tue Feb 15 2022 22:12:00 GMT+0000 (Coordinated Universal Time)",
        "author": "dawid-ziolkowski",
        "readingTime": 7,
        "categories": [
          "kubernetes",
          "platform-engineering"
        ],
        "mainImage": "/blog-images/25a8e0410945d3fe7afe1b2f30acc552.jpg",
        "imageAlt": "a close-up of a keyboard",
        "showCTA": true,
        "ctaCopy": "Unlock new possibilities with Release.com's ephemeral environments for managing custom resources in Kubernetes. Accelerate development cycles and streamline workflows.",
        "ctaLink": "https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=kubernetes-crds",
        "relatedPosts": [
          ""
        ],
        "ogImage": "/blog-images/25a8e0410945d3fe7afe1b2f30acc552.jpg",
        "excerpt": "What are Kubernetes Custom Resource Definitions? What are they useful for and how to create your own? Read all about it",
        "tags": [
          "kubernetes",
          "platform-engineering"
        ],
        "ctaButton": "Try Release for Free",
        "body": {
          "raw": "\nOne of the main reasons that Kubernetes became so popular is the fact that it's so flexible. When we say Kubernetes, we typically think about deploying and managing containers. And while this is, in fact, Kubernetes's main job, it can actually do much more than that. This is possible thanks to something called Custom Resource Definitions, or CRDs for short. In this post, you'll learn what CRDs are and what you can use them for. We'll also take a look at how to create them.\n\n![](/blog-images/470546e1232748209628bb1f33220896.png)\n\n### Kubernetes API\n\nBefore we dive into custom resource definitions, let's first talk about Kubernetes in general. If I asked you, \"What is Kubernetes?\" then you'd probably answer, \"Kubernetes is a container orchestrator.\" This would, of course, be one correct answer.\n\nBut by looking under the Kubernetes hood, you could see that the main component of Kubernetes is an API server and etcd data store. And there are other, more important components like kube-scheduler, kube-controller-manager, and cloud-controller-manager, but pretty much any operation on your cluster needs to go through an API server.\n\nThat API has a few built-in objects that it understands. Things you may be familiar with like Pods, Namespaces, ConfigMaps, Services, or Nodes are all API objects. That's why whenever you execute **kubectl get pods** or **kubectl get nodes**, you get a list of pods or nodes. But if you try to get a list of objects that don't exist in the Kubernetes API—like, for example, **kubectl get biscuits**—you'd get a response similar to this:\n\nerror: the server doesn't have a resource type \"biscuits\"\n\nAnd this is because there is no such thing as \"biscuits\" defined in the Kubernetes API. Quite logical, right? Well, what if I told you that you could add a biscuits definition to your Kubernetes cluster? In fact, you can extend your Kubernetes API with any custom object you like. That's exactly what custom resource definitions are for.\n\n### Why CRDs?\n\nSo what's the point of adding a biscuits definition to your Kubernetes cluster? Remember when I mentioned earlier that the success of Kubernetes comes from its flexibility? The ability to extend the Kubernetes API with custom resource definitions is a really great feature that lets you do something magical. It allows you to instruct Kubernetes to manage more than just containers.\n\nWhy is that such a great thing? Because CRDs together with Kubernetes operators give you almost unlimited possibilities. You can adapt Kubernetes in a way that it will take care of older parts of your infrastructure. If you do it right, you'll be able to avoid bottlenecks and easily modernise things that normally would require long and costly redesigns.\n\n### CRDs on Your Cluster\n\nBefore we dive into creating our own CRD, you need to know two things.\n\nFirstly, creating a custom resource definition is an advanced topic. Many companies don't even need to create any CRDs. The Kubernetes community finds interesting solutions for common problems all the time, and it's likely that any use case you encounter probably already has a CRD you can use! And if you're still new to Kubernetes, you definitely shouldn't jump into CRDs before you understand the basics well.\n\nSecondly, as already mentioned, you don't need to create any CRDs yourself if you don't feel the need to. However, many Kubernetes tools will install their own CRDs, so even if you don't create any yourself, you'll probably still end up having some on your cluster.\n\nOne example is [cert-manager](https://cert-manager.io), a very popular Kubernetes tool for managing certificates. It installs a few CRDs on your cluster in order to do its job. If you execute **kubectl get clusterissuers** before installation of cert-manager, your cluster won't know what ClusterIssuers are:\n\n`error: the server doesn't have a resource type \"clusterissuers\"`\n\nBut if you execute the same command after cert-manager installation, you'll get the list of ClusterIssuers on your cluster.\n\nIn fact, you can list all custom resource definitions installed on your cluster by executing **kubectl get crd**:\n\n`$ kubectl get crd NAME                                    CREATED AT addons.k3s.cattle.io                    2022-01-23T12:48:31Z helmcharts.helm.cattle.io               2022-01-23T12:48:31Z helmchartconfigs.helm.cattle.io         2022-01-23T12:48:31Z serverstransports.traefik.containo.us   2022-01-23T12:49:48Z tlsoptions.traefik.containo.us          2022-01-23T12:49:48Z ingressroutetcps.traefik.containo.us    2022-01-23T12:49:48Z ingressroutes.traefik.containo.us       2022-01-23T12:49:48Z tlsstores.traefik.containo.us           2022-01-23T12:49:48Z middlewares.traefik.containo.us         2022-01-23T12:49:48Z traefikservices.traefik.containo.us     2022-01-23T12:49:48Z middlewaretcps.traefik.containo.us      2022-01-23T12:49:48Z ingressrouteudps.traefik.containo.us    2022-01-23T12:49:48Z`\n\nThe above output comes in the form of **OBJECT.GROUP** and tells me that I can execute commands like **kubectl get addons**...\n\n### How to Create CRDs\n\nOK, forget about biscuits. Let's take a look at some more realistic examples. Imagine that you want Kubernetes to somehow manage your custom routers in your datacenter. For that, you could create a custom resource definition similar to this one:\n\n`apiVersion: apiextensions.k8s.io/v1 kind: CustomResourceDefinition metadata:   # Name of your CRD. Must match the spec block below, and be in the form: .  name: routers.example.com spec: # Group name to use for REST API: /apis//  group: example.com  names: # Plural name to be used in the URL: /apis///    plural: routers    # Singular name to be used as an alias on the CLI and for display    singular: router    # Kind is normally the CamelCased singular type. Your resource manifests use this.    kind: Router    # ShortNames allow shorter string to match your resource on the CLI    shortNames:    - rt  # Scope can be either Namespaced or Cluster-wide  scope: Cluster  versions:    - name: v1      # Each version can be enabled/disabled by Served flag.      served: true      # One and only one version must be marked as the storage version.      storage: true      schema:        openAPIV3Schema:          type: object          properties:            spec:              type: object              properties:                dataCenter:                  type: string                rack:                  type: integer                type:                  type: string                  enum:                  - f5                  - virtual              required: [\"dataCenter\", \"rack\", \"type\"]          required: [\"spec\"]`\n\nYou can apply the above CRD to the cluster by executing **kubectl apply -f router-CRD.yaml**. Once you do that, your Kubernetes cluster will already know what \"router\" is. Therefore, you'll be able to execute **kubectl get routers**. Of course, we just applied the resource definition, not the resource itself. So **kubectl get routers** will return the following:\n\n`No resources found.`\n\nBut as you can see, it doesn't return this:\n\n`error: the server doesn't have a resource type \"routers\"`\n\nWhich means we successfully added a new object to the Kubernetes API. To add an actual router resource, you can construct a YAML definition file like with any other object:\n\n`apiVersion: example.com/v1 kind: Router metadata:  name: example-router spec:  dataCenter: eu-1  rack: 3  type: virtual`\n\n \n\nNow, you can create a new router on your cluster by executing **kubectl apply - f example-router.yaml**, and if you try to get the list of routers again with **kubectl get routers**, you should see one now:\n\n`$ kubectl get routers NAME             AGE example-router   4s`\n\n \n\nCongratulations! You just extended the Kubernetes API.\n\n![](/blog-images/785fbfe1e3cec8abffd9eff571966715.png)\n\n### **What to Do With CRDs**\n\nYou may be thinking, \"OK, great, but that router doesn't do anything!\" And yes, that's right. In its current form, our CRD doesn't do anything besides being processed and stored by the Kubernetes API. And while there are use cases where this is enough, usually CRDs are combined with custom controllers.\n\nCustom controllers are another concept in Kubernetes that lets you actually do something with your custom resources. In our case, we would like to actually create or configure the routers in our datacenter. Therefore, we'd have to write a custom controller and instruct it to listen to the Kubernetes API and wait for any changes to our custom **router** objects.\n\nCustom controllers under the hood are just applications or scripts written in your programming language of choice. They're deployed on the cluster as pods, and their job is to listen to the Kubernetes API and perform some actions based on defined logic.\n\n#### CRD vs. ConfigMap\n\nLast but not least, by looking at CRDs, you may see some similarities with a Kubernetes built-in object, ConfigMap. And if you use CRDs without a custom controller, they may, in fact, serve a similar purpose. They both can be used to store custom configurations. However, there are noticeable differences between them.\n\nFirst of all, ConfigMaps by design are meant to provide configuration for your pods. They can be mounted as files or environment variables into the pod. They work well if you have well-defined config files like, for example, Apache or MySQL config.\n\nCRDs can also be consumed by pods but only by contacting the Kubernetes API. They simply have a different purpose than ConfigMaps. They're not meant to be used to provide configuration to your pods but to extend the Kubernetes API in order to build custom automation.\n\n### Summary\n\nKubernetes's flexibility is what made it so successful (among other things, of course). Now, you can make use of that flexibility by creating your own Kubernetes objects. The possibilities are almost limitless, and it's only up to you how you'll make use of CRDs.\n\nCome back to us for more Kubernetes articles. [Here's](https://release.com/blog/kubernetes-daemonset-tutorial) our article explaining another Kubernetes object, DaemonSets. Also, feel free to take a look at our offerings. We simplify the development process by providing [Environments as a Service.](https://release.com/)\n",
          "code": "var Component=(()=>{var h=Object.create;var s=Object.defineProperty;var d=Object.getOwnPropertyDescriptor;var m=Object.getOwnPropertyNames;var p=Object.getPrototypeOf,y=Object.prototype.hasOwnProperty;var b=(o,e)=>()=>(e||o((e={exports:{}}).exports,e),e.exports),f=(o,e)=>{for(var n in e)s(o,n,{get:e[n],enumerable:!0})},i=(o,e,n,a)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let r of m(e))!y.call(o,r)&&r!==n&&s(o,r,{get:()=>e[r],enumerable:!(a=d(e,r))||a.enumerable});return o};var g=(o,e,n)=>(n=o!=null?h(p(o)):{},i(e||!o||!o.__esModule?s(n,\"default\",{value:o,enumerable:!0}):n,o)),w=o=>i(s({},\"__esModule\",{value:!0}),o);var l=b((x,c)=>{c.exports=_jsx_runtime});var K={};f(K,{default:()=>v,frontmatter:()=>k});var t=g(l()),k={title:\"Kubernetes CRDs: What They Are and Why They Are Useful\",summary:\"What are Kubernetes Custom Resource Definitions? What are they useful for and how to create your own? Read all about it\",publishDate:\"Tue Feb 15 2022 22:12:00 GMT+0000 (Coordinated Universal Time)\",author:\"dawid-ziolkowski\",readingTime:7,categories:[\"kubernetes\",\"platform-engineering\"],mainImage:\"/blog-images/25a8e0410945d3fe7afe1b2f30acc552.jpg\",imageAlt:\"a close-up of a keyboard\",showCTA:!0,ctaCopy:\"Unlock new possibilities with Release.com's ephemeral environments for managing custom resources in Kubernetes. Accelerate development cycles and streamline workflows.\",ctaLink:\"https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=kubernetes-crds\",relatedPosts:[\"\"],ogImage:\"/blog-images/25a8e0410945d3fe7afe1b2f30acc552.jpg\",excerpt:\"What are Kubernetes Custom Resource Definitions? What are they useful for and how to create your own? Read all about it\",tags:[\"kubernetes\",\"platform-engineering\"],ctaButton:\"Try Release for Free\"};function u(o){let e=Object.assign({p:\"p\",img:\"img\",h3:\"h3\",a:\"a\",span:\"span\",strong:\"strong\",code:\"code\",h4:\"h4\"},o.components);return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(e.p,{children:\"One of the main reasons that Kubernetes became so popular is the fact that it's so flexible. When we say Kubernetes, we typically think about deploying and managing containers. And while this is, in fact, Kubernetes's main job, it can actually do much more than that. This is possible thanks to something called Custom Resource Definitions, or CRDs for short. In this post, you'll learn what CRDs are and what you can use them for. We'll also take a look at how to create them.\"}),`\n`,(0,t.jsx)(e.p,{children:(0,t.jsx)(e.img,{src:\"/blog-images/470546e1232748209628bb1f33220896.png\",alt:\"\"})}),`\n`,(0,t.jsxs)(e.h3,{id:\"kubernetes-api\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#kubernetes-api\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Kubernetes API\"]}),`\n`,(0,t.jsx)(e.p,{children:`Before we dive into custom resource definitions, let's first talk about Kubernetes in general. If I asked you, \"What is Kubernetes?\" then you'd probably answer, \"Kubernetes is a container orchestrator.\" This would, of course, be one correct answer.`}),`\n`,(0,t.jsx)(e.p,{children:\"But by looking under the Kubernetes hood, you could see that the main component of Kubernetes is an API server and etcd data store. And there are other, more important components like kube-scheduler, kube-controller-manager, and cloud-controller-manager, but pretty much any operation on your cluster needs to go through an API server.\"}),`\n`,(0,t.jsxs)(e.p,{children:[\"That API has a few built-in objects that it understands. Things you may be familiar with like Pods, Namespaces, ConfigMaps, Services, or Nodes are all API objects. That's why whenever you execute \",(0,t.jsx)(e.strong,{children:\"kubectl get pods\"}),\" or \",(0,t.jsx)(e.strong,{children:\"kubectl get nodes\"}),\", you get a list of pods or nodes. But if you try to get a list of objects that don't exist in the Kubernetes API\\u2014like, for example, \",(0,t.jsx)(e.strong,{children:\"kubectl get biscuits\"}),\"\\u2014you'd get a response similar to this:\"]}),`\n`,(0,t.jsx)(e.p,{children:`error: the server doesn't have a resource type \"biscuits\"`}),`\n`,(0,t.jsx)(e.p,{children:`And this is because there is no such thing as \"biscuits\" defined in the Kubernetes API. Quite logical, right? Well, what if I told you that you could add a biscuits definition to your Kubernetes cluster? In fact, you can extend your Kubernetes API with any custom object you like. That's exactly what custom resource definitions are for.`}),`\n`,(0,t.jsxs)(e.h3,{id:\"why-crds\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#why-crds\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Why CRDs?\"]}),`\n`,(0,t.jsx)(e.p,{children:\"So what's the point of adding a biscuits definition to your Kubernetes cluster? Remember when I mentioned earlier that the success of Kubernetes comes from its flexibility? The ability to extend the Kubernetes API with custom resource definitions is a really great feature that lets you do something magical. It allows you to instruct Kubernetes to manage more than just containers.\"}),`\n`,(0,t.jsx)(e.p,{children:\"Why is that such a great thing? Because CRDs together with Kubernetes operators give you almost unlimited possibilities. You can adapt Kubernetes in a way that it will take care of older parts of your infrastructure. If you do it right, you'll be able to avoid bottlenecks and easily modernise things that normally would require long and costly redesigns.\"}),`\n`,(0,t.jsxs)(e.h3,{id:\"crds-on-your-cluster\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#crds-on-your-cluster\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"CRDs on Your Cluster\"]}),`\n`,(0,t.jsx)(e.p,{children:\"Before we dive into creating our own CRD, you need to know two things.\"}),`\n`,(0,t.jsx)(e.p,{children:\"Firstly, creating a custom resource definition is an advanced topic. Many companies don't even need to create any CRDs. The Kubernetes community finds interesting solutions for common problems all the time, and it's likely that any use case you encounter probably already has a CRD you can use! And if you're still new to Kubernetes, you definitely shouldn't jump into CRDs before you understand the basics well.\"}),`\n`,(0,t.jsx)(e.p,{children:\"Secondly, as already mentioned, you don't need to create any CRDs yourself if you don't feel the need to. However, many Kubernetes tools will install their own CRDs, so even if you don't create any yourself, you'll probably still end up having some on your cluster.\"}),`\n`,(0,t.jsxs)(e.p,{children:[\"One example is \",(0,t.jsx)(e.a,{href:\"https://cert-manager.io\",children:\"cert-manager\"}),\", a very popular Kubernetes tool for managing certificates. It installs a few CRDs on your cluster in order to do its job. If you execute \",(0,t.jsx)(e.strong,{children:\"kubectl get clusterissuers\"}),\" before installation of cert-manager, your cluster won't know what ClusterIssuers are:\"]}),`\n`,(0,t.jsx)(e.p,{children:(0,t.jsx)(e.code,{children:`error: the server doesn't have a resource type \"clusterissuers\"`})}),`\n`,(0,t.jsx)(e.p,{children:\"But if you execute the same command after cert-manager installation, you'll get the list of ClusterIssuers on your cluster.\"}),`\n`,(0,t.jsxs)(e.p,{children:[\"In fact, you can list all custom resource definitions installed on your cluster by executing \",(0,t.jsx)(e.strong,{children:\"kubectl get crd\"}),\":\"]}),`\n`,(0,t.jsx)(e.p,{children:(0,t.jsx)(e.code,{children:\"$ kubectl get crd NAME \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0CREATED AT addons.k3s.cattle.io \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA02022-01-23T12:48:31Z helmcharts.helm.cattle.io \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 2022-01-23T12:48:31Z helmchartconfigs.helm.cattle.io \\xA0 \\xA0 \\xA0 \\xA0 2022-01-23T12:48:31Z serverstransports.traefik.containo.us \\xA0 2022-01-23T12:49:48Z tlsoptions.traefik.containo.us \\xA0 \\xA0 \\xA0 \\xA0 \\xA02022-01-23T12:49:48Z ingressroutetcps.traefik.containo.us \\xA0 \\xA02022-01-23T12:49:48Z ingressroutes.traefik.containo.us \\xA0 \\xA0 \\xA0 2022-01-23T12:49:48Z tlsstores.traefik.containo.us \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 2022-01-23T12:49:48Z middlewares.traefik.containo.us \\xA0 \\xA0 \\xA0 \\xA0 2022-01-23T12:49:48Z traefikservices.traefik.containo.us \\xA0 \\xA0 2022-01-23T12:49:48Z middlewaretcps.traefik.containo.us \\xA0 \\xA0 \\xA02022-01-23T12:49:48Z ingressrouteudps.traefik.containo.us \\xA0 \\xA02022-01-23T12:49:48Z\"})}),`\n`,(0,t.jsxs)(e.p,{children:[\"The above output comes in the form of \",(0,t.jsx)(e.strong,{children:\"OBJECT.GROUP\"}),\" and tells me that I can execute commands like \",(0,t.jsx)(e.strong,{children:\"kubectl get addons\"}),\"...\"]}),`\n`,(0,t.jsxs)(e.h3,{id:\"how-to-create-crds\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#how-to-create-crds\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"How to Create CRDs\"]}),`\n`,(0,t.jsx)(e.p,{children:\"OK, forget about biscuits. Let's take a look at some more realistic examples. Imagine that you want Kubernetes to somehow manage your custom routers in your datacenter. For that, you could create a custom resource definition similar to this one:\"}),`\n`,(0,t.jsx)(e.p,{children:(0,t.jsx)(e.code,{children:'apiVersion: apiextensions.k8s.io/v1 kind: CustomResourceDefinition metadata:  \\xA0# Name of your CRD. Must match the spec block below, and be in the form: .  name: routers.example.com spec: # Group name to use for REST API: /apis//  group: example.com  names: # Plural name to be used in the URL: /apis///  \\xA0 plural: routers  \\xA0 # Singular name to be used as an alias on the CLI and for display  \\xA0 singular: router  \\xA0 # Kind is normally the CamelCased singular type. Your resource manifests use this.  \\xA0 kind: Router  \\xA0 # ShortNames allow shorter string to match your resource on the CLI  \\xA0 shortNames:  \\xA0 - rt  # Scope can be either Namespaced or Cluster-wide  scope: Cluster  versions:  \\xA0 - name: v1  \\xA0 \\xA0 # Each version can be enabled/disabled by Served flag.  \\xA0 \\xA0 served: true  \\xA0 \\xA0 # One and only one version must be marked as the storage version.  \\xA0 \\xA0 storage: true  \\xA0 \\xA0 schema:  \\xA0 \\xA0 \\xA0 openAPIV3Schema:  \\xA0 \\xA0 \\xA0 \\xA0 type: object  \\xA0 \\xA0 \\xA0 \\xA0 properties:  \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 spec:  \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 type: object  \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 properties:  \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 dataCenter:  \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 type: string  \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 rack:  \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 type: integer  \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 type:  \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 type: string  \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 enum:  \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 - f5  \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 - virtual  \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 required: [\"dataCenter\", \"rack\", \"type\"]  \\xA0 \\xA0 \\xA0 \\xA0 required: [\"spec\"]'})}),`\n`,(0,t.jsxs)(e.p,{children:[\"You can apply the above CRD to the cluster by executing \",(0,t.jsx)(e.strong,{children:\"kubectl apply -f router-CRD.yaml\"}),`. Once you do that, your Kubernetes cluster will already know what \"router\" is. Therefore, you'll be able to execute `,(0,t.jsx)(e.strong,{children:\"kubectl get routers\"}),\". Of course, we just applied the resource definition, not the resource itself. So \",(0,t.jsx)(e.strong,{children:\"kubectl get routers\"}),\" will return the following:\"]}),`\n`,(0,t.jsx)(e.p,{children:(0,t.jsx)(e.code,{children:\"No resources found.\"})}),`\n`,(0,t.jsx)(e.p,{children:\"But as you can see, it doesn't return this:\"}),`\n`,(0,t.jsx)(e.p,{children:(0,t.jsx)(e.code,{children:`error: the server doesn't have a resource type \"routers\"`})}),`\n`,(0,t.jsx)(e.p,{children:\"Which means we successfully added a new object to the Kubernetes API. To add an actual router resource, you can construct a YAML definition file like with any other object:\"}),`\n`,(0,t.jsx)(e.p,{children:(0,t.jsx)(e.code,{children:\"apiVersion: example.com/v1 kind: Router metadata:  name: example-router spec:  dataCenter: eu-1  rack: 3  type: virtual\"})}),`\n`,(0,t.jsx)(e.p,{children:\"\\xA0\"}),`\n`,(0,t.jsxs)(e.p,{children:[\"Now, you can create a new router on your cluster by executing \",(0,t.jsx)(e.strong,{children:\"kubectl apply - f example-router.yaml\"}),\", and if you try to get the list of routers again with \",(0,t.jsx)(e.strong,{children:\"kubectl get routers\"}),\", you should see one now:\"]}),`\n`,(0,t.jsx)(e.p,{children:(0,t.jsx)(e.code,{children:\"$ kubectl get routers NAME \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 AGE example-router \\xA0 4s\"})}),`\n`,(0,t.jsx)(e.p,{children:\"\\xA0\"}),`\n`,(0,t.jsx)(e.p,{children:\"Congratulations! You just extended the Kubernetes API.\"}),`\n`,(0,t.jsx)(e.p,{children:(0,t.jsx)(e.img,{src:\"/blog-images/785fbfe1e3cec8abffd9eff571966715.png\",alt:\"\"})}),`\n`,(0,t.jsxs)(e.h3,{id:\"what-to-do-with-crds\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#what-to-do-with-crds\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),(0,t.jsx)(e.strong,{children:\"What to Do With CRDs\"})]}),`\n`,(0,t.jsx)(e.p,{children:`You may be thinking, \"OK, great, but that router doesn't do anything!\" And yes, that's right. In its current form, our CRD doesn't do anything besides being processed and stored by the Kubernetes API. And while there are use cases where this is enough, usually CRDs are combined with custom controllers.`}),`\n`,(0,t.jsxs)(e.p,{children:[\"Custom controllers are another concept in Kubernetes that lets you actually do something with your custom resources. In our case, we would like to actually create or configure the routers in our datacenter. Therefore, we'd have to write a custom controller and instruct it to listen to the Kubernetes API and wait for any changes to our custom \",(0,t.jsx)(e.strong,{children:\"router\"}),\" objects.\"]}),`\n`,(0,t.jsx)(e.p,{children:\"Custom controllers under the hood are just applications or scripts written in your programming language of choice. They're deployed on the cluster as pods, and their job is to listen to the Kubernetes API and perform some actions based on defined logic.\"}),`\n`,(0,t.jsxs)(e.h4,{id:\"crd-vs-configmap\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#crd-vs-configmap\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"CRD vs. ConfigMap\"]}),`\n`,(0,t.jsx)(e.p,{children:\"Last but not least, by looking at CRDs, you may see some similarities with a Kubernetes built-in object, ConfigMap. And if you use CRDs without a custom controller, they may, in fact, serve a similar purpose. They both can be used to store custom configurations. However, there are noticeable differences between them.\"}),`\n`,(0,t.jsx)(e.p,{children:\"First of all, ConfigMaps by design are meant to provide configuration for your pods. They can be mounted as files or environment variables into the pod. They work well if you have well-defined config files like, for example, Apache or MySQL config.\"}),`\n`,(0,t.jsx)(e.p,{children:\"CRDs can also be consumed by pods but only by contacting the Kubernetes API. They simply have a different purpose than ConfigMaps. They're not meant to be used to provide configuration to your pods but to extend the Kubernetes API in order to build custom automation.\"}),`\n`,(0,t.jsxs)(e.h3,{id:\"summary\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#summary\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Summary\"]}),`\n`,(0,t.jsx)(e.p,{children:\"Kubernetes's flexibility is what made it so successful (among other things, of course). Now, you can make use of that flexibility by creating your own Kubernetes objects. The possibilities are almost limitless, and it's only up to you how you'll make use of CRDs.\"}),`\n`,(0,t.jsxs)(e.p,{children:[\"Come back to us for more Kubernetes articles. \",(0,t.jsx)(e.a,{href:\"https://release.com/blog/kubernetes-daemonset-tutorial\",children:\"Here's\"}),\" our article explaining another Kubernetes object, DaemonSets. Also, feel free to take a look at our offerings. We simplify the development process by providing \",(0,t.jsx)(e.a,{href:\"https://release.com/\",children:\"Environments as a Service.\"})]})]})}function C(o={}){let{wrapper:e}=o.components||{};return e?(0,t.jsx)(e,Object.assign({},o,{children:(0,t.jsx)(u,o)})):u(o)}var v=C;return w(K);})();\n;return Component;"
        },
        "_id": "blog/posts/kubernetes-crds.mdx",
        "_raw": {
          "sourceFilePath": "blog/posts/kubernetes-crds.mdx",
          "sourceFileName": "kubernetes-crds.mdx",
          "sourceFileDir": "blog/posts",
          "contentType": "mdx",
          "flattenedPath": "blog/posts/kubernetes-crds"
        },
        "type": "BlogPost",
        "computedSlug": "kubernetes-crds"
      },
      "documentHash": "1739393595022",
      "hasWarnings": false,
      "documentTypeName": "BlogPost"
    },
    "blog/posts/kubernetes-daemonset-tutorial.mdx": {
      "document": {
        "title": "Kubernetes DaemonSets: A Detailed Introductory Tutorial",
        "summary": "Kubernetes deployment strategy: DaemonSets. What are they, what advantages they bring, and when to use them.",
        "publishDate": "Fri Jan 07 2022 17:33:41 GMT+0000 (Coordinated Universal Time)",
        "author": "dawid-ziolkowski",
        "readingTime": 5,
        "categories": [
          "kubernetes",
          "platform-engineering"
        ],
        "mainImage": "/blog-images/4b7c9ffacf7bcc55f80f55cf631007a5.jpg",
        "imageAlt": "keyboard kubernetes daemonsets",
        "showCTA": true,
        "ctaCopy": "Automate Kubernetes environment setup with Release for streamlined DaemonSets deployment and efficient container management.",
        "ctaLink": "https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=kubernetes-daemonset-tutorial",
        "relatedPosts": [
          ""
        ],
        "ogImage": "/blog-images/4b7c9ffacf7bcc55f80f55cf631007a5.jpg",
        "excerpt": "Kubernetes deployment strategy: DaemonSets. What are they, what advantages they bring, and when to use them.",
        "tags": [
          "kubernetes",
          "platform-engineering"
        ],
        "ctaButton": "Try Release for Free",
        "body": {
          "raw": "\nKubernetes is one of the most popular container orchestrator systems. One of the reasons for its popularity is the fact that it offloads you from a lot of maintenance tasks when it comes to containers. It does a lot of stuff for you. For example, Kubernetes saves you a lot of time from planning where to deploy your microservices and spending even more time making sure that all the pods are distributed equally across all available nodes.\n\nBut as with everything, there is no one solution that suits all use cases. That's why Kubernetes has a few different types of deployment strategies. In this post, you'll learn what DaemonSets are, what advantages they bring, and when to use them.\n\n### What Is Kubernetes Deployment?\n\nBefore we dive into DaemonSets, let's make sure we understand the general concept of Kubernetes workflows. [Kubernetes](https://en.wikipedia.org/wiki/Kubernetes) is quite a complex system with a lot of components and options. There are many choices for networking, storage, scaling, etc. But the core function of Kubernetes is to run containers. So, if you want to instruct Kubernetes to run a container (as a [pod](https://kubernetes.io/docs/concepts/workloads/pods/)), you need to create a workflow.\n\n![](/blog-images/629886596ac84b7eb89ebf0aa8228d19.png)\n\nAs with anything else on Kubernetes, there are a few configuration options for workflows. The most common type of workflow is Deployment. Creating a Deployment means telling Kubernetes, \"Please run a container from this Docker image.\" This is, of course, a hugely simplified explanation, but you get the idea. To create a workflow of a Deployment type, you need to include just that in your typical Kubernetes YAML definition:\n\n```yaml\n\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n\t(...)\nspec:\n\t(...)\n\n```\n\n### Deployment in Action\n\nSo what happens when you create a Deployment? Kubernetes will first find appropriate nodes to run your pod. One of the main criteria for being \"appropriate\" is the load on the node. Kubernetes, by default, will try to distribute the load across all nodes. So, for example, say you have five nodes, and on four of them you have 10 pods running, whereas the last one is running only eight. There's a high chance that Kubernetes will schedule any new Deployment on that last node. Also, when one of the nodes becomes unavailable for whatever reason, Kubernetes will try to reschedule all the pods that were running on that node to the remaining nodes, and again, it will try to distribute these pods to all nodes.\n\nAll of this decision-making on where to schedule containers is happening under the hood, and you don't need to worry about where your pods will be scheduled. This is one of the main features of Kubernetes. You just add new nodes whenever your cluster becomes saturated, and Kubernetes does all the management for you.\n\n### Different Types of Workflows\n\nAll of the above is just Kubernetes' default behavior. Of course, sometimes you actually may want to have more control over the scheduling process. You may want to schedule some microservices on specific nodes, something that's often used with multiple node pools. For example, you may want to add a few nodes with high-performance graphics cards and schedule some big data for AI processing microservices specifically on these nodes. This is just one example. There are more use cases where you may want a different behavior from Kubernetes than the default \"schedule my pods anywhere.\" One such use case is the need for scheduling a copy of a pod on every single node. Let me now introduce you to DaemonSets.\n\n### Enter DaemonSet\n\nSo why would you want to schedule the same containers on every single node? There are many possible reasons. The most common one is the need for scheduling a \"daemon\"-type application that needs to perform some action on every node. Common examples are logs or metrics-gathering daemons. It's also possible to schedule a copy of a pod not on all nodes but on a subset of them. This can be useful for scheduling a daemon-type pod, for example, only on a specific node pool.\n\nFor instance, if you want to get metrics (like CPU or RAM usage) from each node, the best option is to schedule a container on every node that will gather these metrics from each individual node. Why not simply schedule one container instead that will gather metrics from all nodes? Well, you would run the risk that the node on which the metrics are running dies for whatever reason, and you'd lose metrics from the whole cluster. Of course, Kubernetes would redeploy that service on another node. But depending on how busy your cluster is, that could take a while, and therefore, you would miss some of the data. In the case of metrics, maybe it wouldn't be such a big deal, but imagine losing logs from all containers for a moment.\n\nBut besides these common use cases, you may simply want to have a copy of the same container on every node for any application-specific use case—things like node-local application caches, for example.\n\n### DaemonSets in Detail\n\nNow that we understand the need behind DaemonSets, let's talk about them in more detail. We know already that the main point of a DaemonSet is to ensure that all nodes are running a copy of a pod. Therefore, unlike with a typical Kubernetes Deployment, you don't specify how many pods you want to run. Kubernetes will automatically run as many pods as you have nodes. Another difference from normal deployment is the fact that in case of a node being removed from the cluster, Kubernetes won't move the pod that belongs to the DaemonSet to a different node but instead will simply destroy it.\n\nSo how do you create a workflow with DaemonSet? Very similarly to a normal Deployment. In fact, as with any other Kubernetes definition, you need to prepare a YAML definition with **apiVersion**, **kind**, and **metadata** fields. However, instead of **Deployment**, the **kind** value, in this case, will be **DaemonSet**. So an example DaemonSet YAML definition could look like this:\n\n```yaml\napiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: fluentd-daemon\nspec:\n  selector:\n    matchLabels:\n      name: fluentd-daemon\n  template:\n    metadata:\n      labels:\n        name: fluentd-daemon\n    spec:\n      containers:\n        - image: fluent/fluentd\n          name: fluentd-daemon\n```\n\nFollowing the idea of a DaemonSet, the above definition will deploy a **fluentd** pod on every node in the cluster. Kubernetes will make sure that there's only one pod on every node. For example, if you have five nodes, you'll have five **fluentd** pods running. If one of the nodes becomes unavailable, you'll have four **fluentd** pods running.\n\n![](/blog-images/93e833a771bb0786e80b1ab5172856b9.png)\n\n### Summary\n\nKubernetes DaemonSets can be a bit tricky to understand at first. They may seem like something against the whole point of Kubernetes. But just like with anything else, there are use cases where something that seems odd is actually useful. In the case of Kubernetes DaemonSets, they're quite commonly used for things like logs or monitoring. Also, don't forget that the main advantages of Kubernetes are flexibility and the ability to adjust it to different companies and infrastructures.\n\nOf course, no one will force you to use DaemonSets. It's totally fine to not use them if you feel like you don't need them. But on the other hand, when you do actually need a daemon-like functionality, it's way better and easier to use DaemonSets than trying to achieve the same with normal Kubernetes Deployment. If you want to learn more about Kubernetes, check out [this post about advanced concepts for Kubernetes pods](https://releasehub.com/blog/kubernetes-pods-advanced-concepts-explained).\n",
          "code": "var Component=(()=>{var c=Object.create;var s=Object.defineProperty;var u=Object.getOwnPropertyDescriptor;var m=Object.getOwnPropertyNames;var f=Object.getPrototypeOf,p=Object.prototype.hasOwnProperty;var y=(t,e)=>()=>(e||t((e={exports:{}}).exports,e),e.exports),g=(t,e)=>{for(var o in e)s(t,o,{get:e[o],enumerable:!0})},r=(t,e,o,i)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let a of m(e))!p.call(t,a)&&a!==o&&s(t,a,{get:()=>e[a],enumerable:!(i=u(e,a))||i.enumerable});return t};var b=(t,e,o)=>(o=t!=null?c(f(t)):{},r(e||!t||!t.__esModule?s(o,\"default\",{value:t,enumerable:!0}):o,t)),w=t=>r(s({},\"__esModule\",{value:!0}),t);var d=y((x,l)=>{l.exports=_jsx_runtime});var K={};g(K,{default:()=>D,frontmatter:()=>k});var n=b(d()),k={title:\"Kubernetes DaemonSets: A Detailed Introductory Tutorial\",summary:\"Kubernetes deployment strategy: DaemonSets. What are they, what advantages they bring, and when to use them.\",publishDate:\"Fri Jan 07 2022 17:33:41 GMT+0000 (Coordinated Universal Time)\",author:\"dawid-ziolkowski\",readingTime:5,categories:[\"kubernetes\",\"platform-engineering\"],mainImage:\"/blog-images/4b7c9ffacf7bcc55f80f55cf631007a5.jpg\",imageAlt:\"keyboard kubernetes daemonsets\",showCTA:!0,ctaCopy:\"Automate Kubernetes environment setup with Release for streamlined DaemonSets deployment and efficient container management.\",ctaLink:\"https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=kubernetes-daemonset-tutorial\",relatedPosts:[\"\"],ogImage:\"/blog-images/4b7c9ffacf7bcc55f80f55cf631007a5.jpg\",excerpt:\"Kubernetes deployment strategy: DaemonSets. What are they, what advantages they bring, and when to use them.\",tags:[\"kubernetes\",\"platform-engineering\"],ctaButton:\"Try Release for Free\"};function h(t){let e=Object.assign({p:\"p\",h3:\"h3\",a:\"a\",span:\"span\",img:\"img\",pre:\"pre\",code:\"code\",strong:\"strong\"},t.components);return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(e.p,{children:\"Kubernetes is one of the most popular container orchestrator systems. One of the reasons for its popularity is the fact that it offloads you from a lot of maintenance tasks when it comes to containers. It does a lot of stuff for you. For example, Kubernetes saves you a lot of time from planning where to deploy your microservices and spending even more time making sure that all the pods are distributed equally across all available nodes.\"}),`\n`,(0,n.jsx)(e.p,{children:\"But as with everything, there is no one solution that suits all use cases. That's why Kubernetes has a few different types of deployment strategies. In this post, you'll learn what DaemonSets are, what advantages they bring, and when to use them.\"}),`\n`,(0,n.jsxs)(e.h3,{id:\"what-is-kubernetes-deployment\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#what-is-kubernetes-deployment\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"What Is Kubernetes Deployment?\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"Before we dive into DaemonSets, let's make sure we understand the general concept of Kubernetes workflows. \",(0,n.jsx)(e.a,{href:\"https://en.wikipedia.org/wiki/Kubernetes\",children:\"Kubernetes\"}),\" is quite a complex system with a lot of components and options. There are many choices for networking, storage, scaling, etc. But the core function of Kubernetes is to run containers. So, if you want to instruct Kubernetes to run a container (as a \",(0,n.jsx)(e.a,{href:\"https://kubernetes.io/docs/concepts/workloads/pods/\",children:\"pod\"}),\"), you need to create a workflow.\"]}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/629886596ac84b7eb89ebf0aa8228d19.png\",alt:\"\"})}),`\n`,(0,n.jsx)(e.p,{children:'As with anything else on Kubernetes, there are a few configuration options for workflows. The most common type of workflow is Deployment. Creating a Deployment means telling Kubernetes, \"Please run a container from this Docker image.\" This is, of course, a hugely simplified explanation, but you get the idea. To create a workflow of a Deployment type, you need to include just that in your typical Kubernetes YAML definition:'}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-yaml\",children:`\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n\t(...)\nspec:\n\t(...)\n\n`})}),`\n`,(0,n.jsxs)(e.h3,{id:\"deployment-in-action\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#deployment-in-action\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Deployment in Action\"]}),`\n`,(0,n.jsx)(e.p,{children:`So what happens when you create a Deployment? Kubernetes will first find appropriate nodes to run your pod. One of the main criteria for being \"appropriate\" is the load on the node. Kubernetes, by default, will try to distribute the load across all nodes. So, for example, say you have five nodes, and on four of them you have 10 pods running, whereas the last one is running only eight. There's a high chance that Kubernetes will schedule any new Deployment on that last node. Also, when one of the nodes becomes unavailable for whatever reason, Kubernetes will try to reschedule all the pods that were running on that node to the remaining nodes, and again, it will try to distribute these pods to all nodes.`}),`\n`,(0,n.jsx)(e.p,{children:\"All of this decision-making on where to schedule containers is happening under the hood, and you don't need to worry about where your pods will be scheduled. This is one of the main features of Kubernetes. You just add new nodes whenever your cluster becomes saturated, and Kubernetes does all the management for you.\"}),`\n`,(0,n.jsxs)(e.h3,{id:\"different-types-of-workflows\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#different-types-of-workflows\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Different Types of Workflows\"]}),`\n`,(0,n.jsx)(e.p,{children:`All of the above is just Kubernetes' default behavior. Of course, sometimes you actually may want to have more control over the scheduling process. You may want to schedule some microservices on specific nodes, something that's often used with multiple node pools. For example, you may want to add a few nodes with high-performance graphics cards and schedule some big data for AI processing microservices specifically on these nodes. This is just one example. There are more use cases where you may want a different behavior from Kubernetes than the default \"schedule my pods anywhere.\" One such use case is the need for scheduling a copy of a pod on every single node. Let me now introduce you to DaemonSets.`}),`\n`,(0,n.jsxs)(e.h3,{id:\"enter-daemonset\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#enter-daemonset\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Enter DaemonSet\"]}),`\n`,(0,n.jsx)(e.p,{children:`So why would you want to schedule the same containers on every single node? There are many possible reasons. The most common one is the need for scheduling a \"daemon\"-type application that needs to perform some action on every node. Common examples are logs or metrics-gathering daemons. It's also possible to schedule a copy of a pod not on all nodes but on a subset of them. This can be useful for scheduling a daemon-type pod, for example, only on a specific node pool.`}),`\n`,(0,n.jsx)(e.p,{children:\"For instance, if you want to get metrics (like CPU or RAM usage) from each node, the best option is to schedule a container on every node that will gather these metrics from each individual node. Why not simply schedule one container instead that will gather metrics from all nodes? Well, you would run the risk that the node on which the metrics are running dies for whatever reason, and you'd lose metrics from the whole cluster. Of course, Kubernetes would redeploy that service on another node. But depending on how busy your cluster is, that could take a while, and therefore, you would miss some of the data. In the case of metrics, maybe it wouldn't be such a big deal, but imagine losing logs from all containers for a moment.\"}),`\n`,(0,n.jsx)(e.p,{children:\"But besides these common use cases, you may simply want to have a copy of the same container on every node for any application-specific use case\\u2014things like node-local application caches, for example.\"}),`\n`,(0,n.jsxs)(e.h3,{id:\"daemonsets-in-detail\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#daemonsets-in-detail\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"DaemonSets in Detail\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Now that we understand the need behind DaemonSets, let's talk about them in more detail. We know already that the main point of a DaemonSet is to ensure that all nodes are running a copy of a pod. Therefore, unlike with a typical Kubernetes Deployment, you don't specify how many pods you want to run. Kubernetes will automatically run as many pods as you have nodes. Another difference from normal deployment is the fact that in case of a node being removed from the cluster, Kubernetes won't move the pod that belongs to the DaemonSet to a different node but instead will simply destroy it.\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"So how do you create a workflow with DaemonSet? Very similarly to a normal Deployment. In fact, as with any other Kubernetes definition, you need to prepare a YAML definition with \",(0,n.jsx)(e.strong,{children:\"apiVersion\"}),\", \",(0,n.jsx)(e.strong,{children:\"kind\"}),\", and \",(0,n.jsx)(e.strong,{children:\"metadata\"}),\" fields. However, instead of \",(0,n.jsx)(e.strong,{children:\"Deployment\"}),\", the \",(0,n.jsx)(e.strong,{children:\"kind\"}),\" value, in this case, will be \",(0,n.jsx)(e.strong,{children:\"DaemonSet\"}),\". So an example DaemonSet YAML definition could look like this:\"]}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-yaml\",children:`apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: fluentd-daemon\nspec:\n  selector:\n  \\xA0 matchLabels:\n  \\xA0 \\xA0 name: fluentd-daemon\n  template:\n  \\xA0 metadata:\n  \\xA0 \\xA0 labels:\n  \\xA0 \\xA0 \\xA0 name: fluentd-daemon\n  \\xA0 spec:\n  \\xA0 \\xA0 containers:\n  \\xA0 \\xA0 \\xA0 - image: fluent/fluentd\n  \\xA0 \\xA0 \\xA0 \\xA0 name: fluentd-daemon\n`})}),`\n`,(0,n.jsxs)(e.p,{children:[\"Following the idea of a DaemonSet, the above definition will deploy a \",(0,n.jsx)(e.strong,{children:\"fluentd\"}),\" pod on every node in the cluster. Kubernetes will make sure that there's only one pod on every node. For example, if you have five nodes, you'll have five \",(0,n.jsx)(e.strong,{children:\"fluentd\"}),\" pods running. If one of the nodes becomes unavailable, you'll have four \",(0,n.jsx)(e.strong,{children:\"fluentd\"}),\" pods running.\"]}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/93e833a771bb0786e80b1ab5172856b9.png\",alt:\"\"})}),`\n`,(0,n.jsxs)(e.h3,{id:\"summary\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#summary\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Summary\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Kubernetes DaemonSets can be a bit tricky to understand at first. They may seem like something against the whole point of Kubernetes. But just like with anything else, there are use cases where something that seems odd is actually useful. In the case of Kubernetes DaemonSets, they're quite commonly used for things like logs or monitoring. Also, don't forget that the main advantages of Kubernetes are flexibility and the ability to adjust it to different companies and infrastructures.\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"Of course, no one will force you to use DaemonSets. It's totally fine to not use them if you feel like you don't need them. But on the other hand, when you do actually need a daemon-like functionality, it's way better and easier to use DaemonSets than trying to achieve the same with normal Kubernetes Deployment. If you want to learn more about Kubernetes, check out \",(0,n.jsx)(e.a,{href:\"https://releasehub.com/blog/kubernetes-pods-advanced-concepts-explained\",children:\"this post about advanced concepts for Kubernetes pods\"}),\".\"]})]})}function v(t={}){let{wrapper:e}=t.components||{};return e?(0,n.jsx)(e,Object.assign({},t,{children:(0,n.jsx)(h,t)})):h(t)}var D=v;return w(K);})();\n;return Component;"
        },
        "_id": "blog/posts/kubernetes-daemonset-tutorial.mdx",
        "_raw": {
          "sourceFilePath": "blog/posts/kubernetes-daemonset-tutorial.mdx",
          "sourceFileName": "kubernetes-daemonset-tutorial.mdx",
          "sourceFileDir": "blog/posts",
          "contentType": "mdx",
          "flattenedPath": "blog/posts/kubernetes-daemonset-tutorial"
        },
        "type": "BlogPost",
        "computedSlug": "kubernetes-daemonset-tutorial"
      },
      "documentHash": "1739393595022",
      "hasWarnings": false,
      "documentTypeName": "BlogPost"
    },
    "blog/posts/kubernetes-environment-variables.mdx": {
      "document": {
        "title": "How to Make the Most of Kubernetes Environment Variables",
        "summary": "There are a few ways to add Kubernetes environment variables. In this post, we are going to learn what they are and when",
        "publishDate": "Mon Feb 07 2022 09:44:46 GMT+0000 (Coordinated Universal Time)",
        "author": "dawid-ziolkowski",
        "readingTime": 5,
        "categories": [
          "kubernetes",
          "platform-engineering"
        ],
        "mainImage": "/blog-images/e99f6e1d23023143b231d942ed8b8ff8.jpg",
        "imageAlt": "kubernetes environment variable secret",
        "showCTA": true,
        "ctaCopy": "Unlock the power of Kubernetes environment variables with Release's on-demand environments for seamless configuration management and deployment.",
        "ctaLink": "https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=kubernetes-environment-variables",
        "relatedPosts": [
          ""
        ],
        "ogImage": "/blog-images/e99f6e1d23023143b231d942ed8b8ff8.jpg",
        "excerpt": "There are a few ways to add Kubernetes environment variables. In this post, we are going to learn what they are and when",
        "tags": [
          "kubernetes",
          "platform-engineering"
        ],
        "ctaButton": "Try Release for Free",
        "body": {
          "raw": "\nIn traditional systems, environment variables play an important role but not always a crucial one. Some applications make more use of environment variables than others. Some prefer configuration files over environment variables. However, when it comes to Kubernetes, environment variables are more important than you may think. It's partially due to how containers work in general and partially due to the specifics of Kubernetes. In this post, you'll learn all about environment variables in Kubernetes.\n\n![environment variables are dynamic key value variables](/blog-images/fa1486359141ce02924db9ee4ba2a0e8.png)\n\n### The Basics\n\nLet's start with the basics. What are environment variables, and why do they exist? Traditionally, environment variables are dynamic key value variables that are accessible to any process running on the system. The operating system itself will set many environment variables that help running processes understand the specifics of the system. Thanks to this, software developers can include logic in their software that makes the programs adjustable to a specific operating system. Environment variables also hold a lot of important information about the user. Things like username, preferred language, user home directory path, and many other useful bits of information.\n\n### User-Defined Environment Variables\n\nAs a user, you can easily create and access your own environment variables. On Unix-based systems, you can do that by executing the **export** command followed by the name of your variable and its value. So, for example, to create an environment variable called **myvar** with a value of **10**, you need to execute the following:\n\n```yaml\nexport myvar=10\n```\n\nYou can then access the value of your variable using a dollar sign followed by your variable name. In our case to print (using Linux command [echo](<https://en.wikipedia.org/wiki/Echo_(command)>)) the value of our variable, we can execute the following:\n\n```yaml\necho $myvar\n```\n\nAnd if you want to print all the environment variables, you can execute either **printenv** or **env** commands. All of this applies to applications running in your pods too.\n\n### Environment Variables in Kubernetes\n\nThe basic principle of environment variables in Kubernetes is the same. However, Kubernetes uses environment variables quite extensively and for a few different things. Therefore, it's good to understand what role environment variables play in Kubernetes. That's especially true if you want to migrate an existing application that doesn't use environment variables that much. You can still create your pods without any environment variables. But if you ignore environment variables in Kubernetes completely, you may lose some of the value of the more powerful Kubernetes features.\n\nBefore we move any further, you also need to know that it's generally good practice when developing microservices to provide configuration to your Docker containers as environment variables whenever possible. This way you can make your Docker image more generic and possibly reuse the same image for different purposes. With that being said, let's see how you can inject some environment variables into your Kubernetes pods.\n\n![The main use case for environment variables in Kubernetes](/blog-images/54e9ae96af3ef573e4be8b343d09d1fd.png)\n\n### Configuration for Your Pods\n\nThe main use case for environment variables in Kubernetes is similar to the one from traditional software development. That is to provide information about the environment to your software. This information is usually used to alter or adapt the way software works to the specifics of the environment. The definition may seem vague, so let me give you an example. Instead of having separate Docker images for your development and production environments, you can use the same image but run your application in a development or production mode based on an environment variable.\n\nIn Kubernetes, environment variables are scoped to a container, and there are three main ways of adding them. Let's break them down.\n\n#### Direct\n\nThe first option is the most straightforward. You can simply specify environment variables directly in your deployment definition with an **env** keyword:\n\n```yaml\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n   name: example-app\nspec:\n   replicas: 1\n   selector:\n     matchLabels:\n       app: example-app\n   template:\n     metadata:\n       labels:\n         app: example-app\n     spec:\n       containers:\n         - name: example-app-dev\n           image: [yourimage]\n           env:\n             - name: ENVIRONMENT\n               value: \"development\"\n```\n\nAs soon as your application is instructed to read the value of an environment variable called \"ENVIRONMENT\", you can use it directly to run your application in the desired mode.\n\nTo run the same application in a production mode, you can simply reuse the same deployment definition. You'll only need to change the environment variable value (and optionally a name of the pod):\n\n```yaml\n---\napiVersion: apps/v1\nkind: Deployment\n(...)\n      containers:\n        - name: example-app-prod\n          image: [yourimage]\n          env:\n            - name: ENVIRONMENT\n              value: \"production\"\n\n```\n\nHere's another example: Imagine that you have a web application that needs to download a product catalogue. This catalogue will then be served to the users. This catalogue may differ in a few ways (by, for example, a country, month, or supplier). This is a perfect use case for an environment variable. Instead of creating many different versions of your application to accommodate different download options, your application can remain generic. Which catalogue it has to download will be determined by the value of some specific environment variable.\n\n#### Secrets\n\nAnother way of providing environment variables to your application is by passing them from Kubernetes [secrets](https://kubernetes.io/docs/concepts/configuration/secret/). You may guess that this is a good option when you need to pass some sensitive information like passwords or tokens. This way you don't specify the value of the environment variable directly in the deployment as we did before. Instead, you instruct Kubernetes to take the value of a specified secret object and use it as a value of an environment variable for your pod.\n\nFor example, if you have a Kubernetes secret like this:\n\n```yaml\napiVersion: v1\nkind: Secret\nmetadata:\n   name: secret_data\ntype: Opaque\nstringData:\n     username: \"example\"\n     password: \"supersecretpassword\"\n```\n\nand you want to pass the password as an environment variable to your pod, you can reference it in the deployment definition as follows:\n\n```yaml\n---\napiVersion: apps/v1\nkind: Deployment\n(...)\n      containers:\n        - name: example-app-prod\n          image: [yourimage]\n          env:\n            # Inject variables from a Kuberentes secret\n            - name: secret_variables\n              valueFrom:\n                secretKeyRef:\n                  name: secret_data\n                  key: password\n\n```\n\nIn your pod, you will then be able to access the actual password (supersecretpassword) by accessing an environment variable called **secret_variable**. For example, in Python you could do it like this:\n\n```yaml\nimport osPASSWORD = os\n.environ.get['secret_variable']\n```\n\nAs you can see, in our example we have **username** and **password** defined in Kubernetes secret, but we are only passing the **password** value to the pod. If you want to pass all the secrets from a Kubernetes secret without specifying each key, you can use **secretRef** instead of **secretKeyRef**. This way, you only need to specify the Kubernetes secret object name, and all the values from it will be automatically loaded as environment variables:\n\n```yaml\n---\napiVersion: apps/v1\nkind: Deployment\n(...)\n      containers:\n        - name: example-app-prod\n          image: [yourimage]\n          env:\n            # Inject variables from a Kuberentes secret\n            - name: secret_variables\n              valueFrom:\n                secretRef:\n                  name: secret_data\n\n```\n\n#### ConfigMaps\n\nAnother way of injecting environment variables into your pods is by using values from [ConfigMaps](https://kubernetes.io/docs/concepts/configuration/configmap/). For example, if you have ConfigMap like this:\n\n```yaml\napiVersion: v1\nkind: ConfigMap\nmetadata:\n   name: config-data\ndata:\n   environment: \"dev\"\n   timezone: \"UTC\"\n```\n\nand you want to load both **environment** and **timezone** as environment variables into your pod, you can add the following **valueFrom** definition to your deployment:\n\n```yaml\n---\napiVersion: apps/v1\nkind: Deployment\n(...)\n      containers:\n        - name: example-app-prod\n          image: [yourimage]\n          env:\n            # Inject variables from a Kuberentes ConfigMap\n            - name: config_variables\n              valueFrom:\n                configMapRef:\n                  name: config-data\n\n```\n\nIn your pod, you'll then be able to see both environment variables as defined in your ConfigMap:\n\n```yaml\n# env\n\nHOSTNAME=5ad4e9e78e57\nenvironment=dev\ntimezone=UTC\n```\n\nAs with secrets, if you don't want to load all values from a ConfigMap, you can define specific keys instead by changing **configMapRef** to **configMapKeyRef.**\n\nThe main difference between passing environment variables from ConfigMaps and specifying them directly as in the first example is the fact that here the environment variable lifecycle is separated from the pod lifecycle. This means you can update the value of your variable independently from the running pod. Or, to put it differently, you'll need to restart the pod yourself in order to load the new value of the environment variables into the pod. On the other hand, when you specify environment variables directly in the deployment, every change to the variables will automatically trigger pod restart.\n\n### Summary\n\nEnvironment variables play an important role in Kubernetes. You can use them not only to provide basic information about the operating system to your application. You can also use them as the main configuration mechanism for your pods or for passing sensitive information. It's not uncommon in Kubernetes to extract as much configuration as possible info ConfigMaps and environment variables to keep your Docker images as generic as possible. As you can see, even something simple like environment variables have a few options in Kubernetes. If you want to learn more, [Regis Wilson wrote about why Kubernetes is hard and what you can do about it.](https://release.com/blog/why-kubernetes-is-so-hard)\n",
          "code": "var Component=(()=>{var m=Object.create;var r=Object.defineProperty;var h=Object.getOwnPropertyDescriptor;var p=Object.getOwnPropertyNames;var u=Object.getPrototypeOf,v=Object.prototype.hasOwnProperty;var f=(a,e)=>()=>(e||a((e={exports:{}}).exports,e),e.exports),y=(a,e)=>{for(var o in e)r(a,o,{get:e[o],enumerable:!0})},s=(a,e,o,t)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let i of p(e))!v.call(a,i)&&i!==o&&r(a,i,{get:()=>e[i],enumerable:!(t=h(e,i))||t.enumerable});return a};var g=(a,e,o)=>(o=a!=null?m(u(a)):{},s(e||!a||!a.__esModule?r(o,\"default\",{value:a,enumerable:!0}):o,a)),b=a=>s(r({},\"__esModule\",{value:!0}),a);var c=f((K,l)=>{l.exports=_jsx_runtime});var N={};y(N,{default:()=>x,frontmatter:()=>w});var n=g(c()),w={title:\"How to Make the Most of Kubernetes Environment Variables\",summary:\"There are a few ways to add Kubernetes environment variables. In this post, we are going to learn what they are and when\",publishDate:\"Mon Feb 07 2022 09:44:46 GMT+0000 (Coordinated Universal Time)\",author:\"dawid-ziolkowski\",readingTime:5,categories:[\"kubernetes\",\"platform-engineering\"],mainImage:\"/blog-images/e99f6e1d23023143b231d942ed8b8ff8.jpg\",imageAlt:\"kubernetes environment variable secret\",showCTA:!0,ctaCopy:\"Unlock the power of Kubernetes environment variables with Release's on-demand environments for seamless configuration management and deployment.\",ctaLink:\"https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=kubernetes-environment-variables\",relatedPosts:[\"\"],ogImage:\"/blog-images/e99f6e1d23023143b231d942ed8b8ff8.jpg\",excerpt:\"There are a few ways to add Kubernetes environment variables. In this post, we are going to learn what they are and when\",tags:[\"kubernetes\",\"platform-engineering\"],ctaButton:\"Try Release for Free\"};function d(a){let e=Object.assign({p:\"p\",img:\"img\",h3:\"h3\",a:\"a\",span:\"span\",strong:\"strong\",pre:\"pre\",code:\"code\",h4:\"h4\"},a.components);return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(e.p,{children:\"In traditional systems, environment variables play an important role but not always a crucial one. Some applications make more use of environment variables than others. Some prefer configuration files over environment variables. However, when it comes to Kubernetes, environment variables are more important than you may think. It's partially due to how containers work in general and partially due to the specifics of Kubernetes. In this post, you'll learn all about environment variables in Kubernetes.\"}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/fa1486359141ce02924db9ee4ba2a0e8.png\",alt:\"environment variables are dynamic key value variables\"})}),`\n`,(0,n.jsxs)(e.h3,{id:\"the-basics\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#the-basics\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"The Basics\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Let's start with the basics. What are environment variables, and why do they exist? Traditionally, environment variables are dynamic key value variables that are accessible to any process running on the system. The operating system itself will set many environment variables that help running processes understand the specifics of the system. Thanks to this, software developers can include logic in their software that makes the programs adjustable to a specific operating system. Environment variables also hold a lot of important information about the user. Things like username, preferred language, user home directory path, and many other useful bits of information.\"}),`\n`,(0,n.jsxs)(e.h3,{id:\"user-defined-environment-variables\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#user-defined-environment-variables\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"User-Defined Environment Variables\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"As a user, you can easily create and access your own environment variables. On Unix-based systems, you can do that by executing the \",(0,n.jsx)(e.strong,{children:\"export\"}),\" command followed by the name of your variable and its value. So, for example, to create an environment variable called \",(0,n.jsx)(e.strong,{children:\"myvar\"}),\" with a value of \",(0,n.jsx)(e.strong,{children:\"10\"}),\", you need to execute the following:\"]}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-yaml\",children:`export myvar=10\n`})}),`\n`,(0,n.jsxs)(e.p,{children:[\"You can then access the value of your variable using a dollar sign followed by your variable name. In our case to print (using Linux command \",(0,n.jsx)(e.a,{href:\"https://en.wikipedia.org/wiki/Echo_(command)\",children:\"echo\"}),\") the value of our variable, we can execute the following:\"]}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-yaml\",children:`echo $myvar\n`})}),`\n`,(0,n.jsxs)(e.p,{children:[\"And if you want to print all the environment variables, you can execute either \",(0,n.jsx)(e.strong,{children:\"printenv\"}),\" or \",(0,n.jsx)(e.strong,{children:\"env\"}),\" commands. All of this applies to applications running in your pods too.\"]}),`\n`,(0,n.jsxs)(e.h3,{id:\"environment-variables-in-kubernetes\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#environment-variables-in-kubernetes\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Environment Variables in Kubernetes\"]}),`\n`,(0,n.jsx)(e.p,{children:\"The basic principle of environment variables in Kubernetes is the same. However, Kubernetes uses environment variables quite extensively and for a few different things. Therefore, it's good to understand what role environment variables play in Kubernetes. That's especially true if you want to migrate an existing application that doesn't use environment variables that much. You can still create your pods without any environment variables. But if you ignore environment variables in Kubernetes completely, you may lose some of the value of the more powerful Kubernetes features.\"}),`\n`,(0,n.jsx)(e.p,{children:\"Before we move any further, you also need to know that it's generally good practice when developing microservices to provide configuration to your Docker containers as environment variables whenever possible. This way you can make your Docker image more generic and possibly reuse the same image for different purposes. With that being said, let's see how you can inject some environment variables into your Kubernetes pods.\"}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/54e9ae96af3ef573e4be8b343d09d1fd.png\",alt:\"The main use case for environment variables in Kubernetes\"})}),`\n`,(0,n.jsxs)(e.h3,{id:\"configuration-for-your-pods\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#configuration-for-your-pods\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Configuration for Your Pods\"]}),`\n`,(0,n.jsx)(e.p,{children:\"The main use case for environment variables in Kubernetes is similar to the one from traditional software development. That is to provide information about the environment to your software. This information is usually used to alter or adapt the way software works to the specifics of the environment. The definition may seem vague, so let me give you an example. Instead of having separate Docker images for your development and production environments, you can use the same image but run your application in a development or production mode based on an environment variable.\"}),`\n`,(0,n.jsx)(e.p,{children:\"In Kubernetes, environment variables are scoped to a container, and there are three main ways of adding them. Let's break them down.\"}),`\n`,(0,n.jsxs)(e.h4,{id:\"direct\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#direct\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Direct\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"The first option is the most straightforward. You can simply specify environment variables directly in your deployment definition with an \",(0,n.jsx)(e.strong,{children:\"env\"}),\" keyword:\"]}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-yaml\",children:`---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  \\xA0name: example-app\nspec:\n  \\xA0replicas: 1\n  \\xA0selector:\n  \\xA0 \\xA0matchLabels:\n  \\xA0 \\xA0 \\xA0app: example-app\n  \\xA0template:\n  \\xA0 \\xA0metadata:\n  \\xA0 \\xA0 \\xA0labels:\n  \\xA0 \\xA0 \\xA0 \\xA0app: example-app\n  \\xA0 \\xA0spec:\n  \\xA0 \\xA0 \\xA0containers:\n  \\xA0 \\xA0 \\xA0 \\xA0- name: example-app-dev\n  \\xA0 \\xA0 \\xA0 \\xA0 \\xA0image: [yourimage]\n  \\xA0 \\xA0 \\xA0 \\xA0 \\xA0env:\n  \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0- name: ENVIRONMENT\n  \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0value: \"development\"\n`})}),`\n`,(0,n.jsx)(e.p,{children:'As soon as your application is instructed to read the value of an environment variable called \"ENVIRONMENT\", you can use it directly to run your application in the desired mode.'}),`\n`,(0,n.jsx)(e.p,{children:\"To run the same application in a production mode, you can simply reuse the same deployment definition. You'll only need to change the environment variable value (and optionally a name of the pod):\"}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-yaml\",children:`---\napiVersion: apps/v1\nkind: Deployment\n(...)\n \\xA0 \\xA0 \\xA0containers:\n \\xA0 \\xA0 \\xA0 \\xA0- name: example-app-prod\n \\xA0 \\xA0 \\xA0 \\xA0 \\xA0image: [yourimage]\n \\xA0 \\xA0 \\xA0 \\xA0 \\xA0env:\n \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0- name: ENVIRONMENT\n \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0value: \"production\"\n\n`})}),`\n`,(0,n.jsx)(e.p,{children:\"Here's another example: Imagine that you have a web application that needs to download a product catalogue. This catalogue will then be served to the users. This catalogue may differ in a few ways (by, for example, a country, month, or supplier). This is a perfect use case for an environment variable. Instead of creating many different versions of your application to accommodate different download options, your application can remain generic. Which catalogue it has to download will be determined by the value of some specific environment variable.\"}),`\n`,(0,n.jsxs)(e.h4,{id:\"secrets\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#secrets\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Secrets\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"Another way of providing environment variables to your application is by passing them from Kubernetes \",(0,n.jsx)(e.a,{href:\"https://kubernetes.io/docs/concepts/configuration/secret/\",children:\"secrets\"}),\". You may guess that this is a good option when you need to pass some sensitive information like passwords or tokens. This way you don't specify the value of the environment variable directly in the deployment as we did before. Instead, you instruct Kubernetes to take the value of a specified secret object and use it as a value of an environment variable for your pod.\"]}),`\n`,(0,n.jsx)(e.p,{children:\"For example, if you have a Kubernetes secret like this:\"}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-yaml\",children:`apiVersion: v1\nkind: Secret\nmetadata:\n  \\xA0name: secret_data\ntype: Opaque\nstringData:\n  \\xA0 \\xA0username: \"example\"\n  \\xA0 \\xA0password: \"supersecretpassword\"\n`})}),`\n`,(0,n.jsx)(e.p,{children:\"and you want to pass the password as an environment variable to your pod, you can reference it in the deployment definition as follows:\"}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-yaml\",children:`---\napiVersion: apps/v1\nkind: Deployment\n(...)\n \\xA0 \\xA0 \\xA0containers:\n \\xA0 \\xA0 \\xA0 \\xA0- name: example-app-prod\n \\xA0 \\xA0 \\xA0 \\xA0 \\xA0image: [yourimage]\n \\xA0 \\xA0 \\xA0 \\xA0 \\xA0env:\n \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0# Inject variables from a Kuberentes secret\n \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0- name: secret_variables\n \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0valueFrom:\n \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0secretKeyRef:\n \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0name: secret_data\n \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0key: password\n\n`})}),`\n`,(0,n.jsxs)(e.p,{children:[\"In your pod, you will then be able to access the actual password (supersecretpassword) by accessing an environment variable called \",(0,n.jsx)(e.strong,{children:\"secret_variable\"}),\". For example, in Python you could do it like this:\"]}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-yaml\",children:`import osPASSWORD = os\n.environ.get['secret_variable']\n`})}),`\n`,(0,n.jsxs)(e.p,{children:[\"As you can see, in our example we have \",(0,n.jsx)(e.strong,{children:\"username\"}),\" and \",(0,n.jsx)(e.strong,{children:\"password\"}),\" defined in Kubernetes secret, but we are only passing the \",(0,n.jsx)(e.strong,{children:\"password\"}),\" value to the pod. If you want to pass all the secrets from a Kubernetes secret without specifying each key, you can use \",(0,n.jsx)(e.strong,{children:\"secretRef\"}),\" instead of \",(0,n.jsx)(e.strong,{children:\"secretKeyRef\"}),\". This way, you only need to specify the Kubernetes secret object name, and all the values from it will be automatically loaded as environment variables:\"]}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-yaml\",children:`---\napiVersion: apps/v1\nkind: Deployment\n(...)\n \\xA0 \\xA0 \\xA0containers:\n \\xA0 \\xA0 \\xA0 \\xA0- name: example-app-prod\n \\xA0 \\xA0 \\xA0 \\xA0 \\xA0image: [yourimage]\n \\xA0 \\xA0 \\xA0 \\xA0 \\xA0env:\n \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0# Inject variables from a Kuberentes secret\n \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0- name: secret_variables\n \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0valueFrom:\n \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0secretRef:\n \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0name: secret_data\n\n`})}),`\n`,(0,n.jsxs)(e.h4,{id:\"configmaps\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#configmaps\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"ConfigMaps\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"Another way of injecting environment variables into your pods is by using values from \",(0,n.jsx)(e.a,{href:\"https://kubernetes.io/docs/concepts/configuration/configmap/\",children:\"ConfigMaps\"}),\". For example, if you have ConfigMap like this:\"]}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-yaml\",children:`apiVersion: v1\nkind: ConfigMap\nmetadata:\n  \\xA0name: config-data\ndata:\n  \\xA0environment: \"dev\"\n  \\xA0timezone: \"UTC\"\n`})}),`\n`,(0,n.jsxs)(e.p,{children:[\"and you want to load both \",(0,n.jsx)(e.strong,{children:\"environment\"}),\" and \",(0,n.jsx)(e.strong,{children:\"timezone\"}),\" as environment variables into your pod, you can add the following \",(0,n.jsx)(e.strong,{children:\"valueFrom\"}),\" definition to your deployment:\"]}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-yaml\",children:`---\napiVersion: apps/v1\nkind: Deployment\n(...)\n \\xA0 \\xA0 \\xA0containers:\n \\xA0 \\xA0 \\xA0 \\xA0- name: example-app-prod\n \\xA0 \\xA0 \\xA0 \\xA0 \\xA0image: [yourimage]\n \\xA0 \\xA0 \\xA0 \\xA0 \\xA0env:\n \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0# Inject variables from a Kuberentes ConfigMap\n \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0- name: config_variables\n \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0valueFrom:\n \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0configMapRef:\n \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0name: config-data\n\n`})}),`\n`,(0,n.jsx)(e.p,{children:\"In your pod, you'll then be able to see both environment variables as defined in your ConfigMap:\"}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-yaml\",children:`# env\n\nHOSTNAME=5ad4e9e78e57\nenvironment=dev\ntimezone=UTC\n`})}),`\n`,(0,n.jsxs)(e.p,{children:[\"As with secrets, if you don't want to load all values from a ConfigMap, you can define specific keys instead by changing \",(0,n.jsx)(e.strong,{children:\"configMapRef\"}),\" to \",(0,n.jsx)(e.strong,{children:\"configMapKeyRef.\"})]}),`\n`,(0,n.jsx)(e.p,{children:\"The main difference between passing environment variables from ConfigMaps and specifying them directly as in the first example is the fact that here the environment variable lifecycle is separated from the pod lifecycle. This means you can update the value of your variable independently from the running pod. Or, to put it differently, you'll need to restart the pod yourself in order to load the new value of the environment variables into the pod. On the other hand, when you specify environment variables directly in the deployment, every change to the variables will automatically trigger pod restart.\"}),`\n`,(0,n.jsxs)(e.h3,{id:\"summary\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#summary\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Summary\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"Environment variables play an important role in Kubernetes. You can use them not only to provide basic information about the operating system to your application. You can also use them as the main configuration mechanism for your pods or for passing sensitive information. It's not uncommon in Kubernetes to extract as much configuration as possible info ConfigMaps and environment variables to keep your Docker images as generic as possible. As you can see, even something simple like environment variables have a few options in Kubernetes. If you want to learn more, \",(0,n.jsx)(e.a,{href:\"https://release.com/blog/why-kubernetes-is-so-hard\",children:\"Regis Wilson wrote about why Kubernetes is hard and what you can do about it.\"})]})]})}function k(a={}){let{wrapper:e}=a.components||{};return e?(0,n.jsx)(e,Object.assign({},a,{children:(0,n.jsx)(d,a)})):d(a)}var x=k;return b(N);})();\n;return Component;"
        },
        "_id": "blog/posts/kubernetes-environment-variables.mdx",
        "_raw": {
          "sourceFilePath": "blog/posts/kubernetes-environment-variables.mdx",
          "sourceFileName": "kubernetes-environment-variables.mdx",
          "sourceFileDir": "blog/posts",
          "contentType": "mdx",
          "flattenedPath": "blog/posts/kubernetes-environment-variables"
        },
        "type": "BlogPost",
        "computedSlug": "kubernetes-environment-variables"
      },
      "documentHash": "1739393595022",
      "hasWarnings": false,
      "documentTypeName": "BlogPost"
    },
    "blog/posts/kubernetes-health-checks-2-ways-to-improve-stability.mdx": {
      "document": {
        "title": "Kubernetes Health Checks - 2 Ways to Improve Stability in Your Production Applications",
        "summary": "These two methods for using Kubernetes health checks will improve your applications running in production.",
        "publishDate": "Wed Mar 24 2021 17:36:54 GMT+0000 (Coordinated Universal Time)",
        "author": "regis-wilson",
        "readingTime": 5,
        "categories": [
          "kubernetes",
          "platform-engineering"
        ],
        "mainImage": "/blog-images/42573700288680e0692c4a12793ae14b.jpg",
        "imageAlt": "A doctor checking the health of a patient representing stability of applications",
        "showCTA": true,
        "ctaCopy": "Improve application stability with Release's on-demand environments for accurate testing and streamlined deployment workflows.",
        "ctaLink": "https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=kubernetes-health-checks-2-ways-to-improve-stability",
        "relatedPosts": [
          ""
        ],
        "ogImage": "/blog-images/42573700288680e0692c4a12793ae14b.jpg",
        "excerpt": "These two methods for using Kubernetes health checks will improve your applications running in production.",
        "tags": [
          "kubernetes",
          "platform-engineering"
        ],
        "ctaButton": "Try Release for Free",
        "body": {
          "raw": "\n### Kubernetes Health Checks - 2 Ways to Improve Stability in Your Production Applications\n\nHealth checks are often a last-minute addition to your application stack, if they are even included at all. Advanced Site Reliability Engineering (SRE) practices try to push best practices (like health checks) forward so they are included early before applications are deployed. Many engineers know intuitively that health checks are important, but getting them implemented correctly—and keeping them up to date—is very hard. This article tries to document best practices for health checks, application development including SRE tenets, and how to improve the stability and even performance of your application when it runs in production.\n\nAt [Release](https://release.com), we have [previously written](https://release.com/blog/kubernetes-pods-advanced-concepts-explained) about how we monitor and configure applications so you can also read that blog post now or at a later time.\n\n### Costs of Downtime and Instability in Production\n\nDo you know how much it costs for your application to go offline? Don’t worry if you don’t—or can’t—know the exact figure: the important thing is to go through the mental process of estimating how much an outage or degradation to your application would “cost.” Costs are not only measured in currency, you need to also consider impacts to your brand, your Net Promoter Score (NPS), chatter online and on social media among customers and potential customers, and even negative reactions in the public media.\n\nI have worked in Site Reliability and DevOps my whole career and I have worked at many different companies whose responses for downtime ranged from the casual “our site will be back up eventually and we’ll be fine” to “we have lost $XXX per minute in revenue and we need to investigate methods for replacing that revenue”. No matter the response, I still did the best job my team and I could muster to keep the application and infrastructure services alive and well. There will always be bugs and issues with the code that is deployed and how it runs, however, if a problem occurs at a lower level in the application stack or in the infrastructure itself, then the application simply has no hope of servicing the needs of the consumers who visit your site.\n\nThe metaphor that I used often was one of cars driving on the motorway: if the roads are wet and slippery, then the cars will be unsafe and dangerous. When and if a crash occurs, then the roads will also be blocked and traffic will stop while the crash is cleaned up. It's true that the cars may run out of petrol, the drivers may get lost and go to the wrong destination, or the cars may not have good horsepower to drive quickly, but all of those factors are a higher order concern in the traffic stack. In this way, I saw my team’s and my job as keeping the roads as clear and safe and uncongested as possible so that the cars could operate at the best possible level.\n\n### The Symptoms Are The Disease\n\nVery early in the internet industry, the best practices for application stability were primitive and reactionary. Site reliability involved a manual post-mortem approach: finding out what happened and then applying monitoring and alerting on that behaviour to alert an operator that something was wrong. The best practices at the time involved a team of on-call engineers and operators who would literally watch an application 24 hours a day, 365 days a year (one extra day for leap years) and respond within a certain timeframe (usually less than fifteen minutes) to manually investigate and fix any issues that came up. In some cases the “team” was actually one poor person tasked with the impossible job of being on-call indefinitely.\n\nThere are several drawbacks to this approach, not the least of which is the human toll such manual response takes and the unsustainable pace. The cost of the team, the cost of staff turnover and training, the losses due to turnaround time and missed calls, and the impact to end users were all huge reasons to implement a better solution.\n\n### Health Checks to the Rescue\n\nOne key initiative that came about in the early aughts was the concept of a health check in the load balancer. I was part of a team that worked with several major load balancer manufacturers to implement a way to not only route traffic to services in our application, but to add monitors and tests (even then we called them “health checks”) to the endpoints which would allow us to add or remove services that were not responding or were unhealthy. The concept was that a web application would respond on a well-known port and respond with a well-known response that proved the application was ready to serve traffic.\n\nFor example, we might query the backend service at _http://192.168.0.10/health-check_ and we expected the service to respond with a string like _200 OK_. This trivial example doesn’t sound like much until you realised that our end-goal was to actually perform some internal checks in the application which would allow us to do more than respond with a static string. For example, the application might check that the database is responding to a sample query that a table exists, and then the application could check that the CPU was at some nominal value. Therefore, the health check could be expanded to something like:\n\n```yaml\nHTTP/1.0 200 OK\n\nChecking DB… ok\nChecking CPU… ok\nChecking User cache… ok\n```\n\nConversely, if something went wrong, the application could respond something like this:\n\n```yaml\nHTTP/1.0 500 CRITICAL\n\nChecking DB... ok\nChecking CPU… ok\nChecking User cache… CORRUPTED\n```\n\nUsing the response code of 200 and looking for the string “OK” (for example), the load balancer manufacturers were able to remove a service from the backend pool, allowing other servers to accept requests and avoid servers that would otherwise have an error. Also, we could set a timeout so that the load balancer would consider no response to be an error. In this way, we can remove traffic from servers that were not responsive. The beauty of the system we were designing was that we were going to be able to monitor errors proactively and directly at the origin. The servers would be removed before they became a problem.\n\nWe would also use the same health check in our monitoring and alerting systems that we had perfected over the previous decades by manually watching them and using them for diagnosis. The difference is that we had more information about what was going wrong, and simultaneously we had more time to respond and properly diagnose the problems without affecting customers at all. Imagine the relief at not having to respond to every alert at 2AM within 15 minutes, but being able to automatically open a ticket to have a technician during the graveyard shift respond within the hour and restart the server and add it back to the pool.\n\nEven better, we were able to convince the load balancer manufacturers to implement an inline-retry policy based on the same idea. For example, if a live service request to a backend server failed with a 500 error code, the load balancer could not only remove the server from the pool, but it could retry the request one more time on a healthy server. With this technology, the loadbalancer could try to resolve the situation before the customer even noticed anything was wrong, and no human could be quicker.\n\n### It’s Not Just Human Labour\n\nWe did better than save human labour and effort in monitoring the systems and responding to problems. After implementing health checks on the load balancers and inside the application, we were able to reduce errors and outages to the point where we actually raised our traffic levels by a double-digit percentage, and also increased actual revenue by a measurable amount. Users who might have encountered an error and navigated away after a Google search were staying around to browse and (more importantly) make purchases. By further tweaking the load balancing algorithms to favour healthier (or faster) servers, we further increased this beneficial business result even further. Steady growth over time occurred as well, because Google saw improved signals from users and fewer errors and therefore moved the site up in rankings. This was a stunning and unexpected outcome that was attributed to removing errors and downtime from our application running with this infrastructure and by utilising SRE practices (long before the term was coined).\n\n### The Modern Solutions\n\nWith the advent of Kubernetes, the lessons learnt the hard way over the past few decades have been carried forward in architecting a resilient and reliable design for complex service interactions. Kubernetes uses the concepts of a probe to test the application for liveness and readiness (there is a third probe that tests for startup delay, but we’re skipping that for the purposes of this article). With these two probes, we can implement a solution that makes applications far more stable and reduces downtime and manual intervention.\n\n### Liveness Probes\n\nThe first solution is the liveness probe which has the job of figuring out if a service is responding properly and within a certain time frame so that it can be considered running properly. If the probes fail, then the pod is considered “dead” and the pod will be terminated and restarted somewhere else in the cluster. For example, a web server may have a memory leak and stop responding after a certain amount of time or number of requests have occurred. Another example might be a database that fills up a temporary disk space area and is unable to process further transactions until the space is cleared out.\n\nYou may be saying to yourself that these seem like errors that should be corrected and dealt with properly rather than simply killing the pod and waiting for it to be rescheduled somewhere else. You would be absolutely correct, but let me counter with a rhetorical question asking, “Given this error condition, what do you want me to do at 2AM when no one is available?” The liveness probes can be excellent at monitoring non-responsive servers without state, but may not be so great at monitoring and restarting services with state, like the database example I gave above. So we recommend using the liveness probe only if you feel it would help more than it would hurt. We also spend extra care and effort to ensure that the liveness probes are very forgiving so they do not trigger on false-positive alarms.\n\nAnother counter-argument to the “fix it” stance has to do with direct or indirect engineering costs and interacting with third party or open-source code. Trying to allocate resources to fully diagnose an intermittent problem, much less attempt to fix the problem can be difficult. In the case of a third party software or an open-source project where getting upstream fixes submitted, prioritised, approved, tested, and pulled back downstream can be enormously expensive and time consuming. Sometimes the answer really is “just restart it”.\n\n### Readiness Probes\n\nThe second solution is a readiness probe which is much like the solution I described earlier in this article with the load balancers. Indeed, the readiness probe does exactly what I’ve described: Kubernetes will periodically run a command to test the service running inside the container to gauge proper and timely responses. The ingress (just a fancy name for the load balancer) will not send traffic to this pod unless and until the readiness probe states that the service is ready for correct operation.\n\nThis helps in some scenarios where a web server may hit a threshold in connections or traffic levels where it may slow down or stop responding to new requests. It may be the case that the application simply cannot handle more than a certain number of transactions and so Kubernetes can use this signal to route traffic to another pod that is less busy. If this slow down or refusal to respond can be correlated with other metrics (like traffic volume, CPU utilisation, etc.) then the [horizontal pod autoscaler](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/) could trigger more resources to be added to the service.\n\nIn fact, we believe that readiness probes are so important to correct functioning of applications that we strongly recommend _all services_ have a health check of some kind enabled and tested. We feel so strongly about this that we have considered making it a warning condition when no health check is configured on a running service in any of your environments at Release. Specifically, we could make readiness probes an opt out requirement rather than an opt in nicety.\n\n### Some examples\n\nHere are some actual examples of health checks that we have implemented for our customers. These examples are generic enough to be applied almost anywhere.\n\nIn this example we do a simple Nginx check on port 80 to ensure that the application is responding before we send traffic to the proxy.\n\n```yaml\n  readiness_probe:\n    exec:\n      command:\n      - curl\n      - \"-Lf\"\n      - http://localhost\n    failure_threshold: 5\n    period_seconds: 30\n    timeout_seconds: 3\n\n```\n\nIn this example we perform a health check against an Elastic search node to ensure that the cluster is healthy before accepting traffic (which presumably cannot be processed yet). This is a straight port from the Docker Compose examples in the open source repositories.\n\n```yaml\n- name: elasticsearch\n image: docker.elastic.co/elasticsearch/elasticsearch:7.9.2\n ports:\n - type: node_port\n   target_port: '9200'\n   port: '9200'\n readiness_probe:\n   exec:\n     command:\n     - curl\n     - \"--fail\"\n     - localhost:9200/_cluster/health\n   timeout_seconds: 2\n   failure_threshold: 3\n   period_seconds: 30\n```\n\nThis example is good to show how a non-HTTP check for a postgres database can be used to ensure the database is up and responding to requests. Note that if this database is not clustered, then application database requests can fail when the health check fails. Your application will need to respond accordingly (either fail in turn to cascade a failover at a higher level, or perform some sort of mitigation so that a graceful failure happens). Recall that if this were a liveness probe, the postgres container would be killed and restarted, which may not be what you want at all.\n\n```yaml\n readiness_probe:\n   exec:\n     command:\n     - psql\n     - \"-h\"\n     - localhost\n     - \"-c\"\n     - SELECT 1\n   period_seconds: 2\n   timeout_seconds: 2\n   failure_threshold: 30\n```\n\n### Step N, Profit\n\nBy implementing either (or both!) of these health checks, you can not only reduce the amount of time humans have to spend monitoring and interfering with applications, but you can even dramatically improve your traffic response levels, response times, and performance. In some cases, you might even be able to measure the impact to your customers’ NPS and/or your company’s top and bottom line.\n\nPhoto by [Hush Naidoo](https://unsplash.com/@hush52?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) on [Unsplash](https://unsplash.com/s/photos/health?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)\n",
          "code": "var Component=(()=>{var d=Object.create;var r=Object.defineProperty;var p=Object.getOwnPropertyDescriptor;var u=Object.getOwnPropertyNames;var m=Object.getPrototypeOf,f=Object.prototype.hasOwnProperty;var g=(a,e)=>()=>(e||a((e={exports:{}}).exports,e),e.exports),w=(a,e)=>{for(var n in e)r(a,n,{get:e[n],enumerable:!0})},s=(a,e,n,i)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let o of u(e))!f.call(a,o)&&o!==n&&r(a,o,{get:()=>e[o],enumerable:!(i=p(e,o))||i.enumerable});return a};var b=(a,e,n)=>(n=a!=null?d(m(a)):{},s(e||!a||!a.__esModule?r(n,\"default\",{value:a,enumerable:!0}):n,a)),y=a=>s(r({},\"__esModule\",{value:!0}),a);var l=g((I,h)=>{h.exports=_jsx_runtime});var T={};w(T,{default:()=>x,frontmatter:()=>v});var t=b(l()),v={title:\"Kubernetes Health Checks - 2 Ways to Improve Stability in Your Production Applications\",summary:\"These two methods for using Kubernetes health checks will improve your applications running in production.\",publishDate:\"Wed Mar 24 2021 17:36:54 GMT+0000 (Coordinated Universal Time)\",author:\"regis-wilson\",readingTime:5,categories:[\"kubernetes\",\"platform-engineering\"],mainImage:\"/blog-images/42573700288680e0692c4a12793ae14b.jpg\",imageAlt:\"A doctor checking the health of a patient representing stability of applications\",showCTA:!0,ctaCopy:\"Improve application stability with Release's on-demand environments for accurate testing and streamlined deployment workflows.\",ctaLink:\"https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=kubernetes-health-checks-2-ways-to-improve-stability\",relatedPosts:[\"\"],ogImage:\"/blog-images/42573700288680e0692c4a12793ae14b.jpg\",excerpt:\"These two methods for using Kubernetes health checks will improve your applications running in production.\",tags:[\"kubernetes\",\"platform-engineering\"],ctaButton:\"Try Release for Free\"};function c(a){let e=Object.assign({h3:\"h3\",a:\"a\",span:\"span\",p:\"p\",em:\"em\",pre:\"pre\",code:\"code\"},a.components);return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsxs)(e.h3,{id:\"kubernetes-health-checks---2-ways-to-improve-stability-in-your-production-applications\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#kubernetes-health-checks---2-ways-to-improve-stability-in-your-production-applications\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Kubernetes Health Checks - 2 Ways to Improve Stability in Your Production Applications\"]}),`\n`,(0,t.jsx)(e.p,{children:\"Health checks are often a last-minute addition to your application stack, if they are even included at all. Advanced Site Reliability Engineering (SRE) practices try to push best practices (like health checks) forward so they are included early before applications are deployed. Many engineers know intuitively that health checks are important, but getting them implemented correctly\\u2014and keeping them up to date\\u2014is very hard. This article tries to document best practices for health checks, application development including SRE tenets, and how to improve the stability and even performance of your application when it runs in production.\"}),`\n`,(0,t.jsxs)(e.p,{children:[\"At \",(0,t.jsx)(e.a,{href:\"https://release.com\",children:\"Release\"}),\", we have \",(0,t.jsx)(e.a,{href:\"https://release.com/blog/kubernetes-pods-advanced-concepts-explained\",children:\"previously written\"}),\" about how we monitor and configure applications so you can also read that blog post now or at a later time.\"]}),`\n`,(0,t.jsxs)(e.h3,{id:\"costs-of-downtime-and-instability-in-production\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#costs-of-downtime-and-instability-in-production\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Costs of Downtime and Instability in Production\"]}),`\n`,(0,t.jsx)(e.p,{children:\"Do you know how much it costs for your application to go offline? Don\\u2019t worry if you don\\u2019t\\u2014or can\\u2019t\\u2014know the exact figure: the important thing is to go through the mental process of estimating how much an outage or degradation to your application would \\u201Ccost.\\u201D Costs are not only measured in currency, you need to also consider impacts to your brand, your Net Promoter Score (NPS), chatter online and on social media among customers and potential customers, and even negative reactions in the public media.\"}),`\n`,(0,t.jsx)(e.p,{children:\"I have worked in Site Reliability and DevOps my whole career and I have worked at many different companies whose responses for downtime ranged from the casual \\u201Cour site will be back up eventually and we\\u2019ll be fine\\u201D to \\u201Cwe have lost $XXX per minute in revenue and we need to investigate methods for replacing that revenue\\u201D. No matter the response, I still did the best job my team and I could muster to keep the application and infrastructure services alive and well. There will always be bugs and issues with the code that is deployed and how it runs, however, if a problem occurs at a lower level in the application stack or in the infrastructure itself, then the application simply has no hope of servicing the needs of the consumers who visit your site.\"}),`\n`,(0,t.jsx)(e.p,{children:\"The metaphor that I used often was one of cars driving on the motorway: if the roads are wet and slippery, then the cars will be unsafe and dangerous. When and if a crash occurs, then the roads will also be blocked and traffic will stop while the crash is cleaned up. It's true that the cars may run out of petrol, the drivers may get lost and go to the wrong destination, or the cars may not have good horsepower to drive quickly, but all of those factors are a higher order concern in the traffic stack. In this way, I saw my team\\u2019s and my job as keeping the roads as clear and safe and uncongested as possible so that the cars could operate at the best possible level.\"}),`\n`,(0,t.jsxs)(e.h3,{id:\"the-symptoms-are-the-disease\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#the-symptoms-are-the-disease\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"The Symptoms Are The Disease\"]}),`\n`,(0,t.jsx)(e.p,{children:\"Very early in the internet industry, the best practices for application stability were primitive and reactionary. Site reliability involved a manual post-mortem approach: finding out what happened and then applying monitoring and alerting on that behaviour to alert an operator that something was wrong. The best practices at the time involved a team of on-call engineers and operators who would literally watch an application 24 hours a day, 365 days a year (one extra day for leap years) and respond within a certain timeframe (usually less than fifteen minutes) to manually investigate and fix any issues that came up. In some cases the \\u201Cteam\\u201D was actually one poor person tasked with the impossible job of being on-call indefinitely.\"}),`\n`,(0,t.jsx)(e.p,{children:\"There are several drawbacks to this approach, not the least of which is the human toll such manual response takes and the unsustainable pace. The cost of the team, the cost of staff turnover and training, the losses due to turnaround time and missed calls, and the impact to end users were all huge reasons to implement a better solution.\"}),`\n`,(0,t.jsxs)(e.h3,{id:\"health-checks-to-the-rescue\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#health-checks-to-the-rescue\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Health Checks to the Rescue\"]}),`\n`,(0,t.jsx)(e.p,{children:\"One key initiative that came about in the early aughts was the concept of a health check in the load balancer. I was part of a team that worked with several major load balancer manufacturers to implement a way to not only route traffic to services in our application, but to add monitors and tests (even then we called them \\u201Chealth checks\\u201D) to the endpoints which would allow us to add or remove services that were not responding or were unhealthy. The concept was that a web application would respond on a well-known port and respond with a well-known response that proved the application was ready to serve traffic.\"}),`\n`,(0,t.jsxs)(e.p,{children:[\"For example, we might query the backend service at \",(0,t.jsx)(e.em,{children:(0,t.jsx)(e.a,{href:\"http://192.168.0.10/health-check\",children:\"http://192.168.0.10/health-check\"})}),\" and we expected the service to respond with a string like \",(0,t.jsx)(e.em,{children:\"200 OK\"}),\". This trivial example doesn\\u2019t sound like much until you realised that our end-goal was to actually perform some internal checks in the application which would allow us to do more than respond with a static string. For example, the application might check that the database is responding to a sample query that a table exists, and then the application could check that the CPU was at some nominal value. Therefore, the health check could be expanded to something like:\"]}),`\n`,(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:\"language-yaml\",children:`HTTP/1.0 200 OK\n\nChecking DB\\u2026 ok\nChecking CPU\\u2026 ok\nChecking User cache\\u2026 ok\n`})}),`\n`,(0,t.jsx)(e.p,{children:\"Conversely, if something went wrong, the application could respond something like this:\"}),`\n`,(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:\"language-yaml\",children:`HTTP/1.0 500 CRITICAL\n\nChecking DB... ok\nChecking CPU\\u2026 ok\nChecking User cache\\u2026 CORRUPTED\n`})}),`\n`,(0,t.jsx)(e.p,{children:\"Using the response code of 200 and looking for the string \\u201COK\\u201D (for example), the load balancer manufacturers were able to remove a service from the backend pool, allowing other servers to accept requests and avoid servers that would otherwise have an error. Also, we could set a timeout so that the load balancer would consider no response to be an error. In this way, we can remove traffic from servers that were not responsive. The beauty of the system we were designing was that we were going to be able to monitor errors proactively and directly at the origin. The servers would be removed before they became a problem.\"}),`\n`,(0,t.jsx)(e.p,{children:\"We would also use the same health check in our monitoring and alerting systems that we had perfected over the previous decades by manually watching them and using them for diagnosis. The difference is that we had more information about what was going wrong, and simultaneously we had more time to respond and properly diagnose the problems without affecting customers at all. Imagine the relief at not having to respond to every alert at 2AM within 15 minutes, but being able to automatically open a ticket to have a technician during the graveyard shift respond within the hour and restart the server and add it back to the pool.\"}),`\n`,(0,t.jsx)(e.p,{children:\"Even better, we were able to convince the load balancer manufacturers to implement an inline-retry policy based on the same idea. For example, if a live service request to a backend server failed with a 500 error code, the load balancer could not only remove the server from the pool, but it could retry the request one more time on a healthy server. With this technology, the loadbalancer could try to resolve the situation before the customer even noticed anything was wrong, and no human could be quicker.\"}),`\n`,(0,t.jsxs)(e.h3,{id:\"its-not-just-human-labour\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#its-not-just-human-labour\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"It\\u2019s Not Just Human Labour\"]}),`\n`,(0,t.jsx)(e.p,{children:\"We did better than save human labour and effort in monitoring the systems and responding to problems. After implementing health checks on the load balancers and inside the application, we were able to reduce errors and outages to the point where we actually raised our traffic levels by a double-digit percentage, and also increased actual revenue by a measurable amount. Users who might have encountered an error and navigated away after a Google search were staying around to browse and (more importantly) make purchases. By further tweaking the load balancing algorithms to favour healthier (or faster) servers, we further increased this beneficial business result even further. Steady growth over time occurred as well, because Google saw improved signals from users and fewer errors and therefore moved the site up in rankings. This was a stunning and unexpected outcome that was attributed to removing errors and downtime from our application running with this infrastructure and by utilising SRE practices (long before the term was coined).\"}),`\n`,(0,t.jsxs)(e.h3,{id:\"the-modern-solutions\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#the-modern-solutions\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"The Modern Solutions\"]}),`\n`,(0,t.jsx)(e.p,{children:\"With the advent of Kubernetes, the lessons learnt the hard way over the past few decades have been carried forward in architecting a resilient and reliable design for complex service interactions. Kubernetes uses the concepts of a probe to test the application for liveness and readiness (there is a third probe that tests for startup delay, but we\\u2019re skipping that for the purposes of this article). With these two probes, we can implement a solution that makes applications far more stable and reduces downtime and manual intervention.\"}),`\n`,(0,t.jsxs)(e.h3,{id:\"liveness-probes\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#liveness-probes\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Liveness Probes\"]}),`\n`,(0,t.jsx)(e.p,{children:\"The first solution is the liveness probe which has the job of figuring out if a service is responding properly and within a certain time frame so that it can be considered running properly. If the probes fail, then the pod is considered \\u201Cdead\\u201D and the pod will be terminated and restarted somewhere else in the cluster. For example, a web server may have a memory leak and stop responding after a certain amount of time or number of requests have occurred. Another example might be a database that fills up a temporary disk space area and is unable to process further transactions until the space is cleared out.\"}),`\n`,(0,t.jsx)(e.p,{children:\"You may be saying to yourself that these seem like errors that should be corrected and dealt with properly rather than simply killing the pod and waiting for it to be rescheduled somewhere else. You would be absolutely correct, but let me counter with a rhetorical question asking, \\u201CGiven this error condition, what do you want me to do at 2AM when no one is available?\\u201D The liveness probes can be excellent at monitoring non-responsive servers without state, but may not be so great at monitoring and restarting services with state, like the database example I gave above. So we recommend using the liveness probe only if you feel it would help more than it would hurt. We also spend extra care and effort to ensure that the liveness probes are very forgiving so they do not trigger on false-positive alarms.\"}),`\n`,(0,t.jsx)(e.p,{children:\"Another counter-argument to the \\u201Cfix it\\u201D stance has to do with direct or indirect engineering costs and interacting with third party or open-source code. Trying to allocate resources to fully diagnose an intermittent problem, much less attempt to fix the problem can be difficult. In the case of a third party software or an open-source project where getting upstream fixes submitted, prioritised, approved, tested, and pulled back downstream can be enormously expensive and time consuming. Sometimes the answer really is \\u201Cjust restart it\\u201D.\"}),`\n`,(0,t.jsxs)(e.h3,{id:\"readiness-probes\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#readiness-probes\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Readiness Probes\"]}),`\n`,(0,t.jsx)(e.p,{children:\"The second solution is a readiness probe which is much like the solution I described earlier in this article with the load balancers. Indeed, the readiness probe does exactly what I\\u2019ve described: Kubernetes will periodically run a command to test the service running inside the container to gauge proper and timely responses. The ingress (just a fancy name for the load balancer) will not send traffic to this pod unless and until the readiness probe states that the service is ready for correct operation.\"}),`\n`,(0,t.jsxs)(e.p,{children:[\"This helps in some scenarios where a web server may hit a threshold in connections or traffic levels where it may slow down or stop responding to new requests. It may be the case that the application simply cannot handle more than a certain number of transactions and so Kubernetes can use this signal to route traffic to another pod that is less busy. If this slow down or refusal to respond can be correlated with other metrics (like traffic volume, CPU utilisation, etc.) then the \",(0,t.jsx)(e.a,{href:\"https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/\",children:\"horizontal pod autoscaler\"}),\" could trigger more resources to be added to the service.\"]}),`\n`,(0,t.jsxs)(e.p,{children:[\"In fact, we believe that readiness probes are so important to correct functioning of applications that we strongly recommend \",(0,t.jsx)(e.em,{children:\"all services\"}),\" have a health check of some kind enabled and tested. We feel so strongly about this that we have considered making it a warning condition when no health check is configured on a running service in any of your environments at Release. Specifically, we could make readiness probes an opt out requirement rather than an opt in nicety.\"]}),`\n`,(0,t.jsxs)(e.h3,{id:\"some-examples\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#some-examples\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Some examples\"]}),`\n`,(0,t.jsx)(e.p,{children:\"Here are some actual examples of health checks that we have implemented for our customers. These examples are generic enough to be applied almost anywhere.\"}),`\n`,(0,t.jsx)(e.p,{children:\"In this example we do a simple Nginx check on port 80 to ensure that the application is responding before we send traffic to the proxy.\"}),`\n`,(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:\"language-yaml\",children:` \\xA0readiness_probe:\n \\xA0 \\xA0exec:\n \\xA0 \\xA0 \\xA0command:\n \\xA0 \\xA0 \\xA0- curl\n \\xA0 \\xA0 \\xA0- \"-Lf\"\n \\xA0 \\xA0 \\xA0- http://localhost\n \\xA0 \\xA0failure_threshold: 5\n \\xA0 \\xA0period_seconds: 30\n \\xA0 \\xA0timeout_seconds: 3\n\n`})}),`\n`,(0,t.jsx)(e.p,{children:\"In this example we perform a health check against an Elastic search node to ensure that the cluster is healthy before accepting traffic (which presumably cannot be processed yet). This is a straight port from the Docker Compose examples in the open source repositories.\"}),`\n`,(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:\"language-yaml\",children:`- name: elasticsearch\n image: docker.elastic.co/elasticsearch/elasticsearch:7.9.2\n ports:\n - type: node_port\n \\xA0 target_port: '9200'\n \\xA0 port: '9200'\n readiness_probe:\n \\xA0 exec:\n \\xA0 \\xA0 command:\n \\xA0 \\xA0 - curl\n \\xA0 \\xA0 - \"--fail\"\n \\xA0 \\xA0 - localhost:9200/_cluster/health\n \\xA0 timeout_seconds: 2\n \\xA0 failure_threshold: 3\n \\xA0 period_seconds: 30\n`})}),`\n`,(0,t.jsx)(e.p,{children:\"This example is good to show how a non-HTTP check for a postgres database can be used to ensure the database is up and responding to requests. Note that if this database is not clustered, then application database requests can fail when the health check fails. Your application will need to respond accordingly (either fail in turn to cascade a failover at a higher level, or perform some sort of mitigation so that a graceful failure happens). Recall that if this were a liveness probe, the postgres container would be killed and restarted, which may not be what you want at all.\"}),`\n`,(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:\"language-yaml\",children:` readiness_probe:\n \\xA0 exec:\n \\xA0 \\xA0 command:\n \\xA0 \\xA0 - psql\n \\xA0 \\xA0 - \"-h\"\n \\xA0 \\xA0 - localhost\n \\xA0 \\xA0 - \"-c\"\n \\xA0 \\xA0 - SELECT 1\n \\xA0 period_seconds: 2\n \\xA0 timeout_seconds: 2\n \\xA0 failure_threshold: 30\n`})}),`\n`,(0,t.jsxs)(e.h3,{id:\"step-n-profit\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#step-n-profit\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Step N, Profit\"]}),`\n`,(0,t.jsx)(e.p,{children:\"By implementing either (or both!) of these health checks, you can not only reduce the amount of time humans have to spend monitoring and interfering with applications, but you can even dramatically improve your traffic response levels, response times, and performance. In some cases, you might even be able to measure the impact to your customers\\u2019 NPS and/or your company\\u2019s top and bottom line.\"}),`\n`,(0,t.jsxs)(e.p,{children:[\"Photo by \",(0,t.jsx)(e.a,{href:\"https://unsplash.com/@hush52?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText\",children:\"Hush Naidoo\"}),\" on \",(0,t.jsx)(e.a,{href:\"https://unsplash.com/s/photos/health?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText\",children:\"Unsplash\"})]})]})}function k(a={}){let{wrapper:e}=a.components||{};return e?(0,t.jsx)(e,Object.assign({},a,{children:(0,t.jsx)(c,a)})):c(a)}var x=k;return y(T);})();\n;return Component;"
        },
        "_id": "blog/posts/kubernetes-health-checks-2-ways-to-improve-stability.mdx",
        "_raw": {
          "sourceFilePath": "blog/posts/kubernetes-health-checks-2-ways-to-improve-stability.mdx",
          "sourceFileName": "kubernetes-health-checks-2-ways-to-improve-stability.mdx",
          "sourceFileDir": "blog/posts",
          "contentType": "mdx",
          "flattenedPath": "blog/posts/kubernetes-health-checks-2-ways-to-improve-stability"
        },
        "type": "BlogPost",
        "computedSlug": "kubernetes-health-checks-2-ways-to-improve-stability"
      },
      "documentHash": "1739393595022",
      "hasWarnings": false,
      "documentTypeName": "BlogPost"
    },
    "blog/posts/kubernetes-how-to-debug-crashloopbackoff-in-a-container.mdx": {
      "document": {
        "title": "Kubernetes - How to Debug CrashLoopBackOff in a Container",
        "summary": "If you've used Kubernetes (k8s), you've probably bumped into the dreaded CrashLoopBackOff. A CrashLoopBackOff is possibl",
        "publishDate": "Thu Feb 04 2021 19:23:56 GMT+0000 (Coordinated Universal Time)",
        "author": "david-giffin",
        "readingTime": 3,
        "categories": [
          "kubernetes",
          "platform-engineering"
        ],
        "mainImage": "/blog-images/4e91e41da26a361f3123c6c7a68815ee.jpg",
        "imageAlt": "Storage containers stacked representing a CrashLoopBackOff in a container",
        "showCTA": true,
        "ctaCopy": "Struggling with CrashLoopBackOff in Kubernetes? Use Release for streamlined debugging environments and faster issue resolution.",
        "ctaLink": "https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=kubernetes-how-to-debug-crashloopbackoff-in-a-container",
        "relatedPosts": [
          ""
        ],
        "ogImage": "/blog-images/4e91e41da26a361f3123c6c7a68815ee.jpg",
        "excerpt": "If you've used Kubernetes (k8s), you've probably bumped into the dreaded CrashLoopBackOff. A CrashLoopBackOff is possibl",
        "tags": [
          "kubernetes",
          "platform-engineering"
        ],
        "ctaButton": "Try Release for Free",
        "body": {
          "raw": "\nIf you've used Kubernetes (k8s), you've probably bumped into the dreaded CrashLoopBackOff. A CrashLoopBackOff is possible for several types of k8s misconfigurations (not able to connect to persistent volumes, init-container misconfiguration, etc). We aren't going to cover how to configure k8s properly in this article, but instead will focus on the harder problem of debugging your code or, even worse, someone else's code 😱\n\nHere is the output from kubectl describe pod for a CrashLoopBackOff:\n\n```ruby\n\nName:             frontend-5c49b595fc-sjzkg\nNamespace:        tedbf02-ac-david-nginx-golang-tmcclung-nginx-golang\nPriority:         0\nStart Time:       Wed, 23 Dec 2020 14:55:49 -0500\nLabels:           app=frontend\n                  pod-template-hash=5c49b595fc\n                  tier=frontend\nStatus:           Running\nIP:              10.1.31.0\nIPs:\nControlled By:   ReplicaSet/frontend-5c49b595fc\nContainers:\n  frontend:\n    Container ID:   docker://a4ed7efcaaa87fe36342cf7532ff1de5cd51b62d3d681dfb9857999300f6c587\n    Image:          .amazonaws.com/tommyrelease/awesome-compose/frontend@sha256:dfd762c\n    Image ID:       docker-pullable://.amazonaws.com/tommyrelease/awesome-compose/frontend@sha256:dfd762c\n    Port:           80/TCP\n    Host Port:      0/TCP\n    State:          Waiting\n      Reason:       CrashLoopBackOff\n    Last State:     Terminated\n      Reason:       Error\n      Exit Code:    1\n      Started:      Sun, 24 Jan 2021 20:25:26 -0500\n      Finished:     Sun, 24 Jan 2021 20:25:26 -0500\n    Ready:          False\n    Restart Count:  9043\n\n```\n\nTwo common problems when starting a container are OCI runtime create failed (which means you are referencing a binary or script that doesn't exist on the container) and container \"Completed\" or \"Error\" which both mean that the code executing on the container failed to run a service and stay running.\n\nHere's an example of an OCI runtime error, trying to execute: \"hello crashloop\":\n\n```ruby\n\nPort:           80/TCP\n    Host Port:    0/TCP\n    Command:\n      hello\n      crashloop\n    State:          Waiting\n      Reason:       CrashLoopBackOff\n    Last State:     Terminated\n      Reason:       ContainerCannotRun\n      Message:      OCI runtime create failed: container_linux.go:370: starting container process caused: exec: \"hello\": executable file not found in $PATH: unknown\n      Exit Code:    127\n      Started:      Mon, 25 Jan 2021 22:20:04 -0500\n      Finished:     Mon, 25 Jan 2021 22:20:04 -0500\n\n```\n\nK8s gives you the exit status of the process in the container when you look at a pod using kubectl or [k9s](https://github.com/derailed/k9s). Common exit statuses from unix processes include 1-125. Each unix command usually has a man page, which provides more details around the various exit codes. Exit code (128 + SIGKILL 9) 137 means that k8s hit the memory limit for your pod and killed your container for you.\n\nHere is the output from kubectl describe pod, showing the container exit code:\n\n```ruby\n\nLast State:     Terminated\n      Reason:       Error\n      Exit Code:    1\n      Started:      Sun, 24 Jan 2021 20:25:26 -0500\n      Finished:     Sun, 24 Jan 2021 20:25:26 -0500\n    Ready:          False\n    Restart Count:  9043\n\n```\n\n### All containers are not created equally.\n\nDocker allows you to define an Entrypoint and Cmd which you can mix and match in a Dockerfile. Entrypoint is the executable, and Cmd are the arguments passed to the Entrypoint. The Dockerfile schema is quite lenient and allows users to set Cmd without Entrypoint, which means that the first argument in Cmd will be the executable to run.\n\nNote: k8s uses a different naming convention for Docker Entrypoint and Cmd. In Kubernetes command is Docker Entrypoint and Kubernetes args is Docker Cmd.\n\n| Description           | The command run by the container | Arguments passed to the command |\n| --------------------- | -------------------------------- | ------------------------------- |\n| Docker field name     | Entrypoint                       | Cmd                             |\n| Kubernetes field name | Command                          | args                            |\n\nThere are a few tricks to understanding how the container you're working with starts up. In order to get the startup command when you're dealing with someone else's container, we need to know the intended Docker Entrypoint and Cmd of the Docker image. If you have the Dockerfile that created the Docker image, then you likely already know the Entrypoint and Cmd, unless you aren't defining them and inheriting from a base image that has them set.\n\nWhen dealing with either off the shelf containers, using someone else's container and you don't have the Dockerfile, or you're inheriting from a base image that you don't have the Dockerfile for, you can use the following steps to get the values you need. First, we pull the container locally using docker pull, then we inspect the container image to get the Entrypoint and Cmd:\n\n- docker pull <image id=\"\"></image>\n- docker inspect <image id=\"\"></image>\n\nHere we use jq to filter the JSON response from docker inspect:\n\n```ruby\n\ndavid@sega:~: docker pull docker.elastic.co/elasticsearch/elasticsearch:7.10.2\n7.10.2: Pulling from elasticsearch/elasticsearch\nddf49b9115d7: Pull complete\ne736878e27ad: Pull complete\n7487c9dcefbe: Pull complete\n9ccb7e6e1f0c: Pull complete\ndcec6dec98db: Pull complete\n8a10b4854661: Pull complete\n1e595aee1b7d: Pull complete\n06cc198dbf22: Pull complete\n55b9b1b50ed8: Pull complete\nDigest: sha256:d528cec81720266974fdfe7a0f12fee928dc02e5a2c754b45b9a84c84695bfd9\nStatus: Downloaded newer image for docker.elastic.co/elasticsearch/elasticsearch:7.10.2\ndocker.elastic.co/elasticsearch/elasticsearch:7.10.2\ndavid@sega:~: docker inspect docker.elastic.co/elasticsearch/elasticsearch:7.10.2 | jq '.[0] .ContainerConfig .Entrypoint'\n[\n  \"/tini\",\n  \"--\",\n  \"/usr/local/bin/docker-entrypoint.sh\"\n]\ndavid@sega:~: docker inspect docker.elastic.co/elasticsearch/elasticsearch:7.10.2 | jq '.[0] .ContainerConfig .Cmd'\n[\n  \"/bin/sh\",\n  \"-c\",\n  \"#(nop) \",\n  \"CMD [\\\"eswrapper\\\"]\"\n]\n\n```\n\n### The Dreaded CrashLoopBackOff\n\nNow that you have all that background, let's get to debugging the CrashLoopBackOff.\n\nIn order to understand what's happening, it's important to be able to inspect the container inside of k8s so the application has all the environment variables and dependent services. Updating the deployment and setting the container Entrypoint or k8s command temporarily to tail -f /dev/null or sleep infinity will give you an opportunity to debug why the service doesn't stay running.\n\nHere's how to configure k8s to override the container Entrypoint:\n\n```ruby\n\napiVersion: extensions/v1beta1\nkind: Deployment\nmetadata:\n  name: elasticsearch\n  namespace: elasticsearch\nspec:\n  progressDeadlineSeconds: 600\n  replicas: 1\n  revisionHistoryLimit: 3\n  selector:\n    matchLabels:\n      app: backend\n      tier: backend\n  strategy:\n    rollingUpdate:\n      maxSurge: 25%\n      maxUnavailable: 25%\n    type: RollingUpdate\n  template:\n    metadata:\n      creationTimestamp: null\n      labels:\n        app: backend\n        tier: backend\n    spec:\n      containers:\n      - command:\n        - tail\n        - \"-f\"\n        - /dev/null\n\n```\n\nHere's the configuration in Release:\n\n```ruby\n\nservices:\n- name: elasticsearch\n  image: docker.elastic.co/elasticsearch/elasticsearch:7.10.2\n  command:\n  - tail\n  - \"-f\"\n  - /dev/null\n\n```\n\nYou can now use kubectl or k9s to exec into the container and take a look around. Using the Entrypoint and Cmd you discovered earlier, you can execute the intended startup command and see how the application is failing.\n\nDepending on the container you're running, it may be missing many of the tools necessary to debug your problem like: curl, lsof, vim; and if it's someone else's code, you probably don't know which version of linux was used to create the image. We typically try all of the common package managers until we find the right one. Most containers these days use Alpine Linux (apk package manager) or a Debian, Ubuntu (apt-get package manager) based image. In some cases we've seen Centos and Fedora, which both use the yum package manager.\n\nOne of the following commands should work depending on the operating system:\n\n- apk\n- apt-get\n- yum\n\nDockerfile maintainers often remove the cache from the package manager to shrink the size of the image, so you may also need to run one of the following:\n\n- apk update\n- apt-get update\n- yum makecache\n\nNow you need to add the necessary tools to help with debugging. Depending on the package manager you found, use one of the following commands to add useful debugging tools:\n\n- apt-get install -y curl vim procps inetutils-tools net-tools lsof\n- apk add curl vim procps net-tools lsof\n- yum install curl vim procps lsof\n\nAt this point, it's up to you to figure out the problem. You can edit files using vim to tweak the container until you understand what's going on. If you forget all of the files you've touched on the container, you can alway kill the pod and the container will restart without your changes. Always remember to write down the steps taken to get the container working. You'll want to use your notes to alter the Dockerfile or add commands to the container startup scripts.\n\n### Debugging Your Containers\n\nWe have created a simple script to get all of the debuging tools, as long as you are working with a container that has curl pre-installed:\n\n```ruby\n\n# install debugging tools on a container with curl pre-installed\n\n/bin/sh -c \"$(curl -fsSL https://raw.githubusercontent.com/releaseapp-io/container-debug/main/install.sh)\"\n\n```\n\n### Conclusion\n\nIn this article, we've learnt how to spot and investigate the CrashLoopBackOff errors in containers. We walked you through how to inspect and investigate the container image itself. We've listed and shown some tools that we use to spot problems and investigate issues. We got several useful and basic tools installed on the image, hopefully regardless of base image. With these steps in mind and all the tools ready at your disposal, go forth and fix all the things!\n",
          "code": "var Component=(()=>{var h=Object.create;var i=Object.defineProperty;var u=Object.getOwnPropertyDescriptor;var p=Object.getOwnPropertyNames;var m=Object.getPrototypeOf,g=Object.prototype.hasOwnProperty;var f=(t,e)=>()=>(e||t((e={exports:{}}).exports,e),e.exports),b=(t,e)=>{for(var a in e)i(t,a,{get:e[a],enumerable:!0})},s=(t,e,a,r)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let o of p(e))!g.call(t,o)&&o!==a&&i(t,o,{get:()=>e[o],enumerable:!(r=u(e,o))||r.enumerable});return t};var y=(t,e,a)=>(a=t!=null?h(m(t)):{},s(e||!t||!t.__esModule?i(a,\"default\",{value:t,enumerable:!0}):a,t)),k=t=>s(i({},\"__esModule\",{value:!0}),t);var l=f((L,c)=>{c.exports=_jsx_runtime});var x={};b(x,{default:()=>v,frontmatter:()=>w});var n=y(l()),w={title:\"Kubernetes - How to Debug CrashLoopBackOff in a Container\",summary:\"If you've used Kubernetes (k8s), you've probably bumped into the dreaded CrashLoopBackOff. A CrashLoopBackOff is possibl\",publishDate:\"Thu Feb 04 2021 19:23:56 GMT+0000 (Coordinated Universal Time)\",author:\"david-giffin\",readingTime:3,categories:[\"kubernetes\",\"platform-engineering\"],mainImage:\"/blog-images/4e91e41da26a361f3123c6c7a68815ee.jpg\",imageAlt:\"Storage containers stacked representing a CrashLoopBackOff in a container\",showCTA:!0,ctaCopy:\"Struggling with CrashLoopBackOff in Kubernetes? Use Release for streamlined debugging environments and faster issue resolution.\",ctaLink:\"https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=kubernetes-how-to-debug-crashloopbackoff-in-a-container\",relatedPosts:[\"\"],ogImage:\"/blog-images/4e91e41da26a361f3123c6c7a68815ee.jpg\",excerpt:\"If you've used Kubernetes (k8s), you've probably bumped into the dreaded CrashLoopBackOff. A CrashLoopBackOff is possibl\",tags:[\"kubernetes\",\"platform-engineering\"],ctaButton:\"Try Release for Free\"};function d(t){let e=Object.assign({p:\"p\",pre:\"pre\",code:\"code\",a:\"a\",h3:\"h3\",span:\"span\",table:\"table\",thead:\"thead\",tr:\"tr\",th:\"th\",tbody:\"tbody\",td:\"td\",ul:\"ul\",li:\"li\"},t.components);return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(e.p,{children:\"If you've used Kubernetes (k8s), you've probably bumped into the dreaded CrashLoopBackOff. A CrashLoopBackOff is possible for several types of k8s misconfigurations (not able to connect to persistent volumes, init-container misconfiguration, etc). We aren't going to cover how to configure k8s properly in this article, but instead will focus on the harder problem of debugging your code or, even worse, someone else's code \\u{1F631}\"}),`\n`,(0,n.jsx)(e.p,{children:\"Here is the output from kubectl describe pod for a CrashLoopBackOff:\"}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-ruby\",children:`\nName:             frontend-5c49b595fc-sjzkg\nNamespace:        tedbf02-ac-david-nginx-golang-tmcclung-nginx-golang\nPriority:         0\nStart Time:       Wed, 23 Dec 2020 14:55:49 -0500\nLabels:           app=frontend\n                  pod-template-hash=5c49b595fc\n                  tier=frontend\nStatus:           Running\nIP:              10.1.31.0\nIPs:\nControlled By:   ReplicaSet/frontend-5c49b595fc\nContainers:\n  frontend:\n    Container ID:   docker://a4ed7efcaaa87fe36342cf7532ff1de5cd51b62d3d681dfb9857999300f6c587\n    Image:          .amazonaws.com/tommyrelease/awesome-compose/frontend@sha256:dfd762c\n    Image ID:       docker-pullable://.amazonaws.com/tommyrelease/awesome-compose/frontend@sha256:dfd762c\n    Port:           80/TCP\n    Host Port:      0/TCP\n    State:          Waiting\n      Reason:       CrashLoopBackOff\n    Last State:     Terminated\n      Reason:       Error\n      Exit Code:    1\n      Started:      Sun, 24 Jan 2021 20:25:26 -0500\n      Finished:     Sun, 24 Jan 2021 20:25:26 -0500\n    Ready:          False\n    Restart Count:  9043\n\n`})}),`\n`,(0,n.jsx)(e.p,{children:`Two common problems when starting a container are OCI runtime create failed (which means you are referencing a binary or script that doesn't exist on the container) and container \"Completed\" or \"Error\" which both mean that the code executing on the container failed to run a service and stay running.`}),`\n`,(0,n.jsx)(e.p,{children:`Here's an example of an OCI runtime error, trying to execute: \"hello crashloop\":`}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-ruby\",children:`\nPort:           80/TCP\n    Host Port:    0/TCP\n    Command:\n      hello\n      crashloop\n    State:          Waiting\n      Reason:       CrashLoopBackOff\n    Last State:     Terminated\n      Reason:       ContainerCannotRun\n      Message:      OCI runtime create failed: container_linux.go:370: starting container process caused: exec: \"hello\": executable file not found in $PATH: unknown\n      Exit Code:    127\n      Started:      Mon, 25 Jan 2021 22:20:04 -0500\n      Finished:     Mon, 25 Jan 2021 22:20:04 -0500\n\n`})}),`\n`,(0,n.jsxs)(e.p,{children:[\"K8s gives you the exit status of the process in the container when you look at a pod using kubectl or \",(0,n.jsx)(e.a,{href:\"https://github.com/derailed/k9s\",children:\"k9s\"}),\". Common exit statuses from unix processes include 1-125. Each unix command usually has a man page, which provides more details around the various exit codes. Exit code (128 + SIGKILL 9) 137 means that k8s hit the memory limit for your pod and killed your container for you.\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Here is the output from kubectl describe pod, showing the container exit code:\"}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-ruby\",children:`\nLast State:     Terminated\n      Reason:       Error\n      Exit Code:    1\n      Started:      Sun, 24 Jan 2021 20:25:26 -0500\n      Finished:     Sun, 24 Jan 2021 20:25:26 -0500\n    Ready:          False\n    Restart Count:  9043\n\n`})}),`\n`,(0,n.jsxs)(e.h3,{id:\"all-containers-are-not-created-equally\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#all-containers-are-not-created-equally\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"All containers are not created equally.\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Docker allows you to define an Entrypoint and Cmd which you can mix and match in a Dockerfile. Entrypoint is the executable, and Cmd are the arguments passed to the Entrypoint. The Dockerfile schema is quite lenient and allows users to set Cmd without Entrypoint, which means that the first argument in Cmd will be the executable to run.\"}),`\n`,(0,n.jsx)(e.p,{children:\"Note: k8s uses a different naming convention for Docker Entrypoint and Cmd. In Kubernetes command is Docker Entrypoint and Kubernetes args is Docker Cmd.\"}),`\n`,(0,n.jsxs)(e.table,{children:[(0,n.jsx)(e.thead,{children:(0,n.jsxs)(e.tr,{children:[(0,n.jsx)(e.th,{children:\"Description\"}),(0,n.jsx)(e.th,{children:\"The command run by the container\"}),(0,n.jsx)(e.th,{children:\"Arguments passed to the command\"})]})}),(0,n.jsxs)(e.tbody,{children:[(0,n.jsxs)(e.tr,{children:[(0,n.jsx)(e.td,{children:\"Docker field name\"}),(0,n.jsx)(e.td,{children:\"Entrypoint\"}),(0,n.jsx)(e.td,{children:\"Cmd\"})]}),(0,n.jsxs)(e.tr,{children:[(0,n.jsx)(e.td,{children:\"Kubernetes field name\"}),(0,n.jsx)(e.td,{children:\"Command\"}),(0,n.jsx)(e.td,{children:\"args\"})]})]})]}),`\n`,(0,n.jsx)(e.p,{children:\"There are a few tricks to understanding how the container you're working with starts up. In order to get the startup command when you're dealing with someone else's container, we need to know the intended Docker Entrypoint and Cmd of the Docker image. If you have the Dockerfile that created the Docker image, then you likely already know the Entrypoint and Cmd, unless you aren't defining them and inheriting from a base image that has them set.\"}),`\n`,(0,n.jsx)(e.p,{children:\"When dealing with either off the shelf containers, using someone else's container and you don't have the Dockerfile, or you're inheriting from a base image that you don't have the Dockerfile for, you can use the following steps to get the values you need. First, we pull the container locally using docker pull, then we inspect the container image to get the Entrypoint and Cmd:\"}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsxs)(e.li,{children:[\"docker pull \",(0,n.jsx)(\"image\",{id:\"\"})]}),`\n`,(0,n.jsxs)(e.li,{children:[\"docker inspect \",(0,n.jsx)(\"image\",{id:\"\"})]}),`\n`]}),`\n`,(0,n.jsx)(e.p,{children:\"Here we use jq to filter the JSON response from docker inspect:\"}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-ruby\",children:`\ndavid@sega:~: docker pull docker.elastic.co/elasticsearch/elasticsearch:7.10.2\n7.10.2: Pulling from elasticsearch/elasticsearch\nddf49b9115d7: Pull complete\ne736878e27ad: Pull complete\n7487c9dcefbe: Pull complete\n9ccb7e6e1f0c: Pull complete\ndcec6dec98db: Pull complete\n8a10b4854661: Pull complete\n1e595aee1b7d: Pull complete\n06cc198dbf22: Pull complete\n55b9b1b50ed8: Pull complete\nDigest: sha256:d528cec81720266974fdfe7a0f12fee928dc02e5a2c754b45b9a84c84695bfd9\nStatus: Downloaded newer image for docker.elastic.co/elasticsearch/elasticsearch:7.10.2\ndocker.elastic.co/elasticsearch/elasticsearch:7.10.2\ndavid@sega:~: docker inspect docker.elastic.co/elasticsearch/elasticsearch:7.10.2 | jq '.[0] .ContainerConfig .Entrypoint'\n[\n  \"/tini\",\n  \"--\",\n  \"/usr/local/bin/docker-entrypoint.sh\"\n]\ndavid@sega:~: docker inspect docker.elastic.co/elasticsearch/elasticsearch:7.10.2 | jq '.[0] .ContainerConfig .Cmd'\n[\n  \"/bin/sh\",\n  \"-c\",\n  \"#(nop) \",\n  \"CMD [\\\\\"eswrapper\\\\\"]\"\n]\n\n`})}),`\n`,(0,n.jsxs)(e.h3,{id:\"the-dreaded-crashloopbackoff\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#the-dreaded-crashloopbackoff\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"The Dreaded CrashLoopBackOff\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Now that you have all that background, let's get to debugging the CrashLoopBackOff.\"}),`\n`,(0,n.jsx)(e.p,{children:\"In order to understand what's happening, it's important to be able to inspect the container inside of k8s so the application has all the environment variables and dependent services. Updating the deployment and setting the container Entrypoint or k8s command temporarily to tail -f /dev/null or sleep infinity will give you an opportunity to debug why the service doesn't stay running.\"}),`\n`,(0,n.jsx)(e.p,{children:\"Here's how to configure k8s to override the container Entrypoint:\"}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-ruby\",children:`\napiVersion: extensions/v1beta1\nkind: Deployment\nmetadata:\n  name: elasticsearch\n  namespace: elasticsearch\nspec:\n  progressDeadlineSeconds: 600\n  replicas: 1\n  revisionHistoryLimit: 3\n  selector:\n    matchLabels:\n      app: backend\n      tier: backend\n  strategy:\n    rollingUpdate:\n      maxSurge: 25%\n      maxUnavailable: 25%\n    type: RollingUpdate\n  template:\n    metadata:\n      creationTimestamp: null\n      labels:\n        app: backend\n        tier: backend\n    spec:\n      containers:\n      - command:\n        - tail\n        - \"-f\"\n        - /dev/null\n\n`})}),`\n`,(0,n.jsx)(e.p,{children:\"Here's the configuration in Release:\"}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-ruby\",children:`\nservices:\n- name: elasticsearch\n  image: docker.elastic.co/elasticsearch/elasticsearch:7.10.2\n  command:\n  - tail\n  - \"-f\"\n  - /dev/null\n\n`})}),`\n`,(0,n.jsx)(e.p,{children:\"You can now use kubectl or k9s to exec into the container and take a look around. Using the Entrypoint and Cmd you discovered earlier, you can execute the intended startup command and see how the application is failing.\"}),`\n`,(0,n.jsx)(e.p,{children:\"Depending on the container you're running, it may be missing many of the tools necessary to debug your problem like: curl, lsof, vim; and if it's someone else's code, you probably don't know which version of linux was used to create the image. We typically try all of the common package managers until we find the right one. Most containers these days use Alpine Linux (apk package manager) or a Debian, Ubuntu (apt-get package manager) based image. In some cases we've seen Centos and Fedora, which both use the yum package manager.\"}),`\n`,(0,n.jsx)(e.p,{children:\"One of the following commands should work depending on the operating system:\"}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsx)(e.li,{children:\"apk\"}),`\n`,(0,n.jsx)(e.li,{children:\"apt-get\"}),`\n`,(0,n.jsx)(e.li,{children:\"yum\"}),`\n`]}),`\n`,(0,n.jsx)(e.p,{children:\"Dockerfile maintainers often remove the cache from the package manager to shrink the size of the image, so you may also need to run one of the following:\"}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsx)(e.li,{children:\"apk update\"}),`\n`,(0,n.jsx)(e.li,{children:\"apt-get update\"}),`\n`,(0,n.jsx)(e.li,{children:\"yum makecache\"}),`\n`]}),`\n`,(0,n.jsx)(e.p,{children:\"Now you need to add the necessary tools to help with debugging. Depending on the package manager you found, use one of the following commands to add useful debugging tools:\"}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsx)(e.li,{children:\"apt-get install -y curl vim procps inetutils-tools net-tools lsof\"}),`\n`,(0,n.jsx)(e.li,{children:\"apk add curl vim procps net-tools lsof\"}),`\n`,(0,n.jsx)(e.li,{children:\"yum install curl vim procps lsof\"}),`\n`]}),`\n`,(0,n.jsx)(e.p,{children:\"At this point, it's up to you to figure out the problem. You can edit files using vim to tweak the container until you understand what's going on. If you forget all of the files you've touched on the container, you can alway kill the pod and the container will restart without your changes. Always remember to write down the steps taken to get the container working. You'll want to use your notes to alter the Dockerfile or add commands to the container startup scripts.\"}),`\n`,(0,n.jsxs)(e.h3,{id:\"debugging-your-containers\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#debugging-your-containers\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Debugging Your Containers\"]}),`\n`,(0,n.jsx)(e.p,{children:\"We have created a simple script to get all of the debuging tools, as long as you are working with a container that has curl pre-installed:\"}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-ruby\",children:`\n# install debugging tools on a container with curl pre-installed\n\n/bin/sh -c \"$(curl -fsSL https://raw.githubusercontent.com/releaseapp-io/container-debug/main/install.sh)\"\n\n`})}),`\n`,(0,n.jsxs)(e.h3,{id:\"conclusion\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#conclusion\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Conclusion\"]}),`\n`,(0,n.jsx)(e.p,{children:\"In this article, we've learnt how to spot and investigate the CrashLoopBackOff errors in containers. We walked you through how to inspect and investigate the container image itself. We've listed and shown some tools that we use to spot problems and investigate issues. We got several useful and basic tools installed on the image, hopefully regardless of base image. With these steps in mind and all the tools ready at your disposal, go forth and fix all the things!\"})]})}function C(t={}){let{wrapper:e}=t.components||{};return e?(0,n.jsx)(e,Object.assign({},t,{children:(0,n.jsx)(d,t)})):d(t)}var v=C;return k(x);})();\n;return Component;"
        },
        "_id": "blog/posts/kubernetes-how-to-debug-crashloopbackoff-in-a-container.mdx",
        "_raw": {
          "sourceFilePath": "blog/posts/kubernetes-how-to-debug-crashloopbackoff-in-a-container.mdx",
          "sourceFileName": "kubernetes-how-to-debug-crashloopbackoff-in-a-container.mdx",
          "sourceFileDir": "blog/posts",
          "contentType": "mdx",
          "flattenedPath": "blog/posts/kubernetes-how-to-debug-crashloopbackoff-in-a-container"
        },
        "type": "BlogPost",
        "computedSlug": "kubernetes-how-to-debug-crashloopbackoff-in-a-container"
      },
      "documentHash": "1739658627695",
      "hasWarnings": false,
      "documentTypeName": "BlogPost"
    },
    "blog/posts/kubernetes-jobs-how-to-create-schedule-run-and-more-2.mdx": {
      "document": {
        "title": "Kubernetes Jobs: How to Create, Schedule, Run, and More",
        "summary": "How to use Kubernetes Jobs and a tutorial on how to create, schedule, configure, and run K8 Jobs.\n",
        "publishDate": "Tue Feb 28 2023 17:42:53 GMT+0000 (Coordinated Universal Time)",
        "author": "ira-casteel",
        "readingTime": 7,
        "categories": [
          "kubernetes",
          "platform-engineering"
        ],
        "mainImage": "/blog-images/37f786a88f65ffdbf93fb90b24f05d72.jpg",
        "imageAlt": "Image with a lot of containers",
        "showCTA": true,
        "ctaCopy": "Automate Kubernetes job environment setup with Release for seamless collaboration, faster testing, and consistent deployments.",
        "ctaLink": "https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=kubernetes-jobs-how-to-create-schedule-run-and-more-2",
        "relatedPosts": [
          ""
        ],
        "ogImage": "/blog-images/37f786a88f65ffdbf93fb90b24f05d72.jpg",
        "excerpt": "How to use Kubernetes Jobs and a tutorial on how to create, schedule, configure, and run K8 Jobs.\n",
        "tags": [
          "kubernetes",
          "platform-engineering"
        ],
        "ctaButton": "Try Release for Free",
        "body": {
          "raw": "\nKubernetes jobs have many real-life applications. For example, you can use jobs to execute a process for data backup. Once the backup is successful, the job stops running the pods. \n\nIn this post, you'll learn how to use Kubernetes jobs. We'll walk you through a tutorial on how to create, schedule, configure, and run Kubernetes Jobs. \n\n### What Is Kubernetes Jobs?\n\nKubernetes jobs are controllers that create one or more [pods](https://release.com/blog/kubernetes-environment-variables#:~:text=Configuration%20for%20Your%20Pods) that will run until execution successfully terminates for a specific number of pods. Once the task assigned to a job completes without any error, the task(job) stops running. In case of a failure, the job attempts to retry until all pods run successfully. You can limit how many times a job retries execution using configurations like **activeDeadline** and **backoffLimit**. \n\n### Use Cases of Kubernetes Jobs (When to Use It)\n\nNow that we know some examples of tasks we can execute using jobs, let's walk through a brief tutorial on how to work with Kubernetes jobs. \n\n#### 1\\. Backups\n\nYou can use Kubernetes jobs to perform a task like the periodic backup of data on your server or application. For example, if you want a backup, you set up a job and run it one time. The job will continue running its pods until the backup completes. If the job fails, it will retry. Hence, you can just start the job and not worry about it stopping until the backup completes. Also, you don't have to worry about the task executing again after the backup succeeds. \n\n#### 2\\. Installing and Uninstalling Services\n\nAnother good example of a task you can perform with Kubernetes jobs is installing new services for your application. Likewise, you can use jobs to remove existing services that you no longer need. Similar to our backup example, these jobs will run and stop executing their pods as soon as they successfully add or remove the target services. If they don’t succeed, the jobs retry the tasks. \n\n### How to Use Kubernetes Jobs\n\nNow that we know some examples of tasks we can execute using jobs, let's walk through a brief tutorial on how to work with Kubernetes jobs. \n\n#### Prerequisites\n\nIn order to follow along better, you'll need to have the following tools and experience: \n\n- Kubernetes installation on your target machine\n- Basic knowledge of [Docker](https://release.com/blog/6-docker-compose-best-practices-for-dev-and-prod) and Kubernetes\n- Knowledge of Terminal and CLI\n\nWith that out of the way, let's walk through the actual steps for using jobs. \n\n#### Step 1: Creating a Job\n\nJust like most operations in Kubernetes, you create jobs using a [**YAML file**](https://en.wikipedia.org/wiki/YAML). The YAML file will contain all the details about your job, like the name of the job, what should happen when a pod fails, and so on. In later steps, we'll take a closer look at the various job configuration options. \n\nOn your machine, create a new file with the name **hello_world_job.yaml** and paste the configuration for your new job into it. For this tutorial, we'll use the following code: \n\n`apiVersion: batch/v1 kind: Job metadata:   name: hello-world-job spec:   template:     metadata:       name: hello-world-job     spec:       containers:       - name: hello-world         image: centos:7         command:          - \"bin/bash\"          - \"-c\"          - \"echo hello world\"       restartPolicy: Never`\n\nThe above configuration is for a job that simply prints \"hello world.\" Next, finish up creating the job by running the following command: \n\n`kubectl apply -f hello_world_job.yaml`\n\nYou should get the following message on your terminal if the command runs successfully: \n\n`job.batch/hello-world-job created`\n\nAlso, you can verify that your job was created by running this command: \n\n`kubectl get jobs`\n\nThe output of this command is a list of all your jobs, similar to the following: \n\n![](/blog-images/3c42d4157cff405812e144cf7c56d89d.svg)\n\nFrom the above photo, we can see that we created our **hello-world-job** successfully. \n\n#### Step 2: Configuring a Job\n\nFrom the previous step, we already have a few configurations for our job. However, let's walk through a few more complex configurations. In order to do that, let's create a new job. Create a new **hello_world_4x.yaml** file and add the following code to it: \n\n`apiVersion: batch/v1 kind: Job metadata:   name: hello-world-4x-job spec:   completions: 4   template:     metadata:       name: hello-world-4x-job     spec:       containers:       - name: hello-world-4x         image: centos:7         command:          - \"bin/bash\"          - \"-c\"          - \"echo hello world\"       restartPolicy: Never`\n\n**Completion**: In this bit of code, we introduce a new configuration (i.e., completions). In step 1, Kubernetes created a single pod that runs our task once. However, using completions, we can perform the same task multiple times. Completions run multiple pods one after the other. \n\nLet's take a look at another configuration option. Again, create a new **hello_world_4x_parallel.yaml** file and add the following code to it: \n\n`apiVersion: batch/v1 kind: Job metadata:   name: hello-world-parallel-job spec:   completions: 4   parallelism: 2   template:     metadata:       name: hello-world-parallel-job     spec:       containers:       - name: hello-world-parallel         image: centos:7         command:          - \"bin/bash\"          - \"-c\"          - \"echo hello world\"       restartPolicy: Never`\n\n**Parallelism**: Notice the new configuration item, parallelism. The previous job executed pods one after another. However, we can configure a job to run pods in parallel using this new configuration. \n\n#### Step 3: Schedule a Job\n\nIf you need to start jobs at a specific time in the future, or you want to run them in a repetitive pattern at specific intervals, you should consider using a **CronJob**. A CronJob creates jobs that repeat using a schedule. You can schedule the job using the cron format and can set the schedule in the **schedule** object. \n\nThe following example YAML file shows a CronJob: \n\n`apiVersion: batch/v1 kind: CronJob metadata:   name: hello-world-cron spec:   schedule: \"*/5 * * * *\"   jobTemplate:     spec:       template:         spec:           containers:           - name: hello-world             image: centos:7             imagePullPolicy: IfNotPresent             command:             - /bin/sh             - -c             - \"echo Hello World\"           restartPolicy: OnFailure`\n\nIn this code, the cron schedule format is the string \"\\*/5 \\* \\* \\* \\*.\" It contains 5 sections (separated with white spaces), representing a minute, hour, day of the month, and day of the week in that order. \"\\*/5\" means the task will run every 5 minutes. To explain the schedule format further, if you change the schedule to \"0 \\*/5 \\* \\* \\*\", the job will execute every 5 hours. Also, setting all 5 fields to \"\\*\" means a job will run every minute. \n\nTo create the job on your machine, run the following command: \n\n`kubectl create -f your-cronjob-yaml-file`\n\nTo see the cronjob you just created, run the **kubectl create -f cronjob.yaml** command. \n\n#### Step 4: Running a Job\n\nTo run a job after creating the YAML file for it, run the **kubectl apply -f \\[yaml-file\\]** command. Replace \\[yaml-file\\] with the actual file name for your job configuration. \n\nYou can verify the status of your job by running the **kubectl get jobs** command. For an even more detailed report, you can run **kubectl describe job \\[job-name\\]**. \n\n#### Step 5: Deleting a Job\n\nFor logging and tracking purposes, jobs and the pods they create do not get deleted even after they stop running. However, when you no longer need them, you can clean old jobs and their pods up. To do this you can use the **kubectl delete jobs/\\[job-name\\]** command.   \n\n### Summing Everything up\n\nIn this post, we've covered what Kubernetes Jobs are—resources that create pods that keep running until successful completion. \n\nYou also learned how to create, configure and run Kubernetes jobs. For jobs that need to run at a specific time or repetitively, you can use the CronJob Kubernetes resource. \n\nFinally, you learned how to delete Kubernetes jobs after they complete. Since a record of jobs and their pods remain even after completion, if you no longer need a specific record, you can delete it by deleting the job.\n\n‍\n",
          "code": "var Component=(()=>{var d=Object.create;var r=Object.defineProperty;var u=Object.getOwnPropertyDescriptor;var p=Object.getOwnPropertyNames;var b=Object.getPrototypeOf,m=Object.prototype.hasOwnProperty;var g=(o,e)=>()=>(e||o((e={exports:{}}).exports,e),e.exports),f=(o,e)=>{for(var t in e)r(o,t,{get:e[t],enumerable:!0})},s=(o,e,t,i)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let a of p(e))!m.call(o,a)&&a!==t&&r(o,a,{get:()=>e[a],enumerable:!(i=u(e,a))||i.enumerable});return o};var w=(o,e,t)=>(t=o!=null?d(b(o)):{},s(e||!o||!o.__esModule?r(t,\"default\",{value:o,enumerable:!0}):t,o)),j=o=>s(r({},\"__esModule\",{value:!0}),o);var c=g((K,l)=>{l.exports=_jsx_runtime});var x={};f(x,{default:()=>v,frontmatter:()=>y});var n=w(c()),y={title:\"Kubernetes Jobs: How to Create, Schedule, Run, and More\",summary:`How to use Kubernetes Jobs and a tutorial on how to create, schedule, configure, and run K8 Jobs.\n`,publishDate:\"Tue Feb 28 2023 17:42:53 GMT+0000 (Coordinated Universal Time)\",author:\"ira-casteel\",readingTime:7,categories:[\"kubernetes\",\"platform-engineering\"],mainImage:\"/blog-images/37f786a88f65ffdbf93fb90b24f05d72.jpg\",imageAlt:\"Image with a lot of containers\",showCTA:!0,ctaCopy:\"Automate Kubernetes job environment setup with Release for seamless collaboration, faster testing, and consistent deployments.\",ctaLink:\"https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=kubernetes-jobs-how-to-create-schedule-run-and-more-2\",relatedPosts:[\"\"],ogImage:\"/blog-images/37f786a88f65ffdbf93fb90b24f05d72.jpg\",excerpt:`How to use Kubernetes Jobs and a tutorial on how to create, schedule, configure, and run K8 Jobs.\n`,tags:[\"kubernetes\",\"platform-engineering\"],ctaButton:\"Try Release for Free\"};function h(o){let e=Object.assign({p:\"p\",h3:\"h3\",a:\"a\",span:\"span\",strong:\"strong\",h4:\"h4\",ul:\"ul\",li:\"li\",code:\"code\",img:\"img\"},o.components);return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(e.p,{children:\"Kubernetes jobs have many real-life applications. For example, you can use jobs to execute a process for data backup. Once the backup is successful, the job stops running the pods.\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:\"In this post, you'll learn how to use Kubernetes jobs. We'll walk you through a tutorial on how to create, schedule, configure, and run Kubernetes Jobs.\\xA0\"}),`\n`,(0,n.jsxs)(e.h3,{id:\"what-is-kubernetes-jobs\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#what-is-kubernetes-jobs\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"What Is Kubernetes Jobs?\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"Kubernetes jobs are controllers that create one or more \",(0,n.jsx)(e.a,{href:\"https://release.com/blog/kubernetes-environment-variables#:~:text=Configuration%20for%20Your%20Pods\",children:\"pods\"}),\" that will run until execution successfully terminates for a specific number of pods. Once the task assigned to a job completes without any error, the task(job) stops running. In case of a failure, the job attempts to retry until all pods run successfully. You can limit how many times a job retries execution using configurations like \",(0,n.jsx)(e.strong,{children:\"activeDeadline\"}),\" and \",(0,n.jsx)(e.strong,{children:\"backoffLimit\"}),\".\\xA0\"]}),`\n`,(0,n.jsxs)(e.h3,{id:\"use-cases-of-kubernetes-jobs-when-to-use-it\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#use-cases-of-kubernetes-jobs-when-to-use-it\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Use Cases of Kubernetes Jobs (When to Use It)\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Now that we know some examples of tasks we can execute using jobs, let's walk through a brief tutorial on how to work with Kubernetes jobs.\\xA0\"}),`\n`,(0,n.jsxs)(e.h4,{id:\"1-backups\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#1-backups\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"1. Backups\"]}),`\n`,(0,n.jsx)(e.p,{children:\"You can use Kubernetes jobs to perform a task like the periodic backup of data on your server or application. For example, if you want a backup, you set up a job and run it one time. The job will continue running its pods until the backup completes. If the job fails, it will retry. Hence, you can just start the job and not worry about it stopping until the backup completes. Also, you don't have to worry about the task executing again after the backup succeeds.\\xA0\"}),`\n`,(0,n.jsxs)(e.h4,{id:\"2-installing-and-uninstalling-services\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#2-installing-and-uninstalling-services\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"2. Installing and Uninstalling Services\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Another good example of a task you can perform with Kubernetes jobs is installing new services for your application. Likewise, you can use jobs to remove existing services that you no longer need. Similar to our backup example, these jobs will run and stop executing their pods as soon as they successfully add or remove the target services. If they don\\u2019t succeed, the jobs retry the tasks.\\xA0\"}),`\n`,(0,n.jsxs)(e.h3,{id:\"how-to-use-kubernetes-jobs\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#how-to-use-kubernetes-jobs\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"How to Use Kubernetes Jobs\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Now that we know some examples of tasks we can execute using jobs, let's walk through a brief tutorial on how to work with Kubernetes jobs.\\xA0\"}),`\n`,(0,n.jsxs)(e.h4,{id:\"prerequisites\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#prerequisites\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Prerequisites\"]}),`\n`,(0,n.jsx)(e.p,{children:\"In order to follow along better, you'll need to have the following tools and experience:\\xA0\"}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsx)(e.li,{children:\"Kubernetes installation on your target machine\"}),`\n`,(0,n.jsxs)(e.li,{children:[\"Basic knowledge of \",(0,n.jsx)(e.a,{href:\"https://release.com/blog/6-docker-compose-best-practices-for-dev-and-prod\",children:\"Docker\"}),\" and Kubernetes\"]}),`\n`,(0,n.jsx)(e.li,{children:\"Knowledge of Terminal and CLI\"}),`\n`]}),`\n`,(0,n.jsx)(e.p,{children:\"With that out of the way, let's walk through the actual steps for using jobs.\\xA0\"}),`\n`,(0,n.jsxs)(e.h4,{id:\"step-1-creating-a-job\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#step-1-creating-a-job\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Step 1: Creating a Job\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"Just like most operations in Kubernetes, you create jobs using a \",(0,n.jsx)(e.a,{href:\"https://en.wikipedia.org/wiki/YAML\",children:(0,n.jsx)(e.strong,{children:\"YAML file\"})}),\". The YAML file will contain all the details about your job, like the name of the job, what should happen when a pod fails, and so on. In later steps, we'll take a closer look at the various job configuration options.\\xA0\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"On your machine, create a new file with the name \",(0,n.jsx)(e.strong,{children:\"hello_world_job.yaml\"}),\" and paste the configuration for your new job into it. For this tutorial, we'll use the following code:\\xA0\"]}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.code,{children:'apiVersion: batch/v1 kind: Job metadata:  \\xA0name: hello-world-job spec:  \\xA0template:  \\xA0 \\xA0metadata:  \\xA0 \\xA0 \\xA0name: hello-world-job  \\xA0 \\xA0spec:  \\xA0 \\xA0 \\xA0containers:  \\xA0 \\xA0 \\xA0- name: hello-world  \\xA0 \\xA0 \\xA0 \\xA0image: centos:7  \\xA0 \\xA0 \\xA0 \\xA0command:  \\xA0 \\xA0 \\xA0 \\xA0 - \"bin/bash\"  \\xA0 \\xA0 \\xA0 \\xA0 - \"-c\"  \\xA0 \\xA0 \\xA0 \\xA0 - \"echo hello world\"  \\xA0 \\xA0 \\xA0restartPolicy: Never'})}),`\n`,(0,n.jsx)(e.p,{children:'The above configuration is for a job that simply prints \"hello world.\" Next, finish up creating the job by running the following command:\\xA0'}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.code,{children:\"kubectl apply -f hello_world_job.yaml\"})}),`\n`,(0,n.jsx)(e.p,{children:\"You should get the following message on your terminal if the command runs successfully:\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.code,{children:\"job.batch/hello-world-job created\"})}),`\n`,(0,n.jsx)(e.p,{children:\"Also, you can verify that your job was created by running this command:\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.code,{children:\"kubectl get jobs\"})}),`\n`,(0,n.jsx)(e.p,{children:\"The output of this command is a list of all your jobs, similar to the following:\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/3c42d4157cff405812e144cf7c56d89d.svg\",alt:\"\"})}),`\n`,(0,n.jsxs)(e.p,{children:[\"From the above photo, we can see that we created our \",(0,n.jsx)(e.strong,{children:\"hello-world-job\"}),\" successfully.\\xA0\"]}),`\n`,(0,n.jsxs)(e.h4,{id:\"step-2-configuring-a-job\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#step-2-configuring-a-job\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Step 2: Configuring a Job\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"From the previous step, we already have a few configurations for our job. However, let's walk through a few more complex configurations. In order to do that, let's create a new job. Create a new \",(0,n.jsx)(e.strong,{children:\"hello_world_4x.yaml\"}),\" file and add the following code to it:\\xA0\"]}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.code,{children:'apiVersion: batch/v1 kind: Job metadata:  \\xA0name: hello-world-4x-job spec:  \\xA0completions: 4  \\xA0template:  \\xA0 \\xA0metadata:  \\xA0 \\xA0 \\xA0name: hello-world-4x-job  \\xA0 \\xA0spec:  \\xA0 \\xA0 \\xA0containers:  \\xA0 \\xA0 \\xA0- name: hello-world-4x  \\xA0 \\xA0 \\xA0 \\xA0image: centos:7  \\xA0 \\xA0 \\xA0 \\xA0command:  \\xA0 \\xA0 \\xA0 \\xA0 - \"bin/bash\"  \\xA0 \\xA0 \\xA0 \\xA0 - \"-c\"  \\xA0 \\xA0 \\xA0 \\xA0 - \"echo hello world\"  \\xA0 \\xA0 \\xA0restartPolicy: Never'})}),`\n`,(0,n.jsxs)(e.p,{children:[(0,n.jsx)(e.strong,{children:\"Completion\"}),\": In this bit of code, we introduce a new configuration (i.e., completions). In step 1, Kubernetes created a single pod that runs our task once. However, using completions, we can perform the same task multiple times. Completions run multiple pods one after the other.\\xA0\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"Let's take a look at another configuration option. Again, create a new \",(0,n.jsx)(e.strong,{children:\"hello_world_4x_parallel.yaml\"}),\" file and add the following code to it:\\xA0\"]}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.code,{children:'apiVersion: batch/v1 kind: Job metadata:  \\xA0name: hello-world-parallel-job spec:  \\xA0completions: 4  \\xA0parallelism: 2  \\xA0template:  \\xA0 \\xA0metadata:  \\xA0 \\xA0 \\xA0name: hello-world-parallel-job  \\xA0 \\xA0spec:  \\xA0 \\xA0 \\xA0containers:  \\xA0 \\xA0 \\xA0- name: hello-world-parallel  \\xA0 \\xA0 \\xA0 \\xA0image: centos:7  \\xA0 \\xA0 \\xA0 \\xA0command:  \\xA0 \\xA0 \\xA0 \\xA0 - \"bin/bash\"  \\xA0 \\xA0 \\xA0 \\xA0 - \"-c\"  \\xA0 \\xA0 \\xA0 \\xA0 - \"echo hello world\"  \\xA0 \\xA0 \\xA0restartPolicy: Never'})}),`\n`,(0,n.jsxs)(e.p,{children:[(0,n.jsx)(e.strong,{children:\"Parallelism\"}),\": Notice the new configuration item, parallelism. The previous job executed pods one after another. However, we can configure a job to run pods in parallel using this new configuration.\\xA0\"]}),`\n`,(0,n.jsxs)(e.h4,{id:\"step-3-schedule-a-job\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#step-3-schedule-a-job\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Step 3: Schedule a Job\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"If you need to start jobs at a specific time in the future, or you want to run them in a repetitive pattern at specific intervals, you should consider using a \",(0,n.jsx)(e.strong,{children:\"CronJob\"}),\". A CronJob creates jobs that repeat using a schedule. You can schedule the job using the cron format and can set the schedule in the \",(0,n.jsx)(e.strong,{children:\"schedule\"}),\" object.\\xA0\"]}),`\n`,(0,n.jsx)(e.p,{children:\"The following example YAML file shows a CronJob:\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.code,{children:'apiVersion: batch/v1 kind: CronJob metadata:  \\xA0name: hello-world-cron spec:  \\xA0schedule: \"*/5 * * * *\"  \\xA0jobTemplate:  \\xA0 \\xA0spec:  \\xA0 \\xA0 \\xA0template:  \\xA0 \\xA0 \\xA0 \\xA0spec:  \\xA0 \\xA0 \\xA0 \\xA0 \\xA0containers:  \\xA0 \\xA0 \\xA0 \\xA0 \\xA0- name: hello-world  \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0image: centos:7  \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0imagePullPolicy: IfNotPresent  \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0command:  \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0- /bin/sh  \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0- -c  \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0- \"echo Hello World\"  \\xA0 \\xA0 \\xA0 \\xA0 \\xA0restartPolicy: OnFailure'})}),`\n`,(0,n.jsx)(e.p,{children:'In this code, the cron schedule format is the string \"*/5 * * * *.\" It contains 5 sections (separated with white spaces), representing a minute, hour, day of the month, and day of the week in that order. \"*/5\" means the task will run every 5 minutes. To explain the schedule format further, if you change the schedule to \"0 */5 * * *\", the job will execute every 5 hours. Also, setting all 5 fields to \"*\" means a job will run every minute.\\xA0'}),`\n`,(0,n.jsx)(e.p,{children:\"To create the job on your machine, run the following command:\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.code,{children:\"kubectl create -f your-cronjob-yaml-file\"})}),`\n`,(0,n.jsxs)(e.p,{children:[\"To see the cronjob you just created, run the \",(0,n.jsx)(e.strong,{children:\"kubectl create -f cronjob.yaml\"}),\" command.\\xA0\"]}),`\n`,(0,n.jsxs)(e.h4,{id:\"step-4-running-a-job\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#step-4-running-a-job\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Step 4: Running a Job\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"To run a job after creating the YAML file for it, run the \",(0,n.jsx)(e.strong,{children:\"kubectl apply -f [yaml-file]\"}),\" command. Replace [yaml-file] with the actual file name for your job configuration.\\xA0\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"You can verify the status of your job by running the \",(0,n.jsx)(e.strong,{children:\"kubectl get jobs\"}),\" command. For an even more detailed report, you can run \",(0,n.jsx)(e.strong,{children:\"kubectl describe job [job-name]\"}),\".\\xA0\"]}),`\n`,(0,n.jsxs)(e.h4,{id:\"step-5-deleting-a-job\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#step-5-deleting-a-job\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Step 5: Deleting a Job\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"For logging and tracking purposes, jobs and the pods they create do not get deleted even after they stop running. However, when you no longer need them, you can clean old jobs and their pods up. To do this you can use the \",(0,n.jsx)(e.strong,{children:\"kubectl delete jobs/[job-name]\"}),\" command. \\xA0\\xA0\"]}),`\n`,(0,n.jsxs)(e.h3,{id:\"summing-everything-up\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#summing-everything-up\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Summing Everything up\"]}),`\n`,(0,n.jsx)(e.p,{children:\"In this post, we've covered what Kubernetes Jobs are\\u2014resources that create pods that keep running until successful completion.\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:\"You also learned how to create, configure and run Kubernetes jobs. For jobs that need to run at a specific time or repetitively, you can use the CronJob Kubernetes resource.\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:\"Finally, you learned how to delete Kubernetes jobs after they complete. Since a record of jobs and their pods remain even after completion, if you no longer need a specific record, you can delete it by deleting the job.\"}),`\n`,(0,n.jsx)(e.p,{children:\"\\u200D\"})]})}function k(o={}){let{wrapper:e}=o.components||{};return e?(0,n.jsx)(e,Object.assign({},o,{children:(0,n.jsx)(h,o)})):h(o)}var v=k;return j(x);})();\n;return Component;"
        },
        "_id": "blog/posts/kubernetes-jobs-how-to-create-schedule-run-and-more-2.mdx",
        "_raw": {
          "sourceFilePath": "blog/posts/kubernetes-jobs-how-to-create-schedule-run-and-more-2.mdx",
          "sourceFileName": "kubernetes-jobs-how-to-create-schedule-run-and-more-2.mdx",
          "sourceFileDir": "blog/posts",
          "contentType": "mdx",
          "flattenedPath": "blog/posts/kubernetes-jobs-how-to-create-schedule-run-and-more-2"
        },
        "type": "BlogPost",
        "computedSlug": "kubernetes-jobs-how-to-create-schedule-run-and-more-2"
      },
      "documentHash": "1739393595023",
      "hasWarnings": false,
      "documentTypeName": "BlogPost"
    },
    "blog/posts/kubernetes-jobs-how-to-create-schedule-run-and-more.mdx": {
      "document": {
        "title": "Kubernetes Jobs: How to Create, Schedule, Run, and More",
        "summary": "Let's walk through a tutorial on how to create, schedule, configure, and run Kubernetes Jobs.",
        "publishDate": "Tue Sep 06 2022 18:29:41 GMT+0000 (Coordinated Universal Time)",
        "author": "tommy-mcclung",
        "readingTime": 4,
        "categories": [
          "kubernetes",
          "platform-engineering"
        ],
        "mainImage": "/blog-images/a127125d021b2ea0b1d611e065508bb3.jpg",
        "imageAlt": "Kubernetes Jobs: How to Create, Schedule, Run, and More",
        "showCTA": true,
        "ctaCopy": "Simplify Kubernetes job management with Release's ephemeral environments. Test, schedule, and run jobs effortlessly for faster deployment cycles.",
        "ctaLink": "https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=kubernetes-jobs-how-to-create-schedule-run-and-more",
        "relatedPosts": [
          ""
        ],
        "ogImage": "/blog-images/a127125d021b2ea0b1d611e065508bb3.jpg",
        "excerpt": "Let's walk through a tutorial on how to create, schedule, configure, and run Kubernetes Jobs.",
        "tags": [
          "kubernetes",
          "platform-engineering"
        ],
        "ctaButton": "Try Release for Free",
        "body": {
          "raw": "\nKubernetes jobs have many real-life applications. For example, you can use jobs to execute a process for data backup. Once the backup is successful, the job stops running the pods. \n\nIn this post, you'll learn how to use Kubernetes jobs. We'll walk you through a tutorial on how to create, schedule, configure, and run Kubernetes Jobs. \n\n### What Are Kubernetes Jobs?\n\nKubernetes jobs are controllers that create one or more [pods](https://release.com/blog/kubernetes-environment-variables#:~:text=Configuration%20for%20Your%20Pods) that will run until execution successfully terminates for a specific number of pods. Once the task assigned to a job completes without any error, the task(job) stops running. In case of a failure, the job attempts to retry until all pods run successfully. You can limit how many times a job retries execution using configurations like **activeDeadline** and **backoffLimit**.\n\n![](/blog-images/61b64d6364c17fa822bc244ed2254bc7.jpeg)\n\n### Use Cases of Kubernetes Jobs (When to Use It)\n\nNow that we know some examples of tasks we can execute using jobs, let's walk through a brief tutorial on how to work with Kubernetes jobs. \n\n#### 1\\. Backups\n\nYou can use Kubernetes jobs to perform a task like the periodic backup of data on your server or application. For example, if you want a backup, you set up a job and run it one time. The job will continue running its pods until the backup completes. If the job fails, it will retry. Hence, you can just start the job and not worry about it stopping until the backup completes. Also, you don't have to worry about the task executing again after the backup succeeds. \n\n#### 2\\. Installing and Uninstalling Services\n\nAnother good example of a task you can perform with Kubernetes jobs is installing new services for your application. Likewise, you can use jobs to remove existing services that you no longer need. Similar to our backup example, these jobs will run and stop executing their pods as soon as they successfully add or remove the target services. If they don’t succeed, the jobs retry the tasks. \n\n### How to Use Kubernetes Jobs\n\nNow that we know some examples of tasks we can execute using jobs, let's walk through a brief tutorial on how to work with Kubernetes jobs. \n\n### Prerequisites\n\nIn order to follow along better, you'll need to have the following tools and experience: \n\n- Kubernetes installation on your target machine\n- Basic knowledge of [Docker](https://releasehub.com/blog/6-docker-compose-best-practices-for-dev-and-prod) and Kubernetes\n- Knowledge of Terminal and CLI\n\nWith that out of the way, let's walk through the actual steps for using jobs. \n\n#### Step 1: Creating a Job\n\nJust like most operations in Kubernetes, you create jobs using a [**YAML file**](https://en.wikipedia.org/wiki/YAML). The YAML file will contain all the details about your job, like the name of the job, what should happen when a pod fails, and so on. In later steps, we'll take a closer look at the various job configuration options. \n\nSo, on your machine, create a new file with the name **hello_world_job.yaml** and paste the configuration for your new job into it. For this tutorial, we'll use the following code: \n\n```yaml\n\napiVersion: batch/v1\nkind: Job\nmetadata:\n  name: hello-world-job\nspec:\n  template:\n    metadata:\n      name: hello-world-job\n    spec:\n      containers:\n      - name: hello-world\n        image: centos:7\n        command:\n         - \"bin/bash\"\n         - \"-c\"\n         - \"echo hello world\"\n      restartPolicy: Never\n\n```\n\nThe above configuration is for a job that simply prints \"hello world.\" Next, finish up creating the job by running the following command: \n\n```yaml\nkubectl apply -f hello_world_job.yaml\n```\n\nYou should get the following message on your terminal if the command runs successfully: \n\n```yaml\njob.batch/hello-world-job created\n```\n\nAlso, you can verify that your job was created by running this command: \n\n```yaml\nkubectl get jobs\n```\n\nThe output of this command is a list of all your jobs, similar to the following: \n\n![](/blog-images/a5aaf0f992a37f835ee63ec9391620d9.svg)\n\nFrom the above photo, we can see that we created our **hello-world-job** successfully. **‍**\n\n#### **Step 2: Configuring a Job**\n\nFrom the previous step, we already have a few configurations for our job. However, let's walk through a few more complex configurations. In order to do that, let's create a new job. Create a new **hello_world_4x.yaml** file and add the following code to it: \n\n```yaml\n\napiVersion: batch/v1\nkind: Job\nmetadata:\n  name: hello-world-4x-job\nspec:\n  completions: 4\n  template:\n    metadata:\n      name: hello-world-4x-job\n    spec:\n      containers:\n      - name: hello-world-4x\n        image: centos:7\n        command:\n         - \"bin/bash\"\n         - \"-c\"\n         - \"echo hello world\"\n      restartPolicy: Never\n\n```\n\n**Completion**: In this bit of code, we introduce a new configuration (i.e., completions). In step 1, Kubernetes created a single pod that runs our task once. However, using completions, we can perform the same task multiple times. Completions run multiple pods one after the other. \n\nLet's take a look at another configuration option. Again, create a new **hello_world_4x_parallel.yaml** file and add the following code to it: \n\n```yaml\n\napiVersion: batch/v1\nkind: Job\nmetadata:\n  name: hello-world-parallel-job\nspec:\n  completions: 4\n  parallelism: 2\n  template:\n    metadata:\n      name: hello-world-parallel-job\n    spec:\n      containers:\n      - name: hello-world-parallel\n        image: centos:7\n        command:\n         - \"bin/bash\"\n         - \"-c\"\n         - \"echo hello world\"\n      restartPolicy: Never\n\n```\n\n‍**Parallelism**: Notice the new configuration item, parallelism. The previous job executed pods one after another. However, we can configure a job to run pods in parallel using this new configuration. \n\n#### Step 3: Schedule a Job\n\nIf you need to start jobs at a specific time in the future, or you want to run them in a repetitive pattern at specific intervals, you should consider using a **CronJob**. A CronJob creates jobs that repeat using a schedule. You can schedule the job using the cron format and can set the schedule in the **schedule** object. \n\nThe following example YAML file shows a CronJob: \n\n```yaml\n\napiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: hello-world-cron\nspec:\n  schedule: \"*/5 * * * *\"\n  jobTemplate:\n    spec:\n      template:\n        spec:\n          containers:\n          - name: hello-world\n            image: centos:7\n            imagePullPolicy: IfNotPresent\n            command:\n            - /bin/sh\n            - -c\n            - \"echo Hello World\"\n          restartPolicy: OnFailure\n\n```\n\nIn this code, the cron schedule format is the string \"\\*/5 \\* \\* \\* \\*.\" It contains 5 sections (separated with white spaces), representing a minute, hour, day of the month, and day of the week in that order. \"\\*/5\" means the task will run every 5 minutes. To explain the schedule format further, if you change the schedule to \"0 \\*/5 \\* \\* \\*\", the job will execute every 5 hours. Also, setting all 5 fields to \"\\*\" means a job will run every minute. \n\nTo create the job on your machine, run the following command: \n\n```yaml\nkubectl create -f your-cronjob-yaml-file\n```\n\nTo see the cronjob you just created, simply run the **kubectl create -f cronjob.yaml** command. \n\n#### Step 4: Running a Job\n\nTo run a job after creating the YAML file for it, simply run the **kubectl apply -f \\[yaml-file\\]** command. Replace \\[yaml-file\\] with the actual file name for your job configuration. \n\nYou can verify the status of your job by running the **kubectl get jobs** command. For an even more detailed report, you can run **kubectl describe job \\[job-name\\]**. \n\n#### Step 5: Deleting a Job\n\nFor logging and tracking purposes, jobs and the pods they create do not get deleted even after they stop running. However, when you no longer need them, you can clean old jobs and their pods up. To do this you can use the **kubectl delete jobs/\\[job-name\\]** command.   \n\n### Summing Everything up\n\nIn this post, we've covered what Kubernetes Jobs are—resources that create pods that keep running until successful completion. \n\nYou also learned how to create, configure and run Kubernetes jobs. For jobs that need to run at a specific time or repetitively, you can use the CronJob Kubernetes resource. \n\nFinally, you learned how to delete Kubernetes jobs after they complete. Since a record of jobs and their pods stay even after completion, if you no longer need that record, you can delete it by deleting the job.\n",
          "code": "var Component=(()=>{var u=Object.create;var r=Object.defineProperty;var d=Object.getOwnPropertyDescriptor;var p=Object.getOwnPropertyNames;var m=Object.getPrototypeOf,b=Object.prototype.hasOwnProperty;var g=(o,e)=>()=>(e||o((e={exports:{}}).exports,e),e.exports),f=(o,e)=>{for(var t in e)r(o,t,{get:e[t],enumerable:!0})},l=(o,e,t,s)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let a of p(e))!b.call(o,a)&&a!==t&&r(o,a,{get:()=>e[a],enumerable:!(s=d(e,a))||s.enumerable});return o};var w=(o,e,t)=>(t=o!=null?u(m(o)):{},l(e||!o||!o.__esModule?r(t,\"default\",{value:o,enumerable:!0}):t,o)),y=o=>l(r({},\"__esModule\",{value:!0}),o);var c=g((K,i)=>{i.exports=_jsx_runtime});var N={};f(N,{default:()=>v,frontmatter:()=>j});var n=w(c()),j={title:\"Kubernetes Jobs: How to Create, Schedule, Run, and More\",summary:\"Let's walk through a tutorial on how to create, schedule, configure, and run Kubernetes Jobs.\",publishDate:\"Tue Sep 06 2022 18:29:41 GMT+0000 (Coordinated Universal Time)\",author:\"tommy-mcclung\",readingTime:4,categories:[\"kubernetes\",\"platform-engineering\"],mainImage:\"/blog-images/a127125d021b2ea0b1d611e065508bb3.jpg\",imageAlt:\"Kubernetes Jobs: How to Create, Schedule, Run, and More\",showCTA:!0,ctaCopy:\"Simplify Kubernetes job management with Release's ephemeral environments. Test, schedule, and run jobs effortlessly for faster deployment cycles.\",ctaLink:\"https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=kubernetes-jobs-how-to-create-schedule-run-and-more\",relatedPosts:[\"\"],ogImage:\"/blog-images/a127125d021b2ea0b1d611e065508bb3.jpg\",excerpt:\"Let's walk through a tutorial on how to create, schedule, configure, and run Kubernetes Jobs.\",tags:[\"kubernetes\",\"platform-engineering\"],ctaButton:\"Try Release for Free\"};function h(o){let e=Object.assign({p:\"p\",h3:\"h3\",a:\"a\",span:\"span\",strong:\"strong\",img:\"img\",h4:\"h4\",ul:\"ul\",li:\"li\",pre:\"pre\",code:\"code\"},o.components);return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(e.p,{children:\"Kubernetes jobs have many real-life applications. For example, you can use jobs to execute a process for data backup. Once the backup is successful, the job stops running the pods.\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:\"In this post, you'll learn how to use Kubernetes jobs. We'll walk you through a tutorial on how to create, schedule, configure, and run Kubernetes Jobs.\\xA0\"}),`\n`,(0,n.jsxs)(e.h3,{id:\"what-are-kubernetes-jobs\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#what-are-kubernetes-jobs\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"What Are Kubernetes Jobs?\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"Kubernetes jobs are controllers that create one or more \",(0,n.jsx)(e.a,{href:\"https://release.com/blog/kubernetes-environment-variables#:~:text=Configuration%20for%20Your%20Pods\",children:\"pods\"}),\" that will run until execution successfully terminates for a specific number of pods. Once the task assigned to a job completes without any error, the task(job) stops running. In case of a failure, the job attempts to retry until all pods run successfully. You can limit how many times a job retries execution using configurations like \",(0,n.jsx)(e.strong,{children:\"activeDeadline\"}),\" and \",(0,n.jsx)(e.strong,{children:\"backoffLimit\"}),\".\"]}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/61b64d6364c17fa822bc244ed2254bc7.jpeg\",alt:\"\"})}),`\n`,(0,n.jsxs)(e.h3,{id:\"use-cases-of-kubernetes-jobs-when-to-use-it\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#use-cases-of-kubernetes-jobs-when-to-use-it\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Use Cases of Kubernetes Jobs (When to Use It)\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Now that we know some examples of tasks we can execute using jobs, let's walk through a brief tutorial on how to work with Kubernetes jobs.\\xA0\"}),`\n`,(0,n.jsxs)(e.h4,{id:\"1-backups\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#1-backups\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"1. Backups\"]}),`\n`,(0,n.jsx)(e.p,{children:\"You can use Kubernetes jobs to perform a task like the periodic backup of data on your server or application. For example, if you want a backup, you set up a job and run it one time. The job will continue running its pods until the backup completes. If the job fails, it will retry. Hence, you can just start the job and not worry about it stopping until the backup completes. Also, you don't have to worry about the task executing again after the backup succeeds.\\xA0\"}),`\n`,(0,n.jsxs)(e.h4,{id:\"2-installing-and-uninstalling-services\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#2-installing-and-uninstalling-services\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"2. Installing and Uninstalling Services\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Another good example of a task you can perform with Kubernetes jobs is installing new services for your application. Likewise, you can use jobs to remove existing services that you no longer need. Similar to our backup example, these jobs will run and stop executing their pods as soon as they successfully add or remove the target services. If they don\\u2019t succeed, the jobs retry the tasks.\\xA0\"}),`\n`,(0,n.jsxs)(e.h3,{id:\"how-to-use-kubernetes-jobs\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#how-to-use-kubernetes-jobs\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"How to Use Kubernetes Jobs\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Now that we know some examples of tasks we can execute using jobs, let's walk through a brief tutorial on how to work with Kubernetes jobs.\\xA0\"}),`\n`,(0,n.jsxs)(e.h3,{id:\"prerequisites\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#prerequisites\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Prerequisites\"]}),`\n`,(0,n.jsx)(e.p,{children:\"In order to follow along better, you'll need to have the following tools and experience:\\xA0\"}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsx)(e.li,{children:\"Kubernetes installation on your target machine\"}),`\n`,(0,n.jsxs)(e.li,{children:[\"Basic knowledge of \",(0,n.jsx)(e.a,{href:\"https://releasehub.com/blog/6-docker-compose-best-practices-for-dev-and-prod\",children:\"Docker\"}),\" and Kubernetes\"]}),`\n`,(0,n.jsx)(e.li,{children:\"Knowledge of Terminal and CLI\"}),`\n`]}),`\n`,(0,n.jsx)(e.p,{children:\"With that out of the way, let's walk through the actual steps for using jobs.\\xA0\"}),`\n`,(0,n.jsxs)(e.h4,{id:\"step-1-creating-a-job\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#step-1-creating-a-job\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Step 1: Creating a Job\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"Just like most operations in Kubernetes, you create jobs using a \",(0,n.jsx)(e.a,{href:\"https://en.wikipedia.org/wiki/YAML\",children:(0,n.jsx)(e.strong,{children:\"YAML file\"})}),\". The YAML file will contain all the details about your job, like the name of the job, what should happen when a pod fails, and so on. In later steps, we'll take a closer look at the various job configuration options.\\xA0\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"So, on your machine, create a new file with the name \",(0,n.jsx)(e.strong,{children:\"hello_world_job.yaml\"}),\" and paste the configuration for your new job into it. For this tutorial, we'll use the following code:\\xA0\"]}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-yaml\",children:`\napiVersion: batch/v1\nkind: Job\nmetadata:\n \\xA0name: hello-world-job\nspec:\n \\xA0template:\n \\xA0 \\xA0metadata:\n \\xA0 \\xA0 \\xA0name: hello-world-job\n \\xA0 \\xA0spec:\n \\xA0 \\xA0 \\xA0containers:\n \\xA0 \\xA0 \\xA0- name: hello-world\n \\xA0 \\xA0 \\xA0 \\xA0image: centos:7\n \\xA0 \\xA0 \\xA0 \\xA0command:\n \\xA0 \\xA0 \\xA0 \\xA0 - \"bin/bash\"\n \\xA0 \\xA0 \\xA0 \\xA0 - \"-c\"\n \\xA0 \\xA0 \\xA0 \\xA0 - \"echo hello world\"\n \\xA0 \\xA0 \\xA0restartPolicy: Never\n\n`})}),`\n`,(0,n.jsx)(e.p,{children:'The above configuration is for a job that simply prints \"hello world.\" Next, finish up creating the job by running the following command:\\xA0'}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-yaml\",children:`kubectl apply -f hello_world_job.yaml\n`})}),`\n`,(0,n.jsx)(e.p,{children:\"You should get the following message on your terminal if the command runs successfully:\\xA0\"}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-yaml\",children:`job.batch/hello-world-job created\n`})}),`\n`,(0,n.jsx)(e.p,{children:\"Also, you can verify that your job was created by running this command:\\xA0\"}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-yaml\",children:`kubectl get jobs\n`})}),`\n`,(0,n.jsx)(e.p,{children:\"The output of this command is a list of all your jobs, similar to the following:\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/a5aaf0f992a37f835ee63ec9391620d9.svg\",alt:\"\"})}),`\n`,(0,n.jsxs)(e.p,{children:[\"From the above photo, we can see that we created our \",(0,n.jsx)(e.strong,{children:\"hello-world-job\"}),\" successfully.\\xA0\",(0,n.jsx)(e.strong,{children:\"\\u200D\"})]}),`\n`,(0,n.jsxs)(e.h4,{id:\"step-2-configuring-a-job\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#step-2-configuring-a-job\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.strong,{children:\"Step 2: Configuring a Job\"})]}),`\n`,(0,n.jsxs)(e.p,{children:[\"From the previous step, we already have a few configurations for our job. However, let's walk through a few more complex configurations. In order to do that, let's create a new job. Create a new \",(0,n.jsx)(e.strong,{children:\"hello_world_4x.yaml\"}),\" file and add the following code to it:\\xA0\"]}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-yaml\",children:`\napiVersion: batch/v1\nkind: Job\nmetadata:\n \\xA0name: hello-world-4x-job\nspec:\n \\xA0completions: 4\n \\xA0template:\n \\xA0 \\xA0metadata:\n \\xA0 \\xA0 \\xA0name: hello-world-4x-job\n \\xA0 \\xA0spec:\n \\xA0 \\xA0 \\xA0containers:\n \\xA0 \\xA0 \\xA0- name: hello-world-4x\n \\xA0 \\xA0 \\xA0 \\xA0image: centos:7\n \\xA0 \\xA0 \\xA0 \\xA0command:\n \\xA0 \\xA0 \\xA0 \\xA0 - \"bin/bash\"\n \\xA0 \\xA0 \\xA0 \\xA0 - \"-c\"\n \\xA0 \\xA0 \\xA0 \\xA0 - \"echo hello world\"\n \\xA0 \\xA0 \\xA0restartPolicy: Never\n\n`})}),`\n`,(0,n.jsxs)(e.p,{children:[(0,n.jsx)(e.strong,{children:\"Completion\"}),\": In this bit of code, we introduce a new configuration (i.e., completions). In step 1, Kubernetes created a single pod that runs our task once. However, using completions, we can perform the same task multiple times. Completions run multiple pods one after the other.\\xA0\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"Let's take a look at another configuration option. Again, create a new \",(0,n.jsx)(e.strong,{children:\"hello_world_4x_parallel.yaml\"}),\" file and add the following code to it:\\xA0\"]}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-yaml\",children:`\napiVersion: batch/v1\nkind: Job\nmetadata:\n \\xA0name: hello-world-parallel-job\nspec:\n \\xA0completions: 4\n \\xA0parallelism: 2\n \\xA0template:\n \\xA0 \\xA0metadata:\n \\xA0 \\xA0 \\xA0name: hello-world-parallel-job\n \\xA0 \\xA0spec:\n \\xA0 \\xA0 \\xA0containers:\n \\xA0 \\xA0 \\xA0- name: hello-world-parallel\n \\xA0 \\xA0 \\xA0 \\xA0image: centos:7\n \\xA0 \\xA0 \\xA0 \\xA0command:\n \\xA0 \\xA0 \\xA0 \\xA0 - \"bin/bash\"\n \\xA0 \\xA0 \\xA0 \\xA0 - \"-c\"\n \\xA0 \\xA0 \\xA0 \\xA0 - \"echo hello world\"\n \\xA0 \\xA0 \\xA0restartPolicy: Never\n\n`})}),`\n`,(0,n.jsxs)(e.p,{children:[\"\\u200D\",(0,n.jsx)(e.strong,{children:\"Parallelism\"}),\": Notice the new configuration item, parallelism. The previous job executed pods one after another. However, we can configure a job to run pods in parallel using this new configuration.\\xA0\"]}),`\n`,(0,n.jsxs)(e.h4,{id:\"step-3-schedule-a-job\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#step-3-schedule-a-job\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Step 3: Schedule a Job\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"If you need to start jobs at a specific time in the future, or you want to run them in a repetitive pattern at specific intervals, you should consider using a \",(0,n.jsx)(e.strong,{children:\"CronJob\"}),\". A CronJob creates jobs that repeat using a schedule. You can schedule the job using the cron format and can set the schedule in the \",(0,n.jsx)(e.strong,{children:\"schedule\"}),\" object.\\xA0\"]}),`\n`,(0,n.jsx)(e.p,{children:\"The following example YAML file shows a CronJob:\\xA0\"}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-yaml\",children:`\napiVersion: batch/v1\nkind: CronJob\nmetadata:\n \\xA0name: hello-world-cron\nspec:\n \\xA0schedule: \"*/5 * * * *\"\n \\xA0jobTemplate:\n \\xA0 \\xA0spec:\n \\xA0 \\xA0 \\xA0template:\n \\xA0 \\xA0 \\xA0 \\xA0spec:\n \\xA0 \\xA0 \\xA0 \\xA0 \\xA0containers:\n \\xA0 \\xA0 \\xA0 \\xA0 \\xA0- name: hello-world\n \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0image: centos:7\n \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0imagePullPolicy: IfNotPresent\n \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0command:\n \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0- /bin/sh\n \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0- -c\n \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0- \"echo Hello World\"\n \\xA0 \\xA0 \\xA0 \\xA0 \\xA0restartPolicy: OnFailure\n\n`})}),`\n`,(0,n.jsx)(e.p,{children:'In this code, the cron schedule format is the string \"*/5 * * * *.\" It contains 5 sections (separated with white spaces), representing a minute, hour, day of the month, and day of the week in that order. \"*/5\" means the task will run every 5 minutes. To explain the schedule format further, if you change the schedule to \"0 */5 * * *\", the job will execute every 5 hours. Also, setting all 5 fields to \"*\" means a job will run every minute.\\xA0'}),`\n`,(0,n.jsx)(e.p,{children:\"To create the job on your machine, run the following command:\\xA0\"}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-yaml\",children:`kubectl create -f your-cronjob-yaml-file\n`})}),`\n`,(0,n.jsxs)(e.p,{children:[\"To see the cronjob you just created, simply run the \",(0,n.jsx)(e.strong,{children:\"kubectl create -f cronjob.yaml\"}),\" command.\\xA0\"]}),`\n`,(0,n.jsxs)(e.h4,{id:\"step-4-running-a-job\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#step-4-running-a-job\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Step 4: Running a Job\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"To run a job after creating the YAML file for it, simply run the \",(0,n.jsx)(e.strong,{children:\"kubectl apply -f [yaml-file]\"}),\" command. Replace [yaml-file] with the actual file name for your job configuration.\\xA0\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"You can verify the status of your job by running the \",(0,n.jsx)(e.strong,{children:\"kubectl get jobs\"}),\" command. For an even more detailed report, you can run \",(0,n.jsx)(e.strong,{children:\"kubectl describe job [job-name]\"}),\".\\xA0\"]}),`\n`,(0,n.jsxs)(e.h4,{id:\"step-5-deleting-a-job\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#step-5-deleting-a-job\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Step 5: Deleting a Job\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"For logging and tracking purposes, jobs and the pods they create do not get deleted even after they stop running. However, when you no longer need them, you can clean old jobs and their pods up. To do this you can use the \",(0,n.jsx)(e.strong,{children:\"kubectl delete jobs/[job-name]\"}),\" command. \\xA0\\xA0\"]}),`\n`,(0,n.jsxs)(e.h3,{id:\"summing-everything-up\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#summing-everything-up\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Summing Everything up\"]}),`\n`,(0,n.jsx)(e.p,{children:\"In this post, we've covered what Kubernetes Jobs are\\u2014resources that create pods that keep running until successful completion.\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:\"You also learned how to create, configure and run Kubernetes jobs. For jobs that need to run at a specific time or repetitively, you can use the CronJob Kubernetes resource.\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:\"Finally, you learned how to delete Kubernetes jobs after they complete. Since a record of jobs and their pods stay even after completion, if you no longer need that record, you can delete it by deleting the job.\"})]})}function k(o={}){let{wrapper:e}=o.components||{};return e?(0,n.jsx)(e,Object.assign({},o,{children:(0,n.jsx)(h,o)})):h(o)}var v=k;return y(N);})();\n;return Component;"
        },
        "_id": "blog/posts/kubernetes-jobs-how-to-create-schedule-run-and-more.mdx",
        "_raw": {
          "sourceFilePath": "blog/posts/kubernetes-jobs-how-to-create-schedule-run-and-more.mdx",
          "sourceFileName": "kubernetes-jobs-how-to-create-schedule-run-and-more.mdx",
          "sourceFileDir": "blog/posts",
          "contentType": "mdx",
          "flattenedPath": "blog/posts/kubernetes-jobs-how-to-create-schedule-run-and-more"
        },
        "type": "BlogPost",
        "computedSlug": "kubernetes-jobs-how-to-create-schedule-run-and-more"
      },
      "documentHash": "1739393595023",
      "hasWarnings": false,
      "documentTypeName": "BlogPost"
    },
    "blog/posts/kubernetes-namespaces-the-ultimate-guide.mdx": {
      "document": {
        "title": "Kubernetes Namespaces: The Ultimate Guide",
        "summary": "In this post we talk about kubernetes namespaces. We discuss what are they, how they work and what they're useful for",
        "publishDate": "Sun Jul 24 2022 23:38:21 GMT+0000 (Coordinated Universal Time)",
        "author": "ashley-penney",
        "readingTime": 7,
        "categories": [
          "kubernetes",
          "platform-engineering"
        ],
        "mainImage": "/blog-images/2266b02b751a19c98eb0df977859ad01.jpg",
        "imageAlt": "a laptop on a desk",
        "showCTA": true,
        "ctaCopy": "Experience seamless namespace isolation with Release.com's ephemeral environments for efficient Kubernetes resource management.",
        "ctaLink": "https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=kubernetes-namespaces-the-ultimate-guide",
        "relatedPosts": [
          ""
        ],
        "ogImage": "/blog-images/2266b02b751a19c98eb0df977859ad01.jpg",
        "excerpt": "In this post we talk about kubernetes namespaces. We discuss what are they, how they work and what they're useful for",
        "tags": [
          "kubernetes",
          "platform-engineering"
        ],
        "ctaButton": "Try Release for Free",
        "body": {
          "raw": "\nThe Kubernetes learning curve can be quite steep, but it's relatively easy to get started, especially today since many cloud providers offer easy-to-deploy, managed Kubernetes solutions. Therefore, you can run your application on a Kubernetes cluster in a few minutes. One of the things you'll probably discover after taking your first steps are Kubernetes namespaces. They're fundamental to Kubernetes, and you'll be using them extensively. But what are they actually, how do they work, and how are they useful? This post will give you the answers.\n\n![](/blog-images/32166a1642fb39bc4d1f7e023a0bd7d6.png)\n\n### What Are Kubernetes Namespaces?\n\nThe official Kubernetes documentation says that namespaces \"provide a mechanism for isolating groups of resources within a single cluster.\" The word \"grouping\" is key here. Namespaces are simple Kubernetes resources (just like deployments and pods) that can be used to create groups of resources. You can think of them as mini virtual clusters within your cluster. If you just started with Kubernetes and have been deploying things on it without specifying a namespace (we'll talk about how to do that later), you were still using a namespace. Every Kubernetes cluster comes with a few namespaces, one of which is called the \"default\". And that's where your deployment will end up if you don't specify otherwise.\n\n### How Are Kubernetes Namespaces Useful?\n\nNow that you know what Kubernetes namespaces are, let's talk about what they can be used for. The main purpose is isolation in multi-tenant environments. Imagine you have two separate teams using the same Kubernetes cluster. Both teams want to deploy, for example, an nginx server. Let's say both teams try to create a deployment as follows:\n\nOnly one team will succeed. The other will get an error message saying that an nginx deployment already exists. A simple solution for that could be to make the names of the deployments unique by, for example, adding a number or the name of the team to the deployment name.\n\n```yaml\nkubectl run nginx-teamA --image=nginx\nkubectl run nginx-teamB --image=nginx\n```\n\nThis would work, but it would be prone to errors, and it requires extra overhead. You won't be able to simply follow a tutorial on the internet about Kubernetes and copy and paste the commands from there because you'd have to adjust the names of resources every single time. Not great. That's exactly why namespaces exist. They isolate resources. This means you can have two nginx deployments running with the same name but in two separate namespaces. From each team's perspective, it would look like they have their own clusters. They won't have to worry about the other team's deployments.\n\n#### Keeping Your Cluster Tidy\n\nAnother good thing about using namespaces is that it's easier to keep your Kubernetes cluster clean. For example, if you're doing a lot of tests on your cluster or creating a lot of proof-of-concept deployments, it may be complicated to clean up afterward. If, for example, you have a hundred deployments running on your cluster, making sure that you delete the old ones used only for some tests can be not only time-consuming but even dangerous. Imagine deleting the production application by mistake when you just wanted to delete its test version. Namespaces can solve that problem too. Simply create a new namespace for each test or POC you want to run, and once you're done, you only need to delete that namespace. All resources within the namespace will be deleted automatically with it.\n\n#### Environment Separation\n\nSome people even use namespaces for separating environments. For example, you could have your application's development and production instances running on the same Kubernetes cluster separated by namespaces. It's cheaper and easier than running two separate clusters. However, it comes with its own disadvantages, so make sure you understand all the implications of doing so.\n\n![](/blog-images/29a9cf0aa0b92de29a291842d4df4661.png)\n\n#### Logical, Not Physical, Separation\n\nThere are many use cases for namespaces, and you can use them however you like as long as it makes sense for your organization. However, you should remember that namespaces provide only a logical and not a physical separation of resources. This means that isolation provided by namespaces is not the same as separation provided by separate clusters. In fact, you should think of namespaces more in terms of grouping capabilities than isolation.\n\nThis is especially important when using namespaces, for example, to separate environments for your application. You should know that your development instance can break your entire cluster, for example, due to too aggressive performance testing or even simple misconfiguration. So, if you run development and production instances in separate namespaces but on the same cluster, breaking the development environment could also bring your production instance down.\n\nYou also should be aware that there is no network isolation between namespaces by default. This means that pods in one namespace can freely talk to pods in any other namespace. This can be an advantage or disadvantage, depending on your needs. The good news is that it's relatively easy to implement namespace-based network isolation for your cluster. You just need to be aware that it doesn't happen automatically.\n\n### How Do Namespaces Work in Kubernetes?\n\nEnough of theory. Let's see namespaces in action. The basic usage of namespaces is the same as with any other objects in Kubernetes. It means you can execute commands like **kubectl create** or **kubectl get** for namespaces. Let's try that.\n\n#### kubectl get namespaces\n\nWe mentioned before that Kubernetes comes with a namespace called \"default\". In fact, there may be more namespaces on your brand-new cluster, depending on how it was deployed and which version of Kubernetes you're using. Let's validate that. To see the list of namespaces on your cluster, you can execute **kubectl get namespaces:**\n\n```yaml\n$ kubectl get namespaces\nNAME              STATUS   AGE\ndefault           Active   69s\nkube-system       Active   69s\nkube-public       Active   69s\nkube-node-lease   Active   69s\n```\n\nAs you can see, my cluster comes with four namespaces. Those named with prefix **kube-** are Kubernetes's own namespaces. **kube-system** holds Kubernetes components, so you definitely don't want to delete anything there. In fact, if you're a beginner, you should not touch any **kube-** prefixed namespaces. For you, there is the **default** namespace created. As mentioned before, this is the namespace where all your resources will be deployed if you don't specify otherwise. For example, if I execute **kubectl get pods** on an empty cluster, I'll see the following message:\n\n```yaml\n$ kubectl get pods\nNo resources found in default namespace.\n```\n\nFollowing the same logic, if I create a pod without specifying a namespace, it will be created in a default namespace:\n\n```yaml\n\n$ kubectl run nginx --image=nginx --restart=Never\npod/nginx created\n\n$ kubectl describe pod nginx\nName:         nginx\nNamespace:    default\n(...)\n\n```\n\nAs with many other Kubernetes objects, you can also use a shortcut, in this case **ns**, instead of typing **namespace** every time, so executing **kubectl get ns** will have the same effect as **kubectl get namespaces:**\n\n```yaml\n$ kubectl get ns\nNAME              STATUS        AGE\ndefault           Active        25m\nkube-system       Active        25m\nkube-public       Active        25m\nkube-node-lease   Active        25m\n```\n\n#### kubectl create namespace\n\nNow that you know how to see which namespaces you have in your cluster, let's add some more. For this, you can execute **kubectl create namespace** followed by the name of the desired namespace:\n\n```yaml\n$ kubectl create namespace frontend\nnamespace/frontend created\n```\n\nIf you execute **kubectl get namespaces** again, you should see your new namespace in the list now:\n\n```yaml\n$ kubectl get namespaces\nNAME              STATUS   AGE\ndefault           Active   14m\nkube-system       Active   14m\nkube-public       Active   14m\nkube-node-lease   Active   14m\nfrontend          Active   8s\n```\n\nNow that you have a new namespace, you can append **\\--namespace=frontend** to any other **kubectl** action to specify that you want to use that specific namespace. So, for example, to deploy your nginx deployment in that namespace, you can run the following:\n\n```yaml\n$ kubectl run nginx --image=nginx --restart=Never --namespace=frontend\npod/nginx created\n```\n\nIf you check the pods on your cluster now, you'll see no running pods.\n\n```yaml\n$ kubectl get pods\nNo resources found in default namespace.\n```\n\nWait, what? Well, read the message again. It says that there are no resources in the **default** namespace. This is expected since we just deployed nginx in the **frontend** namespace. Therefore, you need to append **\\--namespace=frontend** to the **kubectl get pods** command:\n\n```yaml\n$ kubectl get pods --namespace=frontend\nNAME    READY   STATUS    RESTARTS   AGE\nnginx   1/1     Running   0          3m7s\n```\n\nIf you get tired of adding the long **\\--namespace=frontend** parameter every time, you can also force your **kubectl** to use a specific namespace by default instead of the default \"default\" namespace. You can do that by executing the following:\n\n```yaml\nkubectl config set-context --current --namespace=frontend\n```\n\nYou just need to remember that all your commands will now be executed against the **frontend** namespace and not the **default** namespace.\n\n#### kubectl delete namespace\n\nDeleting a namespace is as straightforward as creating one. You can do it by executing the **kubectl delete namespace** followed by the name of the namespace you want to delete. Here's an example:\n\n```yaml\n$ kubectl get namespaces\nNAME STATUS AGE\ndefault Active 22m\nkube-system Active 22m\nkube-public Active 22m\nkube-node-lease Active 22m\nfrontend Active 8m16s\n\n$ kubectl delete namespace frontend\nnamespace \"frontend\" deleted\n\n$ kubectl get namespaces\nNAME STATUS AGE\ndefault Active 22m\nkube-system Active 22m\nkube-public Active 22m\nkube-node-lease Active 22m\n```\n\nKeep in mind that this will delete all resources within the namespace too. Your pods won't automatically move to another namespace. Also keep in mind that for the same reason, deleting the namespace can sometimes take a long time. If you have a lot of resources running in the namespace, Kubernetes will first try to delete all of them before deleting the namespace itself. For example, deleting pods can take some time, depending on their configuration.\n\n#### kubectl describe namespace\n\nAgain, as with any other Kubernetes object, you can execute **kubectl describe** for namespaces, which should give you more detailed information about the namespace. Unlike most resources, however, namespaces are simple objects, therefore there isn't usually much information:\n\n```yaml\n\n$ kubectl describe namespace default\nName:         default\nLabels:       kubernetes.io/metadata.name=default\nAnnotations:  \nStatus:       Active\nNo resource quota.\nNo resource limits.\n\n```\n\n### Namespaces in YAML\n\nIt's great to know how to use namespaces with **kubectl,** but in the real world, you'll probably manage all your Kubernetes resources with YAML files. You'll then need to specify which namespace to use in that YAML file. How do you do that? It's straightforward. Just add **namespace: \\[namespace_name\\]** in the **metadata** section of your Kubernetes definition file.\n\n```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n   name: nginx\n   namespace: frontend\n   labels:\n     name: nginx\nspec:\n   containers:\n   - name: nginx\n     image: nginx\n```\n\nYou can also create a namespace from a YAML file. It follows the same structure as your other Kubernetes YAML files, so you need to specify **apiVersion**, which in the case of namespaces is **v1**, then **kind**, which—you guessed it—is **namespace**. Then just define its name in the metadata section.\n\n```yaml\napiVersion: v1\nkind: Namespace\nmetadata:\n   name: backend\n```\n\nIt's the simplest Kubernetes file you've ever seen, isn't it? You can apply it like any other Kubernetes file using **kubectl apply -f namespace.yaml**.\n\n```yaml\n$ kubectl apply -f namespace.yaml\nnamespace/backend created\n```\n\n![](/blog-images/e2cbeea2c29032aa4a90c0b6ff3d428a.png)\n\n### Summary\n\nKubernetes namespaces are extremely useful. In this post, you learned what they are and how to use them. Now it's up to you and your company how to use them. Just remember that because namespaces can't be nested, it's also possible to overuse them. Namespaces should bring you grouping capabilities, making it easier for you to manage your resources. But if you create too many unnecessary namespaces, you won't make anything easier. But these are extreme cases, and namespaces are usually easy to get right.\n\nIf you want to learn more about Kubernetes, check out other articles on [our blog](https://release.com/blog).\n",
          "code": "var Component=(()=>{var d=Object.create;var o=Object.defineProperty;var h=Object.getOwnPropertyDescriptor;var m=Object.getOwnPropertyNames;var p=Object.getPrototypeOf,g=Object.prototype.hasOwnProperty;var y=(a,e)=>()=>(e||a((e={exports:{}}).exports,e),e.exports),b=(a,e)=>{for(var t in e)o(a,t,{get:e[t],enumerable:!0})},i=(a,e,t,r)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let s of m(e))!g.call(a,s)&&s!==t&&o(a,s,{get:()=>e[s],enumerable:!(r=h(e,s))||r.enumerable});return a};var f=(a,e,t)=>(t=a!=null?d(p(a)):{},i(e||!a||!a.__esModule?o(t,\"default\",{value:a,enumerable:!0}):t,a)),w=a=>i(o({},\"__esModule\",{value:!0}),a);var l=y((T,c)=>{c.exports=_jsx_runtime});var x={};b(x,{default:()=>N,frontmatter:()=>k});var n=f(l()),k={title:\"Kubernetes Namespaces: The Ultimate Guide\",summary:\"In this post we talk about kubernetes namespaces. We discuss what are they, how they work and what they're useful for\",publishDate:\"Sun Jul 24 2022 23:38:21 GMT+0000 (Coordinated Universal Time)\",author:\"ashley-penney\",readingTime:7,categories:[\"kubernetes\",\"platform-engineering\"],mainImage:\"/blog-images/2266b02b751a19c98eb0df977859ad01.jpg\",imageAlt:\"a laptop on a desk\",showCTA:!0,ctaCopy:\"Experience seamless namespace isolation with Release.com's ephemeral environments for efficient Kubernetes resource management.\",ctaLink:\"https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=kubernetes-namespaces-the-ultimate-guide\",relatedPosts:[\"\"],ogImage:\"/blog-images/2266b02b751a19c98eb0df977859ad01.jpg\",excerpt:\"In this post we talk about kubernetes namespaces. We discuss what are they, how they work and what they're useful for\",tags:[\"kubernetes\",\"platform-engineering\"],ctaButton:\"Try Release for Free\"};function u(a){let e=Object.assign({p:\"p\",img:\"img\",h3:\"h3\",a:\"a\",span:\"span\",pre:\"pre\",code:\"code\",h4:\"h4\",strong:\"strong\"},a.components);return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(e.p,{children:\"The Kubernetes learning curve can be quite steep, but it's relatively easy to get started, especially today since many cloud providers offer easy-to-deploy, managed Kubernetes solutions. Therefore, you can run your application on a Kubernetes cluster in a few minutes. One of the things you'll probably discover after taking your first steps are Kubernetes namespaces. They're fundamental to Kubernetes, and you'll be using them extensively. But what are they actually, how do they work, and how are they useful? This post will give you the answers.\"}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/32166a1642fb39bc4d1f7e023a0bd7d6.png\",alt:\"\"})}),`\n`,(0,n.jsxs)(e.h3,{id:\"what-are-kubernetes-namespaces\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#what-are-kubernetes-namespaces\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"What Are Kubernetes Namespaces?\"]}),`\n`,(0,n.jsx)(e.p,{children:`The official Kubernetes documentation says that namespaces \"provide a mechanism for isolating groups of resources within a single cluster.\" The word \"grouping\" is key here. Namespaces are simple Kubernetes resources (just like deployments and pods) that can be used to create groups of resources. You can think of them as mini virtual clusters within your cluster. If you just started with Kubernetes and have been deploying things on it without specifying a namespace (we'll talk about how to do that later), you were still using a namespace. Every Kubernetes cluster comes with a few namespaces, one of which is called the \"default\". And that's where your deployment will end up if you don't specify otherwise.`}),`\n`,(0,n.jsxs)(e.h3,{id:\"how-are-kubernetes-namespaces-useful\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#how-are-kubernetes-namespaces-useful\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"How Are Kubernetes Namespaces Useful?\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Now that you know what Kubernetes namespaces are, let's talk about what they can be used for. The main purpose is isolation in multi-tenant environments. Imagine you have two separate teams using the same Kubernetes cluster. Both teams want to deploy, for example, an nginx server. Let's say both teams try to create a deployment as follows:\"}),`\n`,(0,n.jsx)(e.p,{children:\"Only one team will succeed. The other will get an error message saying that an nginx deployment already exists. A simple solution for that could be to make the names of the deployments unique by, for example, adding a number or the name of the team to the deployment name.\"}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-yaml\",children:`kubectl run nginx-teamA --image=nginx\nkubectl run nginx-teamB --image=nginx\n`})}),`\n`,(0,n.jsx)(e.p,{children:\"This would work, but it would be prone to errors, and it requires extra overhead. You won't be able to simply follow a tutorial on the internet about Kubernetes and copy and paste the commands from there because you'd have to adjust the names of resources every single time. Not great. That's exactly why namespaces exist. They isolate resources. This means you can have two nginx deployments running with the same name but in two separate namespaces. From each team's perspective, it would look like they have their own clusters. They won't have to worry about the other team's deployments.\"}),`\n`,(0,n.jsxs)(e.h4,{id:\"keeping-your-cluster-tidy\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#keeping-your-cluster-tidy\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Keeping Your Cluster Tidy\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Another good thing about using namespaces is that it's easier to keep your Kubernetes cluster clean. For example, if you're doing a lot of tests on your cluster or creating a lot of proof-of-concept deployments, it may be complicated to clean up afterward. If, for example, you have a hundred deployments running on your cluster, making sure that you delete the old ones used only for some tests can be not only time-consuming but even dangerous. Imagine deleting the production application by mistake when you just wanted to delete its test version. Namespaces can solve that problem too. Simply create a new namespace for each test or POC you want to run, and once you're done, you only need to delete that namespace. All resources within the namespace will be deleted automatically with it.\"}),`\n`,(0,n.jsxs)(e.h4,{id:\"environment-separation\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#environment-separation\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Environment Separation\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Some people even use namespaces for separating environments. For example, you could have your application's development and production instances running on the same Kubernetes cluster separated by namespaces. It's cheaper and easier than running two separate clusters. However, it comes with its own disadvantages, so make sure you understand all the implications of doing so.\"}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/29a9cf0aa0b92de29a291842d4df4661.png\",alt:\"\"})}),`\n`,(0,n.jsxs)(e.h4,{id:\"logical-not-physical-separation\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#logical-not-physical-separation\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Logical, Not Physical, Separation\"]}),`\n`,(0,n.jsx)(e.p,{children:\"There are many use cases for namespaces, and you can use them however you like as long as it makes sense for your organization. However, you should remember that namespaces provide only a logical and not a physical separation of resources. This means that isolation provided by namespaces is not the same as separation provided by separate clusters. In fact, you should think of namespaces more in terms of grouping capabilities than isolation.\"}),`\n`,(0,n.jsx)(e.p,{children:\"This is especially important when using namespaces, for example, to separate environments for your application. You should know that your development instance can break your entire cluster, for example, due to too aggressive performance testing or even simple misconfiguration. So, if you run development and production instances in separate namespaces but on the same cluster, breaking the development environment could also bring your production instance down.\"}),`\n`,(0,n.jsx)(e.p,{children:\"You also should be aware that there is no network isolation between namespaces by default. This means that pods in one namespace can freely talk to pods in any other namespace. This can be an advantage or disadvantage, depending on your needs. The good news is that it's relatively easy to implement namespace-based network isolation for your cluster. You just need to be aware that it doesn't happen automatically.\"}),`\n`,(0,n.jsxs)(e.h3,{id:\"how-do-namespaces-work-in-kubernetes\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#how-do-namespaces-work-in-kubernetes\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"How Do Namespaces Work in Kubernetes?\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"Enough of theory. Let's see namespaces in action. The basic usage of namespaces is the same as with any other objects in Kubernetes. It means you can execute commands like \",(0,n.jsx)(e.strong,{children:\"kubectl create\"}),\" or \",(0,n.jsx)(e.strong,{children:\"kubectl get\"}),\" for namespaces. Let's try that.\"]}),`\n`,(0,n.jsxs)(e.h4,{id:\"kubectl-get-namespaces\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#kubectl-get-namespaces\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"kubectl get namespaces\"]}),`\n`,(0,n.jsxs)(e.p,{children:[`We mentioned before that Kubernetes comes with a namespace called \"default\". In fact, there may be more namespaces on your brand-new cluster, depending on how it was deployed and which version of Kubernetes you're using. Let's validate that. To see the list of namespaces on your cluster, you can execute `,(0,n.jsx)(e.strong,{children:\"kubectl get namespaces:\"})]}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-yaml\",children:`$ kubectl get namespaces\nNAME \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0STATUS \\xA0 AGE\ndefault \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 Active \\xA0 69s\nkube-system \\xA0 \\xA0 \\xA0 Active \\xA0 69s\nkube-public \\xA0 \\xA0 \\xA0 Active \\xA0 69s\nkube-node-lease \\xA0 Active \\xA0 69s\n`})}),`\n`,(0,n.jsxs)(e.p,{children:[\"As you can see, my cluster comes with four namespaces. Those named with prefix \",(0,n.jsx)(e.strong,{children:\"kube-\"}),\" are Kubernetes's own namespaces. \",(0,n.jsx)(e.strong,{children:\"kube-system\"}),\" holds Kubernetes components, so you definitely don't want to delete anything there. In fact, if you're a beginner, you should not touch any \",(0,n.jsx)(e.strong,{children:\"kube-\"}),\" prefixed namespaces. For you, there is the \",(0,n.jsx)(e.strong,{children:\"default\"}),\" namespace created. As mentioned before, this is the namespace where all your resources will be deployed if you don't specify otherwise. For example, if I execute \",(0,n.jsx)(e.strong,{children:\"kubectl get pods\"}),\" on an empty cluster, I'll see the following message:\"]}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-yaml\",children:`$ kubectl get pods\nNo resources found in default namespace.\n`})}),`\n`,(0,n.jsx)(e.p,{children:\"Following the same logic, if I create a pod without specifying a namespace, it will be created in a default namespace:\"}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-yaml\",children:`\n$ kubectl run nginx --image=nginx --restart=Never\npod/nginx created\n\n$ kubectl describe pod nginx\nName: \\xA0 \\xA0 \\xA0 \\xA0 nginx\nNamespace: \\xA0 \\xA0default\n(...)\n\n`})}),`\n`,(0,n.jsxs)(e.p,{children:[\"As with many other Kubernetes objects, you can also use a shortcut, in this case \",(0,n.jsx)(e.strong,{children:\"ns\"}),\", instead of typing \",(0,n.jsx)(e.strong,{children:\"namespace\"}),\" every time, so executing \",(0,n.jsx)(e.strong,{children:\"kubectl get ns\"}),\" will have the same effect as \",(0,n.jsx)(e.strong,{children:\"kubectl get namespaces:\"})]}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-yaml\",children:`$ kubectl get ns\nNAME \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0STATUS \\xA0 \\xA0 \\xA0 \\xA0AGE\ndefault \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 Active \\xA0 \\xA0 \\xA0 \\xA025m\nkube-system \\xA0 \\xA0 \\xA0 Active \\xA0 \\xA0 \\xA0 \\xA025m\nkube-public \\xA0 \\xA0 \\xA0 Active \\xA0 \\xA0 \\xA0 \\xA025m\nkube-node-lease \\xA0 Active \\xA0 \\xA0 \\xA0 \\xA025m\n`})}),`\n`,(0,n.jsxs)(e.h4,{id:\"kubectl-create-namespace\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#kubectl-create-namespace\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"kubectl create namespace\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"Now that you know how to see which namespaces you have in your cluster, let's add some more. For this, you can execute \",(0,n.jsx)(e.strong,{children:\"kubectl create namespace\"}),\" followed by the name of the desired namespace:\"]}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-yaml\",children:`$ kubectl create namespace frontend\nnamespace/frontend created\n`})}),`\n`,(0,n.jsxs)(e.p,{children:[\"If you execute \",(0,n.jsx)(e.strong,{children:\"kubectl get namespaces\"}),\" again, you should see your new namespace in the list now:\"]}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-yaml\",children:`$ kubectl get namespaces\nNAME \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0STATUS \\xA0 AGE\ndefault \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 Active \\xA0 14m\nkube-system \\xA0 \\xA0 \\xA0 Active \\xA0 14m\nkube-public \\xA0 \\xA0 \\xA0 Active \\xA0 14m\nkube-node-lease \\xA0 Active \\xA0 14m\nfrontend \\xA0 \\xA0 \\xA0 \\xA0 \\xA0Active \\xA0 8s\n`})}),`\n`,(0,n.jsxs)(e.p,{children:[\"Now that you have a new namespace, you can append \",(0,n.jsx)(e.strong,{children:\"--namespace=frontend\"}),\" to any other \",(0,n.jsx)(e.strong,{children:\"kubectl\"}),\" action to specify that you want to use that specific namespace. So, for example, to deploy your nginx deployment in that namespace, you can run the following:\"]}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-yaml\",children:`$ kubectl run nginx --image=nginx --restart=Never --namespace=frontend\npod/nginx created\n`})}),`\n`,(0,n.jsx)(e.p,{children:\"If you check the pods on your cluster now, you'll see no running pods.\"}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-yaml\",children:`$ kubectl get pods\nNo resources found in default namespace.\n`})}),`\n`,(0,n.jsxs)(e.p,{children:[\"Wait, what? Well, read the message again. It says that there are no resources in the \",(0,n.jsx)(e.strong,{children:\"default\"}),\" namespace. This is expected since we just deployed nginx in the \",(0,n.jsx)(e.strong,{children:\"frontend\"}),\" namespace. Therefore, you need to append \",(0,n.jsx)(e.strong,{children:\"--namespace=frontend\"}),\" to the \",(0,n.jsx)(e.strong,{children:\"kubectl get pods\"}),\" command:\"]}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-yaml\",children:`$ kubectl get pods --namespace=frontend\nNAME \\xA0 \\xA0READY \\xA0 STATUS \\xA0 \\xA0RESTARTS \\xA0 AGE\nnginx \\xA0 1/1 \\xA0 \\xA0 Running \\xA0 0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA03m7s\n`})}),`\n`,(0,n.jsxs)(e.p,{children:[\"If you get tired of adding the long \",(0,n.jsx)(e.strong,{children:\"--namespace=frontend\"}),\" parameter every time, you can also force your \",(0,n.jsx)(e.strong,{children:\"kubectl\"}),' to use a specific namespace by default instead of the default \"default\" namespace. You can do that by executing the following:']}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-yaml\",children:`kubectl config set-context --current --namespace=frontend\n`})}),`\n`,(0,n.jsxs)(e.p,{children:[\"You just need to remember that all your commands will now be executed against the \",(0,n.jsx)(e.strong,{children:\"frontend\"}),\" namespace and not the \",(0,n.jsx)(e.strong,{children:\"default\"}),\" namespace.\"]}),`\n`,(0,n.jsxs)(e.h4,{id:\"kubectl-delete-namespace\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#kubectl-delete-namespace\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"kubectl delete namespace\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"Deleting a namespace is as straightforward as creating one. You can do it by executing the \",(0,n.jsx)(e.strong,{children:\"kubectl delete namespace\"}),\" followed by the name of the namespace you want to delete. Here's an example:\"]}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-yaml\",children:`$ kubectl get namespaces\nNAME STATUS AGE\ndefault Active 22m\nkube-system Active 22m\nkube-public Active 22m\nkube-node-lease Active 22m\nfrontend Active 8m16s\n\n$ kubectl delete namespace frontend\nnamespace \"frontend\" deleted\n\n$ kubectl get namespaces\nNAME STATUS AGE\ndefault Active 22m\nkube-system Active 22m\nkube-public Active 22m\nkube-node-lease Active 22m\n`})}),`\n`,(0,n.jsx)(e.p,{children:\"Keep in mind that this will delete all resources within the namespace too. Your pods won't automatically move to another namespace. Also keep in mind that for the same reason, deleting the namespace can sometimes take a long time. If you have a lot of resources running in the namespace, Kubernetes will first try to delete all of them before deleting the namespace itself. For example, deleting pods can take some time, depending on their configuration.\"}),`\n`,(0,n.jsxs)(e.h4,{id:\"kubectl-describe-namespace\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#kubectl-describe-namespace\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"kubectl describe namespace\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"Again, as with any other Kubernetes object, you can execute \",(0,n.jsx)(e.strong,{children:\"kubectl describe\"}),\" for namespaces, which should give you more detailed information about the namespace. Unlike most resources, however, namespaces are simple objects, therefore there isn't usually much information:\"]}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-yaml\",children:`\n$ kubectl describe namespace default\nName: \\xA0 \\xA0 \\xA0 \\xA0 default\nLabels: \\xA0 \\xA0 \\xA0 kubernetes.io/metadata.name=default\nAnnotations: \\xA0\nStatus: \\xA0 \\xA0 \\xA0 Active\nNo resource quota.\nNo resource limits.\n\n`})}),`\n`,(0,n.jsxs)(e.h3,{id:\"namespaces-in-yaml\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#namespaces-in-yaml\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Namespaces in YAML\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"It's great to know how to use namespaces with \",(0,n.jsx)(e.strong,{children:\"kubectl,\"}),\" but in the real world, you'll probably manage all your Kubernetes resources with YAML files. You'll then need to specify which namespace to use in that YAML file. How do you do that? It's straightforward. Just add \",(0,n.jsx)(e.strong,{children:\"namespace: [namespace_name]\"}),\" in the \",(0,n.jsx)(e.strong,{children:\"metadata\"}),\" section of your Kubernetes definition file.\"]}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-yaml\",children:`apiVersion: v1\nkind: Pod\nmetadata:\n  \\xA0name: nginx\n  \\xA0namespace: frontend\n  \\xA0labels:\n  \\xA0 \\xA0name: nginx\nspec:\n  \\xA0containers:\n  \\xA0- name: nginx\n  \\xA0 \\xA0image: nginx\n`})}),`\n`,(0,n.jsxs)(e.p,{children:[\"You can also create a namespace from a YAML file. It follows the same structure as your other Kubernetes YAML files, so you need to specify \",(0,n.jsx)(e.strong,{children:\"apiVersion\"}),\", which in the case of namespaces is \",(0,n.jsx)(e.strong,{children:\"v1\"}),\", then \",(0,n.jsx)(e.strong,{children:\"kind\"}),\", which\\u2014you guessed it\\u2014is \",(0,n.jsx)(e.strong,{children:\"namespace\"}),\". Then just define its name in the metadata section.\"]}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-yaml\",children:`apiVersion: v1\nkind: Namespace\nmetadata:\n  \\xA0name: backend\n`})}),`\n`,(0,n.jsxs)(e.p,{children:[\"It's the simplest Kubernetes file you've ever seen, isn't it? You can apply it like any other Kubernetes file using \",(0,n.jsx)(e.strong,{children:\"kubectl apply -f namespace.yaml\"}),\".\"]}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-yaml\",children:`$ kubectl apply -f namespace.yaml\nnamespace/backend created\n`})}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/e2cbeea2c29032aa4a90c0b6ff3d428a.png\",alt:\"\"})}),`\n`,(0,n.jsxs)(e.h3,{id:\"summary\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#summary\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Summary\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Kubernetes namespaces are extremely useful. In this post, you learned what they are and how to use them. Now it's up to you and your company how to use them. Just remember that because namespaces can't be nested, it's also possible to overuse them. Namespaces should bring you grouping capabilities, making it easier for you to manage your resources. But if you create too many unnecessary namespaces, you won't make anything easier. But these are extreme cases, and namespaces are usually easy to get right.\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"If you want to learn more about Kubernetes, check out other articles on \",(0,n.jsx)(e.a,{href:\"https://release.com/blog\",children:\"our blog\"}),\".\"]})]})}function v(a={}){let{wrapper:e}=a.components||{};return e?(0,n.jsx)(e,Object.assign({},a,{children:(0,n.jsx)(u,a)})):u(a)}var N=v;return w(x);})();\n;return Component;"
        },
        "_id": "blog/posts/kubernetes-namespaces-the-ultimate-guide.mdx",
        "_raw": {
          "sourceFilePath": "blog/posts/kubernetes-namespaces-the-ultimate-guide.mdx",
          "sourceFileName": "kubernetes-namespaces-the-ultimate-guide.mdx",
          "sourceFileDir": "blog/posts",
          "contentType": "mdx",
          "flattenedPath": "blog/posts/kubernetes-namespaces-the-ultimate-guide"
        },
        "type": "BlogPost",
        "computedSlug": "kubernetes-namespaces-the-ultimate-guide"
      },
      "documentHash": "1739393595023",
      "hasWarnings": false,
      "documentTypeName": "BlogPost"
    },
    "blog/posts/kubernetes-pod-a-beginners-guide-to-an-essential-resource.mdx": {
      "document": {
        "title": "Kubernetes Pod: A Beginner's Guide to an Essential Resource",
        "summary": "Pods are essential to any Kubernetes cluster. Learn everything about one of the most important Kubernetes resources.",
        "publishDate": "Wed Aug 31 2022 16:05:07 GMT+0000 (Coordinated Universal Time)",
        "author": "regis-wilson",
        "readingTime": 4,
        "categories": [
          "kubernetes",
          "platform-engineering"
        ],
        "mainImage": "/blog-images/2266b02b751a19c98eb0df977859ad01.jpg",
        "imageAlt": "a laptop on a desk",
        "showCTA": true,
        "ctaCopy": "Looking to manage Kubernetes pods effortlessly? Try Release.com for streamlined environment creation and faster deployment cycles.",
        "ctaLink": "https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=kubernetes-pod-a-beginners-guide-to-an-essential-resource",
        "relatedPosts": [
          ""
        ],
        "ogImage": "/blog-images/2266b02b751a19c98eb0df977859ad01.jpg",
        "excerpt": "Pods are essential to any Kubernetes cluster. Learn everything about one of the most important Kubernetes resources.",
        "tags": [
          "kubernetes",
          "platform-engineering"
        ],
        "ctaButton": "Try Release for Free",
        "body": {
          "raw": "\nKubernetes is a complex tool, but taking your first steps is relatively easy. This is especially true today when all major cloud providers offer easy one-click creation of Kubernetes clusters; you can have a fully working Kubernetes cluster in a matter of minutes. So, what do you do then? You'll probably deploy some pods. Pods are arguably the most important Kubernetes resources. You may have heard about them already, since deploying pods is usually one of the first things in any Kubernetes tutorial. You may have even heard \"they're kind of like containers.\" In this post, you'll learn everything you need to know about pods. \n\n![A picture containing vegetable, green, pea, edible-pod peaDescription automatically generated](/blog-images/143080a00967445b7f95074d3051b41d.jpeg)\n\n### Kubernetes Pods 101\n\nBefore Kubernetes, everyone was talking about containers. When you wanted to deploy only one small microservice, you'd say that you needed to deploy \"one container.\" On Kubernetes, everyone talks about pods instead. So, when you only want to deploy one microservice, you'll say that you need to deploy one pod. \n\nAre pods the same as containers, then? Well, not really. A pod is the smallest deployable unit in a Kubernetes world. This means that you can't directly deploy a single container in Kubernetes. If you want one container running, you need to package it into a pod and deploy one pod. A pod can also contain more than one container. It's basically like a box for containers. \n\nLong story short: if you mainly deploy single containers, there isn't much difference between a pod and a container. Technically, a pod encapsulates your container, but in general you can treat it similarly to a container. But pods' ability to contain more than one container is what opens doors of possibilities. We'll dive into that later in this post. But before that, let's talk about pod lifecycles. \n\n![Graphical user interface, text, applicationDescription automatically generated](/blog-images/c49d794341cb6fa950f9c4407002f7bd.png)\n\n### Pod Lifecycles\n\nJust like many other resources Kubernetes pods can be in a pending, running, or succeeded/failed state. You can check the status of your pod by executing **kubectl describe pod \\[your_pod_name\\]:** \n\n```yaml\n\n$ kubectl describe pod nginx-deployment-6595874d85-hnjzw\nName:           nginx-deployment-6595874d85-hnjzw\nNamespace:      default\nPriority:       0\nNode:           k3s-worker3/10.133.106.222\nStart Time:     Sun, 21 Aug 2022 12:24:58 +0200\nLabels:         app=nginx\n                pod-template-hash=6595874d85\nAnnotations:    \nStatus:         Pending\n(...)\n\n```\n\nAs you can see from the snippet above, my pod is in a **Pending** state. So, what do these states mean? \n\n### Pending\n\nPending, as the name suggests, means that the pod is waiting for something. Usually, it means that Kubernetes is trying to determine where to deploy that pod. So, in normal circumstances, you'll see your pod in the pending state for the first few seconds after creation. But it may also stay in a pending state longer if, for example, all your nodes are full and Kubernetes can't find a suitable node for your pending pod. In such a case, your pod will stay in a pending state until some other pods finish and free up resources or until you add another node to your cluster. \n\n### Running\n\nRunning is pretty straightforward: It's when everything is working correctly and your pod is active. There is a small caveat to this, though. If your pod consists of multiple containers, then your pod will be in the status \"running\" if at least one of its primary containers starts successfully. This means there's a chance that your pod will be in a running state even though not all containers are actually running. So, in the case of multiple containers, it's always best to double-check individual container states to be sure. \n\n### Succeeded/Failed\n\nSucceeded or failed is what comes after running. As you can imagine, you'll see \"succeeded\" when your pod did its job and finished as expected, and you'll see \"failed\" when your pod terminated due to some error. And again, in the case of multiple containers in one pod, you need to be aware that your pod will end up in a failed state if at least one of the containers ends up having issues. \n\n### Unknown\n\nThe other phase a pod can be in is called \"unknown,\" and you probably won't see it often. A pod will be in a state unknown when Kubernetes literally doesn't know what's happening with the pod. This is usually due to networking issues between the Kubernetes control plane and the node on which the pod suppose to run.   \n\n### What Are Pods Used for?\n\nNow, the big question: What are pods actually used for? The simple answer would be \"to run your application.\" At the end of the day, the point of running Kubernetes is to run containerized applications on it. And pods are the actual resources that make it possible. They encapsulate your containerized application and allow you to run it on your Kubernetes cluster. \n\n![ApplicationDescription automatically generated with medium confidence](/blog-images/05e25541f21a570d7cd3d31bfb7edba2.png)\n\nHowever, it's worth mentioning that usually you won't actually be deploying pods themselves. You'll be using other, higher-level Kubernetes resources like [Deployments](https://kubernetes.io/docs/concepts/workloads/controllers/deployment/) or [DaemonSets](https://release.com/blog/kubernetes-daemonset-tutorial) that will create pods for you. \n\n### Pods vs. Other Resources\n\nPods are only one of many Kubernetes resource types. Most other types are directly or indirectly related to pods, because as we already said, pods are resources that will actually be running your application on the cluster. Therefore, pretty much anything that your application may need—be it a secret or storage or a load balancer—will all need to somehow relate or connect to a pod. \n\nKubernetes secrets can be consumed by pods. Kubernetes service resources used to expose a containerized application on your cluster to the network or internet need to reference a pod. Volumes in Kubernetes are mounted to pods. Kubernetes ConfigMaps used to store configuration files are loaded to pods. These are just a few examples, but in general, pods are usually at the center of everything that's happening on Kubernetes. \n\n![A picture containing pea, vegetable, edible-pod peaDescription automatically generated](/blog-images/ca65057279804f39a7884105ed6dfba7.jpeg)\n\n### How to Create A Pod\n\nI'll show you how to create a pod, but be aware that normally you wouldn't create pods directly. You should use higher-level resources like [Deployments](https://kubernetes.io/docs/concepts/workloads/controllers/deployment/) that will take care of creating pods for you. But if you ever need it for testing or learning purposes, you can create a pod with the following YAML definition: \n\n```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n   name: nginx-pod-example\nspec:\n   containers:\n     - name: nginx\n       image: nginx\n```\n\nYou can apply it just like any other Kubernetes YAML definition, using **kubectl apply -f:** \n\n```yaml\n$ kubectl apply -f pod.yaml\npod/nginx-pod-example created\n\n$ kubectl get pod nginx-pod-example\nNAME                READY   STATUS    RESTARTS   AGE\nnginx-pod-example   1/1     Running   0          6s\n```\n\n### Pods With Multiple Containers\n\nWe mentioned pods with multiple containers already, so let's dive into that a bit more. The first thing for you to know is that pods' ability to run multiple containers is not something you should overuse. For example, it's not meant to be used to combine front-end and back-end microservices into one pod. Quite the opposite; you actually shouldn't combine multiple functional microservices into one pod. \n\nWhy does Kubernetes give you that option then? Well, it's for a different purpose. Putting more than one container into a single pod is useful for adding containers that are like assistants or helpers to your main container. A common example is log gathering containers. Their only job is to read logs from your main container and forward it (usually to some centralized log management solution). Another example is secret management containers. Their job is to securely load secrets from some secret vault and securely pass it to your main container. \n\nAs you can see, multiple containers in a pod are typically used in the main container + secondary containers configuration. We call these secondary containers \"sidecar containers.\" \n\nOf course, even though it's not usually recommended, there's nothing stopping you from combining two containers into one pod. If you have a very specific use case and you think it would make sense in your case, you can add more containers to your pod. You just need to be aware of the consequences of such an approach. The main one is that, in the case of the failure of the pod, both containers will die. \n\n### Summary\n\nAs you can see, pods are pretty straightforward resources. In most cases, you can treat them the same as containers, but they do offer extra sidecar functionality when necessary. \n\nLearned all you need to for pod basics? Read on to our advanced pod concepts article [here](https://release.com/blog/kubernetes-pods-advanced-concepts-explained)!\n",
          "code": "var Component=(()=>{var u=Object.create;var s=Object.defineProperty;var h=Object.getOwnPropertyDescriptor;var p=Object.getOwnPropertyNames;var m=Object.getPrototypeOf,y=Object.prototype.hasOwnProperty;var g=(o,e)=>()=>(e||o((e={exports:{}}).exports,e),e.exports),f=(o,e)=>{for(var t in e)s(o,t,{get:e[t],enumerable:!0})},r=(o,e,t,i)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let a of p(e))!y.call(o,a)&&a!==t&&s(o,a,{get:()=>e[a],enumerable:!(i=h(e,a))||i.enumerable});return o};var b=(o,e,t)=>(t=o!=null?u(m(o)):{},r(e||!o||!o.__esModule?s(t,\"default\",{value:o,enumerable:!0}):t,o)),w=o=>r(s({},\"__esModule\",{value:!0}),o);var c=g((N,l)=>{l.exports=_jsx_runtime});var K={};f(K,{default:()=>x,frontmatter:()=>k});var n=b(c()),k={title:\"Kubernetes Pod: A Beginner's Guide to an Essential Resource\",summary:\"Pods are essential to any Kubernetes cluster. Learn everything about one of the most important Kubernetes resources.\",publishDate:\"Wed Aug 31 2022 16:05:07 GMT+0000 (Coordinated Universal Time)\",author:\"regis-wilson\",readingTime:4,categories:[\"kubernetes\",\"platform-engineering\"],mainImage:\"/blog-images/2266b02b751a19c98eb0df977859ad01.jpg\",imageAlt:\"a laptop on a desk\",showCTA:!0,ctaCopy:\"Looking to manage Kubernetes pods effortlessly? Try Release.com for streamlined environment creation and faster deployment cycles.\",ctaLink:\"https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=kubernetes-pod-a-beginners-guide-to-an-essential-resource\",relatedPosts:[\"\"],ogImage:\"/blog-images/2266b02b751a19c98eb0df977859ad01.jpg\",excerpt:\"Pods are essential to any Kubernetes cluster. Learn everything about one of the most important Kubernetes resources.\",tags:[\"kubernetes\",\"platform-engineering\"],ctaButton:\"Try Release for Free\"};function d(o){let e=Object.assign({p:\"p\",img:\"img\",h3:\"h3\",a:\"a\",span:\"span\",strong:\"strong\",pre:\"pre\",code:\"code\"},o.components);return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(e.p,{children:`Kubernetes is a complex tool, but taking your first steps is relatively easy. This is especially true today when all major cloud providers offer easy one-click creation of Kubernetes clusters; you can have a fully working Kubernetes cluster in a matter of minutes. So, what do you do then? You'll probably deploy some pods. Pods are arguably the most important Kubernetes resources. You may have heard about them already, since deploying pods is usually one of the first things in any Kubernetes tutorial. You may have even heard \"they're kind of like containers.\" In this post, you'll learn everything you need to know about pods.\\xA0`}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/143080a00967445b7f95074d3051b41d.jpeg\",alt:\"A picture containing vegetable, green, pea, edible-pod peaDescription automatically generated\"})}),`\n`,(0,n.jsxs)(e.h3,{id:\"kubernetes-pods-101\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#kubernetes-pods-101\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Kubernetes Pods 101\"]}),`\n`,(0,n.jsx)(e.p,{children:`Before Kubernetes, everyone was talking about containers. When you wanted to deploy only one small microservice, you'd say that you needed to deploy \"one container.\" On Kubernetes, everyone talks about pods instead. So, when you only want to deploy one microservice, you'll say that you need to deploy one pod.\\xA0`}),`\n`,(0,n.jsx)(e.p,{children:\"Are pods the same as containers, then? Well, not really. A pod is the smallest deployable unit in a Kubernetes world. This means that you can't directly deploy a single container in Kubernetes. If you want one container running, you need to package it into a pod and deploy one pod. A pod can also contain more than one container. It's basically like a box for containers.\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:\"Long story short: if you mainly deploy single containers, there isn't much difference between a pod and a container. Technically, a pod encapsulates your container, but in general you can treat it similarly to a container. But pods' ability to contain more than one container is what opens doors of possibilities. We'll dive into that later in this post. But before that, let's talk about pod lifecycles.\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/c49d794341cb6fa950f9c4407002f7bd.png\",alt:\"Graphical user interface, text, applicationDescription automatically generated\"})}),`\n`,(0,n.jsxs)(e.h3,{id:\"pod-lifecycles\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#pod-lifecycles\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Pod Lifecycles\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"Just like many other resources Kubernetes pods can be in a pending, running, or succeeded/failed state. You can check the status of your pod by executing \",(0,n.jsx)(e.strong,{children:\"kubectl describe pod [your_pod_name]:\"}),\"\\xA0\"]}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-yaml\",children:`\n$ kubectl describe pod nginx-deployment-6595874d85-hnjzw\nName: \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 nginx-deployment-6595874d85-hnjzw\nNamespace: \\xA0 \\xA0 \\xA0default\nPriority: \\xA0 \\xA0 \\xA0 0\nNode: \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 k3s-worker3/10.133.106.222\nStart Time: \\xA0 \\xA0 Sun, 21 Aug 2022 12:24:58 +0200\nLabels: \\xA0 \\xA0 \\xA0 \\xA0 app=nginx\n \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0pod-template-hash=6595874d85\nAnnotations: \\xA0 \\xA0\nStatus: \\xA0 \\xA0 \\xA0 \\xA0 Pending\n(...)\n\n`})}),`\n`,(0,n.jsxs)(e.p,{children:[\"As you can see from the snippet above, my pod is in a \",(0,n.jsx)(e.strong,{children:\"Pending\"}),\" state. So, what do these states mean?\\xA0\"]}),`\n`,(0,n.jsxs)(e.h3,{id:\"pending\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#pending\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Pending\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Pending, as the name suggests, means that the pod is waiting for something. Usually, it means that Kubernetes is trying to determine where to deploy that pod. So, in normal circumstances, you'll see your pod in the pending state for the first few seconds after creation. But it may also stay in a pending state longer if, for example, all your nodes are full and Kubernetes can't find a suitable node for your pending pod. In such a case, your pod will stay in a pending state until some other pods finish and free up resources or until you add another node to your cluster.\\xA0\"}),`\n`,(0,n.jsxs)(e.h3,{id:\"running\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#running\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Running\"]}),`\n`,(0,n.jsx)(e.p,{children:`Running is pretty straightforward: It's when everything is working correctly and your pod is active. There is a small caveat to this, though. If your pod consists of multiple containers, then your pod will be in the status \"running\" if at least one of its primary containers starts successfully. This means there's a chance that your pod will be in a running state even though not all containers are actually running. So, in the case of multiple containers, it's always best to double-check individual container states to be sure.\\xA0`}),`\n`,(0,n.jsxs)(e.h3,{id:\"succeededfailed\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#succeededfailed\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Succeeded/Failed\"]}),`\n`,(0,n.jsx)(e.p,{children:`Succeeded or failed is what comes after running. As you can imagine, you'll see \"succeeded\" when your pod did its job and finished as expected, and you'll see \"failed\" when your pod terminated due to some error. And again, in the case of multiple containers in one pod, you need to be aware that your pod will end up in a failed state if at least one of the containers ends up having issues.\\xA0`}),`\n`,(0,n.jsxs)(e.h3,{id:\"unknown\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#unknown\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Unknown\"]}),`\n`,(0,n.jsx)(e.p,{children:`The other phase a pod can be in is called \"unknown,\" and you probably won't see it often. A pod will be in a state unknown when Kubernetes literally doesn't know what's happening with the pod. This is usually due to networking issues between the Kubernetes control plane and the node on which the pod suppose to run. \\xA0\\xA0`}),`\n`,(0,n.jsxs)(e.h3,{id:\"what-are-pods-used-for\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#what-are-pods-used-for\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"What Are Pods Used for?\"]}),`\n`,(0,n.jsx)(e.p,{children:'Now, the big question: What are pods actually used for? The simple answer would be \"to run your application.\" At the end of the day, the point of running Kubernetes is to run containerized applications on it. And pods are the actual resources that make it possible. They encapsulate your containerized application and allow you to run it on your Kubernetes cluster.\\xA0'}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/05e25541f21a570d7cd3d31bfb7edba2.png\",alt:\"ApplicationDescription automatically generated with medium confidence\"})}),`\n`,(0,n.jsxs)(e.p,{children:[\"However, it's worth mentioning that usually you won't actually be deploying pods themselves. You'll be using other, higher-level Kubernetes resources like \",(0,n.jsx)(e.a,{href:\"https://kubernetes.io/docs/concepts/workloads/controllers/deployment/\",children:\"Deployments\"}),\" or \",(0,n.jsx)(e.a,{href:\"https://release.com/blog/kubernetes-daemonset-tutorial\",children:\"DaemonSets\"}),\" that will create pods for you.\\xA0\"]}),`\n`,(0,n.jsxs)(e.h3,{id:\"pods-vs-other-resources\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#pods-vs-other-resources\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Pods vs. Other Resources\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Pods are only one of many Kubernetes resource types. Most other types are directly or indirectly related to pods, because as we already said, pods are resources that will actually be running your application on the cluster. Therefore, pretty much anything that your application may need\\u2014be it a secret or storage or a load balancer\\u2014will all need to somehow relate or connect to a pod.\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:\"Kubernetes secrets can be consumed by pods. Kubernetes service resources used to expose a containerized application on your cluster to the network or internet need to reference a pod. Volumes in Kubernetes are mounted to pods. Kubernetes ConfigMaps used to store configuration files are loaded to pods. These are just a few examples, but in general, pods are usually at the center of everything that's happening on Kubernetes.\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/ca65057279804f39a7884105ed6dfba7.jpeg\",alt:\"A picture containing pea, vegetable, edible-pod peaDescription automatically generated\"})}),`\n`,(0,n.jsxs)(e.h3,{id:\"how-to-create-a-pod\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#how-to-create-a-pod\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"How to Create A Pod\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"I'll show you how to create a pod, but be aware that normally you wouldn't create pods directly. You should use higher-level resources like \",(0,n.jsx)(e.a,{href:\"https://kubernetes.io/docs/concepts/workloads/controllers/deployment/\",children:\"Deployments\"}),\" that will take care of creating pods for you. But if you ever need it for testing or learning purposes, you can create a pod with the following YAML definition:\\xA0\"]}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-yaml\",children:`apiVersion: v1\nkind: Pod\nmetadata:\n  \\xA0name: nginx-pod-example\nspec:\n  \\xA0containers:\n  \\xA0 \\xA0- name: nginx\n  \\xA0 \\xA0 \\xA0image: nginx\n`})}),`\n`,(0,n.jsxs)(e.p,{children:[\"You can apply it just like any other Kubernetes YAML definition, using \",(0,n.jsx)(e.strong,{children:\"kubectl apply -f:\"}),\"\\xA0\"]}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-yaml\",children:`$ kubectl apply -f pod.yaml\npod/nginx-pod-example created\n\n$ kubectl get pod nginx-pod-example\nNAME \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0READY \\xA0 STATUS \\xA0 \\xA0RESTARTS \\xA0 AGE\nnginx-pod-example \\xA0 1/1 \\xA0 \\xA0 Running \\xA0 0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA06s\n`})}),`\n`,(0,n.jsxs)(e.h3,{id:\"pods-with-multiple-containers\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#pods-with-multiple-containers\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Pods With Multiple Containers\"]}),`\n`,(0,n.jsx)(e.p,{children:\"We mentioned pods with multiple containers already, so let's dive into that a bit more. The first thing for you to know is that pods' ability to run multiple containers is not something you should overuse. For example, it's not meant to be used to combine front-end and back-end microservices into one pod. Quite the opposite; you actually shouldn't combine multiple functional microservices into one pod.\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:\"Why does Kubernetes give you that option then? Well, it's for a different purpose. Putting more than one container into a single pod is useful for adding containers that are like assistants or helpers to your main container. A common example is log gathering containers. Their only job is to read logs from your main container and forward it (usually to some centralized log management solution). Another example is secret management containers. Their job is to securely load secrets from some secret vault and securely pass it to your main container.\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:'As you can see, multiple containers in a pod are typically used in the main container + secondary containers configuration. We call these secondary containers \"sidecar containers.\"\\xA0'}),`\n`,(0,n.jsx)(e.p,{children:\"Of course, even though it's not usually recommended, there's nothing stopping you from combining two containers into one pod. If you have a very specific use case and you think it would make sense in your case, you can add more containers to your pod. You just need to be aware of the consequences of such an approach. The main one is that, in the case of the failure of the pod, both containers will die.\\xA0\"}),`\n`,(0,n.jsxs)(e.h3,{id:\"summary\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#summary\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Summary\"]}),`\n`,(0,n.jsx)(e.p,{children:\"As you can see, pods are pretty straightforward resources. In most cases, you can treat them the same as containers, but they do offer extra sidecar functionality when necessary.\\xA0\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"Learned all you need to for pod basics? Read on to our advanced pod concepts article \",(0,n.jsx)(e.a,{href:\"https://release.com/blog/kubernetes-pods-advanced-concepts-explained\",children:\"here\"}),\"!\"]})]})}function v(o={}){let{wrapper:e}=o.components||{};return e?(0,n.jsx)(e,Object.assign({},o,{children:(0,n.jsx)(d,o)})):d(o)}var x=v;return w(K);})();\n;return Component;"
        },
        "_id": "blog/posts/kubernetes-pod-a-beginners-guide-to-an-essential-resource.mdx",
        "_raw": {
          "sourceFilePath": "blog/posts/kubernetes-pod-a-beginners-guide-to-an-essential-resource.mdx",
          "sourceFileName": "kubernetes-pod-a-beginners-guide-to-an-essential-resource.mdx",
          "sourceFileDir": "blog/posts",
          "contentType": "mdx",
          "flattenedPath": "blog/posts/kubernetes-pod-a-beginners-guide-to-an-essential-resource"
        },
        "type": "BlogPost",
        "computedSlug": "kubernetes-pod-a-beginners-guide-to-an-essential-resource"
      },
      "documentHash": "1739393595023",
      "hasWarnings": false,
      "documentTypeName": "BlogPost"
    },
    "blog/posts/kubernetes-pods-advanced-concepts-explained.mdx": {
      "document": {
        "title": "Kubernetes Pods Advanced Concepts Explained",
        "summary": "In this blog post we’ll investigate certain advanced concepts related to Kubernetes init containers, sidecars",
        "publishDate": "Thu Feb 25 2021 01:31:08 GMT+0000 (Coordinated Universal Time)",
        "author": "regis-wilson",
        "readingTime": 8,
        "categories": [
          "kubernetes",
          "platform-engineering"
        ],
        "mainImage": "/blog-images/1a383a77d1cc155f4c2d0d82203179c5.jpg",
        "imageAlt": "Six dolphins swimming in the sea",
        "showCTA": true,
        "ctaCopy": "Optimize your Kubernetes pod management with Release's environment control for seamless init containers, sidecars, and probes integration.",
        "ctaLink": "https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=kubernetes-pods-advanced-concepts-explained",
        "relatedPosts": [
          ""
        ],
        "ogImage": "/blog-images/1a383a77d1cc155f4c2d0d82203179c5.jpg",
        "excerpt": "In this blog post we’ll investigate certain advanced concepts related to Kubernetes init containers, sidecars",
        "tags": [
          "kubernetes",
          "platform-engineering"
        ],
        "ctaButton": "Try Release for Free",
        "body": {
          "raw": "\nIn this blog post we’ll investigate certain advanced concepts related to Kubernetes init containers, sidecars, config maps, and probes. We’ll show you how to implement these concepts in your own cluster, but more importantly how to apply these to your projects in [Release](https://releasehub.com) for both fun and profit.\n\nWe’ll start with a brief introduction to pods and containers in Kubernetes, and then show specific examples of each item listed above. Below you will find a drawing of these examples to keep yourself oriented during our bumpy ride ahead.\n\n![](/blog-images/ceb3152f1e5b768449345108806f7267.jpg)\n\n### Key Kubernetes Pod Concepts\n\nBefore we begin, let’s get a brief overview of some key concepts.\n\n#### Container\n\nIn Docker, a container is an image that bundles layered filesystems which can be deployed as a runnable bundle. This container is usually built with a Dockerfile and has a startup binary or executable command.\n\n##### Sidecar Container\n\nA [sidecar container](https://kubernetes.io/docs/concepts/workloads/pods/#how-pods-manage-multiple-containers) is simply a container that runs alongside other containers in the pod. There’s no official definition of a sidecar concept. The only thing that distinguishes a container as a sidecar container is that you consider it ancillary or secondary to the primary container. Running multiple sidecar containers does not scale well, but does have additional advantages of being able to reuse configuration files and container images. The reason sidecars do not scale well is that they may be overprisioned or wasteful based on the performance of the main application container. However, the tradeoffs can make sense in legacy applications or during migrations toward truly cloud-native designs.\n\n##### Init Container\n\nAn [init container](https://kubernetes.io/docs/concepts/workloads/pods/init-containers/) is simply a container that runs before any other containers in the pod. You can have several init containers that run sequentially. As each container finishes and exits properly (with a zero!), the next container will start. If an init container exits with an error or if it does not finish completely, the pod could go into a [dreaded CrashLoopBackoff](https://releasehub.com/blog/kubernetes-how-to-debug-crashloopbackoff-in-a-container). All of the containers share a filesystem, so the benefit here is that you can use or reuse container images to process, compile, or generate files or documents that can be picked up later by other containers.\n\n##### Probes\n\nAlthough the word “probes” may stir up visions of Alien tools used for discovery and investigation of humans, fear not. These probes will only make your services run better! Kubernetes has [several probes](https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/) for defining the health of containers inside a pod. A startup probe allows the scheduler to tolerate delays in a slow-startup container. A liveness probe allows Kubernetes to restart a faulty or stalled container. A readiness probe allows a container to receive traffic only when it is ready to do so.\n\n#### Pod\n\nYou may harbour some fear in the back of your mind of “pod people” or vegetable clones grown to replace humanity with mindless zombies who hunt and destroy mankind. However, in Kubernetes, the smallest managed unit is the pod. But a pod could be composed of several containers that run in a single process space and filesystem. A pod is usually composed of one container that runs a single process as a service. However, there are several advanced usage examples we will go into that run multiple containers for expanded options and use cases.\n\n#### Node\n\nA Kubernetes node is ultimately a physical machine (which can have several layers of virtualisation) that runs the pod or pods, providing the critical CPU, memory, disk, and network resources. Multiple pods can be spread across multiple nodes, but a single pod is contained on a single node.\n\n#### Volumes\n\nVolumes are simply abstractions of filesystems that can be mounted inside containers. You cannot overlap or nest volume mounts. However, there are several mount types that might be very useful to your use case.\n\n##### configMap\n\nA [configMap](https://kubernetes.io/docs/concepts/storage/volumes/#configmap) is a so-called “blob” of information that can be mounted as a file inside your container. Remember, that this is not an evil, destructive blob out to devour our planet! It is a batch of text that is treated amorphously, like a… well… blob. The usual use case here is for a configuration file or secrets mount.\n\n##### emptyDir\n\nAn [emptyDir](https://kubernetes.io/docs/concepts/storage/volumes/#emptydir) is an empty filesystem that can be written into and used by containers inside a pod. The usual use case here is for temporary storage or initialization files that can be shared.\n\n##### hostPath\n\nA [hostPath](https://kubernetes.io/docs/concepts/storage/volumes/#hostpath) is a filesystem that exists on the Kubernetes node directly and can be shared between containers in the pod. The usual use case here is to store cached files that could be primed from previous deployments if they are available.\n\n##### Persistent Volume Claim (PVC)\n\nA [persistent volume claim](https://kubernetes.io/docs/concepts/storage/persistent-volumes/) is a filesystem that lasts across nodes and pods inside a namespace. Data in a PVC are not erased or destroyed when a pod is removed, only when the namespace is removed. PVCs come in many underlying flavors of storage, depending on your cloud provider and infrastructure architecture.\n\n#### Namespace\n\nA Kubernetes namespace is a collection of resources that are grouped together and generally have access to one another. Multiple pods, deployments, and volume claims (to list a few) will run together, potentially across multiple nodes.\n\n### Sidecars and Init Containers\n\nThe first use case we will cover involves running several containers inside a single pod. Once again, a pod here refers to one or more containers grouped together in Kubernetes, not vegetable human clones grown for evil reasons. In the following scenario, we will examine how multiple containers can share a single process space, filesystem, and network stack.\n\nKeep in mind that most docker and Kubernetes purists will tell you that running more than one process in a container, or having more than one container in a pod is not a good design and will inevitably lead to scalability and architectural issues down the road. These concerns are generally well founded. However, careful application of the following supported and recommended patterns will allow you to thrive either during your transition from a legacy stack to Kubernetes or once you are successfully running your application in a cluster.\n\nOne particular use case we encounter with customers is that their application has a backend container that requires a reverse proxy like Nginx to perform routing, static file serving, and so forth. The best method to achieve this objective would be to create a separate pod with Nginx (for example) and run the two service pods in a single namespace. This gives us the flexibility to scale the backend pods and Nginx pods separately as needed. However, typically the backend service or application needs to also serve static files that are located inside the container filesystem and would not be available across the pod boundary. We agree this is not a preferred pattern to use, but it is common enough with legacy applications that we see it happen.\n\nIn this scenario, we often recommend a sidecar container running Nginx which can be pulled directly from Docker Hub or a custom image can be created. We also recommend that customers reuse their backend application container as an init container that starts with a custom command for creating any initialization or other startup tasks that need to be completed before the application itself starts.\n\nOne feature of this multi-container setup is that the Nginx container can use the “localhost” loopback to communicate with the backend service. Of course the sidecar container might be a logging or monitoring agent, but the principle is the same: the containers can speak with each other over a private network that is potentially not available outside of the pod, unless you make it available. In our Nginx example, the backend could be isolated so that all communication traffic inbound to the service container must be routed to the Nginx proxy.\n\nThe other nice feature of this configuration is that the containers all share a common file system so that the Nginx container can access static files generated by (or stored on) the backend service container.\n\nHere is a link to our documentation that shows an example of running [sidecar](https://docs.releasehub.com/reference-guide/application-settings/application-template#sidecar-containers) and [init](https://docs.releasehub.com/reference-guide/application-settings/application-template#init-containers) containers on Release.\n\n### Probes\n\nAs we have noted, probes are not just for Aliens! Kubernetes uses them to test your application stack and report on its health. Kubernetes will also take action based on these probes, just like an Alien might. There are several probes that are supported natively by Kubernetes. The main use cases we support for our customers are the liveness probe and readiness probe.\n\nThe liveness probe is a way to test whether a container is “alive” or not, and if it fails the probe, then Kubernetes will restart the container. We usually recommend that your application not freeze up or have memory leaks and so forth so that a liveness probe should not be necessary. This “reboot your app to fix the problems” philosophy is not generally considered good practice. However, perfect code is impossible and when services are running in a production container environment, we know that almost anything can (and will) happen.\n\nThe readiness probe is a way to test whether a container is capable of serving traffic or not, and if it fails the probe, then the service port will be removed from the ingress controller. Contrary to our stance on the liveness probe, we strongly encourage and recommend that customers implement a readiness probe on any service that receives inbound traffic. In some sense, we consider a readiness probe mandatory for your production services.\n\nHere is a link to our documentation that shows an example of using a [liveness and readiness probe](https://docs.releasehub.com/reference-guide/application-settings/application-template#readiness-and-liveness-probes) for services running in Release.\n\n### Volumes\n\nThis section gets a bit technical and tricky. Of course, no actual customer stacks would use every single type of volume, container, and probe listed in this article. But we do hope this overview shows all the features that are possible. You should carefully consider the use cases presented below and choose the one that best fits your use case.\n\nHere is a link to our documentation that shows options for our [storage volume types](https://docs.releasehub.com/reference-guide/application-settings/application-template#resources).\n\n#### configMap (Just in Time File Mounts)\n\nA configMap (purposely spelled in [camelCase](https://en.wikipedia.org/wiki/Camel_case#Programming_and_coding)) is not itself a volume in Kubernetes. Strictly speaking, a configMap is just a blob of text that can be stored in the [etcd key-value datastore](https://kubernetes.io/docs/concepts/overview/components/#etcd). However, one convenient use case Release supports is creating a container storage volume that is mounted inside a container as a file whose contents are the text blob stored in etcd. At Release, we call this customer helper function a [Just in Time File Mount](https://docs.releasehub.com/reference-guide/application-settings/file-mounts). The common use case for a configMap at Release is being able to upload a file with configuration details. For example, in our previous example involving an Nginx sidecar, the [nginx.conf](https://www.nginx.com/resources/wiki/start/topics/examples/full/) file could be uploaded as a Just in Time File Mount. _“What do we want? File Mounts! When do we want them? Just in Time!”_\n\n#### emptyDir (Scratch Volume)\n\nAn emptyDir volume is a native Kubernetes construct Release supports for containers in a pod to share empty space that can be mounted locally. This volume is erased as soon as the pod ends its life-cycle, and it is blank to begin with. Thus, the most common use case is for a scratch or temporary location to store files that only need to be stored during the lifetime of the pod.\n\n#### hostPath (Intra-pod Cache or Shared Volume)\n\nThe next example is a native Kubernetes construct that Release supports for containers in a pod to share a filesystem path that stays on a node. The most common use case for a hostPath volume is to store cache or build data that can be generated and re-generated as needed inside a pod. Unlike an emptyDir volume that only lasts as long as the pod does, the hostPath can last as long as the application that deploys the pods. Thus, a container could generate (or compute) files, assets, or data that could be reused or incrementally updated with the next pod deployment on the same node. Release automatically sets the correct permissions and ensures that each namespace has unique files so that data are not leaked between customers.\n\n#### PVC (Long Term Persistent Storage)\n\nThe final example of a volume mount that Release offers is the ability to store data on persistent storage that is available across nodes and pods in a namespace. This long term storage is persistent and does not disappear during pod or node life cycles. Release uses Amazon Web Services (AWS) Elastic File System (EFS), which is their cloud offering of Network File System (NFSv4) storage. This allows customers to store long term data that will persist between deployments, availability zones (AZs), and node failures, and can be shared between multiple pods. The most common use cases for persistent storage of this type are for pre-production databases that need long term storage between deployments.\n\n### Conclusion\n\nIn this article, we’ve given you an overview of key advanced concepts for Kubernetes pods that you will not find anywhere else. If you are confident and practiced in using these examples in your Kubernetes deployments, then you can consider yourself one of the members of an elite club of practitioners. This benefit does not just come with a distinguished title or piece of paper stating your qualifications: it also confers substantial success and accomplishment in your DevOps career journey.\n\nPhoto by [Wynand Uys](https://unsplash.com/@wynand_uys?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) on [Unsplash](https://unsplash.com/s/photos/pod?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)\n",
          "code": "var Component=(()=>{var d=Object.create;var o=Object.defineProperty;var p=Object.getOwnPropertyDescriptor;var u=Object.getOwnPropertyNames;var m=Object.getPrototypeOf,f=Object.prototype.hasOwnProperty;var b=(a,e)=>()=>(e||a((e={exports:{}}).exports,e),e.exports),g=(a,e)=>{for(var t in e)o(a,t,{get:e[t],enumerable:!0})},r=(a,e,t,i)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let s of u(e))!f.call(a,s)&&s!==t&&o(a,s,{get:()=>e[s],enumerable:!(i=p(e,s))||i.enumerable});return a};var y=(a,e,t)=>(t=a!=null?d(m(a)):{},r(e||!a||!a.__esModule?o(t,\"default\",{value:a,enumerable:!0}):t,a)),v=a=>r(o({},\"__esModule\",{value:!0}),a);var l=b((A,c)=>{c.exports=_jsx_runtime});var N={};g(N,{default:()=>x,frontmatter:()=>w});var n=y(l()),w={title:\"Kubernetes Pods Advanced Concepts Explained\",summary:\"In this blog post we\\u2019ll investigate certain advanced concepts related to Kubernetes init containers, sidecars\",publishDate:\"Thu Feb 25 2021 01:31:08 GMT+0000 (Coordinated Universal Time)\",author:\"regis-wilson\",readingTime:8,categories:[\"kubernetes\",\"platform-engineering\"],mainImage:\"/blog-images/1a383a77d1cc155f4c2d0d82203179c5.jpg\",imageAlt:\"Six dolphins swimming in the sea\",showCTA:!0,ctaCopy:\"Optimize your Kubernetes pod management with Release's environment control for seamless init containers, sidecars, and probes integration.\",ctaLink:\"https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=kubernetes-pods-advanced-concepts-explained\",relatedPosts:[\"\"],ogImage:\"/blog-images/1a383a77d1cc155f4c2d0d82203179c5.jpg\",excerpt:\"In this blog post we\\u2019ll investigate certain advanced concepts related to Kubernetes init containers, sidecars\",tags:[\"kubernetes\",\"platform-engineering\"],ctaButton:\"Try Release for Free\"};function h(a){let e=Object.assign({p:\"p\",a:\"a\",img:\"img\",h3:\"h3\",span:\"span\",h4:\"h4\",h5:\"h5\",em:\"em\"},a.components);return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsxs)(e.p,{children:[\"In this blog post we\\u2019ll investigate certain advanced concepts related to Kubernetes init containers, sidecars, config maps, and probes. We\\u2019ll show you how to implement these concepts in your own cluster, but more importantly how to apply these to your projects in \",(0,n.jsx)(e.a,{href:\"https://releasehub.com\",children:\"Release\"}),\" for both fun and profit.\"]}),`\n`,(0,n.jsx)(e.p,{children:\"We\\u2019ll start with a brief introduction to pods and containers in Kubernetes, and then show specific examples of each item listed above. Below you will find a drawing of these examples to keep yourself oriented during our bumpy ride ahead.\"}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/ceb3152f1e5b768449345108806f7267.jpg\",alt:\"\"})}),`\n`,(0,n.jsxs)(e.h3,{id:\"key-kubernetes-pod-concepts\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#key-kubernetes-pod-concepts\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Key Kubernetes Pod Concepts\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Before we begin, let\\u2019s get a brief overview of some key concepts.\"}),`\n`,(0,n.jsxs)(e.h4,{id:\"container\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#container\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Container\"]}),`\n`,(0,n.jsx)(e.p,{children:\"In Docker, a container is an image that bundles layered filesystems which can be deployed as a runnable bundle. This container is usually built with a Dockerfile and has a startup binary or executable command.\"}),`\n`,(0,n.jsxs)(e.h5,{id:\"sidecar-container\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#sidecar-container\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Sidecar Container\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"A \",(0,n.jsx)(e.a,{href:\"https://kubernetes.io/docs/concepts/workloads/pods/#how-pods-manage-multiple-containers\",children:\"sidecar container\"}),\" is simply a container that runs alongside other containers in the pod. There\\u2019s no official definition of a sidecar concept. The only thing that distinguishes a container as a sidecar container is that you consider it ancillary or secondary to the primary container. Running multiple sidecar containers does not scale well, but does have additional advantages of being able to reuse configuration files and container images. The reason sidecars do not scale well is that they may be overprisioned or wasteful based on the performance of the main application container. However, the tradeoffs can make sense in legacy applications or during migrations toward truly cloud-native designs.\"]}),`\n`,(0,n.jsxs)(e.h5,{id:\"init-container\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#init-container\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Init Container\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"An \",(0,n.jsx)(e.a,{href:\"https://kubernetes.io/docs/concepts/workloads/pods/init-containers/\",children:\"init container\"}),\" is simply a container that runs before any other containers in the pod. You can have several init containers that run sequentially. As each container finishes and exits properly (with a zero!), the next container will start. If an init container exits with an error or if it does not finish completely, the pod could go into a \",(0,n.jsx)(e.a,{href:\"https://releasehub.com/blog/kubernetes-how-to-debug-crashloopbackoff-in-a-container\",children:\"dreaded CrashLoopBackoff\"}),\". All of the containers share a filesystem, so the benefit here is that you can use or reuse container images to process, compile, or generate files or documents that can be picked up later by other containers.\"]}),`\n`,(0,n.jsxs)(e.h5,{id:\"probes\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#probes\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Probes\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"Although the word \\u201Cprobes\\u201D may stir up visions of Alien tools used for discovery and investigation of humans, fear not. These probes will only make your services run better! Kubernetes has \",(0,n.jsx)(e.a,{href:\"https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/\",children:\"several probes\"}),\" for defining the health of containers inside a pod. A startup probe allows the scheduler to tolerate delays in a slow-startup container. A liveness probe allows Kubernetes to restart a faulty or stalled container. A readiness probe allows a container to receive traffic only when it is ready to do so.\"]}),`\n`,(0,n.jsxs)(e.h4,{id:\"pod\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#pod\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Pod\"]}),`\n`,(0,n.jsx)(e.p,{children:\"You may harbour some fear in the back of your mind of \\u201Cpod people\\u201D or vegetable clones grown to replace humanity with mindless zombies who hunt and destroy mankind. However, in Kubernetes, the smallest managed unit is the pod. But a pod could be composed of several containers that run in a single process space and filesystem. A pod is usually composed of one container that runs a single process as a service. However, there are several advanced usage examples we will go into that run multiple containers for expanded options and use cases.\"}),`\n`,(0,n.jsxs)(e.h4,{id:\"node\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#node\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Node\"]}),`\n`,(0,n.jsx)(e.p,{children:\"A Kubernetes node is ultimately a physical machine (which can have several layers of virtualisation) that runs the pod or pods, providing the critical CPU, memory, disk, and network resources. Multiple pods can be spread across multiple nodes, but a single pod is contained on a single node.\"}),`\n`,(0,n.jsxs)(e.h4,{id:\"volumes\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#volumes\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Volumes\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Volumes are simply abstractions of filesystems that can be mounted inside containers. You cannot overlap or nest volume mounts. However, there are several mount types that might be very useful to your use case.\"}),`\n`,(0,n.jsxs)(e.h5,{id:\"configmap\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#configmap\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"configMap\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"A \",(0,n.jsx)(e.a,{href:\"https://kubernetes.io/docs/concepts/storage/volumes/#configmap\",children:\"configMap\"}),\" is a so-called \\u201Cblob\\u201D of information that can be mounted as a file inside your container. Remember, that this is not an evil, destructive blob out to devour our planet! It is a batch of text that is treated amorphously, like a\\u2026 well\\u2026 blob. The usual use case here is for a configuration file or secrets mount.\"]}),`\n`,(0,n.jsxs)(e.h5,{id:\"emptydir\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#emptydir\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"emptyDir\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"An \",(0,n.jsx)(e.a,{href:\"https://kubernetes.io/docs/concepts/storage/volumes/#emptydir\",children:\"emptyDir\"}),\" is an empty filesystem that can be written into and used by containers inside a pod. The usual use case here is for temporary storage or initialization files that can be shared.\"]}),`\n`,(0,n.jsxs)(e.h5,{id:\"hostpath\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#hostpath\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"hostPath\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"A \",(0,n.jsx)(e.a,{href:\"https://kubernetes.io/docs/concepts/storage/volumes/#hostpath\",children:\"hostPath\"}),\" is a filesystem that exists on the Kubernetes node directly and can be shared between containers in the pod. The usual use case here is to store cached files that could be primed from previous deployments if they are available.\"]}),`\n`,(0,n.jsxs)(e.h5,{id:\"persistent-volume-claim-pvc\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#persistent-volume-claim-pvc\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Persistent Volume Claim (PVC)\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"A \",(0,n.jsx)(e.a,{href:\"https://kubernetes.io/docs/concepts/storage/persistent-volumes/\",children:\"persistent volume claim\"}),\" is a filesystem that lasts across nodes and pods inside a namespace. Data in a PVC are not erased or destroyed when a pod is removed, only when the namespace is removed. PVCs come in many underlying flavors of storage, depending on your cloud provider and infrastructure architecture.\"]}),`\n`,(0,n.jsxs)(e.h4,{id:\"namespace\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#namespace\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Namespace\"]}),`\n`,(0,n.jsx)(e.p,{children:\"A Kubernetes namespace is a collection of resources that are grouped together and generally have access to one another. Multiple pods, deployments, and volume claims (to list a few) will run together, potentially across multiple nodes.\"}),`\n`,(0,n.jsxs)(e.h3,{id:\"sidecars-and-init-containers\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#sidecars-and-init-containers\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Sidecars and Init Containers\"]}),`\n`,(0,n.jsx)(e.p,{children:\"The first use case we will cover involves running several containers inside a single pod. Once again, a pod here refers to one or more containers grouped together in Kubernetes, not vegetable human clones grown for evil reasons. In the following scenario, we will examine how multiple containers can share a single process space, filesystem, and network stack.\"}),`\n`,(0,n.jsx)(e.p,{children:\"Keep in mind that most docker and Kubernetes purists will tell you that running more than one process in a container, or having more than one container in a pod is not a good design and will inevitably lead to scalability and architectural issues down the road. These concerns are generally well founded. However, careful application of the following supported and recommended patterns will allow you to thrive either during your transition from a legacy stack to Kubernetes or once you are successfully running your application in a cluster.\"}),`\n`,(0,n.jsx)(e.p,{children:\"One particular use case we encounter with customers is that their application has a backend container that requires a reverse proxy like Nginx to perform routing, static file serving, and so forth. The best method to achieve this objective would be to create a separate pod with Nginx (for example) and run the two service pods in a single namespace. This gives us the flexibility to scale the backend pods and Nginx pods separately as needed. However, typically the backend service or application needs to also serve static files that are located inside the container filesystem and would not be available across the pod boundary. We agree this is not a preferred pattern to use, but it is common enough with legacy applications that we see it happen.\"}),`\n`,(0,n.jsx)(e.p,{children:\"In this scenario, we often recommend a sidecar container running Nginx which can be pulled directly from Docker Hub or a custom image can be created. We also recommend that customers reuse their backend application container as an init container that starts with a custom command for creating any initialization or other startup tasks that need to be completed before the application itself starts.\"}),`\n`,(0,n.jsx)(e.p,{children:\"One feature of this multi-container setup is that the Nginx container can use the \\u201Clocalhost\\u201D loopback to communicate with the backend service. Of course the sidecar container might be a logging or monitoring agent, but the principle is the same: the containers can speak with each other over a private network that is potentially not available outside of the pod, unless you make it available. In our Nginx example, the backend could be isolated so that all communication traffic inbound to the service container must be routed to the Nginx proxy.\"}),`\n`,(0,n.jsx)(e.p,{children:\"The other nice feature of this configuration is that the containers all share a common file system so that the Nginx container can access static files generated by (or stored on) the backend service container.\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"Here is a link to our documentation that shows an example of running \",(0,n.jsx)(e.a,{href:\"https://docs.releasehub.com/reference-guide/application-settings/application-template#sidecar-containers\",children:\"sidecar\"}),\" and \",(0,n.jsx)(e.a,{href:\"https://docs.releasehub.com/reference-guide/application-settings/application-template#init-containers\",children:\"init\"}),\" containers on Release.\"]}),`\n`,(0,n.jsxs)(e.h3,{id:\"probes-1\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#probes-1\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Probes\"]}),`\n`,(0,n.jsx)(e.p,{children:\"As we have noted, probes are not just for Aliens! Kubernetes uses them to test your application stack and report on its health. Kubernetes will also take action based on these probes, just like an Alien might. There are several probes that are supported natively by Kubernetes. The main use cases we support for our customers are the liveness probe and readiness probe.\"}),`\n`,(0,n.jsx)(e.p,{children:\"The liveness probe is a way to test whether a container is \\u201Calive\\u201D or not, and if it fails the probe, then Kubernetes will restart the container. We usually recommend that your application not freeze up or have memory leaks and so forth so that a liveness probe should not be necessary. This \\u201Creboot your app to fix the problems\\u201D philosophy is not generally considered good practice. However, perfect code is impossible and when services are running in a production container environment, we know that almost anything can (and will) happen.\"}),`\n`,(0,n.jsx)(e.p,{children:\"The readiness probe is a way to test whether a container is capable of serving traffic or not, and if it fails the probe, then the service port will be removed from the ingress controller. Contrary to our stance on the liveness probe, we strongly encourage and recommend that customers implement a readiness probe on any service that receives inbound traffic. In some sense, we consider a readiness probe mandatory for your production services.\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"Here is a link to our documentation that shows an example of using a \",(0,n.jsx)(e.a,{href:\"https://docs.releasehub.com/reference-guide/application-settings/application-template#readiness-and-liveness-probes\",children:\"liveness and readiness probe\"}),\" for services running in Release.\"]}),`\n`,(0,n.jsxs)(e.h3,{id:\"volumes-1\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#volumes-1\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Volumes\"]}),`\n`,(0,n.jsx)(e.p,{children:\"This section gets a bit technical and tricky. Of course, no actual customer stacks would use every single type of volume, container, and probe listed in this article. But we do hope this overview shows all the features that are possible. You should carefully consider the use cases presented below and choose the one that best fits your use case.\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"Here is a link to our documentation that shows options for our \",(0,n.jsx)(e.a,{href:\"https://docs.releasehub.com/reference-guide/application-settings/application-template#resources\",children:\"storage volume types\"}),\".\"]}),`\n`,(0,n.jsxs)(e.h4,{id:\"configmap-just-in-time-file-mounts\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#configmap-just-in-time-file-mounts\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"configMap (Just in Time File Mounts)\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"A configMap (purposely spelled in \",(0,n.jsx)(e.a,{href:\"https://en.wikipedia.org/wiki/Camel_case#Programming_and_coding\",children:\"camelCase\"}),\") is not itself a volume in Kubernetes. Strictly speaking, a configMap is just a blob of text that can be stored in the \",(0,n.jsx)(e.a,{href:\"https://kubernetes.io/docs/concepts/overview/components/#etcd\",children:\"etcd key-value datastore\"}),\". However, one convenient use case Release supports is creating a container storage volume that is mounted inside a container as a file whose contents are the text blob stored in etcd. At Release, we call this customer helper function a \",(0,n.jsx)(e.a,{href:\"https://docs.releasehub.com/reference-guide/application-settings/file-mounts\",children:\"Just in Time File Mount\"}),\". The common use case for a configMap at Release is being able to upload a file with configuration details. For example, in our previous example involving an Nginx sidecar, the \",(0,n.jsx)(e.a,{href:\"https://www.nginx.com/resources/wiki/start/topics/examples/full/\",children:\"nginx.conf\"}),\" file could be uploaded as a Just in Time File Mount. \",(0,n.jsx)(e.em,{children:\"\\u201CWhat do we want? File Mounts! When do we want them? Just in Time!\\u201D\"})]}),`\n`,(0,n.jsxs)(e.h4,{id:\"emptydir-scratch-volume\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#emptydir-scratch-volume\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"emptyDir (Scratch Volume)\"]}),`\n`,(0,n.jsx)(e.p,{children:\"An emptyDir volume is a native Kubernetes construct Release supports for containers in a pod to share empty space that can be mounted locally. This volume is erased as soon as the pod ends its life-cycle, and it is blank to begin with. Thus, the most common use case is for a scratch or temporary location to store files that only need to be stored during the lifetime of the pod.\"}),`\n`,(0,n.jsxs)(e.h4,{id:\"hostpath-intra-pod-cache-or-shared-volume\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#hostpath-intra-pod-cache-or-shared-volume\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"hostPath (Intra-pod Cache or Shared Volume)\"]}),`\n`,(0,n.jsx)(e.p,{children:\"The next example is a native Kubernetes construct that Release supports for containers in a pod to share a filesystem path that stays on a node. The most common use case for a hostPath volume is to store cache or build data that can be generated and re-generated as needed inside a pod. Unlike an emptyDir volume that only lasts as long as the pod does, the hostPath can last as long as the application that deploys the pods. Thus, a container could generate (or compute) files, assets, or data that could be reused or incrementally updated with the next pod deployment on the same node. Release automatically sets the correct permissions and ensures that each namespace has unique files so that data are not leaked between customers.\"}),`\n`,(0,n.jsxs)(e.h4,{id:\"pvc-long-term-persistent-storage\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#pvc-long-term-persistent-storage\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"PVC (Long Term Persistent Storage)\"]}),`\n`,(0,n.jsx)(e.p,{children:\"The final example of a volume mount that Release offers is the ability to store data on persistent storage that is available across nodes and pods in a namespace. This long term storage is persistent and does not disappear during pod or node life cycles. Release uses Amazon Web Services (AWS) Elastic File System (EFS), which is their cloud offering of Network File System (NFSv4) storage. This allows customers to store long term data that will persist between deployments, availability zones (AZs), and node failures, and can be shared between multiple pods. The most common use cases for persistent storage of this type are for pre-production databases that need long term storage between deployments.\"}),`\n`,(0,n.jsxs)(e.h3,{id:\"conclusion\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#conclusion\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Conclusion\"]}),`\n`,(0,n.jsx)(e.p,{children:\"In this article, we\\u2019ve given you an overview of key advanced concepts for Kubernetes pods that you will not find anywhere else. If you are confident and practiced in using these examples in your Kubernetes deployments, then you can consider yourself one of the members of an elite club of practitioners. This benefit does not just come with a distinguished title or piece of paper stating your qualifications: it also confers substantial success and accomplishment in your DevOps career journey.\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"Photo by \",(0,n.jsx)(e.a,{href:\"https://unsplash.com/@wynand_uys?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText\",children:\"Wynand Uys\"}),\" on \",(0,n.jsx)(e.a,{href:\"https://unsplash.com/s/photos/pod?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText\",children:\"Unsplash\"})]})]})}function k(a={}){let{wrapper:e}=a.components||{};return e?(0,n.jsx)(e,Object.assign({},a,{children:(0,n.jsx)(h,a)})):h(a)}var x=k;return v(N);})();\n;return Component;"
        },
        "_id": "blog/posts/kubernetes-pods-advanced-concepts-explained.mdx",
        "_raw": {
          "sourceFilePath": "blog/posts/kubernetes-pods-advanced-concepts-explained.mdx",
          "sourceFileName": "kubernetes-pods-advanced-concepts-explained.mdx",
          "sourceFileDir": "blog/posts",
          "contentType": "mdx",
          "flattenedPath": "blog/posts/kubernetes-pods-advanced-concepts-explained"
        },
        "type": "BlogPost",
        "computedSlug": "kubernetes-pods-advanced-concepts-explained"
      },
      "documentHash": "1739393595023",
      "hasWarnings": false,
      "documentTypeName": "BlogPost"
    },
    "blog/posts/kubernetes-probes-sre-anywhere-lightning-talk.mdx": {
      "document": {
        "title": "Kubernetes Probes: SRE Anywhere Lightning Talk",
        "summary": "Regis Wilson presents a 5 minute video submission for a lightning talk presentation at the SRE From Anywhere event.",
        "publishDate": "Thu Jun 10 2021 22:11:25 GMT+0000 (Coordinated Universal Time)",
        "author": "regis-wilson",
        "readingTime": 5,
        "categories": [
          "kubernetes",
          "platform-engineering"
        ],
        "mainImage": "/blog-images/63bf7e512270f0f923aaa42de8f1ba58.jpg",
        "imageAlt": "Catchpoint SRE from Anywhere virtual conference announcement banner",
        "showCTA": true,
        "ctaCopy": "Improve Kubernetes probes with Release's ephemeral environments for faster bug resolution and stable deployments.",
        "ctaLink": "https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=kubernetes-probes-sre-anywhere-lightning-talk",
        "relatedPosts": [
          ""
        ],
        "ogImage": "/blog-images/63bf7e512270f0f923aaa42de8f1ba58.jpg",
        "excerpt": "Regis Wilson presents a 5 minute video submission for a lightning talk presentation at the SRE From Anywhere event.",
        "tags": [
          "kubernetes",
          "platform-engineering"
        ],
        "ctaButton": "Try Release for Free",
        "body": {
          "raw": "\nIt was my pleasure to present a five minute lighting talk at the [Catchpoint SRE from Anywhere](https://www.catchpoint.com/sre/from-anywhere-2021) virtual conference on June 10, 2021. You can see the video presentation which was based on my earlier blog post on [Kubernetes probes](https://releasehub.com/blog/kubernetes-health-checks-2-ways-to-improve-stability).\n",
          "code": "var Component=(()=>{var c=Object.create;var s=Object.defineProperty;var h=Object.getOwnPropertyDescriptor;var g=Object.getOwnPropertyNames;var p=Object.getPrototypeOf,b=Object.prototype.hasOwnProperty;var f=(e,t)=>()=>(t||e((t={exports:{}}).exports,t),t.exports),d=(e,t)=>{for(var n in t)s(e,n,{get:t[n],enumerable:!0})},i=(e,t,n,o)=>{if(t&&typeof t==\"object\"||typeof t==\"function\")for(let r of g(t))!b.call(e,r)&&r!==n&&s(e,r,{get:()=>t[r],enumerable:!(o=h(t,r))||o.enumerable});return e};var w=(e,t,n)=>(n=e!=null?c(p(e)):{},i(t||!e||!e.__esModule?s(n,\"default\",{value:e,enumerable:!0}):n,e)),y=e=>i(s({},\"__esModule\",{value:!0}),e);var l=f((R,m)=>{m.exports=_jsx_runtime});var _={};d(_,{default:()=>x,frontmatter:()=>v});var a=w(l()),v={title:\"Kubernetes Probes: SRE Anywhere Lightning Talk\",summary:\"Regis Wilson presents a 5 minute video submission for a lightning talk presentation at the SRE From Anywhere event.\",publishDate:\"Thu Jun 10 2021 22:11:25 GMT+0000 (Coordinated Universal Time)\",author:\"regis-wilson\",readingTime:5,categories:[\"kubernetes\",\"platform-engineering\"],mainImage:\"/blog-images/63bf7e512270f0f923aaa42de8f1ba58.jpg\",imageAlt:\"Catchpoint SRE from Anywhere virtual conference announcement banner\",showCTA:!0,ctaCopy:\"Improve Kubernetes probes with Release's ephemeral environments for faster bug resolution and stable deployments.\",ctaLink:\"https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=kubernetes-probes-sre-anywhere-lightning-talk\",relatedPosts:[\"\"],ogImage:\"/blog-images/63bf7e512270f0f923aaa42de8f1ba58.jpg\",excerpt:\"Regis Wilson presents a 5 minute video submission for a lightning talk presentation at the SRE From Anywhere event.\",tags:[\"kubernetes\",\"platform-engineering\"],ctaButton:\"Try Release for Free\"};function u(e){let t=Object.assign({p:\"p\",a:\"a\"},e.components);return(0,a.jsxs)(t.p,{children:[\"It was my pleasure to present a five minute lighting talk at the \",(0,a.jsx)(t.a,{href:\"https://www.catchpoint.com/sre/from-anywhere-2021\",children:\"Catchpoint SRE\\xA0from Anywhere\"}),\" virtual conference on June 10, 2021. You can see the video presentation which was based on my earlier blog post on \",(0,a.jsx)(t.a,{href:\"https://releasehub.com/blog/kubernetes-health-checks-2-ways-to-improve-stability\",children:\"Kubernetes probes\"}),\".\"]})}function k(e={}){let{wrapper:t}=e.components||{};return t?(0,a.jsx)(t,Object.assign({},e,{children:(0,a.jsx)(u,e)})):u(e)}var x=k;return y(_);})();\n;return Component;"
        },
        "_id": "blog/posts/kubernetes-probes-sre-anywhere-lightning-talk.mdx",
        "_raw": {
          "sourceFilePath": "blog/posts/kubernetes-probes-sre-anywhere-lightning-talk.mdx",
          "sourceFileName": "kubernetes-probes-sre-anywhere-lightning-talk.mdx",
          "sourceFileDir": "blog/posts",
          "contentType": "mdx",
          "flattenedPath": "blog/posts/kubernetes-probes-sre-anywhere-lightning-talk"
        },
        "type": "BlogPost",
        "computedSlug": "kubernetes-probes-sre-anywhere-lightning-talk"
      },
      "documentHash": "1739393595023",
      "hasWarnings": false,
      "documentTypeName": "BlogPost"
    },
    "blog/posts/kubernetes-secrets-management-a-practical-guide.mdx": {
      "document": {
        "title": "Kubernetes Secrets Management: A Practical Guide",
        "summary": "What are Kubernetes secrets? Learn how to do Kubernetes secrets management in this post.",
        "publishDate": "Wed Aug 31 2022 15:40:56 GMT+0000 (Coordinated Universal Time)",
        "author": "ashley-penney",
        "readingTime": 5,
        "categories": [
          "kubernetes",
          "platform-engineering"
        ],
        "mainImage": "/blog-images/a805033176a0e06862c60397cda841da.jpg",
        "imageAlt": "a laptop on a table",
        "showCTA": true,
        "ctaCopy": "Looking to enhance Kubernetes secrets security? Try Release for managing ephemeral environments securely.",
        "ctaLink": "https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=kubernetes-secrets-management-a-practical-guide",
        "relatedPosts": [
          ""
        ],
        "ogImage": "/blog-images/a805033176a0e06862c60397cda841da.jpg",
        "excerpt": "What are Kubernetes secrets? Learn how to do Kubernetes secrets management in this post.",
        "tags": [
          "kubernetes",
          "platform-engineering"
        ],
        "ctaButton": "Try Release for Free",
        "body": {
          "raw": "\nIf you've worked with Kubernetes, you've probably heard of or used Kubernetes secrets. They are one of many Kubernetes resources. As the name suggests, they're meant to be used with secrets in your cluster. Imagine that your application running in a pod on a Kubernetes cluster needs some credentials. \n\nUsing a Kubernetes secret is the most straightforward way to provide these credentials to your application. But are they actually secure? What's the best way to use them? Should you use some other secret management solutions for your Kubernetes cluster? Read on to learn everything about Kubernetes secrets management. \n\n### Why Is Secret Management Important?\n\nBefore we dive into the do's and don'ts of Kubernetes secret management, let's take a moment to discuss why it's important in the first place. You see, Kubernetes secrets are a nice built-in semi-secret management solution, but they are not entirely secret (we'll get to that later), and they don't create a complete secret management solution. The typical problem that quickly arises when you use Kubernetes secrets is how to create and store them securely before they end up in a Kubernetes cluster. Kubernetes doesn't come with any integration to secret vaults out of the box. Therefore, they need a bit more engineering effort beyond simple creation to be secure. \n\n![TextDescription automatically generated](/blog-images/512dd9da29c34e645475ece9aca536a6.png)\n\n### Are Kubernetes Secrets Actually Secure?\n\nAs we mentioned, a critical aspect of Kubernetes secret management is the fact that Kubernetes secrets are not actually that secret. You may be surprised to hear that, but Kubernetes secrets are not encrypted and can be easily read by anyone with access to the cluster. Kubernetes secrets are only encoded using basic [base64](https://en.wikipedia.org/wiki/Base64) format. Let me show you. I'll apply the following YAML definition file of my Kubernetes secret to the cluster using the **kubectl apply** command: \n\n```yaml\n\n$ cat  | kubectl apply -f -\napiVersion: v1\nkind: Secret\nmetadata:\n  name: example-secret\ntype: Opaque\nstringData:\n  username: admin\n  password: super_secret_password\nEOF\nsecret/example-secret created\n\n```\n\nNow that we've created a secret, you'd expect it to be difficult to get the plain text values again from the cluster. If I execute **kubectl describe** on our secret value, Kubernetes won't show you the values by default: \n\n```yaml\n\nkubectl describe secret example-secret\nName:         example-secret\nNamespace:    default\nLabels:      \nAnnotations:  \nType:         Opaque\n\nData\n====\npassword:  21 bytes\nusername:  5 bytes\n\n```\n\nHowever, you can force it to show the values as follows: \n\n```yaml\n$ kubectl get secret example-secret -o jsonpath='{.data}'\n{password:c3VwZXJfc2VjcmV0X3Bhc3N3b3Jk username:YWRtaW4=}\n```\n\nNow, we can see the values, but as you would expect, they're not in plain text. However, as we mentioned before, the values are in base64, which is very easy to decode using base64 binary that comes installed on all modern operating systems. You only need to pipe the above output to a **base64 --decode** command: \n\n```yaml\n$ kubectl get secret example-secret -o jsonpath='{.data.username}' | base64 --decode\nadmin\n$ kubectl get secret example-secret -o jsonpath='{.data.password}' | base64 --decode\nsuper_secret_password\n```\n\nAs you can see, I didn't need to specify any encryption key or certificate. Anyone who has access to my cluster could do the same. \n\n### Is This a Problem?\n\nIs this a Kubernetes bug or vulnerability? No, not really. Kubernetes is simply not a secret management tool. It allows you to use Kubernetes secrets out of the box to get you started, but if you really want to stay secure, you'd use an external secret management solution. Another aspect of this is that it's possible to make Kubernetes secrets a bit more secure by applying RBAC rules to your cluster.\n\n![A picture containing insect, colorful, brightDescription automatically generated](/blog-images/6154afb5901303cb94833156057919d1.jpeg)\n\nAlso, in non-multi-tenant clusters, it's not that big of an issue since access to the cluster is limited to one team anyway. Everyone who has access to the cluster can probably access the secrets too. So, the fact that Kubernetes secrets are not that secret isn't automatically bad. It simply depends on the use case. \n\nFor customers who use Release in AWS, we automatically assign a KMS key at cluster creation so that their secrets are actually encrypted at rest. If you are managing your own EKS cluster, you can find out how to do that by following [these instructions](https://docs.aws.amazon.com/eks/latest/userguide/enable-kms.html).\n\nLet's get into how to actually manage secrets in Kubernetes. \n\n### Secrets vs. GitOps\n\nOne of the most common issues regarding secrets in Kubernetes is that you can't simply commit secret YAML definition files to your Git repository. This is because your secret would be there in plain text (or base64-encoded values if you use **data** instead of **stringData**—but as we just showed, base64 is easy to decode). And since manually applying secrets would be slow and not scalable, you need to find a way to store your YAML secrets definition securely. There are two popular approaches to doing so. Let's discuss both. \n\n### External Secrets\n\nThe first option is to use the [External Secrets](https://external-secrets.io/) tool. The idea behind it is quite clever. You store your secret values in a safe secret vault and only commit to Git repository YAML definition files that, instead of having the actual values, hold the reference to them. Then, you install External Secrets Operator on your cluster. And once you apply this reference YAML file, the ESO will go to your secrets vault, grab the true secret value, and create your ordinary Kubernetes secret on your cluster for you. Here's an example external secret resource definition: \n\n```yaml\n\napiVersion: external-secrets.io/v1beta1\nkind: ExternalSecret\nmetadata:\n  name: example\nspec:\n  refreshInterval: 1h\n  secretStoreRef:\n    name: secretstore-sample\n    kind: SecretStore\n  target:\n    name: secret-to-be-created\n    creationPolicy: Owner\n  data:\n  - secretKey: secret-key-to-be-managed\n    remoteRef:\n      key: provider-key\n      version: provider-key-version\n      property: provider-key-property\n  dataFrom:\n  - extract:\n      key: remote-key-in-the-provider\n\n```\n\nAs you can see, there are no actual secret values here, just pointers to where that value is. So, if that file were exposed and read by someone that shouldn't read it, they still wouldn't know your actual secrets without getting access to your secrets vault. \n\n### SealedSecrets\n\nAnother alternative is to use [SealedSecrets](https://github.com/bitnami-labs/sealed-secrets) project. It works differently but achieves the same result. SealedSecrets lets you encrypt the content of your Kubernetes secret YAML definition file. After encrypting, the file can be safely committed to the Git repository. It could even be exposed to the internet because only the SealedSecrets controller running in your cluster will be able to decrypt it. Here's what it looks like: \n\n```yaml\napiVersion: bitnami.com/v1alpha1\nkind: SealedSecret\nmetadata:\n   name: mysecret\n   namespace: mynamespace\nspec:\n   encryptedData:\n     foo: AgBy3i4OJSWK+PiTySYZZA9rO43cGDEq.....\n```\n\nAfter the controller decrypts it, SealedSecrets will create an ordinary Kubernetes secret for you. Therefore, you won't need to adjust your application code. \n\n### Bypassing Kubernetes Secrets\n\nThe tools mentioned above have one thing in common: at the end of the day, they still create ordinary Kubernetes secrets. In highly regulated environments with strict security rules, it may be necessary to avoid Kubernetes secrets completely because they're only base64 encoded. In such cases, you need to find another solution for passing secrets to your pods. \n\n![TextDescription automatically generated with medium confidence](/blog-images/b3fc341ebfb84d4e7f0620414b5a8a24.png)\n\n### Kubernetes Secrets Store CSI Driver\n\nOne option is to use Kubernetes's own new [Secrets Store CSI Driver](https://secrets-store-csi-driver.sigs.k8s.io/). With this option, you store your secrets in the external secrets store. Then Secrets Store CSI Driver will load them from there and mount them directly to your pods as volumes. Therefore, you'll bypass Kubernetes secrets resources completely. \n\nHowever, there are two downsides to this approach. First is the fact that you'll need to adjust your application to load secrets from files instead of from [environment variables](https://release.com/blog/kubernetes-environment-variables) like with normal Kubernetes secrets. Second, Secrets Store CSI Driver currently has alpha functionality, so it may not be fully stable. \n\n### Hashicorp Vault Injector\n\nAnother option is to use Hashicorp Vault together with their [Secret Injection](https://www.vaultproject.io/docs/platform/k8s/injector) option. The concept is similar to the Kubernetes Secrets Store CSI Driver. You store your secrets in Hashicorp Vault. Then, Hashicorp Vault Agent Injector will get the secrets for you and load them directly to the pod, bypassing Kubernetes secrets. And similarly to CSI Driver, Hashicorp Injector will load your secrets as volumes. However, in the case of Hashicorp, it will be shared memory volume instead of standard inline volume. \n\n![A picture containing wall, indoor, white, oldDescription automatically generated](/blog-images/3916b4e78a41bb0ec3bf7b5f5bd7e78d.jpeg)\n\n### Keep Your Secrets Safe\n\nAs you can see, Kubernetes secret management isn't as straightforward as one may think. It's not as simple as creating Kubernetes secret resources for your pods. Besides the fact that these secrets are not so secret, you must also consider the whole secret life cycle. Even if base64 encoding is enough in your case, you still need to figure out how to store your Kubernetes secret YAML definitions without exposing them. \n\nIn this post, you learned a few ways to do that—and how to bypass Kubernetes secrets completely and pass your credentials directly to your pods. Your choice of option will depend on your use case and company specifics. \n\nIf you want to learn more about Kubernetes or Security, look at [our blog](https://release.com/blog) for more articles.\n",
          "code": "var Component=(()=>{var h=Object.create;var a=Object.defineProperty;var d=Object.getOwnPropertyDescriptor;var m=Object.getOwnPropertyNames;var p=Object.getPrototypeOf,y=Object.prototype.hasOwnProperty;var b=(s,e)=>()=>(e||s((e={exports:{}}).exports,e),e.exports),g=(s,e)=>{for(var n in e)a(s,n,{get:e[n],enumerable:!0})},c=(s,e,n,o)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let r of m(e))!y.call(s,r)&&r!==n&&a(s,r,{get:()=>e[r],enumerable:!(o=d(e,r))||o.enumerable});return s};var f=(s,e,n)=>(n=s!=null?h(p(s)):{},c(e||!s||!s.__esModule?a(n,\"default\",{value:s,enumerable:!0}):n,s)),w=s=>c(a({},\"__esModule\",{value:!0}),s);var l=b((A,i)=>{i.exports=_jsx_runtime});var K={};g(K,{default:()=>S,frontmatter:()=>v});var t=f(l()),v={title:\"Kubernetes Secrets Management: A Practical Guide\",summary:\"What are Kubernetes secrets? Learn how to do Kubernetes secrets management in this post.\",publishDate:\"Wed Aug 31 2022 15:40:56 GMT+0000 (Coordinated Universal Time)\",author:\"ashley-penney\",readingTime:5,categories:[\"kubernetes\",\"platform-engineering\"],mainImage:\"/blog-images/a805033176a0e06862c60397cda841da.jpg\",imageAlt:\"a laptop on a table\",showCTA:!0,ctaCopy:\"Looking to enhance Kubernetes secrets security? Try Release for managing ephemeral environments securely.\",ctaLink:\"https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=kubernetes-secrets-management-a-practical-guide\",relatedPosts:[\"\"],ogImage:\"/blog-images/a805033176a0e06862c60397cda841da.jpg\",excerpt:\"What are Kubernetes secrets? Learn how to do Kubernetes secrets management in this post.\",tags:[\"kubernetes\",\"platform-engineering\"],ctaButton:\"Try Release for Free\"};function u(s){let e=Object.assign({p:\"p\",h3:\"h3\",a:\"a\",span:\"span\",img:\"img\",strong:\"strong\",pre:\"pre\",code:\"code\"},s.components);return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(e.p,{children:\"If you've worked with Kubernetes, you've probably heard of or used Kubernetes secrets. They are one of many Kubernetes resources. As the name suggests, they're meant to be used with secrets in your cluster. Imagine that your application running in a pod on a Kubernetes cluster needs some credentials.\\xA0\"}),`\n`,(0,t.jsx)(e.p,{children:\"Using a Kubernetes secret is the most straightforward way to provide these credentials to your application. But are they actually secure? What's the best way to use them? Should you use some other secret management solutions for your Kubernetes cluster? Read on to learn everything about Kubernetes secrets management.\\xA0\"}),`\n`,(0,t.jsxs)(e.h3,{id:\"why-is-secret-management-important\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#why-is-secret-management-important\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Why Is Secret Management Important?\"]}),`\n`,(0,t.jsx)(e.p,{children:\"Before we dive into the do's and don'ts of Kubernetes secret management, let's take a moment to discuss why it's important in the first place. You see, Kubernetes secrets are a nice built-in semi-secret management solution, but they are not entirely secret (we'll get to that later), and they don't create a complete secret management solution. The typical problem that quickly arises when you use Kubernetes secrets is how to create and store them securely before they end up in a Kubernetes cluster. Kubernetes doesn't come with any integration to secret vaults out of the box. Therefore, they need a bit more engineering effort beyond simple creation to be secure.\\xA0\"}),`\n`,(0,t.jsx)(e.p,{children:(0,t.jsx)(e.img,{src:\"/blog-images/512dd9da29c34e645475ece9aca536a6.png\",alt:\"TextDescription automatically generated\"})}),`\n`,(0,t.jsxs)(e.h3,{id:\"are-kubernetes-secrets-actually-secure\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#are-kubernetes-secrets-actually-secure\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Are Kubernetes Secrets Actually Secure?\"]}),`\n`,(0,t.jsxs)(e.p,{children:[\"As we mentioned, a critical aspect of Kubernetes secret management is the fact that Kubernetes secrets are not actually that secret. You may be surprised to hear that, but Kubernetes secrets are not encrypted and can be easily read by anyone with access to the cluster. Kubernetes secrets are only encoded using basic \",(0,t.jsx)(e.a,{href:\"https://en.wikipedia.org/wiki/Base64\",children:\"base64\"}),\" format. Let me show you. I'll apply the following YAML definition file of my Kubernetes secret to the cluster using the \",(0,t.jsx)(e.strong,{children:\"kubectl apply\"}),\" command:\\xA0\"]}),`\n`,(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:\"language-yaml\",children:`\n$ cat \\xA0| kubectl apply -f -\napiVersion: v1\nkind: Secret\nmetadata:\n \\xA0name: example-secret\ntype: Opaque\nstringData:\n \\xA0username: admin\n \\xA0password: super_secret_password\nEOF\nsecret/example-secret created\n\n`})}),`\n`,(0,t.jsxs)(e.p,{children:[\"Now that we've created a secret, you'd expect it to be difficult to get the plain text values again from the cluster. If I execute \",(0,t.jsx)(e.strong,{children:\"kubectl describe\"}),\" on our secret value, Kubernetes won't show you the values by default:\\xA0\"]}),`\n`,(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:\"language-yaml\",children:`\nkubectl describe secret example-secret\nName: \\xA0 \\xA0 \\xA0 \\xA0 example-secret\nNamespace: \\xA0 \\xA0default\nLabels: \\xA0 \\xA0 \\xA0\nAnnotations: \\xA0\nType: \\xA0 \\xA0 \\xA0 \\xA0 Opaque\n\nData\n====\npassword: \\xA021 bytes\nusername: \\xA05 bytes\n\n`})}),`\n`,(0,t.jsx)(e.p,{children:\"However, you can force it to show the values as follows:\\xA0\"}),`\n`,(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:\"language-yaml\",children:`$ kubectl get secret example-secret -o jsonpath='{.data}'\n{password:c3VwZXJfc2VjcmV0X3Bhc3N3b3Jk username:YWRtaW4=}\n`})}),`\n`,(0,t.jsxs)(e.p,{children:[\"Now, we can see the values, but as you would expect, they're not in plain text. However, as we mentioned before, the values are in base64, which is very easy to decode using base64 binary that comes installed on all modern operating systems. You only need to pipe the above output to a \",(0,t.jsx)(e.strong,{children:\"base64 --decode\"}),\" command:\\xA0\"]}),`\n`,(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:\"language-yaml\",children:`$ kubectl get secret example-secret -o jsonpath='{.data.username}' | base64 --decode\nadmin\n$ kubectl get secret example-secret -o jsonpath='{.data.password}' | base64 --decode\nsuper_secret_password\n`})}),`\n`,(0,t.jsx)(e.p,{children:\"As you can see, I didn't need to specify any encryption key or certificate. Anyone who has access to my cluster could do the same.\\xA0\"}),`\n`,(0,t.jsxs)(e.h3,{id:\"is-this-a-problem\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#is-this-a-problem\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Is This a Problem?\"]}),`\n`,(0,t.jsx)(e.p,{children:\"Is this a Kubernetes bug or vulnerability? No, not really. Kubernetes is simply not a secret management tool. It allows you to use Kubernetes secrets out of the box to get you started, but if you really want to stay secure, you'd use an external secret management solution. Another aspect of this is that it's possible to make Kubernetes secrets a bit more secure by applying RBAC rules to your cluster.\"}),`\n`,(0,t.jsx)(e.p,{children:(0,t.jsx)(e.img,{src:\"/blog-images/6154afb5901303cb94833156057919d1.jpeg\",alt:\"A picture containing insect, colorful, brightDescription automatically generated\"})}),`\n`,(0,t.jsx)(e.p,{children:\"Also, in non-multi-tenant clusters, it's not that big of an issue since access to the cluster is limited to one team anyway. Everyone who has access to the cluster can probably access the secrets too. So, the fact that Kubernetes secrets are not that secret isn't automatically bad. It simply depends on the use case.\\xA0\"}),`\n`,(0,t.jsxs)(e.p,{children:[\"For customers who use Release in AWS, we automatically assign a KMS key at cluster creation so that their secrets are actually encrypted at rest. If you are managing your own EKS cluster, you can find out how to do that by following \",(0,t.jsx)(e.a,{href:\"https://docs.aws.amazon.com/eks/latest/userguide/enable-kms.html\",children:\"these instructions\"}),\".\"]}),`\n`,(0,t.jsx)(e.p,{children:\"Let's get into how to actually manage secrets in Kubernetes.\\xA0\"}),`\n`,(0,t.jsxs)(e.h3,{id:\"secrets-vs-gitops\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#secrets-vs-gitops\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Secrets vs. GitOps\"]}),`\n`,(0,t.jsxs)(e.p,{children:[\"One of the most common issues regarding secrets in Kubernetes is that you can't simply commit secret YAML definition files to your Git repository. This is because your secret would be there in plain text (or base64-encoded values if you use \",(0,t.jsx)(e.strong,{children:\"data\"}),\" instead of \",(0,t.jsx)(e.strong,{children:\"stringData\"}),\"\\u2014but as we just showed, base64 is easy to decode). And since manually applying secrets would be slow and not scalable, you need to find a way to store your YAML secrets definition securely. There are two popular approaches to doing so. Let's discuss both.\\xA0\"]}),`\n`,(0,t.jsxs)(e.h3,{id:\"external-secrets\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#external-secrets\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"External Secrets\"]}),`\n`,(0,t.jsxs)(e.p,{children:[\"The first option is to use the \",(0,t.jsx)(e.a,{href:\"https://external-secrets.io/\",children:\"External Secrets\"}),\" tool. The idea behind it is quite clever. You store your secret values in a safe secret vault and only commit to Git repository YAML definition files that, instead of having the actual values, hold the reference to them. Then, you install External Secrets Operator on your cluster. And once you apply this reference YAML file, the ESO will go to your secrets vault, grab the true secret value, and create your ordinary Kubernetes secret on your cluster for you. Here's an example external secret resource definition:\\xA0\"]}),`\n`,(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:\"language-yaml\",children:`\napiVersion: external-secrets.io/v1beta1\nkind: ExternalSecret\nmetadata:\n \\xA0name: example\nspec:\n \\xA0refreshInterval: 1h\n \\xA0secretStoreRef:\n \\xA0 \\xA0name: secretstore-sample\n \\xA0 \\xA0kind: SecretStore\n \\xA0target:\n \\xA0 \\xA0name: secret-to-be-created\n \\xA0 \\xA0creationPolicy: Owner\n \\xA0data:\n \\xA0- secretKey: secret-key-to-be-managed\n \\xA0 \\xA0remoteRef:\n \\xA0 \\xA0 \\xA0key: provider-key\n \\xA0 \\xA0 \\xA0version: provider-key-version\n \\xA0 \\xA0 \\xA0property: provider-key-property\n \\xA0dataFrom:\n \\xA0- extract:\n \\xA0 \\xA0 \\xA0key: remote-key-in-the-provider\n\n`})}),`\n`,(0,t.jsx)(e.p,{children:\"As you can see, there are no actual secret values here, just pointers to where that value is. So, if that file were exposed and read by someone that shouldn't read it, they still wouldn't know your actual secrets without getting access to your secrets vault.\\xA0\"}),`\n`,(0,t.jsxs)(e.h3,{id:\"sealedsecrets\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#sealedsecrets\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"SealedSecrets\"]}),`\n`,(0,t.jsxs)(e.p,{children:[\"Another alternative is to use \",(0,t.jsx)(e.a,{href:\"https://github.com/bitnami-labs/sealed-secrets\",children:\"SealedSecrets\"}),\" project. It works differently but achieves the same result. SealedSecrets lets you encrypt the content of your Kubernetes secret YAML definition file. After encrypting, the file can be safely committed to the Git repository. It could even be exposed to the internet because only the SealedSecrets controller running in your cluster will be able to decrypt it. Here's what it looks like:\\xA0\"]}),`\n`,(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:\"language-yaml\",children:`apiVersion: bitnami.com/v1alpha1\nkind: SealedSecret\nmetadata:\n  \\xA0name: mysecret\n  \\xA0namespace: mynamespace\nspec:\n  \\xA0encryptedData:\n  \\xA0 \\xA0foo: AgBy3i4OJSWK+PiTySYZZA9rO43cGDEq.....\n`})}),`\n`,(0,t.jsx)(e.p,{children:\"After the controller decrypts it, SealedSecrets will create an ordinary Kubernetes secret for you. Therefore, you won't need to adjust your application code.\\xA0\"}),`\n`,(0,t.jsxs)(e.h3,{id:\"bypassing-kubernetes-secrets\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#bypassing-kubernetes-secrets\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Bypassing Kubernetes Secrets\"]}),`\n`,(0,t.jsx)(e.p,{children:\"The tools mentioned above have one thing in common: at the end of the day, they still create ordinary Kubernetes secrets. In highly regulated environments with strict security rules, it may be necessary to avoid Kubernetes secrets completely because they're only base64 encoded. In such cases, you need to find another solution for passing secrets to your pods.\\xA0\"}),`\n`,(0,t.jsx)(e.p,{children:(0,t.jsx)(e.img,{src:\"/blog-images/b3fc341ebfb84d4e7f0620414b5a8a24.png\",alt:\"TextDescription automatically generated with medium confidence\"})}),`\n`,(0,t.jsxs)(e.h3,{id:\"kubernetes-secrets-store-csi-driver\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#kubernetes-secrets-store-csi-driver\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Kubernetes Secrets Store CSI Driver\"]}),`\n`,(0,t.jsxs)(e.p,{children:[\"One option is to use Kubernetes's own new \",(0,t.jsx)(e.a,{href:\"https://secrets-store-csi-driver.sigs.k8s.io/\",children:\"Secrets Store CSI Driver\"}),\". With this option, you store your secrets in the external secrets store. Then Secrets Store CSI Driver will load them from there and mount them directly to your pods as volumes. Therefore, you'll bypass Kubernetes secrets resources completely.\\xA0\"]}),`\n`,(0,t.jsxs)(e.p,{children:[\"However, there are two downsides to this approach. First is the fact that you'll need to adjust your application to load secrets from files instead of from \",(0,t.jsx)(e.a,{href:\"https://release.com/blog/kubernetes-environment-variables\",children:\"environment variables\"}),\" like with normal Kubernetes secrets. Second, Secrets Store CSI Driver currently has alpha functionality, so it may not be fully stable.\\xA0\"]}),`\n`,(0,t.jsxs)(e.h3,{id:\"hashicorp-vault-injector\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#hashicorp-vault-injector\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Hashicorp Vault Injector\"]}),`\n`,(0,t.jsxs)(e.p,{children:[\"Another option is to use Hashicorp Vault together with their \",(0,t.jsx)(e.a,{href:\"https://www.vaultproject.io/docs/platform/k8s/injector\",children:\"Secret Injection\"}),\" option. The concept is similar to the Kubernetes Secrets Store CSI Driver. You store your secrets in Hashicorp Vault. Then, Hashicorp Vault Agent Injector will get the secrets for you and load them directly to the pod, bypassing Kubernetes secrets. And similarly to CSI Driver, Hashicorp Injector will load your secrets as volumes. However, in the case of Hashicorp, it will be shared memory volume instead of standard inline volume.\\xA0\"]}),`\n`,(0,t.jsx)(e.p,{children:(0,t.jsx)(e.img,{src:\"/blog-images/3916b4e78a41bb0ec3bf7b5f5bd7e78d.jpeg\",alt:\"A picture containing wall, indoor, white, oldDescription automatically generated\"})}),`\n`,(0,t.jsxs)(e.h3,{id:\"keep-your-secrets-safe\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#keep-your-secrets-safe\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Keep Your Secrets Safe\"]}),`\n`,(0,t.jsx)(e.p,{children:\"As you can see, Kubernetes secret management isn't as straightforward as one may think. It's not as simple as creating Kubernetes secret resources for your pods. Besides the fact that these secrets are not so secret, you must also consider the whole secret life cycle. Even if base64 encoding is enough in your case, you still need to figure out how to store your Kubernetes secret YAML definitions without exposing them.\\xA0\"}),`\n`,(0,t.jsx)(e.p,{children:\"In this post, you learned a few ways to do that\\u2014and how to bypass Kubernetes secrets completely and pass your credentials directly to your pods. Your choice of option will depend on your use case and company specifics.\\xA0\"}),`\n`,(0,t.jsxs)(e.p,{children:[\"If you want to learn more about Kubernetes or Security, look at \",(0,t.jsx)(e.a,{href:\"https://release.com/blog\",children:\"our blog\"}),\" for more articles.\"]})]})}function k(s={}){let{wrapper:e}=s.components||{};return e?(0,t.jsx)(e,Object.assign({},s,{children:(0,t.jsx)(u,s)})):u(s)}var S=k;return w(K);})();\n;return Component;"
        },
        "_id": "blog/posts/kubernetes-secrets-management-a-practical-guide.mdx",
        "_raw": {
          "sourceFilePath": "blog/posts/kubernetes-secrets-management-a-practical-guide.mdx",
          "sourceFileName": "kubernetes-secrets-management-a-practical-guide.mdx",
          "sourceFileDir": "blog/posts",
          "contentType": "mdx",
          "flattenedPath": "blog/posts/kubernetes-secrets-management-a-practical-guide"
        },
        "type": "BlogPost",
        "computedSlug": "kubernetes-secrets-management-a-practical-guide"
      },
      "documentHash": "1739393595023",
      "hasWarnings": false,
      "documentTypeName": "BlogPost"
    },
    "blog/posts/kubernetes-statefulset-when-and-how-to-use-it.mdx": {
      "document": {
        "title": "Kubernetes StatefulSet: When and How to Use It",
        "summary": "At first glance, StatefulSets are very similar to standard Deployments, but there are some important differences.",
        "publishDate": "Wed Sep 07 2022 16:19:50 GMT+0000 (Coordinated Universal Time)",
        "author": "nick-busey",
        "readingTime": 5,
        "categories": [
          "kubernetes",
          "platform-engineering"
        ],
        "mainImage": "/blog-images/029fddb95501b95dac344adedfa94257.jpg",
        "imageAlt": "a cell phone with the screen saying eat, sleep, code, repeat",
        "showCTA": true,
        "ctaCopy": "Looking to manage stateful applications in Kubernetes like a pro? Try Release.com for seamless environment provisioning and testing.",
        "ctaLink": "https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=kubernetes-statefulset-when-and-how-to-use-it",
        "relatedPosts": [
          ""
        ],
        "ogImage": "/blog-images/029fddb95501b95dac344adedfa94257.jpg",
        "excerpt": "At first glance, StatefulSets are very similar to standard Deployments, but there are some important differences.",
        "tags": [
          "kubernetes",
          "platform-engineering"
        ],
        "ctaButton": "Try Release for Free",
        "body": {
          "raw": "\n‍[Kubernetes](https://en.wikipedia.org/wiki/Kubernetes) was designed with stateless microservices in mind. But these days, it also comes with support for stateful applications, which is especially handy if you want to migrate your applications gradually. At first glance, StatefulSets are very similar to standard Kubernetes Deployments, but there are some important differences. In this post, you'll learn what StatefulSets actually are and when and how to use them. \n\n### What Are Stateful Applications?\n\nBefore we start explaining Kubernetes StatefulSets, you need to understand what stateful means and the difference between stateless and stateful applications. When you think about cloud-native applications, you most likely have a picture of an application that can run in multiple copies and where any copy can be restarted at any time while traffic is being redirected effortlessly to other instances. \n\nIn order for this model to work, the application needs to get some data from somewhere, execute some functions, and return the data. It can't store the data itself, and it shouldn't be dependent on other pods. If it were, you wouldn't be able to easily kill that instance without risking data loss. But in general, if an application doesn't store data itself in persistent storage and doesn't need to be started together with other microservices in a specific order, then it's stateless. \n\n#### Stateful vs Stateless\n\nAnd as you can probably guess, stateful applications are the opposite. They do need to keep some data in order to work. The most common example of a stateful application is a database. The whole point of, for example, MongoDB or MySQL applications is to store data. Therefore, both MongoDB and MySQL are stateful. You can't simply kill the instance of MongoDB and restart it somewhere else and expect it to work. \n\nFirst of all, by killing it unexpectedly, the data may get corrupted. And second, you can't simply restart MongoDB somewhere else because you need to first somehow reference the same data for it, which usually means either copying data or attaching the same persistent storage to it. \n\nUsing persistent data is not the only thing that can make an application stateful. If your microservice doesn't store any data but needs to be started in a specific order with other microservices, then it's also stateful. Or if you can't simply roll out a new version of the application because you also need to follow specific update procedures, then your application is most likely stateful. \n\nNow that you have that clear, let's talk about Kubernetes StatefulSets. \n\n![Graphical user interface, text, applicationDescription automatically generated](/blog-images/f8993a6902e8b36420f0fc82ff271a93.png)\n\n#### Kubernetes StatefulSet\n\nTraditionally, a normal Kubernetes Deployment assumes that your application is stateless. Therefore, Kubernetes may, at any point, just kill one of your instances and redeploy it elsewhere on the cluster when necessary. If your application is stateful, this could easily create an issue. You would either end up with corrupted data or your application could simply crash and require manual intervention. \n\nTherefore, specifically for stateful applications, Kubernetes offers so-called StatefulSets. These are special Kubernetes objects that will create and manage pods for your stateful application. Unlike in a standard Deployment, StatefulSets are aware that your application is stateful and will therefore treat it accordingly. \n\n#### Stable And Ordered\n\nKubernetes StatefulSets provide two main advantages (for stateful applications) over Deployments: a stable identity of the pods and the ability to follow specific Deployment orders. \n\nStable identity means persistent identity in this case. And persistent pod identity means that when a pod gets rescheduled for whatever reason, it will have the same network identifiers and the same storage assigned to it. So, from the perspective of other pods, it will look like the same pod. This is not the case when using Deployments, and it's very important for the proper working of stateful applications. \n\nWe already mentioned that if your application needs to be deployed or updated in a specific order, that's a good indication that it's stateful. In a traditional Deployment, if you'll have multiple pods in one Deployment, they would be deployed in a random order, which in the case of stateful application would probably mean that the application won't start properly. And therefore, this ability to follow a specific order when deploying or updating is built into the StatefulSets. \n\n![Graphical user interface, text, applicationDescription automatically generated](/blog-images/c340926b66f892274a9e48f60dc44fef.png)\n\n#### Creating StatefulSet\n\nEnough theory. Let's create some StatefulSets. The YAML definition of StatefulSets is very similar to standard Deployments and in a simple example looks like this: \n\n```yaml\napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n   name: example-statefulset\nspec:\n   selector:\n     matchLabels:\n       app: nginx\n   replicas: 1\n   serviceName: nginx\n   template:\n     metadata:\n       labels:\n         app: nginx\n     spec:\n       containers:\n       - name: nginx\n         image: registry.k8s.io/nginx-slim:0.8\n         ports:\n         - containerPort: 80\n           name: web\n```\n\nOnce you save the above code in a YAML file, you can deploy it, as usual, using **kubectl apply**: \n\n```yaml\n$ kubectl apply -f statefulset.yaml\nstatefulset.apps/example-statefulset created\n```\n\nYou can then validate that everything is working with **kubectl get**: \n\n```yaml\n$ kubectl get statefulsets\nNAME                  READY   AGE\nexample-statefulset   1/1     2m4s\n\n$ kubectl get pods\nNAME                    READY   STATUS    RESTARTS   AGE\nexample-statefulset-0   1/1     Running   0          2m8s\n```\n\nOK, your first StatefulSet is up and running. Congratulations. This was, however, a very simple example with only one pod in your StatefulSet. But you'll most likely use StatefulSets with multiple pods to get all the benefits. \n\n![TextDescription automatically generated with medium confidence](/blog-images/5b3db806e95be1ac41e59ed74e466982.jpeg)\n\n### StatefulSets Specifics\n\nLet's spice things up a little to see StatefulSets doing its job. Execute the following command to scale your nginx from one to ten replicas: \n\n```yaml\n$ kubectl scale statefulsets example-statefulset --replicas=10\nstatefulset.apps/example-statefulset scaled\n```\n\nNow, if you watch what's happening, you'll see the specific behavior of StatefulSets: \n\n```yaml\n$ kubectl get pods\nNAME                    READY   STATUS              RESTARTS   AGE\nexample-statefulset-0   1/1     Running             0          11m\nexample-statefulset-1   0/1     ContainerCreating   0          1s\n\n$ kubectl get pods\nNAME                    READY   STATUS              RESTARTS   AGE\nexample-statefulset-0   1/1     Running             0          11m\nexample-statefulset-1   1/1     Running             0          2s\nexample-statefulset-2   0/1     ContainerCreating   0          0s\n\n$ kubectl get pods\nNAME                    READY   STATUS              RESTARTS   AGE\nexample-statefulset-0   1/1     Running             0          11m\nexample-statefulset-1   1/1     Running             0          4s\nexample-statefulset-2   1/1     Running             0          2s\nexample-statefulset-3   0/1     ContainerCreating   0          1s\n\n(...)\n\n$ kubectl get pods\nNAME                    READY   STATUS    RESTARTS   AGE\nexample-statefulset-0   1/1     Running   0          14m\nexample-statefulset-1   1/1     Running   0          3m19s\nexample-statefulset-2   1/1     Running   0          3m17s\nexample-statefulset-3   1/1     Running   0          3m16s\nexample-statefulset-4   1/1     Running   0          3m14s\nexample-statefulset-5   1/1     Running   0          3m13s\nexample-statefulset-6   1/1     Running   0          3m12s\nexample-statefulset-7   1/1     Running   0          3m10s\nexample-statefulset-8   1/1     Running   0          3m9s\nexample-statefulset-9   1/1     Running   0          3m7s\n```\n\nYou can see that Kubernetes provisioned all replicas in order, one by one. This is one of the differences between Deployments and StatefulSets. In Deployments, all pods will be deployed in random order, with more than one pod being created at once. In StatefulSets, it happens sequentially, and pods are even numbered and do not get a random hash assigned as part of the name, which is the case in Deployments. Moreover, if at any point one of the replicas fails to start, the whole process will stop. So, for example, Kubernetes will only create **example-statefulset-5** after **example-statefulset-4** is up and running. \n\n### Name Stays the Same\n\nFollowing the same logic, if something happens to any of the pods, it will be recreated with the same name. \n\n```\n\n$ kubectl delete pod example-statefulset-3\npod \"example-statefulset-3\" deleted\n\n$ kubectl get pods\nNAME                    READY   STATUS    RESTARTS   AGE\nexample-statefulset-0   1/1     Running   0          20m\nexample-statefulset-1   1/1     Running   0          9m41s\nexample-statefulset-2   1/1     Running   0          9m39s\nexample-statefulset-4   1/1     Running   0          9m36s\nexample-statefulset-5   1/1     Running   0          9m35s\nexample-statefulset-6   1/1     Running   0          9m34s\nexample-statefulset-7   1/1     Running   0          9m32s\nexample-statefulset-8   1/1     Running   0          9m31s\nexample-statefulset-9   1/1     Running   0          9m29s\nexample-statefulset-3   1/1     Running   0          1s\n\n```\n\nThis, again, differs from Deployments, where you'd get another randomly named pod. This is important for stateful applications because, most likely, each pod will hold its own state. Therefore, it's crucial not to mix them up. Also, other microservices that would connect to these pods will probably need to always connect to the same pod even if it dies and is rescheduled. \n\nThe same applies to networking. You can always connect to a specific pod by its domain name, like **example-statefulset-6.nginx.default.svc.cluster.local,** and you'll have a guarantee that you'll always reach the same pod. That's not the case with Deployments. \n\n### Summary\n\nKubernetes StatefulSets are really useful. In theory, using them means doing something that Kubernetes wasn't designed to work with in the first place. But it's really hard to have every single application on your cluster stateless. Especially in big environments with dozens or even hundreds of applications, there will always be some microservice that needs to hold some state. In some cases, it simply doesn't make sense to spend time and money on redesigning a stateless application to be stateful if it won't bring much difference or business value. \n\nIn this post, you learned what [StatefulSets](https://docs.releasehub.com/reference-documentation/application-settings/application-template/schema-definition#stateful-sets-and-deployments) are and how to create them. If you want to learn more about other Kubernetes resources, take a look at [our blog](https://release.com/blog).\n",
          "code": "var Component=(()=>{var d=Object.create;var i=Object.defineProperty;var u=Object.getOwnPropertyDescriptor;var h=Object.getOwnPropertyNames;var m=Object.getPrototypeOf,f=Object.prototype.hasOwnProperty;var g=(a,e)=>()=>(e||a((e={exports:{}}).exports,e),e.exports),y=(a,e)=>{for(var n in e)i(a,n,{get:e[n],enumerable:!0})},o=(a,e,n,l)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let s of h(e))!f.call(a,s)&&s!==n&&i(a,s,{get:()=>e[s],enumerable:!(l=u(e,s))||l.enumerable});return a};var b=(a,e,n)=>(n=a!=null?d(m(a)):{},o(e||!a||!a.__esModule?i(n,\"default\",{value:a,enumerable:!0}):n,a)),w=a=>o(i({},\"__esModule\",{value:!0}),a);var c=g((R,r)=>{r.exports=_jsx_runtime});var v={};y(v,{default:()=>k,frontmatter:()=>S});var t=b(c()),S={title:\"Kubernetes StatefulSet: When and How to Use It\",summary:\"At first glance, StatefulSets are very similar to standard Deployments, but there are some important differences.\",publishDate:\"Wed Sep 07 2022 16:19:50 GMT+0000 (Coordinated Universal Time)\",author:\"nick-busey\",readingTime:5,categories:[\"kubernetes\",\"platform-engineering\"],mainImage:\"/blog-images/029fddb95501b95dac344adedfa94257.jpg\",imageAlt:\"a cell phone with the screen saying eat, sleep, code, repeat\",showCTA:!0,ctaCopy:\"Looking to manage stateful applications in Kubernetes like a pro? Try Release.com for seamless environment provisioning and testing.\",ctaLink:\"https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=kubernetes-statefulset-when-and-how-to-use-it\",relatedPosts:[\"\"],ogImage:\"/blog-images/029fddb95501b95dac344adedfa94257.jpg\",excerpt:\"At first glance, StatefulSets are very similar to standard Deployments, but there are some important differences.\",tags:[\"kubernetes\",\"platform-engineering\"],ctaButton:\"Try Release for Free\"};function p(a){let e=Object.assign({p:\"p\",a:\"a\",h3:\"h3\",span:\"span\",h4:\"h4\",img:\"img\",pre:\"pre\",code:\"code\",strong:\"strong\"},a.components);return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsxs)(e.p,{children:[\"\\u200D\",(0,t.jsx)(e.a,{href:\"https://en.wikipedia.org/wiki/Kubernetes\",children:\"Kubernetes\"}),\" was designed with stateless microservices in mind. But these days, it also comes with support for stateful applications, which is especially handy if you want to migrate your applications gradually. At first glance, StatefulSets are very similar to standard Kubernetes Deployments, but there are some important differences. In this post, you'll learn what StatefulSets actually are and when and how to use them.\\xA0\"]}),`\n`,(0,t.jsxs)(e.h3,{id:\"what-are-stateful-applications\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#what-are-stateful-applications\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"What Are Stateful Applications?\"]}),`\n`,(0,t.jsx)(e.p,{children:\"Before we start explaining Kubernetes StatefulSets, you need to understand what stateful means and the difference between stateless and stateful applications. When you think about cloud-native applications, you most likely have a picture of an application that can run in multiple copies and where any copy can be restarted at any time while traffic is being redirected effortlessly to other instances.\\xA0\"}),`\n`,(0,t.jsx)(e.p,{children:\"In order for this model to work, the application needs to get some data from somewhere, execute some functions, and return the data. It can't store the data itself, and it shouldn't be dependent on other pods. If it were, you wouldn't be able to easily kill that instance without risking data loss. But in general, if an application doesn't store data itself in persistent storage and doesn't need to be started together with other microservices in a specific order, then it's stateless.\\xA0\"}),`\n`,(0,t.jsxs)(e.h4,{id:\"stateful-vs-stateless\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#stateful-vs-stateless\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Stateful vs Stateless\"]}),`\n`,(0,t.jsx)(e.p,{children:\"And as you can probably guess, stateful applications are the opposite. They do need to keep some data in order to work. The most common example of a stateful application is a database. The whole point of, for example, MongoDB or MySQL applications is to store data. Therefore, both MongoDB and MySQL are stateful. You can't simply kill the instance of MongoDB and restart it somewhere else and expect it to work.\\xA0\"}),`\n`,(0,t.jsx)(e.p,{children:\"First of all, by killing it unexpectedly, the data may get corrupted. And second, you can't simply restart MongoDB somewhere else because you need to first somehow reference the same data for it, which usually means either copying data or attaching the same persistent storage to it.\\xA0\"}),`\n`,(0,t.jsx)(e.p,{children:\"Using persistent data is not the only thing that can make an application stateful. If your microservice doesn't store any data but needs to be started in a specific order with other microservices, then it's also stateful. Or if you can't simply roll out a new version of the application because you also need to follow specific update procedures, then your application is most likely stateful.\\xA0\"}),`\n`,(0,t.jsx)(e.p,{children:\"Now that you have that clear, let's talk about Kubernetes StatefulSets.\\xA0\"}),`\n`,(0,t.jsx)(e.p,{children:(0,t.jsx)(e.img,{src:\"/blog-images/f8993a6902e8b36420f0fc82ff271a93.png\",alt:\"Graphical user interface, text, applicationDescription automatically generated\"})}),`\n`,(0,t.jsxs)(e.h4,{id:\"kubernetes-statefulset\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#kubernetes-statefulset\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Kubernetes StatefulSet\"]}),`\n`,(0,t.jsx)(e.p,{children:\"Traditionally, a normal Kubernetes Deployment assumes that your application is stateless. Therefore, Kubernetes may, at any point, just kill one of your instances and redeploy it elsewhere on the cluster when necessary. If your application is stateful, this could easily create an issue. You would either end up with corrupted data or your application could simply crash and require manual intervention.\\xA0\"}),`\n`,(0,t.jsx)(e.p,{children:\"Therefore, specifically for stateful applications, Kubernetes offers so-called StatefulSets. These are special Kubernetes objects that will create and manage pods for your stateful application. Unlike in a standard Deployment, StatefulSets are aware that your application is stateful and will therefore treat it accordingly.\\xA0\"}),`\n`,(0,t.jsxs)(e.h4,{id:\"stable-and-ordered\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#stable-and-ordered\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Stable And Ordered\"]}),`\n`,(0,t.jsx)(e.p,{children:\"Kubernetes StatefulSets provide two main advantages (for stateful applications) over Deployments: a stable identity of the pods and the ability to follow specific Deployment orders.\\xA0\"}),`\n`,(0,t.jsx)(e.p,{children:\"Stable identity means persistent identity in this case. And persistent pod identity means that when a pod gets rescheduled for whatever reason, it will have the same network identifiers and the same storage assigned to it. So, from the perspective of other pods, it will look like the same pod. This is not the case when using Deployments, and it's very important for the proper working of stateful applications.\\xA0\"}),`\n`,(0,t.jsx)(e.p,{children:\"We already mentioned that if your application needs to be deployed or updated in a specific order, that's a good indication that it's stateful. In a traditional Deployment, if you'll have multiple pods in one Deployment, they would be deployed in a random order, which in the case of stateful application would probably mean that the application won't start properly. And therefore, this ability to follow a specific order when deploying or updating is built into the StatefulSets.\\xA0\"}),`\n`,(0,t.jsx)(e.p,{children:(0,t.jsx)(e.img,{src:\"/blog-images/c340926b66f892274a9e48f60dc44fef.png\",alt:\"Graphical user interface, text, applicationDescription automatically generated\"})}),`\n`,(0,t.jsxs)(e.h4,{id:\"creating-statefulset\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#creating-statefulset\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Creating StatefulSet\"]}),`\n`,(0,t.jsx)(e.p,{children:\"Enough theory. Let's create some StatefulSets. The YAML definition of StatefulSets is very similar to standard Deployments and in a simple example looks like this:\\xA0\"}),`\n`,(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:\"language-yaml\",children:`apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  \\xA0name: example-statefulset\nspec:\n  \\xA0selector:\n  \\xA0 \\xA0matchLabels:\n  \\xA0 \\xA0 \\xA0app: nginx\n  \\xA0replicas: 1\n  \\xA0serviceName: nginx\n  \\xA0template:\n  \\xA0 \\xA0metadata:\n  \\xA0 \\xA0 \\xA0labels:\n  \\xA0 \\xA0 \\xA0 \\xA0app: nginx\n  \\xA0 \\xA0spec:\n  \\xA0 \\xA0 \\xA0containers:\n  \\xA0 \\xA0 \\xA0- name: nginx\n  \\xA0 \\xA0 \\xA0 \\xA0image: registry.k8s.io/nginx-slim:0.8\n  \\xA0 \\xA0 \\xA0 \\xA0ports:\n  \\xA0 \\xA0 \\xA0 \\xA0- containerPort: 80\n  \\xA0 \\xA0 \\xA0 \\xA0 \\xA0name: web\n`})}),`\n`,(0,t.jsxs)(e.p,{children:[\"Once you save the above code in a YAML file, you can deploy it, as usual, using \",(0,t.jsx)(e.strong,{children:\"kubectl apply\"}),\":\\xA0\"]}),`\n`,(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:\"language-yaml\",children:`$ kubectl apply -f statefulset.yaml\nstatefulset.apps/example-statefulset created\n`})}),`\n`,(0,t.jsxs)(e.p,{children:[\"You can then validate that everything is working with \",(0,t.jsx)(e.strong,{children:\"kubectl get\"}),\":\\xA0\"]}),`\n`,(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:\"language-yaml\",children:`$ kubectl get statefulsets\nNAME \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0READY \\xA0 AGE\nexample-statefulset \\xA0 1/1 \\xA0 \\xA0 2m4s\n\n$ kubectl get pods\nNAME \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0READY \\xA0 STATUS \\xA0 \\xA0RESTARTS \\xA0 AGE\nexample-statefulset-0 \\xA0 1/1 \\xA0 \\xA0 Running \\xA0 0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA02m8s\n`})}),`\n`,(0,t.jsx)(e.p,{children:\"OK, your first StatefulSet is up and running. Congratulations. This was, however, a very simple example with only one pod in your StatefulSet. But you'll most likely use StatefulSets with multiple pods to get all the benefits.\\xA0\"}),`\n`,(0,t.jsx)(e.p,{children:(0,t.jsx)(e.img,{src:\"/blog-images/5b3db806e95be1ac41e59ed74e466982.jpeg\",alt:\"TextDescription automatically generated with medium confidence\"})}),`\n`,(0,t.jsxs)(e.h3,{id:\"statefulsets-specifics\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#statefulsets-specifics\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"StatefulSets Specifics\"]}),`\n`,(0,t.jsx)(e.p,{children:\"Let's spice things up a little to see StatefulSets doing its job. Execute the following command to scale your nginx from one to ten replicas:\\xA0\"}),`\n`,(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:\"language-yaml\",children:`$ kubectl scale statefulsets example-statefulset --replicas=10\nstatefulset.apps/example-statefulset scaled\n`})}),`\n`,(0,t.jsx)(e.p,{children:\"Now, if you watch what's happening, you'll see the specific behavior of StatefulSets:\\xA0\"}),`\n`,(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:\"language-yaml\",children:`$ kubectl get pods\nNAME \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0READY \\xA0 STATUS \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0RESTARTS \\xA0 AGE\nexample-statefulset-0 \\xA0 1/1 \\xA0 \\xA0 Running \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA011m\nexample-statefulset-1 \\xA0 0/1 \\xA0 \\xA0 ContainerCreating \\xA0 0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA01s\n\n$ kubectl get pods\nNAME \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0READY \\xA0 STATUS \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0RESTARTS \\xA0 AGE\nexample-statefulset-0 \\xA0 1/1 \\xA0 \\xA0 Running \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA011m\nexample-statefulset-1 \\xA0 1/1 \\xA0 \\xA0 Running \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA02s\nexample-statefulset-2 \\xA0 0/1 \\xA0 \\xA0 ContainerCreating \\xA0 0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA00s\n\n$ kubectl get pods\nNAME \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0READY \\xA0 STATUS \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0RESTARTS \\xA0 AGE\nexample-statefulset-0 \\xA0 1/1 \\xA0 \\xA0 Running \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA011m\nexample-statefulset-1 \\xA0 1/1 \\xA0 \\xA0 Running \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA04s\nexample-statefulset-2 \\xA0 1/1 \\xA0 \\xA0 Running \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA02s\nexample-statefulset-3 \\xA0 0/1 \\xA0 \\xA0 ContainerCreating \\xA0 0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA01s\n\n(...)\n\n$ kubectl get pods\nNAME \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0READY \\xA0 STATUS \\xA0 \\xA0RESTARTS \\xA0 AGE\nexample-statefulset-0 \\xA0 1/1 \\xA0 \\xA0 Running \\xA0 0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA014m\nexample-statefulset-1 \\xA0 1/1 \\xA0 \\xA0 Running \\xA0 0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA03m19s\nexample-statefulset-2 \\xA0 1/1 \\xA0 \\xA0 Running \\xA0 0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA03m17s\nexample-statefulset-3 \\xA0 1/1 \\xA0 \\xA0 Running \\xA0 0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA03m16s\nexample-statefulset-4 \\xA0 1/1 \\xA0 \\xA0 Running \\xA0 0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA03m14s\nexample-statefulset-5 \\xA0 1/1 \\xA0 \\xA0 Running \\xA0 0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA03m13s\nexample-statefulset-6 \\xA0 1/1 \\xA0 \\xA0 Running \\xA0 0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA03m12s\nexample-statefulset-7 \\xA0 1/1 \\xA0 \\xA0 Running \\xA0 0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA03m10s\nexample-statefulset-8 \\xA0 1/1 \\xA0 \\xA0 Running \\xA0 0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA03m9s\nexample-statefulset-9 \\xA0 1/1 \\xA0 \\xA0 Running \\xA0 0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA03m7s\n`})}),`\n`,(0,t.jsxs)(e.p,{children:[\"You can see that Kubernetes provisioned all replicas in order, one by one. This is one of the differences between Deployments and StatefulSets. In Deployments, all pods will be deployed in random order, with more than one pod being created at once. In StatefulSets, it happens sequentially, and pods are even numbered and do not get a random hash assigned as part of the name, which is the case in Deployments. Moreover, if at any point one of the replicas fails to start, the whole process will stop. So, for example, Kubernetes will only create \",(0,t.jsx)(e.strong,{children:\"example-statefulset-5\"}),\" after \",(0,t.jsx)(e.strong,{children:\"example-statefulset-4\"}),\" is up and running.\\xA0\"]}),`\n`,(0,t.jsxs)(e.h3,{id:\"name-stays-the-same\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#name-stays-the-same\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Name Stays the Same\"]}),`\n`,(0,t.jsx)(e.p,{children:\"Following the same logic, if something happens to any of the pods, it will be recreated with the same name.\\xA0\"}),`\n`,(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{children:`\n$ kubectl delete pod example-statefulset-3\npod \"example-statefulset-3\" deleted\n\n$ kubectl get pods\nNAME \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0READY \\xA0 STATUS \\xA0 \\xA0RESTARTS \\xA0 AGE\nexample-statefulset-0 \\xA0 1/1 \\xA0 \\xA0 Running \\xA0 0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA020m\nexample-statefulset-1 \\xA0 1/1 \\xA0 \\xA0 Running \\xA0 0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA09m41s\nexample-statefulset-2 \\xA0 1/1 \\xA0 \\xA0 Running \\xA0 0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA09m39s\nexample-statefulset-4 \\xA0 1/1 \\xA0 \\xA0 Running \\xA0 0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA09m36s\nexample-statefulset-5 \\xA0 1/1 \\xA0 \\xA0 Running \\xA0 0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA09m35s\nexample-statefulset-6 \\xA0 1/1 \\xA0 \\xA0 Running \\xA0 0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA09m34s\nexample-statefulset-7 \\xA0 1/1 \\xA0 \\xA0 Running \\xA0 0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA09m32s\nexample-statefulset-8 \\xA0 1/1 \\xA0 \\xA0 Running \\xA0 0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA09m31s\nexample-statefulset-9 \\xA0 1/1 \\xA0 \\xA0 Running \\xA0 0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA09m29s\nexample-statefulset-3 \\xA0 1/1 \\xA0 \\xA0 Running \\xA0 0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA01s\n\n`})}),`\n`,(0,t.jsx)(e.p,{children:\"This, again, differs from Deployments, where you'd get another randomly named pod. This is important for stateful applications because, most likely, each pod will hold its own state. Therefore, it's crucial not to mix them up. Also, other microservices that would connect to these pods will probably need to always connect to the same pod even if it dies and is rescheduled.\\xA0\"}),`\n`,(0,t.jsxs)(e.p,{children:[\"The same applies to networking. You can always connect to a specific pod by its domain name, like \",(0,t.jsx)(e.strong,{children:\"example-statefulset-6.nginx.default.svc.cluster.local,\"}),\" and you'll have a guarantee that you'll always reach the same pod. That's not the case with Deployments.\\xA0\"]}),`\n`,(0,t.jsxs)(e.h3,{id:\"summary\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#summary\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Summary\"]}),`\n`,(0,t.jsx)(e.p,{children:\"Kubernetes StatefulSets are really useful. In theory, using them means doing something that Kubernetes wasn't designed to work with in the first place. But it's really hard to have every single application on your cluster stateless. Especially in big environments with dozens or even hundreds of applications, there will always be some microservice that needs to hold some state. In some cases, it simply doesn't make sense to spend time and money on redesigning a stateless application to be stateful if it won't bring much difference or business value.\\xA0\"}),`\n`,(0,t.jsxs)(e.p,{children:[\"In this post, you learned what \",(0,t.jsx)(e.a,{href:\"https://docs.releasehub.com/reference-documentation/application-settings/application-template/schema-definition#stateful-sets-and-deployments\",children:\"StatefulSets\"}),\" are and how to create them. If you want to learn more about other Kubernetes resources, take a look at \",(0,t.jsx)(e.a,{href:\"https://release.com/blog\",children:\"our blog\"}),\".\"]})]})}function x(a={}){let{wrapper:e}=a.components||{};return e?(0,t.jsx)(e,Object.assign({},a,{children:(0,t.jsx)(p,a)})):p(a)}var k=x;return w(v);})();\n;return Component;"
        },
        "_id": "blog/posts/kubernetes-statefulset-when-and-how-to-use-it.mdx",
        "_raw": {
          "sourceFilePath": "blog/posts/kubernetes-statefulset-when-and-how-to-use-it.mdx",
          "sourceFileName": "kubernetes-statefulset-when-and-how-to-use-it.mdx",
          "sourceFileDir": "blog/posts",
          "contentType": "mdx",
          "flattenedPath": "blog/posts/kubernetes-statefulset-when-and-how-to-use-it"
        },
        "type": "BlogPost",
        "computedSlug": "kubernetes-statefulset-when-and-how-to-use-it"
      },
      "documentHash": "1739393595024",
      "hasWarnings": false,
      "documentTypeName": "BlogPost"
    },
    "blog/posts/kubernetes-volumes-what-they-are-and-how-to-use-them.mdx": {
      "document": {
        "title": "Kubernetes Volumes: What They Are and How to Use Them",
        "summary": "Learn how to inject file systems into Kubernetes pods using volumes and talk about the different types of their uses.",
        "publishDate": "Tue Sep 13 2022 18:26:07 GMT+0000 (Coordinated Universal Time)",
        "author": "erik-landerholm",
        "readingTime": 5,
        "categories": [
          "kubernetes",
          "platform-engineering"
        ],
        "mainImage": "/blog-images/e2fad2ca4879f4bfe48dc3b52c46df72.jpg",
        "imageAlt": "A table with a cell phone, pencil, glasses and a plant",
        "showCTA": true,
        "ctaCopy": "Discover how Release.com's dynamic environments streamline Kubernetes volume management for persistent data storage.",
        "ctaLink": "https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=kubernetes-volumes-what-they-are-and-how-to-use-them",
        "relatedPosts": [
          ""
        ],
        "ogImage": "/blog-images/e2fad2ca4879f4bfe48dc3b52c46df72.jpg",
        "excerpt": "Learn how to inject file systems into Kubernetes pods using volumes and talk about the different types of their uses.",
        "tags": [
          "kubernetes",
          "platform-engineering"
        ],
        "ctaButton": "Try Release for Free",
        "body": {
          "raw": "\nBy default, the file system available to a Kubernetes pod is limited to the pod's lifetime. As such, when the pod is deleted, all changes are lost.\n\nBut many applications will need to store data persistently, irrespective of whether a pod is running or not. For example, we need to retain data that was updated in the database or files written. Also, we may want to share a file system across multiple containers, and those may be running on different nodes.\n\nLet's take a look at Kubernetes [volumes](https://kubernetes.io/docs/concepts/storage/volumes/), which can address these problems.\n\n### The Basics\n\nMost data storage that applications use is ultimately file system-based, e.g., even though a database may keep some or all of its data in memory while running, it also keeps it updated in the data files on the file system for persistence. \n\nVolumes allow us to inject the application with a reference to a file system, which the application can then read from or write to. \n\nInjecting the file system makes it independent of the container's lifetime. We need to specify an absolute path where the injected file system should be mounted within the container's file system. \n\nVolumes may be persistent or not. There are many different types of volumes, as we shall see. \n\nA volume has to first be defined using the **volumes** key, and then used by a container using the **volumeMounts** key. \n\n#### Example\n\nBelow is a partial YAML snippet to illustrate how we can define and use volumes in a pod. Depending on the type of volume, its definition and usage could be in separate places. \n\n```yaml\napiVersion: v0\nkind: Pod\nmetadata:\n   name: my-pod\nspec:\n   containers:\n   - image: some-image-name\n     name: my-container\n     volumeMounts:\n     - mountPath: /tempfiles\n       name: temp-files-volume\n   volumes:\n   - name: temp-files-volume\n     emptyDir: {}\n```\n\nHere, we've defined a volume of the **emptyDir** type. We'll see more about this later. \n\nSince this type can only be used at the level of a single pod, not across, it's defined along with the pod. There could be multiple containers in a pod (though usually not), and they could all use the same volume. \n\nSo, if one container in a pod writes a new file to the volume, it would be visible to the other containers in that pod that use that volume. The name of the volume can be anything. \n\nThe **volumeMounts** entry under the container specifies where to mount that volume within the container's file system. In this case, we want /tempfiles. \n\nWhen the application in the container writes to /tempfiles, it'll be writing to the temp-files volume. A container may use many different volumes or none. Note that in order to use volumes, the application in the container has to use the path that we specified in mountPath. \n\nSo, if you want to use a container image with volumes, make sure that the path it uses to read/write files matches the path we specified in **volumeMounts.** \n\nA volume could—depending on its type—specify other attributes like accessModes, i.e., what kind of access it allows. \n\nModes can be **ReadWriteOnce**, **ReadOnlyMany**, **ReadWriteMany**, and **ReadWriteOncePod**. Note that specifying an access mode may not constrain the actual usage by the container. See [access modes](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#access-modes) for details. \n\nNow, let's take a look at different types of volumes. \n\n![A picture containing textDescription automatically generated](/blog-images/bd7448beaaf00cde3327a932d3196820.jpeg)\n\n### Volume Types\n\n#### EmptyDir\n\nKubernetes first creates an emptyDir volume when it assigns the [pod](https://kubernetes.io/docs/concepts/workloads/pods/) using that volume to a [node](https://kubernetes.io/docs/concepts/architecture/nodes/). As the name suggests, it's empty to start with, i.e., it contains no files/directories. \n\nContainers in the same pod can share the volume so that changes made by any container are visible to others. The emptyDir volume persists as long as the pod using it does—a container crash does not delete a pod. \n\nThus, it's an ephemeral or temporary kind of storage for things like cached files/data or intermediate results, etc. Also, we cannot use it to share data across pods. \n\n#### Persistent\n\n[Persistent volumes](https://kubernetes.io/docs/concepts/storage/persistent-volumes/) are defined by an administrator at the Kubernetes cluster level and can be used by multiple nodes in the cluster. They can retain their data even if we delete the pod using them. \n\nApplications in containers can request to use a persistent volume by specifying a persistent volume **claim**. The claim specifies how much storage of what type it requires and using which access mode. \n\nThe cluster can allocate the storage for a claim in two ways: statically if a claim is satisfied by a provisioned volume, and dynamically—for if no volume is available for a claim, the cluster may try to provision the volume dynamically based on the storage class specified. \n\nThe claim with the allocated storage is valid as long as the pod making the claim exists. \n\nThe **reclaim policy** of a volume specifies what to do with a volume once the application no longer needs the volume storage—for example, when we delete a pod using the volume. \n\nAccordingly, we can either retain or delete the data on the volume. Note also that the available access modes will depend on what type of volume is used. Since Kubernetes itself does not provide a file-sharing solution, we need to set that up first. \n\nFor instance, when using NFS, we need to set up the NFS share first, and then we can refer to it when creating a persistent volume. Additionally, we may need to install drivers for supporting that volume on the cluster. \n\n![Graphical user interface, text, applicationDescription automatically generated](/blog-images/59109fced26c504cecd82674233bb3c4.png)\n\n##### YAML Example\n\nLet's look at an example configuration for an NFS volume. \n\n```yaml\napiVersion: v1\nkind: PersistentVolume\nmetadata:\nname: nfs-vol\nspec:\ncapacity:\nstorage: 1Mi\naccessModes:\n  - ReadWriteMany\nnfs:\nserver: nfs-server-name\npath: \"/\"\nmountOptions:\n  - nfsvers=4.1\n```\n\nThe name, capacity, and accessModes are common to all types of volumes, whereas the section at the end, \"nfs\" in this case, is specific to the type of volume. \n\nWe can create the volume with **kubectl apply**. To get information about a volume, we would use \n\n```yaml\nkubectl get pv\n```\n\nNow, create a persistent volume claim, and again with kubectl apply \n\n```yaml\n\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: my-pv-claim\nspec:\n  storageClassName: manual\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 3Gi\n\n```\n\nHere, we can request a particular storage class (useful for dynamic provisioning), the access mode, and the amount of storage needed. \n\nWe can query for a claim using \n\n```yaml\nkubectl get pvc\n```\n\nFinally, we can use the claim in a pod: \n\n```\n\napiVersion: v1\nkind: Pod\nmetadata:\n  name: my-pod\nspec:\n  volumes:\n    - name: my-pv-storage\n      persistentVolumeClaim:\n        claimName: my-pv-claim\n  containers:\n    - name: my-pv-container\n      image: nginx\n      ports:\n        - containerPort: 8080\n          name: \"tomcat-server\"\n      volumeMounts:\n        - mountPath: \"/usr/data\"\n          name: my-pv-storage\n\n```\n\nHere, we link the persistent volume to the claim we created earlier. Then, as usual, we refer to the volume to mount it at the specified path in the container. \n\nNext, let's go through the supported types of persistent volumes. \n\n##### HostPath\n\nThis is probably the easiest way to test persistent volumes. \n\nHostPath mounts content from the node's file system into the pod. It has specific use cases, like when the container needs to run sys tools or access Docker internals. Containers usually shouldn't make any assumptions about the host node, so good practice discourages such use. \n\nAlso, hostPath exposes the host's file system—and potentially the cluster—to security flaws in the application. We should only use it for testing on a single node, as it doesn't work in a multi-node cluster. You can check out the **local** volume type instead. \n\n##### Local\n\nUsing local storage devices mounted on nodes is a better alternative to hostPath for sharing a file system between multiple pods but on the same node. \n\nThe volume definition contains **node affinity,** which points to the particular node name on which the local storage is available. The controller will assign pods using the local storage volume to the node that has the local storage, thus using the node affinity to identify the node name. \n\nIf the node with the local storage becomes unhealthy, the storage will become unavailable, and pods using it will fail too. Thus, local storage is not suitable where fail safety is important. \n\n![Graphical user interface, text, applicationDescription automatically generated](/blog-images/23297143a280c4334df4d2cc653a7d19.png)\n\n#### Projected\n\nA projected volume maps several existing volume sources into the same directory. The supported volume types for this are downwardAPI, secret, configMap, and serviceAccountToken. \n\n#### ISCSI\n\niSCSI—SCSI over IP—is an IP-based standard for transferring data that supports host access by carrying SCSI commands over IP networks. [SCSI](https://en.wikipedia.org/wiki/SCSI) is a set of standards for physically connecting and transferring data between computers and peripheral devices. \n\n#### CSI\n\nThe container storage interface defined by Kubernetes is a standard for exposing arbitrary block and file storage systems to containerized workloads. To support using a new type of file system as a volume, we need to write a CSI driver for that file system and install it on the cluster. A list of CSI drivers can be seen [here](https://kubernetes-csi.github.io/docs/drivers.html), including drivers for file systems on popular cloud providers like AWS and Azure. \n\n#### Fc\n\nFc, or [Fibre Channel storage](https://www.ibm.com/docs/en/ds8880/8.1.1?topic=attachment-fibre-channel-storage-area-networks), is a high-speed network that attaches servers and storage devices. \n\n#### Nfs\n\nA [network file system](https://en.wikipedia.org/wiki/Network_File_System) is a distributed file system originally developed by Sun Microsystems that's based on the [open network computing remote procedure call](https://en.wikipedia.org/wiki/Open_Network_Computing_Remote_Procedure_Call). \n\n#### Cephfs\n\nA [Ceph file system](https://docs.ceph.com/en/quincy/cephfs/) is a POSIX-compliant, open-source file system built on top of Ceph’s distributed object store, **Rados**. It provides a multi-use, highly available, and performant file store. \n\n#### RBD\n\nA Rados block device is the device on which the Ceph file system is built. Block storage allows us to access storage as blocks of raw data rather than files and directories. \n\n#### AwsElasticBlockStore (deprecated)\n\nWe can use this volume type to mount an AWS EBS store. It is now deprecated, so we should use the CSI drivers instead. \n\n#### AzureDisk (deprecated)\n\nThis is used to mount an Azure disk. It is now deprecated, so we should use the CSI drivers instead. \n\nThe above list of persistent volume types is not exhaustive, but it covers the commonly used types. \n\n#### ConfigMap\n\nThis type of volume exposes key value pairs from a ConfigMap as files on the file system. \n\nSpecifically, the key becomes the file name, and the value becomes the file contents. For example, the log-level=debug key value is represented as a file named log-level with contents = \"debug\". We can specify the path at which we want to mount the volume in the container. But first, we need to create a ConfigMap using **kubectl create.** \n\nWe can create it from properties files or literal values. It's also possible to expose the values from the ConfigMap as environment variables for a pod. See [more](https://kubernetes.io/docs/tasks/configure-pod-container/configure-pod-configmap/) for details. \n\n#### Downward API\n\nThe downward API exposes pod and container field values to applications. The downward API volume exposes the key value pairs as files on the file system similar to ConfigMap above. \n\n#### Secret\n\nThis is a tempfs-based file system used to store secrets, e.g., for authentication. It's similar to ConfigMap. We need to first create a  secret using the Kubernetes API. We can also expose secrets as environment variables. \n\n### Conclusion\n\nIn this post, we've highlighted how to inject file systems into Kubernetes pods using volumes. \n\nWe've also explored the different kinds of volumes and their uses. Using volumes allows us to use various types of storage, persist data independent of the pod, and also share data across pods.\n",
          "code": "var Component=(()=>{var d=Object.create;var i=Object.defineProperty;var p=Object.getOwnPropertyDescriptor;var m=Object.getOwnPropertyNames;var u=Object.getPrototypeOf,f=Object.prototype.hasOwnProperty;var g=(t,e)=>()=>(e||t((e={exports:{}}).exports,e),e.exports),y=(t,e)=>{for(var s in e)i(t,s,{get:e[s],enumerable:!0})},r=(t,e,s,o)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let a of m(e))!f.call(t,a)&&a!==s&&i(t,a,{get:()=>e[a],enumerable:!(o=p(e,a))||o.enumerable});return t};var v=(t,e,s)=>(s=t!=null?d(u(t)):{},r(e||!t||!t.__esModule?i(s,\"default\",{value:t,enumerable:!0}):s,t)),w=t=>r(i({},\"__esModule\",{value:!0}),t);var c=g((S,l)=>{l.exports=_jsx_runtime});var T={};y(T,{default:()=>N,frontmatter:()=>b});var n=v(c()),b={title:\"Kubernetes Volumes: What They Are and How to Use Them\",summary:\"Learn how to inject file systems into Kubernetes pods using volumes and talk about the different types of their uses.\",publishDate:\"Tue Sep 13 2022 18:26:07 GMT+0000 (Coordinated Universal Time)\",author:\"erik-landerholm\",readingTime:5,categories:[\"kubernetes\",\"platform-engineering\"],mainImage:\"/blog-images/e2fad2ca4879f4bfe48dc3b52c46df72.jpg\",imageAlt:\"A table with a cell phone, pencil, glasses and a plant\",showCTA:!0,ctaCopy:\"Discover how Release.com's dynamic environments streamline Kubernetes volume management for persistent data storage.\",ctaLink:\"https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=kubernetes-volumes-what-they-are-and-how-to-use-them\",relatedPosts:[\"\"],ogImage:\"/blog-images/e2fad2ca4879f4bfe48dc3b52c46df72.jpg\",excerpt:\"Learn how to inject file systems into Kubernetes pods using volumes and talk about the different types of their uses.\",tags:[\"kubernetes\",\"platform-engineering\"],ctaButton:\"Try Release for Free\"};function h(t){let e=Object.assign({p:\"p\",a:\"a\",h3:\"h3\",span:\"span\",strong:\"strong\",h4:\"h4\",pre:\"pre\",code:\"code\",img:\"img\",h5:\"h5\"},t.components);return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(e.p,{children:\"By default, the file system available to a Kubernetes pod is limited to the pod's lifetime. As such, when the pod is deleted, all changes are lost.\"}),`\n`,(0,n.jsx)(e.p,{children:\"But many applications will need to store data persistently, irrespective of whether a pod is running or not. For example, we need to retain data that was updated in the database or files written. Also, we may want to share a file system across multiple containers, and those may be running on different nodes.\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"Let's take a look at Kubernetes \",(0,n.jsx)(e.a,{href:\"https://kubernetes.io/docs/concepts/storage/volumes/\",children:\"volumes\"}),\", which can address these problems.\"]}),`\n`,(0,n.jsxs)(e.h3,{id:\"the-basics\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#the-basics\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"The Basics\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Most data storage that applications use is ultimately file system-based, e.g., even though a database may keep some or all of its data in memory while running, it also keeps it updated in the data files on the file system for persistence.\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:\"Volumes allow us to inject the application with a reference to a file system, which the application can then read from or write to.\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:\"Injecting the file system makes it independent of the container's lifetime. We need to specify an absolute path where the injected file system should be mounted within the container's file system.\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:\"Volumes may be persistent or not. There are many different types of volumes, as we shall see.\\xA0\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"A volume has to first be defined using the \",(0,n.jsx)(e.strong,{children:\"volumes\"}),\" key, and then used by a container using the \",(0,n.jsx)(e.strong,{children:\"volumeMounts\"}),\" key.\\xA0\"]}),`\n`,(0,n.jsxs)(e.h4,{id:\"example\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#example\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Example\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Below is a partial YAML snippet to illustrate how we can define and use volumes in a pod. Depending on the type of volume, its definition and usage could be in separate places.\\xA0\"}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-yaml\",children:`apiVersion: v0\nkind: Pod\nmetadata:\n  \\xA0name: my-pod\nspec:\n  \\xA0containers:\n  \\xA0- image: some-image-name\n  \\xA0 \\xA0name: my-container\n  \\xA0 \\xA0volumeMounts:\n  \\xA0 \\xA0- mountPath: /tempfiles\n  \\xA0 \\xA0 \\xA0name: temp-files-volume\n  \\xA0volumes:\n  \\xA0- name: temp-files-volume\n  \\xA0 \\xA0emptyDir: {}\n`})}),`\n`,(0,n.jsxs)(e.p,{children:[\"Here, we've defined a volume of the \",(0,n.jsx)(e.strong,{children:\"emptyDir\"}),\" type. We'll see more about this later.\\xA0\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Since this type can only be used at the level of a single pod, not across, it's defined along with the pod. There could be multiple containers in a pod (though usually not), and they could all use the same volume.\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:\"So, if one container in a pod writes a new file to the volume, it would be visible to the other containers in that pod that use that volume. The name of the volume can be anything.\\xA0\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"The \",(0,n.jsx)(e.strong,{children:\"volumeMounts\"}),\" entry under the container specifies where to mount that volume within the container's file system. In this case, we want /tempfiles.\\xA0\"]}),`\n`,(0,n.jsx)(e.p,{children:\"When the application in the container writes to /tempfiles, it'll be writing to the temp-files volume. A container may use many different volumes or none. Note that in order to use volumes, the application in the container has to use the path that we specified in mountPath.\\xA0\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"So, if you want to use a container image with volumes, make sure that the path it uses to read/write files matches the path we specified in \",(0,n.jsx)(e.strong,{children:\"volumeMounts.\"}),\"\\xA0\"]}),`\n`,(0,n.jsx)(e.p,{children:\"A volume could\\u2014depending on its type\\u2014specify other attributes like accessModes, i.e., what kind of access it allows.\\xA0\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"Modes can be \",(0,n.jsx)(e.strong,{children:\"ReadWriteOnce\"}),\", \",(0,n.jsx)(e.strong,{children:\"ReadOnlyMany\"}),\", \",(0,n.jsx)(e.strong,{children:\"ReadWriteMany\"}),\", and \",(0,n.jsx)(e.strong,{children:\"ReadWriteOncePod\"}),\". Note that specifying an access mode may not constrain the actual usage by the container. See \",(0,n.jsx)(e.a,{href:\"https://kubernetes.io/docs/concepts/storage/persistent-volumes/#access-modes\",children:\"access modes\"}),\" for details.\\xA0\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Now, let's take a look at different types of volumes.\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/bd7448beaaf00cde3327a932d3196820.jpeg\",alt:\"A picture containing textDescription automatically generated\"})}),`\n`,(0,n.jsxs)(e.h3,{id:\"volume-types\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#volume-types\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Volume Types\"]}),`\n`,(0,n.jsxs)(e.h4,{id:\"emptydir\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#emptydir\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"EmptyDir\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"Kubernetes first creates an emptyDir volume when it assigns the \",(0,n.jsx)(e.a,{href:\"https://kubernetes.io/docs/concepts/workloads/pods/\",children:\"pod\"}),\" using that volume to a \",(0,n.jsx)(e.a,{href:\"https://kubernetes.io/docs/concepts/architecture/nodes/\",children:\"node\"}),\". As the name suggests, it's empty to start with, i.e., it contains no files/directories.\\xA0\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Containers in the same pod can share the volume so that changes made by any container are visible to others. The emptyDir volume persists as long as the pod using it does\\u2014a container crash does not delete a pod.\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:\"Thus, it's an ephemeral or temporary kind of storage for things like cached files/data or intermediate results, etc. Also, we cannot use it to share data across pods.\\xA0\"}),`\n`,(0,n.jsxs)(e.h4,{id:\"persistent\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#persistent\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Persistent\"]}),`\n`,(0,n.jsxs)(e.p,{children:[(0,n.jsx)(e.a,{href:\"https://kubernetes.io/docs/concepts/storage/persistent-volumes/\",children:\"Persistent volumes\"}),\" are defined by an administrator at the Kubernetes cluster level and can be used by multiple nodes in the cluster. They can retain their data even if we delete the pod using them.\\xA0\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"Applications in containers can request to use a persistent volume by specifying a persistent volume \",(0,n.jsx)(e.strong,{children:\"claim\"}),\". The claim specifies how much storage of what type it requires and using which access mode.\\xA0\"]}),`\n`,(0,n.jsx)(e.p,{children:\"The cluster can allocate the storage for a claim in two ways: statically if a claim is satisfied by a provisioned volume, and dynamically\\u2014for if no volume is available for a claim, the cluster may try to provision the volume dynamically based on the storage class specified.\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:\"The claim with the allocated storage is valid as long as the pod making the claim exists.\\xA0\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"The \",(0,n.jsx)(e.strong,{children:\"reclaim policy\"}),\" of a volume specifies what to do with a volume once the application no longer needs the volume storage\\u2014for example, when we delete a pod using the volume.\\xA0\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Accordingly, we can either retain or delete the data on the volume. Note also that the available access modes will depend on what type of volume is used. Since Kubernetes itself does not provide a file-sharing solution, we need to set that up first.\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:\"For instance, when using NFS, we need to set up the NFS share first, and then we can refer to it when creating a persistent volume. Additionally, we may need to install drivers for supporting that volume on the cluster.\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/59109fced26c504cecd82674233bb3c4.png\",alt:\"Graphical user interface, text, applicationDescription automatically generated\"})}),`\n`,(0,n.jsxs)(e.h5,{id:\"yaml-example\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#yaml-example\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"YAML Example\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Let's look at an example configuration for an NFS volume.\\xA0\"}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-yaml\",children:`apiVersion: v1\nkind: PersistentVolume\nmetadata:\nname: nfs-vol\nspec:\ncapacity:\nstorage: 1Mi\naccessModes:\n  - ReadWriteMany\nnfs:\nserver: nfs-server-name\npath: \"/\"\nmountOptions:\n  - nfsvers=4.1\n`})}),`\n`,(0,n.jsx)(e.p,{children:'The name, capacity, and accessModes are common to all types of volumes, whereas the section at the end, \"nfs\" in this case, is specific to the type of volume.\\xA0'}),`\n`,(0,n.jsxs)(e.p,{children:[\"We can create the volume with \",(0,n.jsx)(e.strong,{children:\"kubectl apply\"}),\". To get information about a volume, we would use\\xA0\"]}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-yaml\",children:`kubectl get pv\n`})}),`\n`,(0,n.jsx)(e.p,{children:\"Now, create a persistent volume claim, and again with kubectl apply\\xA0\"}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-yaml\",children:`\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n \\xA0name: my-pv-claim\nspec:\n \\xA0storageClassName: manual\n \\xA0accessModes:\n \\xA0 \\xA0- ReadWriteOnce\n \\xA0resources:\n \\xA0 \\xA0requests:\n \\xA0 \\xA0 \\xA0storage: 3Gi\n\n`})}),`\n`,(0,n.jsx)(e.p,{children:\"Here, we can request a particular storage class (useful for dynamic provisioning), the access mode, and the amount of storage needed.\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:\"We can query for a claim using\\xA0\"}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-yaml\",children:`kubectl get pvc\n`})}),`\n`,(0,n.jsx)(e.p,{children:\"Finally, we can use the claim in a pod:\\xA0\"}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{children:`\napiVersion: v1\nkind: Pod\nmetadata:\n \\xA0name: my-pod\nspec:\n \\xA0volumes:\n \\xA0 \\xA0- name: my-pv-storage\n \\xA0 \\xA0 \\xA0persistentVolumeClaim:\n \\xA0 \\xA0 \\xA0 \\xA0claimName: my-pv-claim\n \\xA0containers:\n \\xA0 \\xA0- name: my-pv-container\n \\xA0 \\xA0 \\xA0image: nginx\n \\xA0 \\xA0 \\xA0ports:\n \\xA0 \\xA0 \\xA0 \\xA0- containerPort: 8080\n \\xA0 \\xA0 \\xA0 \\xA0 \\xA0name: \"tomcat-server\"\n \\xA0 \\xA0 \\xA0volumeMounts:\n \\xA0 \\xA0 \\xA0 \\xA0- mountPath: \"/usr/data\"\n \\xA0 \\xA0 \\xA0 \\xA0 \\xA0name: my-pv-storage\n\n`})}),`\n`,(0,n.jsx)(e.p,{children:\"Here, we link the persistent volume to the claim we created earlier. Then, as usual, we refer to the volume to mount it at the specified path in the container.\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:\"Next, let's go through the supported types of persistent volumes.\\xA0\"}),`\n`,(0,n.jsxs)(e.h5,{id:\"hostpath\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#hostpath\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"HostPath\"]}),`\n`,(0,n.jsx)(e.p,{children:\"This is probably the easiest way to test persistent volumes.\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:\"HostPath mounts content from the node's file system into the pod. It has specific use cases, like when the container needs to run sys tools or access Docker internals. Containers usually shouldn't make any assumptions about the host node, so good practice discourages such use.\\xA0\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"Also, hostPath exposes the host's file system\\u2014and potentially the cluster\\u2014to security flaws in the application. We should only use it for testing on a single node, as it doesn't work in a multi-node cluster. You can check out the \",(0,n.jsx)(e.strong,{children:\"local\"}),\" volume type instead.\\xA0\"]}),`\n`,(0,n.jsxs)(e.h5,{id:\"local\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#local\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Local\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Using local storage devices mounted on nodes is a better alternative to hostPath for sharing a file system between multiple pods but on the same node.\\xA0\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"The volume definition contains \",(0,n.jsx)(e.strong,{children:\"node affinity,\"}),\" which points to the particular node name on which the local storage is available. The controller will assign pods using the local storage volume to the node that has the local storage, thus using the node affinity to identify the node name.\\xA0\"]}),`\n`,(0,n.jsx)(e.p,{children:\"If the node with the local storage becomes unhealthy, the storage will become unavailable, and pods using it will fail too. Thus, local storage is not suitable where fail safety is important.\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/23297143a280c4334df4d2cc653a7d19.png\",alt:\"Graphical user interface, text, applicationDescription automatically generated\"})}),`\n`,(0,n.jsxs)(e.h4,{id:\"projected\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#projected\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Projected\"]}),`\n`,(0,n.jsx)(e.p,{children:\"A projected volume maps several existing volume sources into the same directory. The supported volume types for this are downwardAPI, secret, configMap, and serviceAccountToken.\\xA0\"}),`\n`,(0,n.jsxs)(e.h4,{id:\"iscsi\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#iscsi\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"ISCSI\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"iSCSI\\u2014SCSI over IP\\u2014is an IP-based standard for transferring data that supports host access by carrying SCSI commands over IP networks. \",(0,n.jsx)(e.a,{href:\"https://en.wikipedia.org/wiki/SCSI\",children:\"SCSI\"}),\" is a set of standards for physically connecting and transferring data between computers and peripheral devices.\\xA0\"]}),`\n`,(0,n.jsxs)(e.h4,{id:\"csi\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#csi\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"CSI\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"The container storage interface defined by Kubernetes is a standard for exposing arbitrary block and file storage systems to containerized workloads. To support using a new type of file system as a volume, we need to write a CSI driver for that file system and install it on the cluster. A list of CSI drivers can be seen \",(0,n.jsx)(e.a,{href:\"https://kubernetes-csi.github.io/docs/drivers.html\",children:\"here\"}),\", including drivers for file systems on popular cloud providers like AWS and Azure.\\xA0\"]}),`\n`,(0,n.jsxs)(e.h4,{id:\"fc\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#fc\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Fc\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"Fc, or \",(0,n.jsx)(e.a,{href:\"https://www.ibm.com/docs/en/ds8880/8.1.1?topic=attachment-fibre-channel-storage-area-networks\",children:\"Fibre Channel storage\"}),\", is a high-speed network that attaches servers and storage devices.\\xA0\"]}),`\n`,(0,n.jsxs)(e.h4,{id:\"nfs\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#nfs\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Nfs\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"A \",(0,n.jsx)(e.a,{href:\"https://en.wikipedia.org/wiki/Network_File_System\",children:\"network file system\"}),\" is a distributed file system originally developed by Sun Microsystems that's based on the \",(0,n.jsx)(e.a,{href:\"https://en.wikipedia.org/wiki/Open_Network_Computing_Remote_Procedure_Call\",children:\"open network computing remote procedure call\"}),\".\\xA0\"]}),`\n`,(0,n.jsxs)(e.h4,{id:\"cephfs\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#cephfs\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Cephfs\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"A \",(0,n.jsx)(e.a,{href:\"https://docs.ceph.com/en/quincy/cephfs/\",children:\"Ceph file system\"}),\" is a POSIX-compliant, open-source file system built on top of Ceph\\u2019s distributed object store, \",(0,n.jsx)(e.strong,{children:\"Rados\"}),\". It provides a multi-use, highly available, and performant file store.\\xA0\"]}),`\n`,(0,n.jsxs)(e.h4,{id:\"rbd\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#rbd\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"RBD\"]}),`\n`,(0,n.jsx)(e.p,{children:\"A Rados block device is the device on which the Ceph file system is built. Block storage allows us to access storage as blocks of raw data rather than files and directories.\\xA0\"}),`\n`,(0,n.jsxs)(e.h4,{id:\"awselasticblockstore-deprecated\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#awselasticblockstore-deprecated\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"AwsElasticBlockStore (deprecated)\"]}),`\n`,(0,n.jsx)(e.p,{children:\"We can use this volume type to mount an AWS EBS store. It is now deprecated, so we should use the CSI drivers instead.\\xA0\"}),`\n`,(0,n.jsxs)(e.h4,{id:\"azuredisk-deprecated\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#azuredisk-deprecated\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"AzureDisk (deprecated)\"]}),`\n`,(0,n.jsx)(e.p,{children:\"This is used to mount an Azure disk. It is now deprecated, so we should use the CSI drivers instead.\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:\"The above list of persistent volume types is not exhaustive, but it covers the commonly used types.\\xA0\"}),`\n`,(0,n.jsxs)(e.h4,{id:\"configmap\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#configmap\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"ConfigMap\"]}),`\n`,(0,n.jsx)(e.p,{children:\"This type of volume exposes key value pairs from a ConfigMap as files on the file system.\\xA0\"}),`\n`,(0,n.jsxs)(e.p,{children:['Specifically, the key becomes the file name, and the value becomes the file contents. For example, the log-level=debug key value is represented as a file named log-level with contents = \"debug\". We can specify the path at which we want to mount the volume in the container. But first, we need to create a ConfigMap using ',(0,n.jsx)(e.strong,{children:\"kubectl create.\"}),\"\\xA0\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"We can create it from properties files or literal values. It's also possible to expose the values from the ConfigMap as environment variables for a pod. See \",(0,n.jsx)(e.a,{href:\"https://kubernetes.io/docs/tasks/configure-pod-container/configure-pod-configmap/\",children:\"more\"}),\" for details.\\xA0\"]}),`\n`,(0,n.jsxs)(e.h4,{id:\"downward-api\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#downward-api\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Downward API\"]}),`\n`,(0,n.jsx)(e.p,{children:\"The downward API exposes pod and container field values to applications. The downward API volume exposes the key value pairs as files on the file system similar to ConfigMap above.\\xA0\"}),`\n`,(0,n.jsxs)(e.h4,{id:\"secret\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#secret\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Secret\"]}),`\n`,(0,n.jsx)(e.p,{children:\"This is a tempfs-based file system used to store secrets, e.g., for authentication. It's similar to ConfigMap. We need to first create a \\xA0secret using the Kubernetes API. We can also expose secrets as environment variables.\\xA0\"}),`\n`,(0,n.jsxs)(e.h3,{id:\"conclusion\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#conclusion\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Conclusion\"]}),`\n`,(0,n.jsx)(e.p,{children:\"In this post, we've highlighted how to inject file systems into Kubernetes pods using volumes.\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:\"We've also explored the different kinds of volumes and their uses. Using volumes allows us to use various types of storage, persist data independent of the pod, and also share data across pods.\"})]})}function k(t={}){let{wrapper:e}=t.components||{};return e?(0,n.jsx)(e,Object.assign({},t,{children:(0,n.jsx)(h,t)})):h(t)}var N=k;return w(T);})();\n;return Component;"
        },
        "_id": "blog/posts/kubernetes-volumes-what-they-are-and-how-to-use-them.mdx",
        "_raw": {
          "sourceFilePath": "blog/posts/kubernetes-volumes-what-they-are-and-how-to-use-them.mdx",
          "sourceFileName": "kubernetes-volumes-what-they-are-and-how-to-use-them.mdx",
          "sourceFileDir": "blog/posts",
          "contentType": "mdx",
          "flattenedPath": "blog/posts/kubernetes-volumes-what-they-are-and-how-to-use-them"
        },
        "type": "BlogPost",
        "computedSlug": "kubernetes-volumes-what-they-are-and-how-to-use-them"
      },
      "documentHash": "1739393595024",
      "hasWarnings": false,
      "documentTypeName": "BlogPost"
    },
    "blog/posts/launch-press-release.mdx": {
      "document": {
        "title": "Sequoia leads $2.7M Seed Round to Launch ReleaseHub, Environments-as-a-Service",
        "summary": "Sequoia leads $2.7M Seed Round to Launch ReleaseHub, Environments-as-a-Service",
        "publishDate": "Thu Apr 29 2021 02:45:32 GMT+0000 (Coordinated Universal Time)",
        "author": "",
        "readingTime": 2,
        "categories": [
          "platform-engineering",
          "kubernetes",
          "product"
        ],
        "mainImage": "/blog-images/846e6cadec45067516ef55aaebd88517.jpg",
        "imageAlt": "Illustration of 3 software development environments created by Release",
        "showCTA": true,
        "ctaCopy": "Simplify environment management, replicate production setups effortlessly with ReleaseHub's Environments-as-a-Service solution.",
        "ctaLink": "https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=launch-press-release",
        "relatedPosts": [
          ""
        ],
        "ogImage": "/blog-images/846e6cadec45067516ef55aaebd88517.jpg",
        "excerpt": "Sequoia leads $2.7M Seed Round to Launch ReleaseHub, Environments-as-a-Service",
        "tags": [
          "platform-engineering",
          "kubernetes",
          "product"
        ],
        "ctaButton": "Try Release for Free",
        "body": {
          "raw": "\nSAN FRANCISCO -- ReleaseHub, maker of Environments-as-a-Service, today announced a seed round of $2.7 million led by Sequoia, with participation from Y Combinator, Rogue VC, Liquid Capital, and other angel investors. ReleaseHub is also announcing general availability of its Environments-as-a-Service platform, which allows organizations to quickly and easily deploy, manage, and reproduce production-replica environments. Customers use ReleaseHub to remove environment bottlenecks in software delivery, deliver on-demand sales demo environments, and deliver production environments and SaaS solutions into their customer’s virtual private clouds.\n\nEnvironments are one of the greatest bottlenecks for software development, causing developers to sit idle and release dates to slip. Software organizations don’t provision enough environments because they are complex to stand up and costly to maintain. ReleaseHub lets developers easily create reproducible environments that are exact replicas of the production environment.\n\nThese environments run on Kubernetes in the software developer’s cloud account and have access to all of the developer’s cloud native services. ReleaseHub environments can be created on-demand, through a command line interface, with a pull request or via an API to allow for easy integration with existing CI/CD solutions.\n\n\"One of the most pervasive problems in software development is that development and production environments are never the same. Access to multiple pre-production environments that are precise replicas of production is a critical facilitator of collaboration between teams, yet managing numerous environments is so time consuming that companies ration access,” said Bogomil Balkansky, partner at Sequoia. “ReleaseHub removes these pain points with its Environments-as-a-Service platform, which makes the software development process more efficient.”\n\nReleaseHub was founded by Tommy McClung, Erik Landerholm and David Giffin, who led the technology team at an ecommerce company where they led the companywide effort to remove environment bottlenecks. There were no commercial solutions at the time so the founders built their own environment bottleneck solution and soon after launched ReleaseHub to commercialize Environments-as-a-Service.\n\n\"As modern cloud-based applications have become more complex with microservices and an ever growing arsenal of cloud services, it has become incredibly difficult for teams to create and maintain numerous environments,\" said McClung, ReleaseHub CEO. “We are transforming environments from an organizational headache into a one-click convenience.”\n\nDatasaur, an artificial intelligence technology company, started using ReleaseHub to accelerate software development and testing, as well as reduce bottlenecks in pre-production development. The company needed to quickly scale and used ReleaseHub to power its production environments. ReleaseHub provided the power of Kubernetes without having to invest in a lengthy and costly Kubernetes migration.\n\n“ReleaseHub enables Datasaur to focus on building applications without requiring a complex environment management infrastructure,” said Ivan Lee, Datasaur founder and CEO. “This has improved the efficiency of launching our services for the Datasaur community and added capabilities that have allowed us to handle our rapid growth.”\n",
          "code": "var Component=(()=>{var d=Object.create;var i=Object.defineProperty;var u=Object.getOwnPropertyDescriptor;var p=Object.getOwnPropertyNames;var h=Object.getPrototypeOf,v=Object.prototype.hasOwnProperty;var f=(n,e)=>()=>(e||n((e={exports:{}}).exports,e),e.exports),g=(n,e)=>{for(var t in e)i(n,t,{get:e[t],enumerable:!0})},s=(n,e,t,r)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let o of p(e))!v.call(n,o)&&o!==t&&i(n,o,{get:()=>e[o],enumerable:!(r=u(e,o))||r.enumerable});return n};var b=(n,e,t)=>(t=n!=null?d(h(n)):{},s(e||!n||!n.__esModule?i(t,\"default\",{value:n,enumerable:!0}):t,n)),w=n=>s(i({},\"__esModule\",{value:!0}),n);var c=f((k,l)=>{l.exports=_jsx_runtime});var C={};g(C,{default:()=>S,frontmatter:()=>y});var a=b(c()),y={title:\"Sequoia leads $2.7M Seed Round to Launch ReleaseHub, Environments-as-a-Service\",summary:\"Sequoia leads $2.7M Seed Round to Launch ReleaseHub, Environments-as-a-Service\",publishDate:\"Thu Apr 29 2021 02:45:32 GMT+0000 (Coordinated Universal Time)\",author:\"\",readingTime:2,categories:[\"platform-engineering\",\"kubernetes\",\"product\"],mainImage:\"/blog-images/846e6cadec45067516ef55aaebd88517.jpg\",imageAlt:\"Illustration of 3 software development environments created by Release\",showCTA:!0,ctaCopy:\"Simplify environment management, replicate production setups effortlessly with ReleaseHub's Environments-as-a-Service solution.\",ctaLink:\"https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=launch-press-release\",relatedPosts:[\"\"],ogImage:\"/blog-images/846e6cadec45067516ef55aaebd88517.jpg\",excerpt:\"Sequoia leads $2.7M Seed Round to Launch ReleaseHub, Environments-as-a-Service\",tags:[\"platform-engineering\",\"kubernetes\",\"product\"],ctaButton:\"Try Release for Free\"};function m(n){let e=Object.assign({p:\"p\"},n.components);return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(e.p,{children:\"SAN FRANCISCO -- ReleaseHub, maker of Environments-as-a-Service, today announced a seed round of $2.7 million led by Sequoia, with participation from Y Combinator, Rogue VC, Liquid Capital, and other angel investors. ReleaseHub is also announcing general availability of its Environments-as-a-Service platform, which allows organizations to quickly and easily deploy, manage, and reproduce production-replica environments. Customers use ReleaseHub to remove environment bottlenecks in software delivery, deliver on-demand sales demo environments, and deliver production environments and SaaS solutions into their customer\\u2019s virtual private clouds.\"}),`\n`,(0,a.jsx)(e.p,{children:\"Environments are one of the greatest bottlenecks for software development, causing developers to sit idle and release dates to slip. Software organizations don\\u2019t provision enough environments because they are complex to stand up and costly to maintain. ReleaseHub lets developers easily create reproducible environments that are exact replicas of the production environment.\"}),`\n`,(0,a.jsx)(e.p,{children:\"These environments run on Kubernetes in the software developer\\u2019s cloud account and have access to all of the developer\\u2019s cloud native services. ReleaseHub environments can be created on-demand, through a command line interface, with a pull request or via an API to allow for easy integration with existing CI/CD solutions.\"}),`\n`,(0,a.jsx)(e.p,{children:'\"One of the most pervasive problems in software development is that development and production environments are never the same. Access to multiple pre-production environments that are precise replicas of production is a critical facilitator of collaboration between teams, yet managing numerous environments is so time consuming that companies ration access,\\u201D said Bogomil Balkansky, partner at Sequoia. \\u201CReleaseHub removes these pain points with its Environments-as-a-Service platform, which makes the software development process more efficient.\\u201D'}),`\n`,(0,a.jsx)(e.p,{children:\"ReleaseHub was founded by Tommy McClung, Erik Landerholm and David Giffin, who led the technology team at an ecommerce company where they led the companywide effort to remove environment bottlenecks. There were no commercial solutions at the time so the founders built their own environment bottleneck solution and soon after launched ReleaseHub to commercialize Environments-as-a-Service.\"}),`\n`,(0,a.jsx)(e.p,{children:'\"As modern cloud-based applications have become more complex with microservices and an ever growing arsenal of cloud services, it has become incredibly difficult for teams to create and maintain numerous environments,\" said McClung, ReleaseHub CEO. \\u201CWe are transforming environments from an organizational headache into a one-click convenience.\\u201D'}),`\n`,(0,a.jsx)(e.p,{children:\"Datasaur, an artificial intelligence technology company, started using ReleaseHub to accelerate software development and testing, as well as reduce bottlenecks in pre-production development. The company needed to quickly scale and used ReleaseHub to power its production environments. ReleaseHub provided the power of Kubernetes without having to invest in a lengthy and costly Kubernetes migration.\"}),`\n`,(0,a.jsx)(e.p,{children:\"\\u201CReleaseHub enables Datasaur to focus on building applications without requiring a complex environment management infrastructure,\\u201D said Ivan Lee, Datasaur founder and CEO. \\u201CThis has improved the efficiency of launching our services for the Datasaur community and added capabilities that have allowed us to handle our rapid growth.\\u201D\"})]})}function R(n={}){let{wrapper:e}=n.components||{};return e?(0,a.jsx)(e,Object.assign({},n,{children:(0,a.jsx)(m,n)})):m(n)}var S=R;return w(C);})();\n;return Component;"
        },
        "_id": "blog/posts/launch-press-release.mdx",
        "_raw": {
          "sourceFilePath": "blog/posts/launch-press-release.mdx",
          "sourceFileName": "launch-press-release.mdx",
          "sourceFileDir": "blog/posts",
          "contentType": "mdx",
          "flattenedPath": "blog/posts/launch-press-release"
        },
        "type": "BlogPost",
        "computedSlug": "launch-press-release"
      },
      "documentHash": "1739393595024",
      "hasWarnings": false,
      "documentTypeName": "BlogPost"
    },
    "blog/posts/lessons-learned-from-maintaining-the-soc-2-type-2-certification-over-the-years.mdx": {
      "document": {
        "title": "Lessons learned from maintaining the SOC 2 Type 2 certification over the years",
        "summary": "Learn why we prioritized security and compliance early in our product development and why we recommend you do the same.",
        "publishDate": "Thu Dec 14 2023 15:26:49 GMT+0000 (Coordinated Universal Time)",
        "author": "david-giffin",
        "readingTime": 5,
        "categories": [
          "platform-engineering",
          "product"
        ],
        "mainImage": "/blog-images/491df328e228c70954de0b8abee4af00.jpg",
        "imageAlt": "Lessons learned from maintaining the SOC 2 Type 2 certification over the years",
        "showCTA": true,
        "ctaCopy": "Enhance your SOC 2 compliance journey with Release's secure, automated environment management platform.",
        "ctaLink": "https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=lessons-learned-from-maintaining-the-soc-2-type-2-certification-over-the-years",
        "relatedPosts": [
          "12-things-you-didnt-know-you-could-do-with-release-part-1; a-managers-guide-to-release-cycles"
        ],
        "ogImage": "/blog-images/491df328e228c70954de0b8abee4af00.jpg",
        "excerpt": "Learn why we prioritized security and compliance early in our product development and why we recommend you do the same.",
        "tags": [
          "platform-engineering",
          "product"
        ],
        "ctaButton": "Try Release for Free",
        "body": {
          "raw": "\nHere at Release we like to say “move fast and break fewer things”. As a startup we experiment, iterate, and bring our ideas to the market faster than most mature companies. However, our speed never comes at the expense of security, compliance, and customer trust. Our customers entrust application environment creation to the Release platform. This means they test, stage, and run their applications in production using our Environments as a Service platform. Although all workloads live in the customers’ own cloud account, we facilitate and orchestrate the underlying infrastructure, so making sure our services meet and exceed security standards is non-negotiable for us.\n\nTo demonstrate our commitment to secure practices, we obtained the SOC 2 Type 2 certification back in 2021 and maintain it to this day. SOC 2 is a major undertaking for any company, and many startups wonder if it’s worth the effort early on. For us, it was a team effort that ultimately made us rethink our practices and the culture we were building as a company. Here are some lessons we learned throughout the process that could help you decide if the SOC 2 stamp of approval is appropriate at your stage of growth.\n\n#### **What is SOC 2 and why it matters?**\n\nService Organization Controls ([SOC](https://www.aicpa-cima.com/topic/audit-assurance/audit-and-assurance-greater-than-soc-2)) 2 is a compliance report standard defined by the American Institute of Certified Public Accountants ([AICPA](https://www.aicpa-cima.com/home)). SOC 2 reports are issued by an independent third-party CPA after a thorough audit that demonstrates how a service organization achieves key compliance controls and objectives. SOC 2 reports focus on non-financial reporting controls that relate to the **security**, **availability**, **processing integrity**, **confidentiality**, and **privacy** of a system. Ultimately these reports help users evaluate the risks associated with the evaluated service.\n\nYou typically hear about SOC 2 Type 1 and Type 2 reports. The key differences between them are the timing and depth of the audit. Type 1 reports on the suitability of design controls at a specific point in time, while Type 2 assesses the operational effectiveness of these controls over a period, usually a minimum of six months. Most organizations prepare for Type 1 assessment, while simultaneously starting to collect the data for the Type 2 assessment to follow shortly after.\n\nSOC 2 Type 2 compliance involves a rigorous process that includes designing controls to meet Trust Service Criteria, implementing these controls, and then undergoing a thorough audit by an independent CPA to prove all controls are working as intended.\n\nThe five Trust Service Criteria crucial for SOC2 Type 2 certification are:\n\n- **Security**: Protection of system resources against unauthorized access.\n- **Availability**: Availability of the system as agreed upon in the contract.\n- **Processing Integrity**: Completeness, validity, accuracy, timeliness, and authorization of system processing.\n- **Confidentiality:** Protection of confidential information as committed or agreed.\n- **Privacy:** Collection, use, retention, disclosure, and disposal of personal information in conformity with an organization's privacy notice\n\nWhen startups take this process seriously, they build resilient internal processes and standards that keep their customers safe, and set them up for success.\n\n#### **What did we learn from our SOC 2 Type 2 journey?**\n\nAfter going through the initial audit, and completing subsequent evaluations here are a few things we learned, that might help other startups:  \n**✅ Start early.** The best time to start the SOC 2 preparations is before you have paying customers and even employees. It sounds early, but the sooner you set the foundation for consistent secure practices, the easier it will be to maintain them. The initial assessment identifies existing controls and shows you the gaps in your current setup (believe it, you will have gaps, so better catch them before anyone is affected), and gives you an opportunity to course correct. Many of our early customers required us to have a SOC 2 certification before signing contracts to use our services, so it was a blessing to already have one in hand to get started with actual paying customers!\n**✅ Team effort.** Based on the assessment, specific controls are implemented. These range from physical security measures to IT governance and data encryption practices. This is an all-hands-on-board effort where everyone cleans up their shop. Once you set the foundation, ongoing compliance becomes standard operations for the company. When the time comes to show evidence of implemented controls, you know you did things right.\n**✅ Automate evidence gathering.** Even the most diligent teams can get stressed when asked to provide specific evidence on the spot. Automating your evidence collection, vendor management and security policies makes the process run much smoother come the audit time. Tools like [Drata](https://drata.com/) and [Vanta](https://www.vanta.com/) centralize and automate control monitoring, reduce the manual toil and give you real-time visibility into your security posture during and between the audits.\n**✅ Learn and improve.** Use the findings from the recurring pen tests to build a more resilient and safer product. The security landscape changes quickly and the guidance for SOC 2 also changes year to year. Make sure to keep track of major developments and use the monitoring features in your compliance tools to keep track of your updated posture.\n**✅ Set the budget**. As you head into the next budgeting cycle, make sure to set aside the funds compliance. Between the tools, the auditor fees, the pen tests and any remediations you will need to make, there will be a cost associated with getting “the stamp”. But make sure to spend your money wisely. Choose a reputable firm and get industry-tested tools. This is not the time to look for a bargain.\n\n#### **In conclusion, is SOC 2 Type 2 certification worth it?**\n\nShort answer: Yes.\n\nSecurity and compliance is not why any of us build startups (unless you’re Drata or Vanta), but it’s the reason why we stay in business. When our customers trust us, we can keep on innovating. Taking the time and putting the effort into validating the safety, security, and integrity of our products gives our customer peace of mind and allows them to rely on the services we provide. After all, customers are running their application environments in Release and we take that responsibility seriously.\n",
          "code": "var Component=(()=>{var h=Object.create;var o=Object.defineProperty;var u=Object.getOwnPropertyDescriptor;var m=Object.getOwnPropertyNames;var p=Object.getPrototypeOf,y=Object.prototype.hasOwnProperty;var g=(n,e)=>()=>(e||n((e={exports:{}}).exports,e),e.exports),f=(n,e)=>{for(var i in e)o(n,i,{get:e[i],enumerable:!0})},s=(n,e,i,r)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let a of m(e))!y.call(n,a)&&a!==i&&o(n,a,{get:()=>e[a],enumerable:!(r=u(e,a))||r.enumerable});return n};var w=(n,e,i)=>(i=n!=null?h(p(n)):{},s(e||!n||!n.__esModule?o(i,\"default\",{value:n,enumerable:!0}):i,n)),v=n=>s(o({},\"__esModule\",{value:!0}),n);var l=g((O,c)=>{c.exports=_jsx_runtime});var C={};f(C,{default:()=>T,frontmatter:()=>b});var t=w(l()),b={title:\"Lessons learned from maintaining the SOC 2 Type 2 certification over the years\",summary:\"Learn why we prioritized security and compliance early in our product development and why we recommend you do the same.\",publishDate:\"Thu Dec 14 2023 15:26:49 GMT+0000 (Coordinated Universal Time)\",author:\"david-giffin\",readingTime:5,categories:[\"platform-engineering\",\"product\"],mainImage:\"/blog-images/491df328e228c70954de0b8abee4af00.jpg\",imageAlt:\"Lessons learned from maintaining the SOC 2 Type 2 certification over the years\",showCTA:!0,ctaCopy:\"Enhance your SOC 2 compliance journey with Release's secure, automated environment management platform.\",ctaLink:\"https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=lessons-learned-from-maintaining-the-soc-2-type-2-certification-over-the-years\",relatedPosts:[\"12-things-you-didnt-know-you-could-do-with-release-part-1; a-managers-guide-to-release-cycles\"],ogImage:\"/blog-images/491df328e228c70954de0b8abee4af00.jpg\",excerpt:\"Learn why we prioritized security and compliance early in our product development and why we recommend you do the same.\",tags:[\"platform-engineering\",\"product\"],ctaButton:\"Try Release for Free\"};function d(n){let e=Object.assign({p:\"p\",h4:\"h4\",a:\"a\",span:\"span\",strong:\"strong\",ul:\"ul\",li:\"li\"},n.components);return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(e.p,{children:\"Here at Release we like to say \\u201Cmove fast and break fewer things\\u201D. As a startup we experiment, iterate, and bring our ideas to the market faster than most mature companies. However, our speed never comes at the expense of security, compliance, and customer trust. Our customers entrust application environment creation to the Release platform. This means they test, stage, and run their applications in production using our Environments as a Service platform. Although all workloads live in the customers\\u2019 own cloud account, we facilitate and orchestrate the underlying infrastructure, so making sure our services meet and exceed security standards is non-negotiable for us.\"}),`\n`,(0,t.jsx)(e.p,{children:\"To demonstrate our commitment to secure practices, we obtained the SOC 2 Type 2 certification back in 2021 and maintain it to this day. SOC 2 is a major undertaking for any company, and many startups wonder if it\\u2019s worth the effort early on. For us, it was a team effort that ultimately made us rethink our practices and the culture we were building as a company. Here are some lessons we learned throughout the process that could help you decide if the SOC 2 stamp of approval is appropriate at your stage of growth.\"}),`\n`,(0,t.jsxs)(e.h4,{id:\"what-is-soc-2-and-why-it-matters\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#what-is-soc-2-and-why-it-matters\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),(0,t.jsx)(e.strong,{children:\"What is SOC 2 and why it matters?\"})]}),`\n`,(0,t.jsxs)(e.p,{children:[\"Service Organization Controls (\",(0,t.jsx)(e.a,{href:\"https://www.aicpa-cima.com/topic/audit-assurance/audit-and-assurance-greater-than-soc-2\",children:\"SOC\"}),\") 2 is a compliance report standard defined by the American Institute of Certified Public Accountants (\",(0,t.jsx)(e.a,{href:\"https://www.aicpa-cima.com/home\",children:\"AICPA\"}),\"). SOC 2 reports are issued by an independent third-party CPA after a thorough audit that demonstrates how a service organization achieves key compliance controls and objectives. SOC 2 reports focus on non-financial reporting controls that relate to the \",(0,t.jsx)(e.strong,{children:\"security\"}),\", \",(0,t.jsx)(e.strong,{children:\"availability\"}),\", \",(0,t.jsx)(e.strong,{children:\"processing integrity\"}),\", \",(0,t.jsx)(e.strong,{children:\"confidentiality\"}),\", and \",(0,t.jsx)(e.strong,{children:\"privacy\"}),\" of a system. Ultimately these reports help users evaluate the risks associated with the evaluated service.\"]}),`\n`,(0,t.jsx)(e.p,{children:\"You typically hear about SOC 2 Type 1 and Type 2 reports. The key differences between them are the timing and depth of the audit. Type 1 reports on the suitability of design controls at a specific point in time, while Type 2 assesses the operational effectiveness of these controls over a period, usually a minimum of six months. Most organizations prepare for Type 1 assessment, while simultaneously starting to collect the data for the Type 2 assessment to follow shortly after.\"}),`\n`,(0,t.jsx)(e.p,{children:\"SOC 2 Type 2 compliance involves a rigorous process that includes designing controls to meet Trust Service Criteria, implementing these controls, and then undergoing a thorough audit by an independent CPA to prove all controls are working as intended.\"}),`\n`,(0,t.jsx)(e.p,{children:\"The five Trust Service Criteria crucial for SOC2 Type 2 certification are:\"}),`\n`,(0,t.jsxs)(e.ul,{children:[`\n`,(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:\"Security\"}),\": Protection of system resources against unauthorized access.\"]}),`\n`,(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:\"Availability\"}),\": Availability of the system as agreed upon in the contract.\"]}),`\n`,(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:\"Processing Integrity\"}),\": Completeness, validity, accuracy, timeliness, and authorization of system processing.\"]}),`\n`,(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:\"Confidentiality:\"}),\" Protection of confidential information as committed or agreed.\"]}),`\n`,(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:\"Privacy:\"}),\" Collection, use, retention, disclosure, and disposal of personal information in conformity with an organization's privacy notice\"]}),`\n`]}),`\n`,(0,t.jsx)(e.p,{children:\"When startups take this process seriously, they build resilient internal processes and standards that keep their customers safe, and set them up for success.\"}),`\n`,(0,t.jsxs)(e.h4,{id:\"what-did-we-learn-from-our-soc-2-type-2-journey\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#what-did-we-learn-from-our-soc-2-type-2-journey\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),(0,t.jsx)(e.strong,{children:\"What did we learn from our SOC 2 Type 2 journey?\"})]}),`\n`,(0,t.jsxs)(e.p,{children:[`After going through the initial audit, and completing subsequent evaluations here are a few things we learned, that might help other startups: \\xA0\n`,(0,t.jsx)(e.strong,{children:\"\\u2705 Start early.\"}),` The best time to start the SOC 2 preparations is before you have paying customers and even employees. It sounds early, but the sooner you set the foundation for consistent secure practices, the easier it will be to maintain them. The initial assessment identifies existing controls and shows you the gaps in your current setup (believe it, you will have gaps, so better catch them before anyone is affected), and gives you an opportunity to course correct. Many of our early customers required us to have a SOC 2 certification before signing contracts to use our services, so it was a blessing to already have one in hand to get started with actual paying customers!\n`,(0,t.jsx)(e.strong,{children:\"\\u2705 Team effort.\"}),` Based on the assessment, specific controls are implemented. These range from physical security measures to IT governance and data encryption practices. This is an all-hands-on-board effort where everyone cleans up their shop. Once you set the foundation, ongoing compliance becomes standard operations for the company. When the time comes to show evidence of implemented controls, you know you did things right.\n`,(0,t.jsx)(e.strong,{children:\"\\u2705 Automate evidence gathering.\"}),\" Even the most diligent teams can get stressed when asked to provide specific evidence on the spot. Automating your evidence collection, vendor management and security policies makes the process run much smoother come the audit time. Tools like \",(0,t.jsx)(e.a,{href:\"https://drata.com/\",children:\"Drata\"}),\" and \",(0,t.jsx)(e.a,{href:\"https://www.vanta.com/\",children:\"Vanta\"}),` centralize and automate control monitoring, reduce the manual toil and give you real-time visibility into your security posture during and between the audits.\n`,(0,t.jsx)(e.strong,{children:\"\\u2705 Learn and improve.\"}),` Use the findings from the recurring pen tests to build a more resilient and safer product. The security landscape changes quickly and the guidance for SOC 2 also changes year to year. Make sure to keep track of major developments and use the monitoring features in your compliance tools to keep track of your updated posture.\n`,(0,t.jsx)(e.strong,{children:\"\\u2705 Set the budget\"}),\". As you head into the next budgeting cycle, make sure to set aside the funds compliance. Between the tools, the auditor fees, the pen tests and any remediations you will need to make, there will be a cost associated with getting \\u201Cthe stamp\\u201D. But make sure to spend your money wisely. Choose a reputable firm and get industry-tested tools. This is not the time to look for a bargain.\"]}),`\n`,(0,t.jsxs)(e.h4,{id:\"in-conclusion-is-soc-2-type-2-certification-worth-it\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#in-conclusion-is-soc-2-type-2-certification-worth-it\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),(0,t.jsx)(e.strong,{children:\"In conclusion, is SOC 2 Type 2 certification worth it?\"})]}),`\n`,(0,t.jsx)(e.p,{children:\"Short answer: Yes.\"}),`\n`,(0,t.jsx)(e.p,{children:\"Security and compliance is not why any of us build startups (unless you\\u2019re Drata or Vanta), but it\\u2019s the reason why we stay in business. When our customers trust us, we can keep on innovating. Taking the time and putting the effort into validating the safety, security, and integrity of our products gives our customer peace of mind and allows them to rely on the services we provide. After all, customers are running their application environments in Release and we take that responsibility seriously.\"})]})}function k(n={}){let{wrapper:e}=n.components||{};return e?(0,t.jsx)(e,Object.assign({},n,{children:(0,t.jsx)(d,n)})):d(n)}var T=k;return v(C);})();\n;return Component;"
        },
        "_id": "blog/posts/lessons-learned-from-maintaining-the-soc-2-type-2-certification-over-the-years.mdx",
        "_raw": {
          "sourceFilePath": "blog/posts/lessons-learned-from-maintaining-the-soc-2-type-2-certification-over-the-years.mdx",
          "sourceFileName": "lessons-learned-from-maintaining-the-soc-2-type-2-certification-over-the-years.mdx",
          "sourceFileDir": "blog/posts",
          "contentType": "mdx",
          "flattenedPath": "blog/posts/lessons-learned-from-maintaining-the-soc-2-type-2-certification-over-the-years"
        },
        "type": "BlogPost",
        "computedSlug": "lessons-learned-from-maintaining-the-soc-2-type-2-certification-over-the-years"
      },
      "documentHash": "1739393595024",
      "hasWarnings": false,
      "documentTypeName": "BlogPost"
    },
    "blog/posts/looking-back-a-big-2023-for-release.mdx": {
      "document": {
        "title": "Looking Back: A Big 2023 for Release",
        "summary": "A look back at what happened over the past year and our favorite highlights from 2023.",
        "publishDate": "Tue Dec 19 2023 21:05:27 GMT+0000 (Coordinated Universal Time)",
        "author": "matt-carter",
        "readingTime": 3,
        "categories": [
          "platform-engineering",
          "product"
        ],
        "mainImage": "/blog-images/3dc80af264c6c9aac260907524f62474.png",
        "imageAlt": "2023 year in review for Release",
        "showCTA": true,
        "ctaCopy": "Empower your team with Release's environment automation for faster data management and seamless container sharing.",
        "ctaLink": "https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=looking-back-a-big-2023-for-release",
        "relatedPosts": [
          ""
        ],
        "ogImage": "/blog-images/3dc80af264c6c9aac260907524f62474.png",
        "excerpt": "A look back at what happened over the past year and our favorite highlights from 2023.",
        "tags": [
          "platform-engineering",
          "product"
        ],
        "ctaButton": "Try Release for Free",
        "body": {
          "raw": "\nWhen the holidays approach and a new year looms on the horizon, I always find myself looking back at what happened over the past 12 months. Reflecting on what the Release team achieved this year, especially in expanding ephemeral environment automation to more developers and use cases, is truly remarkable. Here are some of the highlights from 2023.\n\n‍**What’s in a Name**\n\nChanging the company’s name [from ReleaseHub to Release](https://release.com/blog/meet-the-new-release) was a significant milestone for us. This name more accurately reflects our vision and our mission from the inception to help people release their ideas to the world faster. It aligns well with the expanded use cases, capabilities, and technologies our development team has added to the product.\n**Bigger and Better Data for Every Developer**\n\nAfter the renaming, the next major release delivered improvements to Release Instant Datasets. Conversations with customers consistently underscored the importance of data to effective development and test processes and the complexity of managing massive data in the SDLC. Release Instant Datasets 2.0 made production-like data for developers even more powerful, with improvements to both the workflow and architecture of the tool. (CTO Erik Landerholm [goes into the details of these changes in this blog post](https://release.com/blog/new-and-improved-instant-datasets)). Shortly after, we made [Instant Datasets available as a standalone product](https://release.com/blog/introducing-standalone-instant-datasets-build-and-test-with-realistic-production-like-data-with-ease), enabling everyone to take advantage of data automation whether or not they are using the entire Release platform. \n**Docker Desktop Extension**\n\nInstant Datasets became a crucial component of Release’s support across the software development life cycle. Another major piece of the SDLC puzzle was the launch of [Release Share as a Docker Desktop extension](https://release.com/product/docker-extension). Release Share provides fast simple container sharing to the millions of Docker developers directly from the Docker Desktop application. Shifting environment sharing left to the start of the development process means app delivery teams can create a self-serve version of their app for feedback and testing at every stage of the process.\n**More Integrations for More Choice**\n\nDocker is just one part of a developer’s toolbox we support at Release. Developers ask us all the time “Does Release work with my \\[insert favorite tool name\\]?” We’re proud to have a broad set of partnerships and integrations with the tools and platforms favored by cloud-native app teams. Notably, we’ve improved our [integration with Tonic.ai](https://release.com/blog/the-value-of-data-obfuscation-for-instant-datasets-tonic-meets-release) for data automation and anonymization,  developed [new integrations with GitLab](https://release.com/blog/gitlab-self-managed-now-available-on-release) for more repo choices, and expanded our relationships with AWS and GCP. We’re excited about many significant new integrations coming in early 2024.\n**The Year of AI** \n\nAnd finally, no discussion of the last 12 months would be complete without talking about AI. ChatGPT and OpenAI are having a huge impact on the world, and the Release team [has been creating innovative ways](https://release.com/blog/training-chatgpt-with-custom-libraries-using-extensions) to use it to help make Release more powerful, and more importantly, make AI and dataset training more accessible to developers and teams. [We launched release.ai as our hub for all things AI for DevOps.](https://release.ai/) Today Release users can use interactive prompts with Release to “talk to your infrastructure” and get insights into their environments.  We are also helping companies apply ephemeral environment automation to dataset training and production workloads to make dataset training and optimization more efficient and accessible to more companies. \n\nThank you to our team, our partners, and most importantly our Release community for your collaboration, feedback, and suggestions throughout this year. It's been a wild year for all of us, with a lot of highs, lows, and challenges personally and professionally. It's gratifying and humbling to interact with this awesome community, whether on Slack, at industry events, or Zoom discussions. Here’s to a peaceful and prosperous 2024!\n",
          "code": "var Component=(()=>{var d=Object.create;var r=Object.defineProperty;var m=Object.getOwnPropertyDescriptor;var p=Object.getOwnPropertyNames;var g=Object.getPrototypeOf,f=Object.prototype.hasOwnProperty;var u=(a,e)=>()=>(e||a((e={exports:{}}).exports,e),e.exports),v=(a,e)=>{for(var o in e)r(a,o,{get:e[o],enumerable:!0})},i=(a,e,o,s)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let n of p(e))!f.call(a,n)&&n!==o&&r(a,n,{get:()=>e[n],enumerable:!(s=m(e,n))||s.enumerable});return a};var w=(a,e,o)=>(o=a!=null?d(g(a)):{},i(e||!a||!a.__esModule?r(o,\"default\",{value:a,enumerable:!0}):o,a)),y=a=>i(r({},\"__esModule\",{value:!0}),a);var h=u((I,l)=>{l.exports=_jsx_runtime});var x={};v(x,{default:()=>D,frontmatter:()=>b});var t=w(h()),b={title:\"Looking Back: A Big 2023 for Release\",summary:\"A look back at what happened over the past year and our favorite highlights from 2023.\",publishDate:\"Tue Dec 19 2023 21:05:27 GMT+0000 (Coordinated Universal Time)\",author:\"matt-carter\",readingTime:3,categories:[\"platform-engineering\",\"product\"],mainImage:\"/blog-images/3dc80af264c6c9aac260907524f62474.png\",imageAlt:\"2023 year in review for Release\",showCTA:!0,ctaCopy:\"Empower your team with Release's environment automation for faster data management and seamless container sharing.\",ctaLink:\"https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=looking-back-a-big-2023-for-release\",relatedPosts:[\"\"],ogImage:\"/blog-images/3dc80af264c6c9aac260907524f62474.png\",excerpt:\"A look back at what happened over the past year and our favorite highlights from 2023.\",tags:[\"platform-engineering\",\"product\"],ctaButton:\"Try Release for Free\"};function c(a){let e=Object.assign({p:\"p\",strong:\"strong\",a:\"a\"},a.components);return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(e.p,{children:\"When the holidays approach and a new year looms on the horizon, I always find myself looking back at what happened over the past 12 months. Reflecting on what the Release team achieved this year, especially in expanding ephemeral environment automation to more developers and use cases, is truly remarkable. Here are some of the highlights from 2023.\"}),`\n`,(0,t.jsxs)(e.p,{children:[\"\\u200D\",(0,t.jsx)(e.strong,{children:\"What\\u2019s in a Name\"})]}),`\n`,(0,t.jsxs)(e.p,{children:[\"Changing the company\\u2019s name \",(0,t.jsx)(e.a,{href:\"https://release.com/blog/meet-the-new-release\",children:\"from ReleaseHub to Release\"}),` was a significant milestone for us. This name more accurately reflects our vision and our mission from the inception to help people release their ideas to the world faster. It aligns well with the expanded use cases, capabilities, and technologies our development team has added to the product.\n`,(0,t.jsx)(e.strong,{children:\"Bigger and Better Data for Every Developer\"})]}),`\n`,(0,t.jsxs)(e.p,{children:[\"After the renaming, the next major release delivered improvements to Release Instant Datasets. Conversations with customers consistently underscored the importance of data to effective development and test processes and the complexity of managing massive data in the SDLC. Release Instant Datasets 2.0 made production-like data for developers even more powerful, with improvements to both the workflow and architecture of the tool. (CTO Erik Landerholm \",(0,t.jsx)(e.a,{href:\"https://release.com/blog/new-and-improved-instant-datasets\",children:\"goes into the details of these changes in this blog post\"}),\"). Shortly after, we made \",(0,t.jsx)(e.a,{href:\"https://release.com/blog/introducing-standalone-instant-datasets-build-and-test-with-realistic-production-like-data-with-ease\",children:\"Instant Datasets available as a standalone product\"}),`, enabling everyone to take advantage of data automation whether or not they are using the entire Release platform.\\xA0\n`,(0,t.jsx)(e.strong,{children:\"Docker Desktop Extension\"})]}),`\n`,(0,t.jsxs)(e.p,{children:[\"Instant Datasets became a crucial component of Release\\u2019s support across the software development life cycle. Another major piece of the SDLC puzzle was the launch of \",(0,t.jsx)(e.a,{href:\"https://release.com/product/docker-extension\",children:\"Release Share as a Docker Desktop extension\"}),`. Release Share provides fast simple container sharing to the millions of Docker developers directly from the Docker Desktop application. Shifting environment sharing left to the start of the development process means app delivery teams can create a self-serve version of their app for feedback and testing at every stage of the process.\n`,(0,t.jsx)(e.strong,{children:\"More Integrations for More Choice\"})]}),`\n`,(0,t.jsxs)(e.p,{children:[\"Docker is just one part of a developer\\u2019s toolbox we support at Release. Developers ask us all the time \\u201CDoes Release work with my [insert favorite tool name]?\\u201D We\\u2019re proud to have a broad set of partnerships and integrations with the tools and platforms favored by cloud-native app teams. Notably, we\\u2019ve improved our \",(0,t.jsx)(e.a,{href:\"https://release.com/blog/the-value-of-data-obfuscation-for-instant-datasets-tonic-meets-release\",children:\"integration with Tonic.ai\"}),\" for data automation and anonymization,\\xA0 developed \",(0,t.jsx)(e.a,{href:\"https://release.com/blog/gitlab-self-managed-now-available-on-release\",children:\"new integrations with GitLab\"}),` for more repo choices, and expanded our relationships with AWS and GCP. We\\u2019re excited about many significant new integrations coming in early 2024.\n`,(0,t.jsx)(e.strong,{children:\"The Year of AI\"}),\"\\xA0\"]}),`\n`,(0,t.jsxs)(e.p,{children:[\"And finally, no discussion of the last 12 months would be complete without talking about AI. ChatGPT and OpenAI are having a huge impact on the world, and the Release team \",(0,t.jsx)(e.a,{href:\"https://release.com/blog/training-chatgpt-with-custom-libraries-using-extensions\",children:\"has been creating innovative ways\"}),\" to use it to help make Release more powerful, and more importantly, make AI and dataset training more accessible to developers and teams. \",(0,t.jsx)(e.a,{href:\"https://release.ai/\",children:\"We launched release.ai as our hub for all things AI for DevOps.\"}),\" Today Release users can use interactive prompts with Release to \\u201Ctalk to your infrastructure\\u201D and get insights into their environments.\\xA0 We are also helping companies apply ephemeral environment automation to dataset training and production workloads to make dataset training and optimization more efficient and accessible to more companies.\\xA0\"]}),`\n`,(0,t.jsx)(e.p,{children:\"Thank you to our team, our partners, and most importantly our Release community for your collaboration, feedback, and suggestions throughout this year. It's been a wild year for all of us, with a lot of highs, lows, and challenges personally and professionally. It's gratifying and humbling to interact with this awesome community, whether on Slack, at industry events, or Zoom discussions. Here\\u2019s to a peaceful and prosperous 2024!\"})]})}function k(a={}){let{wrapper:e}=a.components||{};return e?(0,t.jsx)(e,Object.assign({},a,{children:(0,t.jsx)(c,a)})):c(a)}var D=k;return y(x);})();\n;return Component;"
        },
        "_id": "blog/posts/looking-back-a-big-2023-for-release.mdx",
        "_raw": {
          "sourceFilePath": "blog/posts/looking-back-a-big-2023-for-release.mdx",
          "sourceFileName": "looking-back-a-big-2023-for-release.mdx",
          "sourceFileDir": "blog/posts",
          "contentType": "mdx",
          "flattenedPath": "blog/posts/looking-back-a-big-2023-for-release"
        },
        "type": "BlogPost",
        "computedSlug": "looking-back-a-big-2023-for-release"
      },
      "documentHash": "1739393595024",
      "hasWarnings": false,
      "documentTypeName": "BlogPost"
    },
    "blog/posts/mat-werber-why-i-joined-release.mdx": {
      "document": {
        "title": "Mat Werber: Why I joined Release",
        "summary": "I’m very excited to join Release to build their new Solutions Architecture (SA) team.",
        "publishDate": "Thu Feb 24 2022 03:19:21 GMT+0000 (Coordinated Universal Time)",
        "author": "mat-werber",
        "readingTime": 4,
        "categories": [
          "platform-engineering",
          "kubernetes"
        ],
        "mainImage": "/blog-images/8662186006e5c23e06aa2ae6165576ee.jpg",
        "imageAlt": "Release Welcomes Mat Werber",
        "showCTA": true,
        "ctaCopy": "Simplify environment complexity like Kubernetes with Release's on-demand environments for seamless collaboration and efficient testing.",
        "ctaLink": "https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=mat-werber-why-i-joined-release",
        "relatedPosts": [
          ""
        ],
        "ogImage": "/blog-images/8662186006e5c23e06aa2ae6165576ee.jpg",
        "excerpt": "I’m very excited to join Release to build their new Solutions Architecture (SA) team.",
        "tags": [
          "platform-engineering",
          "kubernetes"
        ],
        "ctaButton": "Try Release for Free",
        "body": {
          "raw": "\n#### _Can you introduce yourself and tell us a little bit about your professional background and what you'll be doing at Release?_\n\nMy name is Mat Werber and I’ve worn many hats over my eleven-year career, ranging from IT & financial auditor, ERP implementation, business intelligence, and - most recently four years as a cloud solutions architect at AWS and Apple helping companies of virtually every size and industry innovate faster by building their applications in cloud environments. \n\nI’m very excited to join Release to build their new Solutions Architecture (SA) team. SAs serve as technical advisors, providing our customers like [Datasaur.ai](https://releasehub.com/casestudy/datasaurai) and [Monad](https://releasehub.com/casestudy/monad) with guidance that allows them to to bring their ideas to the world more easily, powered by the ephemeral and persistent environments-as-a-service offered by Release. \n\n#### *How did your experience at previous roles impact your decision to join Release?* \n\nIt’s really quite amazing when you look at how modern cloud services have and continue to transform the developer experience. As engineers, we no longer have to have to wait months or years to build data centers or establish contracts with colocation facilities, rack and stack hardware, or worry about heating, cooling, power, networking, or physical security. With a few API calls, we can offload all of that work to providers like AWS, GCP, Azure and others.  \n\nAt the same time, demand for infrastructure has and continues to skyrocket, driven by greater access to broadband internet, advancements in hardware and software, an explosion of data from new IoT, new streaming music and video platforms, and more. To keep up with this demand, commercial and open source communities continue to create new cloud services, scale-out technologies like Kubernetes and Apache Spark, and design patterns and tooling like microservices, serverless, and CI/CD.\n\nTime and time again, I saw that my customers adopting these technologies were merely shifting complexity to another team or service within the company rather than eliminating it. \n\nFor example, containers helped us solve“it worked on my machine, so why did it fail in staging?”, platforms like Kubernetes (K8s) solved “how do I run this at scale?”, and services like Amazon EKS solved “how do I easily _create_ and manage a K8s control plane?”. However, even with all of this in place, you still have a lot of work to do before that cluster is running your application in production.\n\nNow, I am _not_ picking on Kubernetes. It’s a great technology backed by an awesome open source community and used all across the globe. Rather, I only mean to illustrate the ever-growing complexity needed to build an environment. And, that’s just _one part_ of an environment… you also need to think about how you learn, build, and stitch together:  \n\n- Cloud VPC, DNS, IAM, security, scaling, load balancing\n- Infrastructure-as-code (Terraform, CloudFormation, Pulumi, etc.)\n- Integration and delivery/deployment pipelines\n\nAs a cloud architect, I had a unique vantage point that allowed me to see hundreds of customers deal with these same questions.  \n\nWhat really caught my attention was that even when my customers built a well-designed environment, much of their work inevitably became tech debt as they scaled. For example, at a certain point, they would outgrow their single dev/test/prod environments and realize that they’ll need to create _multiple_ copies of environments to prevent development from slowing to a crawl. Then, there’s the question of how to coordinate deployments when environments have dependencies with one another, rolling back environments due to drift, refreshing or resetting data in test / QA, added infrastructure cost of new environments, engineering-hours to operate all of this, and more. \n\n_Every customer_ I’ve met who develops software relies on environments, and whether boldly testing in prod (_please don’t!),_ managing a few environments or hundreds, they are _all_ encountering the same challenges and forced to reinvent and refactor the wheel on an ongoing basis. Sure, engineers are _paid_ to solve complex problems…but society benefits the most when those problems differentiate their business.\n\n#### _What makes you most excited about the mission and upcoming journey at Release?_\n\nI was first drawn to Release for the same reason I became a cloud architect: I’m passionate about helping others make the world a better place, driven by bringing ideas to life through automation and cloud. Release not only shares this vision but _delivers_ on it by handling the heavy lifting of infrastructure, pipelines, and data needed to transform their customers’ commits into reality. \n\nMy excitement only grew after seeing how comprehensive and widely applicable our platform was to the developer community. Just to name a few, we provide our customers with a multi-cloud solution that offers persistent and ephemeral environments, ephemeral databases with clean, near-production fidelity data, support for serverless, microservices and monoliths, custom cloud resources using customers’ infra-as-code framework of choice, and integration with partner services like LaunchDarkly.  \n\nMost importantly, the company culture and every teammate embody our founding principles and are driving toward the same mission: Allowing customers to release their ideas faster. We’re growing rapidly. If you’re interested, see more information on Release [here](https://releasehub.com/company), or reach me directly regarding open roles.\n",
          "code": "var Component=(()=>{var h=Object.create;var i=Object.defineProperty;var m=Object.getOwnPropertyDescriptor;var u=Object.getOwnPropertyNames;var p=Object.getPrototypeOf,g=Object.prototype.hasOwnProperty;var y=(t,e)=>()=>(e||t((e={exports:{}}).exports,e),e.exports),f=(t,e)=>{for(var a in e)i(t,a,{get:e[a],enumerable:!0})},s=(t,e,a,r)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let o of u(e))!g.call(t,o)&&o!==a&&i(t,o,{get:()=>e[o],enumerable:!(r=m(e,o))||r.enumerable});return t};var w=(t,e,a)=>(a=t!=null?h(p(t)):{},s(e||!t||!t.__esModule?i(a,\"default\",{value:t,enumerable:!0}):a,t)),b=t=>s(i({},\"__esModule\",{value:!0}),t);var c=y((j,l)=>{l.exports=_jsx_runtime});var I={};f(I,{default:()=>x,frontmatter:()=>v});var n=w(c()),v={title:\"Mat Werber: Why I joined Release\",summary:\"I\\u2019m very excited to join Release to build their new Solutions Architecture (SA) team.\",publishDate:\"Thu Feb 24 2022 03:19:21 GMT+0000 (Coordinated Universal Time)\",author:\"mat-werber\",readingTime:4,categories:[\"platform-engineering\",\"kubernetes\"],mainImage:\"/blog-images/8662186006e5c23e06aa2ae6165576ee.jpg\",imageAlt:\"Release Welcomes Mat Werber\",showCTA:!0,ctaCopy:\"Simplify environment complexity like Kubernetes with Release's on-demand environments for seamless collaboration and efficient testing.\",ctaLink:\"https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=mat-werber-why-i-joined-release\",relatedPosts:[\"\"],ogImage:\"/blog-images/8662186006e5c23e06aa2ae6165576ee.jpg\",excerpt:\"I\\u2019m very excited to join Release to build their new Solutions Architecture (SA) team.\",tags:[\"platform-engineering\",\"kubernetes\"],ctaButton:\"Try Release for Free\"};function d(t){let e=Object.assign({h4:\"h4\",a:\"a\",span:\"span\",em:\"em\",p:\"p\",ul:\"ul\",li:\"li\"},t.components);return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsxs)(e.h4,{id:\"can-you-introduce-yourself-and-tell-us-a-little-bit-about-your-professional-background-and-what-youll-be-doing-at-release\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#can-you-introduce-yourself-and-tell-us-a-little-bit-about-your-professional-background-and-what-youll-be-doing-at-release\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.em,{children:\"Can you introduce yourself and tell us a little bit about your professional background and what you'll be doing at Release?\"})]}),`\n`,(0,n.jsx)(e.p,{children:\"My name is Mat Werber and I\\u2019ve worn many hats over my eleven-year career, ranging from IT & financial auditor, ERP implementation, business intelligence, and - most recently four years as a cloud solutions architect at AWS and Apple helping companies of virtually every size and industry innovate faster by building their applications in cloud environments.\\xA0\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"I\\u2019m very excited to join Release to build their new Solutions Architecture (SA) team. SAs serve as technical advisors, providing our customers like \",(0,n.jsx)(e.a,{href:\"https://releasehub.com/casestudy/datasaurai\",children:\"Datasaur.ai\"}),\" and \",(0,n.jsx)(e.a,{href:\"https://releasehub.com/casestudy/monad\",children:\"Monad\"}),\" with guidance that allows them to to bring their ideas to the world more easily, powered by the ephemeral and persistent environments-as-a-service offered by Release.\\xA0\"]}),`\n`,(0,n.jsxs)(e.h4,{id:\"how-did-your-experience-at-previous-roles-impact-your-decision-to-join-release\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#how-did-your-experience-at-previous-roles-impact-your-decision-to-join-release\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.em,{children:\"How did your experience at previous roles impact your decision to join Release?\"}),\"\\xA0\"]}),`\n`,(0,n.jsx)(e.p,{children:\"It\\u2019s really quite amazing when you look at how modern cloud services have and continue to transform the developer experience. As engineers, we no longer have to have to wait months or years to build data centers or establish contracts with colocation facilities, rack and stack hardware, or worry about heating, cooling, power, networking, or physical security. With a few API calls, we can offload all of that work to providers like AWS, GCP, Azure and others.\\xA0\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:\"At the same time, demand for infrastructure has and continues to skyrocket, driven by greater access to broadband internet, advancements in hardware and software, an explosion of data from new IoT, new streaming music and video platforms, and more. To keep up with this demand, commercial and open source communities continue to create new cloud services, scale-out technologies like Kubernetes and Apache Spark, and design patterns and tooling like microservices, serverless, and CI/CD.\"}),`\n`,(0,n.jsx)(e.p,{children:\"Time and time again, I saw that my customers adopting these technologies were merely shifting complexity to another team or service within the company rather than eliminating it.\\xA0\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"For example, containers helped us solve\\u201Cit worked on my machine, so why did it fail in staging?\\u201D, platforms like Kubernetes (K8s) solved \\u201Chow do I run this at scale?\\u201D, and services like Amazon EKS solved \\u201Chow do I easily \",(0,n.jsx)(e.em,{children:\"create\"}),\" and manage a K8s control plane?\\u201D. However, even with all of this in place, you still have a lot of work to do before that cluster is running your application in production.\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"Now, I am \",(0,n.jsx)(e.em,{children:\"not\"}),\" picking on Kubernetes. It\\u2019s a great technology backed by an awesome open source community and used all across the globe. Rather, I only mean to illustrate the ever-growing complexity needed to build an environment. And, that\\u2019s just \",(0,n.jsx)(e.em,{children:\"one part\"}),\" of an environment\\u2026 you also need to think about how you learn, build, and stitch together:\\xA0\\xA0\"]}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsx)(e.li,{children:\"Cloud VPC, DNS, IAM, security, scaling, load balancing\"}),`\n`,(0,n.jsx)(e.li,{children:\"Infrastructure-as-code (Terraform, CloudFormation, Pulumi, etc.)\"}),`\n`,(0,n.jsx)(e.li,{children:\"Integration and delivery/deployment pipelines\"}),`\n`]}),`\n`,(0,n.jsx)(e.p,{children:\"As a cloud architect, I had a unique vantage point that allowed me to see hundreds of customers deal with these same questions.\\xA0\\xA0\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"What really caught my attention was that even when my customers built a well-designed environment, much of their work inevitably became tech debt as they scaled. For example, at a certain point, they would outgrow their single dev/test/prod environments and realize that they\\u2019ll need to create \",(0,n.jsx)(e.em,{children:\"multiple\"}),\" copies of environments to prevent development from slowing to a crawl. Then, there\\u2019s the question of how to coordinate deployments when environments have dependencies with one another, rolling back environments due to drift, refreshing or resetting data in test / QA, added infrastructure cost of new environments, engineering-hours to operate all of this, and more.\\xA0\"]}),`\n`,(0,n.jsxs)(e.p,{children:[(0,n.jsx)(e.em,{children:\"Every customer\"}),\" I\\u2019ve met who develops software relies on environments, and whether boldly testing in prod (\",(0,n.jsx)(e.em,{children:\"please don\\u2019t!),\"}),\" managing a few environments or hundreds, they are \",(0,n.jsx)(e.em,{children:\"all\"}),\" encountering the same challenges and forced to reinvent and refactor the wheel on an ongoing basis. Sure, engineers are \",(0,n.jsx)(e.em,{children:\"paid\"}),\" to solve complex problems\\u2026but society benefits the most when those problems differentiate their business.\"]}),`\n`,(0,n.jsxs)(e.h4,{id:\"what-makes-you-most-excited-about-the-mission-and-upcoming-journey-at-release\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#what-makes-you-most-excited-about-the-mission-and-upcoming-journey-at-release\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.em,{children:\"What makes you most excited about the mission and upcoming journey at Release?\"})]}),`\n`,(0,n.jsxs)(e.p,{children:[\"I was first drawn to Release for the same reason I became a cloud architect: I\\u2019m passionate about helping others make the world a better place, driven by bringing ideas to life through automation and cloud. Release not only shares this vision but \",(0,n.jsx)(e.em,{children:\"delivers\"}),\" on it by handling the heavy lifting of infrastructure, pipelines, and data needed to transform their customers\\u2019 commits into reality.\\xA0\"]}),`\n`,(0,n.jsx)(e.p,{children:\"My excitement only grew after seeing how comprehensive and widely applicable our platform was to the developer community. Just to name a few, we provide our customers with a multi-cloud solution that offers persistent and ephemeral environments, ephemeral databases with clean, near-production fidelity data, support for serverless, microservices and monoliths, custom cloud resources using customers\\u2019 infra-as-code framework of choice, and integration with partner services like LaunchDarkly.\\xA0\\xA0\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"Most importantly, the company culture and every teammate embody our founding principles and are driving toward the same mission: Allowing customers to release their ideas faster. We\\u2019re growing rapidly. If you\\u2019re interested, see more information on Release \",(0,n.jsx)(e.a,{href:\"https://releasehub.com/company\",children:\"here\"}),\", or reach me directly regarding open roles.\"]})]})}function k(t={}){let{wrapper:e}=t.components||{};return e?(0,n.jsx)(e,Object.assign({},t,{children:(0,n.jsx)(d,t)})):d(t)}var x=k;return b(I);})();\n;return Component;"
        },
        "_id": "blog/posts/mat-werber-why-i-joined-release.mdx",
        "_raw": {
          "sourceFilePath": "blog/posts/mat-werber-why-i-joined-release.mdx",
          "sourceFileName": "mat-werber-why-i-joined-release.mdx",
          "sourceFileDir": "blog/posts",
          "contentType": "mdx",
          "flattenedPath": "blog/posts/mat-werber-why-i-joined-release"
        },
        "type": "BlogPost",
        "computedSlug": "mat-werber-why-i-joined-release"
      },
      "documentHash": "1739393595024",
      "hasWarnings": false,
      "documentTypeName": "BlogPost"
    },
    "blog/posts/meet-the-new-release.mdx": {
      "document": {
        "title": "Meet the new Release!",
        "summary": "New year, new name. Check out the new Release!",
        "publishDate": "Fri Jan 13 2023 20:27:52 GMT+0000 (Coordinated Universal Time)",
        "author": "matt-carter",
        "readingTime": 2,
        "categories": [
          "platform-engineering",
          "product"
        ],
        "mainImage": "/blog-images/4b10ddea9172d8780dd08c36f90b18fa.jpg",
        "imageAlt": "Meet the new Release!",
        "showCTA": true,
        "ctaCopy": "Simplify your configuration changes with Release's ephemeral environments for seamless app delivery workflows.",
        "ctaLink": "https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=meet-the-new-release",
        "relatedPosts": [
          ""
        ],
        "ogImage": "/blog-images/4b10ddea9172d8780dd08c36f90b18fa.jpg",
        "excerpt": "New year, new name. Check out the new Release!",
        "tags": [
          "platform-engineering",
          "product"
        ],
        "ctaButton": "Try Release for Free",
        "body": {
          "raw": "\nIt's a new year, and we’re making some changes to our brand and web property. Moving forward, we are now known as simply Release, and our domain name is release.com. This change doesn’t impact the functionality of our current environments-as-a-service offerings, but better reflects how we support the release tasks of app delivery teams across the lifecycle.\n\nThis change may require you to make some minor changes to your configuration files moving forward. All existing releasehub.com pages will redirect to release.com equivalents, which might require reworking your CI/CD configuration files. APIs using ReleaseHub will continue to work after this change.\n\nCustomers who have questions about the change are encouraged to post these to their private Release Slack channel so our team can address them.\n\nThe product roadmap for the year ahead is exciting and is directly evolved from feedback from our users. We’re excited to make this change to our brand and website, and how it sets us up for the years ahead.\n\nI look forward to seeing the great things you release next!\n",
          "code": "var Component=(()=>{var u=Object.create;var o=Object.defineProperty;var h=Object.getOwnPropertyDescriptor;var d=Object.getOwnPropertyNames;var g=Object.getPrototypeOf,p=Object.prototype.hasOwnProperty;var f=(t,e)=>()=>(e||t((e={exports:{}}).exports,e),e.exports),w=(t,e)=>{for(var r in e)o(t,r,{get:e[r],enumerable:!0})},i=(t,e,r,s)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let n of d(e))!p.call(t,n)&&n!==r&&o(t,n,{get:()=>e[n],enumerable:!(s=h(e,n))||s.enumerable});return t};var y=(t,e,r)=>(r=t!=null?u(g(t)):{},i(e||!t||!t.__esModule?o(r,\"default\",{value:t,enumerable:!0}):r,t)),b=t=>i(o({},\"__esModule\",{value:!0}),t);var m=f((j,c)=>{c.exports=_jsx_runtime});var _={};w(_,{default:()=>v,frontmatter:()=>k});var a=y(m()),k={title:\"Meet the new Release!\",summary:\"New year, new name. Check out the new Release!\",publishDate:\"Fri Jan 13 2023 20:27:52 GMT+0000 (Coordinated Universal Time)\",author:\"matt-carter\",readingTime:2,categories:[\"platform-engineering\",\"product\"],mainImage:\"/blog-images/4b10ddea9172d8780dd08c36f90b18fa.jpg\",imageAlt:\"Meet the new Release!\",showCTA:!0,ctaCopy:\"Simplify your configuration changes with Release's ephemeral environments for seamless app delivery workflows.\",ctaLink:\"https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=meet-the-new-release\",relatedPosts:[\"\"],ogImage:\"/blog-images/4b10ddea9172d8780dd08c36f90b18fa.jpg\",excerpt:\"New year, new name. Check out the new Release!\",tags:[\"platform-engineering\",\"product\"],ctaButton:\"Try Release for Free\"};function l(t){let e=Object.assign({p:\"p\"},t.components);return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(e.p,{children:\"It's a new year, and we\\u2019re making some changes to our brand and web property. Moving forward, we are now known as simply Release, and our domain name is release.com. This change doesn\\u2019t impact the functionality of our current environments-as-a-service offerings, but better reflects how we support the release tasks of app delivery teams across the lifecycle.\"}),`\n`,(0,a.jsx)(e.p,{children:\"This change may require you to make some minor changes to your configuration files moving forward. All existing releasehub.com pages will redirect to release.com equivalents, which might require reworking your CI/CD configuration files. APIs using ReleaseHub will continue to work after this change.\"}),`\n`,(0,a.jsx)(e.p,{children:\"Customers who have questions about the change are encouraged to post these to their private Release Slack channel so our team can address them.\"}),`\n`,(0,a.jsx)(e.p,{children:\"The product roadmap for the year ahead is exciting and is directly evolved from feedback from our users. We\\u2019re excited to make this change to our brand and website, and how it sets us up for the years ahead.\"}),`\n`,(0,a.jsx)(e.p,{children:\"I look forward to seeing the great things you release next!\"})]})}function x(t={}){let{wrapper:e}=t.components||{};return e?(0,a.jsx)(e,Object.assign({},t,{children:(0,a.jsx)(l,t)})):l(t)}var v=x;return b(_);})();\n;return Component;"
        },
        "_id": "blog/posts/meet-the-new-release.mdx",
        "_raw": {
          "sourceFilePath": "blog/posts/meet-the-new-release.mdx",
          "sourceFileName": "meet-the-new-release.mdx",
          "sourceFileDir": "blog/posts",
          "contentType": "mdx",
          "flattenedPath": "blog/posts/meet-the-new-release"
        },
        "type": "BlogPost",
        "computedSlug": "meet-the-new-release"
      },
      "documentHash": "1739393595024",
      "hasWarnings": false,
      "documentTypeName": "BlogPost"
    },
    "blog/posts/moderncto-podcast-recap.mdx": {
      "document": {
        "title": "ModernCTO Podcast Recap",
        "summary": "In this episode of the ModernCTO podcast, Joel Beasley talks to Tommy, co-founder and CEO of Release.",
        "publishDate": "Thu Apr 14 2022 07:06:51 GMT+0000 (Coordinated Universal Time)",
        "author": "sam-allen",
        "readingTime": 3,
        "categories": [
          "platform-engineering",
          "product"
        ],
        "mainImage": "/blog-images/5f7385453525a9f0c74784ba0acdbd18.png",
        "imageAlt": "Release on ModernCTO Podcast",
        "showCTA": true,
        "ctaCopy": "Discover how Release simplifies environment management for faster deployment cycles and seamless collaboration.",
        "ctaLink": "https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=moderncto-podcast-recap",
        "relatedPosts": [
          ""
        ],
        "ogImage": "/blog-images/5f7385453525a9f0c74784ba0acdbd18.png",
        "excerpt": "In this episode of the ModernCTO podcast, Joel Beasley talks to Tommy, co-founder and CEO of Release.",
        "tags": [
          "platform-engineering",
          "product"
        ],
        "ctaButton": "Try Release for Free",
        "body": {
          "raw": "\nIn this [episode of the ModernCTO podcast](https://moderncto.io/tommy-mcclung/), Joel Beasley talks to Tommy, co-founder and CEO of Release. They discuss how Tommy founded Release to solve the huge problem of quickly spinning up environments, why the best entrepreneurs are always extremely persistent, and why being vulnerable goes a long way as a leader.\n**Tommy**: *I know how it is to be an entrepreneur. I know the journey you've been through a bit.* \n\n_I guess we have some glutton for punishment or something. I don't know what it is. I just think about going to work for someone else, and I just can't make myself. I mean, I did. I've done it._\n\n_I was the CTO at True Car before this. After a company I started got acquired, the whole time I was there, I could just not get it out of my head that I wanted to do it again. So, there's something about it. That's exciting and freeing. Scary, but I don't know. That's what makes me tick is just going out there and trying to do something crazy._\n**Joel**: _Absolutely. And I love what you guys are doing with Release because you know, my background, like I said, I was a software developer for 17 years. I understand environments and all of that. And when I saw what you guys were doing, I think it was like a month or two ago, I was like, legitimately, this is one of the coolest things I've seen in a long time._\n\n*So I was hoping you could just explain what Release is, what it does.* \n**Tommy**: _Yeah. So Release is environments as a service. Environments are needed for developers, for everything from developing their code, testing, and QA. Running it for their customers. Obviously your production environment is where the customer sees the end result of your work._\n\n*In modern software delivery, there's all sorts of new delivery mechanisms where you might have a customer who comes to you and says, I don't want to have my data co-mingled with all of your other customers in a multi-tenant kind of environment.* \n\n_So, I would rather have a version of what you're selling me, isolated to myself, a single tenant version of that. Sometimes, our customers host those for their customers, like a direct single tenant environment. Sometimes they deliver those into their cloud accounts. So, they're kind of like an enterprise version of their applications. And so, environments are just kind of like this absolutely necessary fundamental part of the software ecosystem._\n\n*Development, all the way through to getting your ideas in front of your customers. They're just really hard. They've always been really hard. Release makes it really easy to spin up and down environments on demand.* \n\n_So, you know, as the CTO of a public company for a, for a handful of years and got to see, you know, what does it take to run a large engineering organization and make them effective and efficient and environments were a really difficult problem to solve, especially when you get to a larger scale company where the applications get really complicated._\n\n_It's not just a rails app and that's all you're running or a JS app. It's got data pipelines, it's got ETL that needs to happen. It's got, you know, lots of different applications, internal and external. Then, you have big teams of engineers trying to collaborate and they all tend to bottleneck around._\n\n_Well, I got to use the staging environment or QA is being used by this team. And so I've got to wait. I really, really felt the pain in trying to get an engineering organization efficient and environments were kind of a headache there. So, me and my co-founders decided we were going to start a company to solve that problem because nothing really existed in the market._\n\nListen to the full episode [here](https://moderncto.io/tommy-mcclung/).\n",
          "code": "var Component=(()=>{var m=Object.create;var r=Object.defineProperty;var c=Object.getOwnPropertyDescriptor;var u=Object.getOwnPropertyNames;var g=Object.getPrototypeOf,y=Object.prototype.hasOwnProperty;var p=(n,e)=>()=>(e||n((e={exports:{}}).exports,e),e.exports),f=(n,e)=>{for(var o in e)r(n,o,{get:e[o],enumerable:!0})},i=(n,e,o,s)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let a of u(e))!y.call(n,a)&&a!==o&&r(n,a,{get:()=>e[a],enumerable:!(s=c(e,a))||s.enumerable});return n};var w=(n,e,o)=>(o=n!=null?m(g(n)):{},i(e||!n||!n.__esModule?r(o,\"default\",{value:n,enumerable:!0}):o,n)),v=n=>i(r({},\"__esModule\",{value:!0}),n);var d=p((C,l)=>{l.exports=_jsx_runtime});var T={};f(T,{default:()=>k,frontmatter:()=>b});var t=w(d()),b={title:\"ModernCTO Podcast Recap\",summary:\"In this episode of the ModernCTO podcast, Joel Beasley talks to Tommy, co-founder and CEO of Release.\",publishDate:\"Thu Apr 14 2022 07:06:51 GMT+0000 (Coordinated Universal Time)\",author:\"sam-allen\",readingTime:3,categories:[\"platform-engineering\",\"product\"],mainImage:\"/blog-images/5f7385453525a9f0c74784ba0acdbd18.png\",imageAlt:\"Release on ModernCTO Podcast\",showCTA:!0,ctaCopy:\"Discover how Release simplifies environment management for faster deployment cycles and seamless collaboration.\",ctaLink:\"https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=moderncto-podcast-recap\",relatedPosts:[\"\"],ogImage:\"/blog-images/5f7385453525a9f0c74784ba0acdbd18.png\",excerpt:\"In this episode of the ModernCTO podcast, Joel Beasley talks to Tommy, co-founder and CEO of Release.\",tags:[\"platform-engineering\",\"product\"],ctaButton:\"Try Release for Free\"};function h(n){let e=Object.assign({p:\"p\",a:\"a\",strong:\"strong\",em:\"em\"},n.components);return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsxs)(e.p,{children:[\"In this \",(0,t.jsx)(e.a,{href:\"https://moderncto.io/tommy-mcclung/\",children:\"episode of the ModernCTO podcast\"}),`, Joel Beasley talks to Tommy, co-founder and CEO of Release. They discuss how Tommy founded Release to solve the huge problem of quickly spinning up environments, why the best entrepreneurs are always extremely persistent, and why being vulnerable goes a long way as a leader.\n`,(0,t.jsx)(e.strong,{children:\"Tommy\"}),\": \",(0,t.jsx)(e.em,{children:\"I know how it is to be an entrepreneur. I know the journey you've been through a bit.\"}),\"\\xA0\"]}),`\n`,(0,t.jsx)(e.p,{children:(0,t.jsx)(e.em,{children:\"I guess we have some glutton for punishment or something. I don't know what it is. I just think about going to work for someone else, and I just can't make myself. I mean, I did. I've done it.\"})}),`\n`,(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.em,{children:\"I was the CTO at True Car before this. After a company I started got acquired, the whole time I was there, I could just not get it out of my head that I wanted to do it again. So, there's something about it. That's exciting and freeing. Scary, but I don't know. That's what makes me tick is just going out there and trying to do something crazy.\"}),`\n`,(0,t.jsx)(e.strong,{children:\"Joel\"}),\": \",(0,t.jsx)(e.em,{children:\"Absolutely. And I love what you guys are doing with Release because you know, my background, like I said, I was a software developer for 17 years. I understand environments and all of that. And when I saw what you guys were doing, I think it was like a month or two ago, I was like, legitimately, this is one of the coolest things I've seen in a long time.\"})]}),`\n`,(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.em,{children:\"So I was hoping you could just explain what Release is, what it does.\"}),`\\xA0\n`,(0,t.jsx)(e.strong,{children:\"Tommy\"}),\": \",(0,t.jsx)(e.em,{children:\"Yeah. So Release is environments as a service. Environments are needed for developers, for everything from developing their code, testing, and QA. Running it for their customers. Obviously your production environment is where the customer sees the end result of your work.\"})]}),`\n`,(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.em,{children:\"In modern software delivery, there's all sorts of new delivery mechanisms where you might have a customer who comes to you and says, I don't want to have my data co-mingled with all of your other customers in a multi-tenant kind of environment.\"}),\"\\xA0\"]}),`\n`,(0,t.jsx)(e.p,{children:(0,t.jsx)(e.em,{children:\"So, I would rather have a version of what you're selling me, isolated to myself, a single tenant version of that. Sometimes, our customers host those for their customers, like a direct single tenant environment. Sometimes they deliver those into their cloud accounts. So, they're kind of like an enterprise version of their applications. And so, environments are just kind of like this absolutely necessary fundamental part of the software ecosystem.\"})}),`\n`,(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.em,{children:\"Development, all the way through to getting your ideas in front of your customers. They're just really hard. They've always been really hard. Release makes it really easy to spin up and down environments on demand.\"}),\"\\xA0\"]}),`\n`,(0,t.jsx)(e.p,{children:(0,t.jsx)(e.em,{children:\"So, you know, as the CTO of a public company for a, for a handful of years and got to see, you know, what does it take to run a large engineering organization and make them effective and efficient and environments were a really difficult problem to solve, especially when you get to a larger scale company where the applications get really complicated.\"})}),`\n`,(0,t.jsx)(e.p,{children:(0,t.jsx)(e.em,{children:\"It's not just a rails app and that's all you're running or a JS app. It's got data pipelines, it's got ETL that needs to happen. It's got, you know, lots of different applications, internal and external. Then, you have big teams of engineers trying to collaborate and they all tend to bottleneck around.\"})}),`\n`,(0,t.jsx)(e.p,{children:(0,t.jsx)(e.em,{children:\"Well, I got to use the staging environment or QA is being used by this team. And so I've got to wait. I really, really felt the pain in trying to get an engineering organization efficient and environments were kind of a headache there. So, me and my co-founders decided we were going to start a company to solve that problem because nothing really existed in the market.\"})}),`\n`,(0,t.jsxs)(e.p,{children:[\"Listen to the full episode \",(0,t.jsx)(e.a,{href:\"https://moderncto.io/tommy-mcclung/\",children:\"here\"}),\".\"]})]})}function I(n={}){let{wrapper:e}=n.components||{};return e?(0,t.jsx)(e,Object.assign({},n,{children:(0,t.jsx)(h,n)})):h(n)}var k=I;return v(T);})();\n;return Component;"
        },
        "_id": "blog/posts/moderncto-podcast-recap.mdx",
        "_raw": {
          "sourceFilePath": "blog/posts/moderncto-podcast-recap.mdx",
          "sourceFileName": "moderncto-podcast-recap.mdx",
          "sourceFileDir": "blog/posts",
          "contentType": "mdx",
          "flattenedPath": "blog/posts/moderncto-podcast-recap"
        },
        "type": "BlogPost",
        "computedSlug": "moderncto-podcast-recap"
      },
      "documentHash": "1739393595024",
      "hasWarnings": false,
      "documentTypeName": "BlogPost"
    },
    "blog/posts/new-and-improved-instant-datasets.mdx": {
      "document": {
        "title": "New and Improved Instant Datasets 2.0",
        "summary": "Our most popular feature gets an upgrade. Now it's even easier to build and test with production-like data.",
        "publishDate": "Tue Jul 11 2023 20:19:22 GMT+0000 (Coordinated Universal Time)",
        "author": "erik-landerholm",
        "readingTime": 5,
        "categories": [
          "platform-engineering",
          "product"
        ],
        "mainImage": "/blog-images/9caeec0a356eac449e6eef994c90f2d1.jpg",
        "imageAlt": "New and Improved Instant Datasets 2.0",
        "showCTA": true,
        "ctaCopy": "For instant access to production-like data in your ephemeral environments, try Release's environment management platform today.",
        "ctaLink": "https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=new-and-improved-instant-datasets",
        "relatedPosts": [
          "12-things-you-didnt-know-you-could-do-with-release-part-1"
        ],
        "ogImage": "/blog-images/9caeec0a356eac449e6eef994c90f2d1.jpg",
        "excerpt": "Our most popular feature gets an upgrade. Now it's even easier to build and test with production-like data.",
        "tags": [
          "platform-engineering",
          "product"
        ],
        "ctaButton": "Try Release for Free",
        "body": {
          "raw": "\nI don’t know about you, but for me the most frustrating thing about local development, remote development, basically any kind of development is getting access to production or production-like data. Getting data, modifying data, securing data, and removing data when finished, these are some of the hardest things about dealing with multiple environments. And almost any sufficiently complicated feature development can benefit from real data. Ephemeral environments are great for developing and testing complicated features, solving race conditions, etc, but none of that is possible without something that gives you instant access to production-like data.\n\nWe created Instant Datasets to allow you to get access to production-like data for your ephemeral environments, instantly.  If you are an existing Release user, you know that Instant Datasets capability has been part of Release from the very beginning and is one of the most popular features of the platform. As such, we are constantly thinking about ways to make it faster, better, and more useful to all developers. Our most recent set of improvements include an architectural overhaul, addition of a new key functionality – a native data obfuscation integration, and (soon) the ability to use Instant Datasets as a stand-alone product, launching July 25th.\n\n### What’s New?\n\nUnder the hood Instant Datasets 1.0 consisted of a homegrown workflow engine, a few state machines and background job processing. This has served us well, but when we decided to improve our data cleansing options, we knew we needed to use a real workflow engine.\n\nBefore, you needed to clean your production data during the deployment process. This greatly increased the time of deployment and was incumbent on our customer to manage this process. The new architecture allows us to simplify the process and add integrations our customers wanted.\n\nWe had great experiences implementing [Temporal](https://github.com/temporalio/temporal) for internal workflows and knew it would also serve us well in this application. Temporal gives us the scalability, durability, performance and extensibility we need to create our world class platform. At the moment, working with Temporal and Ruby/Rails is not a trivial exercise and we faced a number of interesting challenges making it work. But overall it was the right choice for this task. (Keep your eyes open for a deeper dive into how we use Temporal at Release!)\n\nNow, this new architecture gives us the tools to build a workflow that allows our customers to plug in ANY data obfuscation tool as a custom task. To make it even simpler for our users, we created a native integration with [Tonic.ai](http://tonic.ai/) to quickly build an obfuscated dataset and easily use it with your environments. Here is how the Tonic integration works:\n\n![](/blog-images/9b9c951a1aa3dc01dde04a001cb18cc0.png)\n\n_Tonic and Release instant dataset creation workflows._\n\nAll configurations are done in one simple UI in Release. You select your RDS snapshot. Release creates source and destination databases to be processed in Tonic. You provide your Tonic api and workspace ids and Tonic does its magic. Once finished, you get a pool of obfuscated, cleaned, truncated and otherwise modified databases instantly available for all your environments!\n\n![](/blog-images/2033fb58c0704aac9a6081f68f8f3a7e.png)\n\n_Select the source snapshot and configure Tonic all in one simple UI_\n\nTo learn more about the Tonic integration, see the docs [here](https://docs.release.com/reference-documentation/instant-dataset-tasks/tonic-cloud). For more information on Instant Datasets in Release check out our documentation on AWS [here](https://docs.release.com/reference-documentation/instant-datasets-aws) and GCP [here](https://docs.release.com/reference-documentation/instant-datasets-gcp).\n\n_\\*At the time of this post, we have not finished migrating all of our dataset integrations from our Instant Datasets 1.0 architecture to 2.0. For now, the ability to cleanse your data is limited to AWS (RDS and Aurora), but we are actively working on our GCP support and it will be available shortly\\*_  \n\n### What’s Next?\n\nWith our new architecture in hand we have a lot of new functionality to add to Instant Datasets in the coming weeks:\n\n- **Add GCP support on our new architecture.**  At the moment we only support our Tonic integration and new architecture when using AWS databases, both RDS and Aurora are supported. We have support for [CloudSql](https://cloud.google.com/sql) in our Instant Datasets 1.0 and will have that same functionality plus the Tonic integration very soon.\n- **Bring your own obfuscation tool!** Tonic is great, we use it, but if you have other tools or just scripts in a container you would like to run when creating your Instant Dataset we will have support for that in the next couple of weeks also.\n- **More Integrations!** Instant Datasets are useful beyond your traditional databases.  Any set of data that can be cloned or used to create an example can be used by Release to create an Instant Dataset for your environments. We will be adding support for MongoDB Atlas, Neon Serverless, and more in the near future!\n- **Stand-alone Instant Datasets** soon available to all developers, regardless if they already use the Release platform or not. We believe that production-like data is the best kind of data to develop and test with, so we are making Instant Datasets available to everyone, at no cost. [Sign up to be the first to know when stand-alone Instant Datasets launches on July 25th.](https://www2.release.com/instant-datasets)\n\n### What’s in it for you?\n\nTesting and developing with production-like data has always been our default at Release. It allows us to prevent rework, makes potential bugs apparent much sooner, and gives us a realistic preview of how our app will behave under actual loads. We want to share those benefits with the wider developer community and continue adding useful functionality to our platform. Take the new and improved Instant Datasets for a spin and [let us know](mailto:hello@release.com) what you think (and what we should add next).\n",
          "code": "var Component=(()=>{var u=Object.create;var i=Object.defineProperty;var h=Object.getOwnPropertyDescriptor;var p=Object.getOwnPropertyNames;var m=Object.getPrototypeOf,g=Object.prototype.hasOwnProperty;var f=(a,e)=>()=>(e||a((e={exports:{}}).exports,e),e.exports),w=(a,e)=>{for(var n in e)i(a,n,{get:e[n],enumerable:!0})},r=(a,e,n,s)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let o of p(e))!g.call(a,o)&&o!==n&&i(a,o,{get:()=>e[o],enumerable:!(s=h(e,o))||s.enumerable});return a};var y=(a,e,n)=>(n=a!=null?u(m(a)):{},r(e||!a||!a.__esModule?i(n,\"default\",{value:a,enumerable:!0}):n,a)),b=a=>r(i({},\"__esModule\",{value:!0}),a);var d=f((x,l)=>{l.exports=_jsx_runtime});var I={};w(I,{default:()=>T,frontmatter:()=>v});var t=y(d()),v={title:\"New and Improved Instant Datasets 2.0\",summary:\"Our most popular feature gets an upgrade. Now it's even easier to build and test with production-like data.\",publishDate:\"Tue Jul 11 2023 20:19:22 GMT+0000 (Coordinated Universal Time)\",author:\"erik-landerholm\",readingTime:5,categories:[\"platform-engineering\",\"product\"],mainImage:\"/blog-images/9caeec0a356eac449e6eef994c90f2d1.jpg\",imageAlt:\"New and Improved Instant Datasets 2.0\",showCTA:!0,ctaCopy:\"For instant access to production-like data in your ephemeral environments, try Release's environment management platform today.\",ctaLink:\"https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=new-and-improved-instant-datasets\",relatedPosts:[\"12-things-you-didnt-know-you-could-do-with-release-part-1\"],ogImage:\"/blog-images/9caeec0a356eac449e6eef994c90f2d1.jpg\",excerpt:\"Our most popular feature gets an upgrade. Now it's even easier to build and test with production-like data.\",tags:[\"platform-engineering\",\"product\"],ctaButton:\"Try Release for Free\"};function c(a){let e=Object.assign({p:\"p\",h3:\"h3\",a:\"a\",span:\"span\",img:\"img\",em:\"em\",ul:\"ul\",li:\"li\",strong:\"strong\"},a.components);return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(e.p,{children:\"I don\\u2019t know about you, but for me the most frustrating thing about local development, remote development, basically any kind of development is getting access to production or production-like data. Getting data, modifying data, securing data, and removing data when finished, these are some of the hardest things about dealing with multiple environments. And almost any sufficiently complicated feature development can benefit from real data. Ephemeral environments are great for developing and testing complicated features, solving race conditions, etc, but none of that is possible without something that gives you instant access to production-like data.\"}),`\n`,(0,t.jsx)(e.p,{children:\"We created Instant Datasets to allow you to get access to production-like data for your ephemeral environments, instantly. \\xA0If you are an existing Release user, you know that Instant Datasets capability has been part of Release from the very beginning and is one of the most popular features of the platform. As such, we are constantly thinking about ways to make it faster, better, and more useful to all developers. Our most recent set of improvements include an architectural overhaul, addition of a new key functionality \\u2013 a native data obfuscation integration, and (soon) the ability to use Instant Datasets as a stand-alone product, launching July 25th.\"}),`\n`,(0,t.jsxs)(e.h3,{id:\"whats-new\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#whats-new\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"What\\u2019s New?\"]}),`\n`,(0,t.jsx)(e.p,{children:\"Under the hood Instant Datasets 1.0 consisted of a homegrown workflow engine, a few state machines and background job processing. This has served us well, but when we decided to improve our data cleansing options, we knew we needed to use a real workflow engine.\"}),`\n`,(0,t.jsx)(e.p,{children:\"Before, you needed to clean your production data during the deployment process. This greatly increased the time of deployment and was incumbent on our customer to manage this process. The new architecture allows us to simplify the process and add integrations our customers wanted.\"}),`\n`,(0,t.jsxs)(e.p,{children:[\"We had great experiences implementing \",(0,t.jsx)(e.a,{href:\"https://github.com/temporalio/temporal\",children:\"Temporal\"}),\" for internal workflows and knew it would also serve us well in this application. Temporal gives us the scalability, durability, performance and extensibility we need to create our world class platform. At the moment, working with Temporal and Ruby/Rails is not a trivial exercise and we faced a number of interesting challenges making it work. But overall it was the right choice for this task. (Keep your eyes open for a deeper dive into how we use Temporal at Release!)\"]}),`\n`,(0,t.jsxs)(e.p,{children:[\"Now, this new architecture gives us the tools to build a workflow that allows our customers to plug in ANY data obfuscation tool as a custom task. To make it even simpler for our users, we created a native integration with \",(0,t.jsx)(e.a,{href:\"http://tonic.ai/\",children:\"Tonic.ai\"}),\" to quickly build an obfuscated dataset and easily use it with your environments. Here is how the Tonic integration works:\"]}),`\n`,(0,t.jsx)(e.p,{children:(0,t.jsx)(e.img,{src:\"/blog-images/9b9c951a1aa3dc01dde04a001cb18cc0.png\",alt:\"\"})}),`\n`,(0,t.jsx)(e.p,{children:(0,t.jsx)(e.em,{children:\"Tonic and Release instant dataset creation workflows.\"})}),`\n`,(0,t.jsx)(e.p,{children:\"All configurations are done in one simple UI in Release. You select your RDS snapshot. Release creates source and destination databases to be processed in Tonic. You provide your Tonic api and workspace ids and Tonic does its magic. Once finished, you get a pool of obfuscated, cleaned, truncated and otherwise modified databases instantly available for all your environments!\"}),`\n`,(0,t.jsx)(e.p,{children:(0,t.jsx)(e.img,{src:\"/blog-images/2033fb58c0704aac9a6081f68f8f3a7e.png\",alt:\"\"})}),`\n`,(0,t.jsx)(e.p,{children:(0,t.jsx)(e.em,{children:\"Select the source snapshot and configure Tonic all in one simple UI\"})}),`\n`,(0,t.jsxs)(e.p,{children:[\"To learn more about the Tonic integration, see the docs \",(0,t.jsx)(e.a,{href:\"https://docs.release.com/reference-documentation/instant-dataset-tasks/tonic-cloud\",children:\"here\"}),\". For more information on Instant Datasets in Release check out our documentation on AWS \",(0,t.jsx)(e.a,{href:\"https://docs.release.com/reference-documentation/instant-datasets-aws\",children:\"here\"}),\" and GCP \",(0,t.jsx)(e.a,{href:\"https://docs.release.com/reference-documentation/instant-datasets-gcp\",children:\"here\"}),\".\"]}),`\n`,(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.em,{children:\"*At the time of this post, we have not finished migrating all of our dataset integrations from our Instant Datasets 1.0 architecture to 2.0. For now, the ability to cleanse your data is limited to AWS (RDS and Aurora), but we are actively working on our GCP support and it will be available shortly*\"}),\" \\xA0\"]}),`\n`,(0,t.jsxs)(e.h3,{id:\"whats-next\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#whats-next\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"What\\u2019s Next?\"]}),`\n`,(0,t.jsx)(e.p,{children:\"With our new architecture in hand we have a lot of new functionality to add to Instant Datasets in the coming weeks:\"}),`\n`,(0,t.jsxs)(e.ul,{children:[`\n`,(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:\"Add GCP support on our new architecture.\"}),\" \\xA0At the moment we only support our Tonic integration and new architecture when using AWS databases, both RDS and Aurora are supported. We have support for \",(0,t.jsx)(e.a,{href:\"https://cloud.google.com/sql\",children:\"CloudSql\"}),\" in our Instant Datasets 1.0 and will have that same functionality plus the Tonic integration very soon.\"]}),`\n`,(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:\"Bring your own obfuscation tool!\"}),\" Tonic is great, we use it, but if you have other tools or just scripts in a container you would like to run when creating your Instant Dataset we will have support for that in the next couple of weeks also.\"]}),`\n`,(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:\"More Integrations!\"}),\" Instant Datasets are useful beyond your traditional databases. \\xA0Any set of data that can be cloned or used to create an example can be used by Release to create an Instant Dataset for your environments. We will be adding support for MongoDB Atlas, Neon Serverless, and more in the near future!\"]}),`\n`,(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:\"Stand-alone Instant Datasets\"}),\" soon available to all developers, regardless if they already use the Release platform or not. We believe that production-like data is the best kind of data to develop and test with, so we are making Instant Datasets available to everyone, at no cost. \",(0,t.jsx)(e.a,{href:\"https://www2.release.com/instant-datasets\",children:\"Sign up to be the first to know when stand-alone Instant Datasets launches on July 25th.\"})]}),`\n`]}),`\n`,(0,t.jsxs)(e.h3,{id:\"whats-in-it-for-you\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#whats-in-it-for-you\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"What\\u2019s in it for you?\"]}),`\n`,(0,t.jsxs)(e.p,{children:[\"Testing and developing with production-like data has always been our default at Release. It allows us to prevent rework, makes potential bugs apparent much sooner, and gives us a realistic preview of how our app will behave under actual loads. We want to share those benefits with the wider developer community and continue adding useful functionality to our platform. Take the new and improved Instant Datasets for a spin and \",(0,t.jsx)(e.a,{href:\"mailto:hello@release.com\",children:\"let us know\"}),\" what you think (and what we should add next).\"]})]})}function k(a={}){let{wrapper:e}=a.components||{};return e?(0,t.jsx)(e,Object.assign({},a,{children:(0,t.jsx)(c,a)})):c(a)}var T=k;return b(I);})();\n;return Component;"
        },
        "_id": "blog/posts/new-and-improved-instant-datasets.mdx",
        "_raw": {
          "sourceFilePath": "blog/posts/new-and-improved-instant-datasets.mdx",
          "sourceFileName": "new-and-improved-instant-datasets.mdx",
          "sourceFileDir": "blog/posts",
          "contentType": "mdx",
          "flattenedPath": "blog/posts/new-and-improved-instant-datasets"
        },
        "type": "BlogPost",
        "computedSlug": "new-and-improved-instant-datasets"
      },
      "documentHash": "1739393595025",
      "hasWarnings": false,
      "documentTypeName": "BlogPost"
    },
    "blog/posts/on-demand-environments-defined-and-explained.mdx": {
      "document": {
        "title": "On-Demand Environments Defined and Explained",
        "summary": "We cover on-demand environments, the best way to implement them, and the advantages and disadvantages of on-demand envir",
        "publishDate": "Mon Jan 23 2023 14:09:53 GMT+0000 (Coordinated Universal Time)",
        "author": "",
        "readingTime": 6,
        "categories": [
          "platform-engineering",
          "product"
        ],
        "mainImage": "/blog-images/d7cc904aebc0c67b9e417b029995d57f.jpeg",
        "imageAlt": "On-Demand Environments Defined and Explained",
        "showCTA": true,
        "ctaCopy": "Improve feature testing with Release's on-demand environments for seamless collaboration and independent staging. Try it now!",
        "ctaLink": "https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=on-demand-environments-defined-and-explained",
        "relatedPosts": [
          ""
        ],
        "ogImage": "/blog-images/d7cc904aebc0c67b9e417b029995d57f.jpeg",
        "excerpt": "We cover on-demand environments, the best way to implement them, and the advantages and disadvantages of on-demand envir",
        "tags": [
          "platform-engineering",
          "product"
        ],
        "ctaButton": "Try Release for Free",
        "body": {
          "raw": "\nThere is no better feeling than having a seamless development process. However, teams that use a shared staging environment to test their features often complain about feature conflicts and code-breaking. In this post, you’ll learn about on-demand environments and how they can help improve your development process.\n\n![](/blog-images/fefc582df66e8014144f0be8412dbff2.png)\n\n## **What is an On-Demand Environment?** \n\nAn on-demand environment is a temporary, isolated environment that developers set up to test features, build features, or fix bugs without disrupting other development processes. On-demand environments create a full-fledged replica of your project, which includes all microservices and dependencies that your app needs to function. You can create as many on-demand environments as you want and they can run simultaneously. In other words, every developer on your team working on specific features can have their own little playground to make and break their code without hindering other developers' work. And if they mess the replica up completely, they can throw it away and create another one.\n\nUsing an on-demand environment automatically gives you an independent [staging environment](https://releasehub.com/staging-environments) for every single feature. This is great because your developers have a place they can verify that their work meets expectations before merging it into production. In contrast, using a shared staging environment to test features might lead to a bottleneck. The on-demand environment approach is a much better way to go about feature testing in your software development process.\n\n![On-demand environment](/blog-images/e4805408c84441f7f99b3ec3b63b10f1.jpeg)\n\n## **Implementing On-Demand Environments**\n\nTypically, we create this environment manually, which includes configuring all your microservices (for example, dependencies, libraries, databases, and third-party software) using a tool like [Terraform](https://www.terraform.io/), but this approach requires a fair amount of precise reconfiguration of your project setup.\n\n- **Each Git branch should be prefixed** with the name of the task or feature associated with it. This makes it clear and easy to identify what each feature branch represents. For instance, for **fea-productName-branch-URL, Fea** \\= feature name, **productName** = product name, and **branch-URL** = Git branch URL.\n- **Access and parameterize** all your configurations that might differ between developers. For instance, you can package your configurations in a dataset and store them on the [AWS Systems Manager,](https://aws.amazon.com/systems-manager/) then use Terraform to communicate with them whenever you need them to spin up an environment.\n- **Define rules and restrictions** for your environments. You can use Terraform with [Azure Policy](https://learn.microsoft.com/en-us/azure/governance/policy/overview) to create policies for what sort of action can be carried out in each environment. For example, for security, you can restrict your developers from communicating with specific databases or even restrict them from using some services in specific locations.\n- **Consider cost.** You may want to create a configuration that automatically deletes an environment after merging and deleting pull requests. Also, you can configure it to stop every instance of services running on environments that do not require them.\n\n## **Buy an Environment Instead**\n\nThese days, many organizations use remote automated tools for creating on-demand environments. This lets them enjoy all the privileges of a full-fledged environment without bothering themselves or their DevOps team with the task of creating one manually. These automated tools allow you and your team to concentrate on more important tasks, like developing an application with all the features it needs, but without headaches.\n\nAutomation tools can create an on-demand environment out of the box for you, which would include the full-fledged application, its frontend and backend, and every service or dependency it may require to fully function. These tools make creating an environment on demand quite easy and accessible. It takes care of most of the heavy lifting for you, like managing configurations and dependencies and destroying the environment when necessary. In contrast, if you use the manual process of creating these environments, you have to handle all these manipulations by yourself.\n\n## **Advantages of On-Demand Environments**\n\n### **Faster App Release Time**\n\nThe traditional way to test every feature in development involved a shared staging environment. This is not idle, because a staging environment merges all the features in the app for testing and that can result in feature conflict. This delays production because a fix needs to happen before you carry on. On-demand environments come in handy because your team gets an isolated environment where they can work on every feature in your app independently, without worrying about it affecting other development work. This ultimately lets you avoid bottlenecks in your development process, giving your app a faster time to market. And this gives you an edge over competitors that don’t use on-demand environments.\n\n### **Dedicated Testing for Each Feature**\n\nA good software development company seizes any chance they get to test that their product meets exceptions. With an on-demand environment, you get another chance to test your product and you get to test every feature independently. This gives you and your team more time to figure out what works and what doesn’t.\n\n### **Product Quality**\n\nImagine a team using a shared staging environment for testing features in their app. Because of all the problems they might face with testing in a shared staging environment, they might run out of patience and release products that, at the end of the day, don’t really impress. But with on-demand environments, every developer working on a feature gets an isolated environment to build and test their feature before shipping to a staging or production environment. This can immensely improve the quality of your product. Note that you don't have to have a staging environment when you use on-demand environments, but a staging environment lets you test all the features that you worked on individually in the on-demand environment as a unit.\n\n### **Time Saving**\n\nIt takes a lot of time to resolve feature conflicts in a staging environment because you have to figure out which code change or feature broke the code. This wastes time unnecessarily. Time is precious and you need to complete other tasks. On-demand environments let other work continue, so you don’t waste time while you fix broken code. With an isolated development area, the development of your main app continues, unaffected.\n\n### **Happy Team**\n\nAvoiding work stoppages because of feature conflicts makes teams happy and we can all agree that when your team is happy, you're bound to have quality work.\n\n![](/blog-images/087fdf64fb6fd421d4a70b7883ab56b8.png)\n\n## **Disadvantages of On-Demand Environments**\n\nOne of the major concerns with on-demand environments is cost. Because all your micro-services actively work in more than one instance of your application, bills start to pile up. You can mitigate this by using an automation tool that manages the lifespan of all your environments, from when you activate them to when you disable them. These tools can automatically destroy any environment you no longer need, saving you money. You can also define policies for the maximum number of environments that you or your team can create.\n\n## **Conclusion** \n\nWe covered on-demand environments, the best way to implement them, and the advantages and disadvantages of on-demand environments. Now the only thing left is putting the icing on the cake.\n\nIf you read this post in full, you now know some of the tremendous benefits on-demand environments can bring to your development process. [Release](https://releasehub.com/ebook/the-complete-guide-to-automated-software-environments) is one tool you can use to spin up environments on-demand without dealing with lengthy set ups or convoluted processes. It doesn't matter if you have a complex app with [Docker Compose](https://docs.docker.com/compose/) or you just want to deploy a simple [static app](https://docs.releasehub.com/reference-documentation/static-service-deployment) with package.json, Release can generate a template using these files to build your environment on-demand. If any of this intrigues you and you are yearning for an effortless means of creating environments for your projects, don't hesitate to learn about the magic of [Release](https://releasehub.com/ebook/the-complete-guide-to-automated-software-environments).\n",
          "code": "var Component=(()=>{var h=Object.create;var i=Object.defineProperty;var m=Object.getOwnPropertyDescriptor;var u=Object.getOwnPropertyNames;var p=Object.getPrototypeOf,g=Object.prototype.hasOwnProperty;var f=(t,e)=>()=>(e||t((e={exports:{}}).exports,e),e.exports),y=(t,e)=>{for(var a in e)i(t,a,{get:e[a],enumerable:!0})},s=(t,e,a,r)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let o of u(e))!g.call(t,o)&&o!==a&&i(t,o,{get:()=>e[o],enumerable:!(r=m(e,o))||r.enumerable});return t};var v=(t,e,a)=>(a=t!=null?h(p(t)):{},s(e||!t||!t.__esModule?i(a,\"default\",{value:t,enumerable:!0}):a,t)),b=t=>s(i({},\"__esModule\",{value:!0}),t);var d=f((j,c)=>{c.exports=_jsx_runtime});var N={};y(N,{default:()=>T,frontmatter:()=>w});var n=v(d()),w={title:\"On-Demand Environments Defined and Explained\",summary:\"We cover on-demand environments, the best way to implement them, and the advantages and disadvantages of on-demand envir\",publishDate:\"Mon Jan 23 2023 14:09:53 GMT+0000 (Coordinated Universal Time)\",author:\"\",readingTime:6,categories:[\"platform-engineering\",\"product\"],mainImage:\"/blog-images/d7cc904aebc0c67b9e417b029995d57f.jpeg\",imageAlt:\"On-Demand Environments Defined and Explained\",showCTA:!0,ctaCopy:\"Improve feature testing with Release's on-demand environments for seamless collaboration and independent staging. Try it now!\",ctaLink:\"https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=on-demand-environments-defined-and-explained\",relatedPosts:[\"\"],ogImage:\"/blog-images/d7cc904aebc0c67b9e417b029995d57f.jpeg\",excerpt:\"We cover on-demand environments, the best way to implement them, and the advantages and disadvantages of on-demand envir\",tags:[\"platform-engineering\",\"product\"],ctaButton:\"Try Release for Free\"};function l(t){let e=Object.assign({p:\"p\",img:\"img\",h2:\"h2\",a:\"a\",span:\"span\",strong:\"strong\",ul:\"ul\",li:\"li\",h3:\"h3\"},t.components);return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(e.p,{children:\"There is no better feeling than having a seamless development process. However, teams that use a shared staging environment to test their features often complain about feature conflicts and code-breaking. In this post, you\\u2019ll learn about on-demand environments and how they can help improve your development process.\"}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/fefc582df66e8014144f0be8412dbff2.png\",alt:\"\"})}),`\n`,(0,n.jsxs)(e.h2,{id:\"what-is-an-on-demand-environment\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#what-is-an-on-demand-environment\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.strong,{children:\"What is an On-Demand Environment?\"}),\"\\xA0\"]}),`\n`,(0,n.jsx)(e.p,{children:\"An on-demand environment is a temporary, isolated environment that developers set up to test features, build features, or fix bugs without disrupting other development processes. On-demand environments create a full-fledged replica of your project, which includes all microservices and dependencies that your app needs to function. You can create as many on-demand environments as you want and they can run simultaneously. In other words, every developer on your team working on specific features can have their own little playground to make and break their code without hindering other developers' work. And if they mess the replica up completely, they can throw it away and create another one.\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"Using an on-demand environment automatically gives you an independent \",(0,n.jsx)(e.a,{href:\"https://releasehub.com/staging-environments\",children:\"staging environment\"}),\" for every single feature. This is great because your developers have a place they can verify that their work meets expectations before merging it into production. In contrast, using a shared staging environment to test features might lead to a bottleneck. The on-demand environment approach is a much better way to go about feature testing in your software development process.\"]}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/e4805408c84441f7f99b3ec3b63b10f1.jpeg\",alt:\"On-demand environment\"})}),`\n`,(0,n.jsxs)(e.h2,{id:\"implementing-on-demand-environments\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#implementing-on-demand-environments\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.strong,{children:\"Implementing On-Demand Environments\"})]}),`\n`,(0,n.jsxs)(e.p,{children:[\"Typically, we create this environment manually, which includes configuring all your microservices (for example, dependencies, libraries, databases, and third-party software) using a tool like \",(0,n.jsx)(e.a,{href:\"https://www.terraform.io/\",children:\"Terraform\"}),\", but this approach requires a fair amount of precise reconfiguration of your project setup.\"]}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"Each Git branch should be prefixed\"}),\" with the name of the task or feature associated with it. This makes it clear and easy to identify what each feature branch represents. For instance, for \",(0,n.jsx)(e.strong,{children:\"fea-productName-branch-URL, Fea\"}),\" = feature name, \",(0,n.jsx)(e.strong,{children:\"productName\"}),\" = product name, and \",(0,n.jsx)(e.strong,{children:\"branch-URL\"}),\" = Git branch URL.\"]}),`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"Access and parameterize\"}),\" all your configurations that might differ between developers. For instance, you can package your configurations in a dataset and store them on the \",(0,n.jsx)(e.a,{href:\"https://aws.amazon.com/systems-manager/\",children:\"AWS Systems Manager,\"}),\" then use Terraform to communicate with them whenever you need them to spin up an environment.\"]}),`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"Define rules and restrictions\"}),\" for your environments. You can use Terraform with \",(0,n.jsx)(e.a,{href:\"https://learn.microsoft.com/en-us/azure/governance/policy/overview\",children:\"Azure Policy\"}),\" to create policies for what sort of action can be carried out in each environment. For example, for security, you can restrict your developers from communicating with specific databases or even restrict them from using some services in specific locations.\"]}),`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"Consider cost.\"}),\" You may want to create a configuration that automatically deletes an environment after merging and deleting pull requests. Also, you can configure it to stop every instance of services running on environments that do not require them.\"]}),`\n`]}),`\n`,(0,n.jsxs)(e.h2,{id:\"buy-an-environment-instead\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#buy-an-environment-instead\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.strong,{children:\"Buy an Environment Instead\"})]}),`\n`,(0,n.jsx)(e.p,{children:\"These days, many organizations use remote automated tools for creating on-demand environments. This lets them enjoy all the privileges of a full-fledged environment without bothering themselves or their DevOps team with the task of creating one manually. These automated tools allow you and your team to concentrate on more important tasks, like developing an application with all the features it needs, but without headaches.\"}),`\n`,(0,n.jsx)(e.p,{children:\"Automation tools can create an on-demand environment out of the box for you, which would include the full-fledged application, its frontend and backend, and every service or dependency it may require to fully function. These tools make creating an environment on demand quite easy and accessible. It takes care of most of the heavy lifting for you, like managing configurations and dependencies and destroying the environment when necessary. In contrast, if you use the manual process of creating these environments, you have to handle all these manipulations by yourself.\"}),`\n`,(0,n.jsxs)(e.h2,{id:\"advantages-of-on-demand-environments\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#advantages-of-on-demand-environments\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.strong,{children:\"Advantages of On-Demand Environments\"})]}),`\n`,(0,n.jsxs)(e.h3,{id:\"faster-app-release-time\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#faster-app-release-time\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.strong,{children:\"Faster App Release Time\"})]}),`\n`,(0,n.jsx)(e.p,{children:\"The traditional way to test every feature in development involved a shared staging environment. This is not idle, because a staging environment merges all the features in the app for testing and that can result in feature conflict. This delays production because a fix needs to happen before you carry on. On-demand environments come in handy because your team gets an isolated environment where they can work on every feature in your app independently, without worrying about it affecting other development work. This ultimately lets you avoid bottlenecks in your development process, giving your app a faster time to market. And this gives you an edge over competitors that don\\u2019t use on-demand environments.\"}),`\n`,(0,n.jsxs)(e.h3,{id:\"dedicated-testing-for-each-feature\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#dedicated-testing-for-each-feature\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.strong,{children:\"Dedicated Testing for Each Feature\"})]}),`\n`,(0,n.jsx)(e.p,{children:\"A good software development company seizes any chance they get to test that their product meets exceptions. With an on-demand environment, you get another chance to test your product and you get to test every feature independently. This gives you and your team more time to figure out what works and what doesn\\u2019t.\"}),`\n`,(0,n.jsxs)(e.h3,{id:\"product-quality\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#product-quality\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.strong,{children:\"Product Quality\"})]}),`\n`,(0,n.jsx)(e.p,{children:\"Imagine a team using a shared staging environment for testing features in their app. Because of all the problems they might face with testing in a shared staging environment, they might run out of patience and release products that, at the end of the day, don\\u2019t really impress. But with on-demand environments, every developer working on a feature gets an isolated environment to build and test their feature before shipping to a staging or production environment. This can immensely improve the quality of your product. Note that you don't have to have a staging environment when you use on-demand environments, but a staging environment lets you test all the features that you worked on individually in the on-demand environment as a unit.\"}),`\n`,(0,n.jsxs)(e.h3,{id:\"time-saving\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#time-saving\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.strong,{children:\"Time Saving\"})]}),`\n`,(0,n.jsx)(e.p,{children:\"It takes a lot of time to resolve feature conflicts in a staging environment because you have to figure out which code change or feature broke the code. This wastes time unnecessarily. Time is precious and you need to complete other tasks. On-demand environments let other work continue, so you don\\u2019t waste time while you fix broken code. With an isolated development area, the development of your main app continues, unaffected.\"}),`\n`,(0,n.jsxs)(e.h3,{id:\"happy-team\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#happy-team\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.strong,{children:\"Happy Team\"})]}),`\n`,(0,n.jsx)(e.p,{children:\"Avoiding work stoppages because of feature conflicts makes teams happy and we can all agree that when your team is happy, you're bound to have quality work.\"}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/087fdf64fb6fd421d4a70b7883ab56b8.png\",alt:\"\"})}),`\n`,(0,n.jsxs)(e.h2,{id:\"disadvantages-of-on-demand-environments\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#disadvantages-of-on-demand-environments\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.strong,{children:\"Disadvantages of On-Demand Environments\"})]}),`\n`,(0,n.jsx)(e.p,{children:\"One of the major concerns with on-demand environments is cost. Because all your micro-services actively work in more than one instance of your application, bills start to pile up. You can mitigate this by using an automation tool that manages the lifespan of all your environments, from when you activate them to when you disable them. These tools can automatically destroy any environment you no longer need, saving you money. You can also define policies for the maximum number of environments that you or your team can create.\"}),`\n`,(0,n.jsxs)(e.h2,{id:\"conclusion\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#conclusion\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.strong,{children:\"Conclusion\"}),\"\\xA0\"]}),`\n`,(0,n.jsx)(e.p,{children:\"We covered on-demand environments, the best way to implement them, and the advantages and disadvantages of on-demand environments. Now the only thing left is putting the icing on the cake.\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"If you read this post in full, you now know some of the tremendous benefits on-demand environments can bring to your development process. \",(0,n.jsx)(e.a,{href:\"https://releasehub.com/ebook/the-complete-guide-to-automated-software-environments\",children:\"Release\"}),\" is one tool you can use to spin up environments on-demand without dealing with lengthy set ups or convoluted processes. It doesn't matter if you have a complex app with \",(0,n.jsx)(e.a,{href:\"https://docs.docker.com/compose/\",children:\"Docker Compose\"}),\" or you just want to deploy a simple \",(0,n.jsx)(e.a,{href:\"https://docs.releasehub.com/reference-documentation/static-service-deployment\",children:\"static app\"}),\" with package.json, Release can generate a template using these files to build your environment on-demand. If any of this intrigues you and you are yearning for an effortless means of creating environments for your projects, don't hesitate to learn about the magic of \",(0,n.jsx)(e.a,{href:\"https://releasehub.com/ebook/the-complete-guide-to-automated-software-environments\",children:\"Release\"}),\".\"]})]})}function k(t={}){let{wrapper:e}=t.components||{};return e?(0,n.jsx)(e,Object.assign({},t,{children:(0,n.jsx)(l,t)})):l(t)}var T=k;return b(N);})();\n;return Component;"
        },
        "_id": "blog/posts/on-demand-environments-defined-and-explained.mdx",
        "_raw": {
          "sourceFilePath": "blog/posts/on-demand-environments-defined-and-explained.mdx",
          "sourceFileName": "on-demand-environments-defined-and-explained.mdx",
          "sourceFileDir": "blog/posts",
          "contentType": "mdx",
          "flattenedPath": "blog/posts/on-demand-environments-defined-and-explained"
        },
        "type": "BlogPost",
        "computedSlug": "on-demand-environments-defined-and-explained"
      },
      "documentHash": "1739393595025",
      "hasWarnings": false,
      "documentTypeName": "BlogPost"
    },
    "blog/posts/on-demand-testing-spaces-turbocharge-developer-velocity.mdx": {
      "document": {
        "title": "On-Demand Testing Spaces Turbocharge Developer Velocity",
        "summary": "Embracing ephemerality in software development with on-demand ephemeral environments.",
        "publishDate": "Tue Jan 23 2024 22:01:14 GMT+0000 (Coordinated Universal Time)",
        "author": "tommy-mcclung",
        "readingTime": 4,
        "categories": [
          "platform-engineering",
          "kubernetes"
        ],
        "mainImage": "/blog-images/f36efe4b298888a84509e1392695839f.png",
        "imageAlt": "New Stack Release Article",
        "showCTA": true,
        "ctaCopy": "Release.com's ephemeral environments eliminate staging bottlenecks, enabling instant testing spaces for faster code deployment and seamless collaboration.",
        "ctaLink": "https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=on-demand-testing-spaces-turbocharge-developer-velocity",
        "relatedPosts": [
          ""
        ],
        "ogImage": "/blog-images/f36efe4b298888a84509e1392695839f.png",
        "excerpt": "Embracing ephemerality in software development with on-demand ephemeral environments.",
        "tags": [
          "platform-engineering",
          "kubernetes"
        ],
        "ctaButton": "Try Release for Free",
        "body": {
          "raw": "\nWe live in an ephemeral world. From social media stories that vanish within 24 hours, to app messages that self-destruct after viewing, to our speed-of-light news cycle where today’s headlines are forgotten tomorrow. For better or for worse, we as a culture are just not that into permanence.\n\nOne area where ephemerality has been making positive inroads is in the world of software development, where it’s solving a particularly intractable problem. I’m referring to a relatively new innovation known as ephemeral environments — virtual spaces that can be created and destroyed on demand to streamline software development and remove the staging or QA environment bottleneck.\n\n## **The Bottleneck Problem**\n\nDelivering software is hard. With applications [growing in complexity](https://thenewstack.io/the-growing-complexity-of-kubernetes-and-whats-being-done-to-fix-it/), getting ideas from one’s fingertips to users is fraught with challenges.\n\nConsider the gauntlet that a developer’s code must run before it can be released to the world. It must pass through development, staging or QA, and production environments, each of which is critical.\n\nStaging is the environment where code is “staged” — that is, tested and merged — before being run in front of users so dev teams can be sure it works as designed. The staging environment should mirror production as closely as possible, but without exposing the code to clients, customers and the public.\n\nAt many tech organizations, the staging environment is the source of the biggest and most costly bottleneck in software development.\n\nThere are several reasons for this, including:\n\n- **Complexity:** These preproduction environments are actually highly complex platforms comprising dozens or even hundreds of services, cloud platforms and supporting technologies. Standing up a new environment is a highly specialized task that requires the [skills of DevOps](https://thenewstack.io/how-to-close-the-devops-skills-gap/) engineers.\n- **Cost:** In addition to the cost of engaging DevOps talent, for which there is great demand in the industry, staging environments can be expensive to maintain, and they often break. Any shared resource can be a pain point for companies large and small that are trying to rapidly ship new features.\n- **Scarcity:** Most organizations have one or very few of these environments, and only one engineer at a time can deploy to an environment. There are [over 20 million developers](https://en.wikipedia.org/wiki/Software_engineering_demographics) in the world, and pretty much all of them use these environments.\n\nThe result is that developer teams often waste precious time twiddling their thumbs as they wait for testing environments to become available, or argue over priorities and who can use the environment. Teams can sometimes wait days or weeks to [deploy their code](https://thenewstack.io/gitops-at-home-automate-code-deploys-with-kubernetes-and-flux/) due to constraints on single environments. This combination of long queues, idle engineers and delayed releases can cost organizations tens of millions of dollars per year. We estimate managing and maintaining environments costs organizations more than $45 billion a year.\n\n## **On-Demand Ephemeral Staging Environments**\n\nWith ephemeral environments, the traditional idea of “staging” is gone — and with it the need for a single testing and integration environment where all code must merge before going to production. Staging happens on demand and at a click, allowing developers to create a limitless supply of environments for any purpose.\n\nEphemeral environments offer a production-like replica that allows developers to properly test their code (shift left) and isolate bugs to a single branch, while ensuring a smooth merge to staging and production. They can be created automatically with every code pull request, and URLs to the environment can be shared with stakeholders so they can see progress while code is being developed.\n\n## **3 Ways to Get the Most from Ephemeral Environments**\n\nJust as physical pop-up stores offer a range of benefits to retailers, restaurateurs and others — cost-effectiveness and easy experimentation, for example — developer pop-ups offer several advantages over traditional staging. Here are three.\n\n- **Early and frequent feedback:** With developer pop-ups, the environment is updated each time a developer pushes code to their source control system, providing a live reflection of the feature during development. Stakeholders such as product managers, designers and QA are automatically notified when changes are live, and they can preview those changes and give feedback immediately, enabling better code quality in development and minimizing rework.\n- **Get feedback on your branch:** Ephemeral environments can be created for any code branch, on demand, and loaded with your data. Developers can spin up as many isolated environments as they need, get feedback on a specific branch and work with the certainty that, if there is an issue, it’s not in their environment.\n- **Greater developer velocity:** In making bottlenecks a thing of the past, ephemeral environments allow for more frequent release cycles, better customer experiences and — the holy grail of software development — better developer velocity.\n\n[According to McKinsey](https://www.mckinsey.com/industries/technology-media-and-telecommunications/our-insights/developer-velocity-how-software-excellence-fuels-business-performance), developer velocity is a key predictor of performance. Companies in the top 25% of McKinsey’s Developer Velocity Index outperform others in the market by four to five times, are more innovative, and score higher on customer satisfaction and other measures.\n\nEphemeral environments turbocharge development velocity by eliminating bottlenecks in the process and helping companies produce consistent, reliable and plentiful environments on demand so they can get their best ideas to the world quickly.\n",
          "code": "var Component=(()=>{var d=Object.create;var r=Object.defineProperty;var m=Object.getOwnPropertyDescriptor;var p=Object.getOwnPropertyNames;var g=Object.getPrototypeOf,u=Object.prototype.hasOwnProperty;var f=(t,e)=>()=>(e||t((e={exports:{}}).exports,e),e.exports),v=(t,e)=>{for(var o in e)r(t,o,{get:e[o],enumerable:!0})},s=(t,e,o,a)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let i of p(e))!u.call(t,i)&&i!==o&&r(t,i,{get:()=>e[i],enumerable:!(a=m(e,i))||a.enumerable});return t};var y=(t,e,o)=>(o=t!=null?d(g(t)):{},s(e||!t||!t.__esModule?r(o,\"default\",{value:t,enumerable:!0}):o,t)),b=t=>s(r({},\"__esModule\",{value:!0}),t);var c=f((D,l)=>{l.exports=_jsx_runtime});var T={};v(T,{default:()=>x,frontmatter:()=>w});var n=y(c()),w={title:\"On-Demand Testing Spaces Turbocharge Developer Velocity\",summary:\"Embracing ephemerality in software development with on-demand ephemeral environments.\",publishDate:\"Tue Jan 23 2024 22:01:14 GMT+0000 (Coordinated Universal Time)\",author:\"tommy-mcclung\",readingTime:4,categories:[\"platform-engineering\",\"kubernetes\"],mainImage:\"/blog-images/f36efe4b298888a84509e1392695839f.png\",imageAlt:\"New Stack Release Article\",showCTA:!0,ctaCopy:\"Release.com's ephemeral environments eliminate staging bottlenecks, enabling instant testing spaces for faster code deployment and seamless collaboration.\",ctaLink:\"https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=on-demand-testing-spaces-turbocharge-developer-velocity\",relatedPosts:[\"\"],ogImage:\"/blog-images/f36efe4b298888a84509e1392695839f.png\",excerpt:\"Embracing ephemerality in software development with on-demand ephemeral environments.\",tags:[\"platform-engineering\",\"kubernetes\"],ctaButton:\"Try Release for Free\"};function h(t){let e=Object.assign({p:\"p\",h2:\"h2\",a:\"a\",span:\"span\",strong:\"strong\",ul:\"ul\",li:\"li\"},t.components);return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(e.p,{children:\"We live in an ephemeral world. From social media stories that vanish within 24 hours, to app messages that self-destruct after viewing, to our speed-of-light news cycle where today\\u2019s headlines are forgotten tomorrow. For better or for worse, we as a culture are just not that into permanence.\"}),`\n`,(0,n.jsx)(e.p,{children:\"One area where ephemerality has been making positive inroads is in the world of software development, where it\\u2019s solving a particularly intractable problem. I\\u2019m referring to a relatively new innovation known as ephemeral environments \\u2014 virtual spaces that can be created and destroyed on demand to streamline software development and remove the staging or QA environment bottleneck.\"}),`\n`,(0,n.jsxs)(e.h2,{id:\"the-bottleneck-problem\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#the-bottleneck-problem\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.strong,{children:\"The Bottleneck Problem\"})]}),`\n`,(0,n.jsxs)(e.p,{children:[\"Delivering software is hard. With applications \",(0,n.jsx)(e.a,{href:\"https://thenewstack.io/the-growing-complexity-of-kubernetes-and-whats-being-done-to-fix-it/\",children:\"growing in complexity\"}),\", getting ideas from one\\u2019s fingertips to users is fraught with challenges.\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Consider the gauntlet that a developer\\u2019s code must run before it can be released to the world. It must pass through development, staging or QA, and production environments, each of which is critical.\"}),`\n`,(0,n.jsx)(e.p,{children:\"Staging is the environment where code is \\u201Cstaged\\u201D \\u2014 that is, tested and merged \\u2014 before being run in front of users so dev teams can be sure it works as designed. The staging environment should mirror production as closely as possible, but without exposing the code to clients, customers and the public.\"}),`\n`,(0,n.jsx)(e.p,{children:\"At many tech organizations, the staging environment is the source of the biggest and most costly bottleneck in software development.\"}),`\n`,(0,n.jsx)(e.p,{children:\"There are several reasons for this, including:\"}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"Complexity:\"}),\" These preproduction environments are actually highly complex platforms comprising dozens or even hundreds of services, cloud platforms and supporting technologies. Standing up a new environment is a highly specialized task that requires the \",(0,n.jsx)(e.a,{href:\"https://thenewstack.io/how-to-close-the-devops-skills-gap/\",children:\"skills of DevOps\"}),\" engineers.\"]}),`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"Cost:\"}),\" In addition to the cost of engaging DevOps talent, for which there is great demand in the industry, staging environments can be expensive to maintain, and they often break. Any shared resource can be a pain point for companies large and small that are trying to rapidly ship new features.\"]}),`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"Scarcity:\"}),\" Most organizations have one or very few of these environments, and only one engineer at a time can deploy to an environment. There are \",(0,n.jsx)(e.a,{href:\"https://en.wikipedia.org/wiki/Software_engineering_demographics\",children:\"over 20 million developers\"}),\" in the world, and pretty much all of them use these environments.\"]}),`\n`]}),`\n`,(0,n.jsxs)(e.p,{children:[\"The result is that developer teams often waste precious time twiddling their thumbs as they wait for testing environments to become available, or argue over priorities and who can use the environment. Teams can sometimes wait days or weeks to \",(0,n.jsx)(e.a,{href:\"https://thenewstack.io/gitops-at-home-automate-code-deploys-with-kubernetes-and-flux/\",children:\"deploy their code\"}),\" due to constraints on single environments. This combination of long queues, idle engineers and delayed releases can cost organizations tens of millions of dollars per year. We estimate managing and maintaining environments costs organizations more than $45 billion a year.\"]}),`\n`,(0,n.jsxs)(e.h2,{id:\"on-demand-ephemeral-staging-environments\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#on-demand-ephemeral-staging-environments\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.strong,{children:\"On-Demand Ephemeral Staging Environments\"})]}),`\n`,(0,n.jsx)(e.p,{children:\"With ephemeral environments, the traditional idea of \\u201Cstaging\\u201D is gone \\u2014 and with it the need for a single testing and integration environment where all code must merge before going to production. Staging happens on demand and at a click, allowing developers to create a limitless supply of environments for any purpose.\"}),`\n`,(0,n.jsx)(e.p,{children:\"Ephemeral environments offer a production-like replica that allows developers to properly test their code (shift left) and isolate bugs to a single branch, while ensuring a smooth merge to staging and production. They can be created automatically with every code pull request, and URLs to the environment can be shared with stakeholders so they can see progress while code is being developed.\"}),`\n`,(0,n.jsxs)(e.h2,{id:\"3-ways-to-get-the-most-from-ephemeral-environments\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#3-ways-to-get-the-most-from-ephemeral-environments\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.strong,{children:\"3 Ways to Get the Most from Ephemeral Environments\"})]}),`\n`,(0,n.jsx)(e.p,{children:\"Just as physical pop-up stores offer a range of benefits to retailers, restaurateurs and others \\u2014 cost-effectiveness and easy experimentation, for example \\u2014 developer pop-ups offer several advantages over traditional staging. Here are three.\"}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"Early and frequent feedback:\"}),\" With developer pop-ups, the environment is updated each time a developer pushes code to their source control system, providing a live reflection of the feature during development. Stakeholders such as product managers, designers and QA are automatically notified when changes are live, and they can preview those changes and give feedback immediately, enabling better code quality in development and minimizing rework.\"]}),`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"Get feedback on your branch:\"}),\" Ephemeral environments can be created for any code branch, on demand, and loaded with your data. Developers can spin up as many isolated environments as they need, get feedback on a specific branch and work with the certainty that, if there is an issue, it\\u2019s not in their environment.\"]}),`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"Greater developer velocity:\"}),\" In making bottlenecks a thing of the past, ephemeral environments allow for more frequent release cycles, better customer experiences and \\u2014 the holy grail of software development \\u2014 better developer velocity.\"]}),`\n`]}),`\n`,(0,n.jsxs)(e.p,{children:[(0,n.jsx)(e.a,{href:\"https://www.mckinsey.com/industries/technology-media-and-telecommunications/our-insights/developer-velocity-how-software-excellence-fuels-business-performance\",children:\"According to McKinsey\"}),\", developer velocity is a key predictor of performance. Companies in the top 25% of McKinsey\\u2019s Developer Velocity Index outperform others in the market by four to five times, are more innovative, and score higher on customer satisfaction and other measures.\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Ephemeral environments turbocharge development velocity by eliminating bottlenecks in the process and helping companies produce consistent, reliable and plentiful environments on demand so they can get their best ideas to the world quickly.\"})]})}function k(t={}){let{wrapper:e}=t.components||{};return e?(0,n.jsx)(e,Object.assign({},t,{children:(0,n.jsx)(h,t)})):h(t)}var x=k;return b(T);})();\n;return Component;"
        },
        "_id": "blog/posts/on-demand-testing-spaces-turbocharge-developer-velocity.mdx",
        "_raw": {
          "sourceFilePath": "blog/posts/on-demand-testing-spaces-turbocharge-developer-velocity.mdx",
          "sourceFileName": "on-demand-testing-spaces-turbocharge-developer-velocity.mdx",
          "sourceFileDir": "blog/posts",
          "contentType": "mdx",
          "flattenedPath": "blog/posts/on-demand-testing-spaces-turbocharge-developer-velocity"
        },
        "type": "BlogPost",
        "computedSlug": "on-demand-testing-spaces-turbocharge-developer-velocity"
      },
      "documentHash": "1739393595025",
      "hasWarnings": false,
      "documentTypeName": "BlogPost"
    },
    "blog/posts/overcome-devops-talent-gap-with-environments-as-a-service.mdx": {
      "document": {
        "title": "Overcome the DevOps Talent Gap with Environments as a Service",
        "summary": "Is your company affected by the DevOps talent scarcity and how do you catch up to your competitors?",
        "publishDate": "Tue Mar 30 2021 23:52:54 GMT+0000 (Coordinated Universal Time)",
        "author": "amanda",
        "readingTime": 4,
        "categories": [
          "platform-engineering"
        ],
        "mainImage": "/blog-images/d9b7466986ff4a93146f67862030a972.jpg",
        "imageAlt": "Illustration showing the developer environment created by Release",
        "showCTA": true,
        "ctaCopy": "Enhance DevOps efficiency with Release's on-demand environments for seamless collaboration and consistent deployments.",
        "ctaLink": "https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=overcome-devops-talent-gap-with-environments-as-a-service",
        "relatedPosts": [
          ""
        ],
        "ogImage": "/blog-images/d9b7466986ff4a93146f67862030a972.jpg",
        "excerpt": "Is your company affected by the DevOps talent scarcity and how do you catch up to your competitors?",
        "tags": [
          "platform-engineering"
        ],
        "ctaButton": "Try Release for Free",
        "body": {
          "raw": "\n### Overcome the DevOps Talent Gap with Environments as a Service\n\nDevOps is becoming the most critical function in software development but there is a shortage of talent to support needs across technology driven organizations. Applications have evolved and the shift in architecture is now to be more closely intertwined with development than ever. \n\nDevelopers have new requirements that are invented by the evolution toward containerized applications and microservice-based architectures. Instead of just focusing on their code, they now need to be more mindful of how their code relates to the infrastructure that supports their applications. \n\nThe interconnected web of dependencies that live under the hood are difficult to unravel as new technologies, processes, and systems have been introduced and then retired over the years. When large companies have adopted a plan to move toward modern application architectures, they are subject to the constraints of their existing ecosystems. Startups are unique and often can start developing their applications’ architecture from a blank slate.\n\n### The Problem\n\nThe irony of containerized applications and microservice architectures is that the same problems arise around DevOps for both startups and large enterprises. The problems exist around dependency management, server maintenance, and support for consistency across environments. \n\nChanges in computing, technology, and processes are going to continuously evolve and adapt at a speed that is faster than most teams’ ability to keep up. We can only do our best to increase time to value and accelerate productivity to make sure we remain inside the window of survival.\n\nA lack of sufficient DevOps talent should not eliminate the rollout of a strategy to future proof your business. Complementing your existing DevOps team with technologies that add the services and automation in areas that they don’t have bandwidth to reach is how teams can accomplish more with less. \n\nTechnology companies need to support DevOps, automate their workflows, or stand in their place when there is a void of not having a team in place. The reality is that every company is a technology company in the 21st century, and everyone is going to deal with challenges around bandwidth issues in their DevOps organizations. If the talent pool is so small in the industry, there is a high chance that your pool of hires will also be too small for the needs of your business over time.\n\nDevOps hires are so important because not only are they limited but they are the people that determine what the organization should or should not do to streamline development. We are too dependent on people in a field that should allocate investment where it is measurable and within our control.\n\n### The Solution\n\nEnvironments are where the visions and strategies developed by DevOps come to life. \n\nPreview environments that have all of the tooling, technology relationships, data, APIs, and all other criteria vital to application performance result in faster development lifecycles. The value of environments that replicate production in pre-production phases of development is obvious but standing up those environments is a strain on time and resources. In many cases, creating these important environments is so time consuming and costly these important steps are skipped, much to the detriment of the entire business.\n\nRelease complements DevOps teams or can replace the DevOps overhead of smaller organizations through automated execution of workflows. We make it easy to run your engineering organization within an advanced DevOps framework without the additional headcount or reduction of resources. We accomplish this by offering a platform for Environments as a Service (EaaS) where we configure your applications to enable automated creation of environments with each code commit that live in isolation to provide visibility at every change. In this way, organizations can hire DevOps talent without additional headcount, and even gain productivity at mass quantities.\n\nOn-Demand environments take the workload of provisioning and maintaining servers off of your engineering team so that they can focus on code instead of spinning up servers. Release offers  a hands off process to run your microservices and apps on Kubernetes (k8s) to spin up environments within a matter of minutes. DevOps can prioritize more important tasks while engineers can bring software to production with velocity when reproducible environments are accessible to all.\n",
          "code": "var Component=(()=>{var d=Object.create;var i=Object.defineProperty;var p=Object.getOwnPropertyDescriptor;var m=Object.getOwnPropertyNames;var u=Object.getPrototypeOf,v=Object.prototype.hasOwnProperty;var f=(n,e)=>()=>(e||n((e={exports:{}}).exports,e),e.exports),g=(n,e)=>{for(var o in e)i(n,o,{get:e[o],enumerable:!0})},r=(n,e,o,s)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let a of m(e))!v.call(n,a)&&a!==o&&i(n,a,{get:()=>e[a],enumerable:!(s=p(e,a))||s.enumerable});return n};var y=(n,e,o)=>(o=n!=null?d(u(n)):{},r(e||!n||!n.__esModule?i(o,\"default\",{value:n,enumerable:!0}):o,n)),w=n=>r(i({},\"__esModule\",{value:!0}),n);var h=f((x,c)=>{c.exports=_jsx_runtime});var k={};g(k,{default:()=>O,frontmatter:()=>b});var t=y(h()),b={title:\"Overcome the DevOps Talent Gap with Environments as a Service\",summary:\"Is your company affected by the DevOps talent scarcity and how do you catch up to your competitors?\",publishDate:\"Tue Mar 30 2021 23:52:54 GMT+0000 (Coordinated Universal Time)\",author:\"amanda\",readingTime:4,categories:[\"platform-engineering\"],mainImage:\"/blog-images/d9b7466986ff4a93146f67862030a972.jpg\",imageAlt:\"Illustration showing the developer environment created by Release\",showCTA:!0,ctaCopy:\"Enhance DevOps efficiency with Release's on-demand environments for seamless collaboration and consistent deployments.\",ctaLink:\"https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=overcome-devops-talent-gap-with-environments-as-a-service\",relatedPosts:[\"\"],ogImage:\"/blog-images/d9b7466986ff4a93146f67862030a972.jpg\",excerpt:\"Is your company affected by the DevOps talent scarcity and how do you catch up to your competitors?\",tags:[\"platform-engineering\"],ctaButton:\"Try Release for Free\"};function l(n){let e=Object.assign({h3:\"h3\",a:\"a\",span:\"span\",p:\"p\"},n.components);return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsxs)(e.h3,{id:\"overcome-the-devops-talent-gap-with-environments-as-a-service\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#overcome-the-devops-talent-gap-with-environments-as-a-service\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Overcome the DevOps Talent Gap with Environments as a Service\"]}),`\n`,(0,t.jsx)(e.p,{children:\"DevOps is becoming the most critical function in software development but there is a shortage of talent to support needs across technology driven organizations. Applications have evolved and the shift in architecture is now to be more closely intertwined with development than ever.\\xA0\"}),`\n`,(0,t.jsx)(e.p,{children:\"Developers have new requirements that are invented by the evolution toward containerized applications and microservice-based architectures. Instead of just focusing on their code, they now need to be more mindful of how their code relates to the infrastructure that supports their applications.\\xA0\"}),`\n`,(0,t.jsx)(e.p,{children:\"The interconnected web of dependencies that live under the hood are difficult to unravel as new technologies, processes, and systems have been introduced and then retired over the years. When large companies have adopted a plan to move toward modern application architectures, they are subject to the constraints of their existing ecosystems. Startups are unique and often can start developing their applications\\u2019 architecture from a blank slate.\"}),`\n`,(0,t.jsxs)(e.h3,{id:\"the-problem\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#the-problem\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"The Problem\"]}),`\n`,(0,t.jsx)(e.p,{children:\"The irony of containerized applications and microservice architectures is that the same problems arise around DevOps for both startups and large enterprises. The problems exist around dependency management, server maintenance, and support for consistency across environments.\\xA0\"}),`\n`,(0,t.jsx)(e.p,{children:\"Changes in computing, technology, and processes are going to continuously evolve and adapt at a speed that is faster than most teams\\u2019 ability to keep up. We can only do our best to increase time to value and accelerate productivity to make sure we remain inside the window of survival.\"}),`\n`,(0,t.jsx)(e.p,{children:\"A lack of sufficient DevOps talent should not eliminate the rollout of a strategy to future proof your business. Complementing your existing DevOps team with technologies that add the services and automation in areas that they don\\u2019t have bandwidth to reach is how teams can accomplish more with less.\\xA0\"}),`\n`,(0,t.jsx)(e.p,{children:\"Technology companies need to support DevOps, automate their workflows, or stand in their place when there is a void of not having a team in place. The reality is that every company is a technology company in the 21st century, and everyone is going to deal with challenges around bandwidth issues in their DevOps organizations. If the talent pool is so small in the industry, there is a high chance that your pool of hires will also be too small for the needs of your business over time.\"}),`\n`,(0,t.jsx)(e.p,{children:\"DevOps hires are so important because not only are they limited but they are the people that determine what the organization should or should not do to streamline development. We are too dependent on people in a field that should allocate investment where it is measurable and within our control.\"}),`\n`,(0,t.jsxs)(e.h3,{id:\"the-solution\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#the-solution\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"The Solution\"]}),`\n`,(0,t.jsx)(e.p,{children:\"Environments are where the visions and strategies developed by DevOps come to life.\\xA0\"}),`\n`,(0,t.jsx)(e.p,{children:\"Preview environments that have all of the tooling, technology relationships, data, APIs, and all other criteria vital to application performance result in faster development lifecycles. The value of environments that replicate production in pre-production phases of development is obvious but standing up those environments is a strain on time and resources. In many cases, creating these important environments is so time consuming and costly these important steps are skipped, much to the detriment of the entire business.\"}),`\n`,(0,t.jsx)(e.p,{children:\"Release complements DevOps teams or can replace the DevOps overhead of smaller organizations through automated execution of workflows. We make it easy to run your engineering organization within an advanced DevOps framework without the additional headcount or reduction of resources. We accomplish this by offering a platform for Environments as a Service (EaaS) where we configure your applications to enable automated creation of environments with each code commit that live in isolation to provide visibility at every change. In this way, organizations can hire DevOps talent without additional headcount, and even gain productivity at mass quantities.\"}),`\n`,(0,t.jsx)(e.p,{children:\"On-Demand environments take the workload of provisioning and maintaining servers off of your engineering team so that they can focus on code instead of spinning up servers. Release offers\\xA0 a hands off process to run your microservices and apps on Kubernetes (k8s) to spin up environments within a matter of minutes. DevOps can prioritize more important tasks while engineers can bring software to production with velocity when reproducible environments are accessible to all.\"})]})}function D(n={}){let{wrapper:e}=n.components||{};return e?(0,t.jsx)(e,Object.assign({},n,{children:(0,t.jsx)(l,n)})):l(n)}var O=D;return w(k);})();\n;return Component;"
        },
        "_id": "blog/posts/overcome-devops-talent-gap-with-environments-as-a-service.mdx",
        "_raw": {
          "sourceFilePath": "blog/posts/overcome-devops-talent-gap-with-environments-as-a-service.mdx",
          "sourceFileName": "overcome-devops-talent-gap-with-environments-as-a-service.mdx",
          "sourceFileDir": "blog/posts",
          "contentType": "mdx",
          "flattenedPath": "blog/posts/overcome-devops-talent-gap-with-environments-as-a-service"
        },
        "type": "BlogPost",
        "computedSlug": "overcome-devops-talent-gap-with-environments-as-a-service"
      },
      "documentHash": "1739393595025",
      "hasWarnings": false,
      "documentTypeName": "BlogPost"
    },
    "blog/posts/platform-engineering-whats-hype-and-whats-not.mdx": {
      "document": {
        "title": "Platform Engineering: What’s Hype and What’s Not?",
        "summary": "Platform engineering is all the rage these days. Separate the facts from the hype with this roundup.",
        "publishDate": "Tue Jan 30 2024 21:35:44 GMT+0000 (Coordinated Universal Time)",
        "author": "tommy-mcclung",
        "readingTime": 5,
        "categories": [
          "platform-engineering",
          "product"
        ],
        "mainImage": "/blog-images/aa447085214bc561a527d812197f6e52.png",
        "imageAlt": "Platform Engineering: What’s Hype and What’s Not?",
        "showCTA": true,
        "ctaCopy": "Looking to streamline software delivery at scale like platform engineering? Try Release for on-demand environments mirroring production.",
        "ctaLink": "https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=platform-engineering-whats-hype-and-whats-not",
        "relatedPosts": [
          ""
        ],
        "ogImage": "/blog-images/aa447085214bc561a527d812197f6e52.png",
        "excerpt": "Platform engineering is all the rage these days. Separate the facts from the hype with this roundup.",
        "tags": [
          "platform-engineering",
          "product"
        ],
        "ctaButton": "Try Release for Free",
        "body": {
          "raw": "\nThere’s a sea change happening in the software development world. The merging of developers and operations that gave us DevOps may be entering a new chapter. In this new world, the emerging discipline of platform engineering is quickly gaining popularity.\n\nPlatform engineering is the practice of designing and building self-service capabilities to reduce cognitive load for developers and to facilitate fast-flow software delivery. Many large organizations have struggled to reap the benefits of DevOps, in part because shifting more operational and security concerns “left” and into the domain of software developers has created bottlenecks for dev teams. At the same time, faced with a growing cognitive burden of repetitive, time-consuming tasks that kick them out of the flow state of highly productive coding, many devs want less and less to do with ops and the “you build it, you run it” paradigm.\n\nPlatform engineering is emerging as the solution to many of these challenges. But in all the buzz around it, it’s hard to know what’s real and what’s not. To help you separate the facts from the hype, here’s a round-up of viewpoints on what platform engineering is and is not.\n\n### Platform Engineering Is New – Hype\n\nThere are those who hail platform engineering as [the new kid on the block](https://www.youtube.com/watch?v=wXyNHngEN-s). But there’s nothing new about the building of digital platforms as a means of delivering software at scale. It even pre-dates the birth of the DevOps movement in the mid-2000s. According to [Puppet’s _2023 State of DevOps Report_](https://support.puppet.com/hc/en-us/articles/221368047-The-2023-State-of-DevOps-Report-is-here-), large software companies have been taking a platform approach for decades as a way to enable developer teams to build, ship, and operate applications more quickly and at higher quality.\n\nWhat is new, especially in the enterprise space, is the rapidly growing traction of platform engineering as a way for larger companies to improve software delivery at scale. Gartner identified platform engineering (which it interchangeably calls “platform ops”) as one of the [Top Strategic Technology Trends of 2023](https://www.gartner.com/en/information-technology/insights/top-technology-trends). Gartner analysts predict that 80% of software engineering organizations will establish platform teams by 2026, and 75% of those will include developer self-service portals.\n\n### Platform Engineering Has Toppled DevOps – Hype\n\nThose who claim that DevOps is dead and that platform engineering has supplanted it are engaging in hyperbole. “DevOps is dead, long live Platform Engineering!” [tweeted](https://twitter.com/sidpalas/status/1551936840453820417) software engineer and DevOps commentator Sid Palas in 2022. “1. Developers don’t like dealing with infra, 2. Companies need control of their infra as they grow. Platform engineering enables these two facts to coexist.”\n\n### Platform Engineering Is the Next Evolution of DevOps and SRE – Fact\n\nRather than dealing a death blow to DevOps, a more accurate take is that platform engineering is the next evolution of DevOps and SRE (site reliability engineering). In particular, it benefits developers struggling with code production bottlenecks as they wait on internal approvals or fulfillment. It also helps devs deliver on their own timeline rather than that of their IT team. And it helps operator types (such as SREs or DevOps engineers) who are feeling the pain of repetitive request fulfillment and operational firefighting — busy work that keeps them from building their vision for the future.\n\n### Platform Engineering Should Embrace DevOps Culture – Fact\n\nThe agile development practices that are at the core of DevOps culture — such as collaboration, communication, and continuous improvement — have not extended to the operations domain. This has hobbled the ability of agile development teams to quickly deliver products. In order not to perpetuate this dynamic, DevOps team culture should evolve to support platform engineering, and platform teams should embrace DevOps team culture.\n\n### Platform Engineering Is a Con – Hype\n\nThere are those, like independent technology consultant Sam Newman, who argue [platform engineering is just another vendor-generated label to be slapped on to old practices](https://samnewman.io/blog/2023/02/08/dont-call-it-a-platform/) in a bid to mask the horrendously complex technology ecosystems we’ve accumulated. Newman’s concern is that “single-issue” platform teams risk becoming the very bottleneck they’re supposed to alleviate, and can become so focused on managing the tool that they forget about outcomes. Rather than using names like Platform Team, Newman suggests more outcome-oriented labels such as “Delivery Support” or even better “Delivery Enablement.”\n\n### Platform Engineering Is All about Scaling – Fact\n\nPlatform engineering solves the challenges of scaling and accelerating DevOps adoption by dedicating a team to the delivery of a shared self-service platform for app developers. It works best for enterprises with more mature DevOps practices who need to scale and move fast. It often does not make sense in smaller companies, for a single development team, or when multiple divergent platforms need to be supported, where scale is not a driving factor of success yet.\n\n### Platform Engineering Centers on IDPs and Golden Paths – Fact\n\nOne definition of platform engineering is the practice of creating a reusable set of standardized tools, components, and automated processes, often referred to as an [internal developer platform](https://internaldeveloperplatform.org/what-is-an-internal-developer-platform/) (IDP). IDPs, and the teams that build them, provide paths of least resistance that developers can take to complete their day-to-day tasks. These “golden paths” come with recommended tools and best security practices built in, enabling developers to self-serve and self-manage their code.\n\n### Platform Engineering Requires a Platform as Product Approach – Fact\n\nGartner and others recommend treating your platform as a product by treating the developers who use it as your customers, so that they in turn can deliver services to your organization’s customers. Like any other product, pushing your platform on to developers without their input is unlikely to produce positive outcomes. So it’s essential to talk to your internal consumers to solve for their needs. Many traditional infrastructure teams don’t do this and often don’t even understand the workloads running on their platforms.\n\n## Striking a Balance\n\nSuccessful IDPs achieve a balance between allowing developers to remain in the flow state of highly productive coding while eliminating repetitive tasks through automated, full-stack environments. Developers can deliver apps faster because platform engineers smooth the path for them, enabling them to create their own environment with every check-in. This allows devs to review, share, and test apps without waiting in line or worrying about code conflicts. When done well, platform engineering delivers best-in-class developer experiences; provides choices of leading tools, platforms and clouds across the software development lifecycle; and gives self-service access to full-stack environments to every developer.\n",
          "code": "var Component=(()=>{var p=Object.create;var i=Object.defineProperty;var d=Object.getOwnPropertyDescriptor;var g=Object.getOwnPropertyNames;var m=Object.getPrototypeOf,f=Object.prototype.hasOwnProperty;var u=(n,e)=>()=>(e||n((e={exports:{}}).exports,e),e.exports),v=(n,e)=>{for(var a in e)i(n,a,{get:e[a],enumerable:!0})},s=(n,e,a,r)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let o of g(e))!f.call(n,o)&&o!==a&&i(n,o,{get:()=>e[o],enumerable:!(r=d(e,o))||r.enumerable});return n};var w=(n,e,a)=>(a=n!=null?p(m(n)):{},s(e||!n||!n.__esModule?i(a,\"default\",{value:n,enumerable:!0}):a,n)),y=n=>s(i({},\"__esModule\",{value:!0}),n);var c=u((T,l)=>{l.exports=_jsx_runtime});var P={};v(P,{default:()=>D,frontmatter:()=>b});var t=w(c()),b={title:\"Platform Engineering: What\\u2019s Hype and What\\u2019s Not?\",summary:\"Platform engineering is all the rage these days. Separate the facts from the hype with this roundup.\",publishDate:\"Tue Jan 30 2024 21:35:44 GMT+0000 (Coordinated Universal Time)\",author:\"tommy-mcclung\",readingTime:5,categories:[\"platform-engineering\",\"product\"],mainImage:\"/blog-images/aa447085214bc561a527d812197f6e52.png\",imageAlt:\"Platform Engineering: What\\u2019s Hype and What\\u2019s Not?\",showCTA:!0,ctaCopy:\"Looking to streamline software delivery at scale like platform engineering? Try Release for on-demand environments mirroring production.\",ctaLink:\"https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=platform-engineering-whats-hype-and-whats-not\",relatedPosts:[\"\"],ogImage:\"/blog-images/aa447085214bc561a527d812197f6e52.png\",excerpt:\"Platform engineering is all the rage these days. Separate the facts from the hype with this roundup.\",tags:[\"platform-engineering\",\"product\"],ctaButton:\"Try Release for Free\"};function h(n){let e=Object.assign({p:\"p\",h3:\"h3\",a:\"a\",span:\"span\",em:\"em\",h2:\"h2\"},n.components);return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(e.p,{children:\"There\\u2019s a sea change happening in the software development world. The merging of developers and operations that gave us DevOps may be entering a new chapter. In this new world, the emerging discipline of platform engineering is quickly gaining popularity.\"}),`\n`,(0,t.jsx)(e.p,{children:\"Platform engineering is the practice of designing and building self-service capabilities to reduce cognitive load for developers and to facilitate fast-flow software delivery. Many large organizations have struggled to reap the benefits of DevOps, in part because shifting more operational and security concerns \\u201Cleft\\u201D and into the domain of software developers has created bottlenecks for dev teams. At the same time, faced with a growing cognitive burden of repetitive, time-consuming tasks that kick them out of the flow state of highly productive coding, many devs want less and less to do with ops and the \\u201Cyou build it, you run it\\u201D paradigm.\"}),`\n`,(0,t.jsx)(e.p,{children:\"Platform engineering is emerging as the solution to many of these challenges. But in all the buzz around it, it\\u2019s hard to know what\\u2019s real and what\\u2019s not. To help you separate the facts from the hype, here\\u2019s a round-up of viewpoints on what platform engineering is and is not.\"}),`\n`,(0,t.jsxs)(e.h3,{id:\"platform-engineering-is-new--hype\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#platform-engineering-is-new--hype\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Platform Engineering Is New \\u2013 Hype\"]}),`\n`,(0,t.jsxs)(e.p,{children:[\"There are those who hail platform engineering as \",(0,t.jsx)(e.a,{href:\"https://www.youtube.com/watch?v=wXyNHngEN-s\",children:\"the new kid on the block\"}),\". But there\\u2019s nothing new about the building of digital platforms as a means of delivering software at scale. It even pre-dates the birth of the DevOps movement in the mid-2000s. According to \",(0,t.jsxs)(e.a,{href:\"https://support.puppet.com/hc/en-us/articles/221368047-The-2023-State-of-DevOps-Report-is-here-\",children:[\"Puppet\\u2019s \",(0,t.jsx)(e.em,{children:\"2023 State of DevOps Report\"})]}),\", large software companies have been taking a platform approach for decades as a way to enable developer teams to build, ship, and operate applications more quickly and at higher quality.\"]}),`\n`,(0,t.jsxs)(e.p,{children:[\"What is new, especially in the enterprise space, is the rapidly growing traction of platform engineering as a way for larger companies to improve software delivery at scale. Gartner identified platform engineering (which it interchangeably calls \\u201Cplatform ops\\u201D) as one of the \",(0,t.jsx)(e.a,{href:\"https://www.gartner.com/en/information-technology/insights/top-technology-trends\",children:\"Top Strategic Technology Trends of 2023\"}),\". Gartner analysts predict that 80% of software engineering organizations will establish platform teams by 2026, and 75% of those will include developer self-service portals.\"]}),`\n`,(0,t.jsxs)(e.h3,{id:\"platform-engineering-has-toppled-devops--hype\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#platform-engineering-has-toppled-devops--hype\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Platform Engineering Has Toppled DevOps \\u2013 Hype\"]}),`\n`,(0,t.jsxs)(e.p,{children:[\"Those who claim that DevOps is dead and that platform engineering has supplanted it are engaging in hyperbole. \\u201CDevOps is dead, long live Platform Engineering!\\u201D \",(0,t.jsx)(e.a,{href:\"https://twitter.com/sidpalas/status/1551936840453820417\",children:\"tweeted\"}),\" software engineer and DevOps commentator Sid Palas in 2022. \\u201C1. Developers don\\u2019t like dealing with infra, 2. Companies need control of their infra as they grow. Platform engineering enables these two facts to coexist.\\u201D\"]}),`\n`,(0,t.jsxs)(e.h3,{id:\"platform-engineering-is-the-next-evolution-of-devops-and-sre--fact\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#platform-engineering-is-the-next-evolution-of-devops-and-sre--fact\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Platform Engineering Is the Next Evolution of DevOps and SRE \\u2013 Fact\"]}),`\n`,(0,t.jsx)(e.p,{children:\"Rather than dealing a death blow to DevOps, a more accurate take is that platform engineering is the next evolution of DevOps and SRE (site reliability engineering). In particular, it benefits developers struggling with code production bottlenecks as they wait on internal approvals or fulfillment. It also helps devs deliver on their own timeline rather than that of their IT team. And it helps operator types (such as SREs or DevOps engineers) who are feeling the pain of repetitive request fulfillment and operational firefighting \\u2014 busy work that keeps them from building their vision for the future.\"}),`\n`,(0,t.jsxs)(e.h3,{id:\"platform-engineering-should-embrace-devops-culture--fact\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#platform-engineering-should-embrace-devops-culture--fact\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Platform Engineering Should Embrace DevOps Culture \\u2013 Fact\"]}),`\n`,(0,t.jsx)(e.p,{children:\"The agile development practices that are at the core of DevOps culture \\u2014 such as collaboration, communication, and continuous improvement \\u2014 have not extended to the operations domain. This has hobbled the ability of agile development teams to quickly deliver products. In order not to perpetuate this dynamic, DevOps team culture should evolve to support platform engineering, and platform teams should embrace DevOps team culture.\"}),`\n`,(0,t.jsxs)(e.h3,{id:\"platform-engineering-is-a-con--hype\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#platform-engineering-is-a-con--hype\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Platform Engineering Is a Con \\u2013 Hype\"]}),`\n`,(0,t.jsxs)(e.p,{children:[\"There are those, like independent technology consultant Sam Newman, who argue \",(0,t.jsx)(e.a,{href:\"https://samnewman.io/blog/2023/02/08/dont-call-it-a-platform/\",children:\"platform engineering is just another vendor-generated label to be slapped on to old practices\"}),\" in a bid to mask the horrendously complex technology ecosystems we\\u2019ve accumulated. Newman\\u2019s concern is that \\u201Csingle-issue\\u201D platform teams risk becoming the very bottleneck they\\u2019re supposed to alleviate, and can become so focused on managing the tool that they forget about outcomes. Rather than using names like Platform Team, Newman suggests more outcome-oriented labels such as \\u201CDelivery Support\\u201D or even better \\u201CDelivery Enablement.\\u201D\"]}),`\n`,(0,t.jsxs)(e.h3,{id:\"platform-engineering-is-all-about-scaling--fact\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#platform-engineering-is-all-about-scaling--fact\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Platform Engineering Is All about Scaling \\u2013 Fact\"]}),`\n`,(0,t.jsx)(e.p,{children:\"Platform engineering solves the challenges of scaling and accelerating DevOps adoption by dedicating a team to the delivery of a shared self-service platform for app developers. It works best for enterprises with more mature DevOps practices who need to scale and move fast. It often does not make sense in smaller companies, for a single development team, or when multiple divergent platforms need to be supported, where scale is not a driving factor of success yet.\"}),`\n`,(0,t.jsxs)(e.h3,{id:\"platform-engineering-centers-on-idps-and-golden-paths--fact\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#platform-engineering-centers-on-idps-and-golden-paths--fact\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Platform Engineering Centers on IDPs and Golden Paths \\u2013 Fact\"]}),`\n`,(0,t.jsxs)(e.p,{children:[\"One definition of platform engineering is the practice of creating a reusable set of standardized tools, components, and automated processes, often referred to as an \",(0,t.jsx)(e.a,{href:\"https://internaldeveloperplatform.org/what-is-an-internal-developer-platform/\",children:\"internal developer platform\"}),\" (IDP). IDPs, and the teams that build them, provide paths of least resistance that developers can take to complete their day-to-day tasks. These \\u201Cgolden paths\\u201D come with recommended tools and best security practices built in, enabling developers to self-serve and self-manage their code.\"]}),`\n`,(0,t.jsxs)(e.h3,{id:\"platform-engineering-requires-a-platform-as-product-approach--fact\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#platform-engineering-requires-a-platform-as-product-approach--fact\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Platform Engineering Requires a Platform as Product Approach \\u2013 Fact\"]}),`\n`,(0,t.jsx)(e.p,{children:\"Gartner and others recommend treating your platform as a product by treating the developers who use it as your customers, so that they in turn can deliver services to your organization\\u2019s customers. Like any other product, pushing your platform on to developers without their input is unlikely to produce positive outcomes. So it\\u2019s essential to talk to your internal consumers to solve for their needs. Many traditional infrastructure teams don\\u2019t do this and often don\\u2019t even understand the workloads running on their platforms.\"}),`\n`,(0,t.jsxs)(e.h2,{id:\"striking-a-balance\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#striking-a-balance\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Striking a Balance\"]}),`\n`,(0,t.jsx)(e.p,{children:\"Successful IDPs achieve a balance between allowing developers to remain in the flow state of highly productive coding while eliminating repetitive tasks through automated, full-stack environments. Developers can deliver apps faster because platform engineers smooth the path for them, enabling them to create their own environment with every check-in. This allows devs to review, share, and test apps without waiting in line or worrying about code conflicts. When done well, platform engineering delivers best-in-class developer experiences; provides choices of leading tools, platforms and clouds across the software development lifecycle; and gives self-service access to full-stack environments to every developer.\"})]})}function k(n={}){let{wrapper:e}=n.components||{};return e?(0,t.jsx)(e,Object.assign({},n,{children:(0,t.jsx)(h,n)})):h(n)}var D=k;return y(P);})();\n;return Component;"
        },
        "_id": "blog/posts/platform-engineering-whats-hype-and-whats-not.mdx",
        "_raw": {
          "sourceFilePath": "blog/posts/platform-engineering-whats-hype-and-whats-not.mdx",
          "sourceFileName": "platform-engineering-whats-hype-and-whats-not.mdx",
          "sourceFileDir": "blog/posts",
          "contentType": "mdx",
          "flattenedPath": "blog/posts/platform-engineering-whats-hype-and-whats-not"
        },
        "type": "BlogPost",
        "computedSlug": "platform-engineering-whats-hype-and-whats-not"
      },
      "documentHash": "1739393595025",
      "hasWarnings": false,
      "documentTypeName": "BlogPost"
    },
    "blog/posts/podcast-guest-on-screaming-in-the-cloud.mdx": {
      "document": {
        "title": "Podcast Guest on Screaming in the Cloud",
        "summary": "I had the pleasure of reconnecting with Corey Quinn on his excellent podcast, “Screaming in the Cloud”. The far-ranging",
        "publishDate": "Tue Jan 26 2021 22:22:08 GMT+0000 (Coordinated Universal Time)",
        "author": "regis-wilson",
        "readingTime": 1,
        "categories": [
          "platform-engineering",
          "product"
        ],
        "mainImage": "/blog-images/a3d683b9285b42f0a7f2d8625270904f.jpg",
        "imageAlt": "Release on Corey Quinn podcast “Screaming in the Cloud”",
        "showCTA": true,
        "ctaCopy": "Looking to streamline workflows like discussed on \"Screaming in the Cloud\"? Try Release for on-demand environments and faster deployments.",
        "ctaLink": "https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=podcast-guest-on-screaming-in-the-cloud",
        "relatedPosts": [
          ""
        ],
        "ogImage": "/blog-images/a3d683b9285b42f0a7f2d8625270904f.jpg",
        "excerpt": "I had the pleasure of reconnecting with Corey Quinn on his excellent podcast, “Screaming in the Cloud”. The far-ranging",
        "tags": [
          "platform-engineering",
          "product"
        ],
        "ctaButton": "Try Release for Free",
        "body": {
          "raw": "\nI had the pleasure of reconnecting with Corey Quinn on his excellent podcast, [“Screaming in the Cloud”](https://www.lastweekinaws.com/podcast/). The far-ranging topics wandered working together at a company, to discussing how far internet technology has developed, to changing jobs in the middle of a pandemic, to what we do at Release. You can view the full episode and transcript [here](https://www.lastweekinaws.com/podcast/screaming-in-the-cloud/reconnecting-with-an-old-boss-with-regis-wilson/).\n\nYou can listen to the podcast below.\n",
          "code": "var Component=(()=>{var g=Object.create;var i=Object.defineProperty;var h=Object.getOwnPropertyDescriptor;var m=Object.getOwnPropertyNames;var u=Object.getPrototypeOf,p=Object.prototype.hasOwnProperty;var f=(n,e)=>()=>(e||n((e={exports:{}}).exports,e),e.exports),w=(n,e)=>{for(var o in e)i(n,o,{get:e[o],enumerable:!0})},r=(n,e,o,s)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let a of m(e))!p.call(n,a)&&a!==o&&i(n,a,{get:()=>e[a],enumerable:!(s=h(e,a))||s.enumerable});return n};var C=(n,e,o)=>(o=n!=null?g(u(n)):{},r(e||!n||!n.__esModule?i(o,\"default\",{value:n,enumerable:!0}):o,n)),b=n=>r(i({},\"__esModule\",{value:!0}),n);var d=f((k,c)=>{c.exports=_jsx_runtime});var j={};w(j,{default:()=>_,frontmatter:()=>x});var t=C(d()),x={title:\"Podcast Guest on Screaming in the Cloud\",summary:\"I had the pleasure of reconnecting with Corey Quinn on his excellent podcast, \\u201CScreaming in the Cloud\\u201D. The far-ranging\",publishDate:\"Tue Jan 26 2021 22:22:08 GMT+0000 (Coordinated Universal Time)\",author:\"regis-wilson\",readingTime:1,categories:[\"platform-engineering\",\"product\"],mainImage:\"/blog-images/a3d683b9285b42f0a7f2d8625270904f.jpg\",imageAlt:\"Release on Corey Quinn podcast \\u201CScreaming in the Cloud\\u201D\",showCTA:!0,ctaCopy:'Looking to streamline workflows like discussed on \"Screaming in the Cloud\"? Try Release for on-demand environments and faster deployments.',ctaLink:\"https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=podcast-guest-on-screaming-in-the-cloud\",relatedPosts:[\"\"],ogImage:\"/blog-images/a3d683b9285b42f0a7f2d8625270904f.jpg\",excerpt:\"I had the pleasure of reconnecting with Corey Quinn on his excellent podcast, \\u201CScreaming in the Cloud\\u201D. The far-ranging\",tags:[\"platform-engineering\",\"product\"],ctaButton:\"Try Release for Free\"};function l(n){let e=Object.assign({p:\"p\",a:\"a\"},n.components);return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsxs)(e.p,{children:[\"I had the pleasure of reconnecting with Corey Quinn on his excellent podcast, \",(0,t.jsx)(e.a,{href:\"https://www.lastweekinaws.com/podcast/\",children:\"\\u201CScreaming in the Cloud\\u201D\"}),\". The far-ranging topics wandered working together at a company, to discussing how far internet technology has developed, to changing jobs in the middle of a pandemic, to what we do at Release. You can view the full episode and transcript \",(0,t.jsx)(e.a,{href:\"https://www.lastweekinaws.com/podcast/screaming-in-the-cloud/reconnecting-with-an-old-boss-with-regis-wilson/\",children:\"here\"}),\".\"]}),`\n`,(0,t.jsx)(e.p,{children:\"You can listen to the podcast below.\"})]})}function y(n={}){let{wrapper:e}=n.components||{};return e?(0,t.jsx)(e,Object.assign({},n,{children:(0,t.jsx)(l,n)})):l(n)}var _=y;return b(j);})();\n;return Component;"
        },
        "_id": "blog/posts/podcast-guest-on-screaming-in-the-cloud.mdx",
        "_raw": {
          "sourceFilePath": "blog/posts/podcast-guest-on-screaming-in-the-cloud.mdx",
          "sourceFileName": "podcast-guest-on-screaming-in-the-cloud.mdx",
          "sourceFileDir": "blog/posts",
          "contentType": "mdx",
          "flattenedPath": "blog/posts/podcast-guest-on-screaming-in-the-cloud"
        },
        "type": "BlogPost",
        "computedSlug": "podcast-guest-on-screaming-in-the-cloud"
      },
      "documentHash": "1739393595025",
      "hasWarnings": false,
      "documentTypeName": "BlogPost"
    },
    "blog/posts/rails-and-kubernetes-a-guide-to-using-them-together.mdx": {
      "document": {
        "title": "Rails and Kubernetes: A Guide to Using Them Together",
        "summary": "Is Kubernetes only for modern, cloud-native, microservices-based applications or can it be used with Rails monoliths?\n",
        "publishDate": "Mon Sep 26 2022 20:15:41 GMT+0000 (Coordinated Universal Time)",
        "author": "erik-landerholm",
        "readingTime": 3,
        "categories": [
          "kubernetes",
          "platform-engineering"
        ],
        "mainImage": "/blog-images/20dfb68ea8564e9f6a8f0669a46c11e5.jpg",
        "imageAlt": "Rails and Kubernetes: A Guide to Using Them Together",
        "showCTA": true,
        "ctaCopy": "Streamline your Rails application deployment on Kubernetes with Release's on-demand environments for efficient testing and deployment workflows.",
        "ctaLink": "https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=rails-and-kubernetes-a-guide-to-using-them-together",
        "relatedPosts": [
          ""
        ],
        "ogImage": "/blog-images/20dfb68ea8564e9f6a8f0669a46c11e5.jpg",
        "excerpt": "Is Kubernetes only for modern, cloud-native, microservices-based applications or can it be used with Rails monoliths?\n",
        "tags": [
          "kubernetes",
          "platform-engineering"
        ],
        "ctaButton": "Try Release for Free",
        "body": {
          "raw": "\nYou may have heard that Kubernetes is for modern cloud-native, microservices-based applications. Does that mean you can't run it with a monolith Rails application? \n\nNot quite. Technically, nothing will stop you from just deploying your Rails application on Kubernetes. It'll work just fine. But there are a few things to consider to decide if you actually should. \n\nLet's take a look. \n\n### Rails on Kubernetes—Why?\n\nLet's start with the most important question: why would you want to run your Rails application at all? \n\nAs we already mentioned, Rails applications are monolithic, which means most of the application logic is combined into one piece. And Kubernetes was designed to run distributed, a [microservices-based](https://en.wikipedia.org/wiki/Microservices) application where one application consists of multiple smaller pieces. So, it sounds like it's not a good fit for Rails, right? \n\nWell, at first, indeed, it does. But let's dive a bit deeper into it. \n\nWhat's your alternative? Running a Rails application on a normal server, right? \n\nAnd do you have only one server for your Rails application? If it's a very small application or you just started with Rails, then possibly. \n\nBut in most production situations, you'll actually have a few servers working together for one Rails application. This includes one server for the Rails itself, another server handling access to your application—so, for example, NGINX and Puma—a third server for Redis, a fourth server for a database, and a fifth server for background jobs like Delayed Job or Sidekiq. \n\nOn top of that, you'll probably have some load balancer in front of all that and maybe yet another separate server or a dedicated solution for storage. \n\nAnd, suddenly, you realize that your monolithic Rails application actually requires a dozen servers and not just one. **‍**\n\n#### The Problem\n\nThe problem with that approach is that it's not very flexible, and you need to make a lot of compromises. For example, some of your servers may be heavily underutilized most of the time, but you can't scale them down because, from time to time, there is a spike in traffic. And the bigger the application and the more servers you have, the more resources that are probably wasted—or, to put it nicer, not used efficiently. \n\nAnother thing is that it'll take a lot of time and effort to get all these servers maintained—periodic system upgrades, making sure they all have enough free disk space left, etc. \n\nAdding a new server when needed will probably take a while, too. You'll need to deploy it first, then configure the operating system, then install what's needed for Rails, and finally make sure it connects properly with the rest. \n\n![](/blog-images/cfd60170ce84a5144d500bb231a50392.png)\n\n#### Enter the Kubernetes World\n\nThe problems stated in the previous section bring us to Kubernetes. You could simply put all your Rails application pieces to Kubernetes and not worry about individual servers anymore. Kubernetes abstracts infrastructure from you, so you won't need to worry about underlying operating systems, security patching, etc. \n\nSure, your Rails application will still be a monolith, so you won't get all the benefits of Kubernetes, but you'll get some. All the extras that your Rails application may need will still be deployed on Kubernetes as separate pieces, so you'll still benefit from easier scaling and node management.\n\nAnd, last but not least, once your Rails application is on Kubernetes, you'll have an easier path to start decoupling your monolith in microservices if you wish to do so. \n\n#### **How to Use Rails on Kubernetes**‍\n\n‍Now that we know some theory, let's see in practice how to run Rails application on Kubernetes.\n\nIn fact, there's nothing special about running Rails on Kubernetes in comparison with other frameworks. You'll need to package your Rails application into a container and create a YAML manifest file in order to deploy it on Kubernetes. \n\nLet's start with the container. There's no single specific way of packaging Ruby on Rails applications into a container—a lot will depend on how your application is working and how many best practices you want to implement, but here's a good starting point. \n\n#### Docker Container\n\n```yaml\n\nFROM ruby:3.1\n\n# Create directory for our Rails application and set it as working directory\n\nRUN mkdir /app\nWORKDIR /app\n\n# Copy the Gemfile\n\nCOPY Gemfile Gemfile.lock ./\n\n# Install nodejs and yarl (if you need anything else you can add it here)\n\nRUN apt-get update && apt-get install -y nodejs yarn\n\n# Install bundler and run bundle install\n\nRUN gem install bundler\nRUN bundle install\n\n# Copy the Rails application code\n\nCOPY . .\n\n# Precompile the Rails assets.\n\nRUN rake assets:precompile\n\n# Expose your Rails app\n\nEXPOSE 3000\n\n# Run Rails server\n\nCMD [\"rails\", \"server\", \"-b\", \"0.0.0.0\"]\n\n```\n\nSave the above as Dockerfile (with no extension) in your Rails project root directory, and then build your image: \n\n```yaml\n\n$ docker build -t rails_on_k8s:0.1 .\n\nSending build context to Docker daemon  13.58MB\nStep 1/11 : FROM ruby:3.1\n3.1: Pulling from library/ruby\n1671565cc8df: Pull complete\n(...)\n2e02738a3297: Pull complete\nDigest: sha256:74f02cae856057841964d471f0a54a5957dec7079cfe18076c132ce5c6b6ea37\nStatus: Downloaded newer image for ruby:3.1\n ---> e739755aa18e\nStep 2/11 : RUN apt-get update && apt-get install -y nodejs yarn postgresql-client\n ---> Running in 5095ff174667\nGet:1 http://deb.debian.org/debian bullseye InRelease [116 kB]\n(...)\nStep 11/11 : CMD [\"rails\", \"server\", \"-b\", \"0.0.0.0\"]\n ---> Running in ac82bad67bb6\nRemoving intermediate container ac82bad67bb6\n ---> 731093adf23f\nSuccessfully built 731093adf23f\nSuccessfully tagged rails_on_k8s:0.1\n\n```\n\nOK, that's pretty much it. That's how you package a Rails application into a Docker container. \n\nNow what? \n\nWe could start writing out Kubernetes manifest, but it's best to first check if our application is working in the container properly. To do that, run that freshly built image by executing **docker run -p 3000:3000 rails_on_k8s:0.1**, and then open your web browser and head to **localhost:3000.** \n\nIf you see your application, everything is good, and we can proceed. \n\n#### Kubernetes Manifest\n\nOK, we're ready to write a Kubernetes deployment manifest for our Rails application. \n\nIt'll be a fairly simple deployment at first. The only thing you need is to upload your Docker image to some container registry. If you don't have your own private repository, you can use a free account on dockerhub.com. You'll need to register there and follow the instructions on tagging your image and pushing it to the registry. \n\nOnce you have your image ready in a container registry, you can create a Kubernetes manifest: \n\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n   name: rails-on-k8s\nspec:\n   replicas: 1\n   selector:\n     matchLabels:\n       app: rails-k8s\n   template:\n     metadata:\n       labels:\n         app: rails-k8s\n     spec:\n       containers:\n       - name: rails-k8s\n         image: davezworka/rails_on_k8s:0.1\n         ports:\n         - containerPort: 3000\n```\n\nOnce created, it's time to deploy it. We'll use the **kubectl apply** command and pass the YAML file with the **\\-f** parameter followed by the file name: \n\n```yaml\n$ kubectl apply -f k8s_deployment.yaml\ndeployment.apps/rails-on-k8s created\n```\n\nOK, it looks like it worked! Let's check: \n\n```yaml\n$ kubectl get pods\nNAME                            READY   STATUS    RESTARTS   AGE\nrails-on-k8s-7b9f7fb574-8l74g   1/1     Running   0          48s\n```\n\nIt seems like, indeed, we successfully deployed our Rails application on Kubernetes. \n\nBut here's the first tip for you. The fact that your Rails pod is running doesn't actually mean that the application inside is too. Without proper checks added to our deployment definition, it's possible that our Rails server actually failed to start properly and Kubernetes doesn't know about it. \n\nTo validate if the application in the pod is indeed running, we can first check the logs: \n\n```yaml\n\n$ kubectl logs rails-on-k8s-7b9f7fb574-8l74g\n=> Booting Puma\n=> Rails 7.0.3.1 application starting in development\n=> Run `bin/rails server --help` for more startup options\nPuma starting in single mode...\n* Puma version: 5.6.5 (ruby 3.1.2-p20) (\"Birdie's Version\")\n*  Min threads: 5\n*  Max threads: 5\n*  Environment: development\n*          PID: 1\n* Listening on http://0.0.0.0:3000\n\n```\n\nThat looks promising. It seems like our Rails server started properly, so, most likely, everything is good. But the ultimate test would be just to try to access the application. To do that, we first need to expose it somehow. In the Kubernetes world, we do that using Kubernetes services. \n\n#### Kubernetes Services\n\nIn order to create a service for our Rails deployment, create another YAML file with content similar to this, but keep in mind that you may need to adjust the type of the service based on what kind of Kubernetes cluster you're using: \n\n```yaml\napiVersion: v1\nkind: Service\nmetadata:\n   name: rails-service\nspec:\n   type: LoadBalancer\n   selector:\n     app: rails-k8s\n   ports:\n     - port: 80\n       targetPort: 3000\n```\n\nApply the above using **kubectl apply**: \n\n```yaml\n$ kubectl apply -f rails-svc.yaml\nservice/rails-service created\n```\n\nNow, after a few seconds, we should see the IP address of the load balancer that exposes our application: \n\n```yaml\n$ kubectl get svc\nNAME               TYPE           CLUSTER-IP   EXTERNAL-IP     PORT(S)        AGE\nkubernetes         ClusterIP      10.0.0.1               443/TCP        8h\nrails-service      LoadBalancer   10.0.74.93   20.101.11.115   80:31330/TCP   48s\n```\n\nAnd if you open that IP address in your web browser, you should see your application, which in our case is showing a simple \"Hello world\" message: \n\n![](/blog-images/41ada768a12014157ed6f76a40ee7223.jpeg)\n\nAll works well, then. We have a Rails application running on a Kubernetes cluster. \n\nBut this was just a simple example when we only deployed our Rails application. If that's all you'd do, it probably won't give you many benefits to move to Kubernetes. \n\nEarlier in this post, we mentioned that it makes sense to move your Rails application when you have lots of other components supporting your application. \n\nAnd if you do, the process of moving them to Kubernetes would be very similar. You either create a [Docker image](https://release.com/blog/cutting-build-time-in-half-docker-buildx-kubernetes) from scratch for all the components you need or search for ready-to-use images. Companies behind well-known tools like Redis or NGINX are publishing images themselves, so it's a little bit easier to onboard them on Kubernetes. \n\n#### But What About the Database?\n\nThis is a fair question. The database is really important for Rails. Databases can run on Kubernetes too. But depending on your specific needs and due to the nature of databases, it may make more sense to use an SaaS database or keep it on traditional servers. \n\nYou can easily keep your Rails application on a Kubernetes cluster and have it connected to the external database. Of course, for performance reasons, however, you should keep it as close as possible to your Kubernetes, which means ideally in the same cloud provider, the same region, and even the same network. \n\n#### Rails and Kubernetes Summary\n\nWe've only touched the tip of the iceberg here when it comes to Kubernetes, but you learned how to package your Rails application into a container and deploy it on Kubernetes. From there, the next steps will depend on your application and company specifics. \n\nKubernetes itself is a complex system and gives you multiple choices when it comes to deploying applications, networking, and storage. But you now know how to use it for your Rails application and when it wouldn't really make too much sense. \n\nIf you're interested in learning more about Kubernetes, check out our blog [here](https://release.com/blog) for more posts about it.\n",
          "code": "var Component=(()=>{var h=Object.create;var o=Object.defineProperty;var p=Object.getOwnPropertyDescriptor;var u=Object.getOwnPropertyNames;var m=Object.getPrototypeOf,y=Object.prototype.hasOwnProperty;var b=(a,e)=>()=>(e||a((e={exports:{}}).exports,e),e.exports),g=(a,e)=>{for(var t in e)o(a,t,{get:e[t],enumerable:!0})},s=(a,e,t,r)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let i of u(e))!y.call(a,i)&&i!==t&&o(a,i,{get:()=>e[i],enumerable:!(r=p(e,i))||r.enumerable});return a};var f=(a,e,t)=>(t=a!=null?h(m(a)):{},s(e||!a||!a.__esModule?o(t,\"default\",{value:a,enumerable:!0}):t,a)),w=a=>s(o({},\"__esModule\",{value:!0}),a);var c=b((T,l)=>{l.exports=_jsx_runtime});var K={};g(K,{default:()=>R,frontmatter:()=>k});var n=f(c()),k={title:\"Rails and Kubernetes: A Guide to Using Them Together\",summary:`Is Kubernetes only for modern, cloud-native, microservices-based applications or can it be used with Rails monoliths?\n`,publishDate:\"Mon Sep 26 2022 20:15:41 GMT+0000 (Coordinated Universal Time)\",author:\"erik-landerholm\",readingTime:3,categories:[\"kubernetes\",\"platform-engineering\"],mainImage:\"/blog-images/20dfb68ea8564e9f6a8f0669a46c11e5.jpg\",imageAlt:\"Rails and Kubernetes: A Guide to Using Them Together\",showCTA:!0,ctaCopy:\"Streamline your Rails application deployment on Kubernetes with Release's on-demand environments for efficient testing and deployment workflows.\",ctaLink:\"https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=rails-and-kubernetes-a-guide-to-using-them-together\",relatedPosts:[\"\"],ogImage:\"/blog-images/20dfb68ea8564e9f6a8f0669a46c11e5.jpg\",excerpt:`Is Kubernetes only for modern, cloud-native, microservices-based applications or can it be used with Rails monoliths?\n`,tags:[\"kubernetes\",\"platform-engineering\"],ctaButton:\"Try Release for Free\"};function d(a){let e=Object.assign({p:\"p\",h3:\"h3\",a:\"a\",span:\"span\",strong:\"strong\",h4:\"h4\",img:\"img\",pre:\"pre\",code:\"code\"},a.components);return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(e.p,{children:\"You may have heard that Kubernetes is for modern cloud-native, microservices-based applications. Does that mean you can't run it with a monolith Rails application?\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:\"Not quite. Technically, nothing will stop you from just deploying your Rails application on Kubernetes. It'll work just fine. But there are a few things to consider to decide if you actually should.\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:\"Let's take a look.\\xA0\"}),`\n`,(0,n.jsxs)(e.h3,{id:\"rails-on-kuberneteswhy\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#rails-on-kuberneteswhy\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Rails on Kubernetes\\u2014Why?\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Let's start with the most important question: why would you want to run your Rails application at all?\\xA0\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"As we already mentioned, Rails applications are monolithic, which means most of the application logic is combined into one piece. And Kubernetes was designed to run distributed, a \",(0,n.jsx)(e.a,{href:\"https://en.wikipedia.org/wiki/Microservices\",children:\"microservices-based\"}),\" application where one application consists of multiple smaller pieces. So, it sounds like it's not a good fit for Rails, right?\\xA0\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Well, at first, indeed, it does. But let's dive a bit deeper into it.\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:\"What's your alternative? Running a Rails application on a normal server, right?\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:\"And do you have only one server for your Rails application? If it's a very small application or you just started with Rails, then possibly.\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:\"But in most production situations, you'll actually have a few servers working together for one Rails application. This includes one server for the Rails itself, another server handling access to your application\\u2014so, for example, NGINX and Puma\\u2014a third server for Redis, a fourth server for a database, and a fifth server for background jobs like Delayed Job or Sidekiq.\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:\"On top of that, you'll probably have some load balancer in front of all that and maybe yet another separate server or a dedicated solution for storage.\\xA0\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"And, suddenly, you realize that your monolithic Rails application actually requires a dozen servers and not just one.\\xA0\",(0,n.jsx)(e.strong,{children:\"\\u200D\"})]}),`\n`,(0,n.jsxs)(e.h4,{id:\"the-problem\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#the-problem\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"The Problem\"]}),`\n`,(0,n.jsx)(e.p,{children:\"The problem with that approach is that it's not very flexible, and you need to make a lot of compromises. For example, some of your servers may be heavily underutilized most of the time, but you can't scale them down because, from time to time, there is a spike in traffic. And the bigger the application and the more servers you have, the more resources that are probably wasted\\u2014or, to put it nicer, not used efficiently.\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:\"Another thing is that it'll take a lot of time and effort to get all these servers maintained\\u2014periodic system upgrades, making sure they all have enough free disk space left, etc.\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:\"Adding a new server when needed will probably take a while, too. You'll need to deploy it first, then configure the operating system, then install what's needed for Rails, and finally make sure it connects properly with the rest.\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/cfd60170ce84a5144d500bb231a50392.png\",alt:\"\"})}),`\n`,(0,n.jsxs)(e.h4,{id:\"enter-the-kubernetes-world\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#enter-the-kubernetes-world\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Enter the Kubernetes World\"]}),`\n`,(0,n.jsx)(e.p,{children:\"The problems stated in the previous section bring us to Kubernetes. You could simply put all your Rails application pieces to Kubernetes and not worry about individual servers anymore. Kubernetes abstracts infrastructure from you, so you won't need to worry about underlying operating systems, security patching, etc.\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:\"Sure, your Rails application will still be a monolith, so you won't get all the benefits of Kubernetes, but you'll get some. All the extras that your Rails application may need will still be deployed on Kubernetes as separate pieces, so you'll still benefit from easier scaling and node management.\"}),`\n`,(0,n.jsx)(e.p,{children:\"And, last but not least, once your Rails application is on Kubernetes, you'll have an easier path to start decoupling your monolith in microservices if you wish to do so.\\xA0\"}),`\n`,(0,n.jsxs)(e.h4,{id:\"how-to-use-rails-on-kubernetes\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#how-to-use-rails-on-kubernetes\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.strong,{children:\"How to Use Rails on Kubernetes\"}),\"\\u200D\"]}),`\n`,(0,n.jsx)(e.p,{children:\"\\u200DNow that we know some theory, let's see in practice how to run Rails application on Kubernetes.\"}),`\n`,(0,n.jsx)(e.p,{children:\"In fact, there's nothing special about running Rails on Kubernetes in comparison with other frameworks. You'll need to package your Rails application into a container and create a YAML manifest file in order to deploy it on Kubernetes.\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:\"Let's start with the container. There's no single specific way of packaging Ruby on Rails applications into a container\\u2014a lot will depend on how your application is working and how many best practices you want to implement, but here's a good starting point.\\xA0\"}),`\n`,(0,n.jsxs)(e.h4,{id:\"docker-container\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#docker-container\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Docker Container\"]}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-yaml\",children:`\nFROM ruby:3.1\n\n# Create directory for our Rails application and set it as working directory\n\nRUN mkdir /app\nWORKDIR /app\n\n# Copy the Gemfile\n\nCOPY Gemfile Gemfile.lock ./\n\n# Install nodejs and yarl (if you need anything else you can add it here)\n\nRUN apt-get update && apt-get install -y nodejs yarn\n\n# Install bundler and run bundle install\n\nRUN gem install bundler\nRUN bundle install\n\n# Copy the Rails application code\n\nCOPY . .\n\n# Precompile the Rails assets.\n\nRUN rake assets:precompile\n\n# Expose your Rails app\n\nEXPOSE 3000\n\n# Run Rails server\n\nCMD [\"rails\", \"server\", \"-b\", \"0.0.0.0\"]\n\n`})}),`\n`,(0,n.jsx)(e.p,{children:\"Save the above as Dockerfile (with no extension) in your Rails project root directory, and then build your image:\\xA0\"}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-yaml\",children:`\n$ docker build -t rails_on_k8s:0.1 .\n\nSending build context to Docker daemon \\xA013.58MB\nStep 1/11 : FROM ruby:3.1\n3.1: Pulling from library/ruby\n1671565cc8df: Pull complete\n(...)\n2e02738a3297: Pull complete\nDigest: sha256:74f02cae856057841964d471f0a54a5957dec7079cfe18076c132ce5c6b6ea37\nStatus: Downloaded newer image for ruby:3.1\n ---> e739755aa18e\nStep 2/11 : RUN apt-get update && apt-get install -y nodejs yarn postgresql-client\n ---> Running in 5095ff174667\nGet:1 http://deb.debian.org/debian bullseye InRelease [116 kB]\n(...)\nStep 11/11 : CMD [\"rails\", \"server\", \"-b\", \"0.0.0.0\"]\n ---> Running in ac82bad67bb6\nRemoving intermediate container ac82bad67bb6\n ---> 731093adf23f\nSuccessfully built 731093adf23f\nSuccessfully tagged rails_on_k8s:0.1\n\n`})}),`\n`,(0,n.jsx)(e.p,{children:\"OK, that's pretty much it. That's how you package a Rails application into a Docker container.\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:\"Now what?\\xA0\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"We could start writing out Kubernetes manifest, but it's best to first check if our application is working in the container properly. To do that, run that freshly built image by executing \",(0,n.jsx)(e.strong,{children:\"docker run -p 3000:3000 rails_on_k8s:0.1\"}),\", and then open your web browser and head to \",(0,n.jsx)(e.strong,{children:\"localhost:3000.\"}),\"\\xA0\"]}),`\n`,(0,n.jsx)(e.p,{children:\"If you see your application, everything is good, and we can proceed.\\xA0\"}),`\n`,(0,n.jsxs)(e.h4,{id:\"kubernetes-manifest\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#kubernetes-manifest\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Kubernetes Manifest\"]}),`\n`,(0,n.jsx)(e.p,{children:\"OK, we're ready to write a Kubernetes deployment manifest for our Rails application.\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:\"It'll be a fairly simple deployment at first. The only thing you need is to upload your Docker image to some container registry. If you don't have your own private repository, you can use a free account on dockerhub.com. You'll need to register there and follow the instructions on tagging your image and pushing it to the registry.\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:\"Once you have your image ready in a container registry, you can create a Kubernetes manifest:\\xA0\"}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-yaml\",children:`apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  \\xA0name: rails-on-k8s\nspec:\n  \\xA0replicas: 1\n  \\xA0selector:\n  \\xA0 \\xA0matchLabels:\n  \\xA0 \\xA0 \\xA0app: rails-k8s\n  \\xA0template:\n  \\xA0 \\xA0metadata:\n  \\xA0 \\xA0 \\xA0labels:\n  \\xA0 \\xA0 \\xA0 \\xA0app: rails-k8s\n  \\xA0 \\xA0spec:\n  \\xA0 \\xA0 \\xA0containers:\n  \\xA0 \\xA0 \\xA0- name: rails-k8s\n  \\xA0 \\xA0 \\xA0 \\xA0image: davezworka/rails_on_k8s:0.1\n  \\xA0 \\xA0 \\xA0 \\xA0ports:\n  \\xA0 \\xA0 \\xA0 \\xA0- containerPort: 3000\n`})}),`\n`,(0,n.jsxs)(e.p,{children:[\"Once created, it's time to deploy it. We'll use the \",(0,n.jsx)(e.strong,{children:\"kubectl apply\"}),\" command and pass the YAML file with the \",(0,n.jsx)(e.strong,{children:\"-f\"}),\" parameter followed by the file name:\\xA0\"]}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-yaml\",children:`$ kubectl apply -f k8s_deployment.yaml\ndeployment.apps/rails-on-k8s created\n`})}),`\n`,(0,n.jsx)(e.p,{children:\"OK, it looks like it worked! Let's check:\\xA0\"}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-yaml\",children:`$ kubectl get pods\nNAME \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0READY \\xA0 STATUS \\xA0 \\xA0RESTARTS \\xA0 AGE\nrails-on-k8s-7b9f7fb574-8l74g \\xA0 1/1 \\xA0 \\xA0 Running \\xA0 0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA048s\n`})}),`\n`,(0,n.jsx)(e.p,{children:\"It seems like, indeed, we successfully deployed our Rails application on Kubernetes.\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:\"But here's the first tip for you. The fact that your Rails pod is running doesn't actually mean that the application inside is too. Without proper checks added to our deployment definition, it's possible that our Rails server actually failed to start properly and Kubernetes doesn't know about it.\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:\"To validate if the application in the pod is indeed running, we can first check the logs:\\xA0\"}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-yaml\",children:`\n$ kubectl logs rails-on-k8s-7b9f7fb574-8l74g\n=> Booting Puma\n=> Rails 7.0.3.1 application starting in development\n=> Run \\`bin/rails server --help\\` for more startup options\nPuma starting in single mode...\n* Puma version: 5.6.5 (ruby 3.1.2-p20) (\"Birdie's Version\")\n* \\xA0Min threads: 5\n* \\xA0Max threads: 5\n* \\xA0Environment: development\n* \\xA0 \\xA0 \\xA0 \\xA0 \\xA0PID: 1\n* Listening on http://0.0.0.0:3000\n\n`})}),`\n`,(0,n.jsx)(e.p,{children:\"That looks promising. It seems like our Rails server started properly, so, most likely, everything is good. But the ultimate test would be just to try to access the application. To do that, we first need to expose it somehow. In the Kubernetes world, we do that using Kubernetes services.\\xA0\"}),`\n`,(0,n.jsxs)(e.h4,{id:\"kubernetes-services\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#kubernetes-services\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Kubernetes Services\"]}),`\n`,(0,n.jsx)(e.p,{children:\"In order to create a service for our Rails deployment, create another YAML file with content similar to this, but keep in mind that you may need to adjust the type of the service based on what kind of Kubernetes cluster you're using:\\xA0\"}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-yaml\",children:`apiVersion: v1\nkind: Service\nmetadata:\n  \\xA0name: rails-service\nspec:\n  \\xA0type: LoadBalancer\n  \\xA0selector:\n  \\xA0 \\xA0app: rails-k8s\n  \\xA0ports:\n  \\xA0 \\xA0- port: 80\n  \\xA0 \\xA0 \\xA0targetPort: 3000\n`})}),`\n`,(0,n.jsxs)(e.p,{children:[\"Apply the above using \",(0,n.jsx)(e.strong,{children:\"kubectl apply\"}),\":\\xA0\"]}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-yaml\",children:`$ kubectl apply -f rails-svc.yaml\nservice/rails-service created\n`})}),`\n`,(0,n.jsx)(e.p,{children:\"Now, after a few seconds, we should see the IP address of the load balancer that exposes our application:\\xA0\"}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-yaml\",children:`$ kubectl get svc\nNAME \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 TYPE \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 CLUSTER-IP \\xA0 EXTERNAL-IP \\xA0 \\xA0 PORT(S) \\xA0 \\xA0 \\xA0 \\xA0AGE\nkubernetes \\xA0 \\xA0 \\xA0 \\xA0 ClusterIP \\xA0 \\xA0 \\xA010.0.0.1 \\xA0 \\xA0  \\xA0 \\xA0 \\xA0 \\xA0 \\xA0443/TCP \\xA0 \\xA0 \\xA0 \\xA08h\nrails-service \\xA0 \\xA0 \\xA0LoadBalancer \\xA0 10.0.74.93 \\xA0 20.101.11.115 \\xA0 80:31330/TCP \\xA0 48s\n`})}),`\n`,(0,n.jsx)(e.p,{children:'And if you open that IP address in your web browser, you should see your application, which in our case is showing a simple \"Hello world\" message:\\xA0'}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/41ada768a12014157ed6f76a40ee7223.jpeg\",alt:\"\"})}),`\n`,(0,n.jsx)(e.p,{children:\"All works well, then. We have a Rails application running on a Kubernetes cluster.\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:\"But this was just a simple example when we only deployed our Rails application. If that's all you'd do, it probably won't give you many benefits to move to Kubernetes.\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:\"Earlier in this post, we mentioned that it makes sense to move your Rails application when you have lots of other components supporting your application.\\xA0\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"And if you do, the process of moving them to Kubernetes would be very similar. You either create a \",(0,n.jsx)(e.a,{href:\"https://release.com/blog/cutting-build-time-in-half-docker-buildx-kubernetes\",children:\"Docker image\"}),\" from scratch for all the components you need or search for ready-to-use images. Companies behind well-known tools like Redis or NGINX are publishing images themselves, so it's a little bit easier to onboard them on Kubernetes.\\xA0\"]}),`\n`,(0,n.jsxs)(e.h4,{id:\"but-what-about-the-database\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#but-what-about-the-database\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"But What About the Database?\"]}),`\n`,(0,n.jsx)(e.p,{children:\"This is a fair question. The database is really important for Rails. Databases can run on Kubernetes too. But depending on your specific needs and due to the nature of databases, it may make more sense to use an SaaS database or keep it on traditional servers.\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:\"You can easily keep your Rails application on a Kubernetes cluster and have it connected to the external database. Of course, for performance reasons, however, you should keep it as close as possible to your Kubernetes, which means ideally in the same cloud provider, the same region, and even the same network.\\xA0\"}),`\n`,(0,n.jsxs)(e.h4,{id:\"rails-and-kubernetes-summary\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#rails-and-kubernetes-summary\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Rails and Kubernetes Summary\"]}),`\n`,(0,n.jsx)(e.p,{children:\"We've only touched the tip of the iceberg here when it comes to Kubernetes, but you learned how to package your Rails application into a container and deploy it on Kubernetes. From there, the next steps will depend on your application and company specifics.\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:\"Kubernetes itself is a complex system and gives you multiple choices when it comes to deploying applications, networking, and storage. But you now know how to use it for your Rails application and when it wouldn't really make too much sense.\\xA0\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"If you're interested in learning more about Kubernetes, check out our blog \",(0,n.jsx)(e.a,{href:\"https://release.com/blog\",children:\"here\"}),\" for more posts about it.\"]})]})}function v(a={}){let{wrapper:e}=a.components||{};return e?(0,n.jsx)(e,Object.assign({},a,{children:(0,n.jsx)(d,a)})):d(a)}var R=v;return w(K);})();\n;return Component;"
        },
        "_id": "blog/posts/rails-and-kubernetes-a-guide-to-using-them-together.mdx",
        "_raw": {
          "sourceFilePath": "blog/posts/rails-and-kubernetes-a-guide-to-using-them-together.mdx",
          "sourceFileName": "rails-and-kubernetes-a-guide-to-using-them-together.mdx",
          "sourceFileDir": "blog/posts",
          "contentType": "mdx",
          "flattenedPath": "blog/posts/rails-and-kubernetes-a-guide-to-using-them-together"
        },
        "type": "BlogPost",
        "computedSlug": "rails-and-kubernetes-a-guide-to-using-them-together"
      },
      "documentHash": "1739393595025",
      "hasWarnings": false,
      "documentTypeName": "BlogPost"
    },
    "blog/posts/rainbow-deployment-why-and-how-to-do-it.mdx": {
      "document": {
        "title": "Rainbow Deployment: Why and how to do it",
        "summary": "Zero-downtime deployments with rainbow deployments. Learn how they work and what benefits they can bring.",
        "publishDate": "Mon Sep 26 2022 19:51:01 GMT+0000 (Coordinated Universal Time)",
        "author": "tommy-mcclung",
        "readingTime": 4,
        "categories": [
          "kubernetes",
          "platform-engineering"
        ],
        "mainImage": "/blog-images/ab16c0e7873ce501c5d6d46342286504.jpg",
        "imageAlt": "Rainbow Deployment: Why and how to do it",
        "showCTA": true,
        "ctaCopy": "Automate advanced zero-downtime deployments with Release's ephemeral environments for seamless Kubernetes updates.",
        "ctaLink": "https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=rainbow-deployment-why-and-how-to-do-it",
        "relatedPosts": [
          ""
        ],
        "ogImage": "/blog-images/ab16c0e7873ce501c5d6d46342286504.jpg",
        "excerpt": "Zero-downtime deployments with rainbow deployments. Learn how they work and what benefits they can bring.",
        "tags": [
          "kubernetes",
          "platform-engineering"
        ],
        "ctaButton": "Try Release for Free",
        "body": {
          "raw": "\nWhat makes an application modern? One defining factor of modern applications is whether they use zero-downtime deployments. If you can deploy a new version of your application without your users realizing it, it's a good indicator that your application follows modern practices.\n\nIn modern, cloud-native environments, zero-downtime deployments are relatively easy to achieve, however it's not always as simple as deploying a new version of your application and then quickly switching traffic to it. Some applications may need to finish long-running tasks first. Others will have to somehow deal with not breaking user sessions.\n\nWhat this means is that zero-downtime deployments range from basic to advanced.\n\nIn this post, we're interested in the more advanced zero-downtime deployments. We'll talk about what rainbow deployments are, and how you can use them for efficient zero-downtime deployments. \n\n### Zero-Downtime Deployments\n\n**Zero-downtime deployment** is when you release a new version of your application without any downtime. This usually means that you deploy a new version of the application, and users are switched to that new version without even knowing. \n\nZero-downtime deployments are superior to traditional deployments, where you schedule a \"maintenance window\" and show a \"we are down for maintenance\" message to your users for a certain amount of time. In the world of Kubernetes, there are two main ways of completing (near) zero-downtime deployments: the Kubernetes rolling update deployment, and blue/green deployments. Let's quickly go over both so we'll have a good base of knowledge before diving into the rainbow deployments. \n\n#### Rolling Update\n\n[Kubernetes rolling updates](https://kubernetes.io/docs/tutorials/kubernetes-basics/update/update-intro/) are simple and effective. Whereas the traditional software update process is usually done by shutting down the old version and then deploying the new version which, of course, will introduce some downtime —a Kubernetes rolling update first deploys a new version of the application next to the old version, and switches traffic to the new version as soon as it's marked as up and running. Only then is the old version deleted. Therefore, no downtime. \n\n‍\n\n![](/blog-images/af63f58d457486a41af35f532576fcef.jpeg)\n\nHowever, a Kubernetes rolling update has some limitations. Your application needs to be able to handle such a process, you need to think about database access, and it's a very on/off process. Therefore, you don't have any control over when or how gradually traffic is switched to the new version. \n\n#### Blue-Green Deployments\n\nBlue-green deployments are next-level deployments that try to answer the limitations of simple rolling updates. In this model, you always keep two deployments (or two clones of the whole infrastructure). One is called blue and one is called green. At any given time, only one is active and serving traffic, while the other one will be idle. And once you need to release an update, you do that on the idle side, test if everything works, and then switch the traffic. \n\nThis model is better than a simple rolling update because you have control over switching traffic, and you can have the new version running for a few minutes or even hours so that you can do testing to make sure you won't have any surprises once live traffic hits it. \n\nHowever, while better than rolling updates, blue/green deployments also have their limitations. The most important is that you're limited to two environments: blue and green. In many cases, that's enough, but there are use cases where two environments would be a limiting factor, for, example, if you have long-running tasks such as database migrations or AI processing. \n\n#### When Blue-Green is not Enough\n\nImagine a situation where you deploy a new version of your long-running software to your blue environment, you test if it's okay, and you make it your [live environment](https://release.com/blog/setup-test-environment). Then you do the same again for the green environment—you deploy a new version there and switch again from blue to green. \n\nNow, if you'd like to deploy a new version again, you'd have to do it in the blue environment. But blue could still be working on that long-running task. You can't simply stop a database migration in the middle because you'll end up with a corrupted database. So you'll have to wait until the software on the blue environment is finished before you can make another deployment. And that's where rainbow deployments come into play. \n\n#### What is a Rainbow Deployment?\n\nRainbow deployment is the next level of deployment methods that solves the limitation of blue-green deployments. In fact, rainbow deployments are very similar to blue/green deployments, but they're not limited to only two (blue and green) environments. You can have as many colorful environments as you like—thus the name. \n\nAt Release we use Kubernetes namespaces along with our deployment system to automate the creation and removal of rainbow deployments for your application. Release will automatically create and manage a new namespace for each deployment.\n\nThe working principle of rainbow deployment is the same as blue/green deployments, but you can operate on more copies of your application than just two. So, let's take our example from before, in which we would have to wait for the blue environment to finish the long-running task before making a third deployment. With rainbow deployments, you can just add another environment, let's call it yellow. \n\n![](/blog-images/42fec8e28837bbdd43dfb7c6fc243336.jpeg)\n\nNow we have three environments: blue, green, and yellow. Our blue is busy, and green is currently serving traffic. If we want to deploy a new version of our application, we can deploy it to yellow and then switch traffic to it from green. And that's how rainbow deployment works. \n\nThis is a very powerful method of deploying applications because you can avoid downtime as much as possible for as many users as possible. Long-running tasks blocking your deployments provide just one example, but there are more use cases. For example, if your application uses [WebSockets](https://en.wikipedia.org/wiki/WebSocket), no matter how fast and downtime-free your deployments are, you'll still have to disconnect users from their WebSockets sessions, so they'll potentially lose some notifications or other data from your app. Rainbow deployments are the solution: You deploy a new version of your application, and you keep the old one until all users have disconnected from WebSockets sessions. Then you kill the old version of the application. \n\n#### How to do a Rainbow Deployment\n\nNow that you know what rainbow deployments are, let's see how you actually implement them. There is no single standard way of achieving rainbow deployments and there aren't any tools you can install to do rainbow deployments for you—it's more of a do-it-yourself approach. But it isn't all bad news: you can leverage the tools you have to enable rainbow deployments with just a few extra lines of logic. \n\nSo, how do you do it,? You use your current CI/CD pipelines. All you need to do is to point whatever network device you're using to a specific \"color\" of the application when you deploy. In the case of Kubernetes, this could mean changing the **Service** or **Ingress** objects to point to a different deployment.\n\nBelow are some very simple and typical Kubernetes deployment and service definitions: \n\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n   name: nginx-deployment\n   labels:\n     app: nginx\nspec:\n   replicas: 1\n   selector:\n     matchLabels:\n       app: nginx\n   template:\n     metadata:\n       labels:\n         app: nginx\n     spec:\n       containers:\n       - name: nginx\n         image: your_application:0.1\n         ports:\n         - containerPort: 80\n---\napiVersion: v1\nkind: Service\nmetadata:\n   name: nginxservice\nspec:\n   selector:\n     app: nginx\n   ports:\n     - protocol: TCP\n       port: 80\n       name: nginxs\n       targetPort: 80\n```\n\nWe have one deployment and one service that points to that deployment. The service knows which deployment to target based on deployment labels. The service is instructed to search for deployment that has a label **app** with a value of **nginx.** But what if we target the deployment by color too? Well, that would be a rainbow deployment strategy. \n\n#### Enter Rainbow Magic\n\nSo, your definition would look something like this: \n\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n   name: nginx-deployment-[color]\n   labels:\n     app: nginx\nspec:\n   replicas: 1\n   selector:\n     matchLabels:\n       app: nginx\n   template:\n     metadata:\n       labels:\n         app: nginx\n         color: [color]\n     spec:\n       containers:\n       - name: nginx\n         image: your_application:0.2\n         ports:\n         - containerPort: 80\n---\napiVersion: v1\nkind: Service\nmetadata:\n   name: nginxservice\nspec:\n   selector:\n     app: nginx\n     environment: [color]\n   ports:\n     - protocol: TCP\n       port: 80\n       name: nginxs\n       targetPort: 80\n```\n\nAnd it would be in your CI/CD job to replace **\\[color\\]** in the YAML definition every time you want to deploy a new version. So you deploy your application and service for it, then the next time you want to deploy a new version of that application, instead of updating the existing deployment, you create a new deployment and update the existing service to point to that new deployment. And you can repeat that process as many times as you want. Once the old deployments aren't needed anymore, you can delete them. This is the working principle of rainbow deployments. \n\nIt's also worth mentioning that you don't need to use colors to distinguish your deployments—you can use anything. A common example is to use a Git commit hash. Also, this method isn't exclusive to Kubernetes. You can use it in pretty much any infrastructure or environment as long as you have a way to identify a specific deployment and point your network traffic to. \n\n#### Rainbow Deployment Summary\n\nRainbow deployments solve a lot of problems that come with common deployment methods and they bring true benefits to your users. However, rainbow deployments are not a magic solution that will solve all your application problems. Your infrastructure and your application need to be compatible with this approach. Database handling may be especially tricky (for example, you don't want to have two applications writing to the same record in the same database). But these are typical problems that you need to solve anyway when dealing with distributed systems. \n\nOnce you improve the user experience, you can also think about improving your developer productivity. If you want to learn more, take a look at our [documentation here.](https://docs.releasehub.com/reference-documentation/workflows-in-release/rainbow-deployments)\n\n‍\n",
          "code": "var Component=(()=>{var c=Object.create;var i=Object.defineProperty;var p=Object.getOwnPropertyDescriptor;var m=Object.getOwnPropertyNames;var u=Object.getPrototypeOf,y=Object.prototype.hasOwnProperty;var w=(o,e)=>()=>(e||o((e={exports:{}}).exports,e),e.exports),g=(o,e)=>{for(var t in e)i(o,t,{get:e[t],enumerable:!0})},s=(o,e,t,r)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let a of m(e))!y.call(o,a)&&a!==t&&i(o,a,{get:()=>e[a],enumerable:!(r=p(e,a))||r.enumerable});return o};var f=(o,e,t)=>(t=o!=null?c(u(o)):{},s(e||!o||!o.__esModule?i(t,\"default\",{value:o,enumerable:!0}):t,o)),b=o=>s(i({},\"__esModule\",{value:!0}),o);var d=w((I,l)=>{l.exports=_jsx_runtime});var N={};g(N,{default:()=>x,frontmatter:()=>v});var n=f(d()),v={title:\"Rainbow Deployment: Why and how to do it\",summary:\"Zero-downtime deployments with rainbow deployments. Learn how they work and what benefits they can bring.\",publishDate:\"Mon Sep 26 2022 19:51:01 GMT+0000 (Coordinated Universal Time)\",author:\"tommy-mcclung\",readingTime:4,categories:[\"kubernetes\",\"platform-engineering\"],mainImage:\"/blog-images/ab16c0e7873ce501c5d6d46342286504.jpg\",imageAlt:\"Rainbow Deployment: Why and how to do it\",showCTA:!0,ctaCopy:\"Automate advanced zero-downtime deployments with Release's ephemeral environments for seamless Kubernetes updates.\",ctaLink:\"https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=rainbow-deployment-why-and-how-to-do-it\",relatedPosts:[\"\"],ogImage:\"/blog-images/ab16c0e7873ce501c5d6d46342286504.jpg\",excerpt:\"Zero-downtime deployments with rainbow deployments. Learn how they work and what benefits they can bring.\",tags:[\"kubernetes\",\"platform-engineering\"],ctaButton:\"Try Release for Free\"};function h(o){let e=Object.assign({p:\"p\",h3:\"h3\",a:\"a\",span:\"span\",strong:\"strong\",h4:\"h4\",img:\"img\",pre:\"pre\",code:\"code\"},o.components);return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(e.p,{children:\"What makes an application modern? One defining factor of modern applications is whether they use zero-downtime deployments. If you can deploy a new version of your application without your users realizing it, it's a good indicator that your application follows modern practices.\"}),`\n`,(0,n.jsx)(e.p,{children:\"In modern, cloud-native environments, zero-downtime deployments are relatively easy to achieve, however it's not always as simple as deploying a new version of your application and then quickly switching traffic to it. Some applications may need to finish long-running tasks first. Others will have to somehow deal with not breaking user sessions.\"}),`\n`,(0,n.jsx)(e.p,{children:\"What this means is that zero-downtime deployments range from basic to advanced.\"}),`\n`,(0,n.jsx)(e.p,{children:\"In this post, we're interested in the more advanced zero-downtime deployments. We'll talk about what rainbow deployments are, and how you can use them for efficient zero-downtime deployments.\\xA0\"}),`\n`,(0,n.jsxs)(e.h3,{id:\"zero-downtime-deployments\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#zero-downtime-deployments\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Zero-Downtime Deployments\"]}),`\n`,(0,n.jsxs)(e.p,{children:[(0,n.jsx)(e.strong,{children:\"Zero-downtime deployment\"}),\" is when you release a new version of your application without any downtime. This usually means that you deploy a new version of the application, and users are switched to that new version without even knowing.\\xA0\"]}),`\n`,(0,n.jsx)(e.p,{children:`Zero-downtime deployments are superior to traditional deployments, where you schedule a \"maintenance window\" and show a \"we are down for maintenance\" message to your users for a certain amount of time. In the world of Kubernetes, there are two main ways of completing (near) zero-downtime deployments: the Kubernetes rolling update deployment, and blue/green deployments. Let's quickly go over both so we'll have a good base of knowledge before diving into the rainbow deployments.\\xA0`}),`\n`,(0,n.jsxs)(e.h4,{id:\"rolling-update\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#rolling-update\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Rolling Update\"]}),`\n`,(0,n.jsxs)(e.p,{children:[(0,n.jsx)(e.a,{href:\"https://kubernetes.io/docs/tutorials/kubernetes-basics/update/update-intro/\",children:\"Kubernetes rolling updates\"}),\" are simple and effective. Whereas the traditional software update process is usually done by shutting down the old version and then deploying the new version which, of course, will introduce some downtime \\u2014a Kubernetes rolling update first deploys a new version of the application next to the old version, and switches traffic to the new version as soon as it's marked as up and running. Only then is the old version deleted. Therefore, no downtime.\\xA0\"]}),`\n`,(0,n.jsx)(e.p,{children:\"\\u200D\"}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/af63f58d457486a41af35f532576fcef.jpeg\",alt:\"\"})}),`\n`,(0,n.jsx)(e.p,{children:\"However, a Kubernetes rolling update has some limitations. Your application needs to be able to handle such a process, you need to think about database access, and it's a very on/off process. Therefore, you don't have any control over when or how gradually traffic is switched to the new version.\\xA0\"}),`\n`,(0,n.jsxs)(e.h4,{id:\"blue-green-deployments\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#blue-green-deployments\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Blue-Green Deployments\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Blue-green deployments are next-level deployments that try to answer the limitations of simple rolling updates. In this model, you always keep two deployments (or two clones of the whole infrastructure). One is called blue and one is called green. At any given time, only one is active and serving traffic, while the other one will be idle. And once you need to release an update, you do that on the idle side, test if everything works, and then switch the traffic.\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:\"This model is better than a simple rolling update because you have control over switching traffic, and you can have the new version running for a few minutes or even hours so that you can do testing to make sure you won't have any surprises once live traffic hits it.\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:\"However, while better than rolling updates, blue/green deployments also have their limitations. The most important is that you're limited to two environments: blue and green. In many cases, that's enough, but there are use cases where two environments would be a limiting factor, for, example, if you have long-running tasks such as database migrations or AI processing.\\xA0\"}),`\n`,(0,n.jsxs)(e.h4,{id:\"when-blue-green-is-not-enough\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#when-blue-green-is-not-enough\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"When Blue-Green is not Enough\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"Imagine a situation where you deploy a new version of your long-running software to your blue environment, you test if it's okay, and you make it your \",(0,n.jsx)(e.a,{href:\"https://release.com/blog/setup-test-environment\",children:\"live environment\"}),\". Then you do the same again for the green environment\\u2014you deploy a new version there and switch again from blue to green.\\xA0\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Now, if you'd like to deploy a new version again, you'd have to do it in the blue environment. But blue could still be working on that long-running task. You can't simply stop a database migration in the middle because you'll end up with a corrupted database. So you'll have to wait until the software on the blue environment is finished before you can make another deployment. And that's where rainbow deployments come into play.\\xA0\"}),`\n`,(0,n.jsxs)(e.h4,{id:\"what-is-a-rainbow-deployment\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#what-is-a-rainbow-deployment\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"What is a Rainbow Deployment?\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Rainbow deployment is the next level of deployment methods that solves the limitation of blue-green deployments. In fact, rainbow deployments are very similar to blue/green deployments, but they're not limited to only two (blue and green) environments. You can have as many colorful environments as you like\\u2014thus the name.\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:\"At Release we use Kubernetes namespaces along with our deployment system to automate the creation and removal of rainbow deployments for your application. Release will automatically create and manage a new namespace for each deployment.\"}),`\n`,(0,n.jsx)(e.p,{children:\"The working principle of rainbow deployment is the same as blue/green deployments, but you can operate on more copies of your application than just two. So, let's take our example from before, in which we would have to wait for the blue environment to finish the long-running task before making a third deployment. With rainbow deployments, you can just add another environment, let's call it yellow.\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/42fec8e28837bbdd43dfb7c6fc243336.jpeg\",alt:\"\"})}),`\n`,(0,n.jsx)(e.p,{children:\"Now we have three environments: blue, green, and yellow. Our blue is busy, and green is currently serving traffic. If we want to deploy a new version of our application, we can deploy it to yellow and then switch traffic to it from green. And that's how rainbow deployment works.\\xA0\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"This is a very powerful method of deploying applications because you can avoid downtime as much as possible for as many users as possible. Long-running tasks blocking your deployments provide just one example, but there are more use cases. For example, if your application uses \",(0,n.jsx)(e.a,{href:\"https://en.wikipedia.org/wiki/WebSocket\",children:\"WebSockets\"}),\", no matter how fast and downtime-free your deployments are, you'll still have to disconnect users from their WebSockets sessions, so they'll potentially lose some notifications or other data from your app. Rainbow deployments are the solution: You deploy a new version of your application, and you keep the old one until all users have disconnected from WebSockets sessions. Then you kill the old version of the application.\\xA0\"]}),`\n`,(0,n.jsxs)(e.h4,{id:\"how-to-do-a-rainbow-deployment\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#how-to-do-a-rainbow-deployment\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"How to do a Rainbow Deployment\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Now that you know what rainbow deployments are, let's see how you actually implement them. There is no single standard way of achieving rainbow deployments and there aren't any tools you can install to do rainbow deployments for you\\u2014it's more of a do-it-yourself approach. But it isn't all bad news: you can leverage the tools you have to enable rainbow deployments with just a few extra lines of logic.\\xA0\"}),`\n`,(0,n.jsxs)(e.p,{children:[`So, how do you do it,? You use your current CI/CD pipelines. All you need to do is to point whatever network device you're using to a specific \"color\" of the application when you deploy. In the case of Kubernetes, this could mean changing the `,(0,n.jsx)(e.strong,{children:\"Service\"}),\" or \",(0,n.jsx)(e.strong,{children:\"Ingress\"}),\" objects to point to a different deployment.\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Below are some very simple and typical Kubernetes deployment and service definitions:\\xA0\"}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-yaml\",children:`apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  \\xA0name: nginx-deployment\n  \\xA0labels:\n  \\xA0 \\xA0app: nginx\nspec:\n  \\xA0replicas: 1\n  \\xA0selector:\n  \\xA0 \\xA0matchLabels:\n  \\xA0 \\xA0 \\xA0app: nginx\n  \\xA0template:\n  \\xA0 \\xA0metadata:\n  \\xA0 \\xA0 \\xA0labels:\n  \\xA0 \\xA0 \\xA0 \\xA0app: nginx\n  \\xA0 \\xA0spec:\n  \\xA0 \\xA0 \\xA0containers:\n  \\xA0 \\xA0 \\xA0- name: nginx\n  \\xA0 \\xA0 \\xA0 \\xA0image: your_application:0.1\n  \\xA0 \\xA0 \\xA0 \\xA0ports:\n  \\xA0 \\xA0 \\xA0 \\xA0- containerPort: 80\n---\napiVersion: v1\nkind: Service\nmetadata:\n  \\xA0name: nginxservice\nspec:\n  \\xA0selector:\n  \\xA0 \\xA0app: nginx\n  \\xA0ports:\n  \\xA0 \\xA0- protocol: TCP\n  \\xA0 \\xA0 \\xA0port: 80\n  \\xA0 \\xA0 \\xA0name: nginxs\n  \\xA0 \\xA0 \\xA0targetPort: 80\n`})}),`\n`,(0,n.jsxs)(e.p,{children:[\"We have one deployment and one service that points to that deployment. The service knows which deployment to target based on deployment labels. The service is instructed to search for deployment that has a label \",(0,n.jsx)(e.strong,{children:\"app\"}),\" with a value of \",(0,n.jsx)(e.strong,{children:\"nginx.\"}),\" But what if we target the deployment by color too? Well, that would be a rainbow deployment strategy.\\xA0\"]}),`\n`,(0,n.jsxs)(e.h4,{id:\"enter-rainbow-magic\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#enter-rainbow-magic\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Enter Rainbow Magic\"]}),`\n`,(0,n.jsx)(e.p,{children:\"So, your definition would look something like this:\\xA0\"}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-yaml\",children:`apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  \\xA0name: nginx-deployment-[color]\n  \\xA0labels:\n  \\xA0 \\xA0app: nginx\nspec:\n  \\xA0replicas: 1\n  \\xA0selector:\n  \\xA0 \\xA0matchLabels:\n  \\xA0 \\xA0 \\xA0app: nginx\n  \\xA0template:\n  \\xA0 \\xA0metadata:\n  \\xA0 \\xA0 \\xA0labels:\n  \\xA0 \\xA0 \\xA0 \\xA0app: nginx\n  \\xA0 \\xA0 \\xA0 \\xA0color: [color]\n  \\xA0 \\xA0spec:\n  \\xA0 \\xA0 \\xA0containers:\n  \\xA0 \\xA0 \\xA0- name: nginx\n  \\xA0 \\xA0 \\xA0 \\xA0image: your_application:0.2\n  \\xA0 \\xA0 \\xA0 \\xA0ports:\n  \\xA0 \\xA0 \\xA0 \\xA0- containerPort: 80\n---\napiVersion: v1\nkind: Service\nmetadata:\n  \\xA0name: nginxservice\nspec:\n  \\xA0selector:\n  \\xA0 \\xA0app: nginx\n  \\xA0 \\xA0environment: [color]\n  \\xA0ports:\n  \\xA0 \\xA0- protocol: TCP\n  \\xA0 \\xA0 \\xA0port: 80\n  \\xA0 \\xA0 \\xA0name: nginxs\n  \\xA0 \\xA0 \\xA0targetPort: 80\n`})}),`\n`,(0,n.jsxs)(e.p,{children:[\"And it would be in your CI/CD job to replace \",(0,n.jsx)(e.strong,{children:\"[color]\"}),\" in the YAML definition every time you want to deploy a new version. So you deploy your application and service for it, then the next time you want to deploy a new version of that application, instead of updating the existing deployment, you create a new deployment and update the existing service to point to that new deployment. And you can repeat that process as many times as you want. Once the old deployments aren't needed anymore, you can delete them. This is the working principle of rainbow deployments.\\xA0\"]}),`\n`,(0,n.jsx)(e.p,{children:\"It's also worth mentioning that you don't need to use colors to distinguish your deployments\\u2014you can use anything. A common example is to use a Git commit hash. Also, this method isn't exclusive to Kubernetes. You can use it in pretty much any infrastructure or environment as long as you have a way to identify a specific deployment and point your network traffic to.\\xA0\"}),`\n`,(0,n.jsxs)(e.h4,{id:\"rainbow-deployment-summary\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#rainbow-deployment-summary\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Rainbow Deployment Summary\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Rainbow deployments solve a lot of problems that come with common deployment methods and they bring true benefits to your users. However, rainbow deployments are not a magic solution that will solve all your application problems. Your infrastructure and your application need to be compatible with this approach. Database handling may be especially tricky (for example, you don't want to have two applications writing to the same record in the same database). But these are typical problems that you need to solve anyway when dealing with distributed systems.\\xA0\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"Once you improve the user experience, you can also think about improving your developer productivity. If you want to learn more, take a look at our \",(0,n.jsx)(e.a,{href:\"https://docs.releasehub.com/reference-documentation/workflows-in-release/rainbow-deployments\",children:\"documentation here.\"})]}),`\n`,(0,n.jsx)(e.p,{children:\"\\u200D\"})]})}function k(o={}){let{wrapper:e}=o.components||{};return e?(0,n.jsx)(e,Object.assign({},o,{children:(0,n.jsx)(h,o)})):h(o)}var x=k;return b(N);})();\n;return Component;"
        },
        "_id": "blog/posts/rainbow-deployment-why-and-how-to-do-it.mdx",
        "_raw": {
          "sourceFilePath": "blog/posts/rainbow-deployment-why-and-how-to-do-it.mdx",
          "sourceFileName": "rainbow-deployment-why-and-how-to-do-it.mdx",
          "sourceFileDir": "blog/posts",
          "contentType": "mdx",
          "flattenedPath": "blog/posts/rainbow-deployment-why-and-how-to-do-it"
        },
        "type": "BlogPost",
        "computedSlug": "rainbow-deployment-why-and-how-to-do-it"
      },
      "documentHash": "1739393595025",
      "hasWarnings": false,
      "documentTypeName": "BlogPost"
    },
    "blog/posts/release-ai-now-supports-nvidia-nim-accelerating-ai-development-and-deployment.mdx": {
      "document": {
        "title": "Release.ai Now Supports NVIDIA NIM: Accelerating AI Development and Deployment",
        "summary": "Start using NVIDIA NIM and NVIDIA NeMo framework with a simplified setup process",
        "publishDate": "Wed Aug 07 2024 04:26:37 GMT+0000 (Coordinated Universal Time)",
        "author": "tommy-mcclung",
        "readingTime": 4,
        "categories": [
          "ai",
          "nvidia",
          "platform-engineering"
        ],
        "mainImage": "/blog-images/eee56b5123344ee0de6be55503ca89a6.jpg",
        "imageAlt": "Release.ai Now Supports NVIDIA NIM: Accelerating AI Development and Deployment",
        "showCTA": true,
        "ctaCopy": "Simplify NVIDIA NIM deployment with Release: effortless AI environment management for accelerated development workflows.",
        "ctaLink": "https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=release-ai-now-supports-nvidia-nim-accelerating-ai-development-and-deployment",
        "relatedPosts": [
          ""
        ],
        "ogImage": "/blog-images/eee56b5123344ee0de6be55503ca89a6.jpg",
        "excerpt": "Start using NVIDIA NIM and NVIDIA NeMo framework with a simplified setup process",
        "tags": [
          "ai",
          "nvidia",
          "platform-engineering"
        ],
        "ctaButton": "Try Release for Free",
        "body": {
          "raw": "\nRelease.ai is an orchestration and infrastructure management platform for custom AI applications. We handle the complex backend work, allowing developers and data scientists to focus on innovation. Our self-service platform provides powerful building blocks that simplify AI development and deployment, enabling teams to create advanced applications more efficiently.\n\nWe're excited to announce that Release.ai now supports NVIDIA NIM (NVIDIA Inference Microservices), further enhancing our AI development capabilities. As a member of the NVIDIA Inception program, we're bringing powerful, easy-to-deploy AI functionalities to developers while minimizing the complexities of AI infrastructure management.\n\n‍\n**Simplifying AI Development with NIM**\n\nNVIDIA NIM offers a set of easy-to-use inference microservices that streamline the deployment of foundation models. By integrating NIM into our platform, Release.ai is making it simpler than ever for teams to leverage NVIDIA's AI technologies in their projects.\n\n‍\n**Out-of-the-box templates for NVIDIA GPUs**\n\nRelease.ai now offers pre-configured templates optimized for NVIDIA GPUs and the NIM framework. These templates dramatically reduce setup time and learning curves, allowing developers to skip the tedious configuration process and jump straight into AI development. With our ever-growing library of the latest models and frameworks, you can start using NVIDIA NIM without navigating a complex setup process. With a ready-to-run NIM framework (along with the NeMo framework included in Release.ai), developers can quickly spin up an environment with the essential AI tools, build and deploy with confidence.\n\n‍\n\n‍**Effortless NIM Deployment and Management**\n\nWith Release.ai, deploying and managing NIM is streamlined:\n\n1\\. Select a NIM-ready template\n\n2\\. Launch your AI environment with a few clicks\n\n3\\. Let Release.ai handle the ongoing management of your NIM infrastructure\n\nOur platform takes care of provisioning resources based on your team and application needs, and even auto-scales resources as your application demands change.\n\n‍\n**AI Lifecycle Workflows**\n\nRelease.ai goes beyond just spinning up NIM environments. Our platform provides tools to develop, debug, and manage the entire lifecycle of your AI applications. You can create custom workflows to streamline your development process, easily import AI apps into ephemeral environments, and scale efficiently with proper governance in place.\n\n‍\n\n‍**Maximizing ROI on AI Infrastructure**\n\nBy leveraging Release.ai for your NVIDIA NIM deployments, you can:\n\n\\- **Save Money:** Optimize your compute spend with intelligent resource management\n\n\\- **Save Time:** Minimize setup effort and focus on building AI applications\n\n\\- **Scale Efficiently:** Grow your AI initiatives with automated, governed processes\n\n\\- **Keep Data Private:** Maintain full control over your data, models, and infrastructure\n\n‍\n**The Power of the NVIDIA Inception Program**\n\nAs a member of the NVIDIA Inception program, Release.ai is at the forefront of AI innovation. This partnership allows us to work closely with NVIDIA, ensuring we deliver the right capabilities to teams using NVIDIA GPUs and their data science and AI/ML stacks.\n\n‍\n**What's Next: Community Templates and Beyond**\n\nStay tuned for our upcoming community templates, which will allow developers to share and use custom NIM configurations. This feature will further accelerate AI development by leveraging collective expertise.\n\n‍\n**Getting Started**\n\nWhether you're building large language models, working on computer vision projects, or exploring other AI applications, Release.ai with NVIDIA NIM support provides the easiest way for teams to accelerate their AI development journey.\n\n‍\n\nReady to supercharge your AI workflows? Here's how to get started with Release.ai and NVIDIA NIM:\n\n‍\n\n1\\. Sign up for a free Release.ai account at release.ai/sign-up\n\n2\\. Choose a NIM-ready template from our library when creating your application\n\n3\\. Launch your AI environment and start building\n\n‍\n\nFor a limited time, we're offering a sandbox with our premium features. Don't miss this opportunity to transform your AI development process. [Get started with Release.ai and NVIDIA NIM today!](http://release.ai/sign-up)\n",
          "code": "var Component=(()=>{var d=Object.create;var a=Object.defineProperty;var m=Object.getOwnPropertyDescriptor;var u=Object.getOwnPropertyNames;var h=Object.getPrototypeOf,g=Object.prototype.hasOwnProperty;var I=(t,e)=>()=>(e||t((e={exports:{}}).exports,e),e.exports),f=(t,e)=>{for(var i in e)a(t,i,{get:e[i],enumerable:!0})},s=(t,e,i,o)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let r of u(e))!g.call(t,r)&&r!==i&&a(t,r,{get:()=>e[r],enumerable:!(o=m(e,r))||o.enumerable});return t};var y=(t,e,i)=>(i=t!=null?d(h(t)):{},s(e||!t||!t.__esModule?a(i,\"default\",{value:t,enumerable:!0}):i,t)),w=t=>s(a({},\"__esModule\",{value:!0}),t);var c=I((b,l)=>{l.exports=_jsx_runtime});var M={};f(M,{default:()=>N,frontmatter:()=>A});var n=y(c()),A={title:\"Release.ai Now Supports NVIDIA NIM: Accelerating AI Development and Deployment\",summary:\"Start using NVIDIA NIM and NVIDIA NeMo framework with a simplified setup process\",publishDate:\"Wed Aug 07 2024 04:26:37 GMT+0000 (Coordinated Universal Time)\",author:\"tommy-mcclung\",readingTime:4,categories:[\"ai\",\"nvidia\",\"platform-engineering\"],mainImage:\"/blog-images/eee56b5123344ee0de6be55503ca89a6.jpg\",imageAlt:\"Release.ai Now Supports NVIDIA NIM: Accelerating AI Development and Deployment\",showCTA:!0,ctaCopy:\"Simplify NVIDIA NIM deployment with Release: effortless AI environment management for accelerated development workflows.\",ctaLink:\"https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=release-ai-now-supports-nvidia-nim-accelerating-ai-development-and-deployment\",relatedPosts:[\"\"],ogImage:\"/blog-images/eee56b5123344ee0de6be55503ca89a6.jpg\",excerpt:\"Start using NVIDIA NIM and NVIDIA NeMo framework with a simplified setup process\",tags:[\"ai\",\"nvidia\",\"platform-engineering\"],ctaButton:\"Try Release for Free\"};function p(t){let e=Object.assign({p:\"p\",strong:\"strong\",a:\"a\"},t.components);return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(e.p,{children:\"Release.ai is an orchestration and infrastructure management platform for custom AI applications. We handle the complex backend work, allowing developers and data scientists to focus on innovation. Our self-service platform provides powerful building blocks that simplify AI development and deployment, enabling teams to create advanced applications more efficiently.\"}),`\n`,(0,n.jsx)(e.p,{children:\"We're excited to announce that Release.ai now supports NVIDIA NIM (NVIDIA Inference Microservices), further enhancing our AI development capabilities. As a member of the NVIDIA Inception program, we're bringing powerful, easy-to-deploy AI functionalities to developers while minimizing the complexities of AI infrastructure management.\"}),`\n`,(0,n.jsxs)(e.p,{children:[`\\u200D\n`,(0,n.jsx)(e.strong,{children:\"Simplifying AI Development with NIM\"})]}),`\n`,(0,n.jsx)(e.p,{children:\"NVIDIA NIM offers a set of easy-to-use inference microservices that streamline the deployment of foundation models. By integrating NIM into our platform, Release.ai is making it simpler than ever for teams to leverage NVIDIA's AI technologies in their projects.\"}),`\n`,(0,n.jsxs)(e.p,{children:[`\\u200D\n`,(0,n.jsx)(e.strong,{children:\"Out-of-the-box templates for NVIDIA GPUs\"})]}),`\n`,(0,n.jsx)(e.p,{children:\"Release.ai now offers pre-configured templates optimized for NVIDIA GPUs and the NIM framework. These templates dramatically reduce setup time and learning curves, allowing developers to skip the tedious configuration process and jump straight into AI development. With our ever-growing library of the latest models and frameworks, you can start using NVIDIA NIM without navigating a complex setup process. With a ready-to-run NIM framework (along with the NeMo framework included in Release.ai), developers can quickly spin up an environment with the essential AI tools, build and deploy with confidence.\"}),`\n`,(0,n.jsx)(e.p,{children:\"\\u200D\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"\\u200D\",(0,n.jsx)(e.strong,{children:\"Effortless NIM Deployment and Management\"})]}),`\n`,(0,n.jsx)(e.p,{children:\"With Release.ai, deploying and managing NIM is streamlined:\"}),`\n`,(0,n.jsx)(e.p,{children:\"1. Select a NIM-ready template\"}),`\n`,(0,n.jsx)(e.p,{children:\"2. Launch your AI environment with a few clicks\"}),`\n`,(0,n.jsx)(e.p,{children:\"3. Let Release.ai handle the ongoing management of your NIM infrastructure\"}),`\n`,(0,n.jsx)(e.p,{children:\"Our platform takes care of provisioning resources based on your team and application needs, and even auto-scales resources as your application demands change.\"}),`\n`,(0,n.jsxs)(e.p,{children:[`\\u200D\n`,(0,n.jsx)(e.strong,{children:\"AI Lifecycle Workflows\"})]}),`\n`,(0,n.jsx)(e.p,{children:\"Release.ai goes beyond just spinning up NIM environments. Our platform provides tools to develop, debug, and manage the entire lifecycle of your AI applications. You can create custom workflows to streamline your development process, easily import AI apps into ephemeral environments, and scale efficiently with proper governance in place.\"}),`\n`,(0,n.jsx)(e.p,{children:\"\\u200D\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"\\u200D\",(0,n.jsx)(e.strong,{children:\"Maximizing ROI on AI Infrastructure\"})]}),`\n`,(0,n.jsx)(e.p,{children:\"By leveraging Release.ai for your NVIDIA NIM deployments, you can:\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"- \",(0,n.jsx)(e.strong,{children:\"Save Money:\"}),\" Optimize your compute spend with intelligent resource management\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"- \",(0,n.jsx)(e.strong,{children:\"Save Time:\"}),\" Minimize setup effort and focus on building AI applications\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"- \",(0,n.jsx)(e.strong,{children:\"Scale Efficiently:\"}),\" Grow your AI initiatives with automated, governed processes\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"- \",(0,n.jsx)(e.strong,{children:\"Keep Data Private:\"}),\" Maintain full control over your data, models, and infrastructure\"]}),`\n`,(0,n.jsxs)(e.p,{children:[`\\u200D\n`,(0,n.jsx)(e.strong,{children:\"The Power of the NVIDIA Inception Program\"})]}),`\n`,(0,n.jsx)(e.p,{children:\"As a member of the NVIDIA Inception program, Release.ai is at the forefront of AI innovation. This partnership allows us to work closely with NVIDIA, ensuring we deliver the right capabilities to teams using NVIDIA GPUs and their data science and AI/ML stacks.\"}),`\n`,(0,n.jsxs)(e.p,{children:[`\\u200D\n`,(0,n.jsx)(e.strong,{children:\"What's Next: Community Templates and Beyond\"})]}),`\n`,(0,n.jsx)(e.p,{children:\"Stay tuned for our upcoming community templates, which will allow developers to share and use custom NIM configurations. This feature will further accelerate AI development by leveraging collective expertise.\"}),`\n`,(0,n.jsxs)(e.p,{children:[`\\u200D\n`,(0,n.jsx)(e.strong,{children:\"Getting Started\"})]}),`\n`,(0,n.jsx)(e.p,{children:\"Whether you're building large language models, working on computer vision projects, or exploring other AI applications, Release.ai with NVIDIA NIM support provides the easiest way for teams to accelerate their AI development journey.\"}),`\n`,(0,n.jsx)(e.p,{children:\"\\u200D\"}),`\n`,(0,n.jsx)(e.p,{children:\"Ready to supercharge your AI workflows? Here's how to get started with Release.ai and NVIDIA NIM:\"}),`\n`,(0,n.jsx)(e.p,{children:\"\\u200D\"}),`\n`,(0,n.jsx)(e.p,{children:\"1. Sign up for a free Release.ai account at release.ai/sign-up\"}),`\n`,(0,n.jsx)(e.p,{children:\"2. Choose a NIM-ready template from our library when creating your application\"}),`\n`,(0,n.jsx)(e.p,{children:\"3. Launch your AI environment and start building\"}),`\n`,(0,n.jsx)(e.p,{children:\"\\u200D\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"For a limited time, we're offering a sandbox with our premium features. Don't miss this opportunity to transform your AI development process. \",(0,n.jsx)(e.a,{href:\"http://release.ai/sign-up\",children:\"Get started with Release.ai and NVIDIA NIM today!\"})]})]})}function v(t={}){let{wrapper:e}=t.components||{};return e?(0,n.jsx)(e,Object.assign({},t,{children:(0,n.jsx)(p,t)})):p(t)}var N=v;return w(M);})();\n;return Component;"
        },
        "_id": "blog/posts/release-ai-now-supports-nvidia-nim-accelerating-ai-development-and-deployment.mdx",
        "_raw": {
          "sourceFilePath": "blog/posts/release-ai-now-supports-nvidia-nim-accelerating-ai-development-and-deployment.mdx",
          "sourceFileName": "release-ai-now-supports-nvidia-nim-accelerating-ai-development-and-deployment.mdx",
          "sourceFileDir": "blog/posts",
          "contentType": "mdx",
          "flattenedPath": "blog/posts/release-ai-now-supports-nvidia-nim-accelerating-ai-development-and-deployment"
        },
        "type": "BlogPost",
        "computedSlug": "release-ai-now-supports-nvidia-nim-accelerating-ai-development-and-deployment"
      },
      "documentHash": "1739393595025",
      "hasWarnings": false,
      "documentTypeName": "BlogPost"
    },
    "blog/posts/release-delivery-helps-saas-companies-meet-the-needs-of-their-enterprise-customers.mdx": {
      "document": {
        "title": "Release Delivery helps SaaS companies meet the needs of their enterprise customers",
        "summary": "Introducing Release Delivery, our latest addition to the Release environments as a service platform.",
        "publishDate": "Mon Mar 27 2023 17:10:13 GMT+0000 (Coordinated Universal Time)",
        "author": "tommy-mcclung",
        "readingTime": 6,
        "categories": [
          "platform-engineering",
          "product"
        ],
        "mainImage": "/blog-images/996d4f265f0efa066b4aa6f1d1a92715.png",
        "imageAlt": "Release Delivery helps SaaS companies meet the needs of their enterprise customers",
        "showCTA": true,
        "ctaCopy": "Empower SaaS teams to effortlessly deploy single-tenant instances with Release's environment management platform.",
        "ctaLink": "https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=release-delivery-helps-saas-companies-meet-the-needs-of-their-enterprise-customers",
        "relatedPosts": [
          ""
        ],
        "ogImage": "/blog-images/996d4f265f0efa066b4aa6f1d1a92715.png",
        "excerpt": "Introducing Release Delivery, our latest addition to the Release environments as a service platform.",
        "tags": [
          "platform-engineering",
          "product"
        ],
        "ctaButton": "Try Release for Free",
        "body": {
          "raw": "\nToday, I'm excited to announce the general availability of [Release Delivery](https://release.com/product/release-delivery), our latest addition to the Release environments as a service platform.\n\nWorking with numerous cloud-native SaaS providers we saw the common struggle: **scaling the deployment options to meet the hosting demands of the enterprise customers.** These companies would lead with a multi-tenant offering to optimize resource utilization, deliver faster and more predictable updates, and provide overall consistent performance. This approach would create great software, however, many enterprise customers would see the multi-tenant aspect as potentially commingled data, elevated IT risk profile, and longer approval/whitelisting processes. In the end, enterprises will opt for a safer, albeit not as good, single-tenant version from a competitor.\n\nBuilding out a single-tenant version of a SaaS product is a significant investment, and it presents a **tough choice for growing software vendors**. Commit resources to build infrastructure instead of innovation, or miss out on the chance to crack large customer deals.\n\nWe spoke to many SaaS companies who face this conundrum. Do I proactively create the ability to deliver my SaaS application as a single-tenant/self-hosted instance? Or do I wait until a large customer asks for it and then scramble to build it? What if I build it and they decide not to buy it? Did I just waste valuable time building something no one will want?\n**Enter Release Delivery:** the fast way for SaaS companies to **deploy into a variety of environments** required by many large organizations without having to build the functionality themselves.\n\n### How Release Delivery Works\n\n![](/blog-images/996d4f265f0efa066b4aa6f1d1a92715.png)\n\nRelease Delivery templatizes your application deployment and allows you to \"stamp out copies\" custom-tuned to each unique customer environment, turning your SaaS product into an Enterprise-grade SaaS product.\n\nWith Release Delivery we set out to address the key pain points our customers were facing, and here is what we did:\n\n| Problem                                                                                                                                               | Solution                                                                                                                                                                                                                                                                                                                                                                |\n| ----------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| Modifying your application to run in each disparate single-tenant environment often requires numerous choices and unknowns that make the task a chore | Release Delivery uses an application template that codifies the necessary design choices and patterns. We saw hundreds of customers design their applications to be reproduced at a click of a button, with a confidence that the decisions they made to adapt the application for a single-tenant/self-hosted deployment will work and be easy to manage in production |\n| Installing single-tenant applications into customer cloud accounts is tedious and difficult                                                           | Release Delivery simplifies the installation process for your customers. Simply send your customer the installation URL and Release will walk them through every step to get your application running in their cloud account                                                                                                                                            |\n| Managing and debugging numerous installations of a single-tenant application becomes untenable                                                        | For troubleshooting and performance monitoring, Release Delivery provides logging and console access to running applications in your customers' cloud accounts. And you can always add other APM tools such as DataDog to your deployed applications                                                                                                                    |\n| Performing timely updates on all single-tenant applications in customer cloud accounts is difficult and slow                                          | Release Delivery provides a simple mechanism to deploy an upgraded version of your application to one or many deployed single-tenant/self-hosted applications at once. You can also share a \"preview\" version of your app with the customers before going live                                                                                                          |\n| Handling unique customer security requirements for the single-tenant/self-hosted versions of your SaaS app is a challenge                             | Security is top of mind with Release Delivery. In addition to the product being highly secure, including the ability to \"cut the cord\" between Release and your deployments (air-gapped), Release provides in-depth documentation on all necessary security and privacy controls you can provide to your customers during your sales process                            |\n\nWe believe our core Release Environments as a Service product is the best solution for reproducing and maintaining complex applications. So the ability to deploy an application into a single-tenant or self-hosted environment is a natural extension of our core Release product line.\n\n### Unlocking new opportunities\n\nRelease Delivery comes to you after a year-long close collaboration with our select customers, who saw an opportunity to **take their SaaS products to the next level**. Thanks to their generous input and feedback, we navigated the peculiarities of the \"on-cloud\"/self-hosted versioning, unlocked access to enterprise requirements and helped them grow large account sales. With Release Delivery these SaaS providers now give their customers an amazing installation experience, wherever and however they want it. With only a few clicks their applications are deployed and running. Our customers can easily monitor and manage these deployments, and quickly upgrade the instances to keep _their_ customers on the latest app version, **without a heavy lift.**\n\nI am extremely excited about making Release Delivery available to all. If you're a startup or a SaaS company that wants to generate more revenue from an \"on-cloud\"/self-hosted version of your application, talk to us! To learn more, [join my webinar](https://release.com/webinar/release-delivery) on April 20th, where I will show how Release Delivery works and share how it helps companies solve the problems around flexible deployment. Or, you [book a meeting](https://release.com/vpc-book-a-meeting) with our team to see a custom demo and get a trial account.\n",
          "code": "var Component=(()=>{var h=Object.create;var i=Object.defineProperty;var u=Object.getOwnPropertyDescriptor;var p=Object.getOwnPropertyNames;var m=Object.getPrototypeOf,g=Object.prototype.hasOwnProperty;var y=(n,e)=>()=>(e||n((e={exports:{}}).exports,e),e.exports),f=(n,e)=>{for(var a in e)i(n,a,{get:e[a],enumerable:!0})},s=(n,e,a,r)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let o of p(e))!g.call(n,o)&&o!==a&&i(n,o,{get:()=>e[o],enumerable:!(r=u(e,o))||r.enumerable});return n};var v=(n,e,a)=>(a=n!=null?h(m(n)):{},s(e||!n||!n.__esModule?i(a,\"default\",{value:n,enumerable:!0}):a,n)),w=n=>s(i({},\"__esModule\",{value:!0}),n);var c=y((x,l)=>{l.exports=_jsx_runtime});var R={};f(R,{default:()=>S,frontmatter:()=>b});var t=v(c()),b={title:\"Release Delivery helps SaaS companies meet the needs of their enterprise customers\",summary:\"Introducing Release Delivery, our latest addition to the Release environments as a service platform.\",publishDate:\"Mon Mar 27 2023 17:10:13 GMT+0000 (Coordinated Universal Time)\",author:\"tommy-mcclung\",readingTime:6,categories:[\"platform-engineering\",\"product\"],mainImage:\"/blog-images/996d4f265f0efa066b4aa6f1d1a92715.png\",imageAlt:\"Release Delivery helps SaaS companies meet the needs of their enterprise customers\",showCTA:!0,ctaCopy:\"Empower SaaS teams to effortlessly deploy single-tenant instances with Release's environment management platform.\",ctaLink:\"https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=release-delivery-helps-saas-companies-meet-the-needs-of-their-enterprise-customers\",relatedPosts:[\"\"],ogImage:\"/blog-images/996d4f265f0efa066b4aa6f1d1a92715.png\",excerpt:\"Introducing Release Delivery, our latest addition to the Release environments as a service platform.\",tags:[\"platform-engineering\",\"product\"],ctaButton:\"Try Release for Free\"};function d(n){let e=Object.assign({p:\"p\",a:\"a\",strong:\"strong\",h3:\"h3\",span:\"span\",img:\"img\",table:\"table\",thead:\"thead\",tr:\"tr\",th:\"th\",tbody:\"tbody\",td:\"td\",em:\"em\"},n.components);return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsxs)(e.p,{children:[\"Today, I'm excited to announce the general availability of \",(0,t.jsx)(e.a,{href:\"https://release.com/product/release-delivery\",children:\"Release Delivery\"}),\", our latest addition to the Release environments as a service platform.\"]}),`\n`,(0,t.jsxs)(e.p,{children:[\"Working with numerous cloud-native SaaS providers we saw the common struggle: \",(0,t.jsx)(e.strong,{children:\"scaling the deployment options to meet the hosting demands of the enterprise customers.\"}),\" These companies would lead with a multi-tenant offering to optimize resource utilization, deliver faster and more predictable updates, and provide overall consistent performance. This approach would create great software, however, many enterprise customers would see the multi-tenant aspect as potentially commingled data, elevated IT risk profile, and longer approval/whitelisting processes. In the end, enterprises will opt for a safer, albeit not as good, single-tenant version from a competitor.\"]}),`\n`,(0,t.jsxs)(e.p,{children:[\"Building out a single-tenant version of a SaaS product is a significant investment, and it presents a \",(0,t.jsx)(e.strong,{children:\"tough choice for growing software vendors\"}),\". Commit resources to build infrastructure instead of innovation, or miss out on the chance to crack large customer deals.\"]}),`\n`,(0,t.jsxs)(e.p,{children:[`We spoke to many SaaS companies who face this conundrum. Do I proactively create the ability to deliver my SaaS application as a single-tenant/self-hosted instance? Or do I wait until a large customer asks for it and then scramble to build it? What if I build it and they decide not to buy it? Did I just waste valuable time building something no one will want?\n`,(0,t.jsx)(e.strong,{children:\"Enter Release Delivery:\"}),\" the fast way for SaaS companies to \",(0,t.jsx)(e.strong,{children:\"deploy into a variety of environments\"}),\" required by many large organizations without having to build the functionality themselves.\"]}),`\n`,(0,t.jsxs)(e.h3,{id:\"how-release-delivery-works\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#how-release-delivery-works\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"How Release Delivery Works\"]}),`\n`,(0,t.jsx)(e.p,{children:(0,t.jsx)(e.img,{src:\"/blog-images/996d4f265f0efa066b4aa6f1d1a92715.png\",alt:\"\"})}),`\n`,(0,t.jsx)(e.p,{children:'Release Delivery templatizes your application deployment and allows you to \"stamp out copies\" custom-tuned to each unique customer environment, turning your SaaS product into an Enterprise-grade SaaS product.'}),`\n`,(0,t.jsx)(e.p,{children:\"With Release Delivery we set out to address the key pain points our customers were facing, and here is what we did:\"}),`\n`,(0,t.jsxs)(e.table,{children:[(0,t.jsx)(e.thead,{children:(0,t.jsxs)(e.tr,{children:[(0,t.jsx)(e.th,{children:\"Problem\"}),(0,t.jsx)(e.th,{children:\"Solution\"})]})}),(0,t.jsxs)(e.tbody,{children:[(0,t.jsxs)(e.tr,{children:[(0,t.jsx)(e.td,{children:\"Modifying your application to run in each disparate single-tenant environment often requires numerous choices and unknowns that make the task a chore\"}),(0,t.jsx)(e.td,{children:\"Release Delivery uses an application template that codifies the necessary design choices and patterns. We saw hundreds of customers design their applications to be reproduced at a click of a button, with a confidence that the decisions they made to adapt the application for a single-tenant/self-hosted deployment will work and be easy to manage in production\"})]}),(0,t.jsxs)(e.tr,{children:[(0,t.jsx)(e.td,{children:\"Installing single-tenant applications into customer cloud accounts is tedious and difficult\"}),(0,t.jsx)(e.td,{children:\"Release Delivery simplifies the installation process for your customers. Simply send your customer the installation URL and Release will walk them through every step to get your application running in their cloud account\"})]}),(0,t.jsxs)(e.tr,{children:[(0,t.jsx)(e.td,{children:\"Managing and debugging numerous installations of a single-tenant application becomes untenable\"}),(0,t.jsx)(e.td,{children:\"For troubleshooting and performance monitoring, Release Delivery provides logging and console access to running applications in your customers' cloud accounts. And you can always add other APM tools such as DataDog to your deployed applications\"})]}),(0,t.jsxs)(e.tr,{children:[(0,t.jsx)(e.td,{children:\"Performing timely updates on all single-tenant applications in customer cloud accounts is difficult and slow\"}),(0,t.jsx)(e.td,{children:'Release Delivery provides a simple mechanism to deploy an upgraded version of your application to one or many deployed single-tenant/self-hosted applications at once. You can also share a \"preview\" version of your app with the customers before going live'})]}),(0,t.jsxs)(e.tr,{children:[(0,t.jsx)(e.td,{children:\"Handling unique customer security requirements for the single-tenant/self-hosted versions of your SaaS app is a challenge\"}),(0,t.jsx)(e.td,{children:'Security is top of mind with Release Delivery. In addition to the product being highly secure, including the ability to \"cut the cord\" between Release and your deployments (air-gapped), Release provides in-depth documentation on all necessary security and privacy controls you can provide to your customers during your sales process'})]})]})]}),`\n`,(0,t.jsx)(e.p,{children:\"We believe our core Release Environments as a Service product is the best solution for reproducing and maintaining complex applications. So the ability to deploy an application into a single-tenant or self-hosted environment is a natural extension of our core Release product line.\"}),`\n`,(0,t.jsxs)(e.h3,{id:\"unlocking-new-opportunities\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#unlocking-new-opportunities\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Unlocking new opportunities\"]}),`\n`,(0,t.jsxs)(e.p,{children:[\"Release Delivery comes to you after a year-long close collaboration with our select customers, who saw an opportunity to \",(0,t.jsx)(e.strong,{children:\"take their SaaS products to the next level\"}),'. Thanks to their generous input and feedback, we navigated the peculiarities of the \"on-cloud\"/self-hosted versioning, unlocked access to enterprise requirements and helped them grow large account sales. With Release Delivery these SaaS providers now give their customers an amazing installation experience, wherever and however they want it. With only a few clicks their applications are deployed and running. Our customers can easily monitor and manage these deployments, and quickly upgrade the instances to keep ',(0,t.jsx)(e.em,{children:\"their\"}),\" customers on the latest app version, \",(0,t.jsx)(e.strong,{children:\"without a heavy lift.\"})]}),`\n`,(0,t.jsxs)(e.p,{children:[`I am extremely excited about making Release Delivery available to all. If you're a startup or a SaaS company that wants to generate more revenue from an \"on-cloud\"/self-hosted version of your application, talk to us! To learn more, `,(0,t.jsx)(e.a,{href:\"https://release.com/webinar/release-delivery\",children:\"join my webinar\"}),\" on April 20th, where I will show how Release Delivery works and share how it helps companies solve the problems around flexible deployment. Or, you \",(0,t.jsx)(e.a,{href:\"https://release.com/vpc-book-a-meeting\",children:\"book a meeting\"}),\" with our team to see a custom demo and get a trial account.\"]})]})}function k(n={}){let{wrapper:e}=n.components||{};return e?(0,t.jsx)(e,Object.assign({},n,{children:(0,t.jsx)(d,n)})):d(n)}var S=k;return w(R);})();\n;return Component;"
        },
        "_id": "blog/posts/release-delivery-helps-saas-companies-meet-the-needs-of-their-enterprise-customers.mdx",
        "_raw": {
          "sourceFilePath": "blog/posts/release-delivery-helps-saas-companies-meet-the-needs-of-their-enterprise-customers.mdx",
          "sourceFileName": "release-delivery-helps-saas-companies-meet-the-needs-of-their-enterprise-customers.mdx",
          "sourceFileDir": "blog/posts",
          "contentType": "mdx",
          "flattenedPath": "blog/posts/release-delivery-helps-saas-companies-meet-the-needs-of-their-enterprise-customers"
        },
        "type": "BlogPost",
        "computedSlug": "release-delivery-helps-saas-companies-meet-the-needs-of-their-enterprise-customers"
      },
      "documentHash": "1739658628150",
      "hasWarnings": false,
      "documentTypeName": "BlogPost"
    },
    "blog/posts/release-is-going-to-aws-summit-in-new-york-city-on-july-26th-2023.mdx": {
      "document": {
        "title": "Release is going to AWS Summit in New York City on July 26th, 2023",
        "summary": "Join us at AWS Summit in NYC!",
        "publishDate": "Tue Jul 11 2023 20:33:22 GMT+0000 (Coordinated Universal Time)",
        "author": "ira-casteel",
        "readingTime": 3,
        "categories": [
          "platform-engineering",
          "product"
        ],
        "mainImage": "/blog-images/f0d03822b5fa176db180a72839d6ed94.jpg",
        "imageAlt": "photo credit: Lukas Kloeppel",
        "showCTA": true,
        "ctaCopy": "Simplify testing and deployment with Release's on-demand environments, like Instant Datasets for near-production data. Try now!",
        "ctaLink": "https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=release-is-going-to-aws-summit-in-new-york-city-on-july-26th-2023",
        "relatedPosts": [
          ""
        ],
        "ogImage": "/blog-images/f0d03822b5fa176db180a72839d6ed94.jpg",
        "excerpt": "Join us at AWS Summit in NYC!",
        "tags": [
          "platform-engineering",
          "product"
        ],
        "ctaButton": "Try Release for Free",
        "body": {
          "raw": "\nJoin Release for a trip to New York City this summer! We are headed to [AWS Summit NYC](https://aws.amazon.com/events/summits/new-york/?trk=3fc68b46-3c7f-4e12-9881-a329f1ff1877&sc_channel=el) on July 26th and are thrilled to see you all there.  You can find us at the Expo Hall booth #343, as well as in the keynote and breakout sessions, just look for a purple Release t-shirt.\n\nThis year the one-day summit is packed with existing content at all technical levels on a wide range of topics like AI and machine learning, analytics, database, EC2 compute, storage, DevOps & developer productivity, serverless, and much more. Explore the agenda with all 150+ sessions [here](https://aws.amazon.com/events/summits/new-york/agenda/?amer-summit-cards.sort-by=item.additionalFields.startDateTime&amer-summit-cards.sort-order=asc&awsf.amer-summit-session=*all&awsf.amer-summit-level=*all&awsf.amer-summit-areaofinterest=*all&awsf.amer-summit-industry=*all&awsf.amer-summit-roles=*all&awsf.amer-summit-topic=*all). And if you are in NY a day early, check out the [Women of the Cloud event](https://aws.amazon.com/events/summits/new-york/women-of-the-cloud/?trk=cfaaccc0-e11b-4d9e-8f78-794bc8efe479&sc_channel=el) taking place on July 25th, as part of the summit.\n\nWhile you’re at the summit, stop by at our booth to not only test your (and our) rubik's cube solving skills (and get your own cube!), but also learn about all the exciting developments here at Release.\n\nOur stand-alone Instant Datasets is going live on July 25th. Now you can build and test your applications with near-production data available to you instantly. No more waiting to copy gigantic datasets, or using unrealistic seed data that hides production bugs. Instant Datasets generates and maintains a pool of on-demand production-like datasets available to you when you need them.  And for the initial cohort of users we are making Instant Datasets absolutely free. So [sign up for updates](https://www2.release.com/instant-datasets) and snag your free account!\n\nReleaseAI is in preview and we are actively working with the early adopters to make it even easier for you to talk to your infrastructure. [Join the preview today](https://release.ai/waitlist).\n\nLast but not least, we’ve made improvements to our core Release platform making it even easier to create ephemeral replicas of  your production environment with every pull request, every check-in and for every developer. Check it out for yourself with [a free trial](https://release.com/signup).\n\nSee you in NYC!\n",
          "code": "var Component=(()=>{var c=Object.create;var s=Object.defineProperty;var d=Object.getOwnPropertyDescriptor;var h=Object.getOwnPropertyNames;var p=Object.getPrototypeOf,f=Object.prototype.hasOwnProperty;var y=(t,e)=>()=>(e||t((e={exports:{}}).exports,e),e.exports),g=(t,e)=>{for(var n in e)s(t,n,{get:e[n],enumerable:!0})},r=(t,e,n,i)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let o of h(e))!f.call(t,o)&&o!==n&&s(t,o,{get:()=>e[o],enumerable:!(i=d(e,o))||i.enumerable});return t};var w=(t,e,n)=>(n=t!=null?c(p(t)):{},r(e||!t||!t.__esModule?s(n,\"default\",{value:t,enumerable:!0}):n,t)),b=t=>r(s({},\"__esModule\",{value:!0}),t);var m=y((j,l)=>{l.exports=_jsx_runtime});var C={};g(C,{default:()=>x,frontmatter:()=>v});var a=w(m()),v={title:\"Release is going to AWS Summit in New York City on July 26th, 2023\",summary:\"Join us at AWS Summit in NYC!\",publishDate:\"Tue Jul 11 2023 20:33:22 GMT+0000 (Coordinated Universal Time)\",author:\"ira-casteel\",readingTime:3,categories:[\"platform-engineering\",\"product\"],mainImage:\"/blog-images/f0d03822b5fa176db180a72839d6ed94.jpg\",imageAlt:\"photo credit: Lukas Kloeppel\",showCTA:!0,ctaCopy:\"Simplify testing and deployment with Release's on-demand environments, like Instant Datasets for near-production data. Try now!\",ctaLink:\"https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=release-is-going-to-aws-summit-in-new-york-city-on-july-26th-2023\",relatedPosts:[\"\"],ogImage:\"/blog-images/f0d03822b5fa176db180a72839d6ed94.jpg\",excerpt:\"Join us at AWS Summit in NYC!\",tags:[\"platform-engineering\",\"product\"],ctaButton:\"Try Release for Free\"};function u(t){let e=Object.assign({p:\"p\",a:\"a\"},t.components);return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsxs)(e.p,{children:[\"Join Release for a trip to New York City this summer! We are headed to \",(0,a.jsx)(e.a,{href:\"https://aws.amazon.com/events/summits/new-york/?trk=3fc68b46-3c7f-4e12-9881-a329f1ff1877&sc_channel=el\",children:\"AWS Summit NYC\"}),\" on July 26th and are thrilled to see you all there. \\xA0You can find us at the Expo Hall booth #343, as well as in the keynote and breakout sessions, just look for a purple Release t-shirt.\"]}),`\n`,(0,a.jsxs)(e.p,{children:[\"This year the one-day summit is packed with existing content at all technical levels on a wide range of topics like AI and machine learning, analytics, database, EC2 compute, storage, DevOps & developer productivity, serverless, and much more. Explore the agenda with all 150+ sessions \",(0,a.jsx)(e.a,{href:\"https://aws.amazon.com/events/summits/new-york/agenda/?amer-summit-cards.sort-by=item.additionalFields.startDateTime&amer-summit-cards.sort-order=asc&awsf.amer-summit-session=*all&awsf.amer-summit-level=*all&awsf.amer-summit-areaofinterest=*all&awsf.amer-summit-industry=*all&awsf.amer-summit-roles=*all&awsf.amer-summit-topic=*all\",children:\"here\"}),\". And if you are in NY a day early, check out the \",(0,a.jsx)(e.a,{href:\"https://aws.amazon.com/events/summits/new-york/women-of-the-cloud/?trk=cfaaccc0-e11b-4d9e-8f78-794bc8efe479&sc_channel=el\",children:\"Women of the Cloud event\"}),\" taking place on July 25th, as part of the summit.\"]}),`\n`,(0,a.jsx)(e.p,{children:\"While you\\u2019re at the summit, stop by at our booth to not only test your (and our) rubik's cube solving skills (and get your own cube!), but also learn about all the exciting developments here at Release.\"}),`\n`,(0,a.jsxs)(e.p,{children:[\"Our stand-alone Instant Datasets is going live on July 25th. Now you can build and test your applications with near-production data available to you instantly. No more waiting to copy gigantic datasets, or using unrealistic seed data that hides production bugs. Instant Datasets generates and maintains a pool of on-demand production-like datasets available to you when you need them. \\xA0And for the initial cohort of users we are making Instant Datasets absolutely free. So \",(0,a.jsx)(e.a,{href:\"https://www2.release.com/instant-datasets\",children:\"sign up for updates\"}),\" and snag your free account!\"]}),`\n`,(0,a.jsxs)(e.p,{children:[\"ReleaseAI is in preview and we are actively working with the early adopters to make it even easier for you to talk to your infrastructure. \",(0,a.jsx)(e.a,{href:\"https://release.ai/waitlist\",children:\"Join the preview today\"}),\".\"]}),`\n`,(0,a.jsxs)(e.p,{children:[\"Last but not least, we\\u2019ve made improvements to our core Release platform making it even easier to create ephemeral replicas of \\xA0your production environment with every pull request, every check-in and for every developer. Check it out for yourself with \",(0,a.jsx)(e.a,{href:\"https://release.com/signup\",children:\"a free trial\"}),\".\"]}),`\n`,(0,a.jsx)(e.p,{children:\"See you in NYC!\"})]})}function k(t={}){let{wrapper:e}=t.components||{};return e?(0,a.jsx)(e,Object.assign({},t,{children:(0,a.jsx)(u,t)})):u(t)}var x=k;return b(C);})();\n;return Component;"
        },
        "_id": "blog/posts/release-is-going-to-aws-summit-in-new-york-city-on-july-26th-2023.mdx",
        "_raw": {
          "sourceFilePath": "blog/posts/release-is-going-to-aws-summit-in-new-york-city-on-july-26th-2023.mdx",
          "sourceFileName": "release-is-going-to-aws-summit-in-new-york-city-on-july-26th-2023.mdx",
          "sourceFileDir": "blog/posts",
          "contentType": "mdx",
          "flattenedPath": "blog/posts/release-is-going-to-aws-summit-in-new-york-city-on-july-26th-2023"
        },
        "type": "BlogPost",
        "computedSlug": "release-is-going-to-aws-summit-in-new-york-city-on-july-26th-2023"
      },
      "documentHash": "1739393595026",
      "hasWarnings": false,
      "documentTypeName": "BlogPost"
    },
    "blog/posts/release-is-going-to-aws-summit-in-washington-dc-on-june-7-8-2023.mdx": {
      "document": {
        "title": "Release is going to AWS Summit in Washington, DC on June 7-8, 2023",
        "summary": "Join Release at the AWS Summit in Washington, DC ",
        "publishDate": "Wed May 31 2023 13:47:30 GMT+0000 (Coordinated Universal Time)",
        "author": "ira-casteel",
        "readingTime": 2,
        "categories": [
          "platform-engineering",
          "events"
        ],
        "mainImage": "/blog-images/7eee26835be806798ac31b1df8214e10.jpg",
        "imageAlt": "Release is going to AWS Summit in Washington, DC on June 7-8, 2023",
        "showCTA": true,
        "ctaCopy": "Empower your AWS deployments with Release's AWS Marketplace platform for seamless environment management.",
        "ctaLink": "https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=release-is-going-to-aws-summit-in-washington-dc-on-june-7-8-2023",
        "relatedPosts": [
          ""
        ],
        "ogImage": "/blog-images/7eee26835be806798ac31b1df8214e10.jpg",
        "excerpt": "Join Release at the AWS Summit in Washington, DC ",
        "tags": [
          "platform-engineering",
          "events"
        ],
        "ctaButton": "Try Release for Free",
        "body": {
          "raw": "\nRelease is thrilled to participate in this year’s [AWS Summit in Washington, DC](https://aws.amazon.com/events/summits/washington-dc/) on June 7th and 8th. Come join us for this complimentary event to learn about the latest and greatest in cloud computing and how Release and AWS are helping organizations accelerate their digital transformation and deliver mission-critical applications.  \n\nRelease Environments as a Service platform is built on AWS and is deployed directly into your AWS public or GovCloud account. This gives you full control and visibility into your AWS activities, and helps consolidate spend for best cloud discount options. Release is available though the [AWS Marketplace](https://aws.amazon.com/marketplace/pp/prodview-faxqlvzq65fea), providing procurement flexibility and maximization of AWS commitments.\n\nFind us in Booth #363 of the main Expo Hall to talk about all things environments, such as [Release Delivery](https://release.com/product/release-delivery) that allows you to deploy your SaaS apps inside of your customers’ environment, without creating an on-prem version of your product; or [Rainbow deployments](https://release.com/blog/rainbow-deployment-why-and-how-to-do-it), taking your blue-green approach to the next level. See our latest integrations of the generative AI helping build the environment templates even faster with the [Dockerfile auto-generation project](https://release.com/blog/code-to-cloud-simplified-how-release-uses-ai-to-make-deployment-easier), and making sense of your application documentation with the [Gromit project](https://release.com/blog/gromit-an-open-source-ai-assistant-for-your-documentation). Or simply say hi in one of the [numerous breakout sessions](https://aws.amazon.com/events/summits/washington-dc/agenda/?amer-summit-cards.sort-by=item.additionalFields.startDateTime&amer-summit-cards.sort-order=asc&awsf.amer-summit-session=*all&awsf.amer-summit-level=*all&awsf.amer-summit-areaofinterest=*all&awsf.amer-summit-industry=*all&awsf.amer-summit-roles=*all&awsf.amer-summit-topic=*all), lightning talks or workshops. Just look out for the purple Release t-shirt!\n\nWe are excited to spend a couple days learning and sharing our ideas. See you in DC!\n",
          "code": "var Component=(()=>{var u=Object.create;var i=Object.defineProperty;var d=Object.getOwnPropertyDescriptor;var h=Object.getOwnPropertyNames;var p=Object.getPrototypeOf,g=Object.prototype.hasOwnProperty;var f=(t,e)=>()=>(e||t((e={exports:{}}).exports,e),e.exports),y=(t,e)=>{for(var n in e)i(t,n,{get:e[n],enumerable:!0})},r=(t,e,n,s)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let o of h(e))!g.call(t,o)&&o!==n&&i(t,o,{get:()=>e[o],enumerable:!(s=d(e,o))||s.enumerable});return t};var w=(t,e,n)=>(n=t!=null?u(p(t)):{},r(e||!t||!t.__esModule?i(n,\"default\",{value:t,enumerable:!0}):n,t)),b=t=>r(i({},\"__esModule\",{value:!0}),t);var m=f((j,l)=>{l.exports=_jsx_runtime});var x={};y(x,{default:()=>W,frontmatter:()=>v});var a=w(m()),v={title:\"Release is going to AWS Summit in Washington, DC on June 7-8, 2023\",summary:\"Join Release at the AWS Summit in Washington, DC \",publishDate:\"Wed May 31 2023 13:47:30 GMT+0000 (Coordinated Universal Time)\",author:\"ira-casteel\",readingTime:2,categories:[\"platform-engineering\",\"events\"],mainImage:\"/blog-images/7eee26835be806798ac31b1df8214e10.jpg\",imageAlt:\"Release is going to AWS Summit in Washington, DC on June 7-8, 2023\",showCTA:!0,ctaCopy:\"Empower your AWS deployments with Release's AWS Marketplace platform for seamless environment management.\",ctaLink:\"https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=release-is-going-to-aws-summit-in-washington-dc-on-june-7-8-2023\",relatedPosts:[\"\"],ogImage:\"/blog-images/7eee26835be806798ac31b1df8214e10.jpg\",excerpt:\"Join Release at the AWS Summit in Washington, DC \",tags:[\"platform-engineering\",\"events\"],ctaButton:\"Try Release for Free\"};function c(t){let e=Object.assign({p:\"p\",a:\"a\"},t.components);return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsxs)(e.p,{children:[\"Release is thrilled to participate in this year\\u2019s \",(0,a.jsx)(e.a,{href:\"https://aws.amazon.com/events/summits/washington-dc/\",children:\"AWS Summit in Washington, DC\"}),\" on June 7th and 8th. Come join us for this complimentary event to learn about the latest and greatest in cloud computing and how Release and AWS are helping organizations accelerate their digital transformation and deliver mission-critical applications. \\xA0\"]}),`\n`,(0,a.jsxs)(e.p,{children:[\"Release Environments as a Service platform is built on AWS and is deployed directly into your AWS public or GovCloud account. This gives you full control and visibility into your AWS activities, and helps consolidate spend for best cloud discount options. Release is available though the \",(0,a.jsx)(e.a,{href:\"https://aws.amazon.com/marketplace/pp/prodview-faxqlvzq65fea\",children:\"AWS Marketplace\"}),\", providing procurement flexibility and maximization of AWS commitments.\"]}),`\n`,(0,a.jsxs)(e.p,{children:[\"Find us in Booth #363 of the main Expo Hall to talk about all things environments, such as \",(0,a.jsx)(e.a,{href:\"https://release.com/product/release-delivery\",children:\"Release Delivery\"}),\" that allows you to deploy your SaaS apps inside of your customers\\u2019 environment, without creating an on-prem version of your product; or \",(0,a.jsx)(e.a,{href:\"https://release.com/blog/rainbow-deployment-why-and-how-to-do-it\",children:\"Rainbow deployments\"}),\", taking your blue-green approach to the next level. See our latest integrations of the generative AI helping build the environment templates even faster with the \",(0,a.jsx)(e.a,{href:\"https://release.com/blog/code-to-cloud-simplified-how-release-uses-ai-to-make-deployment-easier\",children:\"Dockerfile auto-generation project\"}),\", and making sense of your application documentation with the \",(0,a.jsx)(e.a,{href:\"https://release.com/blog/gromit-an-open-source-ai-assistant-for-your-documentation\",children:\"Gromit project\"}),\". Or simply say hi in one of the \",(0,a.jsx)(e.a,{href:\"https://aws.amazon.com/events/summits/washington-dc/agenda/?amer-summit-cards.sort-by=item.additionalFields.startDateTime&amer-summit-cards.sort-order=asc&awsf.amer-summit-session=*all&awsf.amer-summit-level=*all&awsf.amer-summit-areaofinterest=*all&awsf.amer-summit-industry=*all&awsf.amer-summit-roles=*all&awsf.amer-summit-topic=*all\",children:\"numerous breakout sessions\"}),\", lightning talks or workshops. Just look out for the purple Release t-shirt!\"]}),`\n`,(0,a.jsx)(e.p,{children:\"We are excited to spend a couple days learning and sharing our ideas. See you in DC!\"})]})}function S(t={}){let{wrapper:e}=t.components||{};return e?(0,a.jsx)(e,Object.assign({},t,{children:(0,a.jsx)(c,t)})):c(t)}var W=S;return b(x);})();\n;return Component;"
        },
        "_id": "blog/posts/release-is-going-to-aws-summit-in-washington-dc-on-june-7-8-2023.mdx",
        "_raw": {
          "sourceFilePath": "blog/posts/release-is-going-to-aws-summit-in-washington-dc-on-june-7-8-2023.mdx",
          "sourceFileName": "release-is-going-to-aws-summit-in-washington-dc-on-june-7-8-2023.mdx",
          "sourceFileDir": "blog/posts",
          "contentType": "mdx",
          "flattenedPath": "blog/posts/release-is-going-to-aws-summit-in-washington-dc-on-june-7-8-2023"
        },
        "type": "BlogPost",
        "computedSlug": "release-is-going-to-aws-summit-in-washington-dc-on-june-7-8-2023"
      },
      "documentHash": "1739393595026",
      "hasWarnings": false,
      "documentTypeName": "BlogPost"
    },
    "blog/posts/release-is-going-to-dockercon-2023.mdx": {
      "document": {
        "title": "Release is going to DockerCon 2023",
        "summary": "Join us at DockerCon 2023 for two days of learning, exploring and networking. ",
        "publishDate": "Tue Sep 19 2023 15:01:47 GMT+0000 (Coordinated Universal Time)",
        "author": "ira-casteel",
        "readingTime": 2,
        "categories": [
          "platform-engineering",
          "product"
        ],
        "mainImage": "/blog-images/44058936d25f3d891812538e29cdc34a.jpg",
        "imageAlt": "Moby DockerCon2022",
        "showCTA": true,
        "ctaCopy": "Accelerate Docker project testing with Release's on-demand environments for seamless collaboration and faster bug resolution.",
        "ctaLink": "https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=release-is-going-to-dockercon-2023",
        "relatedPosts": [
          ""
        ],
        "ogImage": "/blog-images/44058936d25f3d891812538e29cdc34a.jpg",
        "excerpt": "Join us at DockerCon 2023 for two days of learning, exploring and networking. ",
        "tags": [
          "platform-engineering",
          "product"
        ],
        "ctaButton": "Try Release for Free",
        "body": {
          "raw": "\nBack-to-school and back-to-conferences! We're excited to announce that Release will be attending [DockerCon 2023](https://www.dockercon.com/) held in-person in Los Angeles this year. As one of the notable tech events in the container industry, DockerCon promises exciting innovations, industry insights, and networking opportunities. Here's what we're looking forward to this year:\n**🐳 Exploring the latest Docker Technology:**   \nDocker reshaped the way we think about, build, and deploy software. DockerCon is an excellent opportunity to learn about new tools, techniques, and best practices, and get a firsthand look at the latest Docker developments. We’re curious to see how these developments might resonate with our initiatives and broader industry landscape.\n**👋 Engaging with the Docker Community:   \n‍**There's nothing like being surrounded by fellow Docker enthusiasts who are just as passionate about containers as we are! DockerCon offers a unique platform to meet experts, share experiences, and foster partnerships. We're excited to exchange insights with fellow attendees and potentially collaborate on future endeavors.\n**📝 Diving into Workshops and Breakout Sessions:  \n‍**DockerCon's workshops are where theory meets practice. We're particularly interested in hands-on sessions that will provide our team with practical knowledge and skills that we can immediately apply to our projects. Check out what one of the Docker Captain has to say about the [upcoming DockerCon workshops](https://www.docker.com/blog/dockercon-workshops-what-to-expect/) and pick a track that makes sense for you.\n**🤓 Sharing Our Own Insights:  \n‍**While we're excited to absorb a ton of new information, we're also thrilled to share our own experiences with the Docker community. Our team has worked on some groundbreaking projects, and we believe our experiences can be valuable to others on their containerization journey. Come see us at the booth or in one of the sessions, just look for a purple Release t-shirt.\n\nDockerCon 2023 promises to be more than just a conference; it's an experience, a learning expedition, and an opportunity to engage with the global container community. We at Release are gearing up for an enlightening few days, and we can't wait to share our learnings and experiences with you once we're back.\n\nP.S. If you're also attending DockerCon 2023, drop us a line! We'd love to catch up and share notes.\n",
          "code": "var Component=(()=>{var d=Object.create;var a=Object.defineProperty;var g=Object.getOwnPropertyDescriptor;var u=Object.getOwnPropertyNames;var p=Object.getPrototypeOf,m=Object.prototype.hasOwnProperty;var w=(o,e)=>()=>(e||o((e={exports:{}}).exports,e),e.exports),k=(o,e)=>{for(var t in e)a(o,t,{get:e[t],enumerable:!0})},i=(o,e,t,s)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let r of u(e))!m.call(o,r)&&r!==t&&a(o,r,{get:()=>e[r],enumerable:!(s=g(e,r))||s.enumerable});return o};var b=(o,e,t)=>(t=o!=null?d(p(o)):{},i(e||!o||!o.__esModule?a(t,\"default\",{value:o,enumerable:!0}):t,o)),f=o=>i(a({},\"__esModule\",{value:!0}),o);var l=w((v,c)=>{c.exports=_jsx_runtime});var C={};k(C,{default:()=>D,frontmatter:()=>y});var n=b(l()),y={title:\"Release is going to DockerCon 2023\",summary:\"Join us at DockerCon 2023 for two days of learning, exploring and networking. \",publishDate:\"Tue Sep 19 2023 15:01:47 GMT+0000 (Coordinated Universal Time)\",author:\"ira-casteel\",readingTime:2,categories:[\"platform-engineering\",\"product\"],mainImage:\"/blog-images/44058936d25f3d891812538e29cdc34a.jpg\",imageAlt:\"Moby DockerCon2022\",showCTA:!0,ctaCopy:\"Accelerate Docker project testing with Release's on-demand environments for seamless collaboration and faster bug resolution.\",ctaLink:\"https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=release-is-going-to-dockercon-2023\",relatedPosts:[\"\"],ogImage:\"/blog-images/44058936d25f3d891812538e29cdc34a.jpg\",excerpt:\"Join us at DockerCon 2023 for two days of learning, exploring and networking. \",tags:[\"platform-engineering\",\"product\"],ctaButton:\"Try Release for Free\"};function h(o){let e=Object.assign({p:\"p\",a:\"a\",strong:\"strong\",br:\"br\"},o.components);return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsxs)(e.p,{children:[\"Back-to-school and back-to-conferences! We're excited to announce that Release will be attending \",(0,n.jsx)(e.a,{href:\"https://www.dockercon.com/\",children:\"DockerCon 2023\"}),` held in-person in Los Angeles this year. As one of the notable tech events in the container industry, DockerCon promises exciting innovations, industry insights, and networking opportunities. Here's what we're looking forward to this year:\n`,(0,n.jsx)(e.strong,{children:\"\\u{1F433} Exploring the latest Docker Technology:\"}),\"\\xA0\",(0,n.jsx)(e.br,{}),`\n`,`Docker reshaped the way we think about, build, and deploy software. DockerCon is an excellent opportunity to learn about new tools, techniques, and best practices, and get a firsthand look at the latest Docker developments. We\\u2019re curious to see how these developments might resonate with our initiatives and broader industry landscape.\n`,(0,n.jsxs)(e.strong,{children:[\"\\u{1F44B} Engaging with the Docker Community:\\xA0\",(0,n.jsx)(e.br,{}),`\n`,\"\\u200D\"]}),`There's nothing like being surrounded by fellow Docker enthusiasts who are just as passionate about containers as we are! DockerCon offers a unique platform to meet experts, share experiences, and foster partnerships. We're excited to exchange insights with fellow attendees and potentially collaborate on future endeavors.\n`,(0,n.jsxs)(e.strong,{children:[\"\\u{1F4DD} Diving into Workshops and Breakout Sessions:\",(0,n.jsx)(e.br,{}),`\n`,\"\\u200D\"]}),\"DockerCon's workshops are where theory meets practice. We're particularly interested in hands-on sessions that will provide our team with practical knowledge and skills that we can immediately apply to our projects. Check out what one of the Docker Captain has to say about the \",(0,n.jsx)(e.a,{href:\"https://www.docker.com/blog/dockercon-workshops-what-to-expect/\",children:\"upcoming DockerCon workshops\"}),` and pick a track that makes sense for you.\n`,(0,n.jsxs)(e.strong,{children:[\"\\u{1F913} Sharing Our Own Insights:\",(0,n.jsx)(e.br,{}),`\n`,\"\\u200D\"]}),\"While we're excited to absorb a ton of new information, we're also thrilled to share our own experiences with the Docker community. Our team has worked on some groundbreaking projects, and we believe our experiences can be valuable to others on their containerization journey. Come see us at the booth or in one of the sessions, just look for a purple Release t-shirt.\"]}),`\n`,(0,n.jsx)(e.p,{children:\"DockerCon 2023 promises to be more than just a conference; it's an experience, a learning expedition, and an opportunity to engage with the global container community. We at Release are gearing up for an enlightening few days, and we can't wait to share our learnings and experiences with you once we're back.\"}),`\n`,(0,n.jsx)(e.p,{children:\"P.S. If you're also attending DockerCon 2023, drop us a line! We'd love to catch up and share notes.\"})]})}function x(o={}){let{wrapper:e}=o.components||{};return e?(0,n.jsx)(e,Object.assign({},o,{children:(0,n.jsx)(h,o)})):h(o)}var D=x;return f(C);})();\n;return Component;"
        },
        "_id": "blog/posts/release-is-going-to-dockercon-2023.mdx",
        "_raw": {
          "sourceFilePath": "blog/posts/release-is-going-to-dockercon-2023.mdx",
          "sourceFileName": "release-is-going-to-dockercon-2023.mdx",
          "sourceFileDir": "blog/posts",
          "contentType": "mdx",
          "flattenedPath": "blog/posts/release-is-going-to-dockercon-2023"
        },
        "type": "BlogPost",
        "computedSlug": "release-is-going-to-dockercon-2023"
      },
      "documentHash": "1739393595026",
      "hasWarnings": false,
      "documentTypeName": "BlogPost"
    },
    "blog/posts/release-is-going-to-kubecon-2023.mdx": {
      "document": {
        "title": "Release is going to KubeCon 2023",
        "summary": "Join us at KubeCon this year to explore all things Kubernetes. ",
        "publishDate": "Tue Oct 17 2023 20:12:13 GMT+0000 (Coordinated Universal Time)",
        "author": "ira-casteel",
        "readingTime": 2,
        "categories": [
          "events",
          "kubernetes",
          "platform-engineering"
        ],
        "mainImage": "/blog-images/a7d375cb1bdee1068c80a4a8da24d622.jpg",
        "imageAlt": "Release is going to KubeCon 2023",
        "showCTA": true,
        "ctaCopy": "Enhance KubeCon experience with Release's ephemeral environments for seamless collaboration, faster testing, and consistent deployments.",
        "ctaLink": "https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=release-is-going-to-kubecon-2023",
        "relatedPosts": [
          "10-kubernetes-namespace-best-practices-to-start-following; how-to-make-kubernetes-config-files-not-suck"
        ],
        "ogImage": "/blog-images/a7d375cb1bdee1068c80a4a8da24d622.jpg",
        "excerpt": "Join us at KubeCon this year to explore all things Kubernetes. ",
        "tags": [
          "events",
          "kubernetes",
          "platform-engineering"
        ],
        "ctaButton": "Try Release for Free",
        "body": {
          "raw": "\nThe countdown has begun! We’re pleased to share that Release will be attending [KubeCon in Chicago](https://events.linuxfoundation.org/kubecon-cloudnativecon-north-america/) on November 6-9. As one of the key events centered on Kubernetes and cloud-native technologies, KubeCon promises insightful sessions, spirited discussions, and invaluable networking opportunities. Here's a sneak peek into what we're anticipating:\n\n‍**☸️ Kubernetes Innovations:**\n\nEvery year, KubeCon presents a myriad of newest trends and best practices in Kubernetes. We’re keen on delving into the latest developments and understanding how they can fit into our ecosystem and benefit our customers. See this year’s [Keynote speakers lineup](https://events.linuxfoundation.org/kubecon-cloudnativecon-north-america/program/keynote-speakers/) for some clues on what will be discussed.\n\n‍**🔊Discussions and Panels:**\n\nKubeCon has always been a melting pot of ideas. The panel discussions and breakout sessions are where some of the best minds come together to discuss the future of cloud-native solutions. We’re all set to participate, contribute, and learn throughout the event. Check out the [Program](https://events.linuxfoundation.org/kubecon-cloudnativecon-north-america/program/schedule/) to build a schedule that fits your interests.\n\n‍**👋 Networking with Cloud Native Enthusiasts:**\n\nKubeCon gathers a diverse group of professionals interested in cloud-native technologies. We're eager to share experiences, discuss challenges, and explore possibilities with peers from across the globe. As always, there are a number of [co-located events](https://events.linuxfoundation.org/kubecon-cloudnativecon-north-america/co-located-events/cncf-hosted-co-located-schedule/) to fit any kind of interest.\n\n‍**🎙️Showcasing Release Contributions:**\n\nStop by the Booth N30 to chat about our latest developments, see a demo and pick up some Release swag. Whether you have a specific challenge in mind or are just curious about what we do, our team is there to engage and share.\n\nDon’t forget to [register](https://events.linuxfoundation.org/kubecon-cloudnativecon-north-america/register/) and see you at the conference.\n",
          "code": "var Component=(()=>{var h=Object.create;var a=Object.defineProperty;var d=Object.getOwnPropertyDescriptor;var g=Object.getOwnPropertyNames;var p=Object.getPrototypeOf,m=Object.prototype.hasOwnProperty;var b=(t,e)=>()=>(e||t((e={exports:{}}).exports,e),e.exports),f=(t,e)=>{for(var o in e)a(t,o,{get:e[o],enumerable:!0})},i=(t,e,o,r)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let s of g(e))!m.call(t,s)&&s!==o&&a(t,s,{get:()=>e[s],enumerable:!(r=d(e,s))||r.enumerable});return t};var v=(t,e,o)=>(o=t!=null?h(p(t)):{},i(e||!t||!t.__esModule?a(o,\"default\",{value:t,enumerable:!0}):o,t)),k=t=>i(a({},\"__esModule\",{value:!0}),t);var l=b((_,c)=>{c.exports=_jsx_runtime});var C={};f(C,{default:()=>x,frontmatter:()=>w});var n=v(l()),w={title:\"Release is going to KubeCon 2023\",summary:\"Join us at KubeCon this year to explore all things Kubernetes. \",publishDate:\"Tue Oct 17 2023 20:12:13 GMT+0000 (Coordinated Universal Time)\",author:\"ira-casteel\",readingTime:2,categories:[\"events\",\"kubernetes\",\"platform-engineering\"],mainImage:\"/blog-images/a7d375cb1bdee1068c80a4a8da24d622.jpg\",imageAlt:\"Release is going to KubeCon 2023\",showCTA:!0,ctaCopy:\"Enhance KubeCon experience with Release's ephemeral environments for seamless collaboration, faster testing, and consistent deployments.\",ctaLink:\"https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=release-is-going-to-kubecon-2023\",relatedPosts:[\"10-kubernetes-namespace-best-practices-to-start-following; how-to-make-kubernetes-config-files-not-suck\"],ogImage:\"/blog-images/a7d375cb1bdee1068c80a4a8da24d622.jpg\",excerpt:\"Join us at KubeCon this year to explore all things Kubernetes. \",tags:[\"events\",\"kubernetes\",\"platform-engineering\"],ctaButton:\"Try Release for Free\"};function u(t){let e=Object.assign({p:\"p\",a:\"a\",strong:\"strong\"},t.components);return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsxs)(e.p,{children:[\"The countdown has begun! We\\u2019re pleased to share that Release will be attending \",(0,n.jsx)(e.a,{href:\"https://events.linuxfoundation.org/kubecon-cloudnativecon-north-america/\",children:\"KubeCon in Chicago\"}),\" on November 6-9. As one of the key events centered on Kubernetes and cloud-native technologies, KubeCon promises insightful sessions, spirited discussions, and invaluable networking opportunities. Here's a sneak peek into what we're anticipating:\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"\\u200D\",(0,n.jsx)(e.strong,{children:\"\\u2638\\uFE0F Kubernetes Innovations:\"})]}),`\n`,(0,n.jsxs)(e.p,{children:[\"Every year, KubeCon presents a myriad of newest trends and best practices in Kubernetes. We\\u2019re keen on delving into the latest developments and understanding how they can fit into our ecosystem and benefit our customers. See this year\\u2019s \",(0,n.jsx)(e.a,{href:\"https://events.linuxfoundation.org/kubecon-cloudnativecon-north-america/program/keynote-speakers/\",children:\"Keynote speakers lineup\"}),\" for some clues on what will be discussed.\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"\\u200D\",(0,n.jsx)(e.strong,{children:\"\\u{1F50A}Discussions and Panels:\"})]}),`\n`,(0,n.jsxs)(e.p,{children:[\"KubeCon has always been a melting pot of ideas. The panel discussions and breakout sessions are where some of the best minds come together to discuss the future of cloud-native solutions. We\\u2019re all set to participate, contribute, and learn throughout the event. Check out the \",(0,n.jsx)(e.a,{href:\"https://events.linuxfoundation.org/kubecon-cloudnativecon-north-america/program/schedule/\",children:\"Program\"}),\" to build a schedule that fits your interests.\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"\\u200D\",(0,n.jsx)(e.strong,{children:\"\\u{1F44B} Networking with Cloud Native Enthusiasts:\"})]}),`\n`,(0,n.jsxs)(e.p,{children:[\"KubeCon gathers a diverse group of professionals interested in cloud-native technologies. We're eager to share experiences, discuss challenges, and explore possibilities with peers from across the globe. As always, there are a number of \",(0,n.jsx)(e.a,{href:\"https://events.linuxfoundation.org/kubecon-cloudnativecon-north-america/co-located-events/cncf-hosted-co-located-schedule/\",children:\"co-located events\"}),\" to fit any kind of interest.\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"\\u200D\",(0,n.jsx)(e.strong,{children:\"\\u{1F399}\\uFE0FShowcasing Release Contributions:\"})]}),`\n`,(0,n.jsx)(e.p,{children:\"Stop by the Booth N30 to chat about our latest developments, see a demo and pick up some Release swag. Whether you have a specific challenge in mind or are just curious about what we do, our team is there to engage and share.\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"Don\\u2019t forget to \",(0,n.jsx)(e.a,{href:\"https://events.linuxfoundation.org/kubecon-cloudnativecon-north-america/register/\",children:\"register\"}),\" and see you at the conference.\"]})]})}function y(t={}){let{wrapper:e}=t.components||{};return e?(0,n.jsx)(e,Object.assign({},t,{children:(0,n.jsx)(u,t)})):u(t)}var x=y;return k(C);})();\n;return Component;"
        },
        "_id": "blog/posts/release-is-going-to-kubecon-2023.mdx",
        "_raw": {
          "sourceFilePath": "blog/posts/release-is-going-to-kubecon-2023.mdx",
          "sourceFileName": "release-is-going-to-kubecon-2023.mdx",
          "sourceFileDir": "blog/posts",
          "contentType": "mdx",
          "flattenedPath": "blog/posts/release-is-going-to-kubecon-2023"
        },
        "type": "BlogPost",
        "computedSlug": "release-is-going-to-kubecon-2023"
      },
      "documentHash": "1739393595026",
      "hasWarnings": false,
      "documentTypeName": "BlogPost"
    },
    "blog/posts/release-is-going-to-oss-north-america.mdx": {
      "document": {
        "title": "Release is going to OSS North America",
        "summary": "",
        "publishDate": "Mon May 08 2023 18:24:44 GMT+0000 (Coordinated Universal Time)",
        "author": "david-giffin",
        "readingTime": 2,
        "categories": [
          "platform-engineering",
          "events"
        ],
        "mainImage": "/blog-images/ebdfe039ad33df75fe4f29e0cef5b478.png",
        "imageAlt": "Release is going to OSS North America",
        "showCTA": true,
        "ctaCopy": "Experience seamless collaboration and faster deployments with Release's ephemeral environments. Try it now!",
        "ctaLink": "https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=release-is-going-to-oss-north-america",
        "relatedPosts": [
          "11-continuous-deployment-tools-and-how-to-choose-one"
        ],
        "ogImage": "/blog-images/ebdfe039ad33df75fe4f29e0cef5b478.png",
        "excerpt": "Come see us at Open Source Summit North America in Vancouver, Canada this week! We are excited to be attending the conference, hearing amazing talks, and see...",
        "tags": [
          "platform-engineering",
          "events"
        ],
        "ctaButton": "Try Release for Free",
        "body": {
          "raw": "\nCome see us at [Open Source Summit North America](https://events.linuxfoundation.org/open-source-summit-north-america/) in Vancouver, Canada this week! We are excited to be attending the conference, hearing amazing talks, and seeing all the cool things the community is working on. This year we are showing off our own open source project “gromit” and are demoing the improvements we made to our Release Development Platform.\n\nHere at Release we are strong believers in the power of open source. In our drive to create a development platform that better serves modern development teams, our founding team has been using and contributing to the open source community for years. Founded in 2019 Release was and still is built entirely with open source software! \n\nThis year, we are excited to announce a contribution to the open source community that allows anyone to take their documentation and create an AI powered assistant. Based on the work done at [Supabase](https://supabase.com/blog/chatgpt-supabase-docs) to create a ChatGPT user interface for documentation, Release is open sourcing [a ruby gem “gromit”](https://rubygems.org/gems/gromit) that provides a vector search backend using Ruby and Redis.\n\nOur vision at Release is for all developers and teams to use the cloud to its full potential without needing to be experts in everything. We do this by simplifying the creation, access and usage of all environments (development, ephemeral, testing, staging, production, etc.), while maintaining access to advanced tooling.\n\nIf you are looking for ways to increase developer productivity through a truly modern cloud agnostic platform or just want to see a cool demo of “gromit” in action come see us at our booth! We love chatting with developers, learning about their successes and challenges (comparing notes) and making teams as productive as possible.  See you there!\n\n‍\n",
          "code": "var Component=(()=>{var d=Object.create;var r=Object.defineProperty;var u=Object.getOwnPropertyDescriptor;var g=Object.getOwnPropertyNames;var h=Object.getPrototypeOf,p=Object.prototype.hasOwnProperty;var f=(t,e)=>()=>(e||t((e={exports:{}}).exports,e),e.exports),b=(t,e)=>{for(var n in e)r(t,n,{get:e[n],enumerable:!0})},s=(t,e,n,i)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let a of g(e))!p.call(t,a)&&a!==n&&r(t,a,{get:()=>e[a],enumerable:!(i=u(e,a))||i.enumerable});return t};var v=(t,e,n)=>(n=t!=null?d(h(t)):{},s(e||!t||!t.__esModule?r(n,\"default\",{value:t,enumerable:!0}):n,t)),y=t=>s(r({},\"__esModule\",{value:!0}),t);var m=f((C,c)=>{c.exports=_jsx_runtime});var R={};b(R,{default:()=>k,frontmatter:()=>w});var o=v(m()),w={title:\"Release is going to OSS North America\",summary:\"\",publishDate:\"Mon May 08 2023 18:24:44 GMT+0000 (Coordinated Universal Time)\",author:\"david-giffin\",readingTime:2,categories:[\"platform-engineering\",\"events\"],mainImage:\"/blog-images/ebdfe039ad33df75fe4f29e0cef5b478.png\",imageAlt:\"Release is going to OSS North America\",showCTA:!0,ctaCopy:\"Experience seamless collaboration and faster deployments with Release's ephemeral environments. Try it now!\",ctaLink:\"https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=release-is-going-to-oss-north-america\",relatedPosts:[\"11-continuous-deployment-tools-and-how-to-choose-one\"],ogImage:\"/blog-images/ebdfe039ad33df75fe4f29e0cef5b478.png\",excerpt:\"Come see us at Open Source Summit North America in Vancouver, Canada this week! We are excited to be attending the conference, hearing amazing talks, and see...\",tags:[\"platform-engineering\",\"events\"],ctaButton:\"Try Release for Free\"};function l(t){let e=Object.assign({p:\"p\",a:\"a\"},t.components);return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsxs)(e.p,{children:[\"Come see us at \",(0,o.jsx)(e.a,{href:\"https://events.linuxfoundation.org/open-source-summit-north-america/\",children:\"Open Source Summit North America\"}),\" in Vancouver, Canada this week! We are excited to be attending the conference, hearing amazing talks, and seeing all the cool things the community is working on. This year we are showing off our own open source project \\u201Cgromit\\u201D and are demoing the improvements we made to our Release Development Platform.\"]}),`\n`,(0,o.jsx)(e.p,{children:\"Here at Release we are strong believers in the power of open source. In our drive to create a development platform that better serves modern development teams, our founding team has been using and contributing to the open source community for years. Founded in 2019 Release was and still is built entirely with open source software!\\xA0\"}),`\n`,(0,o.jsxs)(e.p,{children:[\"This year, we are excited to announce a contribution to the open source community that allows anyone to take their documentation and create an AI powered assistant. Based on the work done at \",(0,o.jsx)(e.a,{href:\"https://supabase.com/blog/chatgpt-supabase-docs\",children:\"Supabase\"}),\" to create a ChatGPT user interface for documentation, Release is open sourcing \",(0,o.jsx)(e.a,{href:\"https://rubygems.org/gems/gromit\",children:\"a ruby gem \\u201Cgromit\\u201D\"}),\" that provides a vector search backend using Ruby and Redis.\"]}),`\n`,(0,o.jsx)(e.p,{children:\"Our vision at Release is for all developers and teams to use the cloud to its full potential without needing to be experts in everything. We do this by simplifying the creation, access and usage of all environments (development, ephemeral, testing, staging, production, etc.), while maintaining access to advanced tooling.\"}),`\n`,(0,o.jsx)(e.p,{children:\"If you are looking for ways to increase developer productivity through a truly modern cloud agnostic platform or just want to see a cool demo of \\u201Cgromit\\u201D in action come see us at our booth! We love chatting with developers, learning about their successes and challenges (comparing notes) and making teams as productive as possible.\\xA0 See you there!\"}),`\n`,(0,o.jsx)(e.p,{children:\"\\u200D\"})]})}function x(t={}){let{wrapper:e}=t.components||{};return e?(0,o.jsx)(e,Object.assign({},t,{children:(0,o.jsx)(l,t)})):l(t)}var k=x;return y(R);})();\n;return Component;"
        },
        "_id": "blog/posts/release-is-going-to-oss-north-america.mdx",
        "_raw": {
          "sourceFilePath": "blog/posts/release-is-going-to-oss-north-america.mdx",
          "sourceFileName": "release-is-going-to-oss-north-america.mdx",
          "sourceFileDir": "blog/posts",
          "contentType": "mdx",
          "flattenedPath": "blog/posts/release-is-going-to-oss-north-america"
        },
        "type": "BlogPost",
        "computedSlug": "release-is-going-to-oss-north-america"
      },
      "documentHash": "1739393595026",
      "hasWarnings": false,
      "documentTypeName": "BlogPost"
    },
    "blog/posts/release-is-going-to-platformcon-2023.mdx": {
      "document": {
        "title": "Release is going to PlatformCon 2023  ",
        "summary": "Join Release for the exciting two days of learning and discovery at PlatformCon 2023 on June 8th and 9th.",
        "publishDate": "Wed Jun 07 2023 03:30:50 GMT+0000 (Coordinated Universal Time)",
        "author": "ira-casteel",
        "readingTime": 2,
        "categories": [
          "platform-engineering",
          "events",
          "kubernetes"
        ],
        "mainImage": "/blog-images/792d7efa495a12dd5f3699b5a09d5d7a.jpg",
        "imageAlt": "Release is going to PlatformCon 2023  ",
        "showCTA": true,
        "ctaCopy": "Streamline collaboration and accelerate deployment with Release's on-demand environments, mirroring production setups for efficient testing and seamless bug resolution.",
        "ctaLink": "https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=release-is-going-to-platformcon-2023",
        "relatedPosts": [
          "12-things-you-didnt-know-you-could-do-with-release-part-1"
        ],
        "ogImage": "/blog-images/792d7efa495a12dd5f3699b5a09d5d7a.jpg",
        "excerpt": "Join Release for the exciting two days of learning and discovery at PlatformCon 2023 on June 8th and 9th.",
        "tags": [
          "platform-engineering",
          "events",
          "kubernetes"
        ],
        "ctaButton": "Try Release for Free",
        "body": {
          "raw": "\nJoin Release for the exciting two days of learning and discovery at [PlatformCon 2023](https://platformcon.com/) on June 8th and 9th. This is the second year of PlatformCon bringing the community of Platform Engineers, DevOps Practitioners and Technology Leaders together to discuss and share insights around the growing field of Platform Engineering. And again it is a free online event, so make sure to sign up and join the discussion.\n\nThis years’ [talks](https://platformcon.com/talks) are organized into five categories: Stories, Tech, Culture, Blueprints and Impact, and sessions across the all tracks are quite impressive. We are looking forward to learning about the history and humble beginnings of containers from Micheal Irvin, a DevRel lead at [Docker](https://www.docker.com/), in his talk “[Containers: Where we came from and where the future's taking us](https://platformcon.com/talks/containers-where-we-came-from-and-where-the-futures-taking-us)”. Exploring the complexities of today's software and distributed systems with Gregor Hohpe, a Director of Enterprise Strategy at [AWS](https://aws.amazon.com/), in his talk “[Building Abstractions, not illusions](https://platformcon.com/talks/build-abstractions-not-illusions)”. And learning valuable practitioner lessons with Reynis Vazquez-Guzman, Sr. Software Engineer at [Affirm](https://www.affirm.com/), in her “​​[Where we went wrong in building a self-service platform on Kubernetes](https://platformcon.com/talks/where-we-went-wrong-in-building-a-self-service-platform-on-kubernetes)” talk.\n\nOur own Nick Busey will be sharing his insights around evolving developer experiences. He will discuss  on-demand environments as a way of providing teams with a fulfilling development experience. In particular, how you build systems to maintain the excitement and ease of collaboration, as you build out your product in his talk “[From skateboard to car: How to evolve your developer experience](https://platformcon.com/talks/from-skateboard-to-car-how-to-evolve-your-developer-experience)” on June 8th, in the Blueprints track.\n\nWhat sessions are you most looking forward to? See you there!\n",
          "code": "var Component=(()=>{var h=Object.create;var r=Object.defineProperty;var m=Object.getOwnPropertyDescriptor;var u=Object.getOwnPropertyNames;var g=Object.getPrototypeOf,f=Object.prototype.hasOwnProperty;var p=(t,e)=>()=>(e||t((e={exports:{}}).exports,e),e.exports),w=(t,e)=>{for(var o in e)r(t,o,{get:e[o],enumerable:!0})},s=(t,e,o,i)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let a of u(e))!f.call(t,a)&&a!==o&&r(t,a,{get:()=>e[a],enumerable:!(i=m(e,a))||i.enumerable});return t};var b=(t,e,o)=>(o=t!=null?h(g(t)):{},s(e||!t||!t.__esModule?r(o,\"default\",{value:t,enumerable:!0}):o,t)),y=t=>s(r({},\"__esModule\",{value:!0}),t);var c=p((j,l)=>{l.exports=_jsx_runtime});var C={};w(C,{default:()=>x,frontmatter:()=>v});var n=b(c()),v={title:\"Release is going to PlatformCon 2023  \",summary:\"Join Release for the exciting two days of learning and discovery at PlatformCon 2023 on June 8th and 9th.\",publishDate:\"Wed Jun 07 2023 03:30:50 GMT+0000 (Coordinated Universal Time)\",author:\"ira-casteel\",readingTime:2,categories:[\"platform-engineering\",\"events\",\"kubernetes\"],mainImage:\"/blog-images/792d7efa495a12dd5f3699b5a09d5d7a.jpg\",imageAlt:\"Release is going to PlatformCon 2023  \",showCTA:!0,ctaCopy:\"Streamline collaboration and accelerate deployment with Release's on-demand environments, mirroring production setups for efficient testing and seamless bug resolution.\",ctaLink:\"https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=release-is-going-to-platformcon-2023\",relatedPosts:[\"12-things-you-didnt-know-you-could-do-with-release-part-1\"],ogImage:\"/blog-images/792d7efa495a12dd5f3699b5a09d5d7a.jpg\",excerpt:\"Join Release for the exciting two days of learning and discovery at PlatformCon 2023 on June 8th and 9th.\",tags:[\"platform-engineering\",\"events\",\"kubernetes\"],ctaButton:\"Try Release for Free\"};function d(t){let e=Object.assign({p:\"p\",a:\"a\"},t.components);return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsxs)(e.p,{children:[\"Join Release for the exciting two days of learning and discovery at \",(0,n.jsx)(e.a,{href:\"https://platformcon.com/\",children:\"PlatformCon 2023\"}),\" on June 8th and 9th. This is the second year of PlatformCon bringing the community of Platform Engineers, DevOps Practitioners and Technology Leaders together to discuss and share insights around the growing field of Platform Engineering. And again it is a free online event, so make sure to sign up and join the discussion.\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"This years\\u2019 \",(0,n.jsx)(e.a,{href:\"https://platformcon.com/talks\",children:\"talks\"}),\" are organized into five categories: Stories, Tech, Culture, Blueprints and Impact, and sessions across the all tracks are quite impressive. We are looking forward to learning about the history and humble beginnings of containers from Micheal Irvin, a DevRel lead at \",(0,n.jsx)(e.a,{href:\"https://www.docker.com/\",children:\"Docker\"}),\", in his talk \\u201C\",(0,n.jsx)(e.a,{href:\"https://platformcon.com/talks/containers-where-we-came-from-and-where-the-futures-taking-us\",children:\"Containers: Where we came from and where the future's taking us\"}),\"\\u201D. Exploring the complexities of today's software and distributed systems with Gregor Hohpe, a Director of Enterprise Strategy at \",(0,n.jsx)(e.a,{href:\"https://aws.amazon.com/\",children:\"AWS\"}),\", in his talk \\u201C\",(0,n.jsx)(e.a,{href:\"https://platformcon.com/talks/build-abstractions-not-illusions\",children:\"Building Abstractions, not illusions\"}),\"\\u201D. And learning valuable practitioner lessons with Reynis Vazquez-Guzman, Sr. Software Engineer at \",(0,n.jsx)(e.a,{href:\"https://www.affirm.com/\",children:\"Affirm\"}),\", in her \\u201C\\u200B\\u200B\",(0,n.jsx)(e.a,{href:\"https://platformcon.com/talks/where-we-went-wrong-in-building-a-self-service-platform-on-kubernetes\",children:\"Where we went wrong in building a self-service platform on Kubernetes\"}),\"\\u201D talk.\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"Our own Nick Busey will be sharing his insights around evolving developer experiences. He will discuss \\xA0on-demand environments as a way of providing teams with a fulfilling development experience. In particular, how you build systems to maintain the excitement and ease of collaboration, as you build out your product in his talk \\u201C\",(0,n.jsx)(e.a,{href:\"https://platformcon.com/talks/from-skateboard-to-car-how-to-evolve-your-developer-experience\",children:\"From skateboard to car: How to evolve your developer experience\"}),\"\\u201D on June 8th, in the Blueprints track.\"]}),`\n`,(0,n.jsx)(e.p,{children:\"What sessions are you most looking forward to? See you there!\"})]})}function k(t={}){let{wrapper:e}=t.components||{};return e?(0,n.jsx)(e,Object.assign({},t,{children:(0,n.jsx)(d,t)})):d(t)}var x=k;return y(C);})();\n;return Component;"
        },
        "_id": "blog/posts/release-is-going-to-platformcon-2023.mdx",
        "_raw": {
          "sourceFilePath": "blog/posts/release-is-going-to-platformcon-2023.mdx",
          "sourceFileName": "release-is-going-to-platformcon-2023.mdx",
          "sourceFileDir": "blog/posts",
          "contentType": "mdx",
          "flattenedPath": "blog/posts/release-is-going-to-platformcon-2023"
        },
        "type": "BlogPost",
        "computedSlug": "release-is-going-to-platformcon-2023"
      },
      "documentHash": "1739393595026",
      "hasWarnings": false,
      "documentTypeName": "BlogPost"
    },
    "blog/posts/release-is-going-to-railsconf-2023.mdx": {
      "document": {
        "title": "Release is going to RailsConf 2023",
        "summary": "RailsConf 2023 is almost here! We will be showing off some exciting additions to the Release Development Platform.",
        "publishDate": "Fri Apr 14 2023 18:07:19 GMT+0000 (Coordinated Universal Time)",
        "author": "erik-landerholm",
        "readingTime": 2,
        "categories": [
          "platform-engineering",
          "kubernetes",
          "ai"
        ],
        "mainImage": "/blog-images/4913678b8a8bea1cd41e57f2115805bb.jpg",
        "imageAlt": "Release is going to RailsConf 2023",
        "showCTA": true,
        "ctaCopy": "Empower your Rails projects with AI-driven cloud deployment on Release for seamless environment management.",
        "ctaLink": "https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=release-is-going-to-railsconf-2023",
        "relatedPosts": [
          ""
        ],
        "ogImage": "/blog-images/4913678b8a8bea1cd41e57f2115805bb.jpg",
        "excerpt": "RailsConf 2023 is almost here! We will be showing off some exciting additions to the Release Development Platform.",
        "tags": [
          "platform-engineering",
          "kubernetes",
          "ai"
        ],
        "ctaButton": "Try Release for Free",
        "body": {
          "raw": "\n[RailsConf 2023](https://railsconf.org/) is almost here and we at Release are delighted to be attending and showing off some really exciting additions to the Release Development Platform.  We love attending RailsConf to hear amazing talks and show people all the cool things we are working on to support developers and teams as they use Rails to create amazing products.  \n\nFounded in 2019 Release was and still is built almost entirely with [Rails](https://rubycentral.org)!  The founding team has 30+ years of Rails experience and Rails has served us perfectly in our drive to create a development platform that serves the modern development organization.\n\nOur vision at Release is for all developers and teams to use the cloud to its full potential without needing to be experts in everything. We do this by simplifying the creation, access and usage of all environments (development, ephemeral, testing, staging, production, etc.), while maintaining access to advanced tooling.\n\nThis year we will be demoing something we are super excited about!  Our newest “Code to Cloud” feature was developed using AI to get your applications running on [AWS](https://docs.release.com/integrations/integrations-overview/aws-integration/aws-howtos) or [GCP](https://docs.release.com/integrations/integrations-overview/gcp-integration) quickly!  Release now gets your Rails project into the cloud without you needing to be docker, k8s, terraform, buildpacks, helm or even cloud specialist. \n\nWe use advanced AI to interrogate your repositories and generate all the necessary configuration (dockerfiles, k8s manifests, etc.) with very little intervention and iteration on your part.  We smooth out the learning curve of moving from something like heroku to AWS while allowing you to retain the simplicity of heroku with the power of AWS!\n\nIf you are looking for ways to increase developer productivity through a truly modern cloud agnostic platform or just want to see a cool demo of getting a Rails repository running in AWS on EKS, come see us at our booth! We love chatting with developers, learning about their successes and challenges (comparing notes) and making teams as productive as possible.  See you there!\n",
          "code": "var Component=(()=>{var d=Object.create;var a=Object.defineProperty;var m=Object.getOwnPropertyDescriptor;var h=Object.getOwnPropertyNames;var u=Object.getPrototypeOf,p=Object.prototype.hasOwnProperty;var f=(t,e)=>()=>(e||t((e={exports:{}}).exports,e),e.exports),w=(t,e)=>{for(var n in e)a(t,n,{get:e[n],enumerable:!0})},r=(t,e,n,s)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let i of h(e))!p.call(t,i)&&i!==n&&a(t,i,{get:()=>e[i],enumerable:!(s=m(e,i))||s.enumerable});return t};var v=(t,e,n)=>(n=t!=null?d(u(t)):{},r(e||!t||!t.__esModule?a(n,\"default\",{value:t,enumerable:!0}):n,t)),y=t=>r(a({},\"__esModule\",{value:!0}),t);var c=f((j,l)=>{l.exports=_jsx_runtime});var x={};w(x,{default:()=>k,frontmatter:()=>b});var o=v(c()),b={title:\"Release is going to RailsConf 2023\",summary:\"RailsConf 2023 is almost here! We will be showing off some exciting additions to the Release Development Platform.\",publishDate:\"Fri Apr 14 2023 18:07:19 GMT+0000 (Coordinated Universal Time)\",author:\"erik-landerholm\",readingTime:2,categories:[\"platform-engineering\",\"kubernetes\",\"ai\"],mainImage:\"/blog-images/4913678b8a8bea1cd41e57f2115805bb.jpg\",imageAlt:\"Release is going to RailsConf 2023\",showCTA:!0,ctaCopy:\"Empower your Rails projects with AI-driven cloud deployment on Release for seamless environment management.\",ctaLink:\"https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=release-is-going-to-railsconf-2023\",relatedPosts:[\"\"],ogImage:\"/blog-images/4913678b8a8bea1cd41e57f2115805bb.jpg\",excerpt:\"RailsConf 2023 is almost here! We will be showing off some exciting additions to the Release Development Platform.\",tags:[\"platform-engineering\",\"kubernetes\",\"ai\"],ctaButton:\"Try Release for Free\"};function g(t){let e=Object.assign({p:\"p\",a:\"a\"},t.components);return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.a,{href:\"https://railsconf.org/\",children:\"RailsConf 2023\"}),\" is almost here and we at Release are delighted to be attending and showing off some really exciting additions to the Release Development Platform.\\xA0 We love attending RailsConf to hear amazing talks and show people all the cool things we are working on to support developers and teams as they use Rails to create amazing products.\\xA0\\xA0\"]}),`\n`,(0,o.jsxs)(e.p,{children:[\"Founded in 2019 Release was and still is built almost entirely with \",(0,o.jsx)(e.a,{href:\"https://rubycentral.org\",children:\"Rails\"}),\"!\\xA0 The founding team has 30+ years of Rails experience and Rails has served us perfectly in our drive to create a development platform that serves the modern development organization.\"]}),`\n`,(0,o.jsx)(e.p,{children:\"Our vision at Release is for all developers and teams to use the cloud to its full potential without needing to be experts in everything. We do this by simplifying the creation, access and usage of all environments (development, ephemeral, testing, staging, production, etc.), while maintaining access to advanced tooling.\"}),`\n`,(0,o.jsxs)(e.p,{children:[\"This year we will be demoing something we are super excited about!\\xA0 Our newest \\u201CCode to Cloud\\u201D feature was developed using AI to get your applications running on \",(0,o.jsx)(e.a,{href:\"https://docs.release.com/integrations/integrations-overview/aws-integration/aws-howtos\",children:\"AWS\"}),\" or \",(0,o.jsx)(e.a,{href:\"https://docs.release.com/integrations/integrations-overview/gcp-integration\",children:\"GCP\"}),\" quickly!\\xA0 Release now gets your Rails project into the cloud without you needing to be docker, k8s, terraform, buildpacks, helm or even cloud specialist.\\xA0\"]}),`\n`,(0,o.jsx)(e.p,{children:\"We use advanced AI to interrogate your repositories and generate all the necessary configuration (dockerfiles, k8s manifests, etc.) with very little intervention and iteration on your part.\\xA0 We smooth out the learning curve of moving from something like heroku to AWS while allowing you to retain the simplicity of heroku with the power of AWS!\"}),`\n`,(0,o.jsx)(e.p,{children:\"If you are looking for ways to increase developer productivity through a truly modern cloud agnostic platform or just want to see a cool demo of getting a Rails repository running in AWS on EKS, come see us at our booth! We love chatting with developers, learning about their successes and challenges (comparing notes) and making teams as productive as possible.\\xA0 See you there!\"})]})}function R(t={}){let{wrapper:e}=t.components||{};return e?(0,o.jsx)(e,Object.assign({},t,{children:(0,o.jsx)(g,t)})):g(t)}var k=R;return y(x);})();\n;return Component;"
        },
        "_id": "blog/posts/release-is-going-to-railsconf-2023.mdx",
        "_raw": {
          "sourceFilePath": "blog/posts/release-is-going-to-railsconf-2023.mdx",
          "sourceFileName": "release-is-going-to-railsconf-2023.mdx",
          "sourceFileDir": "blog/posts",
          "contentType": "mdx",
          "flattenedPath": "blog/posts/release-is-going-to-railsconf-2023"
        },
        "type": "BlogPost",
        "computedSlug": "release-is-going-to-railsconf-2023"
      },
      "documentHash": "1739393595026",
      "hasWarnings": false,
      "documentTypeName": "BlogPost"
    },
    "blog/posts/release-joins-nvidia-inception-partner-program-brings-ai-environments-to-nvidia-gpus-and-stack.mdx": {
      "document": {
        "title": "Release joins NVIDIA Inception Partner Program; Brings AI Environments to NVIDIA GPUs and Stack",
        "summary": "We’re excited to announce that Release has joined the NVIDIA Inception Partner Program.",
        "publishDate": "Wed Feb 14 2024 17:18:19 GMT+0000 (Coordinated Universal Time)",
        "author": "matt-carter",
        "readingTime": 3,
        "categories": [
          "ai",
          "nvidia",
          "platform-engineering"
        ],
        "mainImage": "/blog-images/52a1e496f04e19e587af858e51395e77.webp",
        "imageAlt": "Release joins NVIDIA Inception Partner Program; Brings AI Environments to NVIDIA GPUs and Stack",
        "showCTA": true,
        "ctaCopy": "Unlock seamless AI environment integration with NVIDIA GPUs using Release's ephemeral environments for optimized development and deployment.",
        "ctaLink": "https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=release-joins-nvidia-inception-partner-program-brings-ai-environments-to-nvidia-gpus-and-stack",
        "relatedPosts": [
          ""
        ],
        "ogImage": "/blog-images/52a1e496f04e19e587af858e51395e77.webp",
        "excerpt": "We’re excited to announce that Release has joined the NVIDIA Inception Partner Program.",
        "tags": [
          "ai",
          "nvidia",
          "platform-engineering"
        ],
        "ctaButton": "Try Release for Free",
        "body": {
          "raw": "\nWe’re excited to [announce that Release has joined the NVIDIA Inception Partner Program.](/press-releases/release-joins-nvidia-inception-to-bring-ai-environments-tailored-for-nvidia-stack-and-gpus) The NVIDIA Inception program is an ecosystem of startups working on AI and data science with NVIDIA GPUs. As developer and data science teams begin to build applications and train data sets with NVIDIA NeMo and GPUs, [environments as a service](https://www.youtube.com/watch?v=IKajKw7-Ajs) is a perfect complement to the success and speed of teams wanting to focus on data insights, not complex infrastructure.\n\nAs an Inception Partner, we are working closely with NVIDIA to make sure we are delivering the right capabilities to teams using NVIDIA GPUs and their data science and AI/ML stacks. NVIDIA is the latest partnership for Release, which spans the entire application delivery and deployment lifecycle. We’re committed to integrations with all of the tools our customers use, from repositories and CI/CD tooling to cloud platform vendors and consulting services. \n\nBut why does this matter to you? For starters, it means seamless integration with your preferred cloud platform, ensuring optimal performance without any compatibility headaches. Additionally, our partnerships provide access to valuable resources and support, streamlining deployment and troubleshooting. And let's not forget about staying at the forefront of innovation – with our connections, you'll always be in tune with the latest advancements and trends in technology. \n\nAlong with NVIDIA, Release is working with companies like:\n\n- AWS\n- Google Cloud Platform\n- Docker\n- Tonic\n- Codingscape\n\nYou can get a full list of our partners at release.com/partners\n\nIs there a company you would like to see as a partner? Let us know! Email [partner@release.com](mailto:partner@release.com) and tell us who we should work with.\n",
          "code": "var Component=(()=>{var m=Object.create;var o=Object.defineProperty;var p=Object.getOwnPropertyDescriptor;var h=Object.getOwnPropertyNames;var u=Object.getPrototypeOf,g=Object.prototype.hasOwnProperty;var I=(n,e)=>()=>(e||n((e={exports:{}}).exports,e),e.exports),f=(n,e)=>{for(var a in e)o(n,a,{get:e[a],enumerable:!0})},s=(n,e,a,r)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let i of h(e))!g.call(n,i)&&i!==a&&o(n,i,{get:()=>e[i],enumerable:!(r=p(e,i))||r.enumerable});return n};var w=(n,e,a)=>(a=n!=null?m(u(n)):{},s(e||!n||!n.__esModule?o(a,\"default\",{value:n,enumerable:!0}):a,n)),A=n=>s(o({},\"__esModule\",{value:!0}),n);var c=I((j,l)=>{l.exports=_jsx_runtime});var D={};f(D,{default:()=>b,frontmatter:()=>v});var t=w(c()),v={title:\"Release joins NVIDIA Inception Partner Program; Brings AI Environments to NVIDIA GPUs and Stack\",summary:\"We\\u2019re excited to announce that Release has joined the NVIDIA Inception Partner Program.\",publishDate:\"Wed Feb 14 2024 17:18:19 GMT+0000 (Coordinated Universal Time)\",author:\"matt-carter\",readingTime:3,categories:[\"ai\",\"nvidia\",\"platform-engineering\"],mainImage:\"/blog-images/52a1e496f04e19e587af858e51395e77.webp\",imageAlt:\"Release joins NVIDIA Inception Partner Program; Brings AI Environments to NVIDIA GPUs and Stack\",showCTA:!0,ctaCopy:\"Unlock seamless AI environment integration with NVIDIA GPUs using Release's ephemeral environments for optimized development and deployment.\",ctaLink:\"https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=release-joins-nvidia-inception-partner-program-brings-ai-environments-to-nvidia-gpus-and-stack\",relatedPosts:[\"\"],ogImage:\"/blog-images/52a1e496f04e19e587af858e51395e77.webp\",excerpt:\"We\\u2019re excited to announce that Release has joined the NVIDIA Inception Partner Program.\",tags:[\"ai\",\"nvidia\",\"platform-engineering\"],ctaButton:\"Try Release for Free\"};function d(n){let e=Object.assign({p:\"p\",a:\"a\",ul:\"ul\",li:\"li\"},n.components);return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsxs)(e.p,{children:[\"We\\u2019re excited to \",(0,t.jsx)(e.a,{href:\"/press-releases/release-joins-nvidia-inception-to-bring-ai-environments-tailored-for-nvidia-stack-and-gpus\",children:\"announce that Release has joined the NVIDIA Inception Partner Program.\"}),\" The NVIDIA Inception program is an ecosystem of startups working on AI and data science with NVIDIA GPUs. As developer and data science teams begin to build applications and train data sets with NVIDIA NeMo and GPUs, \",(0,t.jsx)(e.a,{href:\"https://www.youtube.com/watch?v=IKajKw7-Ajs\",children:\"environments as a service\"}),\" is a perfect complement to the success and speed of teams wanting to focus on data insights, not complex infrastructure.\"]}),`\n`,(0,t.jsx)(e.p,{children:\"As an Inception Partner, we are working closely with NVIDIA to make sure we are delivering the right capabilities to teams using NVIDIA GPUs and their data science and AI/ML stacks. NVIDIA is the latest partnership for Release, which spans the entire application delivery and deployment lifecycle. We\\u2019re committed to integrations with all of the tools our customers use, from repositories and CI/CD tooling to cloud platform vendors and consulting services.\\xA0\"}),`\n`,(0,t.jsx)(e.p,{children:\"But why does this matter to you? For starters, it means seamless integration with your preferred cloud platform, ensuring optimal performance without any compatibility headaches. Additionally, our partnerships provide access to valuable resources and support, streamlining deployment and troubleshooting. And let's not forget about staying at the forefront of innovation \\u2013 with our connections, you'll always be in tune with the latest advancements and trends in technology.\\xA0\"}),`\n`,(0,t.jsx)(e.p,{children:\"Along with NVIDIA, Release is working with companies like:\"}),`\n`,(0,t.jsxs)(e.ul,{children:[`\n`,(0,t.jsx)(e.li,{children:\"AWS\"}),`\n`,(0,t.jsx)(e.li,{children:\"Google Cloud Platform\"}),`\n`,(0,t.jsx)(e.li,{children:\"Docker\"}),`\n`,(0,t.jsx)(e.li,{children:\"Tonic\"}),`\n`,(0,t.jsx)(e.li,{children:\"Codingscape\"}),`\n`]}),`\n`,(0,t.jsx)(e.p,{children:\"You can get a full list of our partners at release.com/partners\"}),`\n`,(0,t.jsxs)(e.p,{children:[\"Is there a company you would like to see as a partner? Let us know! Email \",(0,t.jsx)(e.a,{href:\"mailto:partner@release.com\",children:\"partner@release.com\"}),\" and tell us who we should work with.\"]})]})}function y(n={}){let{wrapper:e}=n.components||{};return e?(0,t.jsx)(e,Object.assign({},n,{children:(0,t.jsx)(d,n)})):d(n)}var b=y;return A(D);})();\n;return Component;"
        },
        "_id": "blog/posts/release-joins-nvidia-inception-partner-program-brings-ai-environments-to-nvidia-gpus-and-stack.mdx",
        "_raw": {
          "sourceFilePath": "blog/posts/release-joins-nvidia-inception-partner-program-brings-ai-environments-to-nvidia-gpus-and-stack.mdx",
          "sourceFileName": "release-joins-nvidia-inception-partner-program-brings-ai-environments-to-nvidia-gpus-and-stack.mdx",
          "sourceFileDir": "blog/posts",
          "contentType": "mdx",
          "flattenedPath": "blog/posts/release-joins-nvidia-inception-partner-program-brings-ai-environments-to-nvidia-gpus-and-stack"
        },
        "type": "BlogPost",
        "computedSlug": "release-joins-nvidia-inception-partner-program-brings-ai-environments-to-nvidia-gpus-and-stack"
      },
      "documentHash": "1739393595026",
      "hasWarnings": false,
      "documentTypeName": "BlogPost"
    },
    "blog/posts/release-now-available-through-the-aws-marketplace.mdx": {
      "document": {
        "title": "Release now available through the AWS Marketplace",
        "summary": "We have heard from many customers that they prefer to buy software through the AWS Marketplace, and this announcement me",
        "publishDate": "Mon Nov 21 2022 18:09:58 GMT+0000 (Coordinated Universal Time)",
        "author": "tommy-mcclung",
        "readingTime": 1,
        "categories": [
          "platform-engineering",
          "product"
        ],
        "mainImage": "/blog-images/35d3a8c5eb5d8692679503d011fee3da.svg",
        "imageAlt": "Release logo",
        "showCTA": true,
        "ctaCopy": "Unlock streamlined app deployment in AWS Marketplace with Release's ephemeral environments for faster testing and deployment.",
        "ctaLink": "https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=release-now-available-through-the-aws-marketplace",
        "relatedPosts": [
          ""
        ],
        "ogImage": "/blog-images/35d3a8c5eb5d8692679503d011fee3da.svg",
        "excerpt": "We have heard from many customers that they prefer to buy software through the AWS Marketplace, and this announcement me",
        "tags": [
          "platform-engineering",
          "product"
        ],
        "ctaButton": "Try Release for Free",
        "body": {
          "raw": "\nI’m excited to announce that customers can now purchase Release through the [AWS Marketplace.](https://aws.amazon.com/marketplace/pp/prodview-faxqlvzq65fea) We have heard from many customers that they prefer to buy software through the AWS Marketplace, and this announcement means you can buy Release in the way you and your company prefer.\n\nIf you are unfamiliar with the [AWS Marketplace](https://aws.amazon.com/marketplace), it provides a number of compelling benefits for AWS customers. It streamlines procurement and accelerates the vendor onboarding process by allowing organizations to leverage existing agreements with AWS. It enables quicker self-service transactions through the marketplace portal. And finally, it allows customers to fulfill a portion of their contractual AWS spend commitment, potentially attaining deeper AWS discounts. Specifically, 50% of customer's Release spend counts toward their total AWS spend commitment when transacted via the AWS Marketplace.\n\nAWS Marketplace is the latest addition to our deep commitment to and partnership with AWS. Release helps AWS customers build, test, and deploy their apps with speed and confidence, and this further accelerates how AWS customers can use Release in their app pipelines. If you or your team are looking for more details on our support for AWS apps, you can find additional details [in this white-paper.](https://release.com/whitepaper/aws-releasehub-for-you)\n",
          "code": "var Component=(()=>{var h=Object.create;var o=Object.defineProperty;var p=Object.getOwnPropertyDescriptor;var u=Object.getOwnPropertyNames;var d=Object.getPrototypeOf,f=Object.prototype.hasOwnProperty;var g=(t,e)=>()=>(e||t((e={exports:{}}).exports,e),e.exports),y=(t,e)=>{for(var n in e)o(t,n,{get:e[n],enumerable:!0})},i=(t,e,n,s)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let r of u(e))!f.call(t,r)&&r!==n&&o(t,r,{get:()=>e[r],enumerable:!(s=p(e,r))||s.enumerable});return t};var w=(t,e,n)=>(n=t!=null?h(d(t)):{},i(e||!t||!t.__esModule?o(n,\"default\",{value:t,enumerable:!0}):n,t)),b=t=>i(o({},\"__esModule\",{value:!0}),t);var l=g((M,c)=>{c.exports=_jsx_runtime});var k={};y(k,{default:()=>S,frontmatter:()=>A});var a=w(l()),A={title:\"Release now available through the AWS Marketplace\",summary:\"We have heard from many customers that they prefer to buy software through the AWS Marketplace, and this announcement me\",publishDate:\"Mon Nov 21 2022 18:09:58 GMT+0000 (Coordinated Universal Time)\",author:\"tommy-mcclung\",readingTime:1,categories:[\"platform-engineering\",\"product\"],mainImage:\"/blog-images/35d3a8c5eb5d8692679503d011fee3da.svg\",imageAlt:\"Release logo\",showCTA:!0,ctaCopy:\"Unlock streamlined app deployment in AWS Marketplace with Release's ephemeral environments for faster testing and deployment.\",ctaLink:\"https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=release-now-available-through-the-aws-marketplace\",relatedPosts:[\"\"],ogImage:\"/blog-images/35d3a8c5eb5d8692679503d011fee3da.svg\",excerpt:\"We have heard from many customers that they prefer to buy software through the AWS Marketplace, and this announcement me\",tags:[\"platform-engineering\",\"product\"],ctaButton:\"Try Release for Free\"};function m(t){let e=Object.assign({p:\"p\",a:\"a\"},t.components);return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsxs)(e.p,{children:[\"I\\u2019m excited to announce that customers can now purchase Release through the \",(0,a.jsx)(e.a,{href:\"https://aws.amazon.com/marketplace/pp/prodview-faxqlvzq65fea\",children:\"AWS Marketplace.\"}),\" We have heard from many customers that they prefer to buy software through the AWS Marketplace, and this announcement means you can buy Release in the way you and your company prefer.\"]}),`\n`,(0,a.jsxs)(e.p,{children:[\"If you are unfamiliar with the \",(0,a.jsx)(e.a,{href:\"https://aws.amazon.com/marketplace\",children:\"AWS Marketplace\"}),\", it provides a number of compelling benefits for AWS customers. It streamlines procurement and accelerates the vendor onboarding process by allowing organizations to leverage existing agreements with AWS. It enables quicker self-service transactions through the marketplace portal. And finally, it allows customers to fulfill a portion of their contractual AWS spend commitment, potentially attaining deeper AWS discounts. Specifically, 50% of customer's Release spend counts toward their total AWS spend commitment when transacted via the AWS Marketplace.\"]}),`\n`,(0,a.jsxs)(e.p,{children:[\"AWS Marketplace is the latest addition to our deep commitment to and partnership with AWS. Release helps AWS customers build, test, and deploy their apps with speed and confidence, and this further accelerates how AWS customers can use Release in their app pipelines. If you or your team are looking for more details on our support for AWS apps, you can find additional details \",(0,a.jsx)(e.a,{href:\"https://release.com/whitepaper/aws-releasehub-for-you\",children:\"in this white-paper.\"})]})]})}function W(t={}){let{wrapper:e}=t.components||{};return e?(0,a.jsx)(e,Object.assign({},t,{children:(0,a.jsx)(m,t)})):m(t)}var S=W;return b(k);})();\n;return Component;"
        },
        "_id": "blog/posts/release-now-available-through-the-aws-marketplace.mdx",
        "_raw": {
          "sourceFilePath": "blog/posts/release-now-available-through-the-aws-marketplace.mdx",
          "sourceFileName": "release-now-available-through-the-aws-marketplace.mdx",
          "sourceFileDir": "blog/posts",
          "contentType": "mdx",
          "flattenedPath": "blog/posts/release-now-available-through-the-aws-marketplace"
        },
        "type": "BlogPost",
        "computedSlug": "release-now-available-through-the-aws-marketplace"
      },
      "documentHash": "1739393595026",
      "hasWarnings": false,
      "documentTypeName": "BlogPost"
    },
    "blog/posts/release-offers-support-for-ecs.mdx": {
      "document": {
        "title": "Release Offers Support for ECS",
        "summary": "Release now supports the use of IaC (Infrastructure as Code: Terraform, Pulumi, etc) to create resources in ECS",
        "publishDate": "Tue Nov 29 2022 18:35:10 GMT+0000 (Coordinated Universal Time)",
        "author": "erik-landerholm",
        "readingTime": 4,
        "categories": [
          "platform-engineering",
          "kubernetes"
        ],
        "mainImage": "/blog-images/24e9b4247f1c58e1a7bc5c20c67777fb.jpg",
        "imageAlt": "A group of colorful containers",
        "showCTA": true,
        "ctaCopy": "Empower efficient ECS resource management with Release's unified interface for seamless deployment and monitoring.",
        "ctaLink": "https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=release-offers-support-for-ecs",
        "relatedPosts": [
          ""
        ],
        "ogImage": "/blog-images/24e9b4247f1c58e1a7bc5c20c67777fb.jpg",
        "excerpt": "Release now supports the use of IaC (Infrastructure as Code: Terraform, Pulumi, etc) to create resources in ECS",
        "tags": [
          "platform-engineering",
          "kubernetes"
        ],
        "ctaButton": "Try Release for Free",
        "body": {
          "raw": "\nComplex applications on AWS frequently combine not only the core code but also connections to resources and dependencies (such as ECS Tasks and Lambdas). We're excited to announce support for ECS tasks! Development, testing, and production of complex apps can now be managed through our newest Release update. This latest update includes support for the use of IaC (Infrastructure as Code, for example, Terraform and Pulumi) to create resources in ECS, which you can manage in Release right alongside your EKS resources. In this post, we'll show you some simple set-up steps to add ECS resources to Release, access them through the terminal, and view their logs.\n\nRelease is a Kubernetes-first platform, but we have many customers that have described their environments in detail with an IaC and would like their environments to match their production environments as closely as possible. With our support for ECS you can now view and manage your ECS and EKS resources through Release's single administration interface. Release automatically tags and configures your ECS resources if you use our Infrastructure Runner, but we also allow you to tag and configure any ECS resources and manage them in Release, regardless of how they were deployed.\n\n### AWS ECS Task Support\n\nThe first supported cloud resources are AWS ECS Tasks, but more resource types will be supported soon. Let us know what cloud resources you'd like to see supported!\n\n### Cloud Resources\n\nAfter tagging a resource, it will show up on the Environment page in the Cloud Resources section, below your Instances.\n\n![](/blog-images/4734bb86d72d86a67606a7548656013e.png)\n\n_  \nCloud Resources like this ECS Task are listed on the environment page in a section below Instances. ECS Tasks support Terminal and Logs, which you launch by clicking the corresponding button._\n\n### Interacting with Cloud Resources\n\nCurrently, once you've tagged them, ECS Tasks support a couple of actions: Terminal and Logs (note the Terminal and Logs buttons in the previous screenshot).\n\nYou can launch a web terminal into the task containers by clicking the **Terminal** button:\n\n![](/blog-images/fbf49694fb352b4223c12dc6f30ea402.png)\n\nAnd you can view container logs by clicking the **Logs** button:\n\n![](/blog-images/64a479f61dc2158839d8f73c601a1be9.png)\n\n### How to add Cloud Resources\n\nTo add a cloud resource to your Release environments, you'll need to add a couple of tags/labels to that resource in AWS or GCP.\n\nTwo tags with values are required:\n\n| Tag Key               | Tag Value                | Location in UI                                                                             | Env variable                                                   |\n| --------------------- | ------------------------ | ------------------------------------------------------------------------------------------ | -------------------------------------------------------------- |\n| releasehub.com/app    | name                     | Top of the environment page. In the screenshot below, the app name is `example-voting-app` | `RELEASE_APP_NAME` (see default Release environment variables) |\n| releasehub.com/env-id | (the environment handle) | In the screenshot below, the environment handle is `ted3bff`                               | `RELEASE_ENV_ID` (see default Release environment variables)   |\n\n![](/blog-images/ca4729f2662588d2986369bd3eab969f.png)\n\nAWS Documentation can be found [here](https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs-using-tags.html).\n\n### How to Configure the Logs\n\nIn order to send your ECS task logs to the Release log viewer, you must configure your ECS tasks to use the awslogs log driver:\n\nYou can specify the log driver in the task definition. Inside each entry in containerDefinitions, update (or add) the logConfiguration key to specify the logDriver and options. Options must specify awslogs-group, and awslogs-stream-prefix. The logDriver must be awslogs, but you may choose your own group and stream-prefix.\n\nThat section of your task definition should look something like this when you're done:\n\n```yaml\n{\n  \"containerDefinitions\":\n    [\n      {\n        \"logConfiguration\":\n          {\n            \"logDriver\": \"awslogs\",\n            \"options\":\n              {\n                \"awslogs-group\": \"firelens-container\",\n                \"awslogs-region\": \"us-west-2\",\n                \"awslogs-create-group\": \"true\",\n                \"awslogs-stream-prefix\": \"firelens\",\n              },\n          },\n      },\n    ],\n}\n```\n\nFor more details, check out the [AWS documentation on using the awslogs log driver](https://docs.aws.amazon.com/AmazonECS/latest/developerguide/using_awslogs.html#specify-log-config).\n\n### How to Configure the Terminal\n\nThe following requirements must be met before being able to terminal into any ECS task:\n\n1.  ECS Exec needs to be enabled for the task. ([more info](https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs-exec.html#ecs-exec-enabling))\n2.  ECS Exec IAM permissions need to be added for the task. ([more info](https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs-exec.html#ecs-exec-required-iam-permissions))\n\nFor more details, check out the [AWS documentation on enabling ECS Exec](https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs-exec.html) and the best practices around it.\n\nAfter configuring the ECS task, use the Cloud Resources refresh button and you should be able to choose the container to terminal into. Note that Release will gray out the terminal button for containers that are not running or tasks that do not have ECS Exec enabled.\n\n![](/blog-images/3d39dc71386de1d85df54a097a497bd3.png)\n\n### Try it Yourself\n\nAt Release we always try to support the infrastructure our customers have in place while allowing them to take advantage of any new offerings from AWS. If you would like to manage your containers, regardless of which container management service you use, read through our [documentation](https://docs.releasehub.com/reference-documentation/cloud-resources) and/or schedule a [demo](https://releasehub.com/book-a-demo)!\n",
          "code": "var Component=(()=>{var h=Object.create;var a=Object.defineProperty;var u=Object.getOwnPropertyDescriptor;var m=Object.getOwnPropertyNames;var g=Object.getPrototypeOf,p=Object.prototype.hasOwnProperty;var f=(o,e)=>()=>(e||o((e={exports:{}}).exports,e),e.exports),b=(o,e)=>{for(var t in e)a(o,t,{get:e[t],enumerable:!0})},i=(o,e,t,s)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let r of m(e))!p.call(o,r)&&r!==t&&a(o,r,{get:()=>e[r],enumerable:!(s=u(e,r))||s.enumerable});return o};var w=(o,e,t)=>(t=o!=null?h(g(o)):{},i(e||!o||!o.__esModule?a(t,\"default\",{value:o,enumerable:!0}):t,o)),y=o=>i(a({},\"__esModule\",{value:!0}),o);var l=f((T,c)=>{c.exports=_jsx_runtime});var v={};b(v,{default:()=>k,frontmatter:()=>C});var n=w(l()),C={title:\"Release Offers Support for ECS\",summary:\"Release now supports the use of IaC (Infrastructure as Code: Terraform, Pulumi, etc) to create resources in ECS\",publishDate:\"Tue Nov 29 2022 18:35:10 GMT+0000 (Coordinated Universal Time)\",author:\"erik-landerholm\",readingTime:4,categories:[\"platform-engineering\",\"kubernetes\"],mainImage:\"/blog-images/24e9b4247f1c58e1a7bc5c20c67777fb.jpg\",imageAlt:\"A group of colorful containers\",showCTA:!0,ctaCopy:\"Empower efficient ECS resource management with Release's unified interface for seamless deployment and monitoring.\",ctaLink:\"https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=release-offers-support-for-ecs\",relatedPosts:[\"\"],ogImage:\"/blog-images/24e9b4247f1c58e1a7bc5c20c67777fb.jpg\",excerpt:\"Release now supports the use of IaC (Infrastructure as Code: Terraform, Pulumi, etc) to create resources in ECS\",tags:[\"platform-engineering\",\"kubernetes\"],ctaButton:\"Try Release for Free\"};function d(o){let e=Object.assign({p:\"p\",h3:\"h3\",a:\"a\",span:\"span\",img:\"img\",br:\"br\",strong:\"strong\",table:\"table\",thead:\"thead\",tr:\"tr\",th:\"th\",tbody:\"tbody\",td:\"td\",code:\"code\",pre:\"pre\",ol:\"ol\",li:\"li\"},o.components);return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(e.p,{children:\"Complex applications on AWS frequently combine not only the core code but also connections to resources and dependencies (such as ECS Tasks and Lambdas). We're excited to announce support for ECS tasks! Development, testing, and production of complex apps can now be managed through our newest Release update. This latest update includes support for the use of IaC (Infrastructure as Code, for example, Terraform and Pulumi) to create resources in ECS, which you can manage in Release right alongside your EKS resources. In this post, we'll show you some simple set-up steps to add ECS resources to Release, access them through the terminal, and view their logs.\"}),`\n`,(0,n.jsx)(e.p,{children:\"Release is a Kubernetes-first platform, but we have many customers that have described their environments in detail with an IaC and would like their environments to match their production environments as closely as possible. With our support for ECS you can now view and manage your ECS and EKS resources through Release's single administration interface. Release automatically tags and configures your ECS resources if you use our Infrastructure Runner, but we also allow you to tag and configure any ECS resources and manage them in Release, regardless of how they were deployed.\"}),`\n`,(0,n.jsxs)(e.h3,{id:\"aws-ecs-task-support\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#aws-ecs-task-support\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"AWS ECS Task Support\"]}),`\n`,(0,n.jsx)(e.p,{children:\"The first supported cloud resources are AWS ECS Tasks, but more resource types will be supported soon. Let us know what cloud resources you'd like to see supported!\"}),`\n`,(0,n.jsxs)(e.h3,{id:\"cloud-resources\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#cloud-resources\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Cloud Resources\"]}),`\n`,(0,n.jsx)(e.p,{children:\"After tagging a resource, it will show up on the Environment page in the Cloud Resources section, below your Instances.\"}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/4734bb86d72d86a67606a7548656013e.png\",alt:\"\"})}),`\n`,(0,n.jsxs)(e.p,{children:[\"_\",(0,n.jsx)(e.br,{}),`\n`,\"Cloud Resources like this ECS Task are listed on the environment page in a section below Instances. ECS Tasks support Terminal and Logs, which you launch by clicking the corresponding button._\"]}),`\n`,(0,n.jsxs)(e.h3,{id:\"interacting-with-cloud-resources\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#interacting-with-cloud-resources\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Interacting with Cloud Resources\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Currently, once you've tagged them, ECS Tasks support a couple of actions: Terminal and Logs (note the Terminal and Logs buttons in the previous screenshot).\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"You can launch a web terminal into the task containers by clicking the \",(0,n.jsx)(e.strong,{children:\"Terminal\"}),\" button:\"]}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/fbf49694fb352b4223c12dc6f30ea402.png\",alt:\"\"})}),`\n`,(0,n.jsxs)(e.p,{children:[\"And you can view container logs by clicking the \",(0,n.jsx)(e.strong,{children:\"Logs\"}),\" button:\"]}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/64a479f61dc2158839d8f73c601a1be9.png\",alt:\"\"})}),`\n`,(0,n.jsxs)(e.h3,{id:\"how-to-add-cloud-resources\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#how-to-add-cloud-resources\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"How to add Cloud Resources\"]}),`\n`,(0,n.jsx)(e.p,{children:\"To add a cloud resource to your Release environments, you'll need to add a couple of tags/labels to that resource in AWS or GCP.\"}),`\n`,(0,n.jsx)(e.p,{children:\"Two tags with values are required:\"}),`\n`,(0,n.jsxs)(e.table,{children:[(0,n.jsx)(e.thead,{children:(0,n.jsxs)(e.tr,{children:[(0,n.jsx)(e.th,{children:\"Tag Key\"}),(0,n.jsx)(e.th,{children:\"Tag Value\"}),(0,n.jsx)(e.th,{children:\"Location in UI\"}),(0,n.jsx)(e.th,{children:\"Env variable\"})]})}),(0,n.jsxs)(e.tbody,{children:[(0,n.jsxs)(e.tr,{children:[(0,n.jsx)(e.td,{children:\"releasehub.com/app\"}),(0,n.jsx)(e.td,{children:\"name\"}),(0,n.jsxs)(e.td,{children:[\"Top of the environment page. In the screenshot below, the app name is \",(0,n.jsx)(e.code,{children:\"example-voting-app\"})]}),(0,n.jsxs)(e.td,{children:[(0,n.jsx)(e.code,{children:\"RELEASE_APP_NAME\"}),\" (see default Release environment variables)\"]})]}),(0,n.jsxs)(e.tr,{children:[(0,n.jsx)(e.td,{children:\"releasehub.com/env-id\"}),(0,n.jsx)(e.td,{children:\"(the environment handle)\"}),(0,n.jsxs)(e.td,{children:[\"In the screenshot below, the environment handle is \",(0,n.jsx)(e.code,{children:\"ted3bff\"})]}),(0,n.jsxs)(e.td,{children:[(0,n.jsx)(e.code,{children:\"RELEASE_ENV_ID\"}),\" (see default Release environment variables)\"]})]})]})]}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/ca4729f2662588d2986369bd3eab969f.png\",alt:\"\"})}),`\n`,(0,n.jsxs)(e.p,{children:[\"AWS Documentation can be found \",(0,n.jsx)(e.a,{href:\"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs-using-tags.html\",children:\"here\"}),\".\"]}),`\n`,(0,n.jsxs)(e.h3,{id:\"how-to-configure-the-logs\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#how-to-configure-the-logs\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"How to Configure the Logs\"]}),`\n`,(0,n.jsx)(e.p,{children:\"In order to send your ECS task logs to the Release log viewer, you must configure your ECS tasks to use the awslogs log driver:\"}),`\n`,(0,n.jsx)(e.p,{children:\"You can specify the log driver in the task definition. Inside each entry in containerDefinitions, update (or add) the logConfiguration key to specify the logDriver and options. Options must specify awslogs-group, and awslogs-stream-prefix. The logDriver must be awslogs, but you may choose your own group and stream-prefix.\"}),`\n`,(0,n.jsx)(e.p,{children:\"That section of your task definition should look something like this when you're done:\"}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-yaml\",children:`{\n  \"containerDefinitions\":\n    [\n      {\n        \"logConfiguration\":\n          {\n            \"logDriver\": \"awslogs\",\n            \"options\":\n              {\n                \"awslogs-group\": \"firelens-container\",\n                \"awslogs-region\": \"us-west-2\",\n                \"awslogs-create-group\": \"true\",\n                \"awslogs-stream-prefix\": \"firelens\",\n              },\n          },\n      },\n    ],\n}\n`})}),`\n`,(0,n.jsxs)(e.p,{children:[\"For more details, check out the \",(0,n.jsx)(e.a,{href:\"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/using_awslogs.html#specify-log-config\",children:\"AWS documentation on using the awslogs log driver\"}),\".\"]}),`\n`,(0,n.jsxs)(e.h3,{id:\"how-to-configure-the-terminal\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#how-to-configure-the-terminal\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"How to Configure the Terminal\"]}),`\n`,(0,n.jsx)(e.p,{children:\"The following requirements must be met before being able to terminal into any ECS task:\"}),`\n`,(0,n.jsxs)(e.ol,{children:[`\n`,(0,n.jsxs)(e.li,{children:[\"ECS Exec needs to be enabled for the task. (\",(0,n.jsx)(e.a,{href:\"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs-exec.html#ecs-exec-enabling\",children:\"more info\"}),\")\"]}),`\n`,(0,n.jsxs)(e.li,{children:[\"ECS Exec IAM permissions need to be added for the task. (\",(0,n.jsx)(e.a,{href:\"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs-exec.html#ecs-exec-required-iam-permissions\",children:\"more info\"}),\")\"]}),`\n`]}),`\n`,(0,n.jsxs)(e.p,{children:[\"For more details, check out the \",(0,n.jsx)(e.a,{href:\"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs-exec.html\",children:\"AWS documentation on enabling ECS Exec\"}),\" and the best practices around it.\"]}),`\n`,(0,n.jsx)(e.p,{children:\"After configuring the ECS task, use the Cloud Resources refresh button and you should be able to choose the container to terminal into. Note that Release will gray out the terminal button for containers that are not running or tasks that do not have ECS Exec enabled.\"}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/3d39dc71386de1d85df54a097a497bd3.png\",alt:\"\"})}),`\n`,(0,n.jsxs)(e.h3,{id:\"try-it-yourself\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#try-it-yourself\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Try it Yourself\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"At Release we always try to support the infrastructure our customers have in place while allowing them to take advantage of any new offerings from AWS. If you would like to manage your containers, regardless of which container management service you use, read through our \",(0,n.jsx)(e.a,{href:\"https://docs.releasehub.com/reference-documentation/cloud-resources\",children:\"documentation\"}),\" and/or schedule a \",(0,n.jsx)(e.a,{href:\"https://releasehub.com/book-a-demo\",children:\"demo\"}),\"!\"]})]})}function E(o={}){let{wrapper:e}=o.components||{};return e?(0,n.jsx)(e,Object.assign({},o,{children:(0,n.jsx)(d,o)})):d(o)}var k=E;return y(v);})();\n;return Component;"
        },
        "_id": "blog/posts/release-offers-support-for-ecs.mdx",
        "_raw": {
          "sourceFilePath": "blog/posts/release-offers-support-for-ecs.mdx",
          "sourceFileName": "release-offers-support-for-ecs.mdx",
          "sourceFileDir": "blog/posts",
          "contentType": "mdx",
          "flattenedPath": "blog/posts/release-offers-support-for-ecs"
        },
        "type": "BlogPost",
        "computedSlug": "release-offers-support-for-ecs"
      },
      "documentHash": "1739658628231",
      "hasWarnings": false,
      "documentTypeName": "BlogPost"
    },
    "blog/posts/releasehub-20-million-series-a-led-by-crv.mdx": {
      "document": {
        "title": "ReleaseHub Announces $20 Million Series A Round Led by CRV",
        "summary": "ReleaseHub has completed a $20 million Series A funding round led by CRV with participation by Sequoia and Y Combinator.",
        "publishDate": "Mon Oct 04 2021 21:33:00 GMT+0000 (Coordinated Universal Time)",
        "author": "tommy-mcclung",
        "readingTime": 3,
        "categories": [
          "platform-engineering",
          "kubernetes",
          "product"
        ],
        "mainImage": "/blog-images/54d4a47f57bb85a788c3e08d98a41e7e.png",
        "imageAlt": "Illustration of 3 software development environments created by Release",
        "showCTA": true,
        "ctaCopy": "Simplify environment management for cloud-native apps like ReleaseHub does for creating on-demand full-stack environments.",
        "ctaLink": "https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=releasehub-20-million-series-a-led-by-crv",
        "relatedPosts": [
          ""
        ],
        "ogImage": "/blog-images/54d4a47f57bb85a788c3e08d98a41e7e.png",
        "excerpt": "ReleaseHub has completed a $20 million Series A funding round led by CRV with participation by Sequoia and Y Combinator.",
        "tags": [
          "platform-engineering",
          "kubernetes",
          "product"
        ],
        "ctaButton": "Try Release for Free",
        "body": {
          "raw": "\n[Release,](https://release.com/) which provides environments-as-a-service (EaaS), has announced it has completed a $20 million Series A funding round. Led by [CRV](http://www.crv.com) with participation from [Sequoia](https://www.sequoiacap.com/), [Y Combinator](https://www.ycombinator.com/), [Bow Capital](https://bowcapital.com/team/), [Artisanal Ventures](https://www.artisanalv.com/), [Hack VC](https://hack-vc.com/) and angel investors Amit Agarwal (Datadog), Bill Clerico (WePay) and Chase Gilbert (Built Technologies), the Series A follows a $2.7 million seed round in April 2020.\n\nReleaseHub solves the universal problem of the costs and difficulty of creating, managing and maintaining increasingly complex environments for software developers. Customers of ReleaseHub are using EaaS to create cloud native full-stack environments with data, on-demand. These environments are being used for previewing and QA’ing features in the dev workflow, demoing software with sales demo environments, running production applications and delivering SaaS products into their customer’s virtual private clouds. ReleaseHub’s EaaS delivers an exceptional and simple development experience with environments as its core, with the full capability of the major cloud providers.\n\n“Making developers and organizations more effective and efficient has been a problem and a challenge everywhere we’ve worked,” said co-founder and CEO Tommy McClung. “In addition, development in the cloud has become more complicated. Using Kubernetes and deep integrations with each cloud provider to facilitate the ability to reproduce environments, ReleaseHub can replace a ton of work that has been a bespoke maintenance nightmare allowing developers to do it instantaneously.”\n\nPart of the Y Combinator winter 2020 cohort, ReleaseHub was founded by McClung along with Erik Landerholm and David Giffin. They previously led the technology team at an ecommerce company where they spearheaded the companywide effort to remove environment bottlenecks. There were no commercial solutions at the time so the founders built their own environment management solution and soon after launched ReleaseHub to commercialize EaaS.\n\n\"Time and time again we hear from technology companies that managing their environments is becoming increasingly complex as we move to hybrid and microservice based architectures,” said James Green, venture investor at CRV. “Given this complexity, a logical step to making development teams more productive is a platform to automate many of these workflows around environments. We came across ReleaseHub through the developer community and were blown away by their product and progress. The team experienced this pain themselves while on the leadership team at TrueCar and are now solving this issue at many complex organizations including Shogun, Monad Security and BraveCare. We're delighted to partner with them.”\n\nReflective of the challenges and opportunities of launching, hiring and scaling during the pandemic, ReleaseHub will remain 100 percent remote and has added Artisanal Ventures as an investor to gain an edge in recruiting as they aggressively hire top talent across executive, product, engineering and go-to-market functions. Eschewing the reputation of tech companies that prioritize work over all else, ReleaseHub is leaning into remote work and purposefully advocating for team members to better weave their life, work and family together making it an appealing workplace.\n\n### About CRV\n\n[CRV](https://crv.com/) is a venture capital firm that invests in early-stage enterprise and consumer startups. Since 1970, the firm has invested in more than 400 startups at their most crucial stages, including DoorDash, Airtable, Patreon, Drift and Iterable. Founders need more than capital to build a great company. It takes a partner who understands the entrepreneurial journey and knows what it takes to win. From founding to IPO and beyond, CRV is there every step of the way. Founders rely on CRV to be trusted, long-term, committed partners, which has helped make CRV into one of the longest-running venture capital firms in the world. Learn more about CRV and the companies shaping the future at [crv.com](https://www.crv.com).\n",
          "code": "var Component=(()=>{var m=Object.create;var o=Object.defineProperty;var h=Object.getOwnPropertyDescriptor;var u=Object.getOwnPropertyNames;var p=Object.getPrototypeOf,g=Object.prototype.hasOwnProperty;var f=(a,e)=>()=>(e||a((e={exports:{}}).exports,e),e.exports),v=(a,e)=>{for(var t in e)o(a,t,{get:e[t],enumerable:!0})},s=(a,e,t,r)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let i of u(e))!g.call(a,i)&&i!==t&&o(a,i,{get:()=>e[i],enumerable:!(r=h(e,i))||r.enumerable});return a};var w=(a,e,t)=>(t=a!=null?m(p(a)):{},s(e||!a||!a.__esModule?o(t,\"default\",{value:a,enumerable:!0}):t,a)),b=a=>s(o({},\"__esModule\",{value:!0}),a);var c=f((S,l)=>{l.exports=_jsx_runtime});var k={};v(k,{default:()=>R,frontmatter:()=>y});var n=w(c()),y={title:\"ReleaseHub Announces $20 Million Series A Round Led by CRV\",summary:\"ReleaseHub has completed a $20 million Series A funding round led by CRV with participation by Sequoia and Y Combinator.\",publishDate:\"Mon Oct 04 2021 21:33:00 GMT+0000 (Coordinated Universal Time)\",author:\"tommy-mcclung\",readingTime:3,categories:[\"platform-engineering\",\"kubernetes\",\"product\"],mainImage:\"/blog-images/54d4a47f57bb85a788c3e08d98a41e7e.png\",imageAlt:\"Illustration of 3 software development environments created by Release\",showCTA:!0,ctaCopy:\"Simplify environment management for cloud-native apps like ReleaseHub does for creating on-demand full-stack environments.\",ctaLink:\"https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=releasehub-20-million-series-a-led-by-crv\",relatedPosts:[\"\"],ogImage:\"/blog-images/54d4a47f57bb85a788c3e08d98a41e7e.png\",excerpt:\"ReleaseHub has completed a $20 million Series A funding round led by CRV with participation by Sequoia and Y Combinator.\",tags:[\"platform-engineering\",\"kubernetes\",\"product\"],ctaButton:\"Try Release for Free\"};function d(a){let e=Object.assign({p:\"p\",a:\"a\",h3:\"h3\",span:\"span\"},a.components);return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsxs)(e.p,{children:[(0,n.jsx)(e.a,{href:\"https://release.com/\",children:\"Release,\"}),\" which provides environments-as-a-service (EaaS), has announced it has completed a $20 million Series A funding round. Led by \",(0,n.jsx)(e.a,{href:\"http://www.crv.com\",children:\"CRV\"}),\" with participation from \",(0,n.jsx)(e.a,{href:\"https://www.sequoiacap.com/\",children:\"Sequoia\"}),\", \",(0,n.jsx)(e.a,{href:\"https://www.ycombinator.com/\",children:\"Y Combinator\"}),\", \",(0,n.jsx)(e.a,{href:\"https://bowcapital.com/team/\",children:\"Bow Capital\"}),\", \",(0,n.jsx)(e.a,{href:\"https://www.artisanalv.com/\",children:\"Artisanal Ventures\"}),\", \",(0,n.jsx)(e.a,{href:\"https://hack-vc.com/\",children:\"Hack VC\"}),\" and angel investors Amit Agarwal (Datadog), Bill Clerico (WePay) and Chase Gilbert (Built Technologies), the Series A follows a $2.7 million seed round in April 2020.\"]}),`\n`,(0,n.jsx)(e.p,{children:\"ReleaseHub solves the universal problem of the costs and difficulty of creating, managing and maintaining increasingly complex environments for software developers. Customers of ReleaseHub are using EaaS to create cloud native full-stack environments with data, on-demand. These environments are being used for previewing and QA\\u2019ing features in the dev workflow, demoing software with sales demo environments, running production applications and delivering SaaS products into their customer\\u2019s virtual private clouds. ReleaseHub\\u2019s EaaS delivers an exceptional and simple development experience with environments as its core, with the full capability of the major cloud providers.\"}),`\n`,(0,n.jsx)(e.p,{children:\"\\u201CMaking developers and organizations more effective and efficient has been a problem and a challenge everywhere we\\u2019ve worked,\\u201D said co-founder and CEO Tommy McClung. \\u201CIn addition, development in the cloud has become more complicated. Using Kubernetes and deep integrations with each cloud provider to facilitate the ability to reproduce environments, ReleaseHub can replace a ton of work that has been a bespoke maintenance nightmare allowing developers to do it instantaneously.\\u201D\"}),`\n`,(0,n.jsx)(e.p,{children:\"Part of the Y Combinator winter 2020 cohort, ReleaseHub was founded by McClung along with Erik Landerholm and David Giffin. They previously led the technology team at an ecommerce company where they spearheaded the companywide effort to remove environment bottlenecks. There were no commercial solutions at the time so the founders built their own environment management solution and soon after launched ReleaseHub to commercialize EaaS.\"}),`\n`,(0,n.jsx)(e.p,{children:`\"Time and time again we hear from technology companies that managing their environments is becoming increasingly complex as we move to hybrid and microservice based architectures,\\u201D said James Green, venture investor at CRV. \\u201CGiven this complexity, a logical step to making development teams more productive is a platform to automate many of these workflows around environments. We came across ReleaseHub through the developer community and were blown away by their product and progress. The team experienced this pain themselves while on the leadership team at TrueCar and are now solving this issue at many complex organizations including Shogun, Monad Security and BraveCare. We're delighted to partner with them.\\u201D`}),`\n`,(0,n.jsx)(e.p,{children:\"Reflective of the challenges and opportunities of launching, hiring and scaling during the pandemic, ReleaseHub will remain 100 percent remote and has added Artisanal Ventures as an investor to gain an edge in recruiting as they aggressively hire top talent across executive, product, engineering and go-to-market functions. Eschewing the reputation of tech companies that prioritize work over all else, ReleaseHub is leaning into remote work and purposefully advocating for team members to better weave their life, work and family together making it an appealing workplace.\"}),`\n`,(0,n.jsxs)(e.h3,{id:\"about-crv\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#about-crv\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"About CRV\"]}),`\n`,(0,n.jsxs)(e.p,{children:[(0,n.jsx)(e.a,{href:\"https://crv.com/\",children:\"CRV\"}),\" is a venture capital firm that invests in early-stage enterprise and consumer startups. Since 1970, the firm has invested in more than 400 startups at their most crucial stages, including DoorDash, Airtable, Patreon, Drift and Iterable. Founders need more than capital to build a great company. It takes a partner who understands the entrepreneurial journey and knows what it takes to win. From founding to IPO and beyond, CRV is there every step of the way. Founders rely on CRV to be trusted, long-term, committed partners, which has helped make CRV into one of the longest-running venture capital firms in the world. Learn more about CRV and the companies shaping the future at \",(0,n.jsx)(e.a,{href:\"https://www.crv.com\",children:\"crv.com\"}),\".\"]})]})}function C(a={}){let{wrapper:e}=a.components||{};return e?(0,n.jsx)(e,Object.assign({},a,{children:(0,n.jsx)(d,a)})):d(a)}var R=C;return b(k);})();\n;return Component;"
        },
        "_id": "blog/posts/releasehub-20-million-series-a-led-by-crv.mdx",
        "_raw": {
          "sourceFilePath": "blog/posts/releasehub-20-million-series-a-led-by-crv.mdx",
          "sourceFileName": "releasehub-20-million-series-a-led-by-crv.mdx",
          "sourceFileDir": "blog/posts",
          "contentType": "mdx",
          "flattenedPath": "blog/posts/releasehub-20-million-series-a-led-by-crv"
        },
        "type": "BlogPost",
        "computedSlug": "releasehub-20-million-series-a-led-by-crv"
      },
      "documentHash": "1739393595027",
      "hasWarnings": false,
      "documentTypeName": "BlogPost"
    },
    "blog/posts/releasehub-adds-enterprise-class-capabilities-with-remote-development-environments-datadog-integration-and-instant-data-sets.mdx": {
      "document": {
        "title": "Release Hires New CMO and Adds Enterprise Class Capabilities ",
        "summary": "Release grows marketing team, hires former Docker VP of Marketing as CMO - and announces new features",
        "publishDate": "Fri Oct 21 2022 18:22:04 GMT+0000 (Coordinated Universal Time)",
        "author": "",
        "readingTime": 2,
        "categories": [
          "platform-engineering",
          "product"
        ],
        "mainImage": "/blog-images/c5b8725bd82a4d641d82a85d8dcbae2e.jpg",
        "imageAlt": "Release Hires New CMO and Adds Enterprise Class Capabilities ",
        "showCTA": true,
        "ctaCopy": "Unlock instant data replication and seamless collaboration with Release's EaaS platform for faster releases and efficient testing.",
        "ctaLink": "https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=releasehub-adds-enterprise-class-capabilities-with-remote-development-environments-datadog-integration-and-instant-data-sets",
        "relatedPosts": [
          ""
        ],
        "ogImage": "/blog-images/c5b8725bd82a4d641d82a85d8dcbae2e.jpg",
        "excerpt": "Release grows marketing team, hires former Docker VP of Marketing as CMO - and announces new features",
        "tags": [
          "platform-engineering",
          "product"
        ],
        "ctaButton": "Try Release for Free",
        "body": {
          "raw": "\n**San Fransisco, CA** —  **October 24, 2022** — Release, the leading provider of Environments-as-a-Service (EaaS), today announced enterprise class technologies for remote development environments, Datadog integration, and instant data management to improve developer confidence and increase release velocity. The company also announced it hired Matt Carter, former Docker Vice President of Marketing, as CMO.\n\n‍\n\n“This is the era of Environments as a Service, which is driving fundamental changes throughout the application development process,” said Tommy McClung, Release CEO. “Our goal is to provide developers with the ability to write and commit code without penalty. With ReleaseHub, developers can spin up identical copies of environments as needed, within minutes.”\n\n‍\n\nOne of the biggest bottlenecks in software delivery happens when developers are stuck waiting for access to environments. This results in significant delays in testing, debugging, and deploying software. EaaS mimics true production environments but are spun up and down on demand, so developers avoid development paralysis.\n\n‍\n\nEaaS signals a leap in release velocity and developer confidence. Similar to how containers let developers isolate software code, EaaS frees developers from fixed environments, letting them move quickly, at low cost, and without disrupting the workflow of their team.\n\n‍\n\n“We’re seeing greater EaaS uptake as developer teams recognize traditional environments are a major bottleneck,” said Matt Carter, Release CMO. “I’ve worked at Microsoft, Chef, and Docker, and have seen many technology transitions over the years. As we add more enterprise-class features, we’re seeing similar growth now, with greater adoption among a wider range of organizations.”\n\n‍\n\nAdditionally, Release announced a number of new features that extend EaaS platform value to application development teams, including:\n\n‍\n\n- **Remote Development Environments**\n\nRelease is improving release velocity with new technology that lets developers build code locally while running it remotely within a customer’s cloud account. Remote development environments increase release velocity by letting developers use their full stack, while receiving immediate feedback from production or production-like data in their development environment.\n\n‍\n\n- **Datadog Integration**\n\nReleaseHub is introducing native support for Datadog, enabling developers to work from the leading cloud monitoring platform. Datadog observability gives developers Release insights from their Datadog accounts as well as single sign-on, role-based access control, and secrets management within ReleaseHub. These enterprise class features help organizations manage their DevOps pipelines at scale.\n\n‍\n\n- **Instant Data Sets**\n\nRelease is accelerating the replication of application data with Instant Data Sets, which lets developers create a replica of application data in minutes. Developers can build and test against the actual data their app uses.\n\n‍\n**About Release**\n\nRelease delivers Environments-as-a-Service. It lets developers easily share progress with stakeholders when a full stack environment is created with every pull request and is shareable via custom URLs and directly in Slack. Every environment is a full instance of the app with all its services. ReleaseHub was funded by CRV, Sequoia, Y Combinator, Bow Capital, Artisanal Ventures, Hack VC, and other investors. More information is available at [www.release.com](http://www.release.com/).  \n\n‍\n",
          "code": "var Component=(()=>{var p=Object.create;var r=Object.defineProperty;var m=Object.getOwnPropertyDescriptor;var h=Object.getOwnPropertyNames;var g=Object.getPrototypeOf,u=Object.prototype.hasOwnProperty;var v=(a,e)=>()=>(e||a((e={exports:{}}).exports,e),e.exports),f=(a,e)=>{for(var t in e)r(a,t,{get:e[t],enumerable:!0})},o=(a,e,t,s)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let i of h(e))!u.call(a,i)&&i!==t&&r(a,i,{get:()=>e[i],enumerable:!(s=m(e,i))||s.enumerable});return a};var w=(a,e,t)=>(t=a!=null?p(g(a)):{},o(e||!a||!a.__esModule?r(t,\"default\",{value:a,enumerable:!0}):t,a)),b=a=>o(r({},\"__esModule\",{value:!0}),a);var d=v((D,l)=>{l.exports=_jsx_runtime});var R={};f(R,{default:()=>C,frontmatter:()=>y});var n=w(d()),y={title:\"Release Hires New CMO and Adds Enterprise Class Capabilities \",summary:\"Release grows marketing team, hires former Docker VP of Marketing as CMO - and announces new features\",publishDate:\"Fri Oct 21 2022 18:22:04 GMT+0000 (Coordinated Universal Time)\",author:\"\",readingTime:2,categories:[\"platform-engineering\",\"product\"],mainImage:\"/blog-images/c5b8725bd82a4d641d82a85d8dcbae2e.jpg\",imageAlt:\"Release Hires New CMO and Adds Enterprise Class Capabilities \",showCTA:!0,ctaCopy:\"Unlock instant data replication and seamless collaboration with Release's EaaS platform for faster releases and efficient testing.\",ctaLink:\"https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=releasehub-adds-enterprise-class-capabilities-with-remote-development-environments-datadog-integration-and-instant-data-sets\",relatedPosts:[\"\"],ogImage:\"/blog-images/c5b8725bd82a4d641d82a85d8dcbae2e.jpg\",excerpt:\"Release grows marketing team, hires former Docker VP of Marketing as CMO - and announces new features\",tags:[\"platform-engineering\",\"product\"],ctaButton:\"Try Release for Free\"};function c(a){let e=Object.assign({p:\"p\",strong:\"strong\",ul:\"ul\",li:\"li\",a:\"a\"},a.components);return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsxs)(e.p,{children:[(0,n.jsx)(e.strong,{children:\"San Fransisco, CA\"}),\" \\u2014 \\xA0\",(0,n.jsx)(e.strong,{children:\"October 24, 2022\"}),\" \\u2014 Release, the leading provider of Environments-as-a-Service (EaaS), today announced enterprise class technologies for remote development environments, Datadog integration, and instant data management to improve developer confidence and increase release velocity. The company also announced it hired Matt Carter, former Docker Vice President of Marketing, as CMO.\"]}),`\n`,(0,n.jsx)(e.p,{children:\"\\u200D\"}),`\n`,(0,n.jsx)(e.p,{children:\"\\u201CThis is the era of Environments as a Service, which is driving fundamental changes throughout the application development process,\\u201D said Tommy McClung, Release CEO. \\u201COur goal is to provide developers with the ability to write and commit code without penalty. With ReleaseHub, developers can spin up identical copies of environments as needed, within minutes.\\u201D\"}),`\n`,(0,n.jsx)(e.p,{children:\"\\u200D\"}),`\n`,(0,n.jsx)(e.p,{children:\"One of the biggest bottlenecks in software delivery happens when developers are stuck waiting for access to environments. This results in significant delays in testing, debugging, and deploying software. EaaS mimics true production environments but are spun up and down on demand, so developers avoid development paralysis.\"}),`\n`,(0,n.jsx)(e.p,{children:\"\\u200D\"}),`\n`,(0,n.jsx)(e.p,{children:\"EaaS signals a leap in release velocity and developer confidence. Similar to how containers let developers isolate software code, EaaS frees developers from fixed environments, letting them move quickly, at low cost, and without disrupting the workflow of their team.\"}),`\n`,(0,n.jsx)(e.p,{children:\"\\u200D\"}),`\n`,(0,n.jsx)(e.p,{children:\"\\u201CWe\\u2019re seeing greater EaaS uptake as developer teams recognize traditional environments are a major bottleneck,\\u201D said Matt Carter, Release CMO. \\u201CI\\u2019ve worked at Microsoft, Chef, and Docker, and have seen many technology transitions over the years. As we add more enterprise-class features, we\\u2019re seeing similar growth now, with greater adoption among a wider range of organizations.\\u201D\"}),`\n`,(0,n.jsx)(e.p,{children:\"\\u200D\"}),`\n`,(0,n.jsx)(e.p,{children:\"Additionally, Release announced a number of new features that extend EaaS platform value to application development teams, including:\"}),`\n`,(0,n.jsx)(e.p,{children:\"\\u200D\"}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsx)(e.li,{children:(0,n.jsx)(e.strong,{children:\"Remote Development Environments\"})}),`\n`]}),`\n`,(0,n.jsx)(e.p,{children:\"Release is improving release velocity with new technology that lets developers build code locally while running it remotely within a customer\\u2019s cloud account. Remote development environments increase release velocity by letting developers use their full stack, while receiving immediate feedback from production or production-like data in their development environment.\"}),`\n`,(0,n.jsx)(e.p,{children:\"\\u200D\"}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsx)(e.li,{children:(0,n.jsx)(e.strong,{children:\"Datadog Integration\"})}),`\n`]}),`\n`,(0,n.jsx)(e.p,{children:\"ReleaseHub is introducing native support for Datadog, enabling developers to work from the leading cloud monitoring platform. Datadog observability gives developers Release insights from their Datadog accounts as well as single sign-on, role-based access control, and secrets management within ReleaseHub. These enterprise class features help organizations manage their DevOps pipelines at scale.\"}),`\n`,(0,n.jsx)(e.p,{children:\"\\u200D\"}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsx)(e.li,{children:(0,n.jsx)(e.strong,{children:\"Instant Data Sets\"})}),`\n`]}),`\n`,(0,n.jsx)(e.p,{children:\"Release is accelerating the replication of application data with Instant Data Sets, which lets developers create a replica of application data in minutes. Developers can build and test against the actual data their app uses.\"}),`\n`,(0,n.jsxs)(e.p,{children:[`\\u200D\n`,(0,n.jsx)(e.strong,{children:\"About Release\"})]}),`\n`,(0,n.jsxs)(e.p,{children:[\"Release delivers Environments-as-a-Service. It lets developers easily share progress with stakeholders when a full stack environment is created with every pull request and is shareable via custom URLs and directly in Slack. Every environment is a full instance of the app with all its services. ReleaseHub was funded by CRV, Sequoia, Y Combinator, Bow Capital, Artisanal Ventures, Hack VC, and other investors. More information is available at \",(0,n.jsx)(e.a,{href:\"http://www.release.com/\",children:\"www.release.com\"}),\". \\xA0\"]}),`\n`,(0,n.jsx)(e.p,{children:\"\\u200D\"})]})}function k(a={}){let{wrapper:e}=a.components||{};return e?(0,n.jsx)(e,Object.assign({},a,{children:(0,n.jsx)(c,a)})):c(a)}var C=k;return b(R);})();\n;return Component;"
        },
        "_id": "blog/posts/releasehub-adds-enterprise-class-capabilities-with-remote-development-environments-datadog-integration-and-instant-data-sets.mdx",
        "_raw": {
          "sourceFilePath": "blog/posts/releasehub-adds-enterprise-class-capabilities-with-remote-development-environments-datadog-integration-and-instant-data-sets.mdx",
          "sourceFileName": "releasehub-adds-enterprise-class-capabilities-with-remote-development-environments-datadog-integration-and-instant-data-sets.mdx",
          "sourceFileDir": "blog/posts",
          "contentType": "mdx",
          "flattenedPath": "blog/posts/releasehub-adds-enterprise-class-capabilities-with-remote-development-environments-datadog-integration-and-instant-data-sets"
        },
        "type": "BlogPost",
        "computedSlug": "releasehub-adds-enterprise-class-capabilities-with-remote-development-environments-datadog-integration-and-instant-data-sets"
      },
      "documentHash": "1739393595027",
      "hasWarnings": false,
      "documentTypeName": "BlogPost"
    },
    "blog/posts/releasehub-appoints-kelsey-degeorge-as-chief-revenue-officer.mdx": {
      "document": {
        "title": "Release Appoints Kelsey DeGeorge as Chief Revenue Officer",
        "summary": "Release appoints Kelsey DeGeorge as Chief Revenue Officer. DeGeorge previously managed ISV startup sales at AWS.",
        "publishDate": "Wed Feb 23 2022 23:23:46 GMT+0000 (Coordinated Universal Time)",
        "author": "tommy-mcclung",
        "readingTime": 2,
        "categories": [
          "platform-engineering",
          "product"
        ],
        "mainImage": "/blog-images/54d4a47f57bb85a788c3e08d98a41e7e.png",
        "imageAlt": "Release Appoints Kelsey DeGeorge as Chief Revenue Officer",
        "showCTA": true,
        "ctaCopy": "Empower your team with on-demand environments like Kelsey DeGeorge at Release for streamlined development and faster deployments.",
        "ctaLink": "https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=releasehub-appoints-kelsey-degeorge-as-chief-revenue-officer",
        "relatedPosts": [
          ""
        ],
        "ogImage": "/blog-images/54d4a47f57bb85a788c3e08d98a41e7e.png",
        "excerpt": "Release appoints Kelsey DeGeorge as Chief Revenue Officer. DeGeorge previously managed ISV startup sales at AWS.",
        "tags": [
          "platform-engineering",
          "product"
        ],
        "ctaButton": "Try Release for Free",
        "body": {
          "raw": "\n**February 24, 2022** – Release today announced that Kelsey DeGeorge has been appointed as Chief Revenue Officer. DeGeorge joins Release from Amazon Web Services, where she led a sales segment dedicated to B2B ISV companies, supporting them as consumers of AWS technology while forming joint GTM partnerships. DeGeorge will be responsible for scaling the GTM organization by accelerating Release adoption, elevating Release’s visibility through the channel, including cloud partner programs, and connecting with hundreds of thousands of developers on AWS who use conventional environments today. She will report directly to Release CEO and co-founder Tommy McClung.\n\n“Hiring Kelsey is a major milestone for Release,” said Tommy McClung, Release co-founder and CEO. “We immediately recognized her unmatched knowledge of the software vendor market and her deep insights into how companies can build GTM partnerships with cloud providers, an influential channel strategy in the industry. She is intimately familiar with the challenges ISVs face, and her skillset is the perfect match as we ramp up for the next phase of growth.”\n\nRelease provides Environments-as-a-Service to software developers, easing a significant bottleneck in software production. Developers frequently wait for access to limited environments, costing the software industry $45 billion annually. Having access to Environments-as-a-Service empowers developers to create cloud native full-stack environments on-demand, which can be used for quality analysis, running production applications and delivering software to customers on virtual private clouds or public clouds like AWS.\n\n“I was introduced to Release during my time at AWS and recognized the opportunity that their novel Environments-as-a-Service platform provides,” said DeGeorge. “They’ve created a unique technology with significant growth potential. It’s a rare opportunity to join such an innovative company at the startup stage, and I’m thrilled to be part of the team.”\n\nPrior to joining Release and AWS, DeGeorge held gtm and engineering roles at PTC, Schlumberger, and RealID. She has a BS in Mechanical Engineering from University of Colorado, and is involved in the Society of Women Engineers (SWE) and the Lunar University Network for Astrophysics Research (LUNAR).\n",
          "code": "var Component=(()=>{var g=Object.create;var i=Object.defineProperty;var m=Object.getOwnPropertyDescriptor;var p=Object.getOwnPropertyNames;var h=Object.getPrototypeOf,u=Object.prototype.hasOwnProperty;var f=(n,e)=>()=>(e||n((e={exports:{}}).exports,e),e.exports),v=(n,e)=>{for(var a in e)i(n,a,{get:e[a],enumerable:!0})},s=(n,e,a,r)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let o of p(e))!u.call(n,o)&&o!==a&&i(n,o,{get:()=>e[o],enumerable:!(r=m(e,o))||r.enumerable});return n};var y=(n,e,a)=>(a=n!=null?g(h(n)):{},s(e||!n||!n.__esModule?i(a,\"default\",{value:n,enumerable:!0}):a,n)),w=n=>s(i({},\"__esModule\",{value:!0}),n);var c=f((G,l)=>{l.exports=_jsx_runtime});var D={};v(D,{default:()=>S,frontmatter:()=>b});var t=y(c()),b={title:\"Release Appoints Kelsey DeGeorge as Chief Revenue Officer\",summary:\"Release appoints Kelsey DeGeorge as Chief Revenue Officer. DeGeorge previously managed ISV startup sales at AWS.\",publishDate:\"Wed Feb 23 2022 23:23:46 GMT+0000 (Coordinated Universal Time)\",author:\"tommy-mcclung\",readingTime:2,categories:[\"platform-engineering\",\"product\"],mainImage:\"/blog-images/54d4a47f57bb85a788c3e08d98a41e7e.png\",imageAlt:\"Release Appoints Kelsey DeGeorge as Chief Revenue Officer\",showCTA:!0,ctaCopy:\"Empower your team with on-demand environments like Kelsey DeGeorge at Release for streamlined development and faster deployments.\",ctaLink:\"https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=releasehub-appoints-kelsey-degeorge-as-chief-revenue-officer\",relatedPosts:[\"\"],ogImage:\"/blog-images/54d4a47f57bb85a788c3e08d98a41e7e.png\",excerpt:\"Release appoints Kelsey DeGeorge as Chief Revenue Officer. DeGeorge previously managed ISV startup sales at AWS.\",tags:[\"platform-engineering\",\"product\"],ctaButton:\"Try Release for Free\"};function d(n){let e=Object.assign({p:\"p\",strong:\"strong\"},n.components);return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:\"February 24, 2022\"}),\" \\u2013 Release today announced that Kelsey DeGeorge has been appointed as Chief Revenue Officer. DeGeorge joins Release from Amazon Web Services, where she led a sales segment dedicated to B2B ISV companies, supporting them as consumers of AWS technology while forming joint GTM partnerships. DeGeorge will be responsible for scaling the GTM organization by accelerating Release adoption, elevating Release\\u2019s visibility through the channel, including cloud partner programs, and connecting with hundreds of thousands of developers on AWS who use conventional environments today. She will report directly to Release CEO and co-founder Tommy McClung.\"]}),`\n`,(0,t.jsx)(e.p,{children:\"\\u201CHiring Kelsey is a major milestone for Release,\\u201D said Tommy McClung, Release co-founder and CEO. \\u201CWe immediately recognized her unmatched knowledge of the software vendor market and her deep insights into how companies can build GTM partnerships with cloud providers, an influential channel strategy in the industry. She is intimately familiar with the challenges ISVs face, and her skillset is the perfect match as we ramp up for the next phase of growth.\\u201D\"}),`\n`,(0,t.jsx)(e.p,{children:\"Release provides Environments-as-a-Service to software developers, easing a significant bottleneck in software production. Developers frequently wait for access to limited environments, costing the software industry $45 billion annually. Having access to Environments-as-a-Service empowers developers to create cloud native full-stack environments on-demand, which can be used for quality analysis, running production applications and delivering software to customers on virtual private clouds or public clouds like AWS.\"}),`\n`,(0,t.jsx)(e.p,{children:\"\\u201CI was introduced to Release during my time at AWS and recognized the opportunity that their novel Environments-as-a-Service platform provides,\\u201D said DeGeorge. \\u201CThey\\u2019ve created a unique technology with significant growth potential. It\\u2019s a rare opportunity to join such an innovative company at the startup stage, and I\\u2019m thrilled to be part of the team.\\u201D\"}),`\n`,(0,t.jsx)(e.p,{children:\"Prior to joining Release and AWS, DeGeorge held gtm and engineering roles at PTC, Schlumberger, and RealID. She has a BS in Mechanical Engineering from University of Colorado, and is involved in the Society of Women Engineers (SWE) and the Lunar University Network for Astrophysics Research (LUNAR).\"})]})}function R(n={}){let{wrapper:e}=n.components||{};return e?(0,t.jsx)(e,Object.assign({},n,{children:(0,t.jsx)(d,n)})):d(n)}var S=R;return w(D);})();\n;return Component;"
        },
        "_id": "blog/posts/releasehub-appoints-kelsey-degeorge-as-chief-revenue-officer.mdx",
        "_raw": {
          "sourceFilePath": "blog/posts/releasehub-appoints-kelsey-degeorge-as-chief-revenue-officer.mdx",
          "sourceFileName": "releasehub-appoints-kelsey-degeorge-as-chief-revenue-officer.mdx",
          "sourceFileDir": "blog/posts",
          "contentType": "mdx",
          "flattenedPath": "blog/posts/releasehub-appoints-kelsey-degeorge-as-chief-revenue-officer"
        },
        "type": "BlogPost",
        "computedSlug": "releasehub-appoints-kelsey-degeorge-as-chief-revenue-officer"
      },
      "documentHash": "1739393595027",
      "hasWarnings": false,
      "documentTypeName": "BlogPost"
    },
    "blog/posts/releasehub-sequoia-funded.mdx": {
      "document": {
        "title": "Release your Ideas with Environments as a Service.  Fueled by a Seed Round from Sequoia.",
        "summary": "Today we are super excited to announce our $2.7M Seed round led by Sequoia Capital.",
        "publishDate": "Thu Apr 29 2021 14:13:13 GMT+0000 (Coordinated Universal Time)",
        "author": "tommy-mcclung",
        "readingTime": 2,
        "categories": [
          "platform-engineering",
          "product"
        ],
        "mainImage": "/blog-images/3f2c225da880569cb22482af6db3cfb1.jpg",
        "imageAlt": "Sequoia logo representing the $2.7M Seed round led by Sequoia Capital",
        "showCTA": true,
        "ctaCopy": "Automate environment setup to release code faster, eliminating bottlenecks and reducing deployment delays.",
        "ctaLink": "https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=releasehub-sequoia-funded",
        "relatedPosts": [
          ""
        ],
        "ogImage": "/blog-images/3f2c225da880569cb22482af6db3cfb1.jpg",
        "excerpt": "Today we are super excited to announce our $2.7M Seed round led by Sequoia Capital.",
        "tags": [
          "platform-engineering",
          "product"
        ],
        "ctaButton": "Try Release for Free",
        "body": {
          "raw": "\nToday we are super excited to announce our $2.7M Seed round led by Sequoia Capital. You can read the details of the round in our [press release](https://release.com/press-releases).\n\n[Our mission at Release](https://release.com/blog/the-release-mission) is to help great ideas get to the world quickly. Over the 20 years of our careers in technology we’ve seen the evolution of building software on physical servers that we racked and stacked ourselves to today’s world of building on the cloud. One thing in that timeframe has remained consistent: delivering software isn’t getting any easier.\n\nBefore a software developer’s code can be released to the world it has to be deployed to an environment. Development, staging, and production environments are a critical step to releasing code. These highly complex platforms are built with dozens or hundreds of services, cloud platforms, and supporting technologies. They are so complex that standing up a new environment is a dedicated job for DevOps engineers, some of the most capable and in-demand technical talent in any organization. Environments thus become the biggest and most costly bottleneck in software development.\n\nThere are more than [21 million](https://en.wikipedia.org/wiki/Software_engineering_demographics) developers worldwide and they all use environments, but only one engineer at a time can deploy to an environment. Developers can wait days or weeks to deploy their code due to constraints on single environments. This leads to long queues, idle engineers, and delayed releases costing organizations tens of millions of dollars per year, something we estimate to be more than $45B per year to manage and maintain environments.\n\nWith each **technical advancement** we’ve seen the possibilities of what we can do with software expand. With these advancements, the complexity of our systems are also advancing. Modern applications are anything but simple. Software today is more complex than ever with all of the tools, clouds, services, and interconnectivity at our disposal.\n\nThis **complexity** means getting your ideas from your fingertips to users has become even harder than it was 20 years ago. While we aren’t fighting with racking and stacking hardware and freezing in a data center, we are now dealing with the ever growing complexity of our applications and our environments.\n\nThere was a moment in the early 2000’s when the complexity was embedded within the operating system. Distributed services across the cloud has become our operating system and it feels like the early 2000’s again. Something is needed that **virtualizes** this complexity much like the VM virtualized the operating system back then.\n**Environments** are the manifestation of your application running within this complex technical ecosystem. All the great ideas delivered with software run in environments and we have big plans on how making environments easy to reproduce will get these ideas to the world faster and easier.\n\nRelease can:\n\n- Help you **streamline development** and remove bottlenecks in your development process with ephemeral environments, test and QA environments, as well as staging environments that can be created on-demand.\n- Deliver an **amazing developer experience** to get your code from your mind to production. We’ve been told this is a Heroku-like experience delivered on Kubernetes in your cloud. \n- Deliver your **B2B application** into your customers’ cloud VPC environments\n\nThis is just the beginning of what can be done when your environments can easily be reproduced, created and delivered on-demand.\n\nWe’re proud to have **Sequoia** as our partner in this journey. They’ve consistently been a part of the biggest ideas and greatest companies that have ever been built. We’re especially excited to be working with **Bogomil Balkansky** who spent many years at VMWare thinking about how to deliver simplicity to developers and companies.\n\nI also want to thank our team for all of their hard work since we founded the company. They have made this possible and their energy, enthusiasm, and dedication has made building this company an absolute joy. I know we’re at the starting line but sometimes even getting to the starting line is a journey and this team is the best group of people I’ve ever had the pleasure of working with. If you’re interested in helping us on our mission, we always post new roles here: [https://release.com/company](https://release.com/company).\n\nIf you’re interested in seeing how Release Environments as a Service (EaaS) can help you and your team get your ideas to the world faster, drop us a line at [hello@release.com](mailto:hello@release.com) and we’d love to show you what the future of software development looks like.\n",
          "code": "var Component=(()=>{var c=Object.create;var i=Object.defineProperty;var m=Object.getOwnPropertyDescriptor;var u=Object.getOwnPropertyNames;var p=Object.getPrototypeOf,g=Object.prototype.hasOwnProperty;var y=(t,e)=>()=>(e||t((e={exports:{}}).exports,e),e.exports),w=(t,e)=>{for(var o in e)i(t,o,{get:e[o],enumerable:!0})},s=(t,e,o,r)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let a of u(e))!g.call(t,a)&&a!==o&&i(t,a,{get:()=>e[a],enumerable:!(r=m(e,a))||r.enumerable});return t};var v=(t,e,o)=>(o=t!=null?c(p(t)):{},s(e||!t||!t.__esModule?i(o,\"default\",{value:t,enumerable:!0}):o,t)),f=t=>s(i({},\"__esModule\",{value:!0}),t);var d=y((j,l)=>{l.exports=_jsx_runtime});var S={};w(S,{default:()=>x,frontmatter:()=>b});var n=v(d()),b={title:\"Release your Ideas with Environments as a Service.  Fueled by a Seed Round from Sequoia.\",summary:\"Today we are super excited to announce our $2.7M Seed round led by Sequoia Capital.\",publishDate:\"Thu Apr 29 2021 14:13:13 GMT+0000 (Coordinated Universal Time)\",author:\"tommy-mcclung\",readingTime:2,categories:[\"platform-engineering\",\"product\"],mainImage:\"/blog-images/3f2c225da880569cb22482af6db3cfb1.jpg\",imageAlt:\"Sequoia logo representing the $2.7M Seed round led by Sequoia Capital\",showCTA:!0,ctaCopy:\"Automate environment setup to release code faster, eliminating bottlenecks and reducing deployment delays.\",ctaLink:\"https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=releasehub-sequoia-funded\",relatedPosts:[\"\"],ogImage:\"/blog-images/3f2c225da880569cb22482af6db3cfb1.jpg\",excerpt:\"Today we are super excited to announce our $2.7M Seed round led by Sequoia Capital.\",tags:[\"platform-engineering\",\"product\"],ctaButton:\"Try Release for Free\"};function h(t){let e=Object.assign({p:\"p\",a:\"a\",strong:\"strong\",ul:\"ul\",li:\"li\"},t.components);return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsxs)(e.p,{children:[\"Today we are super excited to announce our $2.7M Seed round led by Sequoia Capital. You can read the details of the round in our \",(0,n.jsx)(e.a,{href:\"https://release.com/press-releases\",children:\"press release\"}),\".\"]}),`\n`,(0,n.jsxs)(e.p,{children:[(0,n.jsx)(e.a,{href:\"https://release.com/blog/the-release-mission\",children:\"Our mission at Release\"}),\" is to help great ideas get to the world quickly. Over the 20 years of our careers in technology we\\u2019ve seen the evolution of building software on physical servers that we racked and stacked ourselves to today\\u2019s world of building on the cloud. One thing in that timeframe has remained consistent: delivering software isn\\u2019t getting any easier.\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Before a software developer\\u2019s code can be released to the world it has to be deployed to an environment. Development, staging, and production environments are a critical step to releasing code. These highly complex platforms are built with dozens or hundreds of services, cloud platforms, and supporting technologies. They are so complex that standing up a new environment is a dedicated job for DevOps engineers, some of the most capable and in-demand technical talent in any organization. Environments thus become the biggest and most costly bottleneck in software development.\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"There are more than \",(0,n.jsx)(e.a,{href:\"https://en.wikipedia.org/wiki/Software_engineering_demographics\",children:\"21 million\"}),\" developers worldwide and they all use environments, but only one engineer at a time can deploy to an environment. Developers can wait days or weeks to deploy their code due to constraints on single environments. This leads to long queues, idle engineers, and delayed releases costing organizations tens of millions of dollars per year, something we estimate to be more than $45B per year to manage and maintain environments.\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"With each \",(0,n.jsx)(e.strong,{children:\"technical advancement\"}),\" we\\u2019ve seen the possibilities of what we can do with software expand. With these advancements, the complexity of our systems are also advancing. Modern applications are anything but simple. Software today is more complex than ever with all of the tools, clouds, services, and interconnectivity at our disposal.\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"This \",(0,n.jsx)(e.strong,{children:\"complexity\"}),\" means getting your ideas from your fingertips to users has become even harder than it was 20 years ago. While we aren\\u2019t fighting with racking and stacking hardware and freezing in a data center, we are now dealing with the ever growing complexity of our applications and our environments.\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"There was a moment in the early 2000\\u2019s when the complexity was embedded within the operating system. Distributed services across the cloud has become our operating system and it feels like the early 2000\\u2019s again. Something is needed that \",(0,n.jsx)(e.strong,{children:\"virtualizes\"}),` this complexity much like the VM virtualized the operating system back then.\n`,(0,n.jsx)(e.strong,{children:\"Environments\"}),\" are the manifestation of your application running within this complex technical ecosystem. All the great ideas delivered with software run in environments and we have big plans on how making environments easy to reproduce will get these ideas to the world faster and easier.\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Release can:\"}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsxs)(e.li,{children:[\"Help you \",(0,n.jsx)(e.strong,{children:\"streamline development\"}),\" and remove bottlenecks in your development process with ephemeral environments, test and QA environments, as well as staging environments that can be created on-demand.\"]}),`\n`,(0,n.jsxs)(e.li,{children:[\"Deliver an \",(0,n.jsx)(e.strong,{children:\"amazing developer experience\"}),\" to get your code from your mind to production. We\\u2019ve been told this is a Heroku-like experience delivered on Kubernetes in your cloud.\\xA0\"]}),`\n`,(0,n.jsxs)(e.li,{children:[\"Deliver your \",(0,n.jsx)(e.strong,{children:\"B2B application\"}),\" into your customers\\u2019 cloud VPC environments\"]}),`\n`]}),`\n`,(0,n.jsx)(e.p,{children:\"This is just the beginning of what can be done when your environments can easily be reproduced, created and delivered on-demand.\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"We\\u2019re proud to have \",(0,n.jsx)(e.strong,{children:\"Sequoia\"}),\" as our partner in this journey. They\\u2019ve consistently been a part of the biggest ideas and greatest companies that have ever been built. We\\u2019re especially excited to be working with \",(0,n.jsx)(e.strong,{children:\"Bogomil Balkansky\"}),\" who spent many years at VMWare thinking about how to deliver simplicity to developers and companies.\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"I also want to thank our team for all of their hard work since we founded the company. They have made this possible and their energy, enthusiasm, and dedication has made building this company an absolute joy. I know we\\u2019re at the starting line but sometimes even getting to the starting line is a journey and this team is the best group of people I\\u2019ve ever had the pleasure of working with. If you\\u2019re interested in helping us on our mission, we always post new roles here: \",(0,n.jsx)(e.a,{href:\"https://release.com/company\",children:\"https://release.com/company\"}),\".\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"If you\\u2019re interested in seeing how Release Environments as a Service (EaaS) can help you and your team get your ideas to the world faster, drop us a line at \",(0,n.jsx)(e.a,{href:\"mailto:hello@release.com\",children:\"hello@release.com\"}),\" and we\\u2019d love to show you what the future of software development looks like.\"]})]})}function k(t={}){let{wrapper:e}=t.components||{};return e?(0,n.jsx)(e,Object.assign({},t,{children:(0,n.jsx)(h,t)})):h(t)}var x=k;return f(S);})();\n;return Component;"
        },
        "_id": "blog/posts/releasehub-sequoia-funded.mdx",
        "_raw": {
          "sourceFilePath": "blog/posts/releasehub-sequoia-funded.mdx",
          "sourceFileName": "releasehub-sequoia-funded.mdx",
          "sourceFileDir": "blog/posts",
          "contentType": "mdx",
          "flattenedPath": "blog/posts/releasehub-sequoia-funded"
        },
        "type": "BlogPost",
        "computedSlug": "releasehub-sequoia-funded"
      },
      "documentHash": "1739393595027",
      "hasWarnings": false,
      "documentTypeName": "BlogPost"
    },
    "blog/posts/remote-development-environments.mdx": {
      "document": {
        "title": "Remote Development Environments",
        "summary": "Use Remote Development Environments to get instant feedback using production like data as early as the development stage",
        "publishDate": "Fri Oct 21 2022 21:07:58 GMT+0000 (Coordinated Universal Time)",
        "author": "erik-landerholm",
        "readingTime": 4,
        "categories": [
          "platform-engineering",
          "product"
        ],
        "mainImage": "/blog-images/dbd507c99d76242126c0c7d52fafeda2.jpg",
        "imageAlt": "Remote Development Environments",
        "showCTA": true,
        "ctaCopy": "Unlock seamless remote development with production-like environments using Release for faster bug resolution and consistent deployments.",
        "ctaLink": "https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=remote-development-environments",
        "relatedPosts": [
          ""
        ],
        "ogImage": "/blog-images/dbd507c99d76242126c0c7d52fafeda2.jpg",
        "excerpt": "Use Remote Development Environments to get instant feedback using production like data as early as the development stage",
        "tags": [
          "platform-engineering",
          "product"
        ],
        "ctaButton": "Try Release for Free",
        "body": {
          "raw": "\n## Introducing Remote Development Environments\n\nKubernetes is the best way to manage and orchestrate modern, complex container-based applications in the cloud.  \n\nWhile this is great for production environments it poses serious challenges for developers and DevOps practitioners. The differences between development and production often result in the utterance of the most dreaded words in application development, “But it works on my machine!” at the most inopportune times, specifically when there is an outage in production.  \n\nThere are a few ways to mitigate this issue, but the only “[win-win-win](https://www.youtube.com/watch?v=rYldJY3t9zk)” solution is a development environment that replicates production. Historically, this was often too difficult, time-consuming, and expensive, but Release has bridged the gap to provide you with a single pane of glass for all categories of environments you might need.\n\nYou may already know that Release can create ephemeral environments on pull requests and run your staging and production environments. But did you know that Release can also install your SaaS-based offerings in your customer’s VPCs, all from the same UI/CLI? \n\nWith this new feature, we have added production-like, cloud-based development environments to our industry-leading offerings. Your developer teams now get all the benefits of a traditional cloud-based development environment:\n\n- Write code on your machine (thin client, laptop, small supercomputer under your desk), but run it remotely with production mirroring hardware, software and configuration.\n- Total control over the configuration of remote development environments: You can build remote development environments to be as much like production as you need.\n- Bring your own tools.  Release doesn’t force you to use a web-based IDE or change anything about your development, build, or deployment process.\n\nWhile other solutions may give you these same benefits, Release has always maintained a focus on merging development, staging, and production, meaning we can offer further benefits that our competitors simply can’t:\n\n- Instant datasets so that you can develop using production or production-like data.  You will no longer need to worry anymore that your seed data and production data have diverged. Test all of your migrations during development on real data before running them in production.  Customer-facing bugs that only appear on production can now be debugged and fixed in record time!\n- Model the most complicated applications and develop against multiple services at once, even if each service needs its own data store and custom configuration.\n\nWith Release, you have the best of all worlds and can finally feel confident that “works on my machine” will be a thing of the past.\n\nLet’s take a look at how you can get started with Release remote development environments.\n\n### Getting Started\n\nStarting remote development with Release consists of two steps:\n\n- Add configuration to a particular environment or Application Template.\n- Use the Release [CLI](https://docs.releasehub.com/cli/getting-started) to set up the environment in development mode.\n\nEach step takes less than a minute to set up and you will be happily remote developing in record time!\n\nWatch a [video showing the set-up steps](https://vimeo.com/manage/videos/762779889/privacy) in detail with an example or read our [how-to-guide](https://gist.github.com/NickBusey/a2107705c3c524283ae725c0ca44f4b0) for more details.\n\n### Configuration\n\nTo enable remote development on an ephemeral environment you will need to add a new high-level stanza to your Application Template or Environment Configuration.\n\n```yaml\n\ndevelopment_environment:\n  services:\n   - name: api #references service from config\n     command: bundle exec rails s -b 0.0.0.0 #optional, can use default command if the same\n     sync: #block to tell Release which paths to sync from local to remote\n     - local_path: \".\"  #path on your local machine to sync to...\n       remote_path: \"/app\" #this remote path in the cloud\n     port_forwards: #port mapping from local to remote, localhost:3010 now points to remote end-point\n     - remote_port: 3000\n       local_port: 3010\n   - name: sidekiq #develop against multiple services\n     command: bundle exec sidekiq\n     sync:\n     - local_path: \".\"\n       remote_path: \"/app\"\n\n```\n\n *Example of multi-service remote development*\n\nThis example allows you to develop against two different services, in the cloud, while exposing ports for the API service but not the _sidekiq_ service. Once you save this configuration, you can use the CLI to activate development mode for this particular environment. You can view the full documentation [here](https://docs.releasehub.com/reference-documentation/application-settings/application-template/schema-definition#development-environments).\n\nThe above syntax works per environment or at the application level and affects all ephemeral environments.\n\n### CLI\n\nNext, use the CLI to set up a bidirectional sync between your local machine and the remote environment. Release remote development environments use a sync process to mirror changes locally with your development environment. This allows you to work locally should you have connectivity issues, unlike developing directly on a remote container in the cloud.\n\nFrom the directory with the source code for the particular environment, run a single command using the Release CLI:\n\n```yaml\nrelease envs dev –app backend –environment env_id\n```\n\n_‘env_id’ is the ID of the environment_\n\nThe CLI will:\n\n- Start a deployment and set the environment to development mode using your configuration.\n- Activate the bidirectional sync service.\n- Set up the port forwards.\n- Launch the command you defined locally via SSH. This allows you to interact with your container as if it was local. You can run commands on the container and send signals (such as ctrl-c) as if it was a local service.\n\n‍\n\n![](/blog-images/04e1b62140aab9d5af33492b452af744.gif)\n\nThat’s it! You are now ready to start making changes locally that will be synced to your remote containers.\n\n### Conclusion\n\nThe addition of Remote Development Environments to our services makes Release the most complete EaaS (Environments as a Service) platform available. Your entire development life cycle can now be implemented using Release environments, from development to production.\n\nThe adoption of new technologies, like k8s, can result in massive changes or regressions in our development process. Release mitigates these trade-offs by offering a complete solution in one platform, including features like instant datasets and developing against multiple services that just aren't possible with other solutions.\n\nNot quite ready to do it on your own? [Set up time](https://calendly.com/d/d35-s6p-4nf/releasehub-full-stack-environment-automation?month=2022-10) with our team to walk you through.\n",
          "code": "var Component=(()=>{var m=Object.create;var a=Object.defineProperty;var h=Object.getOwnPropertyDescriptor;var p=Object.getOwnPropertyNames;var u=Object.getPrototypeOf,g=Object.prototype.hasOwnProperty;var v=(t,e)=>()=>(e||t((e={exports:{}}).exports,e),e.exports),f=(t,e)=>{for(var o in e)a(t,o,{get:e[o],enumerable:!0})},l=(t,e,o,r)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let i of p(e))!g.call(t,i)&&i!==o&&a(t,i,{get:()=>e[i],enumerable:!(r=h(e,i))||r.enumerable});return t};var y=(t,e,o)=>(o=t!=null?m(u(t)):{},l(e||!t||!t.__esModule?a(o,\"default\",{value:t,enumerable:!0}):o,t)),w=t=>l(a({},\"__esModule\",{value:!0}),t);var c=v((T,s)=>{s.exports=_jsx_runtime});var x={};f(x,{default:()=>R,frontmatter:()=>b});var n=y(c()),b={title:\"Remote Development Environments\",summary:\"Use Remote Development Environments to get instant feedback using production like data as early as the development stage\",publishDate:\"Fri Oct 21 2022 21:07:58 GMT+0000 (Coordinated Universal Time)\",author:\"erik-landerholm\",readingTime:4,categories:[\"platform-engineering\",\"product\"],mainImage:\"/blog-images/dbd507c99d76242126c0c7d52fafeda2.jpg\",imageAlt:\"Remote Development Environments\",showCTA:!0,ctaCopy:\"Unlock seamless remote development with production-like environments using Release for faster bug resolution and consistent deployments.\",ctaLink:\"https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=remote-development-environments\",relatedPosts:[\"\"],ogImage:\"/blog-images/dbd507c99d76242126c0c7d52fafeda2.jpg\",excerpt:\"Use Remote Development Environments to get instant feedback using production like data as early as the development stage\",tags:[\"platform-engineering\",\"product\"],ctaButton:\"Try Release for Free\"};function d(t){let e=Object.assign({h2:\"h2\",a:\"a\",span:\"span\",p:\"p\",ul:\"ul\",li:\"li\",h3:\"h3\",pre:\"pre\",code:\"code\",em:\"em\",img:\"img\"},t.components);return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsxs)(e.h2,{id:\"introducing-remote-development-environments\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#introducing-remote-development-environments\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Introducing Remote Development Environments\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Kubernetes is the best way to manage and orchestrate modern, complex container-based applications in the cloud. \\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:\"While this is great for production environments it poses serious challenges for developers and DevOps practitioners. The differences between development and production often result in the utterance of the most dreaded words in application development, \\u201CBut it works on my machine!\\u201D at the most inopportune times, specifically when there is an outage in production. \\xA0\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"There are a few ways to mitigate this issue, but the only \\u201C\",(0,n.jsx)(e.a,{href:\"https://www.youtube.com/watch?v=rYldJY3t9zk\",children:\"win-win-win\"}),\"\\u201D solution is a development environment that replicates production. Historically, this was often too difficult, time-consuming, and expensive, but Release has bridged the gap to provide you with a single pane of glass for all categories of environments you might need.\"]}),`\n`,(0,n.jsx)(e.p,{children:\"You may already know that Release can create ephemeral environments on pull requests and run your staging and production environments. But did you know that Release can also install your SaaS-based offerings in your customer\\u2019s VPCs, all from the same UI/CLI?\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:\"With this new feature, we have added production-like, cloud-based development environments to our industry-leading offerings. Your developer teams now get all the benefits of a traditional cloud-based development environment:\"}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsx)(e.li,{children:\"Write code on your machine (thin client, laptop, small supercomputer under your desk), but run it remotely with production mirroring hardware, software and configuration.\"}),`\n`,(0,n.jsx)(e.li,{children:\"Total control over the configuration of remote development environments: You can build remote development environments to be as much like production as you need.\"}),`\n`,(0,n.jsx)(e.li,{children:\"Bring your own tools. \\xA0Release doesn\\u2019t force you to use a web-based IDE or change anything about your development, build, or deployment process.\"}),`\n`]}),`\n`,(0,n.jsx)(e.p,{children:\"While other solutions may give you these same benefits, Release has always maintained a focus on merging development, staging, and production, meaning we can offer further benefits that our competitors simply can\\u2019t:\"}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsx)(e.li,{children:\"Instant datasets so that you can develop using production or production-like data. \\xA0You will no longer need to worry anymore that your seed data and production data have diverged. Test all of your migrations during development on real data before running them in production. \\xA0Customer-facing bugs that only appear on production can now be debugged and fixed in record time!\"}),`\n`,(0,n.jsx)(e.li,{children:\"Model the most complicated applications and develop against multiple services at once, even if each service needs its own data store and custom configuration.\"}),`\n`]}),`\n`,(0,n.jsx)(e.p,{children:\"With Release, you have the best of all worlds and can finally feel confident that \\u201Cworks on my machine\\u201D will be a thing of the past.\"}),`\n`,(0,n.jsx)(e.p,{children:\"Let\\u2019s take a look at how you can get started with Release remote development environments.\"}),`\n`,(0,n.jsxs)(e.h3,{id:\"getting-started\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#getting-started\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Getting Started\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Starting remote development with Release consists of two steps:\"}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsx)(e.li,{children:\"Add configuration to a particular environment or Application Template.\"}),`\n`,(0,n.jsxs)(e.li,{children:[\"Use the Release \",(0,n.jsx)(e.a,{href:\"https://docs.releasehub.com/cli/getting-started\",children:\"CLI\"}),\" to set up the environment in development mode.\"]}),`\n`]}),`\n`,(0,n.jsx)(e.p,{children:\"Each step takes less than a minute to set up and you will be happily remote developing in record time!\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"Watch a \",(0,n.jsx)(e.a,{href:\"https://vimeo.com/manage/videos/762779889/privacy\",children:\"video showing the set-up steps\"}),\" in detail with an example or read our \",(0,n.jsx)(e.a,{href:\"https://gist.github.com/NickBusey/a2107705c3c524283ae725c0ca44f4b0\",children:\"how-to-guide\"}),\" for more details.\"]}),`\n`,(0,n.jsxs)(e.h3,{id:\"configuration\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#configuration\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Configuration\"]}),`\n`,(0,n.jsx)(e.p,{children:\"To enable remote development on an ephemeral environment you will need to add a new high-level stanza to your Application Template or Environment Configuration.\"}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-yaml\",children:`\ndevelopment_environment:\n \\xA0services:\n \\xA0 - name: api #references service from config\n \\xA0 \\xA0 command: bundle exec rails s -b 0.0.0.0 #optional, can use default command if the same\n \\xA0 \\xA0 sync: #block to tell Release which paths to sync from local to remote\n \\xA0 \\xA0 - local_path: \".\" \\xA0#path on your local machine to sync to...\n \\xA0 \\xA0 \\xA0 remote_path: \"/app\" #this remote path in the cloud\n \\xA0 \\xA0 port_forwards: #port mapping from local to remote, localhost:3010 now points to remote end-point\n \\xA0 \\xA0 - remote_port: 3000\n \\xA0 \\xA0 \\xA0 local_port: 3010\n \\xA0 - name: sidekiq #develop against multiple services\n \\xA0 \\xA0 command: bundle exec sidekiq\n \\xA0 \\xA0 sync:\n \\xA0 \\xA0 - local_path: \".\"\n \\xA0 \\xA0 \\xA0 remote_path: \"/app\"\n\n`})}),`\n`,(0,n.jsxs)(e.p,{children:[\"\\xA0\",(0,n.jsx)(e.em,{children:\"Example of multi-service remote development\"})]}),`\n`,(0,n.jsxs)(e.p,{children:[\"This example allows you to develop against two different services, in the cloud, while exposing ports for the API service but not the \",(0,n.jsx)(e.em,{children:\"sidekiq\"}),\" service. Once you save this configuration, you can use the CLI to activate development mode for this particular environment. You can view the full documentation \",(0,n.jsx)(e.a,{href:\"https://docs.releasehub.com/reference-documentation/application-settings/application-template/schema-definition#development-environments\",children:\"here\"}),\".\"]}),`\n`,(0,n.jsx)(e.p,{children:\"The above syntax works per environment or at the application level and affects all ephemeral environments.\"}),`\n`,(0,n.jsxs)(e.h3,{id:\"cli\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#cli\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"CLI\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Next, use the CLI to set up a bidirectional sync between your local machine and the remote environment. Release remote development environments use a sync process to mirror changes locally with your development environment. This allows you to work locally should you have connectivity issues, unlike developing directly on a remote container in the cloud.\"}),`\n`,(0,n.jsx)(e.p,{children:\"From the directory with the source code for the particular environment, run a single command using the Release CLI:\"}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-yaml\",children:`release envs dev \\u2013app backend \\u2013environment env_id\n`})}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.em,{children:\"\\u2018env_id\\u2019 is the ID of the environment\"})}),`\n`,(0,n.jsx)(e.p,{children:\"The CLI will:\"}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsx)(e.li,{children:\"Start a deployment and set the environment to development mode using your configuration.\"}),`\n`,(0,n.jsx)(e.li,{children:\"Activate the bidirectional sync service.\"}),`\n`,(0,n.jsx)(e.li,{children:\"Set up the port forwards.\"}),`\n`,(0,n.jsx)(e.li,{children:\"Launch the command you defined locally via SSH. This allows you to interact with your container as if it was local. You can run commands on the container and send signals (such as ctrl-c) as if it was a local service.\"}),`\n`]}),`\n`,(0,n.jsx)(e.p,{children:\"\\u200D\"}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/04e1b62140aab9d5af33492b452af744.gif\",alt:\"\"})}),`\n`,(0,n.jsx)(e.p,{children:\"That\\u2019s it! You are now ready to start making changes locally that will be synced to your remote containers.\"}),`\n`,(0,n.jsxs)(e.h3,{id:\"conclusion\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#conclusion\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Conclusion\"]}),`\n`,(0,n.jsx)(e.p,{children:\"The addition of Remote Development Environments to our services makes Release the most complete EaaS (Environments as a Service) platform available. Your entire development life cycle can now be implemented using Release environments, from development to production.\"}),`\n`,(0,n.jsx)(e.p,{children:\"The adoption of new technologies, like k8s, can result in massive changes or regressions in our development process. Release mitigates these trade-offs by offering a complete solution in one platform, including features like instant datasets and developing against multiple services that just aren't possible with other solutions.\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"Not quite ready to do it on your own? \",(0,n.jsx)(e.a,{href:\"https://calendly.com/d/d35-s6p-4nf/releasehub-full-stack-environment-automation?month=2022-10\",children:\"Set up time\"}),\" with our team to walk you through.\"]})]})}function k(t={}){let{wrapper:e}=t.components||{};return e?(0,n.jsx)(e,Object.assign({},t,{children:(0,n.jsx)(d,t)})):d(t)}var R=k;return w(x);})();\n;return Component;"
        },
        "_id": "blog/posts/remote-development-environments.mdx",
        "_raw": {
          "sourceFilePath": "blog/posts/remote-development-environments.mdx",
          "sourceFileName": "remote-development-environments.mdx",
          "sourceFileDir": "blog/posts",
          "contentType": "mdx",
          "flattenedPath": "blog/posts/remote-development-environments"
        },
        "type": "BlogPost",
        "computedSlug": "remote-development-environments"
      },
      "documentHash": "1739393595027",
      "hasWarnings": false,
      "documentTypeName": "BlogPost"
    },
    "blog/posts/rules-for-putting-together-the-article.mdx": {
      "document": {
        "title": "Rules for putting together the article",
        "summary": "",
        "publishDate": "Mon Apr 03 2023 17:16:30 GMT+0000 (Coordinated Universal Time)",
        "author": "",
        "readingTime": 5,
        "categories": [
          "platform-engineering",
          "product"
        ],
        "mainImage": "",
        "imageAlt": "",
        "showCTA": true,
        "ctaCopy": "Unlock precise environment management with Release. Ensure proper formatting and spacing in your articles effortlessly.",
        "ctaLink": "https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=rules-for-putting-together-the-article",
        "relatedPosts": [
          ""
        ],
        "ogImage": "/blog/default-og-image.png",
        "excerpt": "1 - The article should ALWAYS start with a paragraph or a heading H2 Otherwise the body of the post will be misaligned with the social block  2 - The H1 shou...",
        "tags": [
          "platform-engineering",
          "product"
        ],
        "ctaButton": "Try Release for Free",
        "body": {
          "raw": "\n## 1 - The article should ALWAYS start with a paragraph or a heading H2\n\nOtherwise the body of the post will be misaligned with the social block\n\n### 2 - The H1 should never be used, because this tag is reserved for the blog title\n\nIt's bad for SEO having two or more H1 in the same page\n\n### 3 - It's not needed to add bold to the for the headings, because the weight is already set for the headings.\n\n#### Otherwise the headings will look like this: **Heading bold**\n\n### 4 - It's not needed to add extra space between the paragraphs\n\nIt is not necessary to use space between paragraphs as they already have a defined margin.\n\nAdding extra spaces will leave your text with excess space, as we can see below.  \n‍\n\nCan you see? I told you\n\n### 5 - When working with the block codes it's important to pay attention to spacing\n\n```yaml\n  readiness_probe:\n    exec:\n      command:\n      - curl\n      - \"-Lf\"\n      - http://localhost\n    failure_threshold: 5\n    period_seconds: 30\n    timeout_seconds: 3\n\n```\n\n_If you add a description for the blog, make it italic._\n\n### 6 - When you add an image there is no need to add empty lines above and/or below the image, the image itself is already spacing defined.\n\nHere's how you won't need to add a line between the paragraph and the image:\n\n![](/blog-images/9e8bd025d7f442e799770734b97fc833.png)\n\nImage description\n\nIt will be perfect even if you don't have a description for the image, don't worry.\n\nBut see below that if you have an empty line it will create non-harmonious spacing between the paragraph and the image\n\n‍\n\n![](/blog-images/9e8bd025d7f442e799770734b97fc833.png)\n\n‍\n\n### 7 - Take care to always end your article without empty lines at the end, as this will generate empty space at the end of the article.\n\n### 8 - The proper way to write inline code block:\n\n`release instances terminal`\n",
          "code": "var Component=(()=>{var c=Object.create;var o=Object.defineProperty;var p=Object.getOwnPropertyDescriptor;var g=Object.getOwnPropertyNames;var m=Object.getPrototypeOf,w=Object.prototype.hasOwnProperty;var u=(t,e)=>()=>(e||t((e={exports:{}}).exports,e),e.exports),b=(t,e)=>{for(var n in e)o(t,n,{get:e[n],enumerable:!0})},s=(t,e,n,r)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let i of g(e))!w.call(t,i)&&i!==n&&o(t,i,{get:()=>e[i],enumerable:!(r=p(e,i))||r.enumerable});return t};var f=(t,e,n)=>(n=t!=null?c(m(t)):{},s(e||!t||!t.__esModule?o(n,\"default\",{value:t,enumerable:!0}):n,t)),y=t=>s(o({},\"__esModule\",{value:!0}),t);var l=u((T,h)=>{h.exports=_jsx_runtime});var x={};b(x,{default:()=>v,frontmatter:()=>k});var a=f(l()),k={title:\"Rules for putting together the article\",summary:\"\",publishDate:\"Mon Apr 03 2023 17:16:30 GMT+0000 (Coordinated Universal Time)\",author:\"\",readingTime:5,categories:[\"platform-engineering\",\"product\"],mainImage:\"\",imageAlt:\"\",showCTA:!0,ctaCopy:\"Unlock precise environment management with Release. Ensure proper formatting and spacing in your articles effortlessly.\",ctaLink:\"https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=rules-for-putting-together-the-article\",relatedPosts:[\"\"],ogImage:\"/blog/default-og-image.png\",excerpt:\"1 - The article should ALWAYS start with a paragraph or a heading H2 Otherwise the body of the post will be misaligned with the social block  2 - The H1 shou...\",tags:[\"platform-engineering\",\"product\"],ctaButton:\"Try Release for Free\"};function d(t){let e=Object.assign({h2:\"h2\",a:\"a\",span:\"span\",p:\"p\",h3:\"h3\",h4:\"h4\",strong:\"strong\",br:\"br\",pre:\"pre\",code:\"code\",em:\"em\",img:\"img\"},t.components);return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsxs)(e.h2,{id:\"1---the-article-should-always-start-with-a-paragraph-or-a-heading-h2\",children:[(0,a.jsx)(e.a,{className:\"anchor\",href:\"#1---the-article-should-always-start-with-a-paragraph-or-a-heading-h2\",children:(0,a.jsx)(e.span,{className:\"icon icon-link\"})}),\"1 - The article should ALWAYS start with a paragraph or a heading H2\"]}),`\n`,(0,a.jsx)(e.p,{children:\"Otherwise the body of the post will be misaligned with the social block\"}),`\n`,(0,a.jsxs)(e.h3,{id:\"2---the-h1-should-never-be-used-because-this-tag-is-reserved-for-the-blog-title\",children:[(0,a.jsx)(e.a,{className:\"anchor\",href:\"#2---the-h1-should-never-be-used-because-this-tag-is-reserved-for-the-blog-title\",children:(0,a.jsx)(e.span,{className:\"icon icon-link\"})}),\"2 - The H1 should never be used, because this tag is reserved for the blog title\"]}),`\n`,(0,a.jsx)(e.p,{children:\"It's bad for SEO having two or more H1 in the same page\"}),`\n`,(0,a.jsxs)(e.h3,{id:\"3---its-not-needed-to-add-bold-to-the-for-the-headings-because-the-weight-is-already-set-for-the-headings\",children:[(0,a.jsx)(e.a,{className:\"anchor\",href:\"#3---its-not-needed-to-add-bold-to-the-for-the-headings-because-the-weight-is-already-set-for-the-headings\",children:(0,a.jsx)(e.span,{className:\"icon icon-link\"})}),\"3 - It's not needed to add bold to the for the headings, because the weight is already set for the headings.\"]}),`\n`,(0,a.jsxs)(e.h4,{id:\"otherwise-the-headings-will-look-like-this-heading-bold\",children:[(0,a.jsx)(e.a,{className:\"anchor\",href:\"#otherwise-the-headings-will-look-like-this-heading-bold\",children:(0,a.jsx)(e.span,{className:\"icon icon-link\"})}),\"Otherwise the headings will look like this: \",(0,a.jsx)(e.strong,{children:\"Heading bold\"})]}),`\n`,(0,a.jsxs)(e.h3,{id:\"4---its-not-needed-to-add-extra-space-between-the-paragraphs\",children:[(0,a.jsx)(e.a,{className:\"anchor\",href:\"#4---its-not-needed-to-add-extra-space-between-the-paragraphs\",children:(0,a.jsx)(e.span,{className:\"icon icon-link\"})}),\"4 - It's not needed to add extra space between the paragraphs\"]}),`\n`,(0,a.jsx)(e.p,{children:\"It is not necessary to use space between paragraphs as they already have a defined margin.\"}),`\n`,(0,a.jsxs)(e.p,{children:[\"Adding extra spaces will leave your text with excess space, as we can see below.\",(0,a.jsx)(e.br,{}),`\n`,\"\\u200D\"]}),`\n`,(0,a.jsx)(e.p,{children:\"Can you see? I told you\"}),`\n`,(0,a.jsxs)(e.h3,{id:\"5---when-working-with-the-block-codes-its-important-to-pay-attention-to-spacing\",children:[(0,a.jsx)(e.a,{className:\"anchor\",href:\"#5---when-working-with-the-block-codes-its-important-to-pay-attention-to-spacing\",children:(0,a.jsx)(e.span,{className:\"icon icon-link\"})}),\"5 - When working with the block codes it's important to pay attention to spacing\"]}),`\n`,(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:\"language-yaml\",children:` \\xA0readiness_probe:\n \\xA0 \\xA0exec:\n \\xA0 \\xA0 \\xA0command:\n \\xA0 \\xA0 \\xA0- curl\n \\xA0 \\xA0 \\xA0- \"-Lf\"\n \\xA0 \\xA0 \\xA0- http://localhost\n \\xA0 \\xA0failure_threshold: 5\n \\xA0 \\xA0period_seconds: 30\n \\xA0 \\xA0timeout_seconds: 3\n\n`})}),`\n`,(0,a.jsx)(e.p,{children:(0,a.jsx)(e.em,{children:\"If you add a description for the blog, make it italic.\"})}),`\n`,(0,a.jsxs)(e.h3,{id:\"6---when-you-add-an-image-there-is-no-need-to-add-empty-lines-above-andor-below-the-image-the-image-itself-is-already-spacing-defined\",children:[(0,a.jsx)(e.a,{className:\"anchor\",href:\"#6---when-you-add-an-image-there-is-no-need-to-add-empty-lines-above-andor-below-the-image-the-image-itself-is-already-spacing-defined\",children:(0,a.jsx)(e.span,{className:\"icon icon-link\"})}),\"6 - When you add an image there is no need to add empty lines above and/or below the image, the image itself is already spacing defined.\"]}),`\n`,(0,a.jsx)(e.p,{children:\"Here's how you won't need to add a line between the paragraph and the image:\"}),`\n`,(0,a.jsx)(e.p,{children:(0,a.jsx)(e.img,{src:\"/blog-images/9e8bd025d7f442e799770734b97fc833.png\",alt:\"\"})}),`\n`,(0,a.jsx)(e.p,{children:\"Image description\"}),`\n`,(0,a.jsx)(e.p,{children:\"It will be perfect even if you don't have a description for the image, don't worry.\"}),`\n`,(0,a.jsx)(e.p,{children:\"But see below that if you have an empty line it will create non-harmonious spacing between the paragraph and the image\"}),`\n`,(0,a.jsx)(e.p,{children:\"\\u200D\"}),`\n`,(0,a.jsx)(e.p,{children:(0,a.jsx)(e.img,{src:\"/blog-images/9e8bd025d7f442e799770734b97fc833.png\",alt:\"\"})}),`\n`,(0,a.jsx)(e.p,{children:\"\\u200D\"}),`\n`,(0,a.jsxs)(e.h3,{id:\"7---take-care-to-always-end-your-article-without-empty-lines-at-the-end-as-this-will-generate-empty-space-at-the-end-of-the-article\",children:[(0,a.jsx)(e.a,{className:\"anchor\",href:\"#7---take-care-to-always-end-your-article-without-empty-lines-at-the-end-as-this-will-generate-empty-space-at-the-end-of-the-article\",children:(0,a.jsx)(e.span,{className:\"icon icon-link\"})}),\"7 - Take care to always end your article without empty lines at the end, as this will generate empty space at the end of the article.\"]}),`\n`,(0,a.jsxs)(e.h3,{id:\"8---the-proper-way-to-write-inline-code-block\",children:[(0,a.jsx)(e.a,{className:\"anchor\",href:\"#8---the-proper-way-to-write-inline-code-block\",children:(0,a.jsx)(e.span,{className:\"icon icon-link\"})}),\"8 - The proper way to write inline code block:\"]}),`\n`,(0,a.jsx)(e.p,{children:(0,a.jsx)(e.code,{children:\"release instances terminal\"})})]})}function N(t={}){let{wrapper:e}=t.components||{};return e?(0,a.jsx)(e,Object.assign({},t,{children:(0,a.jsx)(d,t)})):d(t)}var v=N;return y(x);})();\n;return Component;"
        },
        "_id": "blog/posts/rules-for-putting-together-the-article.mdx",
        "_raw": {
          "sourceFilePath": "blog/posts/rules-for-putting-together-the-article.mdx",
          "sourceFileName": "rules-for-putting-together-the-article.mdx",
          "sourceFileDir": "blog/posts",
          "contentType": "mdx",
          "flattenedPath": "blog/posts/rules-for-putting-together-the-article"
        },
        "type": "BlogPost",
        "computedSlug": "rules-for-putting-together-the-article"
      },
      "documentHash": "1739393595027",
      "hasWarnings": false,
      "documentTypeName": "BlogPost"
    },
    "blog/posts/secure-secrets-management-with-doppler-in-release-a-step-by-step-guide.mdx": {
      "document": {
        "title": "Secure Secrets Management with Doppler in Release: A Step-by-Step Guide",
        "summary": "Learn to securely manage secrets in Kubernetes by integrating Doppler with Release, ensuring streamlined and safe deplo",
        "publishDate": "Thu Oct 31 2024 21:42:33 GMT+0000 (Coordinated Universal Time)",
        "author": "david-giffin",
        "readingTime": 10,
        "categories": [
          "kubernetes",
          "platform-engineering"
        ],
        "mainImage": "/blog-images/35f85f82ecb6dea3501e739c09ffe74c.webp",
        "imageAlt": "Release Doppler Integration",
        "showCTA": true,
        "ctaCopy": "Enhance your secrets management with Release's ephemeral environments for secure, synchronized deployments. Streamline workflows and ensure reliable secret handling.",
        "ctaLink": "https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=secure-secrets-management-with-doppler-in-release-a-step-by-step-guide",
        "relatedPosts": [
          "kubernetes-secrets-management-a-practical-guide; how-to-manage-gitops-secrets-a-detailed-guide; beyond-k8s-introduction-to-ephemeral-environments"
        ],
        "ogImage": "/blog-images/35f85f82ecb6dea3501e739c09ffe74c.webp",
        "excerpt": "Learn to securely manage secrets in Kubernetes by integrating Doppler with Release, ensuring streamlined and safe deplo",
        "tags": [
          "kubernetes",
          "platform-engineering"
        ],
        "ctaButton": "Try Release for Free",
        "body": {
          "raw": "\nManaging secrets securely is essential for any modern application infrastructure, and integrating [Doppler Secrets Manager](https://www.doppler.com/) with Release can streamline this process. This guide covers how to set up and configure Doppler to work seamlessly with your Release environments, providing a reliable and scalable way to manage secrets across various services and jobs.\n\n### **What is Doppler Secrets Manager?**\n\nDoppler Secrets Manager provides a secure, centralized way to manage and inject sensitive information, such as API keys and database credentials, into your applications. Through the integration of Doppler and Release, you can ensure that these secrets are securely synchronized with your Kubernetes clusters in Release environments.\n\n### **Prerequisites**\n\nBefore you dive into the setup, make sure you have the following ready:\n\n- **Access to a Release environment** with configured Kubernetes clusters.\n- **Release CLI** installed and configured on your local machine.\n- **Doppler account** with generated service tokens that have the necessary permissions.\n\n### **Integrating Doppler with Release**\n\nLet's walk through the steps to securely manage secrets using Doppler in your Release environment.\n\n#### **Step 1: Set Up Kubeconfig for Your Release Cluster**\n\nFirst, you'll need to configure your kubeconfig to access the Release cluster. This will allow kubectl to interact with your Release environment directly.\n\nRun the following commands to generate and configure kubeconfig:\n**bash**\n\n```bash\n\nrelease clusters kubeconfig --account Release --cluster release-development ./export KUBECONFIG=./config-release-development.yaml\n\n```\n\nWith this, the kubeconfig for your Release cluster is now set as the current context for kubectl.\n\n#### **Step 2: Install the Doppler Kubernetes Operator**\n\nThe Doppler Kubernetes Operator is responsible for syncing secrets from Doppler to your Kubernetes environment. To install it, start by adding the Doppler Helm repository and installing the operator:\n**bash**\n\n```bash\n\nhelm repo add doppler https://helm.doppler.comhelm install --generate-name doppler/doppler-kubernetes-operator\n\n```\n\nThis setup deploys the Doppler Kubernetes Operator, ready to securely synchronize secrets.\n\n#### **Step 3: Create a Doppler Token Secret in Kubernetes**\n\nTo allow the Doppler Operator access to your secrets, you’ll need to create a Kubernetes secret with your Doppler service token. Replace YOUR_DOPPLER_SERVICE_TOKEN with the actual token from your Doppler account.\n**bash**\n\n```bash\n\nkubectl create secret generic doppler-token-secret \\  --namespace doppler-operator-system \\  --from-literal=serviceToken=YOUR_DOPPLER_SERVICE_TOKEN\n\n```\n\nWith this token in place, the Doppler Operator can access and synchronize secrets into your Kubernetes environment.\n\n#### **Step 4: Configure Your Release Application to Use Doppler Secrets**\n\nNow, configure your application in Release to use these Doppler-managed secrets. Begin by defining the secrets you need in Doppler, associating each set with a specific Doppler project and configuration. In Release, link these secrets to your services using the secrets_from field within the service configuration. This enables each service to access only the secrets it needs, ensuring secure, targeted access.\n\nFor example, you can define separate secret configurations for a Rails and an AI project:\n**yaml**\n\n```yaml\n\nsecrets:\n  - name: development\n    type: doppler\n    project: rails\n    config: dev\n  - name: development-ai\n    type: doppler\n    project: ai\n    config: dev\n\n```\n\n`‍`‍\n\nNext, associate these secrets with the respective services in Release:\n**yaml**\n\n```yaml\n\nservices:\n  - name: rails\n    image: github-org/rails\n    secrets_from:\n      - development\n  - name: ai-chatbot\n    image: github-org/ai-chatbot\n    secrets_from:\n      - development-ai\njobs:\n  - name: chatbot-setup\n    image: github-org/rails\n    secrets_from:\n      - development\n      - development-ai\n    steps:\n      - run: bundle exec rake chatbot:setup\n\n```\n\nIn this setup:\n\n- The **Rails service** pulls the development secrets from the Rails project in Doppler.\n- The **AI chatbot service** accesses the development-ai secrets from the AI project in Doppler.\n\nThis configuration keeps secrets streamlined and service-specific, enhancing security and simplifying secret management across your environments.\n\n### **Troubleshooting Doppler Secrets Synchronization Issues**\n\nIf you encounter issues accessing secrets, you can view the Doppler operator logs to diagnose and resolve synchronization issues:\n**bash**\n\n```bash\n\nkubectl logs -f deployment/doppler-operator-controller-manager -n doppler-operator-system\n\n```\n\nThis command lets you track the Doppler operator's logs for any potential issues. Common errors include incorrect service account permissions, invalid service tokens, or misconfigured Doppler projects and configurations.\n\n## Conclusion\n\nIntegrating Doppler with Release provides a robust, scalable solution for managing secrets in Kubernetes. By following these steps, you can securely manage secrets across multiple services, enhancing security and maintaining control over sensitive data in your Release environments. With Doppler Secrets Manager, Release environments become even more secure and manageable, empowering development teams to focus on building rather than managing configurations.\n\n‍\n",
          "code": "var Component=(()=>{var h=Object.create;var a=Object.defineProperty;var d=Object.getOwnPropertyDescriptor;var u=Object.getOwnPropertyNames;var g=Object.getPrototypeOf,m=Object.prototype.hasOwnProperty;var f=(r,e)=>()=>(e||r((e={exports:{}}).exports,e),e.exports),y=(r,e)=>{for(var s in e)a(r,s,{get:e[s],enumerable:!0})},i=(r,e,s,o)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let t of u(e))!m.call(r,t)&&t!==s&&a(r,t,{get:()=>e[t],enumerable:!(o=d(e,t))||o.enumerable});return r};var b=(r,e,s)=>(s=r!=null?h(g(r)):{},i(e||!r||!r.__esModule?a(s,\"default\",{value:r,enumerable:!0}):s,r)),v=r=>i(a({},\"__esModule\",{value:!0}),r);var l=f((T,c)=>{c.exports=_jsx_runtime});var R={};y(R,{default:()=>D,frontmatter:()=>k});var n=b(l()),k={title:\"Secure Secrets Management with Doppler in Release: A Step-by-Step Guide\",summary:\"Learn to securely manage secrets in Kubernetes by integrating Doppler with Release, ensuring streamlined and safe deplo\",publishDate:\"Thu Oct 31 2024 21:42:33 GMT+0000 (Coordinated Universal Time)\",author:\"david-giffin\",readingTime:10,categories:[\"kubernetes\",\"platform-engineering\"],mainImage:\"/blog-images/35f85f82ecb6dea3501e739c09ffe74c.webp\",imageAlt:\"Release Doppler Integration\",showCTA:!0,ctaCopy:\"Enhance your secrets management with Release's ephemeral environments for secure, synchronized deployments. Streamline workflows and ensure reliable secret handling.\",ctaLink:\"https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=secure-secrets-management-with-doppler-in-release-a-step-by-step-guide\",relatedPosts:[\"kubernetes-secrets-management-a-practical-guide; how-to-manage-gitops-secrets-a-detailed-guide; beyond-k8s-introduction-to-ephemeral-environments\"],ogImage:\"/blog-images/35f85f82ecb6dea3501e739c09ffe74c.webp\",excerpt:\"Learn to securely manage secrets in Kubernetes by integrating Doppler with Release, ensuring streamlined and safe deplo\",tags:[\"kubernetes\",\"platform-engineering\"],ctaButton:\"Try Release for Free\"};function p(r){let e=Object.assign({p:\"p\",a:\"a\",h3:\"h3\",span:\"span\",strong:\"strong\",ul:\"ul\",li:\"li\",h4:\"h4\",pre:\"pre\",code:\"code\",h2:\"h2\"},r.components);return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsxs)(e.p,{children:[\"Managing secrets securely is essential for any modern application infrastructure, and integrating \",(0,n.jsx)(e.a,{href:\"https://www.doppler.com/\",children:\"Doppler Secrets Manager\"}),\" with Release can streamline this process. This guide covers how to set up and configure Doppler to work seamlessly with your Release environments, providing a reliable and scalable way to manage secrets across various services and jobs.\"]}),`\n`,(0,n.jsxs)(e.h3,{id:\"what-is-doppler-secrets-manager\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#what-is-doppler-secrets-manager\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.strong,{children:\"What is Doppler Secrets Manager?\"})]}),`\n`,(0,n.jsx)(e.p,{children:\"Doppler Secrets Manager provides a secure, centralized way to manage and inject sensitive information, such as API keys and database credentials, into your applications. Through the integration of Doppler and Release, you can ensure that these secrets are securely synchronized with your Kubernetes clusters in Release environments.\"}),`\n`,(0,n.jsxs)(e.h3,{id:\"prerequisites\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#prerequisites\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.strong,{children:\"Prerequisites\"})]}),`\n`,(0,n.jsx)(e.p,{children:\"Before you dive into the setup, make sure you have the following ready:\"}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"Access to a Release environment\"}),\" with configured Kubernetes clusters.\"]}),`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"Release CLI\"}),\" installed and configured on your local machine.\"]}),`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"Doppler account\"}),\" with generated service tokens that have the necessary permissions.\"]}),`\n`]}),`\n`,(0,n.jsxs)(e.h3,{id:\"integrating-doppler-with-release\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#integrating-doppler-with-release\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.strong,{children:\"Integrating Doppler with Release\"})]}),`\n`,(0,n.jsx)(e.p,{children:\"Let's walk through the steps to securely manage secrets using Doppler in your Release environment.\"}),`\n`,(0,n.jsxs)(e.h4,{id:\"step-1-set-up-kubeconfig-for-your-release-cluster\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#step-1-set-up-kubeconfig-for-your-release-cluster\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.strong,{children:\"Step 1: Set Up Kubeconfig for Your Release Cluster\"})]}),`\n`,(0,n.jsx)(e.p,{children:\"First, you'll need to configure your kubeconfig to access the Release cluster. This will allow kubectl to interact with your Release environment directly.\"}),`\n`,(0,n.jsxs)(e.p,{children:[`Run the following commands to generate and configure kubeconfig:\n`,(0,n.jsx)(e.strong,{children:\"bash\"})]}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-bash\",children:`\nrelease clusters kubeconfig --account Release --cluster release-development ./export KUBECONFIG=./config-release-development.yaml\n\n`})}),`\n`,(0,n.jsx)(e.p,{children:\"With this, the kubeconfig for your Release cluster is now set as the current context for kubectl.\"}),`\n`,(0,n.jsxs)(e.h4,{id:\"step-2-install-the-doppler-kubernetes-operator\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#step-2-install-the-doppler-kubernetes-operator\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.strong,{children:\"Step 2: Install the Doppler Kubernetes Operator\"})]}),`\n`,(0,n.jsxs)(e.p,{children:[`The Doppler Kubernetes Operator is responsible for syncing secrets from Doppler to your Kubernetes environment. To install it, start by adding the Doppler Helm repository and installing the operator:\n`,(0,n.jsx)(e.strong,{children:\"bash\"})]}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-bash\",children:`\nhelm repo add doppler https://helm.doppler.comhelm install --generate-name doppler/doppler-kubernetes-operator\n\n`})}),`\n`,(0,n.jsx)(e.p,{children:\"This setup deploys the Doppler Kubernetes Operator, ready to securely synchronize secrets.\"}),`\n`,(0,n.jsxs)(e.h4,{id:\"step-3-create-a-doppler-token-secret-in-kubernetes\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#step-3-create-a-doppler-token-secret-in-kubernetes\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.strong,{children:\"Step 3: Create a Doppler Token Secret in Kubernetes\"})]}),`\n`,(0,n.jsxs)(e.p,{children:[`To allow the Doppler Operator access to your secrets, you\\u2019ll need to create a Kubernetes secret with your Doppler service token. Replace YOUR_DOPPLER_SERVICE_TOKEN with the actual token from your Doppler account.\n`,(0,n.jsx)(e.strong,{children:\"bash\"})]}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-bash\",children:`\nkubectl create secret generic doppler-token-secret \\\\\\xA0\\xA0--namespace doppler-operator-system \\\\\\xA0\\xA0--from-literal=serviceToken=YOUR_DOPPLER_SERVICE_TOKEN\n\n`})}),`\n`,(0,n.jsx)(e.p,{children:\"With this token in place, the Doppler Operator can access and synchronize secrets into your Kubernetes environment.\"}),`\n`,(0,n.jsxs)(e.h4,{id:\"step-4-configure-your-release-application-to-use-doppler-secrets\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#step-4-configure-your-release-application-to-use-doppler-secrets\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.strong,{children:\"Step 4: Configure Your Release Application to Use Doppler Secrets\"})]}),`\n`,(0,n.jsx)(e.p,{children:\"Now, configure your application in Release to use these Doppler-managed secrets. Begin by defining the secrets you need in Doppler, associating each set with a specific Doppler project and configuration. In Release, link these secrets to your services using the secrets_from field within the service configuration. This enables each service to access only the secrets it needs, ensuring secure, targeted access.\"}),`\n`,(0,n.jsxs)(e.p,{children:[`For example, you can define separate secret configurations for a Rails and an AI project:\n`,(0,n.jsx)(e.strong,{children:\"yaml\"})]}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-yaml\",children:`\nsecrets:\n \\xA0- name: development\n \\xA0 \\xA0type: doppler\n \\xA0 \\xA0project: rails\n \\xA0 \\xA0config: dev\n \\xA0- name: development-ai\n \\xA0 \\xA0type: doppler\n \\xA0 \\xA0project: ai\n \\xA0 \\xA0config: dev\n\n`})}),`\n`,(0,n.jsxs)(e.p,{children:[(0,n.jsx)(e.code,{children:\"\\u200D\"}),\"\\u200D\"]}),`\n`,(0,n.jsxs)(e.p,{children:[`Next, associate these secrets with the respective services in Release:\n`,(0,n.jsx)(e.strong,{children:\"yaml\"})]}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-yaml\",children:`\nservices:\n \\xA0- name: rails\n \\xA0 \\xA0image: github-org/rails\n \\xA0 \\xA0secrets_from:\n \\xA0 \\xA0 \\xA0- development\n \\xA0- name: ai-chatbot\n \\xA0 \\xA0image: github-org/ai-chatbot\n \\xA0 \\xA0secrets_from:\n \\xA0 \\xA0 \\xA0- development-ai\njobs:\n \\xA0- name: chatbot-setup\n \\xA0 \\xA0image: github-org/rails\n \\xA0 \\xA0secrets_from:\n \\xA0 \\xA0 \\xA0- development\n \\xA0 \\xA0 \\xA0- development-ai\n \\xA0 \\xA0steps:\n \\xA0 \\xA0 \\xA0- run: bundle exec rake chatbot:setup\n\n`})}),`\n`,(0,n.jsx)(e.p,{children:\"In this setup:\"}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsxs)(e.li,{children:[\"The \",(0,n.jsx)(e.strong,{children:\"Rails service\"}),\" pulls the development secrets from the Rails project in Doppler.\"]}),`\n`,(0,n.jsxs)(e.li,{children:[\"The \",(0,n.jsx)(e.strong,{children:\"AI chatbot service\"}),\" accesses the development-ai secrets from the AI project in Doppler.\"]}),`\n`]}),`\n`,(0,n.jsx)(e.p,{children:\"This configuration keeps secrets streamlined and service-specific, enhancing security and simplifying secret management across your environments.\"}),`\n`,(0,n.jsxs)(e.h3,{id:\"troubleshooting-doppler-secrets-synchronization-issues\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#troubleshooting-doppler-secrets-synchronization-issues\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.strong,{children:\"Troubleshooting Doppler Secrets Synchronization Issues\"})]}),`\n`,(0,n.jsxs)(e.p,{children:[`If you encounter issues accessing secrets, you can view the Doppler operator logs to diagnose and resolve synchronization issues:\n`,(0,n.jsx)(e.strong,{children:\"bash\"})]}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-bash\",children:`\nkubectl logs -f deployment/doppler-operator-controller-manager -n doppler-operator-system\n\n`})}),`\n`,(0,n.jsx)(e.p,{children:\"This command lets you track the Doppler operator's logs for any potential issues. Common errors include incorrect service account permissions, invalid service tokens, or misconfigured Doppler projects and configurations.\"}),`\n`,(0,n.jsxs)(e.h2,{id:\"conclusion\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#conclusion\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Conclusion\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Integrating Doppler with Release provides a robust, scalable solution for managing secrets in Kubernetes. By following these steps, you can securely manage secrets across multiple services, enhancing security and maintaining control over sensitive data in your Release environments. With Doppler Secrets Manager, Release environments become even more secure and manageable, empowering development teams to focus on building rather than managing configurations.\"}),`\n`,(0,n.jsx)(e.p,{children:\"\\u200D\"})]})}function w(r={}){let{wrapper:e}=r.components||{};return e?(0,n.jsx)(e,Object.assign({},r,{children:(0,n.jsx)(p,r)})):p(r)}var D=w;return v(R);})();\n;return Component;"
        },
        "_id": "blog/posts/secure-secrets-management-with-doppler-in-release-a-step-by-step-guide.mdx",
        "_raw": {
          "sourceFilePath": "blog/posts/secure-secrets-management-with-doppler-in-release-a-step-by-step-guide.mdx",
          "sourceFileName": "secure-secrets-management-with-doppler-in-release-a-step-by-step-guide.mdx",
          "sourceFileDir": "blog/posts",
          "contentType": "mdx",
          "flattenedPath": "blog/posts/secure-secrets-management-with-doppler-in-release-a-step-by-step-guide"
        },
        "type": "BlogPost",
        "computedSlug": "secure-secrets-management-with-doppler-in-release-a-step-by-step-guide"
      },
      "documentHash": "1739393595027",
      "hasWarnings": false,
      "documentTypeName": "BlogPost"
    },
    "blog/posts/see-you-at-google-cloud-next-24.mdx": {
      "document": {
        "title": "See you at Google Cloud Next '24!",
        "summary": "Release is heading to Las Vegas for Google Cloud Next '24 (April 9-11).",
        "publishDate": "Mon Apr 01 2024 17:43:24 GMT+0000 (Coordinated Universal Time)",
        "author": "annika-major",
        "readingTime": 3,
        "categories": [
          "platform-engineering",
          "events"
        ],
        "mainImage": "/blog-images/1c0b6c0cd6405902cf1d5cf6e8856028.webp",
        "imageAlt": "Google Cloud Next 24",
        "showCTA": true,
        "ctaCopy": "Optimize your Google Cloud Next '24 experience with Release's ephemeral environments for streamlined collaboration and faster testing.",
        "ctaLink": "https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=see-you-at-google-cloud-next-24",
        "relatedPosts": [
          ""
        ],
        "ogImage": "/blog-images/1c0b6c0cd6405902cf1d5cf6e8856028.webp",
        "excerpt": "Release is heading to Las Vegas for Google Cloud Next '24 (April 9-11).",
        "tags": [
          "platform-engineering",
          "events"
        ],
        "ctaButton": "Try Release for Free",
        "body": {
          "raw": "\nRelease is heading to Las Vegas for [**Google Cloud Next '24**](https://cloud.withgoogle.com/next) (April 9-11). We’re looking forward to sharing product updates, meeting with the community and seeing how Google will shape the future of cloud technology. From artificial intelligence applications to infrastructure solutions, Google Cloud Next '24 represents a new chapter in digital transformation.\n\nStop by the Release booth (#1202), where we'll be showcasing our latest products and advancements, including our [early access program for Release.ai](http://release.ai).\n\n## **What to Expect at Google Cloud Next '24**\n\nWith over 400 sessions, Google Cloud Next, attendees can expect valuable insights, interactive sessions, and networking opportunities. We’re looking forward to sessions focused on:\n\n- AI and ML\n- Application Developers\n- Infrastructure Architecture and IT Operators\n- Data Analysts, Data Scientists, Data Engineers\n- Database Professionals\n- DevOps, ITOps, Platform Engineers, SREs\n- IT Managers and Business Leaders\n- Productivity and Collaboration\n- Security Professionals\n\n## **Who Should Attend Google Cloud Next ‘24**\n\nGoogle Cloud Next is tailored to cater to a diverse range of individuals, including:\n\n- IT professionals and managers seeking to enhance their knowledge and skills.\n- Software developers and architects aiming to stay updated with the latest advancements.\n- Cloud engineers and consultants dedicated to optimizing cloud infrastructure.\n- Business leaders who aspire to harness the power of cloud technologies for both growth and innovation.\n\n## **Release at Google Cloud Next '24**\n\nGoogle Cloud Next '24 is an important event in the industry. It's a chance to share knowledge, discover new ideas, and connect with others. If you're at the event, head over to **Release (booth #1202)** where you can learn more about our products, and sign up for our [early access program for Release.ai](http://release.ai).\n",
          "code": "var Component=(()=>{var h=Object.create;var i=Object.defineProperty;var g=Object.getOwnPropertyDescriptor;var u=Object.getOwnPropertyNames;var p=Object.getPrototypeOf,m=Object.prototype.hasOwnProperty;var f=(o,e)=>()=>(e||o((e={exports:{}}).exports,e),e.exports),x=(o,e)=>{for(var t in e)i(o,t,{get:e[t],enumerable:!0})},l=(o,e,t,r)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let a of u(e))!m.call(o,a)&&a!==t&&i(o,a,{get:()=>e[a],enumerable:!(r=g(e,a))||r.enumerable});return o};var w=(o,e,t)=>(t=o!=null?h(p(o)):{},l(e||!o||!o.__esModule?i(t,\"default\",{value:o,enumerable:!0}):t,o)),b=o=>l(i({},\"__esModule\",{value:!0}),o);var c=f((k,s)=>{s.exports=_jsx_runtime});var N={};x(N,{default:()=>y,frontmatter:()=>v});var n=w(c()),v={title:\"See you at Google Cloud Next '24!\",summary:\"Release is heading to Las Vegas for Google Cloud Next '24 (April 9-11).\",publishDate:\"Mon Apr 01 2024 17:43:24 GMT+0000 (Coordinated Universal Time)\",author:\"annika-major\",readingTime:3,categories:[\"platform-engineering\",\"events\"],mainImage:\"/blog-images/1c0b6c0cd6405902cf1d5cf6e8856028.webp\",imageAlt:\"Google Cloud Next 24\",showCTA:!0,ctaCopy:\"Optimize your Google Cloud Next '24 experience with Release's ephemeral environments for streamlined collaboration and faster testing.\",ctaLink:\"https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=see-you-at-google-cloud-next-24\",relatedPosts:[\"\"],ogImage:\"/blog-images/1c0b6c0cd6405902cf1d5cf6e8856028.webp\",excerpt:\"Release is heading to Las Vegas for Google Cloud Next '24 (April 9-11).\",tags:[\"platform-engineering\",\"events\"],ctaButton:\"Try Release for Free\"};function d(o){let e=Object.assign({p:\"p\",a:\"a\",strong:\"strong\",h2:\"h2\",span:\"span\",ul:\"ul\",li:\"li\"},o.components);return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsxs)(e.p,{children:[\"Release is heading to Las Vegas for \",(0,n.jsx)(e.a,{href:\"https://cloud.withgoogle.com/next\",children:(0,n.jsx)(e.strong,{children:\"Google Cloud Next '24\"})}),\" (April 9-11). We\\u2019re looking forward to sharing product updates, meeting with the community and seeing how Google will shape the future of cloud technology. From artificial intelligence applications to infrastructure solutions, Google Cloud Next '24 represents a new chapter in digital transformation.\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"Stop by the Release booth (#1202), where we'll be showcasing our latest products and advancements, including our \",(0,n.jsx)(e.a,{href:\"http://release.ai\",children:\"early access program for Release.ai\"}),\".\"]}),`\n`,(0,n.jsxs)(e.h2,{id:\"what-to-expect-at-google-cloud-next-24\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#what-to-expect-at-google-cloud-next-24\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.strong,{children:\"What to Expect at Google Cloud Next '24\"})]}),`\n`,(0,n.jsx)(e.p,{children:\"With over 400 sessions, Google Cloud Next, attendees can expect valuable insights, interactive sessions, and networking opportunities. We\\u2019re looking forward to sessions focused on:\"}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsx)(e.li,{children:\"AI and ML\"}),`\n`,(0,n.jsx)(e.li,{children:\"Application Developers\"}),`\n`,(0,n.jsx)(e.li,{children:\"Infrastructure Architecture and IT Operators\"}),`\n`,(0,n.jsx)(e.li,{children:\"Data Analysts, Data Scientists, Data Engineers\"}),`\n`,(0,n.jsx)(e.li,{children:\"Database Professionals\"}),`\n`,(0,n.jsx)(e.li,{children:\"DevOps, ITOps, Platform Engineers, SREs\"}),`\n`,(0,n.jsx)(e.li,{children:\"IT Managers and Business Leaders\"}),`\n`,(0,n.jsx)(e.li,{children:\"Productivity and Collaboration\"}),`\n`,(0,n.jsx)(e.li,{children:\"Security Professionals\"}),`\n`]}),`\n`,(0,n.jsxs)(e.h2,{id:\"who-should-attend-google-cloud-next-24\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#who-should-attend-google-cloud-next-24\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.strong,{children:\"Who Should Attend Google Cloud Next \\u201824\"})]}),`\n`,(0,n.jsx)(e.p,{children:\"Google Cloud Next is tailored to cater to a diverse range of individuals, including:\"}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsx)(e.li,{children:\"IT professionals and managers seeking to enhance their knowledge and skills.\"}),`\n`,(0,n.jsx)(e.li,{children:\"Software developers and architects aiming to stay updated with the latest advancements.\"}),`\n`,(0,n.jsx)(e.li,{children:\"Cloud engineers and consultants dedicated to optimizing cloud infrastructure.\"}),`\n`,(0,n.jsx)(e.li,{children:\"Business leaders who aspire to harness the power of cloud technologies for both growth and innovation.\"}),`\n`]}),`\n`,(0,n.jsxs)(e.h2,{id:\"release-at-google-cloud-next-24\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#release-at-google-cloud-next-24\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),(0,n.jsx)(e.strong,{children:\"Release at Google Cloud Next '24\"})]}),`\n`,(0,n.jsxs)(e.p,{children:[\"Google Cloud Next '24 is an important event in the industry. It's a chance to share knowledge, discover new ideas, and connect with others. If you're at the event, head over to \",(0,n.jsx)(e.strong,{children:\"Release (booth #1202)\"}),\" where you can learn more about our products, and sign up for our \",(0,n.jsx)(e.a,{href:\"http://release.ai\",children:\"early access program for Release.ai\"}),\".\"]})]})}function C(o={}){let{wrapper:e}=o.components||{};return e?(0,n.jsx)(e,Object.assign({},o,{children:(0,n.jsx)(d,o)})):d(o)}var y=C;return b(N);})();\n;return Component;"
        },
        "_id": "blog/posts/see-you-at-google-cloud-next-24.mdx",
        "_raw": {
          "sourceFilePath": "blog/posts/see-you-at-google-cloud-next-24.mdx",
          "sourceFileName": "see-you-at-google-cloud-next-24.mdx",
          "sourceFileDir": "blog/posts",
          "contentType": "mdx",
          "flattenedPath": "blog/posts/see-you-at-google-cloud-next-24"
        },
        "type": "BlogPost",
        "computedSlug": "see-you-at-google-cloud-next-24"
      },
      "documentHash": "1739393595027",
      "hasWarnings": false,
      "documentTypeName": "BlogPost"
    },
    "blog/posts/setup-test-environment.mdx": {
      "document": {
        "title": "How to Setup Test Environments That Are Easy to Maintain",
        "summary": "How to set up test environment | Data, Scope, environment lifetime, and what is the focus and goal at each stage.",
        "publishDate": "Tue Feb 15 2022 22:04:30 GMT+0000 (Coordinated Universal Time)",
        "author": "taurai-mutimutema",
        "readingTime": 6,
        "categories": [
          "platform-engineering",
          "product"
        ],
        "mainImage": "/blog-images/e860bfc6e52f225fe1a870172da90274.jpg",
        "imageAlt": "How to Setup Test Environments That Are Easy to Maintain",
        "showCTA": true,
        "ctaCopy": "Simplify test environment setup with Release's on-demand environments mirroring production for efficient testing and seamless collaboration.",
        "ctaLink": "https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=setup-test-environment",
        "relatedPosts": [
          ""
        ],
        "ogImage": "/blog-images/e860bfc6e52f225fe1a870172da90274.jpg",
        "excerpt": "How to set up test environment | Data, Scope, environment lifetime, and what is the focus and goal at each stage.",
        "tags": [
          "platform-engineering",
          "product"
        ],
        "ctaButton": "Try Release for Free",
        "body": {
          "raw": "\nSetting up test environments is an integral step in ensuring smooth-running and bug-free applications post deployment. But once they're live, test environments also require the same TLC you invest into production servers. With this comes that double-effort dread. Should developers be spending time managing environments instead of the applications?\n\nThis post explains how your engineers can set up test environments that don't demoralize developers with huge effort requirements. We'll review the various test environment options at your disposal, along with a few common challenges you might encounter during the setup phase. We'll then close with actionable solutions to the pain points around test environments.\n\nBefore we discuss strategies, let's investigate why any of this is necessary.\n\n![](/blog-images/b01106eda919a6d670bc40cc5374a2b5.png)\n\n### Why You Need Preproduction Testing Environments\n\nLet's cut to the chase. If you don't invest time and effort into test environments, you'd almost certainly spend resources correcting faulty performance issues in production. Not to take anything away from your development skills, but testing itself is one skill crucial to the success of any software project. The best way to execute tests is in test environments—replicas of the production environment, accessible only to engineers and testers.\n\nSkipping the creation of test environments, or running badly configured ones, can counter the benefits of testing. Whichever stage (and associated environments) your team runs tests at must represent the production environment closely. This cancels out any environment parameter-based bugs later on.\n\n#### Common Testing Environments\n\nTesting saves your team a lot of time (and face). As such, it's worth investing in the most suitable environments for your applications. Not all testing environments are the same. In fact, testing itself takes place at different stages along an application's metamorphosis. Three such milestones, each sparking an environment, commonly appear regardless of your development framework.\n\n- **Development:** Tests run alongside new feature additions. Since this is where the main copy (branch) of an application resides, setting up is near effortless and cost the least to manage.\n- **Testing:** This environment is created after development as a separate environment for engineers to put applications under various tests. This can be a minimum resource allocated container. It's a discovery phase that might itself be replicated to try out different testing frameworks/applications.\n- **Staging:** Staging is perfect for post-development demonstrations of the application to a few users, but not the entire user base. Usually an opportunity for the app' investors/owners to check if the product addresses their business case.\n\nSome QAs will include the production environment as another test opportunity. Yes, continuous tests have become the norm, especially with teams practicing CI/CD principles. Being continuous implies an application is always being worked on. Consequently, changes made due to test results after deployment just mean we're in the development environment, after all.\n\n#### Challenges in Setting Up Test Environments\n\nEven with the knowledge of which environment to create, how well your testing turns out depends on **how** you create and manage the environment. Without proper management of test environments, the following burdens can plague your team:\n\n- **Brittle smoke tests:** This leads to loss of confidence in a project's feasibility. In practice, you'll only find out that your environment caused test failures, and not your application. However, you'll have already wasted valuable time and possibly drowned team morale.\n- **High cost of data synchronization with production:** Running tests on live data (or subsets of it) requires proper planning and infrastructure setup. Unpleasant loss of data, or corruption of live instances, can ruin tests altogether.\n- **Inconsistent test data across teams:** All members of a development team should be up to speed with the most up-to-date test data. Inconsistencies that arise due to badly configured test environments can result in teams wasting efforts on otherwise resolved issues.\n\nAll these problem cases cause setup costs to accumulate. Unmanaged, a team might use the budget on unnecessary or duplicate resources.\n\n#### Traditional Costs of Provisioning and Changing a Test Environment\n\nTypically, test environments will have a cost budget close to that of a live environment. If you already have a live version of the application, you'll be wise to use it as a threshold. You can then provide containers and resources that let teams run tests without \"choking\" the application.\n\nCosts can pile as your team creates new and persisting test environments. This brings the issue of environment lifecycle into focus. If your service provider charges on a resource consumption model, you're best alerting teams of the need to tear environments down after tests. This significantly optimizes your testing routine. Having templates of the various test environments your team requires is another way of making sure they don't end up provisioning too many resources.\n\nWith a versioning platform hosting application code, moving your application from one environment to the next becomes cheaper than manually copying and pasting code. Here you'll save on storage and reduce the range of errors that befall blocks of code as they move from place to place.\n\n### How to Run a Tight Ship Around Test Environments\n\nYou'll have to approach every step of setting up and maintaining test environments with deliberate strategy. Also, it's important not to focus on cost-saving entirely as you'll likely negotiate on the performance and range of possible tests. Balancing these out requires listing out the tests you'll run and the tools for which each environment will require.\n\nDoing the above, along with manually checking for any unnecessary leaks from redundant environments, will surely take all of your engineers' time and effort. You might need to hire someone to focus on just that, but this would just pile costs on your testing process. A better route would use tools to manage test environments.\n\n[Release](https://release.com/) is a good example of tools created to manage and lessen test environment costs. This includes creating on-demand test environments with an Environment as a Service approach for instant deployment and execution of tests. The platform also enables sharable test cases, keeping teams at par with the latest test results across all created environments.\n\nPerhaps the least-discussed issue around testing is how scalable every environment you create is. As an application grows, so too should the environments in which tests are run. Test environment tools that provide a central configuration spot to scale the provision of test spaces become crucial to successful tests.\n\n![](/blog-images/3f26cfb9856a36c12ece5ceddc1fd660.png)\n\n### Last Take: Testing Done Right\n\nKnowing what the focus and goal are at each stage of the test environments' creation makes it much easier to manage them. This allows for proper planning and ample resource allocation for every test case you deem necessary to test your applications.\n\nWith a plan in hand, the task to set up test environments is best left to the mandate of the right tools. Regardless of the environments you need, creating them should be an easy process. On-demand [staging environments](https://release.com/staging-environments) for as many test cases as necessary will resolve most of the pain points we discussed above.\n\nTools also immediately make managing them less stressful compared to manually managing them. They also automate and standardize how you set up test environment variables and share any results thereof. Combine such a tool with test automation frameworks and your developers get to focus on the applications, not the environment.\n\n_This post was written by Taurai Mutimutema._ [_Taurai_](https://twitter.com/rusiqe) _is a systems analyst with a knack for writing, which was probably sparked by the need to document technical processes during code and implementation sessions. He enjoys learning new technology and talks about tech even more than he writes._\n",
          "code": "var Component=(()=>{var m=Object.create;var i=Object.defineProperty;var d=Object.getOwnPropertyDescriptor;var u=Object.getOwnPropertyNames;var p=Object.getPrototypeOf,g=Object.prototype.hasOwnProperty;var f=(t,e)=>()=>(e||t((e={exports:{}}).exports,e),e.exports),v=(t,e)=>{for(var s in e)i(t,s,{get:e[s],enumerable:!0})},r=(t,e,s,a)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let o of u(e))!g.call(t,o)&&o!==s&&i(t,o,{get:()=>e[o],enumerable:!(a=d(e,o))||a.enumerable});return t};var y=(t,e,s)=>(s=t!=null?m(p(t)):{},r(e||!t||!t.__esModule?i(s,\"default\",{value:t,enumerable:!0}):s,t)),w=t=>r(i({},\"__esModule\",{value:!0}),t);var c=f((C,l)=>{l.exports=_jsx_runtime});var j={};v(j,{default:()=>k,frontmatter:()=>b});var n=y(c()),b={title:\"How to Setup Test Environments That Are Easy to Maintain\",summary:\"How to set up test environment | Data, Scope, environment lifetime, and what is the focus and goal at each stage.\",publishDate:\"Tue Feb 15 2022 22:04:30 GMT+0000 (Coordinated Universal Time)\",author:\"taurai-mutimutema\",readingTime:6,categories:[\"platform-engineering\",\"product\"],mainImage:\"/blog-images/e860bfc6e52f225fe1a870172da90274.jpg\",imageAlt:\"How to Setup Test Environments That Are Easy to Maintain\",showCTA:!0,ctaCopy:\"Simplify test environment setup with Release's on-demand environments mirroring production for efficient testing and seamless collaboration.\",ctaLink:\"https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=setup-test-environment\",relatedPosts:[\"\"],ogImage:\"/blog-images/e860bfc6e52f225fe1a870172da90274.jpg\",excerpt:\"How to set up test environment | Data, Scope, environment lifetime, and what is the focus and goal at each stage.\",tags:[\"platform-engineering\",\"product\"],ctaButton:\"Try Release for Free\"};function h(t){let e=Object.assign({p:\"p\",img:\"img\",h3:\"h3\",a:\"a\",span:\"span\",h4:\"h4\",ul:\"ul\",li:\"li\",strong:\"strong\",em:\"em\"},t.components);return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(e.p,{children:\"Setting up test environments is an integral step in ensuring smooth-running and bug-free applications post deployment. But once they're live, test environments also require the same TLC you invest into production servers. With this comes that double-effort dread. Should developers be spending time managing environments instead of the applications?\"}),`\n`,(0,n.jsx)(e.p,{children:\"This post explains how your engineers can set up test environments that don't demoralize developers with huge effort requirements. We'll review the various test environment options at your disposal, along with a few common challenges you might encounter during the setup phase. We'll then close with actionable solutions to the pain points around test environments.\"}),`\n`,(0,n.jsx)(e.p,{children:\"Before we discuss strategies, let's investigate why any of this is necessary.\"}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/b01106eda919a6d670bc40cc5374a2b5.png\",alt:\"\"})}),`\n`,(0,n.jsxs)(e.h3,{id:\"why-you-need-preproduction-testing-environments\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#why-you-need-preproduction-testing-environments\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Why You Need Preproduction Testing Environments\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Let's cut to the chase. If you don't invest time and effort into test environments, you'd almost certainly spend resources correcting faulty performance issues in production. Not to take anything away from your development skills, but testing itself is one skill crucial to the success of any software project. The best way to execute tests is in test environments\\u2014replicas of the production environment, accessible only to engineers and testers.\"}),`\n`,(0,n.jsx)(e.p,{children:\"Skipping the creation of test environments, or running badly configured ones, can counter the benefits of testing. Whichever stage (and associated environments) your team runs tests at must represent the production environment closely. This cancels out any environment parameter-based bugs later on.\"}),`\n`,(0,n.jsxs)(e.h4,{id:\"common-testing-environments\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#common-testing-environments\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Common Testing Environments\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Testing saves your team a lot of time (and face). As such, it's worth investing in the most suitable environments for your applications. Not all testing environments are the same. In fact, testing itself takes place at different stages along an application's metamorphosis. Three such milestones, each sparking an environment, commonly appear regardless of your development framework.\"}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"Development:\"}),\" Tests run alongside new feature additions. Since this is where the main copy (branch) of an application resides, setting up is near effortless and cost the least to manage.\"]}),`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"Testing:\"}),\" This environment is created after development as a separate environment for engineers to put applications under various tests. This can be a minimum resource allocated container. It's a discovery phase that might itself be replicated to try out different testing frameworks/applications.\"]}),`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"Staging:\"}),\" Staging is perfect for post-development demonstrations of the application to a few users, but not the entire user base. Usually an opportunity for the app' investors/owners to check if the product addresses their business case.\"]}),`\n`]}),`\n`,(0,n.jsx)(e.p,{children:\"Some QAs will include the production environment as another test opportunity. Yes, continuous tests have become the norm, especially with teams practicing CI/CD principles. Being continuous implies an application is always being worked on. Consequently, changes made due to test results after deployment just mean we're in the development environment, after all.\"}),`\n`,(0,n.jsxs)(e.h4,{id:\"challenges-in-setting-up-test-environments\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#challenges-in-setting-up-test-environments\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Challenges in Setting Up Test Environments\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"Even with the knowledge of which environment to create, how well your testing turns out depends on \",(0,n.jsx)(e.strong,{children:\"how\"}),\" you create and manage the environment. Without proper management of test environments, the following burdens can plague your team:\"]}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"Brittle smoke tests:\"}),\" This leads to loss of confidence in a project's feasibility. In practice, you'll only find out that your environment caused test failures, and not your application. However, you'll have already wasted valuable time and possibly drowned team morale.\"]}),`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"High cost of data synchronization with production:\"}),\" Running tests on live data (or subsets of it) requires proper planning and infrastructure setup. Unpleasant loss of data, or corruption of live instances, can ruin tests altogether.\"]}),`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"Inconsistent test data across teams:\"}),\" All members of a development team should be up to speed with the most up-to-date test data. Inconsistencies that arise due to badly configured test environments can result in teams wasting efforts on otherwise resolved issues.\"]}),`\n`]}),`\n`,(0,n.jsx)(e.p,{children:\"All these problem cases cause setup costs to accumulate. Unmanaged, a team might use the budget on unnecessary or duplicate resources.\"}),`\n`,(0,n.jsxs)(e.h4,{id:\"traditional-costs-of-provisioning-and-changing-a-test-environment\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#traditional-costs-of-provisioning-and-changing-a-test-environment\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Traditional Costs of Provisioning and Changing a Test Environment\"]}),`\n`,(0,n.jsx)(e.p,{children:`Typically, test environments will have a cost budget close to that of a live environment. If you already have a live version of the application, you'll be wise to use it as a threshold. You can then provide containers and resources that let teams run tests without \"choking\" the application.`}),`\n`,(0,n.jsx)(e.p,{children:\"Costs can pile as your team creates new and persisting test environments. This brings the issue of environment lifecycle into focus. If your service provider charges on a resource consumption model, you're best alerting teams of the need to tear environments down after tests. This significantly optimizes your testing routine. Having templates of the various test environments your team requires is another way of making sure they don't end up provisioning too many resources.\"}),`\n`,(0,n.jsx)(e.p,{children:\"With a versioning platform hosting application code, moving your application from one environment to the next becomes cheaper than manually copying and pasting code. Here you'll save on storage and reduce the range of errors that befall blocks of code as they move from place to place.\"}),`\n`,(0,n.jsxs)(e.h3,{id:\"how-to-run-a-tight-ship-around-test-environments\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#how-to-run-a-tight-ship-around-test-environments\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"How to Run a Tight Ship Around Test Environments\"]}),`\n`,(0,n.jsx)(e.p,{children:\"You'll have to approach every step of setting up and maintaining test environments with deliberate strategy. Also, it's important not to focus on cost-saving entirely as you'll likely negotiate on the performance and range of possible tests. Balancing these out requires listing out the tests you'll run and the tools for which each environment will require.\"}),`\n`,(0,n.jsx)(e.p,{children:\"Doing the above, along with manually checking for any unnecessary leaks from redundant environments, will surely take all of your engineers' time and effort. You might need to hire someone to focus on just that, but this would just pile costs on your testing process. A better route would use tools to manage test environments.\"}),`\n`,(0,n.jsxs)(e.p,{children:[(0,n.jsx)(e.a,{href:\"https://release.com/\",children:\"Release\"}),\" is a good example of tools created to manage and lessen test environment costs. This includes creating on-demand test environments with an Environment as a Service approach for instant deployment and execution of tests. The platform also enables sharable test cases, keeping teams at par with the latest test results across all created environments.\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Perhaps the least-discussed issue around testing is how scalable every environment you create is. As an application grows, so too should the environments in which tests are run. Test environment tools that provide a central configuration spot to scale the provision of test spaces become crucial to successful tests.\"}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/3f26cfb9856a36c12ece5ceddc1fd660.png\",alt:\"\"})}),`\n`,(0,n.jsxs)(e.h3,{id:\"last-take-testing-done-right\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#last-take-testing-done-right\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Last Take: Testing Done Right\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Knowing what the focus and goal are at each stage of the test environments' creation makes it much easier to manage them. This allows for proper planning and ample resource allocation for every test case you deem necessary to test your applications.\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"With a plan in hand, the task to set up test environments is best left to the mandate of the right tools. Regardless of the environments you need, creating them should be an easy process. On-demand \",(0,n.jsx)(e.a,{href:\"https://release.com/staging-environments\",children:\"staging environments\"}),\" for as many test cases as necessary will resolve most of the pain points we discussed above.\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Tools also immediately make managing them less stressful compared to manually managing them. They also automate and standardize how you set up test environment variables and share any results thereof. Combine such a tool with test automation frameworks and your developers get to focus on the applications, not the environment.\"}),`\n`,(0,n.jsxs)(e.p,{children:[(0,n.jsx)(e.em,{children:\"This post was written by Taurai Mutimutema.\"}),\" \",(0,n.jsx)(e.a,{href:\"https://twitter.com/rusiqe\",children:(0,n.jsx)(e.em,{children:\"Taurai\"})}),\" \",(0,n.jsx)(e.em,{children:\"is a systems analyst with a knack for writing, which was probably sparked by the need to document technical processes during code and implementation sessions. He enjoys learning new technology and talks about tech even more than he writes.\"})]})]})}function T(t={}){let{wrapper:e}=t.components||{};return e?(0,n.jsx)(e,Object.assign({},t,{children:(0,n.jsx)(h,t)})):h(t)}var k=T;return w(j);})();\n;return Component;"
        },
        "_id": "blog/posts/setup-test-environment.mdx",
        "_raw": {
          "sourceFilePath": "blog/posts/setup-test-environment.mdx",
          "sourceFileName": "setup-test-environment.mdx",
          "sourceFileDir": "blog/posts",
          "contentType": "mdx",
          "flattenedPath": "blog/posts/setup-test-environment"
        },
        "type": "BlogPost",
        "computedSlug": "setup-test-environment"
      },
      "documentHash": "1739393595027",
      "hasWarnings": false,
      "documentTypeName": "BlogPost"
    },
    "blog/posts/solving-for-dynamic-oauth-2-0-callbacks-with-environment-handles.mdx": {
      "document": {
        "title": "Solving for Dynamic OAuth 2.0 Callbacks with Environment Handles",
        "summary": "In this blog post, we will discuss the concept of Environment Handles and how they can be used to support dynamic OAuth",
        "publishDate": "Wed Jul 20 2022 13:10:36 GMT+0000 (Coordinated Universal Time)",
        "author": "mat-werber",
        "readingTime": 8,
        "categories": [
          "platform-engineering",
          "product"
        ],
        "mainImage": "/blog-images/b2cbef385b87616e21dd815831068d86.jpg",
        "imageAlt": "a keyboard with red lights",
        "showCTA": true,
        "ctaCopy": "Empower developers to easily manage dynamic OAuth callbacks with Release's flexible environment handles.",
        "ctaLink": "https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=solving-for-dynamic-oauth-2-0-callbacks-with-environment-handles",
        "relatedPosts": [
          ""
        ],
        "ogImage": "/blog-images/b2cbef385b87616e21dd815831068d86.jpg",
        "excerpt": "In this blog post, we will discuss the concept of Environment Handles and how they can be used to support dynamic OAuth",
        "tags": [
          "platform-engineering",
          "product"
        ],
        "ctaButton": "Try Release for Free",
        "body": {
          "raw": "\n### Ephemeral Environments and Dynamic Hostnames\n\nWhen creating an ephemeral, on-demand application environment with your environment platform, you may have one or more services, such as a frontend web application or API server, that require internet ingress.\n\nWhile these endpoints can be hard-coded into your environment configuration, it's beneficial to avoid this when possible and instead use a random ID as part of the hostname so that your developers can freely spin up multiple environments from the same definition, if needed.\n\nFor example, cloud environments created with [Release](https://release.com/) will by default generate a random environment ID, **ENV_ID**, and let you use that value anywhere you want within the hostname of an ephemeral ingress endpoint, such as `https://frontend-ENV_ID.example.com`.\n\n#### OAuth 2.0 and the Authorization Grant Flow\n\n[OAuth 2.0](https://oauth.net/2/) (\"OAuth\") is a common standard that allows a user to authorize one application to access their resources hosted within another system. The OAuth standard provides several different methods, referred to as OAuth flows, for which delegated access can be provisioned. While an in-depth discussion of OAuth and the various flows is outside the scope of this blog, we will define the **authorization grant flow** and refer back to it later as follows:\n\n_The authorization grant flow_ is an OAuth 2.0 flow that allows a user of your **client application** to provide your application with delegated permission to access the user's resources hosted within a 3rd-party system's **resource server**. In practice, your client application will redirect the user to the 3rd party's **authorization server** to authenticate the user and, once authenticated, confirm with the user whether they want to grant your client application one or more permissions (**scopes**) to their resources. If the user approves the access, the authorization server will redirect the user to a predefined **callback URL** (aka redirect URL or sign-in redirect) while providing an **authorization grant** to the client application. The client application will then exchange the grant for an **access token** with the authorization server which ultimately allows the client application to access the user's resources on the resource server. The authorization and resource servers are often part of the same application and domain, but they don't have to be.\n\nAs a practical example, if you've ever granted permission for an application to interact with your GitHub account or repositories, as I'm doing with GitKraken in the example below, you were almost certainly using OAuth 2.0 under the hood:\n\n![](/blog-images/944b5c47b11052b893ff5b6c86608dcb.png)\n\nWhen using OAuth 2.0, a client application must be configured within the 3rd-party Authorization server in advance before the client can actually initiate an OAuth flow. The specific configuration varies based on the third-parties implementation of the protocol, but at a minimum, the authorization code flow requires that (1) the client application redirect URL(s) are defined in advance and (2) when first initiating the flow, the client application includes a pre-approved redirect URL as a query parameter.\n\n#### The Challenge\n\nWe now know that:\n\n1.  The DNS endpoint (and thus callback URL) of an ephemeral client application environment is typically not known prior to creating the environment\n2.  The OAuth 2.0 authorization grant flow that you explicitly define the callback URL(s) _prior_ to actually initiating an actual authorization flow\n\n‍**This poses an interesting challenge:** how can you use OAuth 2.0 flows that require pre-defined, static callback URLs when the DNS endpoint of your client application and callback URL is dynamic?\n\n#### Environment Handles\n\nInstead of substituting random environment IDs into the hostnames of your ephemeral environments, you can instead create a pool of predefined environment IDs, aka **Environment Handles**. These could be a simple set of numbers like 1 through 10, a subset of your favorite superhero names, or anything else.\n\nWith this approach, when your platform creates an environment it would randomly select one of the available handles, assign it to the newly-created environment, and mark it as in-use. The handle would remain unavailable until your environment is terminated and the handle is once again marked as available.\n\nFor example, say you know that you will have at most five concurrent environments running at any time and you therefore create a list of handles, one through five. Continuing with our previous example, this means that the set of possible application URLs would be:\n\n- https://frontend-**1**.example.com\n- https://frontend-**2**.example.com\n- https://frontend-**3**.example.com\n- https://frontend-**4**.example.com\n- https://frontend-**5**.example.com\n\nEven though you don't know which environment will get which handle and when, your hostnames are now deterministic because they must be one of the five values above. This ultimately means that you now have a reliable list of callback URLs that you can configure with your 3rd-party OAuth provider and successfully use your ephemeral environments with OAuth 2.0.\n\nAs an example, we use Release to deploy demo application environments that rely on OAuth to sign in and access third-party resources in source control providers like GitHub, and we use a pool of our favorite superhero names as environment handles as shown below:\n\n![](/blog-images/6f0fb6ee691c8dde88aed51d452038ff.png)\n\nAbove, we can see that one of our 23 available handles, _shazam_, is assigned to an active environment to test some frontend notification modals. When we navigate to the frontend URL assigned to that environment, we can see that the _shazam_ handle is used to form the ephemeral frontend hostname:\n\n![](/blog-images/57e9956933fbcf938aac28e4118201c7.png)\n\nThe _https://frontend-shazam-xxxxxx_ URL above, along with the other superhero variations from our environment handle pool, has been pre-configured as a callback URL within Release's OAuth client settings in GitHub, BitBucket, and GitLab so that we can test [OpenID Connect](https://openid.net/connect/) (OIDC), an extension of the OAuth 2.0 protocol, to let users log in with their third-party credentials.\n\n### Considerations when using Environment Handles\n\n#### Environment Handle Pool Size\n\nThe size of your environment handle pool, and thus the number of URLs you need to allow-list in your OAuth client app configuration, is determined based on the number of concurrent ephemeral environments you expect you'll need plus some safety margin. For example, if your team will likely have three environments running at any one time, you might configure five handles just to be safe.\n\n#### Individual vs. Shared OAuth 2.0 Client per Handle\n\nWhen setting up the third-party OAuth clients for your environment handles, you'll need to determine whether you use a single client configuration for all of your environment handles or whether you set up a separate client per handle.\n\nA single client configuration with multiple callback URLs (one for each handle/endpoint) means that you would have a single Client ID and Client Secret to keep track of and results in a simpler configuration. However, whether the OAuth provider supports multiple callback URLs and how many are supported per client configuration may vary, and you may find that you have to create multiple client applications with your OAuth provider.\n\nIf you do configure separate client applications, that means that each handle will have their own distinct set of metadata attributes, such as Client ID or Client Secret. In this case, your environments must be provided and use the metadata values that correspond with the environment handle they are given at runtime.\n\nContinuing with our previous example, Release Environment Handles also allow you to associate plaintext and secret key-value pairs with each environment handle that will automatically be injected as environment variables to your container services or custom jobs that run as part of your environment:\n\n![](/blog-images/276093a1a94ef2e36536fa84283c42b2.png)\n\n#### Alternative approaches to environment handles\n\n##### Wildcard subdomains in OAuth 2.0 callbacks\n\nSome 3rd-parties support the use of wildcard subdomains when allow-listing a client application's callback URL which technically could eliminate the need for a solution like environment handles. However, wildcard subdomains are a security risk (e.g. [subdomain hijacking](https://bolster.ai/blog/subdomain-hijacking-takeover)) and aren't supported by all OAuth 2.0 authorization servers For these reasons, we recommend avoiding them when possible.\n\n_Example - Okta provides a warning if you enable wildcards in your callback URLs:_\n\n![](/blog-images/2cc3eea40d7c71838bf0ff21028b9173.png)\n\n#### OAuth 2.0 Callback Proxy\n\nDepending on the size of your team, number of concurrent environments, or number of OAuth integrations, you may find that you need hundreds of callback URLs or a constantly-increasing number of URLs.\n\nIn these scenarios, environment handles may not be practical and you might instead need to build an OAuth 2.0 callback proxy. Implementation details may vary, but as an example, one approach we've helped customers implement involves modifying their client application's initial OAuth 2.0 authorization code request to:\n\n1.  Use a callback URL that points to a static OAuth2 proxy service with a static hostname (e.g https://_oauth2.example.com/callback) instead of\\_ the client application's true callback URL.\n2.  Encode the client application's true callback URL (e.g. https://_env-123.example.com/callback_) in one of the original request parameters, like **state**.\n\nAfter the user authenticates and approves any requested resource scopes, the authorization server would return the authorization grant to your OAuth2 proxy service's callback URL, which is the _only_ callback URL you need to allow-list in your OAuth client configuration for test purposes. Your OAuth2 proxy service would then decode the **state** parameter to determine your environment's true callback URL and forward the authorization grant to that destination.\n\nWhile outside the scope of this blog, stay tuned, as we plan to share a closer look at an example OAuth 2.0 proxy implementation in the near future.\n\n#### OAuth 2.0 Dynamic Client Registration Protocol\n\nThe Internet Engineering Task Force (IETF) standardized the [OAuth 2.0 Dynamic Client Registration Protocol](https://datatracker.ietf.org/doc/html/rfc7591) in RFC 7591. As the name suggests, it can support just-in-time registration of a client application with an OAuth 2.0 authorization server. However, because it is not yet widely adopted, we haven't yet explored its applicability to ephemeral environments and the authorization code flow. If you've worked with this protocol and have a perspective to share, please let us know.\n\n### Summary\n\nIn this blog, we discussed the challenges that can arise when ephemeral environments with dynamic hostnames need to leverage a protocol like OAuth 2.0 that requires predefining static callback URLs to those same applications, and how solutions like Environment Handles can work within these constraints.\n\nIf you're looking for a platform that makes it easy to create ephemeral environments in _your_ AWS or GCP account, [Release](https://release.com/) can help, and if you're looking to use deterministic hostnames with your Release environment for OAuth callbacks or anything else, we offer [out-of-the-box support for Environment Handles](https://docs.releasehub.com/reference-documentation/account-settings/environment-handles) within your Release environments.\n",
          "code": "var Component=(()=>{var d=Object.create;var o=Object.defineProperty;var u=Object.getOwnPropertyDescriptor;var p=Object.getOwnPropertyNames;var m=Object.getPrototypeOf,f=Object.prototype.hasOwnProperty;var g=(t,e)=>()=>(e||t((e={exports:{}}).exports,e),e.exports),v=(t,e)=>{for(var a in e)o(t,a,{get:e[a],enumerable:!0})},s=(t,e,a,r)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let i of p(e))!f.call(t,i)&&i!==a&&o(t,i,{get:()=>e[i],enumerable:!(r=u(e,i))||r.enumerable});return t};var y=(t,e,a)=>(a=t!=null?d(m(t)):{},s(e||!t||!t.__esModule?o(a,\"default\",{value:t,enumerable:!0}):a,t)),w=t=>s(o({},\"__esModule\",{value:!0}),t);var h=g((R,l)=>{l.exports=_jsx_runtime});var A={};v(A,{default:()=>x,frontmatter:()=>b});var n=y(h()),b={title:\"Solving for Dynamic OAuth 2.0 Callbacks with Environment Handles\",summary:\"In this blog post, we will discuss the concept of Environment Handles and how they can be used to support dynamic OAuth\",publishDate:\"Wed Jul 20 2022 13:10:36 GMT+0000 (Coordinated Universal Time)\",author:\"mat-werber\",readingTime:8,categories:[\"platform-engineering\",\"product\"],mainImage:\"/blog-images/b2cbef385b87616e21dd815831068d86.jpg\",imageAlt:\"a keyboard with red lights\",showCTA:!0,ctaCopy:\"Empower developers to easily manage dynamic OAuth callbacks with Release's flexible environment handles.\",ctaLink:\"https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=solving-for-dynamic-oauth-2-0-callbacks-with-environment-handles\",relatedPosts:[\"\"],ogImage:\"/blog-images/b2cbef385b87616e21dd815831068d86.jpg\",excerpt:\"In this blog post, we will discuss the concept of Environment Handles and how they can be used to support dynamic OAuth\",tags:[\"platform-engineering\",\"product\"],ctaButton:\"Try Release for Free\"};function c(t){let e=Object.assign({h3:\"h3\",a:\"a\",span:\"span\",p:\"p\",strong:\"strong\",code:\"code\",h4:\"h4\",em:\"em\",img:\"img\",ol:\"ol\",li:\"li\",ul:\"ul\",h5:\"h5\"},t.components);return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsxs)(e.h3,{id:\"ephemeral-environments-and-dynamic-hostnames\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#ephemeral-environments-and-dynamic-hostnames\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Ephemeral Environments and Dynamic Hostnames\"]}),`\n`,(0,n.jsx)(e.p,{children:\"When creating an ephemeral, on-demand application environment with your environment platform, you may have one or more services, such as a frontend web application or API server, that require internet ingress.\"}),`\n`,(0,n.jsx)(e.p,{children:\"While these endpoints can be hard-coded into your environment configuration, it's beneficial to avoid this when possible and instead use a random ID as part of the hostname so that your developers can freely spin up multiple environments from the same definition, if needed.\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"For example, cloud environments created with \",(0,n.jsx)(e.a,{href:\"https://release.com/\",children:\"Release\"}),\" will by default generate a random environment ID, \",(0,n.jsx)(e.strong,{children:\"ENV_ID\"}),\", and let you use that value anywhere you want within the hostname of an ephemeral ingress endpoint, such as \",(0,n.jsx)(e.code,{children:\"https://frontend-ENV_ID.example.com\"}),\".\"]}),`\n`,(0,n.jsxs)(e.h4,{id:\"oauth-20-and-the-authorization-grant-flow\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#oauth-20-and-the-authorization-grant-flow\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"OAuth 2.0 and the Authorization Grant Flow\"]}),`\n`,(0,n.jsxs)(e.p,{children:[(0,n.jsx)(e.a,{href:\"https://oauth.net/2/\",children:\"OAuth 2.0\"}),' (\"OAuth\") is a common standard that allows a user to authorize one application to access their resources hosted within another system. The OAuth standard provides several different methods, referred to as OAuth flows, for which delegated access can be provisioned. While an in-depth discussion of OAuth and the various flows is outside the scope of this blog, we will define the ',(0,n.jsx)(e.strong,{children:\"authorization grant flow\"}),\" and refer back to it later as follows:\"]}),`\n`,(0,n.jsxs)(e.p,{children:[(0,n.jsx)(e.em,{children:\"The authorization grant flow\"}),\" is an OAuth 2.0 flow that allows a user of your \",(0,n.jsx)(e.strong,{children:\"client application\"}),\" to provide your application with delegated permission to access the user's resources hosted within a 3rd-party system's \",(0,n.jsx)(e.strong,{children:\"resource server\"}),\". In practice, your client application will redirect the user to the 3rd party's \",(0,n.jsx)(e.strong,{children:\"authorization server\"}),\" to authenticate the user and, once authenticated, confirm with the user whether they want to grant your client application one or more permissions (\",(0,n.jsx)(e.strong,{children:\"scopes\"}),\") to their resources. If the user approves the access, the authorization server will redirect the user to a predefined \",(0,n.jsx)(e.strong,{children:\"callback URL\"}),\" (aka redirect URL or sign-in redirect) while providing an \",(0,n.jsx)(e.strong,{children:\"authorization grant\"}),\" to the client application. The client application will then exchange the grant for an \",(0,n.jsx)(e.strong,{children:\"access token\"}),\" with the authorization server which ultimately allows the client application to access the user's resources on the resource server. The authorization and resource servers are often part of the same application and domain, but they don't have to be.\"]}),`\n`,(0,n.jsx)(e.p,{children:\"As a practical example, if you've ever granted permission for an application to interact with your GitHub account or repositories, as I'm doing with GitKraken in the example below, you were almost certainly using OAuth 2.0 under the hood:\"}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/944b5c47b11052b893ff5b6c86608dcb.png\",alt:\"\"})}),`\n`,(0,n.jsx)(e.p,{children:\"When using OAuth 2.0, a client application must be configured within the 3rd-party Authorization server in advance before the client can actually initiate an OAuth flow. The specific configuration varies based on the third-parties implementation of the protocol, but at a minimum, the authorization code flow requires that (1) the client application redirect URL(s) are defined in advance and (2) when first initiating the flow, the client application includes a pre-approved redirect URL as a query parameter.\"}),`\n`,(0,n.jsxs)(e.h4,{id:\"the-challenge\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#the-challenge\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"The Challenge\"]}),`\n`,(0,n.jsx)(e.p,{children:\"We now know that:\"}),`\n`,(0,n.jsxs)(e.ol,{children:[`\n`,(0,n.jsx)(e.li,{children:\"The DNS endpoint (and thus callback URL) of an ephemeral client application environment is typically not known prior to creating the environment\"}),`\n`,(0,n.jsxs)(e.li,{children:[\"The OAuth 2.0 authorization grant flow that you explicitly define the callback URL(s) \",(0,n.jsx)(e.em,{children:\"prior\"}),\" to actually initiating an actual authorization flow\"]}),`\n`]}),`\n`,(0,n.jsxs)(e.p,{children:[\"\\u200D\",(0,n.jsx)(e.strong,{children:\"This poses an interesting challenge:\"}),\" how can you use OAuth 2.0 flows that require pre-defined, static callback URLs when the DNS endpoint of your client application and callback URL is dynamic?\"]}),`\n`,(0,n.jsxs)(e.h4,{id:\"environment-handles\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#environment-handles\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Environment Handles\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"Instead of substituting random environment IDs into the hostnames of your ephemeral environments, you can instead create a pool of predefined environment IDs, aka \",(0,n.jsx)(e.strong,{children:\"Environment Handles\"}),\". These could be a simple set of numbers like 1 through 10, a subset of your favorite superhero names, or anything else.\"]}),`\n`,(0,n.jsx)(e.p,{children:\"With this approach, when your platform creates an environment it would randomly select one of the available handles, assign it to the newly-created environment, and mark it as in-use. The handle would remain unavailable until your environment is terminated and the handle is once again marked as available.\"}),`\n`,(0,n.jsx)(e.p,{children:\"For example, say you know that you will have at most five concurrent environments running at any time and you therefore create a list of handles, one through five. Continuing with our previous example, this means that the set of possible application URLs would be:\"}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsx)(e.li,{children:(0,n.jsx)(e.a,{href:\"https://frontend-**1**.example.com\",children:\"https://frontend-**1**.example.com\"})}),`\n`,(0,n.jsx)(e.li,{children:(0,n.jsx)(e.a,{href:\"https://frontend-**2**.example.com\",children:\"https://frontend-**2**.example.com\"})}),`\n`,(0,n.jsx)(e.li,{children:(0,n.jsx)(e.a,{href:\"https://frontend-**3**.example.com\",children:\"https://frontend-**3**.example.com\"})}),`\n`,(0,n.jsx)(e.li,{children:(0,n.jsx)(e.a,{href:\"https://frontend-**4**.example.com\",children:\"https://frontend-**4**.example.com\"})}),`\n`,(0,n.jsx)(e.li,{children:(0,n.jsx)(e.a,{href:\"https://frontend-**5**.example.com\",children:\"https://frontend-**5**.example.com\"})}),`\n`]}),`\n`,(0,n.jsx)(e.p,{children:\"Even though you don't know which environment will get which handle and when, your hostnames are now deterministic because they must be one of the five values above. This ultimately means that you now have a reliable list of callback URLs that you can configure with your 3rd-party OAuth provider and successfully use your ephemeral environments with OAuth 2.0.\"}),`\n`,(0,n.jsx)(e.p,{children:\"As an example, we use Release to deploy demo application environments that rely on OAuth to sign in and access third-party resources in source control providers like GitHub, and we use a pool of our favorite superhero names as environment handles as shown below:\"}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/6f0fb6ee691c8dde88aed51d452038ff.png\",alt:\"\"})}),`\n`,(0,n.jsxs)(e.p,{children:[\"Above, we can see that one of our 23 available handles, \",(0,n.jsx)(e.em,{children:\"shazam\"}),\", is assigned to an active environment to test some frontend notification modals. When we navigate to the frontend URL assigned to that environment, we can see that the \",(0,n.jsx)(e.em,{children:\"shazam\"}),\" handle is used to form the ephemeral frontend hostname:\"]}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/57e9956933fbcf938aac28e4118201c7.png\",alt:\"\"})}),`\n`,(0,n.jsxs)(e.p,{children:[\"The \",(0,n.jsx)(e.em,{children:(0,n.jsx)(e.a,{href:\"https://frontend-shazam-xxxxxx\",children:\"https://frontend-shazam-xxxxxx\"})}),\" URL above, along with the other superhero variations from our environment handle pool, has been pre-configured as a callback URL within Release's OAuth client settings in GitHub, BitBucket, and GitLab so that we can test \",(0,n.jsx)(e.a,{href:\"https://openid.net/connect/\",children:\"OpenID Connect\"}),\" (OIDC), an extension of the OAuth 2.0 protocol, to let users log in with their third-party credentials.\"]}),`\n`,(0,n.jsxs)(e.h3,{id:\"considerations-when-using-environment-handles\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#considerations-when-using-environment-handles\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Considerations when using Environment Handles\"]}),`\n`,(0,n.jsxs)(e.h4,{id:\"environment-handle-pool-size\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#environment-handle-pool-size\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Environment Handle Pool Size\"]}),`\n`,(0,n.jsx)(e.p,{children:\"The size of your environment handle pool, and thus the number of URLs you need to allow-list in your OAuth client app configuration, is determined based on the number of concurrent ephemeral environments you expect you'll need plus some safety margin. For example, if your team will likely have three environments running at any one time, you might configure five handles just to be safe.\"}),`\n`,(0,n.jsxs)(e.h4,{id:\"individual-vs-shared-oauth-20-client-per-handle\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#individual-vs-shared-oauth-20-client-per-handle\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Individual vs. Shared OAuth 2.0 Client per Handle\"]}),`\n`,(0,n.jsx)(e.p,{children:\"When setting up the third-party OAuth clients for your environment handles, you'll need to determine whether you use a single client configuration for all of your environment handles or whether you set up a separate client per handle.\"}),`\n`,(0,n.jsx)(e.p,{children:\"A single client configuration with multiple callback URLs (one for each handle/endpoint) means that you would have a single Client ID and Client Secret to keep track of and results in a simpler configuration. However, whether the OAuth provider supports multiple callback URLs and how many are supported per client configuration may vary, and you may find that you have to create multiple client applications with your OAuth provider.\"}),`\n`,(0,n.jsx)(e.p,{children:\"If you do configure separate client applications, that means that each handle will have their own distinct set of metadata attributes, such as Client ID or Client Secret. In this case, your environments must be provided and use the metadata values that correspond with the environment handle they are given at runtime.\"}),`\n`,(0,n.jsx)(e.p,{children:\"Continuing with our previous example, Release Environment Handles also allow you to associate plaintext and secret key-value pairs with each environment handle that will automatically be injected as environment variables to your container services or custom jobs that run as part of your environment:\"}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/276093a1a94ef2e36536fa84283c42b2.png\",alt:\"\"})}),`\n`,(0,n.jsxs)(e.h4,{id:\"alternative-approaches-to-environment-handles\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#alternative-approaches-to-environment-handles\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Alternative approaches to environment handles\"]}),`\n`,(0,n.jsxs)(e.h5,{id:\"wildcard-subdomains-in-oauth-20-callbacks\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#wildcard-subdomains-in-oauth-20-callbacks\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Wildcard subdomains in OAuth 2.0 callbacks\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"Some 3rd-parties support the use of wildcard subdomains when allow-listing a client application's callback URL which technically could eliminate the need for a solution like environment handles. However, wildcard subdomains are a security risk (e.g. \",(0,n.jsx)(e.a,{href:\"https://bolster.ai/blog/subdomain-hijacking-takeover\",children:\"subdomain hijacking\"}),\") and aren't supported by all OAuth 2.0 authorization servers For these reasons, we recommend avoiding them when possible.\"]}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.em,{children:\"Example - Okta provides a warning if you enable wildcards in your callback URLs:\"})}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/2cc3eea40d7c71838bf0ff21028b9173.png\",alt:\"\"})}),`\n`,(0,n.jsxs)(e.h4,{id:\"oauth-20-callback-proxy\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#oauth-20-callback-proxy\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"OAuth 2.0 Callback Proxy\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Depending on the size of your team, number of concurrent environments, or number of OAuth integrations, you may find that you need hundreds of callback URLs or a constantly-increasing number of URLs.\"}),`\n`,(0,n.jsx)(e.p,{children:\"In these scenarios, environment handles may not be practical and you might instead need to build an OAuth 2.0 callback proxy. Implementation details may vary, but as an example, one approach we've helped customers implement involves modifying their client application's initial OAuth 2.0 authorization code request to:\"}),`\n`,(0,n.jsxs)(e.ol,{children:[`\n`,(0,n.jsxs)(e.li,{children:[\"Use a callback URL that points to a static OAuth2 proxy service with a static hostname (e.g \",(0,n.jsx)(e.a,{href:\"https://_oauth2.example.com/callback\",children:\"https://_oauth2.example.com/callback\"}),\")\",\" instead of_ the client application's true callback URL.\"]}),`\n`,(0,n.jsxs)(e.li,{children:[\"Encode the client application's true callback URL (e.g. https://\",(0,n.jsx)(e.em,{children:\"env-123.example.com/callback\"}),\") in one of the original request parameters, like \",(0,n.jsx)(e.strong,{children:\"state\"}),\".\"]}),`\n`]}),`\n`,(0,n.jsxs)(e.p,{children:[\"After the user authenticates and approves any requested resource scopes, the authorization server would return the authorization grant to your OAuth2 proxy service's callback URL, which is the \",(0,n.jsx)(e.em,{children:\"only\"}),\" callback URL you need to allow-list in your OAuth client configuration for test purposes. Your OAuth2 proxy service would then decode the \",(0,n.jsx)(e.strong,{children:\"state\"}),\" parameter to determine your environment's true callback URL and forward the authorization grant to that destination.\"]}),`\n`,(0,n.jsx)(e.p,{children:\"While outside the scope of this blog, stay tuned, as we plan to share a closer look at an example OAuth 2.0 proxy implementation in the near future.\"}),`\n`,(0,n.jsxs)(e.h4,{id:\"oauth-20-dynamic-client-registration-protocol\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#oauth-20-dynamic-client-registration-protocol\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"OAuth 2.0 Dynamic Client Registration Protocol\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"The Internet Engineering Task Force (IETF) standardized the \",(0,n.jsx)(e.a,{href:\"https://datatracker.ietf.org/doc/html/rfc7591\",children:\"OAuth 2.0 Dynamic Client Registration Protocol\"}),\" in RFC 7591. As the name suggests, it can support just-in-time registration of a client application with an OAuth 2.0 authorization server. However, because it is not yet widely adopted, we haven't yet explored its applicability to ephemeral environments and the authorization code flow. If you've worked with this protocol and have a perspective to share, please let us know.\"]}),`\n`,(0,n.jsxs)(e.h3,{id:\"summary\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#summary\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Summary\"]}),`\n`,(0,n.jsx)(e.p,{children:\"In this blog, we discussed the challenges that can arise when ephemeral environments with dynamic hostnames need to leverage a protocol like OAuth 2.0 that requires predefining static callback URLs to those same applications, and how solutions like Environment Handles can work within these constraints.\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"If you're looking for a platform that makes it easy to create ephemeral environments in \",(0,n.jsx)(e.em,{children:\"your\"}),\" AWS or GCP account, \",(0,n.jsx)(e.a,{href:\"https://release.com/\",children:\"Release\"}),\" can help, and if you're looking to use deterministic hostnames with your Release environment for OAuth callbacks or anything else, we offer \",(0,n.jsx)(e.a,{href:\"https://docs.releasehub.com/reference-documentation/account-settings/environment-handles\",children:\"out-of-the-box support for Environment Handles\"}),\" within your Release environments.\"]})]})}function k(t={}){let{wrapper:e}=t.components||{};return e?(0,n.jsx)(e,Object.assign({},t,{children:(0,n.jsx)(c,t)})):c(t)}var x=k;return w(A);})();\n;return Component;"
        },
        "_id": "blog/posts/solving-for-dynamic-oauth-2-0-callbacks-with-environment-handles.mdx",
        "_raw": {
          "sourceFilePath": "blog/posts/solving-for-dynamic-oauth-2-0-callbacks-with-environment-handles.mdx",
          "sourceFileName": "solving-for-dynamic-oauth-2-0-callbacks-with-environment-handles.mdx",
          "sourceFileDir": "blog/posts",
          "contentType": "mdx",
          "flattenedPath": "blog/posts/solving-for-dynamic-oauth-2-0-callbacks-with-environment-handles"
        },
        "type": "BlogPost",
        "computedSlug": "solving-for-dynamic-oauth-2-0-callbacks-with-environment-handles"
      },
      "documentHash": "1739393595027",
      "hasWarnings": false,
      "documentTypeName": "BlogPost"
    },
    "blog/posts/summer-reading-list-docker-edition.mdx": {
      "document": {
        "title": "Summer Reading List: Docker edition   ",
        "summary": "What's on your Summer Reading List? Check out our Docker reading suggestions.",
        "publishDate": "Tue Sep 05 2023 21:24:55 GMT+0000 (Coordinated Universal Time)",
        "author": "ira-casteel",
        "readingTime": 3,
        "categories": [
          "platform-engineering",
          "kubernetes"
        ],
        "mainImage": "/blog-images/9a653a0c2fb53f019ad0dc73251f1f2b.jpg",
        "imageAlt": "image credit: cottonbro studio",
        "showCTA": true,
        "ctaCopy": "Looking to streamline Docker environment management like the pros? Try Release's on-demand environments for faster troubleshooting and efficient testing.",
        "ctaLink": "https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=summer-reading-list-docker-edition",
        "relatedPosts": [
          ""
        ],
        "ogImage": "/blog-images/9a653a0c2fb53f019ad0dc73251f1f2b.jpg",
        "excerpt": "What's on your Summer Reading List? Check out our Docker reading suggestions.",
        "tags": [
          "platform-engineering",
          "kubernetes"
        ],
        "ctaButton": "Try Release for Free",
        "body": {
          "raw": "\nHow are you spending the last days of summer? How about taking a few minutes to brush up on your Docker knowledge? As we prepare for the [DockerCon](https://www.dockercon.com/) conference later this fall, we wanted to get you up-to-speed on all things Docker.\n\nThis installment of the Summer Reading List highlights five especially popular Docker articles that help you understand and troubleshoot Docker challenges. Read on and let us know if you have other favorite Docker resources we could learn from.\n\n#### 🐳 [Creating your first Application in Release with Docker Compose](https://release.com/blog/creating-your-first-application-in-release-with-docker-compose)\n\nLearn how to migrate applications from Docker Compose to Kubernetes using Release. This step-by-step guide shows how to analyze a Compose file, generate an Application Template, set environment variables and build arguments, and deploy an application. Part two the series will cover deployment specifics and Kubernetes objects.\n\n#### 🐳 [How to set Docker Compose Environment Variables](https://release.com/blog/how-to-set-docker-compose-environment-variables)\n\nThis practical guide to managing environment variables in Docker Compose shows you how to define variables in the Compose file, pass variables from the host machine, or store them using a `.env` file. It also explains the priority of variables, offers best practices for handling sensitive data, and goes over solution for secure and efficient environment management.\n\n#### 🐳 [6 Docker Compose Best Practices for Dev and Prod](https://release.com/blog/6-docker-compose-best-practices-for-dev-and-prod)\n\nExplore Docker Compose best practices for both development and production environments. Learn how to use Docker Compose to manage multi-container Docker applications effectively, and get tips on mounting code as a volume, using override files, employing YAML anchors for sharing settings, and configuring CPU and memory limits.\n\n#### 🐳 [How to edit a file in a Docker container](https://release.com/blog/how-to-edit-a-file-in-a-docker-container)\n\nThis hands-on guide explains how to edit files in Docker containers, addressing the lack of preinstalled file editors like Vim or Nano. It offers two methods: editing from the command line and using Visual Studio Code (VS Code) with the Docker extension. The guide emphasizes best practices, including removing unnecessary packages after editing and persisting an editor in the Dockerfile for frequent changes. It also highlights the importance of saving new images after making changes for testing and consistency.\n\n#### 🐳 [Cutting Build Time In Half with Docker’s Buildx Kubernetes Driver](https://release.com/blog/cutting-build-time-in-half-docker-buildx-kubernetes)\n\nFollow along on our journey to improve our build infrastructure by implementing Docker's buildx project. When we faced challenges with slow build times and concurrency bottlenecks in our build process, we turned to buildx, to speed-up both our cached and uncached builds. This blog covers our infrastructure setup and how buildx's features like parallelism, Kubernetes driver, and load balancing helped us optimize our Docker image building process, ultimately benefiting our service delivery and customer experiences.\n\nThis concludes our Docker Summer Reading List. For more engaging reads, check out our [blog](https://release.com/blog). And to use Release in your next project, [sign up for a free account](https://release.com/signup).\n",
          "code": "var Component=(()=>{var h=Object.create;var t=Object.defineProperty;var m=Object.getOwnPropertyDescriptor;var u=Object.getOwnPropertyNames;var p=Object.getPrototypeOf,g=Object.prototype.hasOwnProperty;var f=(n,e)=>()=>(e||n((e={exports:{}}).exports,e),e.exports),b=(n,e)=>{for(var i in e)t(n,i,{get:e[i],enumerable:!0})},s=(n,e,i,a)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let r of u(e))!g.call(n,r)&&r!==i&&t(n,r,{get:()=>e[r],enumerable:!(a=m(e,r))||a.enumerable});return n};var k=(n,e,i)=>(i=n!=null?h(p(n)):{},s(e||!n||!n.__esModule?t(i,\"default\",{value:n,enumerable:!0}):i,n)),w=n=>s(t({},\"__esModule\",{value:!0}),n);var l=f((T,c)=>{c.exports=_jsx_runtime});var x={};b(x,{default:()=>D,frontmatter:()=>v});var o=k(l()),v={title:\"Summer Reading List: Docker edition   \",summary:\"What's on your Summer Reading List? Check out our Docker reading suggestions.\",publishDate:\"Tue Sep 05 2023 21:24:55 GMT+0000 (Coordinated Universal Time)\",author:\"ira-casteel\",readingTime:3,categories:[\"platform-engineering\",\"kubernetes\"],mainImage:\"/blog-images/9a653a0c2fb53f019ad0dc73251f1f2b.jpg\",imageAlt:\"image credit: cottonbro studio\",showCTA:!0,ctaCopy:\"Looking to streamline Docker environment management like the pros? Try Release's on-demand environments for faster troubleshooting and efficient testing.\",ctaLink:\"https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=summer-reading-list-docker-edition\",relatedPosts:[\"\"],ogImage:\"/blog-images/9a653a0c2fb53f019ad0dc73251f1f2b.jpg\",excerpt:\"What's on your Summer Reading List? Check out our Docker reading suggestions.\",tags:[\"platform-engineering\",\"kubernetes\"],ctaButton:\"Try Release for Free\"};function d(n){let e=Object.assign({p:\"p\",a:\"a\",h4:\"h4\",span:\"span\",code:\"code\"},n.components);return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsxs)(e.p,{children:[\"How are you spending the last days of summer? How about taking a few minutes to brush up on your Docker knowledge? As we prepare for the \",(0,o.jsx)(e.a,{href:\"https://www.dockercon.com/\",children:\"DockerCon\"}),\" conference later this fall, we wanted to get you up-to-speed on all things Docker.\"]}),`\n`,(0,o.jsx)(e.p,{children:\"This installment of the Summer Reading List highlights five especially popular Docker articles that help you understand and troubleshoot Docker challenges. Read on and let us know if you have other favorite Docker resources we could learn from.\"}),`\n`,(0,o.jsxs)(e.h4,{id:\"-creating-your-first-application-in-release-with-docker-compose\",children:[(0,o.jsx)(e.a,{className:\"anchor\",href:\"#-creating-your-first-application-in-release-with-docker-compose\",children:(0,o.jsx)(e.span,{className:\"icon icon-link\"})}),\"\\u{1F433} \",(0,o.jsx)(e.a,{href:\"https://release.com/blog/creating-your-first-application-in-release-with-docker-compose\",children:\"Creating your first Application in Release with Docker Compose\"})]}),`\n`,(0,o.jsx)(e.p,{children:\"Learn how to migrate applications from Docker Compose to Kubernetes using Release. This step-by-step guide shows how to analyze a Compose file, generate an Application Template, set environment variables and build arguments, and deploy an application. Part two the series will cover deployment specifics and Kubernetes objects.\"}),`\n`,(0,o.jsxs)(e.h4,{id:\"-how-to-set-docker-compose-environment-variables\",children:[(0,o.jsx)(e.a,{className:\"anchor\",href:\"#-how-to-set-docker-compose-environment-variables\",children:(0,o.jsx)(e.span,{className:\"icon icon-link\"})}),\"\\u{1F433} \",(0,o.jsx)(e.a,{href:\"https://release.com/blog/how-to-set-docker-compose-environment-variables\",children:\"How to set Docker Compose Environment Variables\"})]}),`\n`,(0,o.jsxs)(e.p,{children:[\"This practical guide to managing environment variables in Docker Compose shows you how to define variables in the Compose file, pass variables from the host machine, or store them using a \",(0,o.jsx)(e.code,{children:\".env\"}),\" file. It also explains the priority of variables, offers best practices for handling sensitive data, and goes over solution for secure and efficient environment management.\"]}),`\n`,(0,o.jsxs)(e.h4,{id:\"-6-docker-compose-best-practices-for-dev-and-prod\",children:[(0,o.jsx)(e.a,{className:\"anchor\",href:\"#-6-docker-compose-best-practices-for-dev-and-prod\",children:(0,o.jsx)(e.span,{className:\"icon icon-link\"})}),\"\\u{1F433} \",(0,o.jsx)(e.a,{href:\"https://release.com/blog/6-docker-compose-best-practices-for-dev-and-prod\",children:\"6 Docker Compose Best Practices for Dev and Prod\"})]}),`\n`,(0,o.jsx)(e.p,{children:\"Explore Docker Compose best practices for both development and production environments. Learn how to use Docker Compose to manage multi-container Docker applications effectively, and get tips on mounting code as a volume, using override files, employing YAML anchors for sharing settings, and configuring CPU and memory limits.\"}),`\n`,(0,o.jsxs)(e.h4,{id:\"-how-to-edit-a-file-in-a-docker-container\",children:[(0,o.jsx)(e.a,{className:\"anchor\",href:\"#-how-to-edit-a-file-in-a-docker-container\",children:(0,o.jsx)(e.span,{className:\"icon icon-link\"})}),\"\\u{1F433} \",(0,o.jsx)(e.a,{href:\"https://release.com/blog/how-to-edit-a-file-in-a-docker-container\",children:\"How to edit a file in a Docker container\"})]}),`\n`,(0,o.jsx)(e.p,{children:\"This hands-on guide explains how to edit files in Docker containers, addressing the lack of preinstalled file editors like Vim or Nano. It offers two methods: editing from the command line and using Visual Studio Code (VS Code) with the Docker extension. The guide emphasizes best practices, including removing unnecessary packages after editing and persisting an editor in the Dockerfile for frequent changes. It also highlights the importance of saving new images after making changes for testing and consistency.\"}),`\n`,(0,o.jsxs)(e.h4,{id:\"-cutting-build-time-in-half-with-dockers-buildx-kubernetes-driver\",children:[(0,o.jsx)(e.a,{className:\"anchor\",href:\"#-cutting-build-time-in-half-with-dockers-buildx-kubernetes-driver\",children:(0,o.jsx)(e.span,{className:\"icon icon-link\"})}),\"\\u{1F433} \",(0,o.jsx)(e.a,{href:\"https://release.com/blog/cutting-build-time-in-half-docker-buildx-kubernetes\",children:\"Cutting Build Time In Half with Docker\\u2019s Buildx Kubernetes Driver\"})]}),`\n`,(0,o.jsx)(e.p,{children:\"Follow along on our journey to improve our build infrastructure by implementing Docker's buildx project. When we faced challenges with slow build times and concurrency bottlenecks in our build process, we turned to buildx, to speed-up both our cached and uncached builds. This blog covers our infrastructure setup and how buildx's features like parallelism, Kubernetes driver, and load balancing helped us optimize our Docker image building process, ultimately benefiting our service delivery and customer experiences.\"}),`\n`,(0,o.jsxs)(e.p,{children:[\"This concludes our Docker Summer Reading List. For more engaging reads, check out our \",(0,o.jsx)(e.a,{href:\"https://release.com/blog\",children:\"blog\"}),\". And to use Release in your next project, \",(0,o.jsx)(e.a,{href:\"https://release.com/signup\",children:\"sign up for a free account\"}),\".\"]})]})}function y(n={}){let{wrapper:e}=n.components||{};return e?(0,o.jsx)(e,Object.assign({},n,{children:(0,o.jsx)(d,n)})):d(n)}var D=y;return w(x);})();\n;return Component;"
        },
        "_id": "blog/posts/summer-reading-list-docker-edition.mdx",
        "_raw": {
          "sourceFilePath": "blog/posts/summer-reading-list-docker-edition.mdx",
          "sourceFileName": "summer-reading-list-docker-edition.mdx",
          "sourceFileDir": "blog/posts",
          "contentType": "mdx",
          "flattenedPath": "blog/posts/summer-reading-list-docker-edition"
        },
        "type": "BlogPost",
        "computedSlug": "summer-reading-list-docker-edition"
      },
      "documentHash": "1739393595028",
      "hasWarnings": false,
      "documentTypeName": "BlogPost"
    },
    "blog/posts/summer-reading-list-kubernetes-edition.mdx": {
      "document": {
        "title": "Summer Reading List: Kubernetes edition   ",
        "summary": "What's on your Summer Reading List? Check out our Kubernetes reading suggestions.",
        "publishDate": "Wed Jul 05 2023 16:30:34 GMT+0000 (Coordinated Universal Time)",
        "author": "ira-casteel",
        "readingTime": 3,
        "categories": [
          "kubernetes",
          "platform-engineering"
        ],
        "mainImage": "/blog-images/42ae6c6e825f4b94195d648a8ee49664.jpg",
        "imageAlt": "Summer Reading List: Kubernetes edition   ",
        "showCTA": true,
        "ctaCopy": "Looking to streamline Kubernetes challenges? Try Release for efficient testing environments mirroring production setups.",
        "ctaLink": "https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=summer-reading-list-kubernetes-edition",
        "relatedPosts": [
          ""
        ],
        "ogImage": "/blog-images/42ae6c6e825f4b94195d648a8ee49664.jpg",
        "excerpt": "What's on your Summer Reading List? Check out our Kubernetes reading suggestions.",
        "tags": [
          "kubernetes",
          "platform-engineering"
        ],
        "ctaButton": "Try Release for Free",
        "body": {
          "raw": "\nWhat are your summer plans? Are you headed to the beach, or the mountains? Are you exploring your backyard or traveling abroad? Or do you have a list of projects you want to tackle during these warm months? Wherever summer takes you, this is a great time to brush up your tech skills or learn something new. That’s why we are putting together a series of summer reading lists to help you along your learning journey.  \n\nThis installment of the Summer Reading List highlights five especially popular Kubernetes articles that help you understand and troubleshoot Kubernetes challenges. Read on and let us know if you have a favorite Kubernetes resource we could learn from.\n\n#### 📖 [Why Is Kubernetes So Hard - 4 Reasons Why And What to do About it](https://release.com/blog/why-kubernetes-is-so-hard)\n\nIn this piece, we discuss challenges and complexities of using Kubernetes for application orchestration. We highlight the difficulties in setting up and managing Kubernetes infrastructure, deciphering YAML configurations, and troubleshooting issues. We also look at alternative solutions, including outsourcing Kubernetes management or using automation tools to simplify the deployment process.\n\n#### 📖 [How to Create and Configure Your Kubernetes Service Account](https://release.com/blog/how-to-create-and-configure-your-kubernetes-service-account)\n\nThis article explains the concept of Kubernetes service accounts and how to use them for non-human access to Kubernetes clusters. While human users authenticate to the Kubernetes API using user accounts, non-human users like CI/CD pipelines or monitoring tools require service accounts. The article covers creating service accounts, assigning them to pods, validating their usage, and adjusting permissions. It emphasizes the importance of properly configuring service accounts for security reasons.\n\n#### 📖 [10 Kubernetes Namespace Best Practices to Start Following](https://release.com//blog/10-kubernetes-namespace-best-practices-to-start-following)\n\nHere we discuss the importance of namespaces in Kubernetes. Namespaces are used to segregate and allocate resources in a Kubernetes cluster. Here we cover naming conventions, attaching labels to namespaces, using RBAC for resource allocation, implementing resource quotas and limit ranges, using NetworkPolicy for secure communication, and managing secrets. Following the best practices outlined in this article can improve efficiency, organization, and cost-effectiveness in Kubernetes workflows.\n\n#### 📖 [How to Make the Most of Kubernetes Environment Variables](https://release.com/blog/kubernetes-environment-variables)\n\nEnvironment variables in Kubernetes are dynamic key-value variables that provide information about the environment to software. They can be used to configure pods, pass sensitive information, or make applications adaptable. In this article you will learn about the three main methods for adding environment variables: direct specification, using secrets, and utilizing ConfigMaps, and how leveraging environment variables enables greater flexibility and reusability of Docker images in Kubernetes deployments.\n\n#### 📖 [Kubernetes - How to Debug CrashLoopBackOff in a Container](https://release.com/blog/kubernetes-how-to-debug-crashloopbackoff-in-a-container)\n\nThe article discusses how to debug the \"CrashLoopBackOff\" error in Kubernetes containers, and explains that the error can be caused by misconfigurations or code issues. The article provides steps to inspect and investigate the container image, override the container Entrypoint, and add debugging tools to diagnose and fix the problem. It also emphasizes the importance of documenting the debugging steps for future referenc\n\nThis concludes our Kubernetes Summer Reading List. For more engaging reads, check out our [blog](https://release.com/blog). And to use Release in your next project, [sign up for a free account](https://web.release.com/register).\n",
          "code": "var Component=(()=>{var h=Object.create;var a=Object.defineProperty;var d=Object.getOwnPropertyDescriptor;var g=Object.getOwnPropertyNames;var m=Object.getPrototypeOf,p=Object.prototype.hasOwnProperty;var b=(t,e)=>()=>(e||t((e={exports:{}}).exports,e),e.exports),f=(t,e)=>{for(var s in e)a(t,s,{get:e[s],enumerable:!0})},i=(t,e,s,r)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let o of g(e))!p.call(t,o)&&o!==s&&a(t,o,{get:()=>e[o],enumerable:!(r=d(e,o))||r.enumerable});return t};var y=(t,e,s)=>(s=t!=null?h(m(t)):{},i(e||!t||!t.__esModule?a(s,\"default\",{value:t,enumerable:!0}):s,t)),k=t=>i(a({},\"__esModule\",{value:!0}),t);var u=b((T,c)=>{c.exports=_jsx_runtime});var x={};f(x,{default:()=>K,frontmatter:()=>v});var n=y(u()),v={title:\"Summer Reading List: Kubernetes edition   \",summary:\"What's on your Summer Reading List? Check out our Kubernetes reading suggestions.\",publishDate:\"Wed Jul 05 2023 16:30:34 GMT+0000 (Coordinated Universal Time)\",author:\"ira-casteel\",readingTime:3,categories:[\"kubernetes\",\"platform-engineering\"],mainImage:\"/blog-images/42ae6c6e825f4b94195d648a8ee49664.jpg\",imageAlt:\"Summer Reading List: Kubernetes edition   \",showCTA:!0,ctaCopy:\"Looking to streamline Kubernetes challenges? Try Release for efficient testing environments mirroring production setups.\",ctaLink:\"https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=summer-reading-list-kubernetes-edition\",relatedPosts:[\"\"],ogImage:\"/blog-images/42ae6c6e825f4b94195d648a8ee49664.jpg\",excerpt:\"What's on your Summer Reading List? Check out our Kubernetes reading suggestions.\",tags:[\"kubernetes\",\"platform-engineering\"],ctaButton:\"Try Release for Free\"};function l(t){let e=Object.assign({p:\"p\",h4:\"h4\",a:\"a\",span:\"span\"},t.components);return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(e.p,{children:\"What are your summer plans? Are you headed to the beach, or the mountains? Are you exploring your backyard or traveling abroad? Or do you have a list of projects you want to tackle during these warm months? Wherever summer takes you, this is a great time to brush up your tech skills or learn something new. That\\u2019s why we are putting together a series of summer reading lists to help you along your learning journey. \\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:\"This installment of the Summer Reading List highlights five especially popular Kubernetes articles that help you understand and troubleshoot Kubernetes challenges. Read on and let us know if you have a favorite Kubernetes resource we could learn from.\"}),`\n`,(0,n.jsxs)(e.h4,{id:\"-why-is-kubernetes-so-hard---4-reasons-why-and-what-to-do-about-it\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#-why-is-kubernetes-so-hard---4-reasons-why-and-what-to-do-about-it\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"\\u{1F4D6} \",(0,n.jsx)(e.a,{href:\"https://release.com/blog/why-kubernetes-is-so-hard\",children:\"Why Is Kubernetes So Hard - 4 Reasons Why And What to do About it\"})]}),`\n`,(0,n.jsx)(e.p,{children:\"In this piece, we discuss challenges and complexities of using Kubernetes for application orchestration. We highlight the difficulties in setting up and managing Kubernetes infrastructure, deciphering YAML configurations, and troubleshooting issues. We also look at alternative solutions, including outsourcing Kubernetes management or using automation tools to simplify the deployment process.\"}),`\n`,(0,n.jsxs)(e.h4,{id:\"-how-to-create-and-configure-your-kubernetes-service-account\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#-how-to-create-and-configure-your-kubernetes-service-account\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"\\u{1F4D6} \",(0,n.jsx)(e.a,{href:\"https://release.com/blog/how-to-create-and-configure-your-kubernetes-service-account\",children:\"How to Create and Configure Your Kubernetes Service Account\"})]}),`\n`,(0,n.jsx)(e.p,{children:\"This article explains the concept of Kubernetes service accounts and how to use them for non-human access to Kubernetes clusters. While human users authenticate to the Kubernetes API using user accounts, non-human users like CI/CD pipelines or monitoring tools require service accounts. The article covers creating service accounts, assigning them to pods, validating their usage, and adjusting permissions. It emphasizes the importance of properly configuring service accounts for security reasons.\"}),`\n`,(0,n.jsxs)(e.h4,{id:\"-10-kubernetes-namespace-best-practices-to-start-following\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#-10-kubernetes-namespace-best-practices-to-start-following\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"\\u{1F4D6} \",(0,n.jsx)(e.a,{href:\"https://release.com//blog/10-kubernetes-namespace-best-practices-to-start-following\",children:\"10 Kubernetes Namespace Best Practices to Start Following\"})]}),`\n`,(0,n.jsx)(e.p,{children:\"Here we discuss the importance of namespaces in Kubernetes. Namespaces are used to segregate and allocate resources in a Kubernetes cluster. Here we cover naming conventions, attaching labels to namespaces, using RBAC for resource allocation, implementing resource quotas and limit ranges, using NetworkPolicy for secure communication, and managing secrets. Following the best practices outlined in this article can improve efficiency, organization, and cost-effectiveness in Kubernetes workflows.\"}),`\n`,(0,n.jsxs)(e.h4,{id:\"-how-to-make-the-most-of-kubernetes-environment-variables\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#-how-to-make-the-most-of-kubernetes-environment-variables\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"\\u{1F4D6} \",(0,n.jsx)(e.a,{href:\"https://release.com/blog/kubernetes-environment-variables\",children:\"How to Make the Most of Kubernetes Environment Variables\"})]}),`\n`,(0,n.jsx)(e.p,{children:\"Environment variables in Kubernetes are dynamic key-value variables that provide information about the environment to software. They can be used to configure pods, pass sensitive information, or make applications adaptable. In this article you will learn about the three main methods for adding environment variables: direct specification, using secrets, and utilizing ConfigMaps, and how leveraging environment variables enables greater flexibility and reusability of Docker images in Kubernetes deployments.\"}),`\n`,(0,n.jsxs)(e.h4,{id:\"-kubernetes---how-to-debug-crashloopbackoff-in-a-container\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#-kubernetes---how-to-debug-crashloopbackoff-in-a-container\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"\\u{1F4D6} \",(0,n.jsx)(e.a,{href:\"https://release.com/blog/kubernetes-how-to-debug-crashloopbackoff-in-a-container\",children:\"Kubernetes - How to Debug CrashLoopBackOff in a Container\"})]}),`\n`,(0,n.jsx)(e.p,{children:'The article discusses how to debug the \"CrashLoopBackOff\" error in Kubernetes containers, and explains that the error can be caused by misconfigurations or code issues. The article provides steps to inspect and investigate the container image, override the container Entrypoint, and add debugging tools to diagnose and fix the problem. It also emphasizes the importance of documenting the debugging steps for future referenc'}),`\n`,(0,n.jsxs)(e.p,{children:[\"This concludes our Kubernetes Summer Reading List. For more engaging reads, check out our \",(0,n.jsx)(e.a,{href:\"https://release.com/blog\",children:\"blog\"}),\". And to use Release in your next project, \",(0,n.jsx)(e.a,{href:\"https://web.release.com/register\",children:\"sign up for a free account\"}),\".\"]})]})}function w(t={}){let{wrapper:e}=t.components||{};return e?(0,n.jsx)(e,Object.assign({},t,{children:(0,n.jsx)(l,t)})):l(t)}var K=w;return k(x);})();\n;return Component;"
        },
        "_id": "blog/posts/summer-reading-list-kubernetes-edition.mdx",
        "_raw": {
          "sourceFilePath": "blog/posts/summer-reading-list-kubernetes-edition.mdx",
          "sourceFileName": "summer-reading-list-kubernetes-edition.mdx",
          "sourceFileDir": "blog/posts",
          "contentType": "mdx",
          "flattenedPath": "blog/posts/summer-reading-list-kubernetes-edition"
        },
        "type": "BlogPost",
        "computedSlug": "summer-reading-list-kubernetes-edition"
      },
      "documentHash": "1739393595028",
      "hasWarnings": false,
      "documentTypeName": "BlogPost"
    },
    "blog/posts/syncing-databases-how-to-do-it-and-best-practices.mdx": {
      "document": {
        "title": "Syncing Databases: How to Do It and Best Practices",
        "summary": "Read about database synchronization, what it's used for, how to sync databases, & explore tools to simplify the process",
        "publishDate": "Tue Jun 13 2023 22:18:07 GMT+0000 (Coordinated Universal Time)",
        "author": "regis-wilson",
        "readingTime": 10,
        "categories": [
          "platform-engineering",
          "product"
        ],
        "mainImage": "/blog-images/16c41e5c650eba2f169b980210a41786.jpg",
        "imageAlt": "Syncing Databases: How to Do It and Best Practices",
        "showCTA": true,
        "ctaCopy": "Simplify database synchronization with Release's on-demand environments for seamless collaboration and consistent deployments.",
        "ctaLink": "https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=syncing-databases-how-to-do-it-and-best-practices",
        "relatedPosts": [
          ""
        ],
        "ogImage": "/blog-images/16c41e5c650eba2f169b980210a41786.jpg",
        "excerpt": "Read about database synchronization, what it's used for, how to sync databases, & explore tools to simplify the process",
        "tags": [
          "platform-engineering",
          "product"
        ],
        "ctaButton": "Try Release for Free",
        "body": {
          "raw": "\nAs an engineering leader or developer, you may have encountered the need to sync multiple copies of a database to ensure data consistency across different systems or locations. Whether you're working on a distributed system, a mobile application, or a cloud-based platform, syncing the databases is a crucial task that requires careful planning and execution.\n\nIn this article, we'll look at database synchronization and what to use it for. Next, we'll explore different types of synchronization processes and how to sync databases step by step. We'll also cover some helpful tooling that makes syncing your databases easier and faster. Let's get started!\n\n### **What Is Database Synchronization?**\n\nDatabase synchronization is the process of keeping multiple copies of a database in sync with one another.\n\nThere are different ways of synchronizing databases, depending on the type of database, the network infrastructure, and the application's requirements. Some of the standard **methods** of database synchronization include the following:\n\n#### **Two-way synchronization**\n\nIn two-way synchronization, both databases can make changes and synchronize with each other.\n\n#### **One-way synchronization**\n\nIn one-way synchronization, one database acts as the source, and the other databases are updated to match it.\n\n#### **Incremental synchronization**\n\nWith incremental synchronization, changes are only made since the last synchronization.\n\n#### **File-based synchronization**\n\nWith file-based synchronization, data is exported to a file and imported into the other databases.\n\n‍\n\n![](/blog-images/1f8e981315815da3542b3b262480dd9e.png)\n\n### **Use Cases for Synchronization**\n\nDatabase synchronization is used in various situations where multiple copies of a database are in use and when it's necessary to ensure that the data in each copy is consistent. Some common use cases include the following:\n\n#### **Backup and recovery**\n\nYou can use database synchronization to keep a secondary copy of a database in sync with the primary copy, providing a way to recover from data loss or corruption.\n\n#### **Mobile and offline applications**\n\nApplications that work offline or on mobile devices may need to synchronize data with a central database when a connection becomes available. Database synchronization ensures that the data on the mobile device or offline application is consistent with the data on the central server.\n\n#### **Collaborative platforms**\n\nMultiple users may work on the same data in a collaborative platform. Here, database synchronization ensures that changes made by one user propagate to all other users, maintaining data consistency.\n\n#### **Distributed Systems**\n\nIn a distributed system, multiple copies of a database may run on different servers or in different locations. Database synchronization ensures that changes made to one copy of the database propagate to all other copies, maintaining data consistency across the system.\n\n#### **Cloud-based systems**\n\nCloud-based systems often have multiple copies of a database running in different regions to provide high availability and reduce latency. Database synchronization ensures that data is consistent across all copies of the database.\n\n### **Types of Database Synchronization**\n\nThere are several different types of database synchronization, each with its own advantages and disadvantages. Some common types include the following:\n\n#### **Source/Replica replication**\n\nIn this type of replication, one database acts as the source, and the other databases are updated to match it. The source database receives all updates and changes, which then propagate to the replica databases. It's commonly used for read-heavy workloads.\n\n#### **Multi-master replication**\n\nIn this type of replication, all databases can act as both sources and replicas. Changes made to one database are propagated to all other databases, ensuring that all copies of the data are consistent. This type of replication is helpful for write-heavy workloads.\n\n#### **File-based synchronization**\n\nIn this type of synchronization, data is exported to a file and then imported into the other databases. It's a simple method that's easy to implement, but it can be slow and may not be suitable for large amounts of data.\n\n#### **Log-based replication**\n\nIn this type of replication, changes made to a database are recorded in a log and then propagated to the other databases. This allows for fast and efficient replication but can be more complex to set up and maintain.\n\n#### **Trigger-based replication**\n\nIn this type of replication, triggers are set up on the source database to capture changes, which can then be propagated to the target database. It allows for fine-grained control over which changes are propagated, but it can be resource-intensive and may be unsuitable for high-traffic systems.\n\n#### **Cloud-based database synchronization**\n\nWe can also use cloud services like [AWS Database Migration Service (DMS)](https://aws.amazon.com/dms/) and [Azure Database Migration Service](https://azure.microsoft.com/en-us/products/database-migration) to sync databases. This is a good option if you have a cloud-based infrastructure and want to leverage the scalability and reliability offered by these services.\n\nThe best approach for your use case will depend on the type of data, the number of databases, the network infrastructure, and the requirements of the application.\n\n### ‍**How to Sync Databases**\n\nThe process of syncing databases can vary, depending on the type of databases, the method of synchronization, and the specific requirements of the application. Below is the step-by-step guide on how to sync databases:\n\n#### **Step 1: Understand your use case**\n\nUnderstand the specific requirements of your use case and choose the synchronization method that best fits those needs.\n\n#### **Step 2: Identify the databases to be synced**\n\nDetermine which databases need to be synced and the type of data they contain.\n\n#### **Step 3: Choose a synchronization method**\n\nDecide on the method of synchronization that's most appropriate for your use case. This may be replication, a data syncing tool, a custom script, or a cloud-based service.\n\n#### **Step 4: Configure the databases**\n\nSet up the databases for synchronization. This may include configuring replication settings, installing data syncing tools, or writing custom scripts.\n\n#### **Step 5: Test the synchronization**\n\nTest the synchronization by making changes to one database and verifying that the changes are propagated to the other databases. This will help you identify any issues or bugs before deploying it in production.\n\n#### **Step 6: Schedule synchronization**\n\nSet a schedule for the synchronization to occur regularly. You can do this using a built-in scheduling feature or by writing a custom script.\n\n#### **Step 7: Monitor and troubleshoot**\n\nMonitor the synchronization process and troubleshoot any issues that arise. This may include monitoring replication lag, checking for errors, and addressing any conflicts that occur. It may also include monitoring replication lag, checking for errors, and addressing any conflicts that occur.\n\n#### **Step 8: Maintain and update**\n\nRegularly maintain and update the synchronization process to ensure that it continues to function correctly.\n\n‍**Note:** Some steps may vary, depending on the type of database and the method of synchronization. For example, for a cloud-based service like AWS DMS, the process can be simpler. You have to create the replication task and configure the source and target databases.\n\n### **Tooling for Database Synchronization**\n\nThere are various tools available for database synchronization, depending on the type of database and the method of synchronization you're using. You can achieve database synchronization through various methods, such as the following:\n\n#### **Custom scripts**\n\nYou can write custom scripts using programming languages such as Python or Java to sync databases. This involves writing code to compare data in two databases and making changes as needed.\n\n#### **Replication**\n\nThis involves copying data from one database to another so that changes made to one are reflected in the other.\n\n- [MySQL](https://dev.mysql.com/doc/refman/8.0/en/replication.html) provides built-in replication capabilities, allowing users to replicate data between two or more MySQL servers.\n- [Microsoft SQL Server](https://learn.microsoft.com/en-us/sql/relational-databases/replication/sql-server-replication?view=sql-server-ver16) also provides built-in replication capabilities, including transactional replication and merge replication.\n- [PostgreSQL](https://www.postgresql.org/docs/current/runtime-config-replication.html) offers several replication solutions, including streaming replication and [logical replication](https://www.postgresql.org/docs/15/logical-replication.html).\n- [MongoDB](https://www.mongodb.com/docs/manual/replication/) provides built-in replication features, including replica sets and sharded clusters for horizontal scalability.\n\n#### **Cloud-based services**\n\nBelow are some of the common cloud-based services that you can use to sync databases.\n\n- [AWS Database Migration Service (DMS)](https://aws.amazon.com/dms/) can migrate, replicate, and sync databases between different platforms and environments, including on-premises environments and cloud-based environments like Amazon Web Services (AWS).\n- [Azure Database Migration Service](https://azure.microsoft.com/en-us/products/database-migration) is a fully managed service designed to enable seamless migrations from multiple database sources to Azure.\n\n#### **Data syncing tools**\n\nVarious data syncing tools can automate the process of keeping databases in sync.\n\n- [SymmetricDS](https://www.symmetricds.org/) is open-source data synchronization software that supports multiple relational databases, including MySQL, PostgreSQL, Oracle, and more.\n- [Talend](https://www.talend.com/), [Informatica](https://www.informatica.com/), and [Boomi](https://boomi.com/) can automate the process of keeping databases in sync.\n- [Oracle GoldenGate](https://www.oracle.com/in/integration/goldengate/) is a real-time data integration and replication software for heterogeneous environments, including Oracle, SQL Server, DB2, and more.\n\nThese are some of the popular tooling used for database synchronization. However, the best tool depends on the type of database, the method of synchronization, and the specific requirements of the application.\n\n### **Best Practices**\n\nWhen working with database synchronization, there are several best practices that can help you ensure that your synchronization process is efficient and reliable.\n\n#### **Keep your databases in sync**\n\nRegularly check and compare data between the different copies of the database and make updates as needed.\n\n#### **Use a replication tool**\n\nUse a replication tool that best fits your use case and the type of data you're working with.\n\n#### **Backups**\n\nRegularly back up your databases to ensure that you can recover from data loss or corruption.\n\n#### **Use cloud-based services**\n\nYou can also use Cloud-based services like AWS DMS and Azure Database Migration Service to sync databases. It's a good option if you have a cloud-based infrastructure and want to leverage the scalability and reliability offered by these services.\n\n‍\n\n![](/blog-images/11c2a64f57f07276266f10d2b4187dbd.png)\n\n#### **Security**\n\nEnsure that your synchronization process is secure by encrypting data in transit and at rest and by implementing access controls.\n\nBy following these best practices, you can ensure that your databases are kept in sync and that your synchronization process is efficient and reliable. Additionally, it's always important to be aware of the particularities of the database you're working with, the replication tool you're using, and the type of data you're syncing in order to apply the best practices in a way that fits your specific needs.\n\n### **Conclusion**\n\nSyncing databases is a crucial task that requires careful planning and execution. By understanding your use case, choosing the right synchronization method, and following best practices such as testing, monitoring, and maintaining, you can ensure that your databases sync effectively and efficiently. Additionally, by using cloud-based services, you can reduce the complexity of the process and ensure that your databases are always in sync.\n\nIf you're looking for a solution to manage your [remote development environments](https://release.com/blog/remote-development-environments), you can use [Release](https://release.com/book-a-demo) to create, manage and share development environments, collaborate with your team, and automate your release pipeline, all in one place. [Book a demo](https://release.com/book-a-demo) today and see the difference it can make in your remote development process\n\n‍\n",
          "code": "var Component=(()=>{var d=Object.create;var t=Object.defineProperty;var p=Object.getOwnPropertyDescriptor;var m=Object.getOwnPropertyNames;var u=Object.getPrototypeOf,y=Object.prototype.hasOwnProperty;var f=(n,e)=>()=>(e||n((e={exports:{}}).exports,e),e.exports),b=(n,e)=>{for(var i in e)t(n,i,{get:e[i],enumerable:!0})},r=(n,e,i,o)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let s of m(e))!y.call(n,s)&&s!==i&&t(n,s,{get:()=>e[s],enumerable:!(o=p(e,s))||o.enumerable});return n};var g=(n,e,i)=>(i=n!=null?d(u(n)):{},r(e||!n||!n.__esModule?t(i,\"default\",{value:n,enumerable:!0}):i,n)),w=n=>r(t({},\"__esModule\",{value:!0}),n);var l=f((D,c)=>{c.exports=_jsx_runtime});var N={};b(N,{default:()=>k,frontmatter:()=>v});var a=g(l()),v={title:\"Syncing Databases: How to Do It and Best Practices\",summary:\"Read about database synchronization, what it's used for, how to sync databases, & explore tools to simplify the process\",publishDate:\"Tue Jun 13 2023 22:18:07 GMT+0000 (Coordinated Universal Time)\",author:\"regis-wilson\",readingTime:10,categories:[\"platform-engineering\",\"product\"],mainImage:\"/blog-images/16c41e5c650eba2f169b980210a41786.jpg\",imageAlt:\"Syncing Databases: How to Do It and Best Practices\",showCTA:!0,ctaCopy:\"Simplify database synchronization with Release's on-demand environments for seamless collaboration and consistent deployments.\",ctaLink:\"https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=syncing-databases-how-to-do-it-and-best-practices\",relatedPosts:[\"\"],ogImage:\"/blog-images/16c41e5c650eba2f169b980210a41786.jpg\",excerpt:\"Read about database synchronization, what it's used for, how to sync databases, & explore tools to simplify the process\",tags:[\"platform-engineering\",\"product\"],ctaButton:\"Try Release for Free\"};function h(n){let e=Object.assign({p:\"p\",h3:\"h3\",a:\"a\",span:\"span\",strong:\"strong\",h4:\"h4\",img:\"img\",ul:\"ul\",li:\"li\"},n.components);return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(e.p,{children:\"As an engineering leader or developer, you may have encountered the need to sync multiple copies of a database to ensure data consistency across different systems or locations. Whether you're working on a distributed system, a mobile application, or a cloud-based platform, syncing the databases is a crucial task that requires careful planning and execution.\"}),`\n`,(0,a.jsx)(e.p,{children:\"In this article, we'll look at database synchronization and what to use it for. Next, we'll explore different types of synchronization processes and how to sync databases step by step. We'll also cover some helpful tooling that makes syncing your databases easier and faster. Let's get started!\"}),`\n`,(0,a.jsxs)(e.h3,{id:\"what-is-database-synchronization\",children:[(0,a.jsx)(e.a,{className:\"anchor\",href:\"#what-is-database-synchronization\",children:(0,a.jsx)(e.span,{className:\"icon icon-link\"})}),(0,a.jsx)(e.strong,{children:\"What Is Database Synchronization?\"})]}),`\n`,(0,a.jsx)(e.p,{children:\"Database synchronization is the process of keeping multiple copies of a database in sync with one another.\"}),`\n`,(0,a.jsxs)(e.p,{children:[\"There are different ways of synchronizing databases, depending on the type of database, the network infrastructure, and the application's requirements. Some of the standard \",(0,a.jsx)(e.strong,{children:\"methods\"}),\" of database synchronization include the following:\"]}),`\n`,(0,a.jsxs)(e.h4,{id:\"two-way-synchronization\",children:[(0,a.jsx)(e.a,{className:\"anchor\",href:\"#two-way-synchronization\",children:(0,a.jsx)(e.span,{className:\"icon icon-link\"})}),(0,a.jsx)(e.strong,{children:\"Two-way synchronization\"})]}),`\n`,(0,a.jsx)(e.p,{children:\"In two-way synchronization, both databases can make changes and synchronize with each other.\"}),`\n`,(0,a.jsxs)(e.h4,{id:\"one-way-synchronization\",children:[(0,a.jsx)(e.a,{className:\"anchor\",href:\"#one-way-synchronization\",children:(0,a.jsx)(e.span,{className:\"icon icon-link\"})}),(0,a.jsx)(e.strong,{children:\"One-way synchronization\"})]}),`\n`,(0,a.jsx)(e.p,{children:\"In one-way synchronization, one database acts as the source, and the other databases are updated to match it.\"}),`\n`,(0,a.jsxs)(e.h4,{id:\"incremental-synchronization\",children:[(0,a.jsx)(e.a,{className:\"anchor\",href:\"#incremental-synchronization\",children:(0,a.jsx)(e.span,{className:\"icon icon-link\"})}),(0,a.jsx)(e.strong,{children:\"Incremental synchronization\"})]}),`\n`,(0,a.jsx)(e.p,{children:\"With incremental synchronization, changes are only made since the last synchronization.\"}),`\n`,(0,a.jsxs)(e.h4,{id:\"file-based-synchronization\",children:[(0,a.jsx)(e.a,{className:\"anchor\",href:\"#file-based-synchronization\",children:(0,a.jsx)(e.span,{className:\"icon icon-link\"})}),(0,a.jsx)(e.strong,{children:\"File-based synchronization\"})]}),`\n`,(0,a.jsx)(e.p,{children:\"With file-based synchronization, data is exported to a file and imported into the other databases.\"}),`\n`,(0,a.jsx)(e.p,{children:\"\\u200D\"}),`\n`,(0,a.jsx)(e.p,{children:(0,a.jsx)(e.img,{src:\"/blog-images/1f8e981315815da3542b3b262480dd9e.png\",alt:\"\"})}),`\n`,(0,a.jsxs)(e.h3,{id:\"use-cases-for-synchronization\",children:[(0,a.jsx)(e.a,{className:\"anchor\",href:\"#use-cases-for-synchronization\",children:(0,a.jsx)(e.span,{className:\"icon icon-link\"})}),(0,a.jsx)(e.strong,{children:\"Use Cases for Synchronization\"})]}),`\n`,(0,a.jsx)(e.p,{children:\"Database synchronization is used in various situations where multiple copies of a database are in use and when it's necessary to ensure that the data in each copy is consistent. Some common use cases include the following:\"}),`\n`,(0,a.jsxs)(e.h4,{id:\"backup-and-recovery\",children:[(0,a.jsx)(e.a,{className:\"anchor\",href:\"#backup-and-recovery\",children:(0,a.jsx)(e.span,{className:\"icon icon-link\"})}),(0,a.jsx)(e.strong,{children:\"Backup and recovery\"})]}),`\n`,(0,a.jsx)(e.p,{children:\"You can use database synchronization to keep a secondary copy of a database in sync with the primary copy, providing a way to recover from data loss or corruption.\"}),`\n`,(0,a.jsxs)(e.h4,{id:\"mobile-and-offline-applications\",children:[(0,a.jsx)(e.a,{className:\"anchor\",href:\"#mobile-and-offline-applications\",children:(0,a.jsx)(e.span,{className:\"icon icon-link\"})}),(0,a.jsx)(e.strong,{children:\"Mobile and offline applications\"})]}),`\n`,(0,a.jsx)(e.p,{children:\"Applications that work offline or on mobile devices may need to synchronize data with a central database when a connection becomes available. Database synchronization ensures that the data on the mobile device or offline application is consistent with the data on the central server.\"}),`\n`,(0,a.jsxs)(e.h4,{id:\"collaborative-platforms\",children:[(0,a.jsx)(e.a,{className:\"anchor\",href:\"#collaborative-platforms\",children:(0,a.jsx)(e.span,{className:\"icon icon-link\"})}),(0,a.jsx)(e.strong,{children:\"Collaborative platforms\"})]}),`\n`,(0,a.jsx)(e.p,{children:\"Multiple users may work on the same data in a collaborative platform. Here, database synchronization ensures that changes made by one user propagate to all other users, maintaining data consistency.\"}),`\n`,(0,a.jsxs)(e.h4,{id:\"distributed-systems\",children:[(0,a.jsx)(e.a,{className:\"anchor\",href:\"#distributed-systems\",children:(0,a.jsx)(e.span,{className:\"icon icon-link\"})}),(0,a.jsx)(e.strong,{children:\"Distributed Systems\"})]}),`\n`,(0,a.jsx)(e.p,{children:\"In a distributed system, multiple copies of a database may run on different servers or in different locations. Database synchronization ensures that changes made to one copy of the database propagate to all other copies, maintaining data consistency across the system.\"}),`\n`,(0,a.jsxs)(e.h4,{id:\"cloud-based-systems\",children:[(0,a.jsx)(e.a,{className:\"anchor\",href:\"#cloud-based-systems\",children:(0,a.jsx)(e.span,{className:\"icon icon-link\"})}),(0,a.jsx)(e.strong,{children:\"Cloud-based systems\"})]}),`\n`,(0,a.jsx)(e.p,{children:\"Cloud-based systems often have multiple copies of a database running in different regions to provide high availability and reduce latency. Database synchronization ensures that data is consistent across all copies of the database.\"}),`\n`,(0,a.jsxs)(e.h3,{id:\"types-of-database-synchronization\",children:[(0,a.jsx)(e.a,{className:\"anchor\",href:\"#types-of-database-synchronization\",children:(0,a.jsx)(e.span,{className:\"icon icon-link\"})}),(0,a.jsx)(e.strong,{children:\"Types of Database Synchronization\"})]}),`\n`,(0,a.jsx)(e.p,{children:\"There are several different types of database synchronization, each with its own advantages and disadvantages. Some common types include the following:\"}),`\n`,(0,a.jsxs)(e.h4,{id:\"sourcereplica-replication\",children:[(0,a.jsx)(e.a,{className:\"anchor\",href:\"#sourcereplica-replication\",children:(0,a.jsx)(e.span,{className:\"icon icon-link\"})}),(0,a.jsx)(e.strong,{children:\"Source/Replica replication\"})]}),`\n`,(0,a.jsx)(e.p,{children:\"In this type of replication, one database acts as the source, and the other databases are updated to match it. The source database receives all updates and changes, which then propagate to the replica databases. It's commonly used for read-heavy workloads.\"}),`\n`,(0,a.jsxs)(e.h4,{id:\"multi-master-replication\",children:[(0,a.jsx)(e.a,{className:\"anchor\",href:\"#multi-master-replication\",children:(0,a.jsx)(e.span,{className:\"icon icon-link\"})}),(0,a.jsx)(e.strong,{children:\"Multi-master replication\"})]}),`\n`,(0,a.jsx)(e.p,{children:\"In this type of replication, all databases can act as both sources and replicas. Changes made to one database are propagated to all other databases, ensuring that all copies of the data are consistent. This type of replication is helpful for write-heavy workloads.\"}),`\n`,(0,a.jsxs)(e.h4,{id:\"file-based-synchronization-1\",children:[(0,a.jsx)(e.a,{className:\"anchor\",href:\"#file-based-synchronization-1\",children:(0,a.jsx)(e.span,{className:\"icon icon-link\"})}),(0,a.jsx)(e.strong,{children:\"File-based synchronization\"})]}),`\n`,(0,a.jsx)(e.p,{children:\"In this type of synchronization, data is exported to a file and then imported into the other databases. It's a simple method that's easy to implement, but it can be slow and may not be suitable for large amounts of data.\"}),`\n`,(0,a.jsxs)(e.h4,{id:\"log-based-replication\",children:[(0,a.jsx)(e.a,{className:\"anchor\",href:\"#log-based-replication\",children:(0,a.jsx)(e.span,{className:\"icon icon-link\"})}),(0,a.jsx)(e.strong,{children:\"Log-based replication\"})]}),`\n`,(0,a.jsx)(e.p,{children:\"In this type of replication, changes made to a database are recorded in a log and then propagated to the other databases. This allows for fast and efficient replication but can be more complex to set up and maintain.\"}),`\n`,(0,a.jsxs)(e.h4,{id:\"trigger-based-replication\",children:[(0,a.jsx)(e.a,{className:\"anchor\",href:\"#trigger-based-replication\",children:(0,a.jsx)(e.span,{className:\"icon icon-link\"})}),(0,a.jsx)(e.strong,{children:\"Trigger-based replication\"})]}),`\n`,(0,a.jsx)(e.p,{children:\"In this type of replication, triggers are set up on the source database to capture changes, which can then be propagated to the target database. It allows for fine-grained control over which changes are propagated, but it can be resource-intensive and may be unsuitable for high-traffic systems.\"}),`\n`,(0,a.jsxs)(e.h4,{id:\"cloud-based-database-synchronization\",children:[(0,a.jsx)(e.a,{className:\"anchor\",href:\"#cloud-based-database-synchronization\",children:(0,a.jsx)(e.span,{className:\"icon icon-link\"})}),(0,a.jsx)(e.strong,{children:\"Cloud-based database synchronization\"})]}),`\n`,(0,a.jsxs)(e.p,{children:[\"We can also use cloud services like \",(0,a.jsx)(e.a,{href:\"https://aws.amazon.com/dms/\",children:\"AWS Database Migration Service (DMS)\"}),\" and \",(0,a.jsx)(e.a,{href:\"https://azure.microsoft.com/en-us/products/database-migration\",children:\"Azure Database Migration Service\"}),\" to sync databases. This is a good option if you have a cloud-based infrastructure and want to leverage the scalability and reliability offered by these services.\"]}),`\n`,(0,a.jsx)(e.p,{children:\"The best approach for your use case will depend on the type of data, the number of databases, the network infrastructure, and the requirements of the application.\"}),`\n`,(0,a.jsxs)(e.h3,{id:\"how-to-sync-databases\",children:[(0,a.jsx)(e.a,{className:\"anchor\",href:\"#how-to-sync-databases\",children:(0,a.jsx)(e.span,{className:\"icon icon-link\"})}),\"\\u200D\",(0,a.jsx)(e.strong,{children:\"How to Sync Databases\"})]}),`\n`,(0,a.jsx)(e.p,{children:\"The process of syncing databases can vary, depending on the type of databases, the method of synchronization, and the specific requirements of the application. Below is the step-by-step guide on how to sync databases:\"}),`\n`,(0,a.jsxs)(e.h4,{id:\"step-1-understand-your-use-case\",children:[(0,a.jsx)(e.a,{className:\"anchor\",href:\"#step-1-understand-your-use-case\",children:(0,a.jsx)(e.span,{className:\"icon icon-link\"})}),(0,a.jsx)(e.strong,{children:\"Step 1: Understand your use case\"})]}),`\n`,(0,a.jsx)(e.p,{children:\"Understand the specific requirements of your use case and choose the synchronization method that best fits those needs.\"}),`\n`,(0,a.jsxs)(e.h4,{id:\"step-2-identify-the-databases-to-be-synced\",children:[(0,a.jsx)(e.a,{className:\"anchor\",href:\"#step-2-identify-the-databases-to-be-synced\",children:(0,a.jsx)(e.span,{className:\"icon icon-link\"})}),(0,a.jsx)(e.strong,{children:\"Step 2: Identify the databases to be synced\"})]}),`\n`,(0,a.jsx)(e.p,{children:\"Determine which databases need to be synced and the type of data they contain.\"}),`\n`,(0,a.jsxs)(e.h4,{id:\"step-3-choose-a-synchronization-method\",children:[(0,a.jsx)(e.a,{className:\"anchor\",href:\"#step-3-choose-a-synchronization-method\",children:(0,a.jsx)(e.span,{className:\"icon icon-link\"})}),(0,a.jsx)(e.strong,{children:\"Step 3: Choose a synchronization method\"})]}),`\n`,(0,a.jsx)(e.p,{children:\"Decide on the method of synchronization that's most appropriate for your use case. This may be replication, a data syncing tool, a custom script, or a cloud-based service.\"}),`\n`,(0,a.jsxs)(e.h4,{id:\"step-4-configure-the-databases\",children:[(0,a.jsx)(e.a,{className:\"anchor\",href:\"#step-4-configure-the-databases\",children:(0,a.jsx)(e.span,{className:\"icon icon-link\"})}),(0,a.jsx)(e.strong,{children:\"Step 4: Configure the databases\"})]}),`\n`,(0,a.jsx)(e.p,{children:\"Set up the databases for synchronization. This may include configuring replication settings, installing data syncing tools, or writing custom scripts.\"}),`\n`,(0,a.jsxs)(e.h4,{id:\"step-5-test-the-synchronization\",children:[(0,a.jsx)(e.a,{className:\"anchor\",href:\"#step-5-test-the-synchronization\",children:(0,a.jsx)(e.span,{className:\"icon icon-link\"})}),(0,a.jsx)(e.strong,{children:\"Step 5: Test the synchronization\"})]}),`\n`,(0,a.jsx)(e.p,{children:\"Test the synchronization by making changes to one database and verifying that the changes are propagated to the other databases. This will help you identify any issues or bugs before deploying it in production.\"}),`\n`,(0,a.jsxs)(e.h4,{id:\"step-6-schedule-synchronization\",children:[(0,a.jsx)(e.a,{className:\"anchor\",href:\"#step-6-schedule-synchronization\",children:(0,a.jsx)(e.span,{className:\"icon icon-link\"})}),(0,a.jsx)(e.strong,{children:\"Step 6: Schedule synchronization\"})]}),`\n`,(0,a.jsx)(e.p,{children:\"Set a schedule for the synchronization to occur regularly. You can do this using a built-in scheduling feature or by writing a custom script.\"}),`\n`,(0,a.jsxs)(e.h4,{id:\"step-7-monitor-and-troubleshoot\",children:[(0,a.jsx)(e.a,{className:\"anchor\",href:\"#step-7-monitor-and-troubleshoot\",children:(0,a.jsx)(e.span,{className:\"icon icon-link\"})}),(0,a.jsx)(e.strong,{children:\"Step 7: Monitor and troubleshoot\"})]}),`\n`,(0,a.jsx)(e.p,{children:\"Monitor the synchronization process and troubleshoot any issues that arise. This may include monitoring replication lag, checking for errors, and addressing any conflicts that occur. It may also include monitoring replication lag, checking for errors, and addressing any conflicts that occur.\"}),`\n`,(0,a.jsxs)(e.h4,{id:\"step-8-maintain-and-update\",children:[(0,a.jsx)(e.a,{className:\"anchor\",href:\"#step-8-maintain-and-update\",children:(0,a.jsx)(e.span,{className:\"icon icon-link\"})}),(0,a.jsx)(e.strong,{children:\"Step 8: Maintain and update\"})]}),`\n`,(0,a.jsx)(e.p,{children:\"Regularly maintain and update the synchronization process to ensure that it continues to function correctly.\"}),`\n`,(0,a.jsxs)(e.p,{children:[\"\\u200D\",(0,a.jsx)(e.strong,{children:\"Note:\"}),\" Some steps may vary, depending on the type of database and the method of synchronization. For example, for a cloud-based service like AWS DMS, the process can be simpler. You have to create the replication task and configure the source and target databases.\"]}),`\n`,(0,a.jsxs)(e.h3,{id:\"tooling-for-database-synchronization\",children:[(0,a.jsx)(e.a,{className:\"anchor\",href:\"#tooling-for-database-synchronization\",children:(0,a.jsx)(e.span,{className:\"icon icon-link\"})}),(0,a.jsx)(e.strong,{children:\"Tooling for Database Synchronization\"})]}),`\n`,(0,a.jsx)(e.p,{children:\"There are various tools available for database synchronization, depending on the type of database and the method of synchronization you're using. You can achieve database synchronization through various methods, such as the following:\"}),`\n`,(0,a.jsxs)(e.h4,{id:\"custom-scripts\",children:[(0,a.jsx)(e.a,{className:\"anchor\",href:\"#custom-scripts\",children:(0,a.jsx)(e.span,{className:\"icon icon-link\"})}),(0,a.jsx)(e.strong,{children:\"Custom scripts\"})]}),`\n`,(0,a.jsx)(e.p,{children:\"You can write custom scripts using programming languages such as Python or Java to sync databases. This involves writing code to compare data in two databases and making changes as needed.\"}),`\n`,(0,a.jsxs)(e.h4,{id:\"replication\",children:[(0,a.jsx)(e.a,{className:\"anchor\",href:\"#replication\",children:(0,a.jsx)(e.span,{className:\"icon icon-link\"})}),(0,a.jsx)(e.strong,{children:\"Replication\"})]}),`\n`,(0,a.jsx)(e.p,{children:\"This involves copying data from one database to another so that changes made to one are reflected in the other.\"}),`\n`,(0,a.jsxs)(e.ul,{children:[`\n`,(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.a,{href:\"https://dev.mysql.com/doc/refman/8.0/en/replication.html\",children:\"MySQL\"}),\" provides built-in replication capabilities, allowing users to replicate data between two or more MySQL servers.\"]}),`\n`,(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.a,{href:\"https://learn.microsoft.com/en-us/sql/relational-databases/replication/sql-server-replication?view=sql-server-ver16\",children:\"Microsoft SQL Server\"}),\" also provides built-in replication capabilities, including transactional replication and merge replication.\"]}),`\n`,(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.a,{href:\"https://www.postgresql.org/docs/current/runtime-config-replication.html\",children:\"PostgreSQL\"}),\" offers several replication solutions, including streaming replication and \",(0,a.jsx)(e.a,{href:\"https://www.postgresql.org/docs/15/logical-replication.html\",children:\"logical replication\"}),\".\"]}),`\n`,(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.a,{href:\"https://www.mongodb.com/docs/manual/replication/\",children:\"MongoDB\"}),\" provides built-in replication features, including replica sets and sharded clusters for horizontal scalability.\"]}),`\n`]}),`\n`,(0,a.jsxs)(e.h4,{id:\"cloud-based-services\",children:[(0,a.jsx)(e.a,{className:\"anchor\",href:\"#cloud-based-services\",children:(0,a.jsx)(e.span,{className:\"icon icon-link\"})}),(0,a.jsx)(e.strong,{children:\"Cloud-based services\"})]}),`\n`,(0,a.jsx)(e.p,{children:\"Below are some of the common cloud-based services that you can use to sync databases.\"}),`\n`,(0,a.jsxs)(e.ul,{children:[`\n`,(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.a,{href:\"https://aws.amazon.com/dms/\",children:\"AWS Database Migration Service (DMS)\"}),\" can migrate, replicate, and sync databases between different platforms and environments, including on-premises environments and cloud-based environments like Amazon Web Services (AWS).\"]}),`\n`,(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.a,{href:\"https://azure.microsoft.com/en-us/products/database-migration\",children:\"Azure Database Migration Service\"}),\" is a fully managed service designed to enable seamless migrations from multiple database sources to Azure.\"]}),`\n`]}),`\n`,(0,a.jsxs)(e.h4,{id:\"data-syncing-tools\",children:[(0,a.jsx)(e.a,{className:\"anchor\",href:\"#data-syncing-tools\",children:(0,a.jsx)(e.span,{className:\"icon icon-link\"})}),(0,a.jsx)(e.strong,{children:\"Data syncing tools\"})]}),`\n`,(0,a.jsx)(e.p,{children:\"Various data syncing tools can automate the process of keeping databases in sync.\"}),`\n`,(0,a.jsxs)(e.ul,{children:[`\n`,(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.a,{href:\"https://www.symmetricds.org/\",children:\"SymmetricDS\"}),\" is open-source data synchronization software that supports multiple relational databases, including MySQL, PostgreSQL, Oracle, and more.\"]}),`\n`,(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.a,{href:\"https://www.talend.com/\",children:\"Talend\"}),\", \",(0,a.jsx)(e.a,{href:\"https://www.informatica.com/\",children:\"Informatica\"}),\", and \",(0,a.jsx)(e.a,{href:\"https://boomi.com/\",children:\"Boomi\"}),\" can automate the process of keeping databases in sync.\"]}),`\n`,(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.a,{href:\"https://www.oracle.com/in/integration/goldengate/\",children:\"Oracle GoldenGate\"}),\" is a real-time data integration and replication software for heterogeneous environments, including Oracle, SQL Server, DB2, and more.\"]}),`\n`]}),`\n`,(0,a.jsx)(e.p,{children:\"These are some of the popular tooling used for database synchronization. However, the best tool depends on the type of database, the method of synchronization, and the specific requirements of the application.\"}),`\n`,(0,a.jsxs)(e.h3,{id:\"best-practices\",children:[(0,a.jsx)(e.a,{className:\"anchor\",href:\"#best-practices\",children:(0,a.jsx)(e.span,{className:\"icon icon-link\"})}),(0,a.jsx)(e.strong,{children:\"Best Practices\"})]}),`\n`,(0,a.jsx)(e.p,{children:\"When working with database synchronization, there are several best practices that can help you ensure that your synchronization process is efficient and reliable.\"}),`\n`,(0,a.jsxs)(e.h4,{id:\"keep-your-databases-in-sync\",children:[(0,a.jsx)(e.a,{className:\"anchor\",href:\"#keep-your-databases-in-sync\",children:(0,a.jsx)(e.span,{className:\"icon icon-link\"})}),(0,a.jsx)(e.strong,{children:\"Keep your databases in sync\"})]}),`\n`,(0,a.jsx)(e.p,{children:\"Regularly check and compare data between the different copies of the database and make updates as needed.\"}),`\n`,(0,a.jsxs)(e.h4,{id:\"use-a-replication-tool\",children:[(0,a.jsx)(e.a,{className:\"anchor\",href:\"#use-a-replication-tool\",children:(0,a.jsx)(e.span,{className:\"icon icon-link\"})}),(0,a.jsx)(e.strong,{children:\"Use a replication tool\"})]}),`\n`,(0,a.jsx)(e.p,{children:\"Use a replication tool that best fits your use case and the type of data you're working with.\"}),`\n`,(0,a.jsxs)(e.h4,{id:\"backups\",children:[(0,a.jsx)(e.a,{className:\"anchor\",href:\"#backups\",children:(0,a.jsx)(e.span,{className:\"icon icon-link\"})}),(0,a.jsx)(e.strong,{children:\"Backups\"})]}),`\n`,(0,a.jsx)(e.p,{children:\"Regularly back up your databases to ensure that you can recover from data loss or corruption.\"}),`\n`,(0,a.jsxs)(e.h4,{id:\"use-cloud-based-services\",children:[(0,a.jsx)(e.a,{className:\"anchor\",href:\"#use-cloud-based-services\",children:(0,a.jsx)(e.span,{className:\"icon icon-link\"})}),(0,a.jsx)(e.strong,{children:\"Use cloud-based services\"})]}),`\n`,(0,a.jsx)(e.p,{children:\"You can also use Cloud-based services like AWS DMS and Azure Database Migration Service to sync databases. It's a good option if you have a cloud-based infrastructure and want to leverage the scalability and reliability offered by these services.\"}),`\n`,(0,a.jsx)(e.p,{children:\"\\u200D\"}),`\n`,(0,a.jsx)(e.p,{children:(0,a.jsx)(e.img,{src:\"/blog-images/11c2a64f57f07276266f10d2b4187dbd.png\",alt:\"\"})}),`\n`,(0,a.jsxs)(e.h4,{id:\"security\",children:[(0,a.jsx)(e.a,{className:\"anchor\",href:\"#security\",children:(0,a.jsx)(e.span,{className:\"icon icon-link\"})}),(0,a.jsx)(e.strong,{children:\"Security\"})]}),`\n`,(0,a.jsx)(e.p,{children:\"Ensure that your synchronization process is secure by encrypting data in transit and at rest and by implementing access controls.\"}),`\n`,(0,a.jsx)(e.p,{children:\"By following these best practices, you can ensure that your databases are kept in sync and that your synchronization process is efficient and reliable. Additionally, it's always important to be aware of the particularities of the database you're working with, the replication tool you're using, and the type of data you're syncing in order to apply the best practices in a way that fits your specific needs.\"}),`\n`,(0,a.jsxs)(e.h3,{id:\"conclusion\",children:[(0,a.jsx)(e.a,{className:\"anchor\",href:\"#conclusion\",children:(0,a.jsx)(e.span,{className:\"icon icon-link\"})}),(0,a.jsx)(e.strong,{children:\"Conclusion\"})]}),`\n`,(0,a.jsx)(e.p,{children:\"Syncing databases is a crucial task that requires careful planning and execution. By understanding your use case, choosing the right synchronization method, and following best practices such as testing, monitoring, and maintaining, you can ensure that your databases sync effectively and efficiently. Additionally, by using cloud-based services, you can reduce the complexity of the process and ensure that your databases are always in sync.\"}),`\n`,(0,a.jsxs)(e.p,{children:[\"If you're looking for a solution to manage your \",(0,a.jsx)(e.a,{href:\"https://release.com/blog/remote-development-environments\",children:\"remote development environments\"}),\", you can use \",(0,a.jsx)(e.a,{href:\"https://release.com/book-a-demo\",children:\"Release\"}),\" to create, manage and share development environments, collaborate with your team, and automate your release pipeline, all in one place. \",(0,a.jsx)(e.a,{href:\"https://release.com/book-a-demo\",children:\"Book a demo\"}),\" today and see the difference it can make in your remote development process\"]}),`\n`,(0,a.jsx)(e.p,{children:\"\\u200D\"})]})}function z(n={}){let{wrapper:e}=n.components||{};return e?(0,a.jsx)(e,Object.assign({},n,{children:(0,a.jsx)(h,n)})):h(n)}var k=z;return w(N);})();\n;return Component;"
        },
        "_id": "blog/posts/syncing-databases-how-to-do-it-and-best-practices.mdx",
        "_raw": {
          "sourceFilePath": "blog/posts/syncing-databases-how-to-do-it-and-best-practices.mdx",
          "sourceFileName": "syncing-databases-how-to-do-it-and-best-practices.mdx",
          "sourceFileDir": "blog/posts",
          "contentType": "mdx",
          "flattenedPath": "blog/posts/syncing-databases-how-to-do-it-and-best-practices"
        },
        "type": "BlogPost",
        "computedSlug": "syncing-databases-how-to-do-it-and-best-practices"
      },
      "documentHash": "1739393595028",
      "hasWarnings": false,
      "documentTypeName": "BlogPost"
    },
    "blog/posts/talk-to-your-infrastructure-in-plain-english-introducing-release-ai.mdx": {
      "document": {
        "title": "Talk to your infrastructure in plain English: introducing Release AI",
        "summary": "",
        "publishDate": "Fri Aug 25 2023 18:40:50 GMT+0000 (Coordinated Universal Time)",
        "author": "tommy-mcclung",
        "readingTime": 3,
        "categories": [
          "ai",
          "platform-engineering",
          "product"
        ],
        "mainImage": "",
        "imageAlt": "",
        "showCTA": true,
        "ctaCopy": "Unlock instant infrastructure insights in plain English with Release AI. Spin up environments effortlessly for faster app delivery.",
        "ctaLink": "https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=talk-to-your-infrastructure-in-plain-english-introducing-release-ai",
        "relatedPosts": [
          ""
        ],
        "ogImage": "/blog/default-og-image.png",
        "excerpt": "The wait(list) is over. Release AI is now available to everyone, and for a limited time it’s absolutely free. I am thrilled to share this innovative AI tool ...",
        "tags": [
          "ai",
          "platform-engineering",
          "product"
        ],
        "ctaButton": "Try Release for Free",
        "body": {
          "raw": "\nThe wait(list) is over. Release AI is now available to everyone, and for a limited time it’s absolutely free. I am thrilled to share this innovative AI tool designed to redefine the way you build and deliver applications so you can experience our context-aware Release AI, directly through your CLI. Join our launch community, and share your feedback that will shape the feature roadmap of Release AI.\n\nSign up for your free account [here](https://beta.release.com/ai/register) and let us know what you think.  \n\nRelease helps teams release their ideas to the world faster. For developers like you, it means spinning up ephemeral environments in seconds and seeing exactly how your app will perform in production, or using production-like data to test your creations under real loads. With Release AI it also means getting all your infrastructure questions answered without relying on the limited resources of your DevOps team. Originally, DevOps was meant to empower developers with operational tasks, but instead it turned into an additional hurdle on the path to delivering product features quickly.\n\nRelease AI lets you tap into the information needed to deploy and release your ideas to the world. Now you don’t need to track down your go-to DevOps engineer and wait for them to run complex queries to answer questions like:\n\n- What's going on in my AWS account?\n- How is it configured?\n- How is my Kubernetes cluster setup?\n- Are pods crashing?\n- What are the IP addresses of my instances?\n- What was the bill in my AWS account yesterday?\n\nWhat used to take hours or days to figure out, is now available directly in your command line. With Release AI you can talk to your underlying infrastructure in plain English and have it perform complex tasks with your context in mind. You can even schedule tasks, so you can ask these questions on a recurring basis and get the result delivered to you automatically.  This removes a bottleneck in your ability to deliver software faster, and gets the DevOps out of the business of being highly paid developer support engineers.\n\nTo get started, [sign up for a Release AI account](https://beta.release.com/ai/register) and follow the steps in our [Quickstart guide](https://docs.release.com/release-ai/quickstart) to get up and running. [Join our Slack Channel](https://join.slack.com/t/release-ai/shared_invite/zt-20dxgp2o0-ePCXzsag6l8_i4HwOgRItg) to stay up to date on new developments and share your insights. Make sure to check our [docs](https://docs.release.com/release-ai/introduction) for more details on how Release AI works, to get sample prompts and to see advanced configurations.\n\nWe believe Release AI has the potential to transform the way you work, empowering you to achieve new levels of efficiency, accuracy, and productivity. However, we cannot accomplish this alone. Your expertise and feedback are invaluable in shaping the future of Release AI, and ensuring that it genuinely caters to the needs of professionals like you. Join us for this preview phase, available free for all Release users via the CLI.\n",
          "code": "var Component=(()=>{var d=Object.create;var o=Object.defineProperty;var h=Object.getOwnPropertyDescriptor;var p=Object.getOwnPropertyNames;var m=Object.getPrototypeOf,g=Object.prototype.hasOwnProperty;var y=(n,e)=>()=>(e||n((e={exports:{}}).exports,e),e.exports),f=(n,e)=>{for(var a in e)o(n,a,{get:e[a],enumerable:!0})},s=(n,e,a,r)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let i of p(e))!g.call(n,i)&&i!==a&&o(n,i,{get:()=>e[i],enumerable:!(r=h(e,i))||r.enumerable});return n};var w=(n,e,a)=>(a=n!=null?d(m(n)):{},s(e||!n||!n.__esModule?o(a,\"default\",{value:n,enumerable:!0}):a,n)),v=n=>s(o({},\"__esModule\",{value:!0}),n);var u=y((R,l)=>{l.exports=_jsx_runtime});var A={};f(A,{default:()=>I,frontmatter:()=>k});var t=w(u()),k={title:\"Talk to your infrastructure in plain English: introducing Release AI\",summary:\"\",publishDate:\"Fri Aug 25 2023 18:40:50 GMT+0000 (Coordinated Universal Time)\",author:\"tommy-mcclung\",readingTime:3,categories:[\"ai\",\"platform-engineering\",\"product\"],mainImage:\"\",imageAlt:\"\",showCTA:!0,ctaCopy:\"Unlock instant infrastructure insights in plain English with Release AI. Spin up environments effortlessly for faster app delivery.\",ctaLink:\"https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=talk-to-your-infrastructure-in-plain-english-introducing-release-ai\",relatedPosts:[\"\"],ogImage:\"/blog/default-og-image.png\",excerpt:\"The wait(list) is over. Release AI is now available to everyone, and for a limited time it\\u2019s absolutely free. I am thrilled to share this innovative AI tool ...\",tags:[\"ai\",\"platform-engineering\",\"product\"],ctaButton:\"Try Release for Free\"};function c(n){let e=Object.assign({p:\"p\",a:\"a\",ul:\"ul\",li:\"li\"},n.components);return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(e.p,{children:\"The wait(list) is over. Release AI is now available to everyone, and for a limited time it\\u2019s absolutely free. I am thrilled to share this innovative AI tool designed to redefine the way you build and deliver applications so you can experience our context-aware Release AI, directly through your CLI. Join our launch community, and share your feedback that will shape the feature roadmap of Release AI.\"}),`\n`,(0,t.jsxs)(e.p,{children:[\"Sign up for your free account \",(0,t.jsx)(e.a,{href:\"https://beta.release.com/ai/register\",children:\"here\"}),\" and let us know what you think. \\xA0\"]}),`\n`,(0,t.jsx)(e.p,{children:\"Release helps teams release their ideas to the world faster. For developers like you, it means spinning up ephemeral environments in seconds and seeing exactly how your app will perform in production, or using production-like data to test your creations under real loads. With Release AI it also means getting all your infrastructure questions answered without relying on the limited resources of your DevOps team. Originally, DevOps was meant to empower developers with operational tasks, but instead it turned into an additional hurdle on the path to delivering product features quickly.\"}),`\n`,(0,t.jsx)(e.p,{children:\"Release AI lets you tap into the information needed to deploy and release your ideas to the world. Now you don\\u2019t need to track down your go-to DevOps engineer and wait for them to run complex queries to answer questions like:\"}),`\n`,(0,t.jsxs)(e.ul,{children:[`\n`,(0,t.jsx)(e.li,{children:\"What's going on in my AWS account?\"}),`\n`,(0,t.jsx)(e.li,{children:\"How is it configured?\"}),`\n`,(0,t.jsx)(e.li,{children:\"How is my Kubernetes cluster setup?\"}),`\n`,(0,t.jsx)(e.li,{children:\"Are pods crashing?\"}),`\n`,(0,t.jsx)(e.li,{children:\"What are the IP addresses of my instances?\"}),`\n`,(0,t.jsx)(e.li,{children:\"What was the bill in my AWS account yesterday?\"}),`\n`]}),`\n`,(0,t.jsx)(e.p,{children:\"What used to take hours or days to figure out, is now available directly in your command line. With Release AI you can talk to your underlying infrastructure in plain English and have it perform complex tasks with your context in mind. You can even schedule tasks, so you can ask these questions on a recurring basis and get the result delivered to you automatically. \\xA0This removes a bottleneck in your ability to deliver software faster, and gets the DevOps out of the business of being highly paid developer support engineers.\"}),`\n`,(0,t.jsxs)(e.p,{children:[\"To get started, \",(0,t.jsx)(e.a,{href:\"https://beta.release.com/ai/register\",children:\"sign up for a Release AI account\"}),\" and follow the steps in our \",(0,t.jsx)(e.a,{href:\"https://docs.release.com/release-ai/quickstart\",children:\"Quickstart guide\"}),\" to get up and running. \",(0,t.jsx)(e.a,{href:\"https://join.slack.com/t/release-ai/shared_invite/zt-20dxgp2o0-ePCXzsag6l8_i4HwOgRItg\",children:\"Join our Slack Channel\"}),\" to stay up to date on new developments and share your insights. Make sure to check our \",(0,t.jsx)(e.a,{href:\"https://docs.release.com/release-ai/introduction\",children:\"docs\"}),\" for more details on how Release AI works, to get sample prompts and to see advanced configurations.\"]}),`\n`,(0,t.jsx)(e.p,{children:\"We believe Release AI has the potential to transform the way you work, empowering you to achieve new levels of efficiency, accuracy, and productivity. However, we cannot accomplish this alone. Your expertise and feedback are invaluable in shaping the future of Release AI, and ensuring that it genuinely caters to the needs of professionals like you. Join us for this preview phase, available free for all Release users via the CLI.\"})]})}function b(n={}){let{wrapper:e}=n.components||{};return e?(0,t.jsx)(e,Object.assign({},n,{children:(0,t.jsx)(c,n)})):c(n)}var I=b;return v(A);})();\n;return Component;"
        },
        "_id": "blog/posts/talk-to-your-infrastructure-in-plain-english-introducing-release-ai.mdx",
        "_raw": {
          "sourceFilePath": "blog/posts/talk-to-your-infrastructure-in-plain-english-introducing-release-ai.mdx",
          "sourceFileName": "talk-to-your-infrastructure-in-plain-english-introducing-release-ai.mdx",
          "sourceFileDir": "blog/posts",
          "contentType": "mdx",
          "flattenedPath": "blog/posts/talk-to-your-infrastructure-in-plain-english-introducing-release-ai"
        },
        "type": "BlogPost",
        "computedSlug": "talk-to-your-infrastructure-in-plain-english-introducing-release-ai"
      },
      "documentHash": "1739393595028",
      "hasWarnings": false,
      "documentTypeName": "BlogPost"
    },
    "blog/posts/terraform-kubernetes-deployment-a-detailed-walkthrough.mdx": {
      "document": {
        "title": "Terraform Kubernetes Deployment: A Detailed Walkthrough",
        "summary": " Did you know that you can use Terraform for deployment of your Kubernetes clusters? Learn how and why to do it here.",
        "publishDate": "Sat Aug 06 2022 22:18:19 GMT+0000 (Coordinated Universal Time)",
        "author": "regis-wilson",
        "readingTime": 6,
        "categories": [
          "kubernetes",
          "platform-engineering"
        ],
        "mainImage": "/blog-images/93cadc79db3755d6a026b27fe882105c.jpg",
        "imageAlt": "Terraform Kubernetes Deployment: A Detailed Walkthrough",
        "showCTA": true,
        "ctaCopy": "Looking to streamline Kubernetes deployment workflows? Try Release.com for ephemeral environments that mirror production, enabling faster testing and deployment cycles.",
        "ctaLink": "https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=terraform-kubernetes-deployment-a-detailed-walkthrough",
        "relatedPosts": [
          ""
        ],
        "ogImage": "/blog-images/93cadc79db3755d6a026b27fe882105c.jpg",
        "excerpt": " Did you know that you can use Terraform for deployment of your Kubernetes clusters? Learn how and why to do it here.",
        "tags": [
          "kubernetes",
          "platform-engineering"
        ],
        "ctaButton": "Try Release for Free",
        "body": {
          "raw": "\nTerraform and Kubernetes are two of the most popular tools in their categories. Terraform is widely adopted as the tool of choice for infrastructure as code, and Kubernetes is number one when it comes to orchestrating containers. Is it possible to combine both? Sure! You can use Terraform to deploy your Kubernetes clusters. It's actually quite common, and it lets you deploy Kubernetes just like the rest of your infrastructure. In this post, you'll learn how to do it.\n\n### Terraform + Kubernetes: How and Why?\n\nWe have two main questions to answer here. How can you deploy Kubernetes with Terraform, and why would you do that? Let's start with the latter.\n\n![ApplicationDescription automatically generated with medium confidence](/blog-images/a82632ed72605aab10dcbb180de0d4d1.png)\n\nThe answer doesn't differ from \"Why would you deploy anything with Terraform?\" From that perspective, there's nothing special about Kubernetes, and you get the same benefit by using Terraform to deploy it as with any other infrastructure. You get automation, infrastructure versioning, reliability, and even the ability to perform infrastructure security scanning.\n\nAs for how, the answer is actually similar. You can deploy Kubernetes with Terraform just like any other infrastructure. Meaning, you first need to find a Kubernetes resource definition for Terraform (we'll show you that shortly), adjust some parameters for your needs, add it to your Terraform code, and you're done. And just like with any other resource, Terraform will be able to track changes to your cluster and update its configuration after you make changes to the code.\n\n### Deploying Kubernetes: First Steps\n\nEnough theory. Let's see how it works in practice. First, you need to find a Terraform provider for your cloud. If you want to deploy Kubernetes on [DigitalOcean](https://www.digitalocean.com/), you'd need to follow [this documentation](https://registry.terraform.io/providers/digitalocean/digitalocean/latest/docs/resources/kubernetes_cluster). For [Microsoft Azure](https://azure.microsoft.com/en-us/), you'd need to head [here](https://registry.terraform.io/providers/hashicorp/azurerm/latest/docs/resources/kubernetes_cluster) for details. And for [Google Cloud](https://cloud.google.com/), you need to check [here](https://registry.terraform.io/providers/hashicorp/google/latest/docs/resources/container_cluster). These are just a few examples. But no matter which cloud provider you're using, the general approach will be the same. For today's example, we'll use DigitalOcean.\n\nTo start from nothing, in the simplest scenario, you need to create two files named **provider.tf** and **main.tf**. You could do it all in one file, but it's a good practice to separate providers and main resource definitions. In the code below, you can define your DigitalOcean provider for Terraform and pass your DigitalOcean token:\n\n```yaml\n\nterraform {\n  required_providers {\n    digitalocean = {\n      source = \"digitalocean/digitalocean\"\n      version = \"~> 2.0\"\n    }\n  }\n}\nvariable \"do_token\" {\n  default = \"[replace_with_your_token]\"\n}\n\n# Configure the DigitalOcean Provider\n\nprovider \"digitalocean\" {\n  token = var.do_token\n}\n\n```\n\nIn **main.tf** you can now define your Kubernetes.\n\n```yaml\nresource \"digitalocean_kubernetes_cluster\" \"test\" {\nname   = \"test_cluster\"\nregion = \"nyc1\"\nversion = \"1.22.11-do.0\"\nnode_pool {\nname       = \"worker-pool\"\nsize       = \"s-2vcpu-2gb\"\nnode_count = 3\n}\n}\n```\n\nNow that you have your Terraform files prepared, you need three things. First, you need to initiate the DigitalOcean provider. You can do that with **terraform init**.\n\n```yaml\n\n# terraform init\n\nInitializing the backend...\nInitializing provider plugins...\n- Finding digitalocean/digitalocean versions matching \"~> 2.0\"...\n- Installing digitalocean/digitalocean v2.21.0...\n- Installed digitalocean/digitalocean v2.21.0 (signed by a HashiCorp partner, key ID F82037E524B9C0E8)\nPartner and community providers are signed by their developers.\nIf you'd like to know more about provider signing, you can read about it here:\nhttps://www.terraform.io/docs/cli/plugins/signing.html\nTerraform has created a lock file .terraform.lock.hcl to record the provider\nselections it made above. Include this file in your version control repository\nso that Terraform can guarantee to make the same selections by default when\nyou run \"terraform init\" in the future.\nTerraform has been successfully initialized!\nYou may now begin working with Terraform. Try running \"terraform plan\" to see\nany changes that are required for your infrastructure. All Terraform commands\nshould now work.\nIf you ever set or change modules or backend configuration for Terraform,\nrerun this command to reinitialize your working directory. If you forget, other\ncommands will detect it and remind you to do so if necessary.\n\n```\n\nThen you can run your **terraform plan**, which will show you planned changes to the infrastructure (which in this case should be creating a new Kubernetes cluster).\n\n```yaml\n\n# terraform plan\n\nTerraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following\nsymbols:\n  + create\nTerraform will perform the following actions:\n  # digitalocean_kubernetes_cluster.test will be created\n  + resource \"digitalocean_kubernetes_cluster\" \"test\" {\n      + cluster_subnet = (known after apply)\n      + created_at     = (known after apply)\n      + endpoint       = (known after apply)\n      + ha             = false\n      + id             = (known after apply)\n      + ipv4_address   = (known after apply)\n      + kube_config    = (sensitive value)\n      + name           = \"test-cluster\"\n      + region         = \"nyc1\"\n      + service_subnet = (known after apply)\n      + status         = (known after apply)\n      + surge_upgrade  = true\n      + updated_at     = (known after apply)\n      + urn            = (known after apply)\n      + version        = \"1.22.11-do.0\"\n      + vpc_uuid       = (known after apply)\n      + maintenance_policy {\n          + day        = (known after apply)\n          + duration   = (known after apply)\n          + start_time = (known after apply)\n        }\n      + node_pool {\n          + actual_node_count = (known after apply)\n          + auto_scale        = false\n          + id                = (known after apply)\n          + name              = \"worker-pool\"\n          + node_count        = 3\n          + nodes             = (known after apply)\n          + size              = \"s-2vcpu-2gb\"\n        }\n    }\nPlan: 1 to add, 0 to change, 0 to destroy.\n\n```\n\nThe plan looks good. One resource will be added, and that's your Kubernetes cluster, so you can go ahead and apply the changes with **terraform apply**.\n\n```yaml\n\n# terraform apply\n\n(...)\nPlan: 1 to add, 0 to change, 0 to destroy.\nDo you want to perform these actions?\n  Terraform will perform the actions described above.\n  Only 'yes' will be accepted to approve.\n  Enter a value: yes\ndigitalocean_kubernetes_cluster.test: Creating...\ndigitalocean_kubernetes_cluster.test: Still creating... [10s elapsed]\n(...)\ndigitalocean_kubernetes_cluster.test: Still creating... [7m10s elapsed]\ndigitalocean_kubernetes_cluster.test: Creation complete after 7m16s [id=49fd0517-a4a5-41e8-997d-1412c081e000]\nApply complete! Resources: 1 added, 0 changed, 0 destroyed.\n\n```\n\nIf you now head to your DigitalOcean portal to validate, you can indeed see it there.\n\n![DigitalOcean portal](/blog-images/4ae1114850cfa16a93e987103c7eb5e0.png)\n\nAnd that's it! That's how you deploy Kubernetes with Terraform.\n\n![A picture containing device, control panelDescription automatically generated](/blog-images/14771b0c5e49d3c50009a99ebfdcd091.jpeg)\n\n### Deploying Kubernetes: Next Steps\n\nNow that you know how it works in general, there are a few things that you need to learn next. First, all you've done is deploy basic, minimal Kubernetes. In more realistic scenarios, you'll probably want to parametrize more options for your Kubernetes. This, however, will highly depend on what you actually need. If you know what you need, you can head to the Terraform documentation and check [**argument reference**](https://registry.terraform.io/providers/digitalocean/digitalocean/latest/docs/resources/kubernetes_cluster#argument-reference) for your Kubernetes resource. Find what you need and add it to your code.\n\nFor example, if you'd like your Kubernetes cluster to automatically upgrade, you can find the following in the documentation:\n\n![Auto upgrade](/blog-images/0a5e2c452dafd796806726969310f5a2.png)\n\nTo make your freshly deployed cluster automatically upgrade, you just need to add the following to your Kubernetes resource definition in **main.tf** as follows:\n\n```yaml\nresource \"digitalocean_kubernetes_cluster\" \"test\" {\nname   = \"test-cluster\"\nregion = \"nyc1\"\nversion = \"1.22.11-do.0\"\nauto_upgrade = true\nnode_pool {\nname       = \"worker-pool\"\nsize       = \"s-2vcpu-2gb\"\nnode_count = 3\n}\n}\n```\n\nBut you're not there yet. You can quickly see in the DigitalOcean portal that the cluster currently does not automatically upgrade.\n\n![auto upgrade in portal](/blog-images/a2dfca6ce7870345b1829ffe4f4a457e.png)\n\nAutomatic upgrades are disabled now, so you can run **terraform plan** again to check what Terraform will try to do.\n\n```yaml\n# terraform plan\n\nTerraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols:  ~ update in-place\nTerraform will perform the following actions:\n   # digitalocean_kubernetes_cluster.test will be updated in-place\n  ~ resource \"digitalocean_kubernetes_cluster\" \"test\" {\n  ~ auto_upgrade   = false -> true\n  id             = \"49fd0517-a4a5-41e8-997d-1412c081e000\"\n  name           = \"test-cluster\"\n  tags           = []\n  # (13 unchanged attributes hidden)\n  # (2 unchanged blocks hidden)\n  }\nPlan: 0 to add, 1 to change, 0 to destroy\n```\n\nAs expected, Terraform will now try to update your cluster in place and add an auto-upgrade option to it. Let's go ahead and apply that change.\n\n```yaml\n\n# terraform apply\n\n(...)\nPlan: 0 to add, 1 to change, 0 to destroy.\nDo you want to perform these actions?\n  Terraform will perform the actions described above.\n  Only 'yes' will be accepted to approve.\n  Enter a value: yes\ndigitalocean_kubernetes_cluster.test: Modifying... [id=49fd0517-a4a5-41e8-997d-1412c081e000]\ndigitalocean_kubernetes_cluster.test: Modifications complete after 2s [id=49fd0517-a4a5-41e8-997d-1412c081e000]\nApply complete! Resources: 0 added, 1 changed, 0 destroyed.\n\n```\n\nThe change was quickly applied to your cluster, and if you double check in the portal again, you can see that, indeed, the auto-upgrade option is now enabled.\n\n![auto-upgrade enabled](/blog-images/9730642cc35deb2d6c3b63888fe4d71e.png)\n\n### Destroying Kubernetes\n\nIf you no longer want your Kubernetes cluster, you can destroy it just as easily as you deployed it. All you need to do is execute **terraform destroy**.\n\n```yaml\n\n# terraform destroy\n\ndigitalocean_kubernetes_cluster.test: Refreshing state... [id=49fd0517-a4a5-41e8-997d-1412c081e000]\nTerraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols:\n  - destroy\nTerraform will perform the following actions:\n  # digitalocean_kubernetes_cluster.test will be destroyed\n  - resource \"digitalocean_kubernetes_cluster\" \"test\" {\n      - auto_upgrade   = true -> null\n      - cluster_subnet = \"10.244.0.0/16\" -> null\n      - created_at     = \"2022-07-24 06:15:34 +0000 UTC\" -> null\n      - endpoint       = \"https://49fd0517-a4a5-41e8-997d-1412c081e000.k8s.ondigitalocean.com\" -> null\n      - ha             = false -> null\n      - id             = \"49fd0517-a4a5-41e8-997d-1412c081e000\" -> null\n      - kube_config    = (sensitive value)\n      - name           = \"test-cluster\" -> null\n      - region         = \"nyc1\" -> null\n      - service_subnet = \"10.245.0.0/16\" -> null\n      - status         = \"running\" -> null\n      - surge_upgrade  = true -> null\n      - tags           = [] -> null\n      - updated_at     = \"2022-07-24 06:37:27 +0000 UTC\" -> null\n      - urn            = \"do:kubernetes:49fd0517-a4a5-41e8-997d-1412c081e000\" -> null\n      - version        = \"1.22.11-do.0\" -> null\n      - vpc_uuid       = \"877cc187-97ad-426c-9301-079e3683d351\" -> null\n      - maintenance_policy {\n          - day        = \"any\" -> null\n          - duration   = \"4h0m0s\" -> null\n          - start_time = \"10:00\" -> null\n        }\n      - node_pool {\n          - actual_node_count = 3 -> null\n          - auto_scale        = false -> null\n          - id                = \"8df9b48c-329d-41f5-899e-b7b896e28e15\" -> null\n          - labels            = {} -> null\n          - max_nodes         = 0 -> null\n          - min_nodes         = 0 -> null\n          - name              = \"worker-pool\" -> null\n          - node_count        = 3 -> null\n          - nodes             = [\n              - {\n                  - created_at = \"2022-07-24 06:15:34 +0000 UTC\"\n                  - droplet_id = \"309670716\"\n                  - id         = \"b82aeb19-78d8-4571-91e6-a0c2cffdb1db\"\n                  - name       = \"worker-pool-c1766\"\n                  - status     = \"running\"\n                  - updated_at = \"2022-07-24 06:19:09 +0000 UTC\"\n                },\n              - {\n                  - created_at = \"2022-07-24 06:15:34 +0000 UTC\"\n                  - droplet_id = \"309670715\"\n                  - id         = \"6b0d1ecf-4e48-427b-99a9-0e153056238d\"\n                  - name       = \"worker-pool-c176t\"\n                  - status     = \"running\"\n                  - updated_at = \"2022-07-24 06:18:27 +0000 UTC\"\n                },\n              - {\n                  - created_at = \"2022-07-24 06:15:34 +0000 UTC\"\n                  - droplet_id = \"309670717\"\n                  - id         = \"5ea0e536-96aa-4171-8602-dc0ab19e9888\"\n                  - name       = \"worker-pool-c176l\"\n                  - status     = \"running\"\n                  - updated_at = \"2022-07-24 06:18:27 +0000 UTC\"\n                },\n            ] -> null\n          - size              = \"s-2vcpu-2gb\" -> null\n          - tags              = [] -> null\n        }\n    }\nPlan: 0 to add, 0 to change, 1 to destroy.\nDo you really want to destroy all resources?\n  Terraform will destroy all your managed infrastructure, as shown above.\n  There is no undo. Only 'yes' will be accepted to confirm.\n  Enter a value: yes\ndigitalocean_kubernetes_cluster.test: Destroying... [id=49fd0517-a4a5-41e8-997d-1412c081e000]\ndigitalocean_kubernetes_cluster.test: Destruction complete after 1s\nDestroy complete! Resources: 1 destroyed.\n\n```\n\nJust like that, the cluster is gone.\n\n### Summary\n\nAnd there you have it. That's how you can manage Kubernetes clusters with Terraform. You used DigitalOcean Kubernetes for this purpose, but as mentioned before, the process will be exactly the same for other providers. You'll just need to initiate different providers in **provider.tf** and then adjust the Kubernetes resource definition in **main.tf**. It's best to follow Terraform documentation for that. You'll find examples and argument references for major cloud providers.\n\nManaging infrastructure with Terraform definitely helps you save time, but did you know that you can also easily spin up an environment on [Release](https://release.com/) directly from your docker-compose file? [Give it a shot here](https://release.com/), and if you want to expand your Terraform knowledge further, take a look at our [post about for_each](https://release.com/blog/terraforms-for-each-examples).\n",
          "code": "var Component=(()=>{var u=Object.create;var a=Object.defineProperty;var h=Object.getOwnPropertyDescriptor;var p=Object.getOwnPropertyNames;var g=Object.getPrototypeOf,m=Object.prototype.hasOwnProperty;var f=(r,e)=>()=>(e||r((e={exports:{}}).exports,e),e.exports),y=(r,e)=>{for(var t in e)a(r,t,{get:e[t],enumerable:!0})},l=(r,e,t,i)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let o of p(e))!m.call(r,o)&&o!==t&&a(r,o,{get:()=>e[o],enumerable:!(i=h(e,o))||i.enumerable});return r};var b=(r,e,t)=>(t=r!=null?u(g(r)):{},l(e||!r||!r.__esModule?a(t,\"default\",{value:r,enumerable:!0}):t,r)),w=r=>l(a({},\"__esModule\",{value:!0}),r);var d=f((K,s)=>{s.exports=_jsx_runtime});var T={};y(T,{default:()=>v,frontmatter:()=>k});var n=b(d()),k={title:\"Terraform Kubernetes Deployment: A Detailed Walkthrough\",summary:\" Did you know that you can use Terraform for deployment of your Kubernetes clusters? Learn how and why to do it here.\",publishDate:\"Sat Aug 06 2022 22:18:19 GMT+0000 (Coordinated Universal Time)\",author:\"regis-wilson\",readingTime:6,categories:[\"kubernetes\",\"platform-engineering\"],mainImage:\"/blog-images/93cadc79db3755d6a026b27fe882105c.jpg\",imageAlt:\"Terraform Kubernetes Deployment: A Detailed Walkthrough\",showCTA:!0,ctaCopy:\"Looking to streamline Kubernetes deployment workflows? Try Release.com for ephemeral environments that mirror production, enabling faster testing and deployment cycles.\",ctaLink:\"https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=terraform-kubernetes-deployment-a-detailed-walkthrough\",relatedPosts:[\"\"],ogImage:\"/blog-images/93cadc79db3755d6a026b27fe882105c.jpg\",excerpt:\" Did you know that you can use Terraform for deployment of your Kubernetes clusters? Learn how and why to do it here.\",tags:[\"kubernetes\",\"platform-engineering\"],ctaButton:\"Try Release for Free\"};function c(r){let e=Object.assign({p:\"p\",h3:\"h3\",a:\"a\",span:\"span\",img:\"img\",strong:\"strong\",pre:\"pre\",code:\"code\"},r.components);return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(e.p,{children:\"Terraform and Kubernetes are two of the most popular tools in their categories. Terraform is widely adopted as the tool of choice for infrastructure as code, and Kubernetes is number one when it comes to orchestrating containers. Is it possible to combine both? Sure! You can use Terraform to deploy your Kubernetes clusters. It's actually quite common, and it lets you deploy Kubernetes just like the rest of your infrastructure. In this post, you'll learn how to do it.\"}),`\n`,(0,n.jsxs)(e.h3,{id:\"terraform--kubernetes-how-and-why\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#terraform--kubernetes-how-and-why\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Terraform + Kubernetes: How and Why?\"]}),`\n`,(0,n.jsx)(e.p,{children:\"We have two main questions to answer here. How can you deploy Kubernetes with Terraform, and why would you do that? Let's start with the latter.\"}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/a82632ed72605aab10dcbb180de0d4d1.png\",alt:\"ApplicationDescription automatically generated with medium confidence\"})}),`\n`,(0,n.jsx)(e.p,{children:`The answer doesn't differ from \"Why would you deploy anything with Terraform?\" From that perspective, there's nothing special about Kubernetes, and you get the same benefit by using Terraform to deploy it as with any other infrastructure. You get automation, infrastructure versioning, reliability, and even the ability to perform infrastructure security scanning.`}),`\n`,(0,n.jsx)(e.p,{children:\"As for how, the answer is actually similar. You can deploy Kubernetes with Terraform just like any other infrastructure. Meaning, you first need to find a Kubernetes resource definition for Terraform (we'll show you that shortly), adjust some parameters for your needs, add it to your Terraform code, and you're done. And just like with any other resource, Terraform will be able to track changes to your cluster and update its configuration after you make changes to the code.\"}),`\n`,(0,n.jsxs)(e.h3,{id:\"deploying-kubernetes-first-steps\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#deploying-kubernetes-first-steps\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Deploying Kubernetes: First Steps\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"Enough theory. Let's see how it works in practice. First, you need to find a Terraform provider for your cloud. If you want to deploy Kubernetes on \",(0,n.jsx)(e.a,{href:\"https://www.digitalocean.com/\",children:\"DigitalOcean\"}),\", you'd need to follow \",(0,n.jsx)(e.a,{href:\"https://registry.terraform.io/providers/digitalocean/digitalocean/latest/docs/resources/kubernetes_cluster\",children:\"this documentation\"}),\". For \",(0,n.jsx)(e.a,{href:\"https://azure.microsoft.com/en-us/\",children:\"Microsoft Azure\"}),\", you'd need to head \",(0,n.jsx)(e.a,{href:\"https://registry.terraform.io/providers/hashicorp/azurerm/latest/docs/resources/kubernetes_cluster\",children:\"here\"}),\" for details. And for \",(0,n.jsx)(e.a,{href:\"https://cloud.google.com/\",children:\"Google Cloud\"}),\", you need to check \",(0,n.jsx)(e.a,{href:\"https://registry.terraform.io/providers/hashicorp/google/latest/docs/resources/container_cluster\",children:\"here\"}),\". These are just a few examples. But no matter which cloud provider you're using, the general approach will be the same. For today's example, we'll use DigitalOcean.\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"To start from nothing, in the simplest scenario, you need to create two files named \",(0,n.jsx)(e.strong,{children:\"provider.tf\"}),\" and \",(0,n.jsx)(e.strong,{children:\"main.tf\"}),\". You could do it all in one file, but it's a good practice to separate providers and main resource definitions. In the code below, you can define your DigitalOcean provider for Terraform and pass your DigitalOcean token:\"]}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-yaml\",children:`\nterraform {\n \\xA0required_providers {\n \\xA0 \\xA0digitalocean = {\n \\xA0 \\xA0 \\xA0source = \"digitalocean/digitalocean\"\n \\xA0 \\xA0 \\xA0version = \"~> 2.0\"\n \\xA0 \\xA0}\n \\xA0}\n}\nvariable \"do_token\" {\n \\xA0default = \"[replace_with_your_token]\"\n}\n\n# Configure the DigitalOcean Provider\n\nprovider \"digitalocean\" {\n \\xA0token = var.do_token\n}\n\n`})}),`\n`,(0,n.jsxs)(e.p,{children:[\"In \",(0,n.jsx)(e.strong,{children:\"main.tf\"}),\" you can now define your Kubernetes.\"]}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-yaml\",children:`resource \"digitalocean_kubernetes_cluster\" \"test\" {\nname \\xA0 = \"test_cluster\"\nregion = \"nyc1\"\nversion = \"1.22.11-do.0\"\nnode_pool {\nname \\xA0 \\xA0 \\xA0 = \"worker-pool\"\nsize \\xA0 \\xA0 \\xA0 = \"s-2vcpu-2gb\"\nnode_count = 3\n}\n}\n`})}),`\n`,(0,n.jsxs)(e.p,{children:[\"Now that you have your Terraform files prepared, you need three things. First, you need to initiate the DigitalOcean provider. You can do that with \",(0,n.jsx)(e.strong,{children:\"terraform init\"}),\".\"]}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-yaml\",children:`\n# terraform init\n\nInitializing the backend...\nInitializing provider plugins...\n- Finding digitalocean/digitalocean versions matching \"~> 2.0\"...\n- Installing digitalocean/digitalocean v2.21.0...\n- Installed digitalocean/digitalocean v2.21.0 (signed by a HashiCorp partner, key ID F82037E524B9C0E8)\nPartner and community providers are signed by their developers.\nIf you'd like to know more about provider signing, you can read about it here:\nhttps://www.terraform.io/docs/cli/plugins/signing.html\nTerraform has created a lock file .terraform.lock.hcl to record the provider\nselections it made above. Include this file in your version control repository\nso that Terraform can guarantee to make the same selections by default when\nyou run \"terraform init\" in the future.\nTerraform has been successfully initialized!\nYou may now begin working with Terraform. Try running \"terraform plan\" to see\nany changes that are required for your infrastructure. All Terraform commands\nshould now work.\nIf you ever set or change modules or backend configuration for Terraform,\nrerun this command to reinitialize your working directory. If you forget, other\ncommands will detect it and remind you to do so if necessary.\n\n`})}),`\n`,(0,n.jsxs)(e.p,{children:[\"Then you can run your \",(0,n.jsx)(e.strong,{children:\"terraform plan\"}),\", which will show you planned changes to the infrastructure (which in this case should be creating a new Kubernetes cluster).\"]}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-yaml\",children:`\n# terraform plan\n\nTerraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following\nsymbols:\n \\xA0+ create\nTerraform will perform the following actions:\n \\xA0# digitalocean_kubernetes_cluster.test will be created\n \\xA0+ resource \"digitalocean_kubernetes_cluster\" \"test\" {\n \\xA0 \\xA0 \\xA0+ cluster_subnet = (known after apply)\n \\xA0 \\xA0 \\xA0+ created_at \\xA0 \\xA0 = (known after apply)\n \\xA0 \\xA0 \\xA0+ endpoint \\xA0 \\xA0 \\xA0 = (known after apply)\n \\xA0 \\xA0 \\xA0+ ha \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 = false\n \\xA0 \\xA0 \\xA0+ id \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 = (known after apply)\n \\xA0 \\xA0 \\xA0+ ipv4_address \\xA0 = (known after apply)\n \\xA0 \\xA0 \\xA0+ kube_config \\xA0 \\xA0= (sensitive value)\n \\xA0 \\xA0 \\xA0+ name \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 = \"test-cluster\"\n \\xA0 \\xA0 \\xA0+ region \\xA0 \\xA0 \\xA0 \\xA0 = \"nyc1\"\n \\xA0 \\xA0 \\xA0+ service_subnet = (known after apply)\n \\xA0 \\xA0 \\xA0+ status \\xA0 \\xA0 \\xA0 \\xA0 = (known after apply)\n \\xA0 \\xA0 \\xA0+ surge_upgrade \\xA0= true\n \\xA0 \\xA0 \\xA0+ updated_at \\xA0 \\xA0 = (known after apply)\n \\xA0 \\xA0 \\xA0+ urn \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0= (known after apply)\n \\xA0 \\xA0 \\xA0+ version \\xA0 \\xA0 \\xA0 \\xA0= \"1.22.11-do.0\"\n \\xA0 \\xA0 \\xA0+ vpc_uuid \\xA0 \\xA0 \\xA0 = (known after apply)\n \\xA0 \\xA0 \\xA0+ maintenance_policy {\n \\xA0 \\xA0 \\xA0 \\xA0 \\xA0+ day \\xA0 \\xA0 \\xA0 \\xA0= (known after apply)\n \\xA0 \\xA0 \\xA0 \\xA0 \\xA0+ duration \\xA0 = (known after apply)\n \\xA0 \\xA0 \\xA0 \\xA0 \\xA0+ start_time = (known after apply)\n \\xA0 \\xA0 \\xA0 \\xA0}\n \\xA0 \\xA0 \\xA0+ node_pool {\n \\xA0 \\xA0 \\xA0 \\xA0 \\xA0+ actual_node_count = (known after apply)\n \\xA0 \\xA0 \\xA0 \\xA0 \\xA0+ auto_scale \\xA0 \\xA0 \\xA0 \\xA0= false\n \\xA0 \\xA0 \\xA0 \\xA0 \\xA0+ id \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0= (known after apply)\n \\xA0 \\xA0 \\xA0 \\xA0 \\xA0+ name \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0= \"worker-pool\"\n \\xA0 \\xA0 \\xA0 \\xA0 \\xA0+ node_count \\xA0 \\xA0 \\xA0 \\xA0= 3\n \\xA0 \\xA0 \\xA0 \\xA0 \\xA0+ nodes \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 = (known after apply)\n \\xA0 \\xA0 \\xA0 \\xA0 \\xA0+ size \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0= \"s-2vcpu-2gb\"\n \\xA0 \\xA0 \\xA0 \\xA0}\n \\xA0 \\xA0}\nPlan: 1 to add, 0 to change, 0 to destroy.\n\n`})}),`\n`,(0,n.jsxs)(e.p,{children:[\"The plan looks good. One resource will be added, and that's your Kubernetes cluster, so you can go ahead and apply the changes with \",(0,n.jsx)(e.strong,{children:\"terraform apply\"}),\".\"]}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-yaml\",children:`\n# terraform apply\n\n(...)\nPlan: 1 to add, 0 to change, 0 to destroy.\nDo you want to perform these actions?\n \\xA0Terraform will perform the actions described above.\n \\xA0Only 'yes' will be accepted to approve.\n \\xA0Enter a value: yes\ndigitalocean_kubernetes_cluster.test: Creating...\ndigitalocean_kubernetes_cluster.test: Still creating... [10s elapsed]\n(...)\ndigitalocean_kubernetes_cluster.test: Still creating... [7m10s elapsed]\ndigitalocean_kubernetes_cluster.test: Creation complete after 7m16s [id=49fd0517-a4a5-41e8-997d-1412c081e000]\nApply complete! Resources: 1 added, 0 changed, 0 destroyed.\n\n`})}),`\n`,(0,n.jsx)(e.p,{children:\"If you now head to your DigitalOcean portal to validate, you can indeed see it there.\"}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/4ae1114850cfa16a93e987103c7eb5e0.png\",alt:\"DigitalOcean portal\"})}),`\n`,(0,n.jsx)(e.p,{children:\"And that's it! That's how you deploy Kubernetes with Terraform.\"}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/14771b0c5e49d3c50009a99ebfdcd091.jpeg\",alt:\"A picture containing device, control panelDescription automatically generated\"})}),`\n`,(0,n.jsxs)(e.h3,{id:\"deploying-kubernetes-next-steps\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#deploying-kubernetes-next-steps\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Deploying Kubernetes: Next Steps\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"Now that you know how it works in general, there are a few things that you need to learn next. First, all you've done is deploy basic, minimal Kubernetes. In more realistic scenarios, you'll probably want to parametrize more options for your Kubernetes. This, however, will highly depend on what you actually need. If you know what you need, you can head to the Terraform documentation and check \",(0,n.jsx)(e.a,{href:\"https://registry.terraform.io/providers/digitalocean/digitalocean/latest/docs/resources/kubernetes_cluster#argument-reference\",children:(0,n.jsx)(e.strong,{children:\"argument reference\"})}),\" for your Kubernetes resource. Find what you need and add it to your code.\"]}),`\n`,(0,n.jsx)(e.p,{children:\"For example, if you'd like your Kubernetes cluster to automatically upgrade, you can find the following in the documentation:\"}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/0a5e2c452dafd796806726969310f5a2.png\",alt:\"Auto upgrade\"})}),`\n`,(0,n.jsxs)(e.p,{children:[\"To make your freshly deployed cluster automatically upgrade, you just need to add the following to your Kubernetes resource definition in \",(0,n.jsx)(e.strong,{children:\"main.tf\"}),\" as follows:\"]}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-yaml\",children:`resource \"digitalocean_kubernetes_cluster\" \"test\" {\nname \\xA0 = \"test-cluster\"\nregion = \"nyc1\"\nversion = \"1.22.11-do.0\"\nauto_upgrade = true\nnode_pool {\nname \\xA0 \\xA0 \\xA0 = \"worker-pool\"\nsize \\xA0 \\xA0 \\xA0 = \"s-2vcpu-2gb\"\nnode_count = 3\n}\n}\n`})}),`\n`,(0,n.jsx)(e.p,{children:\"But you're not there yet. You can quickly see in the DigitalOcean portal that the cluster currently does not automatically upgrade.\"}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/a2dfca6ce7870345b1829ffe4f4a457e.png\",alt:\"auto upgrade in portal\"})}),`\n`,(0,n.jsxs)(e.p,{children:[\"Automatic upgrades are disabled now, so you can run \",(0,n.jsx)(e.strong,{children:\"terraform plan\"}),\" again to check what Terraform will try to do.\"]}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-yaml\",children:`# terraform plan\n\nTerraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols: \\xA0~ update in-place\nTerraform will perform the following actions:\n  \\xA0# digitalocean_kubernetes_cluster.test will be updated in-place\n  ~ resource \"digitalocean_kubernetes_cluster\" \"test\" {\n  ~ auto_upgrade \\xA0 = false -> true\n  id \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 = \"49fd0517-a4a5-41e8-997d-1412c081e000\"\n  name \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 = \"test-cluster\"\n  tags \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 = []\n  # (13 unchanged attributes hidden)\n  # (2 unchanged blocks hidden)\n  }\nPlan: 0 to add, 1 to change, 0 to destroy\n`})}),`\n`,(0,n.jsx)(e.p,{children:\"As expected, Terraform will now try to update your cluster in place and add an auto-upgrade option to it. Let's go ahead and apply that change.\"}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-yaml\",children:`\n# terraform apply\n\n(...)\nPlan: 0 to add, 1 to change, 0 to destroy.\nDo you want to perform these actions?\n \\xA0Terraform will perform the actions described above.\n \\xA0Only 'yes' will be accepted to approve.\n \\xA0Enter a value: yes\ndigitalocean_kubernetes_cluster.test: Modifying... [id=49fd0517-a4a5-41e8-997d-1412c081e000]\ndigitalocean_kubernetes_cluster.test: Modifications complete after 2s [id=49fd0517-a4a5-41e8-997d-1412c081e000]\nApply complete! Resources: 0 added, 1 changed, 0 destroyed.\n\n`})}),`\n`,(0,n.jsx)(e.p,{children:\"The change was quickly applied to your cluster, and if you double check in the portal again, you can see that, indeed, the auto-upgrade option is now enabled.\"}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/9730642cc35deb2d6c3b63888fe4d71e.png\",alt:\"auto-upgrade enabled\"})}),`\n`,(0,n.jsxs)(e.h3,{id:\"destroying-kubernetes\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#destroying-kubernetes\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Destroying Kubernetes\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"If you no longer want your Kubernetes cluster, you can destroy it just as easily as you deployed it. All you need to do is execute \",(0,n.jsx)(e.strong,{children:\"terraform destroy\"}),\".\"]}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-yaml\",children:`\n# terraform destroy\n\ndigitalocean_kubernetes_cluster.test: Refreshing state... [id=49fd0517-a4a5-41e8-997d-1412c081e000]\nTerraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols:\n \\xA0- destroy\nTerraform will perform the following actions:\n \\xA0# digitalocean_kubernetes_cluster.test will be destroyed\n \\xA0- resource \"digitalocean_kubernetes_cluster\" \"test\" {\n \\xA0 \\xA0 \\xA0- auto_upgrade \\xA0 = true -> null\n \\xA0 \\xA0 \\xA0- cluster_subnet = \"10.244.0.0/16\" -> null\n \\xA0 \\xA0 \\xA0- created_at \\xA0 \\xA0 = \"2022-07-24 06:15:34 +0000 UTC\" -> null\n \\xA0 \\xA0 \\xA0- endpoint \\xA0 \\xA0 \\xA0 = \"https://49fd0517-a4a5-41e8-997d-1412c081e000.k8s.ondigitalocean.com\" -> null\n \\xA0 \\xA0 \\xA0- ha \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 = false -> null\n \\xA0 \\xA0 \\xA0- id \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 = \"49fd0517-a4a5-41e8-997d-1412c081e000\" -> null\n \\xA0 \\xA0 \\xA0- kube_config \\xA0 \\xA0= (sensitive value)\n \\xA0 \\xA0 \\xA0- name \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 = \"test-cluster\" -> null\n \\xA0 \\xA0 \\xA0- region \\xA0 \\xA0 \\xA0 \\xA0 = \"nyc1\" -> null\n \\xA0 \\xA0 \\xA0- service_subnet = \"10.245.0.0/16\" -> null\n \\xA0 \\xA0 \\xA0- status \\xA0 \\xA0 \\xA0 \\xA0 = \"running\" -> null\n \\xA0 \\xA0 \\xA0- surge_upgrade \\xA0= true -> null\n \\xA0 \\xA0 \\xA0- tags \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 = [] -> null\n \\xA0 \\xA0 \\xA0- updated_at \\xA0 \\xA0 = \"2022-07-24 06:37:27 +0000 UTC\" -> null\n \\xA0 \\xA0 \\xA0- urn \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0= \"do:kubernetes:49fd0517-a4a5-41e8-997d-1412c081e000\" -> null\n \\xA0 \\xA0 \\xA0- version \\xA0 \\xA0 \\xA0 \\xA0= \"1.22.11-do.0\" -> null\n \\xA0 \\xA0 \\xA0- vpc_uuid \\xA0 \\xA0 \\xA0 = \"877cc187-97ad-426c-9301-079e3683d351\" -> null\n \\xA0 \\xA0 \\xA0- maintenance_policy {\n \\xA0 \\xA0 \\xA0 \\xA0 \\xA0- day \\xA0 \\xA0 \\xA0 \\xA0= \"any\" -> null\n \\xA0 \\xA0 \\xA0 \\xA0 \\xA0- duration \\xA0 = \"4h0m0s\" -> null\n \\xA0 \\xA0 \\xA0 \\xA0 \\xA0- start_time = \"10:00\" -> null\n \\xA0 \\xA0 \\xA0 \\xA0}\n \\xA0 \\xA0 \\xA0- node_pool {\n \\xA0 \\xA0 \\xA0 \\xA0 \\xA0- actual_node_count = 3 -> null\n \\xA0 \\xA0 \\xA0 \\xA0 \\xA0- auto_scale \\xA0 \\xA0 \\xA0 \\xA0= false -> null\n \\xA0 \\xA0 \\xA0 \\xA0 \\xA0- id \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0= \"8df9b48c-329d-41f5-899e-b7b896e28e15\" -> null\n \\xA0 \\xA0 \\xA0 \\xA0 \\xA0- labels \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0= {} -> null\n \\xA0 \\xA0 \\xA0 \\xA0 \\xA0- max_nodes \\xA0 \\xA0 \\xA0 \\xA0 = 0 -> null\n \\xA0 \\xA0 \\xA0 \\xA0 \\xA0- min_nodes \\xA0 \\xA0 \\xA0 \\xA0 = 0 -> null\n \\xA0 \\xA0 \\xA0 \\xA0 \\xA0- name \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0= \"worker-pool\" -> null\n \\xA0 \\xA0 \\xA0 \\xA0 \\xA0- node_count \\xA0 \\xA0 \\xA0 \\xA0= 3 -> null\n \\xA0 \\xA0 \\xA0 \\xA0 \\xA0- nodes \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 = [\n \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0- {\n \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0- created_at = \"2022-07-24 06:15:34 +0000 UTC\"\n \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0- droplet_id = \"309670716\"\n \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0- id \\xA0 \\xA0 \\xA0 \\xA0 = \"b82aeb19-78d8-4571-91e6-a0c2cffdb1db\"\n \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0- name \\xA0 \\xA0 \\xA0 = \"worker-pool-c1766\"\n \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0- status \\xA0 \\xA0 = \"running\"\n \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0- updated_at = \"2022-07-24 06:19:09 +0000 UTC\"\n \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0},\n \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0- {\n \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0- created_at = \"2022-07-24 06:15:34 +0000 UTC\"\n \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0- droplet_id = \"309670715\"\n \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0- id \\xA0 \\xA0 \\xA0 \\xA0 = \"6b0d1ecf-4e48-427b-99a9-0e153056238d\"\n \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0- name \\xA0 \\xA0 \\xA0 = \"worker-pool-c176t\"\n \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0- status \\xA0 \\xA0 = \"running\"\n \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0- updated_at = \"2022-07-24 06:18:27 +0000 UTC\"\n \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0},\n \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0- {\n \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0- created_at = \"2022-07-24 06:15:34 +0000 UTC\"\n \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0- droplet_id = \"309670717\"\n \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0- id \\xA0 \\xA0 \\xA0 \\xA0 = \"5ea0e536-96aa-4171-8602-dc0ab19e9888\"\n \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0- name \\xA0 \\xA0 \\xA0 = \"worker-pool-c176l\"\n \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0- status \\xA0 \\xA0 = \"running\"\n \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0- updated_at = \"2022-07-24 06:18:27 +0000 UTC\"\n \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0},\n \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0] -> null\n \\xA0 \\xA0 \\xA0 \\xA0 \\xA0- size \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0= \"s-2vcpu-2gb\" -> null\n \\xA0 \\xA0 \\xA0 \\xA0 \\xA0- tags \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0= [] -> null\n \\xA0 \\xA0 \\xA0 \\xA0}\n \\xA0 \\xA0}\nPlan: 0 to add, 0 to change, 1 to destroy.\nDo you really want to destroy all resources?\n \\xA0Terraform will destroy all your managed infrastructure, as shown above.\n \\xA0There is no undo. Only 'yes' will be accepted to confirm.\n \\xA0Enter a value: yes\ndigitalocean_kubernetes_cluster.test: Destroying... [id=49fd0517-a4a5-41e8-997d-1412c081e000]\ndigitalocean_kubernetes_cluster.test: Destruction complete after 1s\nDestroy complete! Resources: 1 destroyed.\n\n`})}),`\n`,(0,n.jsx)(e.p,{children:\"Just like that, the cluster is gone.\"}),`\n`,(0,n.jsxs)(e.h3,{id:\"summary\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#summary\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Summary\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"And there you have it. That's how you can manage Kubernetes clusters with Terraform. You used DigitalOcean Kubernetes for this purpose, but as mentioned before, the process will be exactly the same for other providers. You'll just need to initiate different providers in \",(0,n.jsx)(e.strong,{children:\"provider.tf\"}),\" and then adjust the Kubernetes resource definition in \",(0,n.jsx)(e.strong,{children:\"main.tf\"}),\". It's best to follow Terraform documentation for that. You'll find examples and argument references for major cloud providers.\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"Managing infrastructure with Terraform definitely helps you save time, but did you know that you can also easily spin up an environment on \",(0,n.jsx)(e.a,{href:\"https://release.com/\",children:\"Release\"}),\" directly from your docker-compose file? \",(0,n.jsx)(e.a,{href:\"https://release.com/\",children:\"Give it a shot here\"}),\", and if you want to expand your Terraform knowledge further, take a look at our \",(0,n.jsx)(e.a,{href:\"https://release.com/blog/terraforms-for-each-examples\",children:\"post about for_each\"}),\".\"]})]})}function _(r={}){let{wrapper:e}=r.components||{};return e?(0,n.jsx)(e,Object.assign({},r,{children:(0,n.jsx)(c,r)})):c(r)}var v=_;return w(T);})();\n;return Component;"
        },
        "_id": "blog/posts/terraform-kubernetes-deployment-a-detailed-walkthrough.mdx",
        "_raw": {
          "sourceFilePath": "blog/posts/terraform-kubernetes-deployment-a-detailed-walkthrough.mdx",
          "sourceFileName": "terraform-kubernetes-deployment-a-detailed-walkthrough.mdx",
          "sourceFileDir": "blog/posts",
          "contentType": "mdx",
          "flattenedPath": "blog/posts/terraform-kubernetes-deployment-a-detailed-walkthrough"
        },
        "type": "BlogPost",
        "computedSlug": "terraform-kubernetes-deployment-a-detailed-walkthrough"
      },
      "documentHash": "1739393595028",
      "hasWarnings": false,
      "documentTypeName": "BlogPost"
    },
    "blog/posts/terraform-vs-kubernetes-whats-the-difference-and-why-it-matters.mdx": {
      "document": {
        "title": "Terraform vs Kubernetes: What's the Difference and Why It Matters",
        "summary": "We'll look at Terraform and Kubernetes and how you can use them separately or together to automate and scale your cloud",
        "publishDate": "Tue Feb 21 2023 17:30:53 GMT+0000 (Coordinated Universal Time)",
        "author": "eric-goebelbecker",
        "readingTime": 7,
        "categories": [
          "platform-engineering",
          "kubernetes"
        ],
        "mainImage": "/blog-images/814523c36864bffb33d1170c4b4682ff.jpg",
        "imageAlt": "Terraform vs Kubernetes",
        "showCTA": true,
        "ctaCopy": "Looking to automate and scale your cloud environments seamlessly like Terraform and Kubernetes? Try Release for on-demand environment provisioning and streamline workflows.",
        "ctaLink": "https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=terraform-vs-kubernetes-whats-the-difference-and-why-it-matters",
        "relatedPosts": [
          ""
        ],
        "ogImage": "/blog-images/814523c36864bffb33d1170c4b4682ff.jpg",
        "excerpt": "We'll look at Terraform and Kubernetes and how you can use them separately or together to automate and scale your cloud",
        "tags": [
          "platform-engineering",
          "kubernetes"
        ],
        "ctaButton": "Try Release for Free",
        "body": {
          "raw": "\nTerraform vs. Kubernetes. Which tool is better? Which one do you need? \n\nSometimes people confuse these two cloud automation tools, but it turns out they're not the same thing, and they're not in competition with each other. As a matter of fact, they work very well together. [Terraform](https://www.terraform.io) is an [infrastructure as code](https://en.wikipedia.org/wiki/Infrastructure_as_code) tool, while [Kubernetes](https://kubernetes.io) is a [container orchestration](https://en.wikipedia.org/wiki/Orchestration_(computing) system. Terraform can help you build and manage reproducible cloud infrastructure. Kubernetes can help you create more robust cloud applications. \n\nThis post will explain what Terraform and Kubernetes are and how you can use them separately or together to automate and scale your cloud environments. \n\n![](/blog-images/9e8aa21a7347960d68600df2f6c623aa.png)\n\n### Terraform\n\nTerraform is [Hashicorp's](https://www.hashicorp.com) infrastructure as code automation tool for provisioning and configuring cloud services. It has a consistent command-line interface (CLI) to more than 300 different cloud APIs. You describe your cloud environments in a declarative configuration syntax and then execute Terraform [plans](https://www.terraform.io/docs/glossary#plan-verb) to build them. Hashicorp offers both open-source and licensed versions of Terraform. \n\nWith Terraform you: \n\n- Increase your infrastructure automation and eliminate preventable mistakes.\n- Use the same workflow to build environments in different public clouds\n- Manage your infrastructure like code with configuration files, pull requests, and reviews\n- Have a powerful tool for ensuring that your development, staging, and production environment are consistent.\n\nTerraform uses Hashicorp Configuration Language (HCL) to describe infrastructure resources. The CLI reads these files and executes your plans using the cloud provider's native APIS. \n\nYou can use Terraform to provision a wide variety of different tools via a robust plugin system. Plugins are called \"providers.\" Hashicorp supports hundreds of official providers, [maintains an official registry](https://registry.terraform.io/browse/providers), and has an [API](https://www.terraform.io/cloud-docs/api-docs/private-registry/providers) so you can write your own. They even have a Kubernetes provider that we'll discuss later. \n\n### Terraform HCL\n\nHCL is a user-friendly language that is easy to write and easy to read. If you understand the underlying infrastructure that you're using Terraform to provision, you can figure out HCL rather quickly. But it's not a restricted or limited programming language. It has variables, data types, and control flow. \n\nThis is how you declare a string variable in HCL: \n\n```hcl\n\nvariable \"username\" {\n  default = \"webmaster\"\n}\n\n```\n\nThis is an example of a list: \n\n```hcl\n\nvariable \"servers\" {\n    default = [\"webserver\", \"databaseserver\", \"backupserver\"]\n}\n\n```\n\nHere's a for loop that traverses the list and outputs each name in upper case: \n\n```hcl\n\nvariable \"servers\" {\n    default = [\"webserver\", \"databaseserver\", \"backupserver\"]\n}\n\noutput \"forloop_terraform\" {\n   value = [\n      for name in var.servers : upper(name)\n   ]\n}\n\n```\n\nAdditionally, HCL has many powerful features, like maps, functions, and code libraries. \n\n### Terraform Examples\n\nTerraform is cloud-agnostic, but unfortunately, that doesn't mean you can write one HCL file and use it with more than one cloud. Each cloud provider had distinct APIs and very different configuration models. \n\nHere is code from the [Terraform tutorial](https://learn.hashicorp.com/terraform?utm_source=terraform_io) for provisioning an Amazon Web Service (AWS) EC2 instance. \n\n```hcl\n\nterraform {\n  required_providers {\n    aws = {\n      source  = \"hashicorp/aws\"\n      version = \"~> 3.27\"\n    }\n  }\n\n  required_version = \">= 1.1.0\"\n}\n\nprovider \"aws\" {\n  profile = \"default\"\n  region  = \"us-west-2\"\n}\n\nresource \"aws_instance\" \"app_server\" {\n  ami           = \"ami-830c94e3\"\n  instance_type = \"t2.micro\"\n\n  tags = {\n    Name = \"ExampleAppServerInstance\"\n  }\n}\n\n```\n\nHere is a similar example for GCP: \n\n```hcl\n\nprovider \"google\" {\n  project = \"terraform-examples-gcloud\"\n  region  = \"us-east1\"\n}\n\nresource \"google_compute_instance\" \"example\" {\n  name          = \"example\"\n  machine_type  = \"f1-micro\"\n  zone          = \"us-east1-b\"\n  \n  boot_disk {\n    initialize_params {\n      image = \"ubuntu-1604-lts\"\n    }\n  }\n  \n  network_interface {\n    network = \"default\"\n\n    access_config {\n      // Ephemeral IP\n    }\n  }\n  \n  tags = [\"terraform-example\"]\n}\n\n```\n\nYou can separate your code into modules and separate variable declarations from code. This helps abstract out the difference between different cloud platforms. \n\n![](/blog-images/078c35869988e1041da4242cd3632216.png)\n\n### Kubernetes\n\nKubernetes ([K8s](https://release.com/blog/why-kubernetes-is-so-hard)) is an open-source container orchestration system. Like Terraform, it's an automation tool, but for containerized applications, not infrastructure. K8s automates deploying, scaling, and managing applications that run in containers. Usually, those containers are [Docker](https://www.docker.com). \n\nGoogle created K8s, but it is not limited to Google infrastructure. You can run K8 on-premises, on a cluster you build and manage yourself in the cloud, or on a cloud provider's implementation. GCP offers [GKE](https://cloud.google.com/kubernetes-engine/), AWS offers its own [managed implementation](https://cloud.google.com/kubernetes-engine/), and Azure has [AKS](https://azure.microsoft.com/en-us/services/kubernetes-service/#overview). \n\n### K8s Benefits\n\nWhen you run your app with Kubernetes, you get: \n\n- **Automated deployments:** You tell K8s how you want your application to look, and it figures out how to get there gradually, without disrupting uptime. So, K8s can deploy or roll back your code for you.\n- **On-the-fly and automatic scaling:** You can tell K8s how much memory and CPU a container needs. Then it figures out how to fit them into the nodes you allocated to your cluster. K8s ability to make the most of your system resources is one of its most powerful abilities.\n- **Storage management:** K8s coordinates mounting and unmounting storage for your containers. It works with block storage, proprietary cloud systems, and more.\n- **Dynamic load balancing:** K8s makes containers available via an IP address or DNS name to the outside world. If traffic to a single instance reaches a designated threshold, it will distribute the load to more than one instance.\n- **Configuration:** K8s has tools for storing and managing configuration information, including secrets and tokens. These tools are a way to safely store secrets and push changes to applications without building a deploying new container.\n- **Automatic recovery and failover:** When containers fail or stop unexpectedly, K8s will restart them for you. You can provide a custom health check to K8s, and it will restart your container when the check fails.\n\nSo with Kubernetes, you get the portability of containers combined with a robust automated infrastructure that runs in the cloud. \n\n### Terraform and Kubernetes Together\n\nSo, you can see how \"Terraform vs. Kubernetes\" isn't an entirely valid comparison. Terraform is an automation tool for building infrastructure. Kubernetes is an automation platform for running applications. They are both automation tools, and both can help you make your cloud applications easier to build, scale, and maintain. But, they solve different problems and are far from mutually exclusive. \n\nHashicorp offers an official [Kubernetes provider](https://registry.terraform.io/providers/hashicorp/kubernetes/latest/docs/guides/getting-started). It's [open-source and available on Github](https://github.com/hashicorp/terraform-provider-kubernetes). The documentation is complete, and Hashicorp offers sample HCL scripts for [EKS](https://github.com/hashicorp/terraform-provider-kubernetes/tree/main/_examples/eks), [GKE](https://github.com/hashicorp/terraform-provider-kubernetes/tree/main/_examples/gke), and [AKS](https://github.com/hashicorp/terraform-provider-kubernetes/tree/main/_examples/aks) with the course code. \n\nThere are several reasons to provision a cloud Kubernetes cluster with Terraform instead of using their proprietary tools. The obvious reason is that Terraform's cloud-agnostic features make it easier to build clusters to other cloud providers, even if you consider the differences between systems. Even if you don't want to move to a different cloud provider, you may find it necessary to run on an alternative public cloud for disaster recovery. Another good reason is to make it easier to create different environments inside the same cloud provider. In both cases, using Terraform to build K8s clusters acts as a form of documentation. Instead of proprietary shell scripts or worse, steps in a web interface, you have HCL scripts that document how you built your k8s cluster. \n\nYou can [manage a Kubernetes cluster with Terraform, too.](https://learn.hashicorp.com/tutorials/terraform/kubernetes-provider) \n\n### Cloud Automation Tools\n\nWe covered what Terraform and Kubernetes are, how they differ, and how you can use them together to make your cloud infrastructure easier to build and manage. Terraform is an automated infrastructure as code tool for provisioning and managing cloud infrastructure. It supports a large number of different platforms and services with a scripting language for describing the services you need and how Terraform is to provision them. Kubernetes is a container orchestration platform. You use it to automate deploying, scaling, recovering, and load-balancing your containerized applications. It runs on every major public cloud platform, as well as on-premises. \n\nTerraform automates infrastructure. Kubernetes automates containers and applications. Rather than competing, they complement each other. You can use Terraform to deploy your Kubernetes clusters and make it easy to reproduce them in different environments.\n",
          "code": "var Component=(()=>{var d=Object.create;var t=Object.defineProperty;var h=Object.getOwnPropertyDescriptor;var m=Object.getOwnPropertyNames;var p=Object.getPrototypeOf,f=Object.prototype.hasOwnProperty;var g=(n,e)=>()=>(e||n((e={exports:{}}).exports,e),e.exports),b=(n,e)=>{for(var a in e)t(n,a,{get:e[a],enumerable:!0})},s=(n,e,a,i)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let o of m(e))!f.call(n,o)&&o!==a&&t(n,o,{get:()=>e[o],enumerable:!(i=h(e,o))||i.enumerable});return n};var y=(n,e,a)=>(a=n!=null?d(p(n)):{},s(e||!n||!n.__esModule?t(a,\"default\",{value:n,enumerable:!0}):a,n)),v=n=>s(t({},\"__esModule\",{value:!0}),n);var l=g((x,c)=>{c.exports=_jsx_runtime});var K={};b(K,{default:()=>T,frontmatter:()=>w});var r=y(l()),w={title:\"Terraform vs Kubernetes: What's the Difference and Why It Matters\",summary:\"We'll look at Terraform and Kubernetes and how you can use them separately or together to automate and scale your cloud\",publishDate:\"Tue Feb 21 2023 17:30:53 GMT+0000 (Coordinated Universal Time)\",author:\"eric-goebelbecker\",readingTime:7,categories:[\"platform-engineering\",\"kubernetes\"],mainImage:\"/blog-images/814523c36864bffb33d1170c4b4682ff.jpg\",imageAlt:\"Terraform vs Kubernetes\",showCTA:!0,ctaCopy:\"Looking to automate and scale your cloud environments seamlessly like Terraform and Kubernetes? Try Release for on-demand environment provisioning and streamline workflows.\",ctaLink:\"https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=terraform-vs-kubernetes-whats-the-difference-and-why-it-matters\",relatedPosts:[\"\"],ogImage:\"/blog-images/814523c36864bffb33d1170c4b4682ff.jpg\",excerpt:\"We'll look at Terraform and Kubernetes and how you can use them separately or together to automate and scale your cloud\",tags:[\"platform-engineering\",\"kubernetes\"],ctaButton:\"Try Release for Free\"};function u(n){let e=Object.assign({p:\"p\",a:\"a\",img:\"img\",h3:\"h3\",span:\"span\",ul:\"ul\",li:\"li\",pre:\"pre\",code:\"code\",strong:\"strong\"},n.components);return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(e.p,{children:\"Terraform vs. Kubernetes. Which tool is better? Which one do you need?\\xA0\"}),`\n`,(0,r.jsxs)(e.p,{children:[\"Sometimes people confuse these two cloud automation tools, but it turns out they're not the same thing, and they're not in competition with each other. As a matter of fact, they work very well together. \",(0,r.jsx)(e.a,{href:\"https://www.terraform.io\",children:\"Terraform\"}),\" is an \",(0,r.jsx)(e.a,{href:\"https://en.wikipedia.org/wiki/Infrastructure_as_code\",children:\"infrastructure as code\"}),\" tool, while \",(0,r.jsx)(e.a,{href:\"https://kubernetes.io\",children:\"Kubernetes\"}),\" is a [container orchestration](\",(0,r.jsx)(e.a,{href:\"https://en.wikipedia.org/wiki/Orchestration_(computing)\",children:\"https://en.wikipedia.org/wiki/Orchestration_(computing)\"}),\" system. Terraform can help you build and manage reproducible cloud infrastructure. Kubernetes can help you create more robust cloud applications.\\xA0\"]}),`\n`,(0,r.jsx)(e.p,{children:\"This post will explain what Terraform and Kubernetes are and how you can use them separately or together to automate and scale your cloud environments.\\xA0\"}),`\n`,(0,r.jsx)(e.p,{children:(0,r.jsx)(e.img,{src:\"/blog-images/9e8aa21a7347960d68600df2f6c623aa.png\",alt:\"\"})}),`\n`,(0,r.jsxs)(e.h3,{id:\"terraform\",children:[(0,r.jsx)(e.a,{className:\"anchor\",href:\"#terraform\",children:(0,r.jsx)(e.span,{className:\"icon icon-link\"})}),\"Terraform\"]}),`\n`,(0,r.jsxs)(e.p,{children:[\"Terraform is \",(0,r.jsx)(e.a,{href:\"https://www.hashicorp.com\",children:\"Hashicorp's\"}),\" infrastructure as code automation tool for provisioning and configuring cloud services. It has a consistent command-line interface (CLI) to more than 300 different cloud APIs. You describe your cloud environments in a declarative configuration syntax and then execute Terraform \",(0,r.jsx)(e.a,{href:\"https://www.terraform.io/docs/glossary#plan-verb\",children:\"plans\"}),\" to build them. Hashicorp offers both open-source and licensed versions of Terraform.\\xA0\"]}),`\n`,(0,r.jsx)(e.p,{children:\"With Terraform you:\\xA0\"}),`\n`,(0,r.jsxs)(e.ul,{children:[`\n`,(0,r.jsx)(e.li,{children:\"Increase your infrastructure automation and eliminate preventable mistakes.\"}),`\n`,(0,r.jsx)(e.li,{children:\"Use the same workflow to build environments in different public clouds\"}),`\n`,(0,r.jsx)(e.li,{children:\"Manage your infrastructure like code with configuration files, pull requests, and reviews\"}),`\n`,(0,r.jsx)(e.li,{children:\"Have a powerful tool for ensuring that your development, staging, and production environment are consistent.\"}),`\n`]}),`\n`,(0,r.jsx)(e.p,{children:\"Terraform uses Hashicorp Configuration Language (HCL) to describe infrastructure resources. The CLI reads these files and executes your plans using the cloud provider's native APIS.\\xA0\"}),`\n`,(0,r.jsxs)(e.p,{children:['You can use Terraform to provision a wide variety of different tools via a robust plugin system. Plugins are called \"providers.\" Hashicorp supports hundreds of official providers, ',(0,r.jsx)(e.a,{href:\"https://registry.terraform.io/browse/providers\",children:\"maintains an official registry\"}),\", and has an \",(0,r.jsx)(e.a,{href:\"https://www.terraform.io/cloud-docs/api-docs/private-registry/providers\",children:\"API\"}),\" so you can write your own. They even have a Kubernetes provider that we'll discuss later.\\xA0\"]}),`\n`,(0,r.jsxs)(e.h3,{id:\"terraform-hcl\",children:[(0,r.jsx)(e.a,{className:\"anchor\",href:\"#terraform-hcl\",children:(0,r.jsx)(e.span,{className:\"icon icon-link\"})}),\"Terraform HCL\"]}),`\n`,(0,r.jsx)(e.p,{children:\"HCL is a user-friendly language that is easy to write and easy to read. If you understand the underlying infrastructure that you're using Terraform to provision, you can figure out HCL rather quickly. But it's not a restricted or limited programming language. It has variables, data types, and control flow.\\xA0\"}),`\n`,(0,r.jsx)(e.p,{children:\"This is how you declare a string variable in HCL:\\xA0\"}),`\n`,(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:\"language-hcl\",children:`\nvariable \"username\" {\n \\xA0default = \"webmaster\"\n}\n\n`})}),`\n`,(0,r.jsx)(e.p,{children:\"This is an example of a list:\\xA0\"}),`\n`,(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:\"language-hcl\",children:`\nvariable \"servers\" {\n \\xA0 \\xA0default = [\"webserver\", \"databaseserver\", \"backupserver\"]\n}\n\n`})}),`\n`,(0,r.jsx)(e.p,{children:\"Here's a for loop that traverses the list and outputs each name in upper case:\\xA0\"}),`\n`,(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:\"language-hcl\",children:`\nvariable \"servers\" {\n \\xA0 \\xA0default = [\"webserver\", \"databaseserver\", \"backupserver\"]\n}\n\noutput \"forloop_terraform\" {\n \\xA0 value = [\n \\xA0 \\xA0 \\xA0for name in var.servers : upper(name)\n \\xA0 ]\n}\n\n`})}),`\n`,(0,r.jsx)(e.p,{children:\"Additionally, HCL has many powerful features, like maps, functions, and code libraries.\\xA0\"}),`\n`,(0,r.jsxs)(e.h3,{id:\"terraform-examples\",children:[(0,r.jsx)(e.a,{className:\"anchor\",href:\"#terraform-examples\",children:(0,r.jsx)(e.span,{className:\"icon icon-link\"})}),\"Terraform Examples\"]}),`\n`,(0,r.jsx)(e.p,{children:\"Terraform is cloud-agnostic, but unfortunately, that doesn't mean you can write one HCL file and use it with more than one cloud. Each cloud provider had distinct APIs and very different configuration models.\\xA0\"}),`\n`,(0,r.jsxs)(e.p,{children:[\"Here is code from the \",(0,r.jsx)(e.a,{href:\"https://learn.hashicorp.com/terraform?utm_source=terraform_io\",children:\"Terraform tutorial\"}),\" for provisioning an Amazon Web Service (AWS) EC2 instance.\\xA0\"]}),`\n`,(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:\"language-hcl\",children:`\nterraform {\n \\xA0required_providers {\n \\xA0 \\xA0aws = {\n \\xA0 \\xA0 \\xA0source \\xA0= \"hashicorp/aws\"\n \\xA0 \\xA0 \\xA0version = \"~> 3.27\"\n \\xA0 \\xA0}\n \\xA0}\n\n \\xA0required_version = \">= 1.1.0\"\n}\n\nprovider \"aws\" {\n \\xA0profile = \"default\"\n \\xA0region \\xA0= \"us-west-2\"\n}\n\nresource \"aws_instance\" \"app_server\" {\n \\xA0ami \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 = \"ami-830c94e3\"\n \\xA0instance_type = \"t2.micro\"\n\n \\xA0tags = {\n \\xA0 \\xA0Name = \"ExampleAppServerInstance\"\n \\xA0}\n}\n\n`})}),`\n`,(0,r.jsx)(e.p,{children:\"Here is a similar example for GCP:\\xA0\"}),`\n`,(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:\"language-hcl\",children:`\nprovider \"google\" {\n \\xA0project = \"terraform-examples-gcloud\"\n \\xA0region \\xA0= \"us-east1\"\n}\n\nresource \"google_compute_instance\" \"example\" {\n \\xA0name \\xA0 \\xA0 \\xA0 \\xA0 \\xA0= \"example\"\n \\xA0machine_type \\xA0= \"f1-micro\"\n \\xA0zone \\xA0 \\xA0 \\xA0 \\xA0 \\xA0= \"us-east1-b\"\n \\xA0\n \\xA0boot_disk {\n \\xA0 \\xA0initialize_params {\n \\xA0 \\xA0 \\xA0image = \"ubuntu-1604-lts\"\n \\xA0 \\xA0}\n \\xA0}\n \\xA0\n \\xA0network_interface {\n \\xA0 \\xA0network = \"default\"\n\n \\xA0 \\xA0access_config {\n \\xA0 \\xA0 \\xA0// Ephemeral IP\n \\xA0 \\xA0}\n \\xA0}\n \\xA0\n \\xA0tags = [\"terraform-example\"]\n}\n\n`})}),`\n`,(0,r.jsx)(e.p,{children:\"You can separate your code into modules and separate variable declarations from code. This helps abstract out the difference between different cloud platforms.\\xA0\"}),`\n`,(0,r.jsx)(e.p,{children:(0,r.jsx)(e.img,{src:\"/blog-images/078c35869988e1041da4242cd3632216.png\",alt:\"\"})}),`\n`,(0,r.jsxs)(e.h3,{id:\"kubernetes\",children:[(0,r.jsx)(e.a,{className:\"anchor\",href:\"#kubernetes\",children:(0,r.jsx)(e.span,{className:\"icon icon-link\"})}),\"Kubernetes\"]}),`\n`,(0,r.jsxs)(e.p,{children:[\"Kubernetes (\",(0,r.jsx)(e.a,{href:\"https://release.com/blog/why-kubernetes-is-so-hard\",children:\"K8s\"}),\") is an open-source container orchestration system. Like Terraform, it's an automation tool, but for containerized applications, not infrastructure. K8s automates deploying, scaling, and managing applications that run in containers. Usually, those containers are \",(0,r.jsx)(e.a,{href:\"https://www.docker.com\",children:\"Docker\"}),\".\\xA0\"]}),`\n`,(0,r.jsxs)(e.p,{children:[\"Google created K8s, but it is not limited to Google infrastructure. You can run K8 on-premises, on a cluster you build and manage yourself in the cloud, or on a cloud provider's implementation. GCP offers \",(0,r.jsx)(e.a,{href:\"https://cloud.google.com/kubernetes-engine/\",children:\"GKE\"}),\", AWS offers its own \",(0,r.jsx)(e.a,{href:\"https://cloud.google.com/kubernetes-engine/\",children:\"managed implementation\"}),\", and Azure has \",(0,r.jsx)(e.a,{href:\"https://azure.microsoft.com/en-us/services/kubernetes-service/#overview\",children:\"AKS\"}),\".\\xA0\"]}),`\n`,(0,r.jsxs)(e.h3,{id:\"k8s-benefits\",children:[(0,r.jsx)(e.a,{className:\"anchor\",href:\"#k8s-benefits\",children:(0,r.jsx)(e.span,{className:\"icon icon-link\"})}),\"K8s Benefits\"]}),`\n`,(0,r.jsx)(e.p,{children:\"When you run your app with Kubernetes, you get:\\xA0\"}),`\n`,(0,r.jsxs)(e.ul,{children:[`\n`,(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:\"Automated deployments:\"}),\" You tell K8s how you want your application to look, and it figures out how to get there gradually, without disrupting uptime. So, K8s can deploy or roll back your code for you.\"]}),`\n`,(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:\"On-the-fly and automatic scaling:\"}),\" You can tell K8s how much memory and CPU a container needs. Then it figures out how to fit them into the nodes you allocated to your cluster. K8s ability to make the most of your system resources is one of its most powerful abilities.\"]}),`\n`,(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:\"Storage management:\"}),\" K8s coordinates mounting and unmounting storage for your containers. It works with block storage, proprietary cloud systems, and more.\"]}),`\n`,(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:\"Dynamic load balancing:\"}),\" K8s makes containers available via an IP address or DNS name to the outside world. If traffic to a single instance reaches a designated threshold, it will distribute the load to more than one instance.\"]}),`\n`,(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:\"Configuration:\"}),\" K8s has tools for storing and managing configuration information, including secrets and tokens. These tools are a way to safely store secrets and push changes to applications without building a deploying new container.\"]}),`\n`,(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:\"Automatic recovery and failover:\"}),\" When containers fail or stop unexpectedly, K8s will restart them for you. You can provide a custom health check to K8s, and it will restart your container when the check fails.\"]}),`\n`]}),`\n`,(0,r.jsx)(e.p,{children:\"So with Kubernetes, you get the portability of containers combined with a robust automated infrastructure that runs in the cloud.\\xA0\"}),`\n`,(0,r.jsxs)(e.h3,{id:\"terraform-and-kubernetes-together\",children:[(0,r.jsx)(e.a,{className:\"anchor\",href:\"#terraform-and-kubernetes-together\",children:(0,r.jsx)(e.span,{className:\"icon icon-link\"})}),\"Terraform and Kubernetes Together\"]}),`\n`,(0,r.jsx)(e.p,{children:`So, you can see how \"Terraform vs. Kubernetes\" isn't an entirely valid comparison. Terraform is an automation tool for building infrastructure. Kubernetes is an automation platform for running applications. They are both automation tools, and both can help you make your cloud applications easier to build, scale, and maintain. But, they solve different problems and are far from mutually exclusive.\\xA0`}),`\n`,(0,r.jsxs)(e.p,{children:[\"Hashicorp offers an official \",(0,r.jsx)(e.a,{href:\"https://registry.terraform.io/providers/hashicorp/kubernetes/latest/docs/guides/getting-started\",children:\"Kubernetes provider\"}),\". It's \",(0,r.jsx)(e.a,{href:\"https://github.com/hashicorp/terraform-provider-kubernetes\",children:\"open-source and available on Github\"}),\". The documentation is complete, and Hashicorp offers sample HCL scripts for \",(0,r.jsx)(e.a,{href:\"https://github.com/hashicorp/terraform-provider-kubernetes/tree/main/_examples/eks\",children:\"EKS\"}),\", \",(0,r.jsx)(e.a,{href:\"https://github.com/hashicorp/terraform-provider-kubernetes/tree/main/_examples/gke\",children:\"GKE\"}),\", and \",(0,r.jsx)(e.a,{href:\"https://github.com/hashicorp/terraform-provider-kubernetes/tree/main/_examples/aks\",children:\"AKS\"}),\" with the course code.\\xA0\"]}),`\n`,(0,r.jsx)(e.p,{children:\"There are several reasons to provision a cloud Kubernetes cluster with Terraform instead of using their proprietary tools. The obvious reason is that Terraform's cloud-agnostic features make it easier to build clusters to other cloud providers, even if you consider the differences between systems. Even if you don't want to move to a different cloud provider, you may find it necessary to run on an alternative public cloud for disaster recovery. Another good reason is to make it easier to create different environments inside the same cloud provider. In both cases, using Terraform to build K8s clusters acts as a form of documentation. Instead of proprietary shell scripts or worse, steps in a web interface, you have HCL scripts that document how you built your k8s cluster.\\xA0\"}),`\n`,(0,r.jsxs)(e.p,{children:[\"You can \",(0,r.jsx)(e.a,{href:\"https://learn.hashicorp.com/tutorials/terraform/kubernetes-provider\",children:\"manage a Kubernetes cluster with Terraform, too.\"}),\"\\xA0\"]}),`\n`,(0,r.jsxs)(e.h3,{id:\"cloud-automation-tools\",children:[(0,r.jsx)(e.a,{className:\"anchor\",href:\"#cloud-automation-tools\",children:(0,r.jsx)(e.span,{className:\"icon icon-link\"})}),\"Cloud Automation Tools\"]}),`\n`,(0,r.jsx)(e.p,{children:\"We covered what Terraform and Kubernetes are, how they differ, and how you can use them together to make your cloud infrastructure easier to build and manage. Terraform is an automated infrastructure as code tool for provisioning and managing cloud infrastructure. It supports a large number of different platforms and services with a scripting language for describing the services you need and how Terraform is to provision them. Kubernetes is a container orchestration platform. You use it to automate deploying, scaling, recovering, and load-balancing your containerized applications. It runs on every major public cloud platform, as well as on-premises.\\xA0\"}),`\n`,(0,r.jsx)(e.p,{children:\"Terraform automates infrastructure. Kubernetes automates containers and applications. Rather than competing, they complement each other. You can use Terraform to deploy your Kubernetes clusters and make it easy to reproduce them in different environments.\"})]})}function k(n={}){let{wrapper:e}=n.components||{};return e?(0,r.jsx)(e,Object.assign({},n,{children:(0,r.jsx)(u,n)})):u(n)}var T=k;return v(K);})();\n;return Component;"
        },
        "_id": "blog/posts/terraform-vs-kubernetes-whats-the-difference-and-why-it-matters.mdx",
        "_raw": {
          "sourceFilePath": "blog/posts/terraform-vs-kubernetes-whats-the-difference-and-why-it-matters.mdx",
          "sourceFileName": "terraform-vs-kubernetes-whats-the-difference-and-why-it-matters.mdx",
          "sourceFileDir": "blog/posts",
          "contentType": "mdx",
          "flattenedPath": "blog/posts/terraform-vs-kubernetes-whats-the-difference-and-why-it-matters"
        },
        "type": "BlogPost",
        "computedSlug": "terraform-vs-kubernetes-whats-the-difference-and-why-it-matters"
      },
      "documentHash": "1739393595028",
      "hasWarnings": false,
      "documentTypeName": "BlogPost"
    },
    "blog/posts/terraforms-for-each-examples.mdx": {
      "document": {
        "title": "How to Use Terraform's `for_each`, With Examples",
        "summary": "Creating multiple resources with Terraform? Use for_each. In this post, you'll learn what is it as well as how and when",
        "publishDate": "Fri Jan 07 2022 00:32:29 GMT+0000 (Coordinated Universal Time)",
        "author": "dawid-ziolkowski",
        "readingTime": 5,
        "categories": [
          "platform-engineering",
          "product"
        ],
        "mainImage": "/blog-images/f6d54e0d3ec06d682bc6894ce90e2a79.jpg",
        "imageAlt": "photo of computer code ",
        "showCTA": true,
        "ctaCopy": "Automate infrastructure setup with Release's on-demand environments, simplifying Terraform `for_each` usage. Boost productivity and scalability.",
        "ctaLink": "https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=terraforms-for-each-examples",
        "relatedPosts": [
          ""
        ],
        "ogImage": "/blog-images/f6d54e0d3ec06d682bc6894ce90e2a79.jpg",
        "excerpt": "Creating multiple resources with Terraform? Use for_each. In this post, you'll learn what is it as well as how and when",
        "tags": [
          "platform-engineering",
          "product"
        ],
        "ctaButton": "Try Release for Free",
        "body": {
          "raw": "\nTerraform is one of the most popular [infrastructure as code](http://en.wikipedia.org/wiki/Infrastructure_as_code) (IaC) tools. With Terraform, you can write code that defines the infrastructure components you want and the configuration for them. You then execute that code, and Terraform will make sure that your infrastructure is set up the way you defined it. This means either creating new components or responding with \"All of these are already created.\"\n\nSounds easy, right?\n\nIn reality, Terraform code can be quite complicated. That's especially likely when you want to do some advanced stuff, like iterating over a certain number of resources.\n\nFor example, let's say you want to create multiple virtual machines with the same configuration. For cases like that, in Terraform you can use \\`for_each\\`**.** In this post, you'll learn what that is as well as how and when to use it.\n\n### Infrastructure as Code With Terraform\n\nIf you're new to [Terraform](https://www.terraform.io/), before we move on, you need to understand what it actually is.\n\nModern environments are quite complex, and you have a few options when you want to add, change or destroy some components of your infrastructure. If you're on the cloud, you can go to your cloud provider web UI and execute any necessary action from there. You can also use CLI or even write some scripts yourself and call your cloud provider API.\n\nThere are, however, some limitations to all these options. Clicking in the web UI doesn't scale. And you don't want to create dozens of different services every time you need a new environment.\n\nUsing CLI or writing your own scripts is a step forward. But why not take two steps forward instead and use a dedicated infrastructure-as- code tool?\n\n![](/blog-images/b930959b2b5487279ba61c965cf667f0.png)\n\nTerraform is one such tool. You write Terraform-specific code defining how you want your infrastructure to look. Then you execute Terraform and everything is taken care of for you. It's a highly efficient and scalable way of creating infrastructure.\n\nAlso, one of the biggest advantages of using Terraform is that it will  keep the state of your infrastructure saved. Therefore, it will always try to have your infrastructure in sync. So once you execute Terraform, it will only create, change or destroy resources that aren't in sync with the saved state.\n\n### Terraform Meta-Arguments\n\nBefore we dive into explaining how \\`for_each\\` works, let's briefly talk about what it actually is.\n\nIn Terraform, we mainly talk about resource blocks and module blocks. For example, when you want to create a virtual machine, you need to define a resource block with configuration details of that machine. Within the resource and module block, you can also use one of the five so-called meta-arguments. These are special instructions that aren't part of the resource configuration per se, but they instruct Terraform to do some action in relation to that resource. And one of these instructions is \\`for_each\\`.\n\nAs I already mentioned, the main purpose of the \\`for_each\\` meta-argument is to create multiple instances of a resource. So, as you can imagine, it's quite useful to know.\n\nIt's also worth mentioning that \\`for_each\\` has been added to Terraform in version 0.12. But I hope you've already upgraded to Terraform 1.x anyway.\n\n### Multiple Resources\n\nTo understand better what purpose \\`for_each\\` serves, let's see how you could achieve the same outcome in Terraform without using \\`for_each\\`.\n\nThe outcome we're talking about is deploying multiple copies of the same resource. So, let's take virtual machines, for example.\n\nNormally, to deploy more than one virtual machine, you'd have to specify multiple resource blocks, like this:\n\n```yaml\nresource \"google_compute_instance\" \"vm1\" {\nname         = \"vm1\"\nmachine_type = \"e2-small\"\nzone         = \"us-central1-a\"\n(...)\n}\n\nresource \"google_compute_instance\" \"vm2\" {\nname         = \"vm2\"\nmachine_type = \"e2-medium\"\nzone         = \"us-central1-a\"\n(...)\n}\n\nresource \"google_compute_instance\" \"vm3\" {\nname         = \"vm3\"\nmachine_type = \"f1-micro\"\nzone         = \"us-central1-a\"\n(...)\n}\n```\n\nSeems like a lot of duplicated code, doesn't it? That's exactly where \\`for_each\\` can help.\n\nInstead of duplicating all that code for each virtual machine, you can define your resource once and provide a map or a set of strings to iterate over.\n\nTake a look at the example. This is how achieving the same results as above would look with \\`for_each\\`:\n\n```yaml\nresource \"google_compute_instance\" \"vm\" {\nfor_each = {\n\"vm1\" = \"e2-small\"\n\"vm2\" = \"e2-medium\"\n\"vm3\" = \"f1-micro\"\n}\nname = each.key\nmachine_type = each.value\nzone = \"us-central1-a\"\n(...)\n}\n```\n\nAs you can see, we defined the configuration parameters that differ per virtual machine as key-value pairs in the \\`for_each\\` block and left the parameters that are the same for each VM in the resource block. Then, we accessed the key-value pair by special keywords \\`each.key**\\`** and \\`each.value**\\`**.\n\nWhat if you want to pass more than just two (key and value) parameters? For example, what if you want to also parameterize the zone in the above example? You can simply change the value to a map, as follows:\n\n```yaml\nresource \"google_compute_instance\" \"vm\" {\nfor_each = {\n\"vm1\" = { vm_size = \"e2-small\", zone = \"us-central1-a\" }\n\"vm2\" = { vm_size = \"e2-medium\", zone = \"us-central1-b\" }\n\"vm3\" = { vm_size = \"f1-micro\", zone = \"us-central1-c\" }\n}\nname = each.key\nmachine_type = each.value.vm_size\nzone = each.value.zone\n(...)\n}\n```\n\nYou can pass as many parameters in the value as you want. Then in the actual resource configuration, you can reference them with \\`each.value.parameter_key**.**\n\nTo keep your code clean and have the ability to reuse values for different resources, you can even extract the actual parameters into a variable:\n\n```yaml\nlocals {\nvirtual_machines = {\n\"vm1\" = { vm_size = \"e2-small\", zone = \"us-central1-a\" },\n\"vm2\" = { vm_size = \"e2-medium\", zone = \"us-central1-b\" },\n\"vm3\" = { vm_size = \"f1-micro\", zone = \"us-central1-c\" }\n}\n}\n\nresource \"google_compute_instance\" \"vm\" {\nfor_each = local.virtual_machines\nname = each.key\nmachine_type = each.value.vm_size\nzone = each.value.zone\n(...)\n}\n```\n\n### \\`for_each\\` Versus \\`count\\`\n\nIf you're not new to Terraform, you may have used another meta-argument that seems like the same thing: \\`count**\\`.** And while \\`count**\\`** also lets you create multiple instances of the same resource, there's a difference between \\`count**\\`** and \\`for_each\\`. The latter isn't sensitive to changes in the order of resources.\n\nA common issue with \\`count**\\`** is that once you delete any resource other than the last one, Terraform will try to force replace all resources that the index doesn't match.\n\nYou don't have that problem with \\`for_each\\` because it uses the key of a map as an index. You can't use both \\`count**\\`** and \\`for_each\\` on the same resources, but why would you anyway?\n\nAre there any drawbacks to \\`for_each\\`? Yes.\n\n### Limitations of \\`for_each\\`\n\nWhile \\`for_each\\` is pretty straightforward to use, there are some limitations you should be aware of. First of all, the keys in your \\`for_each\\` map block must have a known value. Therefore, for example, they can't be generated on the fly by functions (like \\`bcrypt**\\`** or \\`timestamp**\\`**). They also can't refer to resource-specific attributes that are provided by a cloud provider, like a cloud resource ID. Another limitation is the fact that you can't use sensitive values as arguments for \\`for_each\\`. Basically, when using \\`for_each\\`, you need to directly specify the values.\n\n![](/blog-images/c666e716b8d4bffe24d4c1e59a0478ab.png)\n\n### Summing up and Learning More\n\n\\`for_each\\` is probably one of the most commonly used Terraform meta-arguments. Modern environments usually consist of multiple instances of resources for high-availability and scalability reasons. Using \\`for_each\\` is relatively easy, but you need a solid understanding of how it works to get the most benefits from it. It also has its own limitations.\n\nIn this article, you learned how \\`for_each\\` works and got some tips on how to use it efficiently. Now, you can try to play around with it yourself or look into [other meta-arguments](https://www.terraform.io/language/meta-arguments/depends_on).\n\n### Terraform at Scale\n\n[Release](https://hubs.li/Q011Rfby0)’s Environment-as-a-Service, is an easy to use, highly scalable service that leverages Terraform to create snapshots of even the most complex environments and automatically manages their creation and teardown as part of your development lifecycle.\n",
          "code": "var Component=(()=>{var u=Object.create;var t=Object.defineProperty;var m=Object.getOwnPropertyDescriptor;var d=Object.getOwnPropertyNames;var f=Object.getPrototypeOf,p=Object.prototype.hasOwnProperty;var y=(n,e)=>()=>(e||n((e={exports:{}}).exports,e),e.exports),g=(n,e)=>{for(var o in e)t(n,o,{get:e[o],enumerable:!0})},i=(n,e,o,s)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let r of d(e))!p.call(n,r)&&r!==o&&t(n,r,{get:()=>e[r],enumerable:!(s=m(e,r))||s.enumerable});return n};var w=(n,e,o)=>(o=n!=null?u(f(n)):{},i(e||!n||!n.__esModule?t(o,\"default\",{value:n,enumerable:!0}):o,n)),v=n=>i(t({},\"__esModule\",{value:!0}),n);var l=y((z,c)=>{c.exports=_jsx_runtime});var T={};g(T,{default:()=>k,frontmatter:()=>_});var a=w(l()),_={title:\"How to Use Terraform's `for_each`, With Examples\",summary:\"Creating multiple resources with Terraform? Use for_each. In this post, you'll learn what is it as well as how and when\",publishDate:\"Fri Jan 07 2022 00:32:29 GMT+0000 (Coordinated Universal Time)\",author:\"dawid-ziolkowski\",readingTime:5,categories:[\"platform-engineering\",\"product\"],mainImage:\"/blog-images/f6d54e0d3ec06d682bc6894ce90e2a79.jpg\",imageAlt:\"photo of computer code \",showCTA:!0,ctaCopy:\"Automate infrastructure setup with Release's on-demand environments, simplifying Terraform `for_each` usage. Boost productivity and scalability.\",ctaLink:\"https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=terraforms-for-each-examples\",relatedPosts:[\"\"],ogImage:\"/blog-images/f6d54e0d3ec06d682bc6894ce90e2a79.jpg\",excerpt:\"Creating multiple resources with Terraform? Use for_each. In this post, you'll learn what is it as well as how and when\",tags:[\"platform-engineering\",\"product\"],ctaButton:\"Try Release for Free\"};function h(n){let e=Object.assign({p:\"p\",a:\"a\",strong:\"strong\",h3:\"h3\",span:\"span\",img:\"img\",pre:\"pre\",code:\"code\"},n.components);return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsxs)(e.p,{children:[\"Terraform is one of the most popular \",(0,a.jsx)(e.a,{href:\"http://en.wikipedia.org/wiki/Infrastructure_as_code\",children:\"infrastructure as code\"}),' (IaC) tools. With Terraform, you can write code that defines the infrastructure components you want and the configuration for them. You then execute that code, and Terraform will make sure that your infrastructure is set up the way you defined it. This means either creating new components or responding with \"All of these are already created.\"']}),`\n`,(0,a.jsx)(e.p,{children:\"Sounds easy, right?\"}),`\n`,(0,a.jsx)(e.p,{children:\"In reality, Terraform code can be quite complicated. That's especially likely when you want to do some advanced stuff, like iterating over a certain number of resources.\"}),`\n`,(0,a.jsxs)(e.p,{children:[\"For example, let's say you want to create multiple virtual machines with the same configuration. For cases like that, in Terraform you can use `for_each`\",(0,a.jsx)(e.strong,{children:\".\"}),\" In this post, you'll learn what that is as well as how and when to use it.\"]}),`\n`,(0,a.jsxs)(e.h3,{id:\"infrastructure-as-code-with-terraform\",children:[(0,a.jsx)(e.a,{className:\"anchor\",href:\"#infrastructure-as-code-with-terraform\",children:(0,a.jsx)(e.span,{className:\"icon icon-link\"})}),\"Infrastructure as Code With Terraform\"]}),`\n`,(0,a.jsxs)(e.p,{children:[\"If you're new to \",(0,a.jsx)(e.a,{href:\"https://www.terraform.io/\",children:\"Terraform\"}),\", before we move on, you need to understand what it actually is.\"]}),`\n`,(0,a.jsx)(e.p,{children:\"Modern environments are quite complex, and you have a few options when you want to add, change or destroy some components of your infrastructure. If you're on the cloud, you can go to your cloud provider web UI and execute any necessary action from there. You can also use CLI or even write some scripts yourself and call your cloud provider API.\"}),`\n`,(0,a.jsx)(e.p,{children:\"There are, however, some limitations to all these options. Clicking in the web UI doesn't scale. And you don't want to create dozens of different services every time you need a new environment.\"}),`\n`,(0,a.jsx)(e.p,{children:\"Using CLI or writing your own scripts is a step forward. But why not take two steps forward instead and use a dedicated infrastructure-as- code tool?\"}),`\n`,(0,a.jsx)(e.p,{children:(0,a.jsx)(e.img,{src:\"/blog-images/b930959b2b5487279ba61c965cf667f0.png\",alt:\"\"})}),`\n`,(0,a.jsx)(e.p,{children:\"Terraform is one such tool. You write Terraform-specific code defining how you want your infrastructure to look. Then you execute Terraform and everything is taken care of for you. It's a highly efficient and scalable way of creating infrastructure.\"}),`\n`,(0,a.jsx)(e.p,{children:\"Also, one of the biggest advantages of using Terraform is that it will \\xA0keep the state of your infrastructure saved. Therefore, it will always try to have your infrastructure in sync. So once you execute Terraform, it will only create, change or destroy resources that aren't in sync with the saved state.\"}),`\n`,(0,a.jsxs)(e.h3,{id:\"terraform-meta-arguments\",children:[(0,a.jsx)(e.a,{className:\"anchor\",href:\"#terraform-meta-arguments\",children:(0,a.jsx)(e.span,{className:\"icon icon-link\"})}),\"Terraform Meta-Arguments\"]}),`\n`,(0,a.jsx)(e.p,{children:\"Before we dive into explaining how `for_each` works, let's briefly talk about what it actually is.\"}),`\n`,(0,a.jsx)(e.p,{children:\"In Terraform, we mainly talk about resource blocks and module blocks. For example, when you want to create a virtual machine, you need to define a resource block with configuration details of that machine. Within the resource and module block, you can also use one of the five so-called meta-arguments. These are special instructions that aren't part of the resource configuration per se, but they instruct Terraform to do some action in relation to that resource. And one of these instructions is `for_each`.\"}),`\n`,(0,a.jsx)(e.p,{children:\"As I already mentioned, the main purpose of the `for_each` meta-argument is to create multiple instances of a resource. So, as you can imagine, it's quite useful to know.\"}),`\n`,(0,a.jsx)(e.p,{children:\"It's also worth mentioning that `for_each` has been added to Terraform in version 0.12. But I hope you've already upgraded to Terraform 1.x anyway.\"}),`\n`,(0,a.jsxs)(e.h3,{id:\"multiple-resources\",children:[(0,a.jsx)(e.a,{className:\"anchor\",href:\"#multiple-resources\",children:(0,a.jsx)(e.span,{className:\"icon icon-link\"})}),\"Multiple Resources\"]}),`\n`,(0,a.jsx)(e.p,{children:\"To understand better what purpose `for_each` serves, let's see how you could achieve the same outcome in Terraform without using `for_each`.\"}),`\n`,(0,a.jsx)(e.p,{children:\"The outcome we're talking about is deploying multiple copies of the same resource. So, let's take virtual machines, for example.\"}),`\n`,(0,a.jsx)(e.p,{children:\"Normally, to deploy more than one virtual machine, you'd have to specify multiple resource blocks, like this:\"}),`\n`,(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:\"language-yaml\",children:`resource \"google_compute_instance\" \"vm1\" {\nname \\xA0 \\xA0 \\xA0 \\xA0 = \"vm1\"\nmachine_type = \"e2-small\"\nzone \\xA0 \\xA0 \\xA0 \\xA0 = \"us-central1-a\"\n(...)\n}\n\nresource \"google_compute_instance\" \"vm2\" {\nname \\xA0 \\xA0 \\xA0 \\xA0 = \"vm2\"\nmachine_type = \"e2-medium\"\nzone \\xA0 \\xA0 \\xA0 \\xA0 = \"us-central1-a\"\n(...)\n}\n\nresource \"google_compute_instance\" \"vm3\" {\nname \\xA0 \\xA0 \\xA0 \\xA0 = \"vm3\"\nmachine_type = \"f1-micro\"\nzone \\xA0 \\xA0 \\xA0 \\xA0 = \"us-central1-a\"\n(...)\n}\n`})}),`\n`,(0,a.jsx)(e.p,{children:\"Seems like a lot of duplicated code, doesn't it? That's exactly where `for_each` can help.\"}),`\n`,(0,a.jsx)(e.p,{children:\"Instead of duplicating all that code for each virtual machine, you can define your resource once and provide a map or a set of strings to iterate over.\"}),`\n`,(0,a.jsx)(e.p,{children:\"Take a look at the example. This is how achieving the same results as above would look with `for_each`:\"}),`\n`,(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:\"language-yaml\",children:`resource \"google_compute_instance\" \"vm\" {\nfor_each = {\n\"vm1\" = \"e2-small\"\n\"vm2\" = \"e2-medium\"\n\"vm3\" = \"f1-micro\"\n}\nname = each.key\nmachine_type = each.value\nzone = \"us-central1-a\"\n(...)\n}\n`})}),`\n`,(0,a.jsx)(e.p,{children:\"As you can see, we defined the configuration parameters that differ per virtual machine as key-value pairs in the `for_each` block and left the parameters that are the same for each VM in the resource block. Then, we accessed the key-value pair by special keywords `each.key**`** and `each.value**`**.\"}),`\n`,(0,a.jsx)(e.p,{children:\"What if you want to pass more than just two (key and value) parameters? For example, what if you want to also parameterize the zone in the above example? You can simply change the value to a map, as follows:\"}),`\n`,(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:\"language-yaml\",children:`resource \"google_compute_instance\" \"vm\" {\nfor_each = {\n\"vm1\" = { vm_size = \"e2-small\", zone = \"us-central1-a\" }\n\"vm2\" = { vm_size = \"e2-medium\", zone = \"us-central1-b\" }\n\"vm3\" = { vm_size = \"f1-micro\", zone = \"us-central1-c\" }\n}\nname = each.key\nmachine_type = each.value.vm_size\nzone = each.value.zone\n(...)\n}\n`})}),`\n`,(0,a.jsx)(e.p,{children:\"You can pass as many parameters in the value as you want. Then in the actual resource configuration, you can reference them with `each.value.parameter_key**.**\"}),`\n`,(0,a.jsx)(e.p,{children:\"To keep your code clean and have the ability to reuse values for different resources, you can even extract the actual parameters into a variable:\"}),`\n`,(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:\"language-yaml\",children:`locals {\nvirtual_machines = {\n\"vm1\" = { vm_size = \"e2-small\", zone = \"us-central1-a\" },\n\"vm2\" = { vm_size = \"e2-medium\", zone = \"us-central1-b\" },\n\"vm3\" = { vm_size = \"f1-micro\", zone = \"us-central1-c\" }\n}\n}\n\nresource \"google_compute_instance\" \"vm\" {\nfor_each = local.virtual_machines\nname = each.key\nmachine_type = each.value.vm_size\nzone = each.value.zone\n(...)\n}\n`})}),`\n`,(0,a.jsxs)(e.h3,{id:\"for_each-versus-count\",children:[(0,a.jsx)(e.a,{className:\"anchor\",href:\"#for_each-versus-count\",children:(0,a.jsx)(e.span,{className:\"icon icon-link\"})}),\"`for_each` Versus `count`\"]}),`\n`,(0,a.jsx)(e.p,{children:\"If you're not new to Terraform, you may have used another meta-argument that seems like the same thing: `count**`.** And while `count**`** also lets you create multiple instances of the same resource, there's a difference between `count**`** and `for_each`. The latter isn't sensitive to changes in the order of resources.\"}),`\n`,(0,a.jsx)(e.p,{children:\"A common issue with `count**`** is that once you delete any resource other than the last one, Terraform will try to force replace all resources that the index doesn't match.\"}),`\n`,(0,a.jsx)(e.p,{children:\"You don't have that problem with `for_each` because it uses the key of a map as an index. You can't use both `count**`** and `for_each` on the same resources, but why would you anyway?\"}),`\n`,(0,a.jsx)(e.p,{children:\"Are there any drawbacks to `for_each`? Yes.\"}),`\n`,(0,a.jsxs)(e.h3,{id:\"limitations-of-for_each\",children:[(0,a.jsx)(e.a,{className:\"anchor\",href:\"#limitations-of-for_each\",children:(0,a.jsx)(e.span,{className:\"icon icon-link\"})}),\"Limitations of `for_each`\"]}),`\n`,(0,a.jsx)(e.p,{children:\"While `for_each` is pretty straightforward to use, there are some limitations you should be aware of. First of all, the keys in your `for_each` map block must have a known value. Therefore, for example, they can't be generated on the fly by functions (like `bcrypt**`** or `timestamp**`**). They also can't refer to resource-specific attributes that are provided by a cloud provider, like a cloud resource ID. Another limitation is the fact that you can't use sensitive values as arguments for `for_each`. Basically, when using `for_each`, you need to directly specify the values.\"}),`\n`,(0,a.jsx)(e.p,{children:(0,a.jsx)(e.img,{src:\"/blog-images/c666e716b8d4bffe24d4c1e59a0478ab.png\",alt:\"\"})}),`\n`,(0,a.jsxs)(e.h3,{id:\"summing-up-and-learning-more\",children:[(0,a.jsx)(e.a,{className:\"anchor\",href:\"#summing-up-and-learning-more\",children:(0,a.jsx)(e.span,{className:\"icon icon-link\"})}),\"Summing up and Learning More\"]}),`\n`,(0,a.jsx)(e.p,{children:\"`for_each` is probably one of the most commonly used Terraform meta-arguments. Modern environments usually consist of multiple instances of resources for high-availability and scalability reasons. Using `for_each` is relatively easy, but you need a solid understanding of how it works to get the most benefits from it. It also has its own limitations.\"}),`\n`,(0,a.jsxs)(e.p,{children:[\"In this article, you learned how `for_each` works and got some tips on how to use it efficiently. Now, you can try to play around with it yourself or look into \",(0,a.jsx)(e.a,{href:\"https://www.terraform.io/language/meta-arguments/depends_on\",children:\"other meta-arguments\"}),\".\"]}),`\n`,(0,a.jsxs)(e.h3,{id:\"terraform-at-scale\",children:[(0,a.jsx)(e.a,{className:\"anchor\",href:\"#terraform-at-scale\",children:(0,a.jsx)(e.span,{className:\"icon icon-link\"})}),\"Terraform at Scale\"]}),`\n`,(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.a,{href:\"https://hubs.li/Q011Rfby0\",children:\"Release\"}),\"\\u2019s Environment-as-a-Service, is an easy to use, highly scalable service that leverages Terraform to create snapshots of even the most complex environments and automatically manages their creation and teardown as part of your development lifecycle.\"]})]})}function b(n={}){let{wrapper:e}=n.components||{};return e?(0,a.jsx)(e,Object.assign({},n,{children:(0,a.jsx)(h,n)})):h(n)}var k=b;return v(T);})();\n;return Component;"
        },
        "_id": "blog/posts/terraforms-for-each-examples.mdx",
        "_raw": {
          "sourceFilePath": "blog/posts/terraforms-for-each-examples.mdx",
          "sourceFileName": "terraforms-for-each-examples.mdx",
          "sourceFileDir": "blog/posts",
          "contentType": "mdx",
          "flattenedPath": "blog/posts/terraforms-for-each-examples"
        },
        "type": "BlogPost",
        "computedSlug": "terraforms-for-each-examples"
      },
      "documentHash": "1739393595028",
      "hasWarnings": false,
      "documentTypeName": "BlogPost"
    },
    "blog/posts/test-environment-a-definition-and-how-to-guide.mdx": {
      "document": {
        "title": "Test Environment: A Definition and How-to Guide",
        "summary": "What a test environment is and how to set one up. ",
        "publishDate": "Wed Dec 21 2022 07:46:12 GMT+0000 (Coordinated Universal Time)",
        "author": "erik-landerholm",
        "readingTime": 6,
        "categories": [
          "platform-engineering",
          "product"
        ],
        "mainImage": "/blog-images/63498e869bdfbca60345e4f2217782dc.jpg",
        "imageAlt": "A hand holding a magnifying glass over a laptop keyboard",
        "showCTA": true,
        "ctaCopy": "Improve software quality with Release's on-demand environments for testing and staging, reducing errors and accelerating deployment.",
        "ctaLink": "https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=test-environment-a-definition-and-how-to-guide",
        "relatedPosts": [
          ""
        ],
        "ogImage": "/blog-images/63498e869bdfbca60345e4f2217782dc.jpg",
        "excerpt": "What a test environment is and how to set one up. ",
        "tags": [
          "platform-engineering",
          "product"
        ],
        "ctaButton": "Try Release for Free",
        "body": {
          "raw": "\nDevelopers today are under increasing pressure to produce high-quality software, with 89% of companies now competing primarily on [customer experience](https://www.smartkarrot.com/resources/blog/customer-experience-statistics/#:~:text=General%20customer%20experience%20statistics&text=89%25%20of%20businesses%20compete%20primarily,warm%20and%20friendly%20customer%20experience.). Today, customers expect high-quality software that’s secure, reliable, and easy to use. When you fail to meet those expectations, adoption of your product is likely to plummet. \n\nTo ensure software quality, developers frequently choose to set up test environments where they can safely isolate code and analyze performance. Simply put, test environments help to reduce production errors, lower development costs, and improve customer satisfaction.  \n\nThis post offers a primer on test environments covering what a test environment is, how it fits into the software development life cycle, best practices for managing a test environment, and a step-by-step guide for setting one up.  \n\n![](/blog-images/19e7c960a35759874587fd6518cb9526.jpg)\n\n### What is a Test Environment?\n\nA test environment is a dedicated space—usually a server or container—for [running software tests](https://www.forbes.com/sites/forbestechcouncil/2022/10/04/how-to-improve-the-quality-of-software-testing/). You can add a copy of an application to a test environment to inspect it for bugs and vulnerabilities. Usually, creating a test environment involves setting up a physical or virtual server and configuring it according to the specifications of the application that’s being tested.  \n\nA dedicated test environment helps you ensure accuracy and consistency when code is analyzed. Engineers can alter code and experiment with it in the virtual environment and assess the results in real-time. For example, a developer could use a test environment to check the compatibility between different code versions and address any underlying bugs. \n\nSome projects may call for a single test environment while others may require multiple test environments for each access instance. Depending on your needs, you can either run tests in a fixed order or simultaneously in multiple environments.\n\n![](/blog-images/2e316bb6210e988c5477e384509d4f32.png)\n\n### What are the Different Environments Testing is Done in?\n\nMany developer teams use dedicated testing environments, but you can also test your software in different kinds of environments. Let's explore what these testing environments are.\n\n#### Development Environment\n\nThe development environment is where engineers write code and make changes. Preliminary testing also takes place in the development environment before advancing code to other stages. To illustrate, an engineer might set up a development environment to build an application and create a sample for additional testing and modifications. \n\n#### Staging Environment\n\nAfter software has been through development and testing, it moves into a staging environment. The staging environment closely resembles live production. The main purpose of the staging environment is to provide a window of insight into how the software will perform after deployment.  \n\nCompanies often use staging environments to present final projects before moving them along for final testing and production. Staging environments also provide a chance for stakeholders to see a digital project in action, collaborate, and make adjustments.  \n\n![](<https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63a2b88bf3c0d818f8a3effc_unnamed%20(1).png>)\n\n#### Production Environment\n\nThe production environment is the final stage of software development, when the software is live and running on a production server. At this point, users can access and interact with the software in real-time.  For example, all software that you use—like Slack or Teams—is currently in a live production environment.  \n\nEven though the software is live at this point, companies often continue to test and iterate on it in the production environment. Testing in production can provide better accuracy and allow for more frequent deployments. \n\n### Why Should You Use a Software Testing Environment?\n\nWhile testing in production is possible, there are several advantages to setting up a private, dedicated testing environment.  \n\n#### Support Different Tests\n\nIt’s possible to run several different types of analysis within a testing environment, depending on your needs. For example, you can set up a test environment to run integration tests, unit tests, [user acceptance tests](https://release.com/blog/user-acceptance-testing-checklist), performance tests, alpha tests, and beta tests.  \n\n#### Save Money\n\nIn general, testing software earlier in the development life cycle eliminates rework, enables you to make changes more easily, and significantly reduces development costs.  \n\n![](<https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63a2b8bc3f0f382c76568939_unnamed%20(2).png>)\n\n#### Ensure Product Quality\n\nAll too often, vulnerabilities are discovered too late in the development process and inadvertently pushed into production. With a testing environment in place, you can address underlying bugs and vulnerabilities more easily, resulting in a better quality product and fewer user complaints. \n\n#### Improve Testing Accuracy\n\nA dedicated test environment ensures greater accuracy and consistency when you analyze code because tests can be conducted in isolation from other activities and resources that may interfere with test results.\n\n### What is a Good Test Environment? Best Practices for Managing Test Environments\n\nThere is no single way to set up and manage test environments—developers tend to use different strategies depending on their workflows and preferences. However, here are a few best practices for managing test environments that can help you keep your test environments safe, effective, and efficient.  \n\n#### Document Your Test Environment\n\nThoroughly document the steps you take to set up test environments. Good documentation lets other team members replicate environments and ensures consistency.  \n\n#### Outline Testing Goals\n\nOutline your specific testing goals before moving code into a test environment to save time, prevent rework, and enable testing engineers to move with speed and confidence.  \n\n#### Secure Your Environment\n\nTest environments contain a wealth of data and can pose security risks. To [keep risk low](https://www.cio.com/article/219952/10-things-cios-must-absolutely-know-about-their-software.html), it’s critical to properly secure your test environment and restrict access through effective user authentication protocols. \n\n#### Don’t Strive for Perfection\n\nIt isn't always possible to create test environments that align perfectly with production environments. Attempting to create test environments that match production perfectly will only waste time and delay development. Test environments don’t have to be an exact match of production to perform well.  \n\n### How to Create a Test Environment\n\nCreating a test environment is a multistep process, which typically involves the following actions.  \n\n#### 1\\. Determine Your Objectives\n\nThe first step is to clearly define your objectives and outline your reasons for creating a test environment. Before moving on, make sure that a test environment is necessary for your specific use case. \n\n#### 2\\. Select an Environment\n\nThe next step is to pick a physical or virtual environment. At this point, you must determine whether you want to use the cloud or an on-prem server.  \n\n#### 3\\. Optimize Your Network\n\nA poorly performing network can negatively impact testing and lead to inaccurate results. It’s worth taking time to make sure that your network is optimized and secure. \n\n#### 4\\. Plan Your Data Strategy\n\nTest data plays a critical role in the software development process. You should have a plan in place for collecting, analyzing, and securing the data that you collect during software tests.  \n\n#### 5\\. Automate Test Environments to Improve Efficiency\n\nMost businesses now use automation to manage their test environments. For example, Jenkins is a popular free open-source tool that enables automated testing.\n\n![](<https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63a2b8f81084672248a5ab23_unnamed%20(3).png>)\n\n### Save Time with On-Demand Environments\n\nWithout a doubt, test environments are a crucial part of the software development process. However, they can take time to [set up and maintain](https://release.com/blog/setup-test-environment), which can slow things down and frustrate engineers.  \n\nTo save time, Release now offers on-demand environments as a service. With the help of Release, your team can instantly deploy fast, scalable, managed environments that are easy to replicate and access. Improve accuracy and reliability and free your developers to spend more time analyzing and optimizing code.  \n\nFor further reading on how to streamline development and build better applications, check out Release’s [complete guide to automated software environments](https://release.com/ebook/the-complete-guide-to-automated-software-environments).\n",
          "code": "var Component=(()=>{var m=Object.create;var o=Object.defineProperty;var h=Object.getOwnPropertyDescriptor;var p=Object.getOwnPropertyNames;var u=Object.getPrototypeOf,f=Object.prototype.hasOwnProperty;var v=(t,e)=>()=>(e||t((e={exports:{}}).exports,e),e.exports),g=(t,e)=>{for(var i in e)o(t,i,{get:e[i],enumerable:!0})},r=(t,e,i,s)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let a of p(e))!f.call(t,a)&&a!==i&&o(t,a,{get:()=>e[a],enumerable:!(s=h(e,a))||s.enumerable});return t};var y=(t,e,i)=>(i=t!=null?m(u(t)):{},r(e||!t||!t.__esModule?o(i,\"default\",{value:t,enumerable:!0}):i,t)),w=t=>r(o({},\"__esModule\",{value:!0}),t);var l=v((E,c)=>{c.exports=_jsx_runtime});var T={};g(T,{default:()=>N,frontmatter:()=>b});var n=y(l()),b={title:\"Test Environment: A Definition and How-to Guide\",summary:\"What a test environment is and how to set one up. \",publishDate:\"Wed Dec 21 2022 07:46:12 GMT+0000 (Coordinated Universal Time)\",author:\"erik-landerholm\",readingTime:6,categories:[\"platform-engineering\",\"product\"],mainImage:\"/blog-images/63498e869bdfbca60345e4f2217782dc.jpg\",imageAlt:\"A hand holding a magnifying glass over a laptop keyboard\",showCTA:!0,ctaCopy:\"Improve software quality with Release's on-demand environments for testing and staging, reducing errors and accelerating deployment.\",ctaLink:\"https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=test-environment-a-definition-and-how-to-guide\",relatedPosts:[\"\"],ogImage:\"/blog-images/63498e869bdfbca60345e4f2217782dc.jpg\",excerpt:\"What a test environment is and how to set one up. \",tags:[\"platform-engineering\",\"product\"],ctaButton:\"Try Release for Free\"};function d(t){let e=Object.assign({p:\"p\",a:\"a\",img:\"img\",h3:\"h3\",span:\"span\",h4:\"h4\"},t.components);return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsxs)(e.p,{children:[\"Developers today are under increasing pressure to produce high-quality software, with 89% of companies now competing primarily on \",(0,n.jsx)(e.a,{href:\"https://www.smartkarrot.com/resources/blog/customer-experience-statistics/#:~:text=General%20customer%20experience%20statistics&text=89%25%20of%20businesses%20compete%20primarily,warm%20and%20friendly%20customer%20experience.\",children:\"customer experience\"}),\". Today, customers expect high-quality software that\\u2019s secure, reliable, and easy to use. When you fail to meet those expectations, adoption of your product is likely to plummet.\\xA0\"]}),`\n`,(0,n.jsx)(e.p,{children:\"To ensure software quality, developers frequently choose to set up test environments where they can safely isolate code and analyze performance. Simply put, test environments help to reduce production errors, lower development costs, and improve customer satisfaction. \\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:\"This post offers a primer on test environments covering what a test environment is, how it fits into the software development life cycle, best practices for managing a test environment, and a step-by-step guide for setting one up. \\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/19e7c960a35759874587fd6518cb9526.jpg\",alt:\"\"})}),`\n`,(0,n.jsxs)(e.h3,{id:\"what-is-a-test-environment\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#what-is-a-test-environment\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"What is a Test Environment?\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"A test environment is a dedicated space\\u2014usually a server or container\\u2014for \",(0,n.jsx)(e.a,{href:\"https://www.forbes.com/sites/forbestechcouncil/2022/10/04/how-to-improve-the-quality-of-software-testing/\",children:\"running software tests\"}),\". You can add a copy of an application to a test environment to inspect it for bugs and vulnerabilities. Usually, creating a test environment involves setting up a physical or virtual server and configuring it according to the specifications of the application that\\u2019s being tested. \\xA0\"]}),`\n`,(0,n.jsx)(e.p,{children:\"A dedicated test environment helps you ensure accuracy and consistency when code is analyzed. Engineers can alter code and experiment with it in the virtual environment and assess the results in real-time. For example, a developer could use a test environment to check the compatibility between different code versions and address any underlying bugs.\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:\"Some projects may call for a single test environment while others may require multiple test environments for each access instance. Depending on your needs, you can either run tests in a fixed order or simultaneously in multiple environments.\"}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/2e316bb6210e988c5477e384509d4f32.png\",alt:\"\"})}),`\n`,(0,n.jsxs)(e.h3,{id:\"what-are-the-different-environments-testing-is-done-in\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#what-are-the-different-environments-testing-is-done-in\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"What are the Different Environments Testing is Done in?\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Many developer teams use dedicated testing environments, but you can also test your software in different kinds of environments. Let's explore what these testing environments are.\"}),`\n`,(0,n.jsxs)(e.h4,{id:\"development-environment\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#development-environment\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Development Environment\"]}),`\n`,(0,n.jsx)(e.p,{children:\"The development environment is where engineers write code and make changes. Preliminary testing also takes place in the development environment before advancing code to other stages. To illustrate, an engineer might set up a development environment to build an application and create a sample for additional testing and modifications.\\xA0\"}),`\n`,(0,n.jsxs)(e.h4,{id:\"staging-environment\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#staging-environment\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Staging Environment\"]}),`\n`,(0,n.jsx)(e.p,{children:\"After software has been through development and testing, it moves into a staging environment. The staging environment closely resembles live production. The main purpose of the staging environment is to provide a window of insight into how the software will perform after deployment. \\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:\"Companies often use staging environments to present final projects before moving them along for final testing and production. Staging environments also provide a chance for stakeholders to see a digital project in action, collaborate, and make adjustments. \\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63a2b88bf3c0d818f8a3effc_unnamed%20(1).png\",alt:\"\"})}),`\n`,(0,n.jsxs)(e.h4,{id:\"production-environment\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#production-environment\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Production Environment\"]}),`\n`,(0,n.jsx)(e.p,{children:\"The production environment is the final stage of software development, when the software is live and running on a production server. At this point, users can access and interact with the software in real-time. \\xA0For example, all software that you use\\u2014like Slack or Teams\\u2014is currently in a live production environment. \\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:\"Even though the software is live at this point, companies often continue to test and iterate on it in the production environment. Testing in production can provide better accuracy and allow for more frequent deployments.\\xA0\"}),`\n`,(0,n.jsxs)(e.h3,{id:\"why-should-you-use-a-software-testing-environment\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#why-should-you-use-a-software-testing-environment\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Why Should You Use a Software Testing Environment?\"]}),`\n`,(0,n.jsx)(e.p,{children:\"While testing in production is possible, there are several advantages to setting up a private, dedicated testing environment. \\xA0\"}),`\n`,(0,n.jsxs)(e.h4,{id:\"support-different-tests\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#support-different-tests\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Support Different Tests\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"It\\u2019s possible to run several different types of analysis within a testing environment, depending on your needs. For example, you can set up a test environment to run integration tests, unit tests, \",(0,n.jsx)(e.a,{href:\"https://release.com/blog/user-acceptance-testing-checklist\",children:\"user acceptance tests\"}),\", performance tests, alpha tests, and beta tests. \\xA0\"]}),`\n`,(0,n.jsxs)(e.h4,{id:\"save-money\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#save-money\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Save Money\"]}),`\n`,(0,n.jsx)(e.p,{children:\"In general, testing software earlier in the development life cycle eliminates rework, enables you to make changes more easily, and significantly reduces development costs. \\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63a2b8bc3f0f382c76568939_unnamed%20(2).png\",alt:\"\"})}),`\n`,(0,n.jsxs)(e.h4,{id:\"ensure-product-quality\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#ensure-product-quality\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Ensure Product Quality\"]}),`\n`,(0,n.jsx)(e.p,{children:\"All too often, vulnerabilities are discovered too late in the development process and inadvertently pushed into production. With a testing environment in place, you can address underlying bugs and vulnerabilities more easily, resulting in a better quality product and fewer user complaints.\\xA0\"}),`\n`,(0,n.jsxs)(e.h4,{id:\"improve-testing-accuracy\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#improve-testing-accuracy\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Improve Testing Accuracy\"]}),`\n`,(0,n.jsx)(e.p,{children:\"A dedicated test environment ensures greater accuracy and consistency when you analyze code because tests can be conducted in isolation from other activities and resources that may interfere with test results.\"}),`\n`,(0,n.jsxs)(e.h3,{id:\"what-is-a-good-test-environment-best-practices-for-managing-test-environments\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#what-is-a-good-test-environment-best-practices-for-managing-test-environments\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"What is a Good Test Environment? Best Practices for Managing Test Environments\"]}),`\n`,(0,n.jsx)(e.p,{children:\"There is no single way to set up and manage test environments\\u2014developers tend to use different strategies depending on their workflows and preferences. However, here are a few best practices for managing test environments that can help you keep your test environments safe, effective, and efficient. \\xA0\"}),`\n`,(0,n.jsxs)(e.h4,{id:\"document-your-test-environment\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#document-your-test-environment\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Document Your Test Environment\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Thoroughly document the steps you take to set up test environments. Good documentation lets other team members replicate environments and ensures consistency. \\xA0\"}),`\n`,(0,n.jsxs)(e.h4,{id:\"outline-testing-goals\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#outline-testing-goals\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Outline Testing Goals\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Outline your specific testing goals before moving code into a test environment to save time, prevent rework, and enable testing engineers to move with speed and confidence. \\xA0\"}),`\n`,(0,n.jsxs)(e.h4,{id:\"secure-your-environment\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#secure-your-environment\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Secure Your Environment\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"Test environments contain a wealth of data and can pose security risks. To \",(0,n.jsx)(e.a,{href:\"https://www.cio.com/article/219952/10-things-cios-must-absolutely-know-about-their-software.html\",children:\"keep risk low\"}),\", it\\u2019s critical to properly secure your test environment and restrict access through effective user authentication protocols.\\xA0\"]}),`\n`,(0,n.jsxs)(e.h4,{id:\"dont-strive-for-perfection\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#dont-strive-for-perfection\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Don\\u2019t Strive for Perfection\"]}),`\n`,(0,n.jsx)(e.p,{children:\"It isn't always possible to create test environments that align perfectly with production environments. Attempting to create test environments that match production perfectly will only waste time and delay development. Test environments don\\u2019t have to be an exact match of production to perform well. \\xA0\"}),`\n`,(0,n.jsxs)(e.h3,{id:\"how-to-create-a-test-environment\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#how-to-create-a-test-environment\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"How to Create a Test Environment\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Creating a test environment is a multistep process, which typically involves the following actions. \\xA0\"}),`\n`,(0,n.jsxs)(e.h4,{id:\"1-determine-your-objectives\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#1-determine-your-objectives\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"1. Determine Your Objectives\"]}),`\n`,(0,n.jsx)(e.p,{children:\"The first step is to clearly define your objectives and outline your reasons for creating a test environment. Before moving on, make sure that a test environment is necessary for your specific use case.\\xA0\"}),`\n`,(0,n.jsxs)(e.h4,{id:\"2-select-an-environment\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#2-select-an-environment\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"2. Select an Environment\"]}),`\n`,(0,n.jsx)(e.p,{children:\"The next step is to pick a physical or virtual environment. At this point, you must determine whether you want to use the cloud or an on-prem server. \\xA0\"}),`\n`,(0,n.jsxs)(e.h4,{id:\"3-optimize-your-network\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#3-optimize-your-network\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"3. Optimize Your Network\"]}),`\n`,(0,n.jsx)(e.p,{children:\"A poorly performing network can negatively impact testing and lead to inaccurate results. It\\u2019s worth taking time to make sure that your network is optimized and secure.\\xA0\"}),`\n`,(0,n.jsxs)(e.h4,{id:\"4-plan-your-data-strategy\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#4-plan-your-data-strategy\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"4. Plan Your Data Strategy\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Test data plays a critical role in the software development process. You should have a plan in place for collecting, analyzing, and securing the data that you collect during software tests. \\xA0\"}),`\n`,(0,n.jsxs)(e.h4,{id:\"5-automate-test-environments-to-improve-efficiency\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#5-automate-test-environments-to-improve-efficiency\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"5. Automate Test Environments to Improve Efficiency\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Most businesses now use automation to manage their test environments. For example, Jenkins is a popular free open-source tool that enables automated testing.\"}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63a2b8f81084672248a5ab23_unnamed%20(3).png\",alt:\"\"})}),`\n`,(0,n.jsxs)(e.h3,{id:\"save-time-with-on-demand-environments\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#save-time-with-on-demand-environments\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Save Time with On-Demand Environments\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"Without a doubt, test environments are a crucial part of the software development process. However, they can take time to \",(0,n.jsx)(e.a,{href:\"https://release.com/blog/setup-test-environment\",children:\"set up and maintain\"}),\", which can slow things down and frustrate engineers. \\xA0\"]}),`\n`,(0,n.jsx)(e.p,{children:\"To save time, Release now offers on-demand environments as a service. With the help of Release, your team can instantly deploy fast, scalable, managed environments that are easy to replicate and access. Improve accuracy and reliability and free your developers to spend more time analyzing and optimizing code. \\xA0\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"For further reading on how to streamline development and build better applications, check out Release\\u2019s \",(0,n.jsx)(e.a,{href:\"https://release.com/ebook/the-complete-guide-to-automated-software-environments\",children:\"complete guide to automated software environments\"}),\".\"]})]})}function k(t={}){let{wrapper:e}=t.components||{};return e?(0,n.jsx)(e,Object.assign({},t,{children:(0,n.jsx)(d,t)})):d(t)}var N=k;return w(T);})();\n;return Component;"
        },
        "_id": "blog/posts/test-environment-a-definition-and-how-to-guide.mdx",
        "_raw": {
          "sourceFilePath": "blog/posts/test-environment-a-definition-and-how-to-guide.mdx",
          "sourceFileName": "test-environment-a-definition-and-how-to-guide.mdx",
          "sourceFileDir": "blog/posts",
          "contentType": "mdx",
          "flattenedPath": "blog/posts/test-environment-a-definition-and-how-to-guide"
        },
        "type": "BlogPost",
        "computedSlug": "test-environment-a-definition-and-how-to-guide"
      },
      "documentHash": "1739393595028",
      "hasWarnings": false,
      "documentTypeName": "BlogPost"
    },
    "blog/posts/test-guild-webinar-the-secret-to-continuous-testing.mdx": {
      "document": {
        "title": "Test Guild Webinar: The Secret to Continuous Testing",
        "summary": "Tommy McClung joins Joe Colantonio on the Test Guild webinar to discuss the secret ingredient to continuous testing.",
        "publishDate": "Sun Mar 20 2022 21:22:28 GMT+0000 (Coordinated Universal Time)",
        "author": "sam-allen",
        "readingTime": 2,
        "categories": [
          "platform-engineering",
          "product"
        ],
        "mainImage": "/blog-images/03b07b7b575271632659ece4d5fcb9c0.jpg",
        "imageAlt": "The Secret Ingredient to Continuous Testing Webinar",
        "showCTA": true,
        "ctaCopy": "Improve continuous testing with on-demand environments for faster deployment cycles. Try Release now!",
        "ctaLink": "https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=test-guild-webinar-the-secret-to-continuous-testing",
        "relatedPosts": [
          ""
        ],
        "ogImage": "/blog-images/03b07b7b575271632659ece4d5fcb9c0.jpg",
        "excerpt": "Tommy McClung joins Joe Colantonio on the Test Guild webinar to discuss the secret ingredient to continuous testing.",
        "tags": [
          "platform-engineering",
          "product"
        ],
        "ctaButton": "Try Release for Free",
        "body": {
          "raw": "\nTommy McClung joins Joe Colantonio on the [Test Guild webinar](https://vimeo.com/680498142/7cea1c8bf0) to discuss the secret ingredient to continuous testing.\n\nTommy, the co-founder and CEO of Release, has been building scalable infrastructure for at least over 20 years. He’s also a serial entrepreneur. He co-founded CarWoo!, which was acquired by TrueCar.\n\nIf you want to know the secret to continuous testing, including people and processes, and why environments are key to many of the bottlenecks that engineers and developers face, you’re in the right place.\n\nTommy started his career at RLX Technologies, the inventor of the blade server, which was later sold to Hewlett Packard. Back then, there were no virtual machines, and the way that developers could improve productivity consisted of squishing as many servers into a rack as you possibly could. Then developers had to have software that could make it all work. At RLX technologies, they had installations of thousands and thousands of blade servers.\n\nLater on in his career, Tommy went on to start CarWoo!, an automotive company, which was later sold to TrueCar. At TrueCar, he was tasked with building a new product, and encountered many issues with environments. TrueCar had a very large QA team that was very dependent on environments. It was very difficult to build, deploy, and test code. The idea of continuous integration and continuous testing was not even possible because of the ecosystem and how few environments they had. Later, as CTO of TrueCar, Tommy set out to solve this environments issue so that his team could do true continuous deployment, continuous integration, and continuous testing. \n\nTommy has been working on this problem for many years. As a developer at heart, he cares about getting great ideas to the world quickly, and environments tend to be a key ingredient in making that happen. This is why he created Release and the concept of ephemeral environments, or “environments as a service”. With Release, developers can create on demand environments with the click of a button or via a pull request.\n\n[Watch the webinar](https://vimeo.com/680498142/7cea1c8bf0) below to see how on-demand environments work. Learn more about the variety of [use cases for ephemeral environments](https://release.com/use-cases): staging, production, QA, integration environments, customer facing environments, sandbox environments, and more. Environments are needed throughout the entire product development life cycle.\n\n‍\n\nWant to try Release for yourself? [Request a demo.](https://release.com/)\n",
          "code": "var Component=(()=>{var h=Object.create;var s=Object.defineProperty;var l=Object.getOwnPropertyDescriptor;var m=Object.getOwnPropertyNames;var g=Object.getPrototypeOf,p=Object.prototype.hasOwnProperty;var f=(t,e)=>()=>(e||t((e={exports:{}}).exports,e),e.exports),y=(t,e)=>{for(var o in e)s(t,o,{get:e[o],enumerable:!0})},i=(t,e,o,r)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let a of m(e))!p.call(t,a)&&a!==o&&s(t,a,{get:()=>e[a],enumerable:!(r=l(e,a))||r.enumerable});return t};var b=(t,e,o)=>(o=t!=null?h(g(t)):{},i(e||!t||!t.__esModule?s(o,\"default\",{value:t,enumerable:!0}):o,t)),v=t=>i(s({},\"__esModule\",{value:!0}),t);var d=f((x,c)=>{c.exports=_jsx_runtime});var k={};y(k,{default:()=>C,frontmatter:()=>w});var n=b(d()),w={title:\"Test Guild Webinar: The Secret to Continuous Testing\",summary:\"Tommy McClung joins Joe Colantonio on the Test Guild webinar to discuss the secret ingredient to continuous testing.\",publishDate:\"Sun Mar 20 2022 21:22:28 GMT+0000 (Coordinated Universal Time)\",author:\"sam-allen\",readingTime:2,categories:[\"platform-engineering\",\"product\"],mainImage:\"/blog-images/03b07b7b575271632659ece4d5fcb9c0.jpg\",imageAlt:\"The Secret Ingredient to Continuous Testing Webinar\",showCTA:!0,ctaCopy:\"Improve continuous testing with on-demand environments for faster deployment cycles. Try Release now!\",ctaLink:\"https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=test-guild-webinar-the-secret-to-continuous-testing\",relatedPosts:[\"\"],ogImage:\"/blog-images/03b07b7b575271632659ece4d5fcb9c0.jpg\",excerpt:\"Tommy McClung joins Joe Colantonio on the Test Guild webinar to discuss the secret ingredient to continuous testing.\",tags:[\"platform-engineering\",\"product\"],ctaButton:\"Try Release for Free\"};function u(t){let e=Object.assign({p:\"p\",a:\"a\"},t.components);return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsxs)(e.p,{children:[\"Tommy McClung joins Joe Colantonio on the \",(0,n.jsx)(e.a,{href:\"https://vimeo.com/680498142/7cea1c8bf0\",children:\"Test Guild webinar\"}),\" to discuss the secret ingredient to continuous testing.\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Tommy, the co-founder and CEO of Release, has been building scalable infrastructure for at least over 20 years. He\\u2019s also a serial entrepreneur. He co-founded CarWoo!, which was acquired by TrueCar.\"}),`\n`,(0,n.jsx)(e.p,{children:\"If you want to know the secret to continuous testing, including people and processes, and why environments are key to many of the bottlenecks that engineers and developers face, you\\u2019re in the right place.\"}),`\n`,(0,n.jsx)(e.p,{children:\"Tommy started his career at RLX Technologies, the inventor of the blade server, which was later sold to Hewlett Packard. Back then, there were no virtual machines, and the way that developers could improve productivity consisted of squishing as many servers into a rack as you possibly could. Then developers had to have software that could make it all work. At RLX technologies, they had installations of thousands and thousands of blade servers.\"}),`\n`,(0,n.jsx)(e.p,{children:\"Later on in his career, Tommy went on to start CarWoo!, an automotive company, which was later sold to TrueCar. At TrueCar, he was tasked with building a new product, and encountered many issues with environments. TrueCar had a very large QA team that was very dependent on environments. It was very difficult to build, deploy, and test code. The idea of continuous integration and continuous testing was not even possible because of the ecosystem and how few environments they had. Later, as CTO of TrueCar, Tommy set out to solve this environments issue so that his team could do true continuous deployment, continuous integration, and continuous testing.\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:\"Tommy has been working on this problem for many years. As a developer at heart, he cares about getting great ideas to the world quickly, and environments tend to be a key ingredient in making that happen. This is why he created Release and the concept of ephemeral environments, or \\u201Cenvironments as a service\\u201D. With Release, developers can create on demand environments with the click of a button or via a pull request.\"}),`\n`,(0,n.jsxs)(e.p,{children:[(0,n.jsx)(e.a,{href:\"https://vimeo.com/680498142/7cea1c8bf0\",children:\"Watch the webinar\"}),\" below to see how on-demand environments work. Learn more about the variety of \",(0,n.jsx)(e.a,{href:\"https://release.com/use-cases\",children:\"use cases for ephemeral environments\"}),\": staging, production, QA, integration environments, customer facing environments, sandbox environments, and more. Environments are needed throughout the entire product development life cycle.\"]}),`\n`,(0,n.jsx)(e.p,{children:\"\\u200D\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"Want to try Release for yourself? \",(0,n.jsx)(e.a,{href:\"https://release.com/\",children:\"Request a demo.\"})]})]})}function T(t={}){let{wrapper:e}=t.components||{};return e?(0,n.jsx)(e,Object.assign({},t,{children:(0,n.jsx)(u,t)})):u(t)}var C=T;return v(k);})();\n;return Component;"
        },
        "_id": "blog/posts/test-guild-webinar-the-secret-to-continuous-testing.mdx",
        "_raw": {
          "sourceFilePath": "blog/posts/test-guild-webinar-the-secret-to-continuous-testing.mdx",
          "sourceFileName": "test-guild-webinar-the-secret-to-continuous-testing.mdx",
          "sourceFileDir": "blog/posts",
          "contentType": "mdx",
          "flattenedPath": "blog/posts/test-guild-webinar-the-secret-to-continuous-testing"
        },
        "type": "BlogPost",
        "computedSlug": "test-guild-webinar-the-secret-to-continuous-testing"
      },
      "documentHash": "1739393595028",
      "hasWarnings": false,
      "documentTypeName": "BlogPost"
    },
    "blog/posts/testing-environment-types-and-what-theyre-used-for.mdx": {
      "document": {
        "title": "Testing Environment Types and What They're Used For",
        "summary": "Check out this article to learn about different testing environment types, what they're used for, and which one is right",
        "publishDate": "Mon Jan 09 2023 07:58:17 GMT+0000 (Coordinated Universal Time)",
        "author": "",
        "readingTime": 6,
        "categories": [
          "platform-engineering"
        ],
        "mainImage": "/blog-images/35d3a8c5eb5d8692679503d011fee3da.svg",
        "imageAlt": "Testing Environment Types and What They're Used For",
        "showCTA": true,
        "ctaCopy": "Improve code quality with accurate testing environments. Try Release for replicating production conditions effortlessly.",
        "ctaLink": "https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=testing-environment-types-and-what-theyre-used-for",
        "relatedPosts": [
          ""
        ],
        "ogImage": "/blog-images/35d3a8c5eb5d8692679503d011fee3da.svg",
        "excerpt": "Check out this article to learn about different testing environment types, what they're used for, and which one is right",
        "tags": [
          "platform-engineering"
        ],
        "ctaButton": "Try Release for Free",
        "body": {
          "raw": "\nThe testing environment is crucial for the success of any software project. It allows developers to test their code against a variety of conditions, including different operating systems, hardware configurations, and user scenarios. This makes it possible to identify and fix bugs before the software is released to the public. \n\nA well-designed testing environment can also help improve the quality of the code. By forcing developers to test their code under different conditions, they're more likely to write robust and error-free code. \n\nIn this blog, we'll look at different types of testing environments and what they're suitable for. There's no one-size-fits-all solution. We need to understand where and how to invest our time and effort. \n\n### What is a Testing Environment?\n\nA testing environment is a separate environment used for testing purposes. It's a set of conditions created to test a software application or system. It typically includes test data, tools, software, and the necessary hardware and networking infrastructure. \n\nA testing environment should be as similar as possible to the production environment in order to provide accurate results. However, it's often not possible or practical to create an exact replica of the production environment. Therefore, it's essential to carefully consider which aspects of the production environment are most important for testing and to create a testing environment that accurately reflects those aspects. \n\nCreating a [suitable testing environment](https://release.com/blog/setup-test-environment) is critical to the software development process and can significantly impact the final product's quality. \n\n‍\n\n![](/blog-images/47bf8e4d6bc87ec1a0886f6115b4444f.png)\n\n### Understanding Testing Environment Types\n\nThere are many different testing environments, each with its advantages and disadvantages. Some of the most common types of testing environments include: \n\n#### Security Testing Environment\n\nA security testing environment is a controlled environment where security testing can be conducted. This environment includes the necessary tools, resources, and personnel to safely and effectively conduct security testing. \n\nA security testing environment should be designed to mimic the production environment as closely as possible to accurately assess the system's security. To this end, the security testing environment should include the same hardware, software, data, and network configurations as the production environment. \n\nThis environment should be isolated from the production environment to prevent any potential damage to the system under test. This isolation can be accomplished through the use of physical or logical separation. \n\nA security testing environment should also be well-documented so that all stakeholders can understand the purpose and scope of the testing being conducted. This documentation should include a list of all tools and resources used and a detailed testing plan. \n\n#### Performance Testing Environment\n\nA performance testing environment is a testing setup used to assess a software application's performance. This testing is typically used to evaluate the response time of an application under various load conditions. \n\nThis environment typically includes a test client, a test server, and a network. The test client is used to generate load on the test server. The test server is used to host the application under test. The network is used to connect the test client and the test server. \n\nAn effective performance testing environment can help developers optimize their code for better performance and reliability. It also helps identify potential improvement areas in the development process itself. \n\n![](/blog-images/281cb9b431507eca023ca3b01c91ce16.png)\n\n#### System Integration Testing\n\nSystem Integration Testing (SIT) is software testing that verifies the interactions between various system components. It's typically performed after all individual unit tests have been completed to ensure that the multiple components of the system work together as intended. SIT can be used to test interfaces between different software applications and hardware and software components. \n\nSIT is an essential step in the software development process, as it can help identify errors and potential problems that may not have been apparent during unit testing. By testing the interactions between different system components, SIT can help ensure that the system is functioning correctly.\n\nSIT can be a time-consuming and complex process, particularly for large and complex systems. However, SIT is essential to ensure the system's quality before it is deployed. \n\n#### User Acceptance Testing\n\nUser acceptance testing ([UAT](<https://en.wikipedia.org/wiki/Acceptance_testing#User_acceptance_testing:~:text=product%20provider/developer).-,User%20acceptance%20testing,-%5Bedit%5D>)) verifies that a software application meets the business requirements of the end user. It's the last stage of testing before the software is released to the end user. It's also known as beta testing or end-user testing. \n\nThe goal of UAT is to ensure that the software application is fit for purpose and meets the user's expectations. UAT aims to identify any issues that may impact the user's ability to use the software application effectively. UAT is typically conducted by the end user, but it can also be performed by a third-party testing company. \n\nUAT is a vital part of the software development process and should be given the same attention as other types of testing, such as functional and system testing. It should be conducted in a controlled environment, such as a test lab, to ensure that any issues that are identified can be effectively managed. \n\nBy conducting [UAT](https://release.com/blog/user-acceptance-testing-best-practices), organizations can ensure that the software application meets the end user's needs and that any issues are identified and resolved before the software is released to the end user. As such, it shouldn't be overlooked. \n\n#### QA Environment\n\nA quality assurance environment (QA environment) is a set of hardware and software tools used to test the functionality of a computer system or application. QA environments simulate real-world conditions so that developers can identify and fix bugs before a product is released to customers. \n\nThere are many different types of QA environments, and the tools used in each environment vary depending on the project's specific needs. For example, a QA environment for a web-based application might include a web server, a database server, and a test client. A QA environment for a desktop application might consist of a test client and a set of test data. \n\nQA environments can be complex and expensive to set up and maintain. However, they're essential for ensuring the quality of a product before it is released to customers. \n\n### Best Practices for Setting Up a Testing Environment\n\nThere are various best practices to consider when setting up a testing environment. Let's discuss some of them. \n\n- Always create a dedicated testing environment. This ensures that testing can be carried out independently from other activities and that other factors do not affect the results.\n- Ensure that the testing environment is as similar as possible to the production environment. This includes factors such as using the same operating system and software versions and having the same hardware configurations.\n- Set up a well-defined process for setting up and maintaining the testing environment. This should include clear instructions for how to install and configure the environment, as well as how to keep it running smoothly.\n- Establish a robust system for managing and tracking changes to the testing environment. This helps ensure that changes are made safely and that the environment is always in a known state.\n\n### Testing Environment Types Conclusion\n\nIn the agile world, it's essential to have different types of testing environments in place. This allows for more comprehensive software testing and can help identify potential issues early on. \n\nIt was our pleasure to share with you what testing environment types are and some information on how they can be used. We hope this information will help you as you continue on your path to creating better test cases and meeting your testing goals. Check out [our latest guide](https://release.com/ebook/the-complete-guide-to-automated-software-environments) if you'd like to know more about automating software environments.\n",
          "code": "var Component=(()=>{var h=Object.create;var a=Object.defineProperty;var p=Object.getOwnPropertyDescriptor;var m=Object.getOwnPropertyNames;var u=Object.getPrototypeOf,f=Object.prototype.hasOwnProperty;var g=(n,e)=>()=>(e||n((e={exports:{}}).exports,e),e.exports),y=(n,e)=>{for(var s in e)a(n,s,{get:e[s],enumerable:!0})},r=(n,e,s,o)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let i of m(e))!f.call(n,i)&&i!==s&&a(n,i,{get:()=>e[i],enumerable:!(o=p(e,i))||o.enumerable});return n};var v=(n,e,s)=>(s=n!=null?h(u(n)):{},r(e||!n||!n.__esModule?a(s,\"default\",{value:n,enumerable:!0}):s,n)),b=n=>r(a({},\"__esModule\",{value:!0}),n);var l=g((x,c)=>{c.exports=_jsx_runtime});var A={};y(A,{default:()=>k,frontmatter:()=>w});var t=v(l()),w={title:\"Testing Environment Types and What They're Used For\",summary:\"Check out this article to learn about different testing environment types, what they're used for, and which one is right\",publishDate:\"Mon Jan 09 2023 07:58:17 GMT+0000 (Coordinated Universal Time)\",author:\"\",readingTime:6,categories:[\"platform-engineering\"],mainImage:\"/blog-images/35d3a8c5eb5d8692679503d011fee3da.svg\",imageAlt:\"Testing Environment Types and What They're Used For\",showCTA:!0,ctaCopy:\"Improve code quality with accurate testing environments. Try Release for replicating production conditions effortlessly.\",ctaLink:\"https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=testing-environment-types-and-what-theyre-used-for\",relatedPosts:[\"\"],ogImage:\"/blog-images/35d3a8c5eb5d8692679503d011fee3da.svg\",excerpt:\"Check out this article to learn about different testing environment types, what they're used for, and which one is right\",tags:[\"platform-engineering\"],ctaButton:\"Try Release for Free\"};function d(n){let e=Object.assign({p:\"p\",h3:\"h3\",a:\"a\",span:\"span\",img:\"img\",h4:\"h4\",ul:\"ul\",li:\"li\"},n.components);return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(e.p,{children:\"The testing environment is crucial for the success of any software project. It allows developers to test their code against a variety of conditions, including different operating systems, hardware configurations, and user scenarios. This makes it possible to identify and fix bugs before the software is released to the public.\\xA0\"}),`\n`,(0,t.jsx)(e.p,{children:\"A well-designed testing environment can also help improve the quality of the code. By forcing developers to test their code under different conditions, they're more likely to write robust and error-free code.\\xA0\"}),`\n`,(0,t.jsx)(e.p,{children:\"In this blog, we'll look at different types of testing environments and what they're suitable for. There's no one-size-fits-all solution. We need to understand where and how to invest our time and effort.\\xA0\"}),`\n`,(0,t.jsxs)(e.h3,{id:\"what-is-a-testing-environment\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#what-is-a-testing-environment\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"What is a Testing Environment?\"]}),`\n`,(0,t.jsx)(e.p,{children:\"A testing environment is a separate environment used for testing purposes. It's a set of conditions created to test a software application or system. It typically includes test data, tools, software, and the necessary hardware and networking infrastructure.\\xA0\"}),`\n`,(0,t.jsx)(e.p,{children:\"A testing environment should be as similar as possible to the production environment in order to provide accurate results. However, it's often not possible or practical to create an exact replica of the production environment. Therefore, it's essential to carefully consider which aspects of the production environment are most important for testing and to create a testing environment that accurately reflects those aspects.\\xA0\"}),`\n`,(0,t.jsxs)(e.p,{children:[\"Creating a \",(0,t.jsx)(e.a,{href:\"https://release.com/blog/setup-test-environment\",children:\"suitable testing environment\"}),\" is critical to the software development process and can significantly impact the final product's quality.\\xA0\"]}),`\n`,(0,t.jsx)(e.p,{children:\"\\u200D\"}),`\n`,(0,t.jsx)(e.p,{children:(0,t.jsx)(e.img,{src:\"/blog-images/47bf8e4d6bc87ec1a0886f6115b4444f.png\",alt:\"\"})}),`\n`,(0,t.jsxs)(e.h3,{id:\"understanding-testing-environment-types\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#understanding-testing-environment-types\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Understanding Testing Environment Types\"]}),`\n`,(0,t.jsx)(e.p,{children:\"There are many different testing environments, each with its advantages and disadvantages. Some of the most common types of testing environments include:\\xA0\"}),`\n`,(0,t.jsxs)(e.h4,{id:\"security-testing-environment\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#security-testing-environment\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Security Testing Environment\"]}),`\n`,(0,t.jsx)(e.p,{children:\"A security testing environment is a controlled environment where security testing can be conducted. This environment includes the necessary tools, resources, and personnel to safely and effectively conduct security testing.\\xA0\"}),`\n`,(0,t.jsx)(e.p,{children:\"A security testing environment should be designed to mimic the production environment as closely as possible to accurately assess the system's security. To this end, the security testing environment should include the same hardware, software, data, and network configurations as the production environment.\\xA0\"}),`\n`,(0,t.jsx)(e.p,{children:\"This environment should be isolated from the production environment to prevent any potential damage to the system under test. This isolation can be accomplished through the use of physical or logical separation.\\xA0\"}),`\n`,(0,t.jsx)(e.p,{children:\"A security testing environment should also be well-documented so that all stakeholders can understand the purpose and scope of the testing being conducted. This documentation should include a list of all tools and resources used and a detailed testing plan.\\xA0\"}),`\n`,(0,t.jsxs)(e.h4,{id:\"performance-testing-environment\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#performance-testing-environment\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Performance Testing Environment\"]}),`\n`,(0,t.jsx)(e.p,{children:\"A performance testing environment is a testing setup used to assess a software application's performance. This testing is typically used to evaluate the response time of an application under various load conditions.\\xA0\"}),`\n`,(0,t.jsx)(e.p,{children:\"This environment typically includes a test client, a test server, and a network. The test client is used to generate load on the test server. The test server is used to host the application under test. The network is used to connect the test client and the test server.\\xA0\"}),`\n`,(0,t.jsx)(e.p,{children:\"An effective performance testing environment can help developers optimize their code for better performance and reliability. It also helps identify potential improvement areas in the development process itself.\\xA0\"}),`\n`,(0,t.jsx)(e.p,{children:(0,t.jsx)(e.img,{src:\"/blog-images/281cb9b431507eca023ca3b01c91ce16.png\",alt:\"\"})}),`\n`,(0,t.jsxs)(e.h4,{id:\"system-integration-testing\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#system-integration-testing\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"System Integration Testing\"]}),`\n`,(0,t.jsx)(e.p,{children:\"System Integration Testing (SIT) is software testing that verifies the interactions between various system components. It's typically performed after all individual unit tests have been completed to ensure that the multiple components of the system work together as intended. SIT can be used to test interfaces between different software applications and hardware and software components.\\xA0\"}),`\n`,(0,t.jsx)(e.p,{children:\"SIT is an essential step in the software development process, as it can help identify errors and potential problems that may not have been apparent during unit testing. By testing the interactions between different system components, SIT can help ensure that the system is functioning correctly.\"}),`\n`,(0,t.jsx)(e.p,{children:\"SIT can be a time-consuming and complex process, particularly for large and complex systems. However, SIT is essential to ensure the system's quality before it is deployed.\\xA0\"}),`\n`,(0,t.jsxs)(e.h4,{id:\"user-acceptance-testing\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#user-acceptance-testing\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"User Acceptance Testing\"]}),`\n`,(0,t.jsxs)(e.p,{children:[\"User acceptance testing (\",(0,t.jsx)(e.a,{href:\"https://en.wikipedia.org/wiki/Acceptance_testing#User_acceptance_testing:~:text=product%20provider/developer).-,User%20acceptance%20testing,-%5Bedit%5D\",children:\"UAT\"}),\") verifies that a software application meets the business requirements of the end user. It's the last stage of testing before the software is released to the end user. It's also known as beta testing or end-user testing.\\xA0\"]}),`\n`,(0,t.jsx)(e.p,{children:\"The goal of UAT is to ensure that the software application is fit for purpose and meets the user's expectations. UAT aims to identify any issues that may impact the user's ability to use the software application effectively. UAT is typically conducted by the end user, but it can also be performed by a third-party testing company.\\xA0\"}),`\n`,(0,t.jsx)(e.p,{children:\"UAT is a vital part of the software development process and should be given the same attention as other types of testing, such as functional and system testing. It should be conducted in a controlled environment, such as a test lab, to ensure that any issues that are identified can be effectively managed.\\xA0\"}),`\n`,(0,t.jsxs)(e.p,{children:[\"By conducting \",(0,t.jsx)(e.a,{href:\"https://release.com/blog/user-acceptance-testing-best-practices\",children:\"UAT\"}),\", organizations can ensure that the software application meets the end user's needs and that any issues are identified and resolved before the software is released to the end user. As such, it shouldn't be overlooked.\\xA0\"]}),`\n`,(0,t.jsxs)(e.h4,{id:\"qa-environment\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#qa-environment\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"QA Environment\"]}),`\n`,(0,t.jsx)(e.p,{children:\"A quality assurance environment (QA environment) is a set of hardware and software tools used to test the functionality of a computer system or application. QA environments simulate real-world conditions so that developers can identify and fix bugs before a product is released to customers.\\xA0\"}),`\n`,(0,t.jsx)(e.p,{children:\"There are many different types of QA environments, and the tools used in each environment vary depending on the project's specific needs. For example, a QA environment for a web-based application might include a web server, a database server, and a test client. A QA environment for a desktop application might consist of a test client and a set of test data.\\xA0\"}),`\n`,(0,t.jsx)(e.p,{children:\"QA environments can be complex and expensive to set up and maintain. However, they're essential for ensuring the quality of a product before it is released to customers.\\xA0\"}),`\n`,(0,t.jsxs)(e.h3,{id:\"best-practices-for-setting-up-a-testing-environment\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#best-practices-for-setting-up-a-testing-environment\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Best Practices for Setting Up a Testing Environment\"]}),`\n`,(0,t.jsx)(e.p,{children:\"There are various best practices to consider when setting up a testing environment. Let's discuss some of them.\\xA0\"}),`\n`,(0,t.jsxs)(e.ul,{children:[`\n`,(0,t.jsx)(e.li,{children:\"Always create a dedicated testing environment. This ensures that testing can be carried out independently from other activities and that other factors do not affect the results.\"}),`\n`,(0,t.jsx)(e.li,{children:\"Ensure that the testing environment is as similar as possible to the production environment. This includes factors such as using the same operating system and software versions and having the same hardware configurations.\"}),`\n`,(0,t.jsx)(e.li,{children:\"Set up a well-defined process for setting up and maintaining the testing environment. This should include clear instructions for how to install and configure the environment, as well as how to keep it running smoothly.\"}),`\n`,(0,t.jsx)(e.li,{children:\"Establish a robust system for managing and tracking changes to the testing environment. This helps ensure that changes are made safely and that the environment is always in a known state.\"}),`\n`]}),`\n`,(0,t.jsxs)(e.h3,{id:\"testing-environment-types-conclusion\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#testing-environment-types-conclusion\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Testing Environment Types Conclusion\"]}),`\n`,(0,t.jsx)(e.p,{children:\"In the agile world, it's essential to have different types of testing environments in place. This allows for more comprehensive software testing and can help identify potential issues early on.\\xA0\"}),`\n`,(0,t.jsxs)(e.p,{children:[\"It was our pleasure to share with you what testing environment types are and some information on how they can be used. We hope this information will help you as you continue on your path to creating better test cases and meeting your testing goals. Check out \",(0,t.jsx)(e.a,{href:\"https://release.com/ebook/the-complete-guide-to-automated-software-environments\",children:\"our latest guide\"}),\" if you'd like to know more about automating software environments.\"]})]})}function T(n={}){let{wrapper:e}=n.components||{};return e?(0,t.jsx)(e,Object.assign({},n,{children:(0,t.jsx)(d,n)})):d(n)}var k=T;return b(A);})();\n;return Component;"
        },
        "_id": "blog/posts/testing-environment-types-and-what-theyre-used-for.mdx",
        "_raw": {
          "sourceFilePath": "blog/posts/testing-environment-types-and-what-theyre-used-for.mdx",
          "sourceFileName": "testing-environment-types-and-what-theyre-used-for.mdx",
          "sourceFileDir": "blog/posts",
          "contentType": "mdx",
          "flattenedPath": "blog/posts/testing-environment-types-and-what-theyre-used-for"
        },
        "type": "BlogPost",
        "computedSlug": "testing-environment-types-and-what-theyre-used-for"
      },
      "documentHash": "1739393595029",
      "hasWarnings": false,
      "documentTypeName": "BlogPost"
    },
    "blog/posts/the-release-mission.mdx": {
      "document": {
        "title": "The Release Mission",
        "summary": "Hey everyone, we’re Tommy, David and Erik, co-founders of Release. We wanted to take a minute and kick off our blog with",
        "publishDate": "Tue Jan 26 2021 22:22:08 GMT+0000 (Coordinated Universal Time)",
        "author": "tommy-mcclung",
        "readingTime": 6,
        "categories": [
          "platform-engineering",
          "product"
        ],
        "mainImage": "/blog-images/b90bc87bff92987fdc1d983029e60eaa.jpg",
        "imageAlt": "The man in the moon symbolizing the Release mission",
        "showCTA": true,
        "ctaCopy": "Simplify managing product velocity and complexity with Release's ephemeral environments for streamlined workflows and faster deployments.",
        "ctaLink": "https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=the-release-mission",
        "relatedPosts": [
          ""
        ],
        "ogImage": "/blog-images/b90bc87bff92987fdc1d983029e60eaa.jpg",
        "excerpt": "Hey everyone, we’re Tommy, David and Erik, co-founders of Release. We wanted to take a minute and kick off our blog with",
        "tags": [
          "platform-engineering",
          "product"
        ],
        "ctaButton": "Try Release for Free",
        "body": {
          "raw": "\nHey everyone, we’re Tommy, David and Erik, co-founders of Release. We wanted to take a minute and kick off our blog with a post about our mission at Release. We’re a mission driven organization working on one of the most important unsolved problems in all of technology.\n\n> _Release exists so great ideas can freely and easily make their way into the world._\n\n### Getting great ideas into the world is still too hard\n\nWe’ve been building software and products for the greater part of 20 years. We’ve been involved in every type of company from the smallest startups (just the three of us) to being a part of the growth of a unicorn (David at Etsy) to becoming the technology leadership team at a public company (TC). There is one thing that has remained a constant throughout our entire careers in technology. Great ideas are trapped behind systems that are too difficult to manage and great ideas remain stuck in the minds of brilliant people with no way to unleash them on the world.\n\nEvery organization has some velocity in which they are delivering ideas. It’s a function of the number of people in the organization times how easy a change is to make and release. What tends to happen is these two numbers are inversely correlated. The larger the number of people, the harder it is to release ideas and changes. Why is this?\n\nIn the beginning, when you’re a startup, the number of people is small but the ability to change the system is incredibly high. Primarily because no one is paying attention and you have nothing to lose. You can release software, it can break and no one knows. So you just go about releasing as fast as possible. Things break. No one cares. You either have no users or very few early adopters and they expect things to break. You’re optimizing for velocity over all else and it is an amazing time of delivery. (and happiness for everyone).\n\nAs you start to add customers and employees you can feel product velocity slowing. You become more careful, you have more to lose. You start putting checks and balances in place to ensure changes have high quality and meet your users needs. You begin protecting what you’ve built and for good reason. You’ve got customers paying you, employees who’s lives depend on these customers and the products you’re delivering. You now have something to lose. The bigger you get, the more you have to lose.\n\n### Managing complexity as you grow gets harder\n\nAt the core of these product velocity/release issues is how difficult technology is to manage as things scale. As you add engineers, you add capacity to change and introduce new products but you’ve also added a ton of ways things can now break and your systems get more and more complex. Every change you introduce into your technology ecosystem introduces a risk or a reward.\n\nBeing careful becomes what you do. You start testing more diligently, you may start automating things. All in an effort to keep velocity high and minimize your risk. As you become more diligent, you add more people to either add new things or protect against things breaking. (Automation, QA, etc…). With more people, more risk, more testing, more automation… the cycle continues.\n\nI’m sure if you could chart output per employee at every company. You’d find that in every organization, the output per employee is inversely correlated to the number of people in the organization. The more employees you have, the more complex everything gets and the less each individual employee produces.\n\nAs product velocity slows, other issues emerge. There becomes a time when the output per employee drops to a level that only so many things can get done as more change is introduced into the system. The systems can’t handle the change… so now ideas start to get trapped. You have to meet and make decisions on what ideas will you attempt. It’s easy for ideas to be ignored and not attempted on their merits any longer. They are attempted based on who has the most seniority to make a decision in an organization. Politics emerge, ideas are trapped.\n\nAt the core of this problem are the systems the organization uses to get their ideas into the world. The better the systems are to handle change, the more ideas they can try. The less ideas remain trapped and the more the organization can grow.\n\n### You’re at a disadvantage\n\nYou are at a disadvantage to the great technology companies. They understand this issue and have overcome it, otherwise they wouldn’t be where they are. They have solved slowing innovation by investing heavily in their internal systems and platforms.\n\nI just did a quick search on Facebook’s job hiring site as of the date of this writing… There are currently 338 open Infrastructure positions at Facebook. 338. The same search for the number of software engineering roles open at Facebook turned up 237. Over 100 more infrastructure roles than software engineers. That was even more unexpected than I though.\n\nHere’s the same data on Google. 778 Software Engineering Roles. 1072 Infrastructure roles, 200 DevOps roles.\n\nIf one company’s platform to allow change (with low risk) is higher than another’s, they have an inherent strategic advantage. They can adapt faster, they can attempt new business lines faster, they can outmaneuver competitors because they are fast. They’ve found a way to increase productivity per employee and scale at the same time.\n\nBut it comes at a massive cost which is prohibitively high for most organizations.\n\nThere are two things that are alarming for companies that are not Facebook or Google (or any other mega company). The first is you can’t compete with them for talent. They will always be able to pay DevOps and infrastructure engineers more than you. So the best in the world aren’t going to be working for you. The second is, if you aren’t investing in your infrastructure, you’re not moving as fast as you could… and ideas in your organization are trapped. A good measure to know if you’re in this situation is to think about the politics in your organization. If you feel like you have a lot of politics, you probably have trapped ideas.\n\n### Why we started Release and our Mission\n\nIn every company we’ve been involved in from the smallest startups to the public companies, we’ve had to combat slowing innovation. We’ve had to invest heavily in building platforms to enable rapid delivery. Nothing existed in the world that just got the job done. We had to stitch together solutions, hire teams, spend millions of dollars to try and make our engineering orgs move fast. And even then, we had to keep building on it, keep spending to maintain and evolve our platforms.\n\nAnd guess what? It wasn’t our core competency as a business, but we knew if we didn’t invest heavily we wouldn’t be able to innovate We knew, for a fact, that great ideas were trapped and we were in an endless cat and mouse game. One could argue that building these internal systems enables customer happiness, but I would argue that largely these are solved problems and a distraction for most organizations.\n\nRelease exists so ideas can freely make their way to the world. When ideas can freely be released, the best ideas will emerge, politics will lessen and power structures in organizations will become less important. To achieve this mission we’re building the technological capability that supports rapid change that is accessible to everyone. We’re starting with environment management because we believe it’s one of the hardest unsolved problems, but our driving force is enabling companies to deliver ideas fast.\n",
          "code": "var Component=(()=>{var c=Object.create;var i=Object.defineProperty;var m=Object.getOwnPropertyDescriptor;var u=Object.getOwnPropertyNames;var g=Object.getPrototypeOf,p=Object.prototype.hasOwnProperty;var y=(o,e)=>()=>(e||o((e={exports:{}}).exports,e),e.exports),f=(o,e)=>{for(var a in e)i(o,a,{get:e[a],enumerable:!0})},r=(o,e,a,s)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let n of u(e))!p.call(o,n)&&n!==a&&i(o,n,{get:()=>e[n],enumerable:!(s=m(e,n))||s.enumerable});return o};var w=(o,e,a)=>(a=o!=null?c(g(o)):{},r(e||!o||!o.__esModule?i(a,\"default\",{value:o,enumerable:!0}):a,o)),b=o=>r(i({},\"__esModule\",{value:!0}),o);var l=y((j,h)=>{h.exports=_jsx_runtime});var x={};f(x,{default:()=>T,frontmatter:()=>v});var t=w(l()),v={title:\"The Release Mission\",summary:\"Hey everyone, we\\u2019re Tommy, David and Erik, co-founders of Release. We wanted to take a minute and kick off our blog with\",publishDate:\"Tue Jan 26 2021 22:22:08 GMT+0000 (Coordinated Universal Time)\",author:\"tommy-mcclung\",readingTime:6,categories:[\"platform-engineering\",\"product\"],mainImage:\"/blog-images/b90bc87bff92987fdc1d983029e60eaa.jpg\",imageAlt:\"The man in the moon symbolizing the Release mission\",showCTA:!0,ctaCopy:\"Simplify managing product velocity and complexity with Release's ephemeral environments for streamlined workflows and faster deployments.\",ctaLink:\"https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=the-release-mission\",relatedPosts:[\"\"],ogImage:\"/blog-images/b90bc87bff92987fdc1d983029e60eaa.jpg\",excerpt:\"Hey everyone, we\\u2019re Tommy, David and Erik, co-founders of Release. We wanted to take a minute and kick off our blog with\",tags:[\"platform-engineering\",\"product\"],ctaButton:\"Try Release for Free\"};function d(o){let e=Object.assign({p:\"p\",blockquote:\"blockquote\",em:\"em\",h3:\"h3\",a:\"a\",span:\"span\"},o.components);return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(e.p,{children:\"Hey everyone, we\\u2019re Tommy, David and Erik, co-founders of Release. We wanted to take a minute and kick off our blog with a post about our mission at Release. We\\u2019re a mission driven organization working on one of the most important unsolved problems in all of technology.\"}),`\n`,(0,t.jsxs)(e.blockquote,{children:[`\n`,(0,t.jsx)(e.p,{children:(0,t.jsx)(e.em,{children:\"Release exists so great ideas can freely and easily make their way into the world.\"})}),`\n`]}),`\n`,(0,t.jsxs)(e.h3,{id:\"getting-great-ideas-into-the-world-is-still-too-hard\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#getting-great-ideas-into-the-world-is-still-too-hard\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Getting great ideas into the world is still too hard\"]}),`\n`,(0,t.jsx)(e.p,{children:\"We\\u2019ve been building software and products for the greater part of 20 years. We\\u2019ve been involved in every type of company from the smallest startups (just the three of us) to being a part of the growth of a unicorn (David at Etsy) to becoming the technology leadership team at a public company (TC). There is one thing that has remained a constant throughout our entire careers in technology. Great ideas are trapped behind systems that are too difficult to manage and great ideas remain stuck in the minds of brilliant people with no way to unleash them on the world.\"}),`\n`,(0,t.jsx)(e.p,{children:\"Every organization has some velocity in which they are delivering ideas. It\\u2019s a function of the number of people in the organization times how easy a change is to make and release. What tends to happen is these two numbers are inversely correlated. The larger the number of people, the harder it is to release ideas and changes. Why is this?\"}),`\n`,(0,t.jsx)(e.p,{children:\"In the beginning, when you\\u2019re a startup, the number of people is small but the ability to change the system is incredibly high. Primarily because no one is paying attention and you have nothing to lose. You can release software, it can break and no one knows. So you just go about releasing as fast as possible. Things break. No one cares. You either have no users or very few early adopters and they expect things to break. You\\u2019re optimizing for velocity over all else and it is an amazing time of delivery. (and happiness for everyone).\"}),`\n`,(0,t.jsx)(e.p,{children:\"As you start to add customers and employees you can feel product velocity slowing. You become more careful, you have more to lose. You start putting checks and balances in place to ensure changes have high quality and meet your users needs. You begin protecting what you\\u2019ve built and for good reason. You\\u2019ve got customers paying you, employees who\\u2019s lives depend on these customers and the products you\\u2019re delivering. You now have something to lose. The bigger you get, the more you have to lose.\"}),`\n`,(0,t.jsxs)(e.h3,{id:\"managing-complexity-as-you-grow-gets-harder\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#managing-complexity-as-you-grow-gets-harder\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Managing complexity as you grow gets harder\"]}),`\n`,(0,t.jsx)(e.p,{children:\"At the core of these product velocity/release issues is how difficult technology is to manage as things scale. As you add engineers, you add capacity to change and introduce new products but you\\u2019ve also added a ton of ways things can now break and your systems get more and more complex. Every change you introduce into your technology ecosystem introduces a risk or a reward.\"}),`\n`,(0,t.jsx)(e.p,{children:\"Being careful becomes what you do. You start testing more diligently, you may start automating things. All in an effort to keep velocity high and minimize your risk. As you become more diligent, you add more people to either add new things or protect against things breaking. (Automation, QA, etc\\u2026). With more people, more risk, more testing, more automation\\u2026 the cycle continues.\"}),`\n`,(0,t.jsx)(e.p,{children:\"I\\u2019m sure if you could chart output per employee at every company. You\\u2019d find that in every organization, the output per employee is inversely correlated to the number of people in the organization. The more employees you have, the more complex everything gets and the less each individual employee produces.\"}),`\n`,(0,t.jsx)(e.p,{children:\"As product velocity slows, other issues emerge. There becomes a time when the output per employee drops to a level that only so many things can get done as more change is introduced into the system. The systems can\\u2019t handle the change\\u2026 so now ideas start to get trapped. You have to meet and make decisions on what ideas will you attempt. It\\u2019s easy for ideas to be ignored and not attempted on their merits any longer. They are attempted based on who has the most seniority to make a decision in an organization. Politics emerge, ideas are trapped.\"}),`\n`,(0,t.jsx)(e.p,{children:\"At the core of this problem are the systems the organization uses to get their ideas into the world. The better the systems are to handle change, the more ideas they can try. The less ideas remain trapped and the more the organization can grow.\"}),`\n`,(0,t.jsxs)(e.h3,{id:\"youre-at-a-disadvantage\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#youre-at-a-disadvantage\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"You\\u2019re at a disadvantage\"]}),`\n`,(0,t.jsx)(e.p,{children:\"You are at a disadvantage to the great technology companies. They understand this issue and have overcome it, otherwise they wouldn\\u2019t be where they are. They have solved slowing innovation by investing heavily in their internal systems and platforms.\"}),`\n`,(0,t.jsx)(e.p,{children:\"I just did a quick search on Facebook\\u2019s job hiring site as of the date of this writing\\u2026 There are currently 338 open Infrastructure positions at Facebook. 338. The same search for the number of software engineering roles open at Facebook turned up 237. Over 100 more infrastructure roles than software engineers. That was even more unexpected than I though.\"}),`\n`,(0,t.jsx)(e.p,{children:\"Here\\u2019s the same data on Google. 778 Software Engineering Roles. 1072 Infrastructure roles, 200 DevOps roles.\"}),`\n`,(0,t.jsx)(e.p,{children:\"If one company\\u2019s platform to allow change (with low risk) is higher than another\\u2019s, they have an inherent strategic advantage. They can adapt faster, they can attempt new business lines faster, they can outmaneuver competitors because they are fast. They\\u2019ve found a way to increase productivity per employee and scale at the same time.\"}),`\n`,(0,t.jsx)(e.p,{children:\"But it comes at a massive cost which is prohibitively high for most organizations.\"}),`\n`,(0,t.jsx)(e.p,{children:\"There are two things that are alarming for companies that are not Facebook or Google (or any other mega company). The first is you can\\u2019t compete with them for talent. They will always be able to pay DevOps and infrastructure engineers more than you. So the best in the world aren\\u2019t going to be working for you. The second is, if you aren\\u2019t investing in your infrastructure, you\\u2019re not moving as fast as you could\\u2026 and ideas in your organization are trapped. A good measure to know if you\\u2019re in this situation is to think about the politics in your organization. If you feel like you have a lot of politics, you probably have trapped ideas.\"}),`\n`,(0,t.jsxs)(e.h3,{id:\"why-we-started-release-and-our-mission\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#why-we-started-release-and-our-mission\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Why we started Release and our Mission\"]}),`\n`,(0,t.jsx)(e.p,{children:\"In every company we\\u2019ve been involved in from the smallest startups to the public companies, we\\u2019ve had to combat slowing innovation. We\\u2019ve had to invest heavily in building platforms to enable rapid delivery. Nothing existed in the world that just got the job done. We had to stitch together solutions, hire teams, spend millions of dollars to try and make our engineering orgs move fast. And even then, we had to keep building on it, keep spending to maintain and evolve our platforms.\"}),`\n`,(0,t.jsx)(e.p,{children:\"And guess what? It wasn\\u2019t our core competency as a business, but we knew if we didn\\u2019t invest heavily we wouldn\\u2019t be able to innovate We knew, for a fact, that great ideas were trapped and we were in an endless cat and mouse game. One could argue that building these internal systems enables customer happiness, but I would argue that largely these are solved problems and a distraction for most organizations.\"}),`\n`,(0,t.jsx)(e.p,{children:\"Release exists so ideas can freely make their way to the world. When ideas can freely be released, the best ideas will emerge, politics will lessen and power structures in organizations will become less important. To achieve this mission we\\u2019re building the technological capability that supports rapid change that is accessible to everyone. We\\u2019re starting with environment management because we believe it\\u2019s one of the hardest unsolved problems, but our driving force is enabling companies to deliver ideas fast.\"})]})}function k(o={}){let{wrapper:e}=o.components||{};return e?(0,t.jsx)(e,Object.assign({},o,{children:(0,t.jsx)(d,o)})):d(o)}var T=k;return b(x);})();\n;return Component;"
        },
        "_id": "blog/posts/the-release-mission.mdx",
        "_raw": {
          "sourceFilePath": "blog/posts/the-release-mission.mdx",
          "sourceFileName": "the-release-mission.mdx",
          "sourceFileDir": "blog/posts",
          "contentType": "mdx",
          "flattenedPath": "blog/posts/the-release-mission"
        },
        "type": "BlogPost",
        "computedSlug": "the-release-mission"
      },
      "documentHash": "1739393595029",
      "hasWarnings": false,
      "documentTypeName": "BlogPost"
    },
    "blog/posts/the-value-of-data-obfuscation-for-instant-datasets-tonic-meets-release.mdx": {
      "document": {
        "title": "The Value of Data Obfuscation for Instant Datasets: Tonic Meets Release",
        "summary": "In this post, Tonic CEO Andrew Colombo shares his views on the value of data obfuscation for software testing and more",
        "publishDate": "Mon Aug 28 2023 16:00:20 GMT+0000 (Coordinated Universal Time)",
        "author": "andrew-colombi",
        "readingTime": 5,
        "categories": [
          "platform-engineering",
          "product"
        ],
        "mainImage": "/blog-images/5f19a30bfdf0de373a070fa80cd83151.jpg",
        "imageAlt": "The Value of Data Obfuscation for Instant Datasets: Tonic Meets Release",
        "showCTA": true,
        "ctaCopy": "Enhance data privacy in testing with Release's ephemeral environments, ensuring secure, accurate testing experiences. Streamline workflows now!",
        "ctaLink": "https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=the-value-of-data-obfuscation-for-instant-datasets-tonic-meets-release",
        "relatedPosts": [
          "introducing-standalone-instant-datasets-build-and-test-with-realistic-production-like-data-with-ease"
        ],
        "ogImage": "/blog-images/5f19a30bfdf0de373a070fa80cd83151.jpg",
        "excerpt": "In this post, Tonic CEO Andrew Colombo shares his views on the value of data obfuscation for software testing and more",
        "tags": [
          "platform-engineering",
          "product"
        ],
        "ctaButton": "Try Release for Free",
        "body": {
          "raw": "\nDevelopers and DevOps teams know and appreciate the importance of data privacy. We don’t have to tell you twice that your company’s reputation and ability to hold on to your customers hinges on their trust in your company to keep their data safe.\n\n‍[Tonic](https://www.tonic.ai/)’s test data platform is the industry-leading one-stop-shop for safe, realistic data for lower environments. [Release](https://release.com/) and [Tonic](https://www.tonic.ai/) share the same goal: getting production-like data into development and staging environments as efficiently as possible to streamline the development cycle and enhance software quality. With the launch of Release’s [Instant Datasets](https://release.com/product/instant-datasets) and their native integration with Tonic, we’re excited to make obfuscated test data easily accessible in your ephemeral environments as well. A match made in developer productivity heaven.\n\nIn this blog post I’ll discuss the value of data obfuscation for software testing and why the marriage between data obfuscation and reliable, isolated, and replicable development and testing environments is a love story for the ages.\n\nAlready excited to engage more in the conversation? Sign-up for the [September webinar](https://release.com/webinar/release-tonic-best-practices) with Release’s CTO and co-founder Erik Landerholm and myself, Tonic’s CTO and co-founder Andrew Colombi, to learn best practices for getting developers the data they need!\n\n### Understanding Data Obfuscation\n\nData obfuscation techniques are especially useful for staging and testing as they can strike the perfect balance between data privacy and data utility. Obfuscation techniques such as masking, encryption, generalization, randomization, and tokenization effectively hide sensitive information while maintaining the integrity of your production data to allow for the most accurate and secure testing experience.\n\nThere are several different reasons why you should be thinking about obfuscating your test data:\n\n1.  **General privacy protection -** Your users, customers, and patients entrust you with their personally identifiable information, financial details, and medical records. Your reputation as a reliable, trust-worthy company hinges on keeping that information safe.\n2.  **Regulatory compliance -** Your company not only has to answer to your customers, but also to the powers that be. Data protection policies such as GDPR, HIPAA, and CCPA are all examples of ways organizations are held accountable to maintaining the privacy of those individuals they collect data from.\n3.  **Data sharing -** Of course your team is talented, but fresh perspectives and specialized skill sets can improve any brand or product. If data needs to be shared with those outside your organization such as researchers or partners, data obfuscation can be used to do so without compromising your sensitive data.\n4.  **Testing and development -** Develop, test, test, test, release - that should be your workflow to push out the best software possible. But along the way, data used for software development and testing can be at risk of exposure. Obfuscating your production databases is one way of allowing developers to access the data they need to ship the highest quality products while still respecting the privacy of your customers.\n\nNo matter what type of sensitive data you have, data obfuscation can be done using a variety of techniques:\n\n- **Tokenization -** replacing sensitive data with tokens, or random strings of characters, while the original data is securely stored.\n- **Encryption -** transforming data into a format that is unreadable by an attacker using complex algorithms and the maintenance of a decryption key so that the data can be transformed back.\n- **Masking -** a set of techniques to hide a certain subset of sensitive data.\n- **Generalization** **\\-** reducing the granularity and specificity of data by aggregating it or putting it into a more broad format.\n- **Randomization -** perturbing data or adding random noise to it in order to make it challenging to decipher individuals’ information.\n- **Synthetic data generation -** creating new data based on the patterns in a real dataset or based on rules defined by a user.\n\nThe good news is that we at Tonic have built our platform to seamlessly offer all of the above obfuscation methods and streamline your success in implementing them in your testing environments. And with the Release Instant Datasets, it’s that much easier to access the exact dataset you need.\n\n### Data Obfuscation for Software Testing\n\nTesting applications on production data is a dangerous game. Test data obfuscation techniques create a dataset that is safe for use in testing of all kinds.\n\nWith Tonic you can customize which, how, and where you obfuscate your production data as well as who has access to what. This allows for flexible workflows making sure that everyone gets the data they need to deliver at the highest caliber.\n\nCombining obfuscated data with ephemeral environments takes software testing to the next level. Ephemeral environments allow engineers the ability to work in reliable and isolated testing environments, accelerate their feedback cycles for quicker bug fixes, and boost collaboration to ultimately produce higher-quality software and better user experiences. We’re big fans of the solutions [Release](https://release.com/) provides.\n\nCruise on over to the [Tonic.ai blog](https://www.tonic.ai/blog/test-your-apps-with-high-fidelity-production-like-data-with-release-and-tonic) and check out Release’s CTO and cofounder Erik Landerholm’s guest blog post about the integration and sign up for our collaborative [September webinar](https://release.com/webinar/release-tonic-best-practices) to learn more about how to use production-like, secure data for your application development and testing processes.\n",
          "code": "var Component=(()=>{var h=Object.create;var i=Object.defineProperty;var u=Object.getOwnPropertyDescriptor;var f=Object.getOwnPropertyNames;var p=Object.getPrototypeOf,g=Object.prototype.hasOwnProperty;var m=(a,e)=>()=>(e||a((e={exports:{}}).exports,e),e.exports),b=(a,e)=>{for(var n in e)i(a,n,{get:e[n],enumerable:!0})},r=(a,e,n,s)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let o of f(e))!g.call(a,o)&&o!==n&&i(a,o,{get:()=>e[o],enumerable:!(s=u(e,o))||s.enumerable});return a};var y=(a,e,n)=>(n=a!=null?h(p(a)):{},r(e||!a||!a.__esModule?i(n,\"default\",{value:a,enumerable:!0}):n,a)),w=a=>r(i({},\"__esModule\",{value:!0}),a);var l=m((C,c)=>{c.exports=_jsx_runtime});var x={};b(x,{default:()=>T,frontmatter:()=>v});var t=y(l()),v={title:\"The Value of Data Obfuscation for Instant Datasets: Tonic Meets Release\",summary:\"In this post, Tonic CEO Andrew Colombo shares his views on the value of data obfuscation for software testing and more\",publishDate:\"Mon Aug 28 2023 16:00:20 GMT+0000 (Coordinated Universal Time)\",author:\"andrew-colombi\",readingTime:5,categories:[\"platform-engineering\",\"product\"],mainImage:\"/blog-images/5f19a30bfdf0de373a070fa80cd83151.jpg\",imageAlt:\"The Value of Data Obfuscation for Instant Datasets: Tonic Meets Release\",showCTA:!0,ctaCopy:\"Enhance data privacy in testing with Release's ephemeral environments, ensuring secure, accurate testing experiences. Streamline workflows now!\",ctaLink:\"https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=the-value-of-data-obfuscation-for-instant-datasets-tonic-meets-release\",relatedPosts:[\"introducing-standalone-instant-datasets-build-and-test-with-realistic-production-like-data-with-ease\"],ogImage:\"/blog-images/5f19a30bfdf0de373a070fa80cd83151.jpg\",excerpt:\"In this post, Tonic CEO Andrew Colombo shares his views on the value of data obfuscation for software testing and more\",tags:[\"platform-engineering\",\"product\"],ctaButton:\"Try Release for Free\"};function d(a){let e=Object.assign({p:\"p\",a:\"a\",h3:\"h3\",span:\"span\",ol:\"ol\",li:\"li\",strong:\"strong\",ul:\"ul\"},a.components);return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(e.p,{children:\"Developers and DevOps teams know and appreciate the importance of data privacy. We don\\u2019t have to tell you twice that your company\\u2019s reputation and ability to hold on to your customers hinges on their trust in your company to keep their data safe.\"}),`\n`,(0,t.jsxs)(e.p,{children:[\"\\u200D\",(0,t.jsx)(e.a,{href:\"https://www.tonic.ai/\",children:\"Tonic\"}),\"\\u2019s test data platform is the industry-leading one-stop-shop for safe, realistic data for lower environments. \",(0,t.jsx)(e.a,{href:\"https://release.com/\",children:\"Release\"}),\" and \",(0,t.jsx)(e.a,{href:\"https://www.tonic.ai/\",children:\"Tonic\"}),\" share the same goal: getting production-like data into development and staging environments as efficiently as possible to streamline the development cycle and enhance software quality. With the launch of Release\\u2019s \",(0,t.jsx)(e.a,{href:\"https://release.com/product/instant-datasets\",children:\"Instant Datasets\"}),\" and their native integration with Tonic, we\\u2019re excited to make obfuscated test data easily accessible in your ephemeral environments as well. A match made in developer productivity heaven.\"]}),`\n`,(0,t.jsx)(e.p,{children:\"In this blog post I\\u2019ll discuss the value of data obfuscation for software testing and why the marriage between data obfuscation and reliable, isolated, and replicable development and testing environments is a love story for the ages.\"}),`\n`,(0,t.jsxs)(e.p,{children:[\"Already excited to engage more in the conversation? Sign-up for the \",(0,t.jsx)(e.a,{href:\"https://release.com/webinar/release-tonic-best-practices\",children:\"September webinar\"}),\" with Release\\u2019s CTO and co-founder Erik Landerholm and myself, Tonic\\u2019s CTO and co-founder Andrew Colombi, to learn best practices for getting developers the data they need!\"]}),`\n`,(0,t.jsxs)(e.h3,{id:\"understanding-data-obfuscation\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#understanding-data-obfuscation\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Understanding Data Obfuscation\"]}),`\n`,(0,t.jsx)(e.p,{children:\"Data obfuscation techniques are especially useful for staging and testing as they can strike the perfect balance between data privacy and data utility. Obfuscation techniques such as masking, encryption, generalization, randomization, and tokenization effectively hide sensitive information while maintaining the integrity of your production data to allow for the most accurate and secure testing experience.\"}),`\n`,(0,t.jsx)(e.p,{children:\"There are several different reasons why you should be thinking about obfuscating your test data:\"}),`\n`,(0,t.jsxs)(e.ol,{children:[`\n`,(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:\"General privacy protection -\"}),\" Your users, customers, and patients entrust you with their personally identifiable information, financial details, and medical records. Your reputation as a reliable, trust-worthy company hinges on keeping that information safe.\"]}),`\n`,(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:\"Regulatory compliance -\"}),\" Your company not only has to answer to your customers, but also to the powers that be. Data protection policies such as GDPR, HIPAA, and CCPA are all examples of ways organizations are held accountable to maintaining the privacy of those individuals they collect data from.\"]}),`\n`,(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:\"Data sharing -\"}),\" Of course your team is talented, but fresh perspectives and specialized skill sets can improve any brand or product. If data needs to be shared with those outside your organization such as researchers or partners, data obfuscation can be used to do so without compromising your sensitive data.\"]}),`\n`,(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:\"Testing and development -\"}),\" Develop, test, test, test, release - that should be your workflow to push out the best software possible. But along the way, data used for software development and testing can be at risk of exposure. Obfuscating your production databases is one way of allowing developers to access the data they need to ship the highest quality products while still respecting the privacy of your customers.\"]}),`\n`]}),`\n`,(0,t.jsx)(e.p,{children:\"No matter what type of sensitive data you have, data obfuscation can be done using a variety of techniques:\"}),`\n`,(0,t.jsxs)(e.ul,{children:[`\n`,(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:\"Tokenization -\"}),\" replacing sensitive data with tokens, or random strings of characters, while the original data is securely stored.\"]}),`\n`,(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:\"Encryption -\"}),\" transforming data into a format that is unreadable by an attacker using complex algorithms and the maintenance of a decryption key so that the data can be transformed back.\"]}),`\n`,(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:\"Masking -\"}),\" a set of techniques to hide a certain subset of sensitive data.\"]}),`\n`,(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:\"Generalization\"}),\" \",(0,t.jsx)(e.strong,{children:\"-\"}),\" reducing the granularity and specificity of data by aggregating it or putting it into a more broad format.\"]}),`\n`,(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:\"Randomization -\"}),\" perturbing data or adding random noise to it in order to make it challenging to decipher individuals\\u2019 information.\"]}),`\n`,(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:\"Synthetic data generation -\"}),\" creating new data based on the patterns in a real dataset or based on rules defined by a user.\"]}),`\n`]}),`\n`,(0,t.jsx)(e.p,{children:\"The good news is that we at Tonic have built our platform to seamlessly offer all of the above obfuscation methods and streamline your success in implementing them in your testing environments. And with the Release Instant Datasets, it\\u2019s that much easier to access the exact dataset you need.\"}),`\n`,(0,t.jsxs)(e.h3,{id:\"data-obfuscation-for-software-testing\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#data-obfuscation-for-software-testing\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Data Obfuscation for Software Testing\"]}),`\n`,(0,t.jsx)(e.p,{children:\"Testing applications on production data is a dangerous game. Test data obfuscation techniques create a dataset that is safe for use in testing of all kinds.\"}),`\n`,(0,t.jsx)(e.p,{children:\"With Tonic you can customize which, how, and where you obfuscate your production data as well as who has access to what. This allows for flexible workflows making sure that everyone gets the data they need to deliver at the highest caliber.\"}),`\n`,(0,t.jsxs)(e.p,{children:[\"Combining obfuscated data with ephemeral environments takes software testing to the next level. Ephemeral environments allow engineers the ability to work in reliable and isolated testing environments, accelerate their feedback cycles for quicker bug fixes, and boost collaboration to ultimately produce higher-quality software and better user experiences. We\\u2019re big fans of the solutions \",(0,t.jsx)(e.a,{href:\"https://release.com/\",children:\"Release\"}),\" provides.\"]}),`\n`,(0,t.jsxs)(e.p,{children:[\"Cruise on over to the \",(0,t.jsx)(e.a,{href:\"https://www.tonic.ai/blog/test-your-apps-with-high-fidelity-production-like-data-with-release-and-tonic\",children:\"Tonic.ai blog\"}),\" and check out Release\\u2019s CTO and cofounder Erik Landerholm\\u2019s guest blog post about the integration and sign up for our collaborative \",(0,t.jsx)(e.a,{href:\"https://release.com/webinar/release-tonic-best-practices\",children:\"September webinar\"}),\" to learn more about how to use production-like, secure data for your application development and testing processes.\"]})]})}function k(a={}){let{wrapper:e}=a.components||{};return e?(0,t.jsx)(e,Object.assign({},a,{children:(0,t.jsx)(d,a)})):d(a)}var T=k;return w(x);})();\n;return Component;"
        },
        "_id": "blog/posts/the-value-of-data-obfuscation-for-instant-datasets-tonic-meets-release.mdx",
        "_raw": {
          "sourceFilePath": "blog/posts/the-value-of-data-obfuscation-for-instant-datasets-tonic-meets-release.mdx",
          "sourceFileName": "the-value-of-data-obfuscation-for-instant-datasets-tonic-meets-release.mdx",
          "sourceFileDir": "blog/posts",
          "contentType": "mdx",
          "flattenedPath": "blog/posts/the-value-of-data-obfuscation-for-instant-datasets-tonic-meets-release"
        },
        "type": "BlogPost",
        "computedSlug": "the-value-of-data-obfuscation-for-instant-datasets-tonic-meets-release"
      },
      "documentHash": "1739393595029",
      "hasWarnings": false,
      "documentTypeName": "BlogPost"
    },
    "blog/posts/three-ways-youre-doing-modern-web-development-wrong.mdx": {
      "document": {
        "title": "Three ways you're doing modern web development wrong",
        "summary": "Ensure that your team is using modern development practices with these 3 best practices",
        "publishDate": "Fri Dec 16 2022 22:05:19 GMT+0000 (Coordinated Universal Time)",
        "author": "nick-busey",
        "readingTime": 3,
        "categories": [
          "platform-engineering",
          "product"
        ],
        "mainImage": "/blog-images/f6d54e0d3ec06d682bc6894ce90e2a79.jpg",
        "imageAlt": "a computer screen with a keyboard",
        "showCTA": true,
        "ctaCopy": "Improve development speed and quality by using Release for ephemeral environments and fresh production-like data.",
        "ctaLink": "https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=three-ways-youre-doing-modern-web-development-wrong",
        "relatedPosts": [
          ""
        ],
        "ogImage": "/blog-images/f6d54e0d3ec06d682bc6894ce90e2a79.jpg",
        "excerpt": "Ensure that your team is using modern development practices with these 3 best practices",
        "tags": [
          "platform-engineering",
          "product"
        ],
        "ctaButton": "Try Release for Free",
        "body": {
          "raw": "\nModern web development has come a long way in the past 20 years, but many companies seem like they're still stuck in the late 90s with the way they do development.\n\nEnsuring that your team is using modern development practices is beneficial in many ways, including easier hiring and onboarding, faster feature development, reduced bugs, and increased developer velocity. Not to mention keeping your developers happy and “in the zone”. Make things easier on your developers, and they will be able to complete work faster, at a higher quality, and be happier while doing it. These problems may not be obvious at first, but as you scale, they quickly become more and more painful. Here are three common mistakes you are probably making right now.\n\n### Mistake #1: Working In A Fixed Number Of Environments\n\nYour developer has finally written code, and now wants QA to review and test it. QA should not be testing on their local machines, they should be testing in something that more closely resembles a production environment. This leaves only cloud or on-premises environments as viable options for testing.\n\nIf you haven't done the undifferentiated heavy lifting of setting up an in-house auto-scaling environment solution, then you are likely stuck with a fixed number of environments for developers and QA teams to share.\n\nThis can slow the entire team down, and the added overhead of coordinating who is using what environments can be painful. (\"Is anyone using dev3?\" Sound familiar?)\n\n#### Improvement #1: Get Ephemeral\n\nUse an EaaS platform like Release. Creating an internal EaaS system is likely not your business’s core competency, and isn’t going to differentiate you in your industry. Just like you probably shouldn’t try and roll your own email delivery system, you are better off focusing on providing your customers value through features and bug fixes, rather than reinventing the wheel on building something that has already been built in a more fully featured manner than what is likely to come from an internal tool.\n\n### Mistake #2: Not Using Production-Like Data\n\nSeeding your dev environment with a predefined set of records is a very common way to do things, and also a very common cause of uncaught bugs making their way into production.\n\nThere is simply no way that developers will be able to maintain a set of database seed files that will accurately represent the way your system is used in the real world.\n\nDeveloper-defined seed files tend to stay on the happy path, while the sad path, where data are incomplete, incorrect, or conflicting, often remains untested.\n\n#### Improvement #2: Fresh Data Every Time\n\nUse a system that provides your developers with access to a fresh copy of production-like data to use every time they start work on a new branch. With Release’s Instant Datasets feature, you can get this going on day one.\n\nIf you have sensitive data in your production database such as PII, you can use a tool like tonic.ai to fuzz or obfuscate sensitive data before using Release’s Instant Dataset feature, allowing every environment to have access to real, fresh, legally compliant data.\n\n### Mistake #3: Running Everything Locally\n\nWhile it is good to have the ability to run locally when needed (example: working without internet access), it tends to slow down actual development in modern stacks.\n\nRunning 10 different services when you're only working on or trying debug a single one of them, is an unnecessary drain on the battery, extra load on the developer’s machine, and can lead to what I refer to as \"hovercraft mode\", when the laptop fans spin so fast it sounds like your laptop is about to take off.\n\nIn addition to all of that, onboarding tends to be much slower due to having to get every new hire’s development machine setup exactly right with all the dependencies required across multiple different tech stacks.\n\n#### Improvement #3: Look To The Cloud\n\nInstead of relying on local dev for everything, you could utilize a cloud based development model. Release Remote Development Environments allow developers to work in the exact type of environment their code will eventually be executed in, which can reduce bugs that come from environment and configuration problems. Beyond that a hybrid approach can offer the best of both worlds and bridge the gap between fully committing to either method. For example, you can run only the frontend locally, while the rest of the stack runs in a shared cloud environment.\n\n### Time to Modernize\n\nTo avoid these issues and improve your team's development practices, consider investing in modern development tools and processes, such as [Remote Development Environments](https://release.com/blog/remote-development-environments) and using [Production Data for testing](https://docs.releasehub.com/reference-documentation/instant-datasets-aws). These changes may require some initial effort and investment, but they will pay off in the long run by making your team more efficient, effective, and happy.\n",
          "code": "var Component=(()=>{var h=Object.create;var i=Object.defineProperty;var m=Object.getOwnPropertyDescriptor;var u=Object.getOwnPropertyNames;var p=Object.getPrototypeOf,y=Object.prototype.hasOwnProperty;var g=(t,e)=>()=>(e||t((e={exports:{}}).exports,e),e.exports),f=(t,e)=>{for(var o in e)i(t,o,{get:e[o],enumerable:!0})},s=(t,e,o,r)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let a of u(e))!y.call(t,a)&&a!==o&&i(t,a,{get:()=>e[a],enumerable:!(r=m(e,a))||r.enumerable});return t};var v=(t,e,o)=>(o=t!=null?h(p(t)):{},s(e||!t||!t.__esModule?i(o,\"default\",{value:t,enumerable:!0}):o,t)),b=t=>s(i({},\"__esModule\",{value:!0}),t);var d=g((T,l)=>{l.exports=_jsx_runtime});var I={};f(I,{default:()=>x,frontmatter:()=>w});var n=v(d()),w={title:\"Three ways you're doing modern web development wrong\",summary:\"Ensure that your team is using modern development practices with these 3 best practices\",publishDate:\"Fri Dec 16 2022 22:05:19 GMT+0000 (Coordinated Universal Time)\",author:\"nick-busey\",readingTime:3,categories:[\"platform-engineering\",\"product\"],mainImage:\"/blog-images/f6d54e0d3ec06d682bc6894ce90e2a79.jpg\",imageAlt:\"a computer screen with a keyboard\",showCTA:!0,ctaCopy:\"Improve development speed and quality by using Release for ephemeral environments and fresh production-like data.\",ctaLink:\"https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=three-ways-youre-doing-modern-web-development-wrong\",relatedPosts:[\"\"],ogImage:\"/blog-images/f6d54e0d3ec06d682bc6894ce90e2a79.jpg\",excerpt:\"Ensure that your team is using modern development practices with these 3 best practices\",tags:[\"platform-engineering\",\"product\"],ctaButton:\"Try Release for Free\"};function c(t){let e=Object.assign({p:\"p\",h3:\"h3\",a:\"a\",span:\"span\",h4:\"h4\"},t.components);return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(e.p,{children:\"Modern web development has come a long way in the past 20 years, but many companies seem like they're still stuck in the late 90s with the way they do development.\"}),`\n`,(0,n.jsx)(e.p,{children:\"Ensuring that your team is using modern development practices is beneficial in many ways, including easier hiring and onboarding, faster feature development, reduced bugs, and increased developer velocity. Not to mention keeping your developers happy and \\u201Cin the zone\\u201D. Make things easier on your developers, and they will be able to complete work faster, at a higher quality, and be happier while doing it. These problems may not be obvious at first, but as you scale, they quickly become more and more painful. Here are three common mistakes you are probably making right now.\"}),`\n`,(0,n.jsxs)(e.h3,{id:\"mistake-1-working-in-a-fixed-number-of-environments\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#mistake-1-working-in-a-fixed-number-of-environments\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Mistake #1: Working In A Fixed Number Of Environments\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Your developer has finally written code, and now wants QA to review and test it. QA should not be testing on their local machines, they should be testing in something that more closely resembles a production environment. This leaves only cloud or on-premises environments as viable options for testing.\"}),`\n`,(0,n.jsx)(e.p,{children:\"If you haven't done the undifferentiated heavy lifting of setting up an in-house auto-scaling environment solution, then you are likely stuck with a fixed number of environments for developers and QA teams to share.\"}),`\n`,(0,n.jsx)(e.p,{children:'This can slow the entire team down, and the added overhead of coordinating who is using what environments can be painful. (\"Is anyone using dev3?\" Sound familiar?)'}),`\n`,(0,n.jsxs)(e.h4,{id:\"improvement-1-get-ephemeral\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#improvement-1-get-ephemeral\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Improvement #1: Get Ephemeral\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Use an EaaS platform like Release. Creating an internal EaaS system is likely not your business\\u2019s core competency, and isn\\u2019t going to differentiate you in your industry. Just like you probably shouldn\\u2019t try and roll your own email delivery system, you are better off focusing on providing your customers value through features and bug fixes, rather than reinventing the wheel on building something that has already been built in a more fully featured manner than what is likely to come from an internal tool.\"}),`\n`,(0,n.jsxs)(e.h3,{id:\"mistake-2-not-using-production-like-data\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#mistake-2-not-using-production-like-data\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Mistake #2: Not Using Production-Like Data\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Seeding your dev environment with a predefined set of records is a very common way to do things, and also a very common cause of uncaught bugs making their way into production.\"}),`\n`,(0,n.jsx)(e.p,{children:\"There is simply no way that developers will be able to maintain a set of database seed files that will accurately represent the way your system is used in the real world.\"}),`\n`,(0,n.jsx)(e.p,{children:\"Developer-defined seed files tend to stay on the happy path, while the sad path, where data are incomplete, incorrect, or conflicting, often remains untested.\"}),`\n`,(0,n.jsxs)(e.h4,{id:\"improvement-2-fresh-data-every-time\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#improvement-2-fresh-data-every-time\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Improvement #2: Fresh Data Every Time\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Use a system that provides your developers with access to a fresh copy of production-like data to use every time they start work on a new branch. With Release\\u2019s Instant Datasets feature, you can get this going on day one.\"}),`\n`,(0,n.jsx)(e.p,{children:\"If you have sensitive data in your production database such as PII, you can use a tool like tonic.ai to fuzz or obfuscate sensitive data before using Release\\u2019s Instant Dataset feature, allowing every environment to have access to real, fresh, legally compliant data.\"}),`\n`,(0,n.jsxs)(e.h3,{id:\"mistake-3-running-everything-locally\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#mistake-3-running-everything-locally\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Mistake #3: Running Everything Locally\"]}),`\n`,(0,n.jsx)(e.p,{children:\"While it is good to have the ability to run locally when needed (example: working without internet access), it tends to slow down actual development in modern stacks.\"}),`\n`,(0,n.jsx)(e.p,{children:`Running 10 different services when you're only working on or trying debug a single one of them, is an unnecessary drain on the battery, extra load on the developer\\u2019s machine, and can lead to what I refer to as \"hovercraft mode\", when the laptop fans spin so fast it sounds like your laptop is about to take off.`}),`\n`,(0,n.jsx)(e.p,{children:\"In addition to all of that, onboarding tends to be much slower due to having to get every new hire\\u2019s development machine setup exactly right with all the dependencies required across multiple different tech stacks.\"}),`\n`,(0,n.jsxs)(e.h4,{id:\"improvement-3-look-to-the-cloud\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#improvement-3-look-to-the-cloud\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Improvement #3: Look To The Cloud\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Instead of relying on local dev for everything, you could utilize a cloud based development model. Release Remote Development Environments allow developers to work in the exact type of environment their code will eventually be executed in, which can reduce bugs that come from environment and configuration problems. Beyond that a hybrid approach can offer the best of both worlds and bridge the gap between fully committing to either method. For example, you can run only the frontend locally, while the rest of the stack runs in a shared cloud environment.\"}),`\n`,(0,n.jsxs)(e.h3,{id:\"time-to-modernize\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#time-to-modernize\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Time to Modernize\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"To avoid these issues and improve your team's development practices, consider investing in modern development tools and processes, such as \",(0,n.jsx)(e.a,{href:\"https://release.com/blog/remote-development-environments\",children:\"Remote Development Environments\"}),\" and using \",(0,n.jsx)(e.a,{href:\"https://docs.releasehub.com/reference-documentation/instant-datasets-aws\",children:\"Production Data for testing\"}),\". These changes may require some initial effort and investment, but they will pay off in the long run by making your team more efficient, effective, and happy.\"]})]})}function k(t={}){let{wrapper:e}=t.components||{};return e?(0,n.jsx)(e,Object.assign({},t,{children:(0,n.jsx)(c,t)})):c(t)}var x=k;return b(I);})();\n;return Component;"
        },
        "_id": "blog/posts/three-ways-youre-doing-modern-web-development-wrong.mdx",
        "_raw": {
          "sourceFilePath": "blog/posts/three-ways-youre-doing-modern-web-development-wrong.mdx",
          "sourceFileName": "three-ways-youre-doing-modern-web-development-wrong.mdx",
          "sourceFileDir": "blog/posts",
          "contentType": "mdx",
          "flattenedPath": "blog/posts/three-ways-youre-doing-modern-web-development-wrong"
        },
        "type": "BlogPost",
        "computedSlug": "three-ways-youre-doing-modern-web-development-wrong"
      },
      "documentHash": "1739393595029",
      "hasWarnings": false,
      "documentTypeName": "BlogPost"
    },
    "blog/posts/training-chatgpt-with-custom-libraries-using-extensions.mdx": {
      "document": {
        "title": "Training ChatGPT with Custom Libraries Using Extensions",
        "summary": "How we are now leveraging embeddings and vector databases to generate prompts for ChatGPT.",
        "publishDate": "Mon Apr 24 2023 21:20:19 GMT+0000 (Coordinated Universal Time)",
        "author": "david-giffin",
        "readingTime": 6,
        "categories": [
          "ai",
          "platform-engineering"
        ],
        "mainImage": "/blog-images/c66a474523317a52c700e3703639fe54.png",
        "imageAlt": "A huge book shelf full of books",
        "showCTA": true,
        "ctaCopy": "Enhance ChatGPT training with Release's dynamic environments for accurate AI model generation and testing.",
        "ctaLink": "https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=training-chatgpt-with-custom-libraries-using-extensions",
        "relatedPosts": [
          "rainbow-deployment-why-and-how-to-do-it"
        ],
        "ogImage": "/blog-images/c66a474523317a52c700e3703639fe54.png",
        "excerpt": "How we are now leveraging embeddings and vector databases to generate prompts for ChatGPT.",
        "tags": [
          "ai",
          "platform-engineering"
        ],
        "ctaButton": "Try Release for Free",
        "body": {
          "raw": "\nI'm excited to share with you some of the fascinating work we've been doing here at Release. Our team has been exploring the power of embeddings, vector databases, and language models to create innovative product features. In this post, I'll explain our journey as we explored OpenAI and ChatGPT and how we are now leveraging **embeddings and vector databases** to **generate prompts for ChatGPT**.\n\nWe began to look into various ways to leverage AI in our product. We were hoping that we could have ChatGPT generate Release Application Templates, the blueprints that we use to describe an application in Release. We quickly realized that ChatGPT is only trained on data before September 2021 and it was questionable if it knew anything about Release. \n\n![Image of a solicitiation in ChatGPT](/blog-images/5518599a59a79501590504565f7715e9.png)\n\nRelease supports using both Docker and Docker Compose so you would be able to use these files in Release to generate an Application Template.  But it was clear that ChatGPT needed to be trained using the Release documentation or a large corpus Release Application Templates  if it was going to generate one from scratch.\n\n### Exploring ChatGPT Plugins\n\nChatGPT Plugins seemed like the best way to give ChatGPT outside knowledge from its training set. We signed up for the ChatGPT Plugin waitlist and eventually got access to [ChatGPT Plugins](https://openai.com/blog/chatgpt-plugins). The [ChatGPT Retrieval Plugin](https://github.com/openai/chatgpt-retrieval-plugin) seemed like a place to start experimenting with ChatGPT Plugins and get an understanding of how they work.\n\nAfter adding a few files to the [chatgpt-retrival-plugin](https://github.com/openai/chatgpt-retrieval-plugin/compare/main...davidgiffin:chatgpt-retrieval-plugin:main?expand=1) we had it running Release. Then we started working on loading the data into the plugin, converting all of our docs into JSON and uploading them into the retrieval plugin using the \\`/upsert\\` endpoint. Once the plugin was configured ChatGPT we were able to ask ChatGPT to \"How do I create an application template in Release\"\n\n![Image of a solicitiation in ChatGPT](/blog-images/2dee5aa12a753b1dbd6e6f6dda136721.png)\n\n The retrieval plugin works well for asking a question that can be answered using the documentation it has access to. However it was unclear when plug-ins are going to be generally available for all users to access. We plan to develop a ChatGPT Plugin that everyone can use once that happens. \n\n### Using Embeddings and Prompt Generation\n\nAs our team continued to explore the AI space we came across an article from the [Supabase Blog](https://supabase.com/blog/chatgpt-supabase-docs). The article explained a different approach to \"train\" ChatGPT. Instead of ChatGPT having access to our documentation directly you could feed snippets of the docs to ChatGPT in the prompt. Here is the prompt template that takes the users question and the relevant snippets from the docs to answer a users question:\n\n```none\n\n`\n      You are a very enthusiastic Release representative who loves\n      to help people! Given the following sections from the Release\n      documentation, answer the question using only that information,\n      outputted in markdown format. If you are unsure and the answer\n      is not explicitly written in the documentation, say\n      \"Sorry, I don't know how to help with that.\"\n      \n      Context sections:\n      ${contextText}\nx\n\n      \"\"\"\n      Answer as markdown (including related code snippets if available):\n`\n\n```\n\nThe folks who helped build the Supabase AI functionality also created an open source standalone project [next.js OpenAI Search Starter.](https://github.com/supabase-community/nextjs-openai-doc-search) We have been using this project as a starting point for our AI based documentation search.\n\n### What are Embeddings?\n\nBoth the ChatGPT Retrieval Plugin and Supabase's AI Documentation Search rely on generating, storing and searching embeddings. So what is an embedding?\n\nEmbeddings are a way to represent text, images, or other types of data in a numerical format that can be easily processed by machine learning algorithms. In the context of natural language processing (NLP), word embeddings are vector representations of words, where each word is mapped to a fixed-size vector in a high-dimensional space. These vectors capture the semantic and syntactic relationships between words, allowing us to perform mathematical operations on them. The following diagram shows the relationship between various sentences:\n\n![](/blog-images/7d710cf63d90a88defe3050c4c710f39.png)\n\n[image of sentence embeddings - from DeepAI](https://deepai.org/publication/in-search-for-linear-relations-in-sentence-embedding-spaces)\n\nEmbeddings can be used to find words that are semantically similar to a given word. By calculating the cosine similarity between the vectors of two words, we can determine how similar their meanings are. This is a powerful tool for tasks such as text classification, sentiment analysis, and language translation. \n\n### What are Vector Databases?\n\nVector databases, also known as vector search engines, are specialized databases designed to store and search for high-dimensional vectors efficiently. They enable fast similarity search and nearest neighbor search, which are essential operations when working with embeddings.\n\nSupabase's AI Documentation Search uses [pgvector](https://github.com/pgvector/pgvector) to store and retrieve embeddings. But many other vector databases exist today:\n\n[Pinecone](https://github.com/openai/openai-cookbook/tree/main/examples/vector_databases/pinecone), a fully managed vector database\n\n‍[Weaviate](https://github.com/openai/openai-cookbook/tree/main/examples/vector_databases/weaviate), an open-source vector search engine\n\n[Redis](https://github.com/openai/openai-cookbook/tree/main/examples/vector_databases/redis), a vector database\n\n[Qdrant](https://github.com/openai/openai-cookbook/tree/main/examples/vector_databases/qdrant), a vector search engine\n\n[Milvus](https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/Using_vector_databases_for_embeddings_search.ipynb), a vector database built for scalable similarity search\n\n[Chroma](https://github.com/chroma-core/chroma), an open-source embeddings store\n\n[Typesense](https://typesense.org/docs/0.24.0/api/vector-search.html), fast open source vector search\n\nAll of these databases support three basic things: storing embeddings as vectors, the ability to search the embedding/vectors and finally sorting the results based on similarity. When using OpenAI's \\`text-embedding-ada-002\\` model to generate embeddings OpenAI recommends using [cosine similarity](https://typesense.org/docs/0.24.0/api/vector-search.html) which is built into most of the vector databases listed above. \n\n### How to Generate, Store and Search Embeddings\n\n[OpenAI provides an API endpoint](https://platform.openai.com/docs/api-reference/embeddings) to generate embeddings from any text string. \n\n````ruby\n\n```ruby\n        # OpenAI recommends replacing newlines with spaces\n        # for best results (specific to embeddings)\n        input = section.gsub(/\\n/m, ' ')\n        response = openai.embeddings(parameters: { input: input, model: \"text-embedding-ada-002\"})\n\n        token_count = response['usage']['total_tokens'] # number of tokens used\n        embedding = response['data'].first['embedding'] # array of 1536 floats\n````\n\n````\n\nStoring this data in Redis [redis-stack-server](https://redis.io/docs/stack/) and making it searchable requires an index. To create an index using redis-stack-server you need to issue the following command:\n\n```ruby\n\n````\n\nFT.CREATE index ON JSON PREFIX 1 item: SCHEMA $.id AS id TEXT $.content AS content TEXT $.token_count AS token_count NUMERIC $.embedding AS embedding VECTOR FLAT 6 DIM 1536 DISTANCE_METRIC COSINE TYPE FLOAT64\n\n```\n\n```\n\nNow we can store items into Redis and havethem indexed with the following command:\n\n```JSON\n{\n  \"id\": \"963a2117895ec9a29f242f906fd188c6\",\n  \"content\": \"# App Imports: …\",\n  \"embedding\": [0.008565563, 0.012807296]\n}\n```\n\n````\n\nNote that if you don't provide all 1536 dimensions of the vector your data will not be indexed by Redis and it will give you no error response.\n\nSearching Redis for results and sorting them can be done with the following command:\n\n```redis\nFT.SEARCH index @embedding:[VECTOR_RANGE $r $BLOB]=>{$YIELD_DISTANCE_AS: my_scores} \\\n  PARAMS 4 BLOB \\x00\\x00\\x00 r 5 LIMIT 0 10 SORTBY my_scores DIALECT 2\n````\n\n```\n\nNote that BLOB provided is in binary format and needs to have all 1536 dimensions of vector data as well. We use the OpenAI Embeddings API to generate the embedding vector and convert it to a binary in Ruby using \\`embedding.pack(\"E\\*\")\\`.\n\n### Release ChatGPT Powered Documentation Search\n\nWe have replaced the backend [next.js OpenAI Search Starter](https://github.com/supabase-community/nextjs-openai-doc-search) with Ruby and Redis. We will be releasing our project as an open source Gem that will allow anyone to quickly add AI based document searching to their site.\n\n![](/blog-images/21df17a7235a67ebe788dd7d1ea3d26b.png)\n\nWe have a [working example](https://frontend-vapey-prod.releaseapp.gethandsup.com/) of the Release AI Powered Documentation Search using slightly modified version of the [next.js OpenAI Search Starter.](https://github.com/supabase-community/nextjs-openai-doc-search\\)) We've added support for scrolling, better rendering of markdown (which the Supabase version had) and the ability to plugin your search API backend.\n\n### Conclusion\n\nBy combining the power of embeddings, vector databases, and language models like ChatGPT, we've been able to create product features that provide valuable insights and enhance user experiences. Whether it's answering customer queries, generating personalized content, or providing recommendations, our approach has opened up new possibilities for innovation.\n\nWe're excited about the potential of this technology, and we're looking forward to exploring new ways to leverage it in the future. As we continue to develop and refine our product offerings, we're committed to staying at the forefront of AI and NLP research. Our goal is to create tools and solutions that empower businesses and individuals to harness the power of language models in meaningful and impactful ways.\n\nThank you for taking the time to read our blog post. We hope you found it informative and that it sparked your curiosity about the exciting possibilities that embeddings, vector databases, and language models like ChatGPT have to offer. If you have any questions or would like to learn more about our work at Release, [please feel free to reach out to us or book a demo.](https://release.com/book-a-demo) We'd love to hear from you!\n```\n",
          "code": "var Component=(()=>{var l=Object.create;var i=Object.defineProperty;var p=Object.getOwnPropertyDescriptor;var g=Object.getOwnPropertyNames;var m=Object.getPrototypeOf,u=Object.prototype.hasOwnProperty;var b=(a,e)=>()=>(e||a((e={exports:{}}).exports,e),e.exports),f=(a,e)=>{for(var t in e)i(a,t,{get:e[t],enumerable:!0})},r=(a,e,t,s)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let o of g(e))!u.call(a,o)&&o!==t&&i(a,o,{get:()=>e[o],enumerable:!(s=p(e,o))||s.enumerable});return a};var w=(a,e,t)=>(t=a!=null?l(m(a)):{},r(e||!a||!a.__esModule?i(t,\"default\",{value:a,enumerable:!0}):t,a)),v=a=>r(i({},\"__esModule\",{value:!0}),a);var c=b((C,d)=>{d.exports=_jsx_runtime});var x={};f(x,{default:()=>k,frontmatter:()=>y});var n=w(c()),y={title:\"Training ChatGPT with Custom Libraries Using Extensions\",summary:\"How we are now leveraging embeddings and vector databases to generate prompts for ChatGPT.\",publishDate:\"Mon Apr 24 2023 21:20:19 GMT+0000 (Coordinated Universal Time)\",author:\"david-giffin\",readingTime:6,categories:[\"ai\",\"platform-engineering\"],mainImage:\"/blog-images/c66a474523317a52c700e3703639fe54.png\",imageAlt:\"A huge book shelf full of books\",showCTA:!0,ctaCopy:\"Enhance ChatGPT training with Release's dynamic environments for accurate AI model generation and testing.\",ctaLink:\"https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=training-chatgpt-with-custom-libraries-using-extensions\",relatedPosts:[\"rainbow-deployment-why-and-how-to-do-it\"],ogImage:\"/blog-images/c66a474523317a52c700e3703639fe54.png\",excerpt:\"How we are now leveraging embeddings and vector databases to generate prompts for ChatGPT.\",tags:[\"ai\",\"platform-engineering\"],ctaButton:\"Try Release for Free\"};function h(a){let e=Object.assign({p:\"p\",strong:\"strong\",img:\"img\",h3:\"h3\",a:\"a\",span:\"span\",pre:\"pre\",code:\"code\"},a.components);return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsxs)(e.p,{children:[\"I'm excited to share with you some of the fascinating work we've been doing here at Release. Our team has been exploring the power of embeddings, vector databases, and language models to create innovative product features. In this post, I'll explain our journey as we explored OpenAI and ChatGPT and how we are now leveraging \",(0,n.jsx)(e.strong,{children:\"embeddings and vector databases\"}),\" to \",(0,n.jsx)(e.strong,{children:\"generate prompts for ChatGPT\"}),\".\"]}),`\n`,(0,n.jsx)(e.p,{children:\"We began to look into various ways to leverage AI in our product. We were hoping that we could have ChatGPT generate Release Application Templates, the blueprints that we use to describe an application in Release. We quickly realized that ChatGPT is only trained on data before September 2021 and it was questionable if it knew anything about Release.\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/5518599a59a79501590504565f7715e9.png\",alt:\"Image of a solicitiation in ChatGPT\"})}),`\n`,(0,n.jsx)(e.p,{children:\"Release supports using both Docker and Docker Compose so you would be able to use these files in Release to generate an Application Template.\\xA0 But it was clear that ChatGPT needed to be trained using the Release documentation or a large corpus Release Application Templates\\xA0 if it was going to generate one from scratch.\"}),`\n`,(0,n.jsxs)(e.h3,{id:\"exploring-chatgpt-plugins\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#exploring-chatgpt-plugins\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Exploring ChatGPT Plugins\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"ChatGPT Plugins seemed like the best way to give ChatGPT outside knowledge from its training set. We signed up for the ChatGPT Plugin waitlist and eventually got access to \",(0,n.jsx)(e.a,{href:\"https://openai.com/blog/chatgpt-plugins\",children:\"ChatGPT Plugins\"}),\". The \",(0,n.jsx)(e.a,{href:\"https://github.com/openai/chatgpt-retrieval-plugin\",children:\"ChatGPT Retrieval Plugin\"}),\" seemed like a place to start experimenting with ChatGPT Plugins and get an understanding of how they work.\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"After adding a few files to the \",(0,n.jsx)(e.a,{href:\"https://github.com/openai/chatgpt-retrieval-plugin/compare/main...davidgiffin:chatgpt-retrieval-plugin:main?expand=1\",children:\"chatgpt-retrival-plugin\"}),' we had it running Release. Then we started working on loading the data into the plugin, converting all of our docs into JSON and uploading them into the retrieval plugin using the `/upsert` endpoint. Once the plugin was configured ChatGPT we were able to ask ChatGPT to \"How do I create an application template in Release\"']}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/2dee5aa12a753b1dbd6e6f6dda136721.png\",alt:\"Image of a solicitiation in ChatGPT\"})}),`\n`,(0,n.jsx)(e.p,{children:\"\\xA0The retrieval plugin works well for asking a question that can be answered using the documentation it has access to. However it was unclear when plug-ins are going to be generally available for all users to access. We plan to develop a ChatGPT Plugin that everyone can use once that happens.\\xA0\"}),`\n`,(0,n.jsxs)(e.h3,{id:\"using-embeddings-and-prompt-generation\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#using-embeddings-and-prompt-generation\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Using Embeddings and Prompt Generation\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"As our team continued to explore the AI space we came across an article from the \",(0,n.jsx)(e.a,{href:\"https://supabase.com/blog/chatgpt-supabase-docs\",children:\"Supabase Blog\"}),'. The article explained a different approach to \"train\" ChatGPT. Instead of ChatGPT having access to our documentation directly you could feed snippets of the docs to ChatGPT in the prompt. Here is the prompt template that takes the users question and the relevant snippets from the docs to answer a users question:']}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-none\",children:`\n\\`\n \\xA0 \\xA0 \\xA0You are a very enthusiastic Release representative who loves\n \\xA0 \\xA0 \\xA0to help people! Given the following sections from the Release\n \\xA0 \\xA0 \\xA0documentation, answer the question using only that information,\n \\xA0 \\xA0 \\xA0outputted in markdown format. If you are unsure and the answer\n \\xA0 \\xA0 \\xA0is not explicitly written in the documentation, say\n \\xA0 \\xA0 \\xA0\"Sorry, I don't know how to help with that.\"\n \\xA0 \\xA0 \\xA0\n \\xA0 \\xA0 \\xA0Context sections:\n \\xA0 \\xA0 \\xA0\\${contextText}\nx\n\n \\xA0 \\xA0 \\xA0\"\"\"\n \\xA0 \\xA0 \\xA0Answer as markdown (including related code snippets if available):\n\\`\n\n`})}),`\n`,(0,n.jsxs)(e.p,{children:[\"The folks who helped build the Supabase AI functionality also created an open source standalone project \",(0,n.jsx)(e.a,{href:\"https://github.com/supabase-community/nextjs-openai-doc-search\",children:\"next.js OpenAI Search Starter.\"}),\" We have been using this project as a starting point for our AI based documentation search.\"]}),`\n`,(0,n.jsxs)(e.h3,{id:\"what-are-embeddings\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#what-are-embeddings\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"What are Embeddings?\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Both the ChatGPT Retrieval Plugin and Supabase's AI Documentation Search rely on generating, storing and searching embeddings. So what is an embedding?\"}),`\n`,(0,n.jsx)(e.p,{children:\"Embeddings are a way to represent text, images, or other types of data in a numerical format that can be easily processed by machine learning algorithms. In the context of natural language processing (NLP), word embeddings are vector representations of words, where each word is mapped to a fixed-size vector in a high-dimensional space. These vectors capture the semantic and syntactic relationships between words, allowing us to perform mathematical operations on them. The following diagram shows the relationship between various sentences:\"}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/7d710cf63d90a88defe3050c4c710f39.png\",alt:\"\"})}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.a,{href:\"https://deepai.org/publication/in-search-for-linear-relations-in-sentence-embedding-spaces\",children:\"image of sentence embeddings - from DeepAI\"})}),`\n`,(0,n.jsx)(e.p,{children:\"Embeddings can be used to find words that are semantically similar to a given word. By calculating the cosine similarity between the vectors of two words, we can determine how similar their meanings are. This is a powerful tool for tasks such as text classification, sentiment analysis, and language translation.\\xA0\"}),`\n`,(0,n.jsxs)(e.h3,{id:\"what-are-vector-databases\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#what-are-vector-databases\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"What are Vector Databases?\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Vector databases, also known as vector search engines, are specialized databases designed to store and search for high-dimensional vectors efficiently. They enable fast similarity search and nearest neighbor search, which are essential operations when working with embeddings.\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"Supabase's AI Documentation Search uses \",(0,n.jsx)(e.a,{href:\"https://github.com/pgvector/pgvector\",children:\"pgvector\"}),\" to store and retrieve embeddings. But many other vector databases exist today:\"]}),`\n`,(0,n.jsxs)(e.p,{children:[(0,n.jsx)(e.a,{href:\"https://github.com/openai/openai-cookbook/tree/main/examples/vector_databases/pinecone\",children:\"Pinecone\"}),\", a fully managed vector database\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"\\u200D\",(0,n.jsx)(e.a,{href:\"https://github.com/openai/openai-cookbook/tree/main/examples/vector_databases/weaviate\",children:\"Weaviate\"}),\", an open-source vector search engine\"]}),`\n`,(0,n.jsxs)(e.p,{children:[(0,n.jsx)(e.a,{href:\"https://github.com/openai/openai-cookbook/tree/main/examples/vector_databases/redis\",children:\"Redis\"}),\", a vector database\"]}),`\n`,(0,n.jsxs)(e.p,{children:[(0,n.jsx)(e.a,{href:\"https://github.com/openai/openai-cookbook/tree/main/examples/vector_databases/qdrant\",children:\"Qdrant\"}),\", a vector search engine\"]}),`\n`,(0,n.jsxs)(e.p,{children:[(0,n.jsx)(e.a,{href:\"https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/Using_vector_databases_for_embeddings_search.ipynb\",children:\"Milvus\"}),\", a vector database built for scalable similarity search\"]}),`\n`,(0,n.jsxs)(e.p,{children:[(0,n.jsx)(e.a,{href:\"https://github.com/chroma-core/chroma\",children:\"Chroma\"}),\", an open-source embeddings store\"]}),`\n`,(0,n.jsxs)(e.p,{children:[(0,n.jsx)(e.a,{href:\"https://typesense.org/docs/0.24.0/api/vector-search.html\",children:\"Typesense\"}),\", fast open source vector search\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"All of these databases support three basic things: storing embeddings as vectors, the ability to search the embedding/vectors and finally sorting the results based on similarity. When using OpenAI's `text-embedding-ada-002` model to generate embeddings OpenAI recommends using \",(0,n.jsx)(e.a,{href:\"https://typesense.org/docs/0.24.0/api/vector-search.html\",children:\"cosine similarity\"}),\" which is built into most of the vector databases listed above.\\xA0\"]}),`\n`,(0,n.jsxs)(e.h3,{id:\"how-to-generate-store-and-search-embeddings\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#how-to-generate-store-and-search-embeddings\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"How to Generate, Store and Search Embeddings\"]}),`\n`,(0,n.jsxs)(e.p,{children:[(0,n.jsx)(e.a,{href:\"https://platform.openai.com/docs/api-reference/embeddings\",children:\"OpenAI provides an API endpoint\"}),\" to generate embeddings from any text string.\\xA0\"]}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-ruby\",children:`\n\\`\\`\\`ruby\n \\xA0 \\xA0 \\xA0 \\xA0# OpenAI recommends replacing newlines with spaces\n \\xA0 \\xA0 \\xA0 \\xA0# for best results (specific to embeddings)\n \\xA0 \\xA0 \\xA0 \\xA0input = section.gsub(/\\\\n/m, ' ')\n \\xA0 \\xA0 \\xA0 \\xA0response = openai.embeddings(parameters: { input: input, model: \"text-embedding-ada-002\"})\n\n \\xA0 \\xA0 \\xA0 \\xA0token_count = response['usage']['total_tokens'] # number of tokens used\n \\xA0 \\xA0 \\xA0 \\xA0embedding = response['data'].first['embedding'] # array of 1536 floats\n`})}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{children:`\nStoring this data in Redis [redis-stack-server](https://redis.io/docs/stack/) and making it searchable requires an index. To create an index using redis-stack-server you need to issue the following command:\n\n\\`\\`\\`ruby\n\n`})}),`\n`,(0,n.jsx)(e.p,{children:\"FT.CREATE index ON JSON PREFIX 1 item: SCHEMA $.id AS id TEXT $.content AS content TEXT $.token_count AS token_count NUMERIC $.embedding AS embedding VECTOR FLAT 6 DIM 1536 DISTANCE_METRIC COSINE TYPE FLOAT64\"}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{})}),`\n`,(0,n.jsx)(e.p,{children:\"Now we can store items into Redis and havethem indexed with the following command:\"}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-JSON\",children:`{\n  \"id\": \"963a2117895ec9a29f242f906fd188c6\",\n  \"content\": \"# App Imports: \\u2026\",\n  \"embedding\": [0.008565563, 0.012807296]\n}\n`})}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{children:`\nNote that if you don't provide all 1536 dimensions of the vector your data will not be indexed by Redis and it will give you no error response.\n\nSearching Redis for results and sorting them can be done with the following command:\n\n\\`\\`\\`redis\nFT.SEARCH index @embedding:[VECTOR_RANGE $r $BLOB]=>{$YIELD_DISTANCE_AS: my_scores} \\\\\n  PARAMS 4 BLOB \\\\x00\\\\x00\\\\x00 r 5 LIMIT 0 10 SORTBY my_scores DIALECT 2\n`})}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{children:`\nNote that BLOB provided is in binary format and needs to have all 1536 dimensions of vector data as well. We use the OpenAI Embeddings API to generate the embedding vector and convert it to a binary in Ruby using \\\\\\`embedding.pack(\"E\\\\*\")\\\\\\`.\n\n### Release ChatGPT Powered Documentation Search\n\nWe have replaced the backend [next.js OpenAI Search Starter](https://github.com/supabase-community/nextjs-openai-doc-search) with Ruby and Redis. We will be releasing our project as an open source Gem that will allow anyone to quickly add AI based document searching to their site.\n\n![](/blog-images/21df17a7235a67ebe788dd7d1ea3d26b.png)\n\nWe have a [working example](https://frontend-vapey-prod.releaseapp.gethandsup.com/) of the Release AI Powered Documentation Search using slightly modified version of the [next.js OpenAI Search Starter.](https://github.com/supabase-community/nextjs-openai-doc-search\\\\)) We've added support for scrolling, better rendering of markdown (which the Supabase version had) and the ability to plugin your search API backend.\n\n### Conclusion\n\nBy combining the power of embeddings, vector databases, and language models like ChatGPT, we've been able to create product features that provide valuable insights and enhance user experiences. Whether it's answering customer queries, generating personalized content, or providing recommendations, our approach has opened up new possibilities for innovation.\n\nWe're excited about the potential of this technology, and we're looking forward to exploring new ways to leverage it in the future. As we continue to develop and refine our product offerings, we're committed to staying at the forefront of AI and NLP research. Our goal is to create tools and solutions that empower businesses and individuals to harness the power of language models in meaningful and impactful ways.\n\nThank you for taking the time to read our blog post. We hope you found it informative and that it sparked your curiosity about the exciting possibilities that embeddings, vector databases, and language models like ChatGPT have to offer. If you have any questions or would like to learn more about our work at Release, [please feel free to reach out to us or book a demo.](https://release.com/book-a-demo) We'd love to hear from you!\n`})})]})}function T(a={}){let{wrapper:e}=a.components||{};return e?(0,n.jsx)(e,Object.assign({},a,{children:(0,n.jsx)(h,a)})):h(a)}var k=T;return v(x);})();\n;return Component;"
        },
        "_id": "blog/posts/training-chatgpt-with-custom-libraries-using-extensions.mdx",
        "_raw": {
          "sourceFilePath": "blog/posts/training-chatgpt-with-custom-libraries-using-extensions.mdx",
          "sourceFileName": "training-chatgpt-with-custom-libraries-using-extensions.mdx",
          "sourceFileDir": "blog/posts",
          "contentType": "mdx",
          "flattenedPath": "blog/posts/training-chatgpt-with-custom-libraries-using-extensions"
        },
        "type": "BlogPost",
        "computedSlug": "training-chatgpt-with-custom-libraries-using-extensions"
      },
      "documentHash": "1739393595029",
      "hasWarnings": false,
      "documentTypeName": "BlogPost"
    },
    "blog/posts/tripactions-selects-releasehub-to-give-full-stack-environments-to-every-developer.mdx": {
      "document": {
        "title": "TripActions Selects Release to Give Full Stack Environments to Every Developer",
        "summary": "Release’s environments can be spun up or down on-demand, increasing product release velocity and eliminating develope",
        "publishDate": "Thu Sep 29 2022 12:03:52 GMT+0000 (Coordinated Universal Time)",
        "author": "",
        "readingTime": 2,
        "categories": [
          "platform-engineering",
          "product"
        ],
        "mainImage": "/blog-images/353a332b97f5f28110b22c4058ab5c49.jpg",
        "imageAlt": "TripActions Selects Release to Give Full Stack Environments to Every Developer",
        "showCTA": true,
        "ctaCopy": "Empower developers with on-demand, ephemeral full-stack environments like TripActions using ReleaseHub for seamless collaboration and faster software delivery.",
        "ctaLink": "https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=tripactions-selects-releasehub-to-give-full-stack-environments-to-every-developer",
        "relatedPosts": [
          ""
        ],
        "ogImage": "/blog-images/353a332b97f5f28110b22c4058ab5c49.jpg",
        "excerpt": "Release’s environments can be spun up or down on-demand, increasing product release velocity and eliminating develope",
        "tags": [
          "platform-engineering",
          "product"
        ],
        "ctaButton": "Try Release for Free",
        "body": {
          "raw": "\n**SAN FRANCISCO, September 29, 2022** — Release, the leading provider of on-demand software environments, today announced that TripActions, an industry-leading all-in-one travel, corporate card, and expense management solution, has selected ReleaseHub to provide its developers with ephemeral, full-stack staging environments that can be spun up and down on demand.\n\n‍\n\nSwitching to Release’s on-demand staging environments will allow TripActions to stand up and tear down identical production-like environments instantly, eliminating common delays that developers across all industries experience with conventional shared environments. With ReleaseHub, organizations reduce the cost of wasted developer time, and increase operational efficiency and productivity.\n\n‍\n\n“Developers around the world understand how frustrating and pervasive staging environment delays are,” said Tommy McClung, co-founder and CEO of Release. “TripActions is in a unique position because they’re an industry leader at the cutting edge of innovation, with thousands of global customers, so they have unparalleled insight into developer pain points. Delays frequently leave the most talented, highly-paid professionals stuck waiting in limbo. Our ephemeral environments eliminate the limbo time entirely.”\n\n‍\n\nWith Release, organizations — including TripActions — can also focus on attracting and retaining the highest quality professionals by removing staging delays, a continual source of frustration for developers across all industries.\n\n‍\n\n“Release will enable us to increase our release cadence and reduce developer down-time,” said Chris Willmore, director of productivity engineering at TripActions. “We have hundreds of developers and if they are using conventional shared staging environments, many end up sitting around, waiting for access. Release eliminates the waiting time, improves developer flow, saves wasted resources, and helps us deliver software faster.”\n\n‍\n\nRelease delivers Environments-as-a-Service using environments-as-code. Unlike other solutions, ReleaseHub abstracts away much of the work required by developers. The process of creating environments-as-code is automated and customizable. The result allows developers to manage environments simply via code in their development workflow. ReleaseHub supports running custom Infrastructure-as-a-Code including Terraform, Helm and other Infrastructure-as-a-Service providers.\n\n‍\n**About Release**\n\nRelease delivers Environments-as-a-Service. It lets developers easily share progress with stakeholders when a full stack environment is created with every pull request and is shareable via custom URLs and directly in Slack. Every environment is a full instance of the app with all its services. ReleaseHub was funded by CRV, Sequoia, Y Combinator, Bow Capital, Artisanal Ventures, Hack VC, and other investors. More information is available at [www.release.com](http://www.release.com/).  \n\n‍\n**About TripActions**\n\nTripActions is the all-in-one travel, corporate card, and expense management solution, providing customers around the globe with unprecedented visibility and control over spend. Trusted by travel managers and finance teams alike, TripActions and TripActions Liquid leverage real-time data to help companies keep traveling employees safe, reduce spend, and drive productivity.  Learn more at www.tripactions.com\n\n‍\n",
          "code": "var Component=(()=>{var p=Object.create;var r=Object.defineProperty;var u=Object.getOwnPropertyDescriptor;var m=Object.getOwnPropertyNames;var h=Object.getPrototypeOf,v=Object.prototype.hasOwnProperty;var g=(t,e)=>()=>(e||t((e={exports:{}}).exports,e),e.exports),f=(t,e)=>{for(var a in e)r(t,a,{get:e[a],enumerable:!0})},o=(t,e,a,s)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let i of m(e))!v.call(t,i)&&i!==a&&r(t,i,{get:()=>e[i],enumerable:!(s=u(e,i))||s.enumerable});return t};var w=(t,e,a)=>(a=t!=null?p(h(t)):{},o(e||!t||!t.__esModule?r(a,\"default\",{value:t,enumerable:!0}):a,t)),b=t=>o(r({},\"__esModule\",{value:!0}),t);var d=g((C,l)=>{l.exports=_jsx_runtime});var A={};f(A,{default:()=>T,frontmatter:()=>y});var n=w(d()),y={title:\"TripActions Selects Release to Give Full Stack Environments to Every Developer\",summary:\"Release\\u2019s environments can be spun up or down on-demand, increasing product release velocity and eliminating develope\",publishDate:\"Thu Sep 29 2022 12:03:52 GMT+0000 (Coordinated Universal Time)\",author:\"\",readingTime:2,categories:[\"platform-engineering\",\"product\"],mainImage:\"/blog-images/353a332b97f5f28110b22c4058ab5c49.jpg\",imageAlt:\"TripActions Selects Release to Give Full Stack Environments to Every Developer\",showCTA:!0,ctaCopy:\"Empower developers with on-demand, ephemeral full-stack environments like TripActions using ReleaseHub for seamless collaboration and faster software delivery.\",ctaLink:\"https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=tripactions-selects-releasehub-to-give-full-stack-environments-to-every-developer\",relatedPosts:[\"\"],ogImage:\"/blog-images/353a332b97f5f28110b22c4058ab5c49.jpg\",excerpt:\"Release\\u2019s environments can be spun up or down on-demand, increasing product release velocity and eliminating develope\",tags:[\"platform-engineering\",\"product\"],ctaButton:\"Try Release for Free\"};function c(t){let e=Object.assign({p:\"p\",strong:\"strong\",a:\"a\"},t.components);return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsxs)(e.p,{children:[(0,n.jsx)(e.strong,{children:\"SAN FRANCISCO, September 29, 2022\"}),\" \\u2014 Release, the leading provider of on-demand software environments, today announced that TripActions, an industry-leading all-in-one travel, corporate card, and expense management solution, has selected ReleaseHub to provide its developers with ephemeral, full-stack staging environments that can be spun up and down on demand.\"]}),`\n`,(0,n.jsx)(e.p,{children:\"\\u200D\"}),`\n`,(0,n.jsx)(e.p,{children:\"Switching to Release\\u2019s on-demand staging environments will allow TripActions to stand up and tear down identical production-like environments instantly, eliminating common delays that developers across all industries experience with conventional shared environments. With ReleaseHub, organizations reduce the cost of wasted developer time, and increase operational efficiency and productivity.\"}),`\n`,(0,n.jsx)(e.p,{children:\"\\u200D\"}),`\n`,(0,n.jsx)(e.p,{children:\"\\u201CDevelopers around the world understand how frustrating and pervasive staging environment delays are,\\u201D said Tommy McClung, co-founder and CEO of Release. \\u201CTripActions is in a unique position because they\\u2019re an industry leader at the cutting edge of innovation, with thousands of global customers, so they have unparalleled insight into developer pain points. Delays frequently leave the most talented, highly-paid professionals stuck waiting in limbo. Our ephemeral environments eliminate the limbo time entirely.\\u201D\"}),`\n`,(0,n.jsx)(e.p,{children:\"\\u200D\"}),`\n`,(0,n.jsx)(e.p,{children:\"With Release, organizations \\u2014 including TripActions \\u2014 can also focus on attracting and retaining the highest quality professionals by removing staging delays, a continual source of frustration for developers across all industries.\"}),`\n`,(0,n.jsx)(e.p,{children:\"\\u200D\"}),`\n`,(0,n.jsx)(e.p,{children:\"\\u201CRelease will enable us to increase our release cadence and reduce developer down-time,\\u201D said Chris Willmore, director of productivity engineering at TripActions. \\u201CWe have hundreds of developers and if they are using conventional shared staging environments, many end up sitting around, waiting for access. Release eliminates the waiting time, improves developer flow, saves wasted resources, and helps us deliver software faster.\\u201D\"}),`\n`,(0,n.jsx)(e.p,{children:\"\\u200D\"}),`\n`,(0,n.jsx)(e.p,{children:\"Release delivers Environments-as-a-Service using environments-as-code. Unlike other solutions, ReleaseHub abstracts away much of the work required by developers. The process of creating environments-as-code is automated and customizable. The result allows developers to manage environments simply via code in their development workflow. ReleaseHub supports running custom Infrastructure-as-a-Code including Terraform, Helm and other Infrastructure-as-a-Service providers.\"}),`\n`,(0,n.jsxs)(e.p,{children:[`\\u200D\n`,(0,n.jsx)(e.strong,{children:\"About Release\"})]}),`\n`,(0,n.jsxs)(e.p,{children:[\"Release delivers Environments-as-a-Service. It lets developers easily share progress with stakeholders when a full stack environment is created with every pull request and is shareable via custom URLs and directly in Slack. Every environment is a full instance of the app with all its services. ReleaseHub was funded by CRV, Sequoia, Y Combinator, Bow Capital, Artisanal Ventures, Hack VC, and other investors. More information is available at \",(0,n.jsx)(e.a,{href:\"http://www.release.com/\",children:\"www.release.com\"}),\". \\xA0\"]}),`\n`,(0,n.jsxs)(e.p,{children:[`\\u200D\n`,(0,n.jsx)(e.strong,{children:\"About TripActions\"})]}),`\n`,(0,n.jsxs)(e.p,{children:[\"TripActions is the all-in-one travel, corporate card, and expense management solution, providing customers around the globe with unprecedented visibility and control over spend. Trusted by travel managers and finance teams alike, TripActions and TripActions Liquid leverage real-time data to help companies keep traveling employees safe, reduce spend, and drive productivity. \\xA0Learn more at \",(0,n.jsx)(e.a,{href:\"http://www.tripactions.com\",children:\"www.tripactions.com\"})]}),`\n`,(0,n.jsx)(e.p,{children:\"\\u200D\"})]})}function R(t={}){let{wrapper:e}=t.components||{};return e?(0,n.jsx)(e,Object.assign({},t,{children:(0,n.jsx)(c,t)})):c(t)}var T=R;return b(A);})();\n;return Component;"
        },
        "_id": "blog/posts/tripactions-selects-releasehub-to-give-full-stack-environments-to-every-developer.mdx",
        "_raw": {
          "sourceFilePath": "blog/posts/tripactions-selects-releasehub-to-give-full-stack-environments-to-every-developer.mdx",
          "sourceFileName": "tripactions-selects-releasehub-to-give-full-stack-environments-to-every-developer.mdx",
          "sourceFileDir": "blog/posts",
          "contentType": "mdx",
          "flattenedPath": "blog/posts/tripactions-selects-releasehub-to-give-full-stack-environments-to-every-developer"
        },
        "type": "BlogPost",
        "computedSlug": "tripactions-selects-releasehub-to-give-full-stack-environments-to-every-developer"
      },
      "documentHash": "1739393595029",
      "hasWarnings": false,
      "documentTypeName": "BlogPost"
    },
    "blog/posts/use-product-thinking-to-establish-your-idp.mdx": {
      "document": {
        "title": "Use Product Thinking to Establish Your IDP",
        "summary": "Learn how to build a success platform by combining IDP's goals, components and problems to solve, with product thinking.",
        "publishDate": "Tue Sep 26 2023 20:35:51 GMT+0000 (Coordinated Universal Time)",
        "author": "sylvia-fronczak",
        "readingTime": 10,
        "categories": [
          "platform-engineering",
          "product"
        ],
        "mainImage": "/blog-images/c910c71d4c0e51fd3672ff01c8164ba1.jpg",
        "imageAlt": "Use Product Thinking to Establish Your IDP",
        "showCTA": true,
        "ctaCopy": "Build a successful IDP with Release's on-demand environments for streamlined workflows and faster deployment cycles.",
        "ctaLink": "https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=use-product-thinking-to-establish-your-idp",
        "relatedPosts": [
          "what-is-an-internal-developer-platform-and-why-should-i-have-one"
        ],
        "ogImage": "/blog-images/c910c71d4c0e51fd3672ff01c8164ba1.jpg",
        "excerpt": "Learn how to build a success platform by combining IDP's goals, components and problems to solve, with product thinking.",
        "tags": [
          "platform-engineering",
          "product"
        ],
        "ctaButton": "Try Release for Free",
        "body": {
          "raw": "\nIn previous posts, we talked about the [benefits of Internal Developer Platforms](https://release.com/blog/what-is-an-internal-developer-platform-and-why-should-i-have-one) (IDPs) and the [components that form a great IDP](https://release.com/blog/components-of-a-successful-idp-build-a-product-your-developers-actually-want-to-use). But getting all the benefits out of an IDP isn’t just about gathering a bunch of tools together in one UI and assuming everyone will use it.\n\nInstead, we need to treat the entire platform, with all its components and integrations as a product—a product that solves problems and improves your developers’ workflows.\n\nWithin each organization, an IDP might solve different problems in different ways. And in order for you to get the benefits out of the IDP for your organization, you’ll need to take a product-thinking approach and fulfill the needs of your customers—the developers.\n\nIn this post, we’ll talk about how you can combine the goals of an IDP, the problems it solves through various components, and product-thinking principles to build a successful platform. If your organization is new to product thinking, or if you haven’t used those skills when developing tooling for your software developers, this will give you a starting point with frameworks as well as common elements and metrics that you can build on when kicking off your IDP journey.\n\nFirst, let’s look at product thinking and a few approaches and frameworks that provide your organization with the tools it needs to build a successful product.\n\n#### The Product-Thinking Approach\n\nIf you’re not familiar with product thinking, it may seem like a nebulous idea that requires the most experienced product managers with fancy frameworks and high consulting fees. However, product thinking, at its core, involves identifying and understanding the problems your customers experience and then prioritizing solutions that provide the most value.\n\nOrganizations use various frameworks to drive product thinking. These frameworks identify activities that support your discovery and understanding of the problems and prioritization of the work to be done.\n\n![](/blog-images/fdff2db86417969bb149d9deef764f3c.png)\n\nLet’s take a look at some frameworks that organizations use to drive product thinking. You can use one or combination of these approaches, as they complement each other well.\n\n##### 🚀 Lean Startup: Build, Measure, and Learn\n\nThe [Lean Startup](https://theleanstartup.com/) methodology broke down the process of building products into three iterative steps: build, measure, and learn. This methodology combines UX, product, and engineering to build products by solving problems using minimum viable products (MVPs) and customer validation. The focus is on learning more about your customer and their needs from each iteration of the build-measure-learn cycle.\n\nWhat would that look like for an IDP?\n\nIn the **build** phase, you can begin by integrating one component of an IDP or automating one part of a workflow that your developers must do. For example, your org may frequently build new microservices or stand-alone products. Each of your teams requires an easy way to set up new environments and CI/CD pipelines for these microservices. Your IDP could first automate a basic CI/CD pipeline so that teams don’t have to do that themselves.\n\nIn the **measure** phase, you’ll see how your developers are using your IDP MVP through usage metrics and feedback. Looking at the MVP example that creates a pipeline, how often do teams use it? How often do they edit the pipelines later to fit their use cases? And what do they change?\n\nNext, we enter the **learn** phase. For our IDP example, we will assess our findings. Then perhaps we’ll learn that the pipeline is good enough for now and that provisioning infrastructure needs more automation or features. On the other hand, we could learn that the pipeline isn’t flexible enough or doesn’t provide easy rollback capabilities. Either way, we now know what we can build next and restart the cycle.\n\n##### 💎 Double Diamond Model: Discover, Define, Develop, and Deliver\n\nThe [double diamond mode](https://www.designcouncil.org.uk/our-resources/archive/articles/double-diamond-universally-accepted-depiction-design-process) also takes an iterative approach to building a product. We repeatedly work through its four phases—discover, define, develop, and deliver—over the life of the product.\n\nDuring **discovery**, you’ll explore and understand the problem space. For IDPs, this could include interviewing engineers, watching them work, and gathering available metrics on where the most pain exists in the development workflow.\n\nOnce we’ve pulled our research from discovery, you’ll move to the **define** phase. You’ll shift to framing the problem to a smaller scope and determining which problem you will solve first.\n\nIn the **development** phase, the development team will build solutions. This may include building components, or more likely, it includes configuring prebuilt platform as a service (PaaS) solutions to work with your infrastructure and organization.\n\n![](/blog-images/bf21bc0b1c90dad589f0b58bf5217bc7.png)\n\nAnd finally, you’ll **deliver** the initial product to market: your development teams.\n\nThen you’ll start at the beginning again, refining and validating your choices from the previous phases.\n\n##### 🔁 Iterative Development Using Agile Methodologies\n\nOur next approach also iterates on the product but focuses on the engineering side. With Agile, we focus on people from cross-functional groups working together to build working software through customer collaboration. Although product thinking is still used in each iteration, it’s more about how the product is actually built.\n\nOur first iteration may be an MVP like the ones we see in Lean Startup, or it could be a more refined product spec that has already been proved out. Once our first iteration of the product is complete, we’ll release it to the developer community that uses the IDP and ensure that expectations have been met. We’ll work with the developers and other stakeholders to ensure that we build the right product and deliver value to the customers.\n\nAt this point, you’ve been introduced or had a refresher on some common product thinking frameworks. So what’s next? We’ll want to consider how we can incorporate them into our product and what [components](https://release.com/blog/components-of-a-successful-idp-build-a-product-your-developers-actually-want-to-use) we’ll need to harness in order to build a successful IDP.\n\n#### Product Thinking Best Practices\n\nRegardless of the methodology or framework you use, you need to follow certain best practices to understand the problems your developers experience and prioritize the solutions that provide the most value:\n\n1.  Interdisciplinary team\n2.  User-centered approach\n3.  Focus on value\n4.  Iterative development\n5.  Measurable outcomes\n\n##### 1\\. Interdisciplinary team\n\nMost product-driven approaches include an interdisciplinary or cross-functional team. To build the right IDP, you need the right people looking at the problem from different perspectives.\n\nYour cross-functional team should, at minimum, include these main roles:\n\n- **Product:** to assess and prioritize the potential problems and solutions.\n- **UX:** to work with the developers to understand workflows and pain points. UX uses their tools to improve efficiency, flow, and usability of the IDP.\n- **Engineering:** to serve two purposes. First, as the customer of the IDP, their experience and feedback will provide you with insights into making their workflow better. Second, they’ll use their engineering experience to build out the IDP. This could mean building a component or integration or just configuring and automating various integrations.\n\n![](/blog-images/d271141535ba3cbc0089a01b4c48f3db.png)\n\nIn addition, consider adding subject matter experts (SMEs) from data, security, and compliance. When aligned with the mission of improving developer workflows, they’ll advise on what automation can fulfill different regulatory or security requirements, removing manual processes from the developers’ workflow.\n\n##### **2\\. User-Centered Approach**\n\nFor IDPs, your development teams are your main users. Understanding their environment and how they work will provide critical data for solving the right problems in the right way.\n\nTo understand the developers’ workflow, you’ll conduct interviews, get feedback, and collect data on their current processes. You also have to understand the developers’ experience with their current tools and how long different processes take. Where is their toil, and what steps take more time? What tools are better suited for a traditional UI, and what works best as a CLI, API, or other integration?\n\nOne important note: Just because the developers offer feedback doesn’t mean that you must act on it. Product thinking isn’t about taking orders from customers; it's about understanding problems. When you understand problems, you’ll need to dig in and find where the problems come from and which are more important to solve.\n\n##### **3\\. Focus on Value**\n\nTo start your IDP, your interdisciplinary team creates clear objectives and goals that focus on the value. Consider what problems or types of problems you are trying to solve and how you’ll measure success.\n\n![](/blog-images/896c26befa04f11393b6e7562d498318.png)\n\nAgain, you don’t have to solve all the use cases for a particular problem. Using the objectives for your product, focus on the features that will give you the biggest bang for your buck. Solve for 80% of the scenarios that your developers encounter and leave the other 20% for custom solutions that teams can work through on their own. And realize that the ideal solution may not be worth the cost when compared to the current or a slightly improved workflow.\n\n##### **4\\. Iterative Development**\n\nMost product-driven frameworks and methodologies include iterative development. This is especially true of the IDP, as there are so many options and so much that can be integrated in different ways. Your main focus will be on starting small and shipping new bits of functionality frequently.\n\n##### **5\\. Measurable Outcomes**\n\nMeasuring the success of IDPs or any developer tooling can be difficult. Consider these metrics to ensure that you’re providing value, and automate their collection from the start.\n\n- **Adoption rate:** For all features and integrations, measure how many developers are using the platform and how frequently they use it.\n- **Time saved:** Track how much time developers save by using the features of your platform when compared to their previous workflow.\n- **Feature usage:** Determine which features are used the most and which ones the least.\n- **Satisfaction scores:** Periodically collect satisfaction scores from developers to gauge their experiences with the platform.\n\nYou may be tempted to track metrics like release or feature velocity or the number of bugs a development team produces after onboarding to an IDP, but these have very little to do with the IDP. Focus on what’s directly affected by your IDP.\n\n#### To Sum Up\n\nWhen building an IDP, use product thinking to ensure you’re providing the right value and spending your resources well.\n\nGo back to our post on [build vs. buy](https://release.com/blog/build-vs-buy-where-to-focus-your-energy-with-idps) and consider what components can be brought in off the shelf with little development effort. That will provide you with fast feedback and the ability to understand the problem before sinking a lot of time into building it yourself.\n\nFor optimal success, include an interdisciplinary team, focus on the user and the value you can provide, measure your success, and iterate, iterate, iterate.\n\n_This post was written by Sylvia Fronczak._ [_Sylvia_](https://sylviafronczak.com/) _is a software developer who has worked in various industries with various software methodologies. She’s currently focused on design practices that the whole team can own, understand, and evolve over time._\n",
          "code": "var Component=(()=>{var h=Object.create;var i=Object.defineProperty;var u=Object.getOwnPropertyDescriptor;var p=Object.getOwnPropertyNames;var m=Object.getPrototypeOf,g=Object.prototype.hasOwnProperty;var f=(t,e)=>()=>(e||t((e={exports:{}}).exports,e),e.exports),b=(t,e)=>{for(var n in e)i(t,n,{get:e[n],enumerable:!0})},s=(t,e,n,a)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let r of p(e))!g.call(t,r)&&r!==n&&i(t,r,{get:()=>e[r],enumerable:!(a=u(e,r))||a.enumerable});return t};var v=(t,e,n)=>(n=t!=null?h(m(t)):{},s(e||!t||!t.__esModule?i(n,\"default\",{value:t,enumerable:!0}):n,t)),w=t=>s(i({},\"__esModule\",{value:!0}),t);var d=f((x,l)=>{l.exports=_jsx_runtime});var D={};b(D,{default:()=>I,frontmatter:()=>y});var o=v(d()),y={title:\"Use Product Thinking to Establish Your IDP\",summary:\"Learn how to build a success platform by combining IDP's goals, components and problems to solve, with product thinking.\",publishDate:\"Tue Sep 26 2023 20:35:51 GMT+0000 (Coordinated Universal Time)\",author:\"sylvia-fronczak\",readingTime:10,categories:[\"platform-engineering\",\"product\"],mainImage:\"/blog-images/c910c71d4c0e51fd3672ff01c8164ba1.jpg\",imageAlt:\"Use Product Thinking to Establish Your IDP\",showCTA:!0,ctaCopy:\"Build a successful IDP with Release's on-demand environments for streamlined workflows and faster deployment cycles.\",ctaLink:\"https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=use-product-thinking-to-establish-your-idp\",relatedPosts:[\"what-is-an-internal-developer-platform-and-why-should-i-have-one\"],ogImage:\"/blog-images/c910c71d4c0e51fd3672ff01c8164ba1.jpg\",excerpt:\"Learn how to build a success platform by combining IDP's goals, components and problems to solve, with product thinking.\",tags:[\"platform-engineering\",\"product\"],ctaButton:\"Try Release for Free\"};function c(t){let e=Object.assign({p:\"p\",a:\"a\",h4:\"h4\",span:\"span\",img:\"img\",h5:\"h5\",strong:\"strong\",ol:\"ol\",li:\"li\",ul:\"ul\",em:\"em\"},t.components);return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsxs)(e.p,{children:[\"In previous posts, we talked about the \",(0,o.jsx)(e.a,{href:\"https://release.com/blog/what-is-an-internal-developer-platform-and-why-should-i-have-one\",children:\"benefits of Internal Developer Platforms\"}),\" (IDPs) and the \",(0,o.jsx)(e.a,{href:\"https://release.com/blog/components-of-a-successful-idp-build-a-product-your-developers-actually-want-to-use\",children:\"components that form a great IDP\"}),\". But getting all the benefits out of an IDP isn\\u2019t just about gathering a bunch of tools together in one UI and assuming everyone will use it.\"]}),`\n`,(0,o.jsx)(e.p,{children:\"Instead, we need to treat the entire platform, with all its components and integrations as a product\\u2014a product that solves problems and improves your developers\\u2019 workflows.\"}),`\n`,(0,o.jsx)(e.p,{children:\"Within each organization, an IDP might solve different problems in different ways. And in order for you to get the benefits out of the IDP for your organization, you\\u2019ll need to take a product-thinking approach and fulfill the needs of your customers\\u2014the developers.\"}),`\n`,(0,o.jsx)(e.p,{children:\"In this post, we\\u2019ll talk about how you can combine the goals of an IDP, the problems it solves through various components, and product-thinking principles to build a successful platform. If your organization is new to product thinking, or if you haven\\u2019t used those skills when developing tooling for your software developers, this will give you a starting point with frameworks as well as common elements and metrics that you can build on when kicking off your IDP journey.\"}),`\n`,(0,o.jsx)(e.p,{children:\"First, let\\u2019s look at product thinking and a few approaches and frameworks that provide your organization with the tools it needs to build a successful product.\"}),`\n`,(0,o.jsxs)(e.h4,{id:\"the-product-thinking-approach\",children:[(0,o.jsx)(e.a,{className:\"anchor\",href:\"#the-product-thinking-approach\",children:(0,o.jsx)(e.span,{className:\"icon icon-link\"})}),\"The Product-Thinking Approach\"]}),`\n`,(0,o.jsx)(e.p,{children:\"If you\\u2019re not familiar with product thinking, it may seem like a nebulous idea that requires the most experienced product managers with fancy frameworks and high consulting fees. However, product thinking, at its core, involves identifying and understanding the problems your customers experience and then prioritizing solutions that provide the most value.\"}),`\n`,(0,o.jsx)(e.p,{children:\"Organizations use various frameworks to drive product thinking. These frameworks identify activities that support your discovery and understanding of the problems and prioritization of the work to be done.\"}),`\n`,(0,o.jsx)(e.p,{children:(0,o.jsx)(e.img,{src:\"/blog-images/fdff2db86417969bb149d9deef764f3c.png\",alt:\"\"})}),`\n`,(0,o.jsx)(e.p,{children:\"Let\\u2019s take a look at some frameworks that organizations use to drive product thinking. You can use one or combination of these approaches, as they complement each other well.\"}),`\n`,(0,o.jsxs)(e.h5,{id:\"-lean-startup-build-measure-and-learn\",children:[(0,o.jsx)(e.a,{className:\"anchor\",href:\"#-lean-startup-build-measure-and-learn\",children:(0,o.jsx)(e.span,{className:\"icon icon-link\"})}),\"\\u{1F680} Lean Startup: Build, Measure, and Learn\"]}),`\n`,(0,o.jsxs)(e.p,{children:[\"The \",(0,o.jsx)(e.a,{href:\"https://theleanstartup.com/\",children:\"Lean Startup\"}),\" methodology broke down the process of building products into three iterative steps: build, measure, and learn. This methodology combines UX, product, and engineering to build products by solving problems using minimum viable products (MVPs) and customer validation. The focus is on learning more about your customer and their needs from each iteration of the build-measure-learn cycle.\"]}),`\n`,(0,o.jsx)(e.p,{children:\"What would that look like for an IDP?\"}),`\n`,(0,o.jsxs)(e.p,{children:[\"In the \",(0,o.jsx)(e.strong,{children:\"build\"}),\" phase, you can begin by integrating one component of an IDP or automating one part of a workflow that your developers must do. For example, your org may frequently build new microservices or stand-alone products. Each of your teams requires an easy way to set up new environments and CI/CD pipelines for these microservices. Your IDP could first automate a basic CI/CD pipeline so that teams don\\u2019t have to do that themselves.\"]}),`\n`,(0,o.jsxs)(e.p,{children:[\"In the \",(0,o.jsx)(e.strong,{children:\"measure\"}),\" phase, you\\u2019ll see how your developers are using your IDP MVP through usage metrics and feedback. Looking at the MVP example that creates a pipeline, how often do teams use it? How often do they edit the pipelines later to fit their use cases? And what do they change?\"]}),`\n`,(0,o.jsxs)(e.p,{children:[\"Next, we enter the \",(0,o.jsx)(e.strong,{children:\"learn\"}),\" phase. For our IDP example, we will assess our findings. Then perhaps we\\u2019ll learn that the pipeline is good enough for now and that provisioning infrastructure needs more automation or features. On the other hand, we could learn that the pipeline isn\\u2019t flexible enough or doesn\\u2019t provide easy rollback capabilities. Either way, we now know what we can build next and restart the cycle.\"]}),`\n`,(0,o.jsxs)(e.h5,{id:\"-double-diamond-model-discover-define-develop-and-deliver\",children:[(0,o.jsx)(e.a,{className:\"anchor\",href:\"#-double-diamond-model-discover-define-develop-and-deliver\",children:(0,o.jsx)(e.span,{className:\"icon icon-link\"})}),\"\\u{1F48E} Double Diamond Model: Discover, Define, Develop, and Deliver\"]}),`\n`,(0,o.jsxs)(e.p,{children:[\"The \",(0,o.jsx)(e.a,{href:\"https://www.designcouncil.org.uk/our-resources/archive/articles/double-diamond-universally-accepted-depiction-design-process\",children:\"double diamond mode\"}),\" also takes an iterative approach to building a product. We repeatedly work through its four phases\\u2014discover, define, develop, and deliver\\u2014over the life of the product.\"]}),`\n`,(0,o.jsxs)(e.p,{children:[\"During \",(0,o.jsx)(e.strong,{children:\"discovery\"}),\", you\\u2019ll explore and understand the problem space. For IDPs, this could include interviewing engineers, watching them work, and gathering available metrics on where the most pain exists in the development workflow.\"]}),`\n`,(0,o.jsxs)(e.p,{children:[\"Once we\\u2019ve pulled our research from discovery, you\\u2019ll move to the \",(0,o.jsx)(e.strong,{children:\"define\"}),\" phase. You\\u2019ll shift to framing the problem to a smaller scope and determining which problem you will solve first.\"]}),`\n`,(0,o.jsxs)(e.p,{children:[\"In the \",(0,o.jsx)(e.strong,{children:\"development\"}),\" phase, the development team will build solutions. This may include building components, or more likely, it includes configuring prebuilt platform as a service (PaaS) solutions to work with your infrastructure and organization.\"]}),`\n`,(0,o.jsx)(e.p,{children:(0,o.jsx)(e.img,{src:\"/blog-images/bf21bc0b1c90dad589f0b58bf5217bc7.png\",alt:\"\"})}),`\n`,(0,o.jsxs)(e.p,{children:[\"And finally, you\\u2019ll \",(0,o.jsx)(e.strong,{children:\"deliver\"}),\" the initial product to market: your development teams.\"]}),`\n`,(0,o.jsx)(e.p,{children:\"Then you\\u2019ll start at the beginning again, refining and validating your choices from the previous phases.\"}),`\n`,(0,o.jsxs)(e.h5,{id:\"-iterative-development-using-agile-methodologies\",children:[(0,o.jsx)(e.a,{className:\"anchor\",href:\"#-iterative-development-using-agile-methodologies\",children:(0,o.jsx)(e.span,{className:\"icon icon-link\"})}),\"\\u{1F501} Iterative Development Using Agile Methodologies\"]}),`\n`,(0,o.jsx)(e.p,{children:\"Our next approach also iterates on the product but focuses on the engineering side. With Agile, we focus on people from cross-functional groups working together to build working software through customer collaboration. Although product thinking is still used in each iteration, it\\u2019s more about how the product is actually built.\"}),`\n`,(0,o.jsx)(e.p,{children:\"Our first iteration may be an MVP like the ones we see in Lean Startup, or it could be a more refined product spec that has already been proved out. Once our first iteration of the product is complete, we\\u2019ll release it to the developer community that uses the IDP and ensure that expectations have been met. We\\u2019ll work with the developers and other stakeholders to ensure that we build the right product and deliver value to the customers.\"}),`\n`,(0,o.jsxs)(e.p,{children:[\"At this point, you\\u2019ve been introduced or had a refresher on some common product thinking frameworks. So what\\u2019s next? We\\u2019ll want to consider how we can incorporate them into our product and what \",(0,o.jsx)(e.a,{href:\"https://release.com/blog/components-of-a-successful-idp-build-a-product-your-developers-actually-want-to-use\",children:\"components\"}),\" we\\u2019ll need to harness in order to build a successful IDP.\"]}),`\n`,(0,o.jsxs)(e.h4,{id:\"product-thinking-best-practices\",children:[(0,o.jsx)(e.a,{className:\"anchor\",href:\"#product-thinking-best-practices\",children:(0,o.jsx)(e.span,{className:\"icon icon-link\"})}),\"Product Thinking Best Practices\"]}),`\n`,(0,o.jsx)(e.p,{children:\"Regardless of the methodology or framework you use, you need to follow certain best practices to understand the problems your developers experience and prioritize the solutions that provide the most value:\"}),`\n`,(0,o.jsxs)(e.ol,{children:[`\n`,(0,o.jsx)(e.li,{children:\"Interdisciplinary team\"}),`\n`,(0,o.jsx)(e.li,{children:\"User-centered approach\"}),`\n`,(0,o.jsx)(e.li,{children:\"Focus on value\"}),`\n`,(0,o.jsx)(e.li,{children:\"Iterative development\"}),`\n`,(0,o.jsx)(e.li,{children:\"Measurable outcomes\"}),`\n`]}),`\n`,(0,o.jsxs)(e.h5,{id:\"1-interdisciplinary-team\",children:[(0,o.jsx)(e.a,{className:\"anchor\",href:\"#1-interdisciplinary-team\",children:(0,o.jsx)(e.span,{className:\"icon icon-link\"})}),\"1. Interdisciplinary team\"]}),`\n`,(0,o.jsx)(e.p,{children:\"Most product-driven approaches include an interdisciplinary or cross-functional team. To build the right IDP, you need the right people looking at the problem from different perspectives.\"}),`\n`,(0,o.jsx)(e.p,{children:\"Your cross-functional team should, at minimum, include these main roles:\"}),`\n`,(0,o.jsxs)(e.ul,{children:[`\n`,(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:\"Product:\"}),\" to assess and prioritize the potential problems and solutions.\"]}),`\n`,(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:\"UX:\"}),\" to work with the developers to understand workflows and pain points. UX uses their tools to improve efficiency, flow, and usability of the IDP.\"]}),`\n`,(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:\"Engineering:\"}),\" to serve two purposes. First, as the customer of the IDP, their experience and feedback will provide you with insights into making their workflow better. Second, they\\u2019ll use their engineering experience to build out the IDP. This could mean building a component or integration or just configuring and automating various integrations.\"]}),`\n`]}),`\n`,(0,o.jsx)(e.p,{children:(0,o.jsx)(e.img,{src:\"/blog-images/d271141535ba3cbc0089a01b4c48f3db.png\",alt:\"\"})}),`\n`,(0,o.jsx)(e.p,{children:\"In addition, consider adding subject matter experts (SMEs) from data, security, and compliance. When aligned with the mission of improving developer workflows, they\\u2019ll advise on what automation can fulfill different regulatory or security requirements, removing manual processes from the developers\\u2019 workflow.\"}),`\n`,(0,o.jsxs)(e.h5,{id:\"2-user-centered-approach\",children:[(0,o.jsx)(e.a,{className:\"anchor\",href:\"#2-user-centered-approach\",children:(0,o.jsx)(e.span,{className:\"icon icon-link\"})}),(0,o.jsx)(e.strong,{children:\"2. User-Centered Approach\"})]}),`\n`,(0,o.jsx)(e.p,{children:\"For IDPs, your development teams are your main users. Understanding their environment and how they work will provide critical data for solving the right problems in the right way.\"}),`\n`,(0,o.jsx)(e.p,{children:\"To understand the developers\\u2019 workflow, you\\u2019ll conduct interviews, get feedback, and collect data on their current processes. You also have to understand the developers\\u2019 experience with their current tools and how long different processes take. Where is their toil, and what steps take more time? What tools are better suited for a traditional UI, and what works best as a CLI, API, or other integration?\"}),`\n`,(0,o.jsx)(e.p,{children:\"One important note: Just because the developers offer feedback doesn\\u2019t mean that you must act on it. Product thinking isn\\u2019t about taking orders from customers; it's about understanding problems. When you understand problems, you\\u2019ll need to dig in and find where the problems come from and which are more important to solve.\"}),`\n`,(0,o.jsxs)(e.h5,{id:\"3-focus-on-value\",children:[(0,o.jsx)(e.a,{className:\"anchor\",href:\"#3-focus-on-value\",children:(0,o.jsx)(e.span,{className:\"icon icon-link\"})}),(0,o.jsx)(e.strong,{children:\"3. Focus on Value\"})]}),`\n`,(0,o.jsx)(e.p,{children:\"To start your IDP, your interdisciplinary team creates clear objectives and goals that focus on the value. Consider what problems or types of problems you are trying to solve and how you\\u2019ll measure success.\"}),`\n`,(0,o.jsx)(e.p,{children:(0,o.jsx)(e.img,{src:\"/blog-images/896c26befa04f11393b6e7562d498318.png\",alt:\"\"})}),`\n`,(0,o.jsx)(e.p,{children:\"Again, you don\\u2019t have to solve all the use cases for a particular problem. Using the objectives for your product, focus on the features that will give you the biggest bang for your buck. Solve for 80% of the scenarios that your developers encounter and leave the other 20% for custom solutions that teams can work through on their own. And realize that the ideal solution may not be worth the cost when compared to the current or a slightly improved workflow.\"}),`\n`,(0,o.jsxs)(e.h5,{id:\"4-iterative-development\",children:[(0,o.jsx)(e.a,{className:\"anchor\",href:\"#4-iterative-development\",children:(0,o.jsx)(e.span,{className:\"icon icon-link\"})}),(0,o.jsx)(e.strong,{children:\"4. Iterative Development\"})]}),`\n`,(0,o.jsx)(e.p,{children:\"Most product-driven frameworks and methodologies include iterative development. This is especially true of the IDP, as there are so many options and so much that can be integrated in different ways. Your main focus will be on starting small and shipping new bits of functionality frequently.\"}),`\n`,(0,o.jsxs)(e.h5,{id:\"5-measurable-outcomes\",children:[(0,o.jsx)(e.a,{className:\"anchor\",href:\"#5-measurable-outcomes\",children:(0,o.jsx)(e.span,{className:\"icon icon-link\"})}),(0,o.jsx)(e.strong,{children:\"5. Measurable Outcomes\"})]}),`\n`,(0,o.jsx)(e.p,{children:\"Measuring the success of IDPs or any developer tooling can be difficult. Consider these metrics to ensure that you\\u2019re providing value, and automate their collection from the start.\"}),`\n`,(0,o.jsxs)(e.ul,{children:[`\n`,(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:\"Adoption rate:\"}),\" For all features and integrations, measure how many developers are using the platform and how frequently they use it.\"]}),`\n`,(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:\"Time saved:\"}),\" Track how much time developers save by using the features of your platform when compared to their previous workflow.\"]}),`\n`,(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:\"Feature usage:\"}),\" Determine which features are used the most and which ones the least.\"]}),`\n`,(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:\"Satisfaction scores:\"}),\" Periodically collect satisfaction scores from developers to gauge their experiences with the platform.\"]}),`\n`]}),`\n`,(0,o.jsx)(e.p,{children:\"You may be tempted to track metrics like release or feature velocity or the number of bugs a development team produces after onboarding to an IDP, but these have very little to do with the IDP. Focus on what\\u2019s directly affected by your IDP.\"}),`\n`,(0,o.jsxs)(e.h4,{id:\"to-sum-up\",children:[(0,o.jsx)(e.a,{className:\"anchor\",href:\"#to-sum-up\",children:(0,o.jsx)(e.span,{className:\"icon icon-link\"})}),\"To Sum Up\"]}),`\n`,(0,o.jsx)(e.p,{children:\"When building an IDP, use product thinking to ensure you\\u2019re providing the right value and spending your resources well.\"}),`\n`,(0,o.jsxs)(e.p,{children:[\"Go back to our post on \",(0,o.jsx)(e.a,{href:\"https://release.com/blog/build-vs-buy-where-to-focus-your-energy-with-idps\",children:\"build vs. buy\"}),\" and consider what components can be brought in off the shelf with little development effort. That will provide you with fast feedback and the ability to understand the problem before sinking a lot of time into building it yourself.\"]}),`\n`,(0,o.jsx)(e.p,{children:\"For optimal success, include an interdisciplinary team, focus on the user and the value you can provide, measure your success, and iterate, iterate, iterate.\"}),`\n`,(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.em,{children:\"This post was written by Sylvia Fronczak.\"}),\" \",(0,o.jsx)(e.a,{href:\"https://sylviafronczak.com/\",children:(0,o.jsx)(e.em,{children:\"Sylvia\"})}),\" \",(0,o.jsx)(e.em,{children:\"is a software developer who has worked in various industries with various software methodologies. She\\u2019s currently focused on design practices that the whole team can own, understand, and evolve over time.\"})]})]})}function k(t={}){let{wrapper:e}=t.components||{};return e?(0,o.jsx)(e,Object.assign({},t,{children:(0,o.jsx)(c,t)})):c(t)}var I=k;return w(D);})();\n;return Component;"
        },
        "_id": "blog/posts/use-product-thinking-to-establish-your-idp.mdx",
        "_raw": {
          "sourceFilePath": "blog/posts/use-product-thinking-to-establish-your-idp.mdx",
          "sourceFileName": "use-product-thinking-to-establish-your-idp.mdx",
          "sourceFileDir": "blog/posts",
          "contentType": "mdx",
          "flattenedPath": "blog/posts/use-product-thinking-to-establish-your-idp"
        },
        "type": "BlogPost",
        "computedSlug": "use-product-thinking-to-establish-your-idp"
      },
      "documentHash": "1739393595029",
      "hasWarnings": false,
      "documentTypeName": "BlogPost"
    },
    "blog/posts/user-acceptance-testing-best-practices.mdx": {
      "document": {
        "title": "User Acceptance Testing Best Practices",
        "summary": "In this blog post you'll learn what is user acceptance testing, and user acceptance testing best practices.",
        "publishDate": "Tue Jan 04 2022 23:05:44 GMT+0000 (Coordinated Universal Time)",
        "author": "tommy-mcclung",
        "readingTime": 5,
        "categories": [
          "platform-engineering",
          "product"
        ],
        "mainImage": "/blog-images/59e5f8637296625094df0ed87b76f6df.jpg",
        "imageAlt": "a coffee cup and a book",
        "showCTA": true,
        "ctaCopy": "Automate UAT environment setup with Release for faster bug resolution and efficient testing cycles.",
        "ctaLink": "https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=user-acceptance-testing-best-practices",
        "relatedPosts": [
          ""
        ],
        "ogImage": "/blog-images/59e5f8637296625094df0ed87b76f6df.jpg",
        "excerpt": "In this blog post you'll learn what is user acceptance testing, and user acceptance testing best practices.",
        "tags": [
          "platform-engineering",
          "product"
        ],
        "ctaButton": "Try Release for Free",
        "body": {
          "raw": "\nSoftware testing plays a critical role in the end-to-end software product development process as it ensures as well as upholds the overall product quality.\n\nAmong the different phases of this software testing include integration testing, unit testing, system testing, acceptance testing, and the last phase of software testing, known as the [UAT or User Acceptance Testing](https://releasehub.com/user-acceptance-testing-with-ephemeral-environments), which plays a significant role in the entire software testing process.\n\nThis blog series aims to explore more about user acceptance testing, including what it is, UAT best practices, stages of UAT, disadvantages, challenges, and more.\n\nThis is a 4-part series on User Acceptance Testing (UAT)\n\n- **Part 1**: **What is User Acceptance Testing and its Best Practices**\n- Part 2: [How to Prepare for User Acceptance Testing?](https://releasehub.com/blog/how-do-you-prepare-for-user-acceptance-testing)\n- Part 3: [User Acceptance Testing Challenges & UAT Environment Examples](https://releasehub.com//blog/what-are-the-challenges-faced-during-uat-testing)\n- Part 4: [UAT Checklist](https://releasehub.com/blog/user-acceptance-testing-checklist)\n\n### What is User Acceptance Testing?\n\nUser acceptance testing or UAT is a specific type of testing which is mainly performed by real users in the last stage of testing before the application or software is released to the live production environment.\n\nThe primary reason why you need user acceptance testing is to validate if all business requirements of the software are met or not. This must be necessarily done before releasing the software to the market. This type of testing sometimes is also known as beta testing, application testing, or, more often, end-user testing.\n\nThe key purpose of UAT is to validate end-to-end business flow. It is similar to black-box testing, where two or more end-users will be involved. Put simply, UAT is essential because:\n\n- It is a critical part of the software testing procedure and may help find defects that were not detected earlier.\n- It helps users confirm if the developed solution meets the requirements and fulfills the needs of target users.\n- The needs and requirements of the customers are unique, unpredictable. Implementing UAT helps to effectively fill this gap and identify scenarios that were not properly defined or requirements that need revisit.\n\n### User Acceptance Testing Best Practices\n\nIn this section, we are going to discuss some of the user acceptance testing best practices to ensure that your application is thoroughly tested, the software meets user demands in real-world scenarios, bugs get resolved much before launch, and all your clients are successful and happy.\n\nAmong these best practices are:\n\n#### ‍\\*\\*Identify your target audience\n\n\\*\\*\n\nWhen it comes to UAT, it is crucial to identify your target audience and understand their unique problems and needs. This allows you to ensure that there is no wastage of time / resources on something that won't work for users.\n\n#### **Prepare a realistic test environment and data**‍\n\nAnother important requirement for UAT is that the environment and test data must resemble the production environment as closely as possible. In an ideal scenario, the [UAT environment](https://releasehub.com/user-acceptance-testing-with-ephemeral-environments) should be completely separate from the QA environment. However, if this is not possible, it is best to have a complete refresh before UAT, where QA professionals should check the refreshed environment to ensure it's working as expected. Spinning up a demo environment can be cumbersome and expensive. With Release's [Ephemeral Environments](https://releasehub.com/ephemeral-environments) it’s possible to view a developer’s changes at any time or any place. You don’t need to worry about having the proper environment variables in place or running any code locally.\n\n#### \\*\\*Create proper test criteria\n\n\\*\\*\n\nThe test lead or a QA manager should work with QA professionals to be able to ensure that the test coverage is complete. Here checklists can offer an effective alternative to test cases and scripts. Also, the acceptance criteria should form the basis of testing user stories.\n\n#### \\*\\*Be very specific\n\n\\*\\*\n\nTest cases in UAT need to be as detailed, thorough, and as specific as possible. It is best to be clear about and specify what buttons to click, what data to enter, what accounts to use, and what results the user should see. In addition to this, it should also cover how new functionality fits in well with existing pieces.\n\n#### \\*\\*Create robust bug communication standards\n\n\\*\\*\n\nBugs are inevitable in any application, and hence it is important to focus on how they are communicated to quickly resolve the issues. Make sure to be as specific as possible when creating a bug. Being vague can lead to confusion among developers, increases triaging times and further pushes back the timeline for  the bug.\n\n#### \\*\\*Create user stories based on specific business requirements and scenarios\n\n\\*\\*\n\nUser acceptance testing can be very useful if you target the right set of users. And to be able to target the expected users, it is required to create it in sync with the business requirements. Also, before proceeding with the user acceptance tests, it's very important to clearly define the event that you would consider a success and what would define that the product passed/failed the acceptance test?\n\nNow that you are familiar with user acceptance testing best practices, next we'll look at [How to Prepare for User Acceptance testing](https://releasehub.com/blog/how-do-you-prepare-for-user-acceptance-testing).\n",
          "code": "var Component=(()=>{var d=Object.create;var i=Object.defineProperty;var p=Object.getOwnPropertyDescriptor;var u=Object.getOwnPropertyNames;var m=Object.getPrototypeOf,g=Object.prototype.hasOwnProperty;var f=(n,e)=>()=>(e||n((e={exports:{}}).exports,e),e.exports),b=(n,e)=>{for(var s in e)i(n,s,{get:e[s],enumerable:!0})},c=(n,e,s,r)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let a of u(e))!g.call(n,a)&&a!==s&&i(n,a,{get:()=>e[a],enumerable:!(r=p(e,a))||r.enumerable});return n};var w=(n,e,s)=>(s=n!=null?d(m(n)):{},c(e||!n||!n.__esModule?i(s,\"default\",{value:n,enumerable:!0}):s,n)),y=n=>c(i({},\"__esModule\",{value:!0}),n);var l=f((x,o)=>{o.exports=_jsx_runtime});var U={};b(U,{default:()=>A,frontmatter:()=>v});var t=w(l()),v={title:\"User Acceptance Testing Best Practices\",summary:\"In this blog post you'll learn what is user acceptance testing, and user acceptance testing best practices.\",publishDate:\"Tue Jan 04 2022 23:05:44 GMT+0000 (Coordinated Universal Time)\",author:\"tommy-mcclung\",readingTime:5,categories:[\"platform-engineering\",\"product\"],mainImage:\"/blog-images/59e5f8637296625094df0ed87b76f6df.jpg\",imageAlt:\"a coffee cup and a book\",showCTA:!0,ctaCopy:\"Automate UAT environment setup with Release for faster bug resolution and efficient testing cycles.\",ctaLink:\"https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=user-acceptance-testing-best-practices\",relatedPosts:[\"\"],ogImage:\"/blog-images/59e5f8637296625094df0ed87b76f6df.jpg\",excerpt:\"In this blog post you'll learn what is user acceptance testing, and user acceptance testing best practices.\",tags:[\"platform-engineering\",\"product\"],ctaButton:\"Try Release for Free\"};function h(n){let e=Object.assign({p:\"p\",a:\"a\",ul:\"ul\",li:\"li\",strong:\"strong\",h3:\"h3\",span:\"span\",h4:\"h4\"},n.components);return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(e.p,{children:\"Software testing plays a critical role in the end-to-end software product development process as it ensures as well as upholds the overall product quality.\"}),`\n`,(0,t.jsxs)(e.p,{children:[\"Among the different phases of this software testing include integration testing, unit testing, system testing, acceptance testing, and the last phase of software testing, known as the \",(0,t.jsx)(e.a,{href:\"https://releasehub.com/user-acceptance-testing-with-ephemeral-environments\",children:\"UAT or User Acceptance Testing\"}),\", which plays a significant role in the entire software testing process.\"]}),`\n`,(0,t.jsx)(e.p,{children:\"This blog series aims to explore more about user acceptance testing, including what it is, UAT best practices, stages of UAT, disadvantages, challenges, and more.\"}),`\n`,(0,t.jsx)(e.p,{children:\"This is a 4-part series on User Acceptance Testing (UAT)\"}),`\n`,(0,t.jsxs)(e.ul,{children:[`\n`,(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:\"Part 1\"}),\": \",(0,t.jsx)(e.strong,{children:\"What is User Acceptance Testing and its Best Practices\"})]}),`\n`,(0,t.jsxs)(e.li,{children:[\"Part 2: \",(0,t.jsx)(e.a,{href:\"https://releasehub.com/blog/how-do-you-prepare-for-user-acceptance-testing\",children:\"How to Prepare for User Acceptance Testing?\"})]}),`\n`,(0,t.jsxs)(e.li,{children:[\"Part 3: \",(0,t.jsx)(e.a,{href:\"https://releasehub.com//blog/what-are-the-challenges-faced-during-uat-testing\",children:\"User Acceptance Testing Challenges & UAT Environment Examples\"})]}),`\n`,(0,t.jsxs)(e.li,{children:[\"Part 4: \",(0,t.jsx)(e.a,{href:\"https://releasehub.com/blog/user-acceptance-testing-checklist\",children:\"UAT Checklist\"})]}),`\n`]}),`\n`,(0,t.jsxs)(e.h3,{id:\"what-is-user-acceptance-testing\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#what-is-user-acceptance-testing\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"What is User Acceptance Testing?\"]}),`\n`,(0,t.jsx)(e.p,{children:\"User acceptance testing or UAT is a specific type of testing which is mainly performed by real users in the last stage of testing before the application or software is released to the live production environment.\"}),`\n`,(0,t.jsx)(e.p,{children:\"The primary reason why you need user acceptance testing is to validate if all business requirements of the software are met or not. This must be necessarily done before releasing the software to the market. This type of testing sometimes is also known as beta testing, application testing, or, more often, end-user testing.\"}),`\n`,(0,t.jsx)(e.p,{children:\"The key purpose of UAT is to validate end-to-end business flow. It is similar to black-box testing, where two or more end-users will be involved. Put simply, UAT is essential because:\"}),`\n`,(0,t.jsxs)(e.ul,{children:[`\n`,(0,t.jsx)(e.li,{children:\"It is a critical part of the software testing procedure and may help find defects that were not detected earlier.\"}),`\n`,(0,t.jsx)(e.li,{children:\"It helps users confirm if the developed solution meets the requirements and fulfills the needs of target users.\"}),`\n`,(0,t.jsx)(e.li,{children:\"The needs and requirements of the customers are unique, unpredictable. Implementing UAT helps to effectively fill this gap and identify scenarios that were not properly defined or requirements that need revisit.\"}),`\n`]}),`\n`,(0,t.jsxs)(e.h3,{id:\"user-acceptance-testing-best-practices\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#user-acceptance-testing-best-practices\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"User Acceptance Testing Best Practices\"]}),`\n`,(0,t.jsx)(e.p,{children:\"In this section, we are going to discuss some of the user acceptance testing best practices to ensure that your application is thoroughly tested, the software meets user demands in real-world scenarios, bugs get resolved much before launch, and all your clients are successful and happy.\"}),`\n`,(0,t.jsx)(e.p,{children:\"Among these best practices are:\"}),`\n`,(0,t.jsxs)(e.h4,{id:\"identify-your-target-audience\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#identify-your-target-audience\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"\\u200D**Identify your target audience\"]}),`\n`,(0,t.jsx)(e.p,{children:\"**\"}),`\n`,(0,t.jsx)(e.p,{children:\"When it comes to UAT, it is crucial to identify your target audience and understand their unique problems and needs. This allows you to ensure that there is no wastage of time / resources on something that won't work for users.\"}),`\n`,(0,t.jsxs)(e.h4,{id:\"prepare-a-realistic-test-environment-and-data\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#prepare-a-realistic-test-environment-and-data\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),(0,t.jsx)(e.strong,{children:\"Prepare a realistic test environment and data\"}),\"\\u200D\"]}),`\n`,(0,t.jsxs)(e.p,{children:[\"Another important requirement for UAT is that the environment and test data must resemble the production environment as closely as possible. In an ideal scenario, the \",(0,t.jsx)(e.a,{href:\"https://releasehub.com/user-acceptance-testing-with-ephemeral-environments\",children:\"UAT environment\"}),\" should be completely separate from the QA environment. However, if this is not possible, it is best to have a complete refresh before UAT, where QA professionals should check the refreshed environment to ensure it's working as expected. Spinning up a demo environment can be cumbersome and expensive. With Release's \",(0,t.jsx)(e.a,{href:\"https://releasehub.com/ephemeral-environments\",children:\"Ephemeral Environments\"}),\" it\\u2019s possible to view a developer\\u2019s changes at any time or any place. You don\\u2019t need to worry about having the proper environment variables in place or running any code locally.\"]}),`\n`,(0,t.jsxs)(e.h4,{id:\"create-proper-test-criteria\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#create-proper-test-criteria\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"**Create proper test criteria\"]}),`\n`,(0,t.jsx)(e.p,{children:\"**\"}),`\n`,(0,t.jsx)(e.p,{children:\"The test lead or a QA manager should work with QA professionals to be able to ensure that the test coverage is complete. Here checklists can offer an effective alternative to test cases and scripts. Also, the acceptance criteria should form the basis of testing user stories.\"}),`\n`,(0,t.jsxs)(e.h4,{id:\"be-very-specific\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#be-very-specific\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"**Be very specific\"]}),`\n`,(0,t.jsx)(e.p,{children:\"**\"}),`\n`,(0,t.jsx)(e.p,{children:\"Test cases in UAT need to be as detailed, thorough, and as specific as possible. It is best to be clear about and specify what buttons to click, what data to enter, what accounts to use, and what results the user should see. In addition to this, it should also cover how new functionality fits in well with existing pieces.\"}),`\n`,(0,t.jsxs)(e.h4,{id:\"create-robust-bug-communication-standards\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#create-robust-bug-communication-standards\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"**Create robust bug communication standards\"]}),`\n`,(0,t.jsx)(e.p,{children:\"**\"}),`\n`,(0,t.jsx)(e.p,{children:\"Bugs are inevitable in any application, and hence it is important to focus on how they are communicated to quickly resolve the issues. Make sure to be as specific as possible when creating a bug. Being vague can lead to confusion among developers, increases triaging times and further pushes back the timeline for\\xA0 the bug.\"}),`\n`,(0,t.jsxs)(e.h4,{id:\"create-user-stories-based-on-specific-business-requirements-and-scenarios\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#create-user-stories-based-on-specific-business-requirements-and-scenarios\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"**Create user stories based on specific business requirements and scenarios\"]}),`\n`,(0,t.jsx)(e.p,{children:\"**\"}),`\n`,(0,t.jsx)(e.p,{children:\"User acceptance testing can be very useful if you target the right set of users. And to be able to target the expected users, it is required to create it in sync with the business requirements. Also, before proceeding with the user acceptance tests, it's very important to clearly define the event that you would consider a success and what would define that the product passed/failed the acceptance test?\"}),`\n`,(0,t.jsxs)(e.p,{children:[\"Now that you are familiar with user acceptance testing best practices, next we'll look at \",(0,t.jsx)(e.a,{href:\"https://releasehub.com/blog/how-do-you-prepare-for-user-acceptance-testing\",children:\"How to Prepare for User Acceptance testing\"}),\".\"]})]})}function T(n={}){let{wrapper:e}=n.components||{};return e?(0,t.jsx)(e,Object.assign({},n,{children:(0,t.jsx)(h,n)})):h(n)}var A=T;return y(U);})();\n;return Component;"
        },
        "_id": "blog/posts/user-acceptance-testing-best-practices.mdx",
        "_raw": {
          "sourceFilePath": "blog/posts/user-acceptance-testing-best-practices.mdx",
          "sourceFileName": "user-acceptance-testing-best-practices.mdx",
          "sourceFileDir": "blog/posts",
          "contentType": "mdx",
          "flattenedPath": "blog/posts/user-acceptance-testing-best-practices"
        },
        "type": "BlogPost",
        "computedSlug": "user-acceptance-testing-best-practices"
      },
      "documentHash": "1739393595029",
      "hasWarnings": false,
      "documentTypeName": "BlogPost"
    },
    "blog/posts/user-acceptance-testing-checklist.mdx": {
      "document": {
        "title": "User Acceptance Testing Checklist",
        "summary": "User Acceptance Testing Checklist: Follow this checklist to get best results with your UAT testing.",
        "publishDate": "Wed Jan 05 2022 03:07:39 GMT+0000 (Coordinated Universal Time)",
        "author": "tommy-mcclung",
        "readingTime": 5,
        "categories": [
          "platform-engineering",
          "product"
        ],
        "mainImage": "/blog-images/05c8957c0f0900c77a5b259adfc56e69.jpg",
        "imageAlt": "a woman sitting at a table writing on a book",
        "showCTA": true,
        "ctaCopy": "Explore Release for seamless UAT environment setup and testing efficiency, aligning with your UAT checklist needs.",
        "ctaLink": "https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=user-acceptance-testing-checklist",
        "relatedPosts": [
          ""
        ],
        "ogImage": "/blog-images/05c8957c0f0900c77a5b259adfc56e69.jpg",
        "excerpt": "User Acceptance Testing Checklist: Follow this checklist to get best results with your UAT testing.",
        "tags": [
          "platform-engineering",
          "product"
        ],
        "ctaButton": "Try Release for Free",
        "body": {
          "raw": "\nThis is a 4-part series on User Acceptance Testing (UAT)\n\n- Part 1: [What is User Acceptance Testing and its Best Practices](https://release.com/blog/user-acceptance-testing-best-practices)\n- Part 2: [How to Prepare for User Acceptance Testing?](https://release.com/blog/how-do-you-prepare-for-user-acceptance-testing)\n- Part 3: [User Acceptance Testing Challenges & UAT Environment Examples](https://release.com/blog/what-are-the-challenges-faced-during-uat-testing)\n- **Part 4: UAT Checklist**\n\n### UAT Checklist\n\nTo be able to perform seamless user acceptance testing, it is important to ensure that the mentioned test stages are thoroughly covered as part of the UAT for best results.\n\n#### Initiating the user acceptance testing project\n\nThis is the first stage that helps to prevent any future issues. If there are several things to be kept in mind and you don’t know what to do initially, here are the stages/checklist of preparation:\n\n- Make a list and contact your future stakeholders\n- Discuss with end-users the objectives, aims, and key deliverables of the project\n- Pick a single point of contact for testing in the team\n- Ascertain all the documents and the UAT resources\n- Create the project template and gear up UAT training for the team\n\n#### Planning the User Acceptance Testing and define the methodology\n\nAt this point, make a strategy that holds all the information collected at the previous stage to help you plan the execution and formulate the final results. At this step, make sure to-\n\n- Recognize to the overall UAT methodology for evaluating the right UAT solution\n- Explicitly define the specifications of business and clarify them with the team\n- Assess various existing documentation to serve as a reference for test basis\n- Ensure that various business requirements are included and documented.\n\n#### User acceptance testing design\n\nThis stage clearly states the test points and verifies that the earlier stages are delivered successfully. At this step, make sure to-\n\n- Set clear expectations at the beginning of UAT\n- List test conditions and approaches to kick off UAT\n- Define the criteria as well as the test cases based on the existing one\n- List testing scenarios and prepare test cases\n- Ensure that the test cases consist of all the business requirements\n\n#### User acceptance testing execution\n\nWhile comprehensive preparation is the key, things can still go wrong during UAT kick-off. Therefore, the project has to be monitored, led, and tracked on all stages to achieve the UAT timeline. At this stage, make sure-\n\n- Users give enough time to execute the UAT test scripts \n- Your team executes the testing as per the defined test plan and strategy\n- All defects are reported accurately and promptly \n- Conduct meetings daily to communicate status and address concerns as they come up\n- Schedule dedicated time for defect resolution and re-testing of functionality\n\n#### UAT exit\n\nThis is the final phase and helps produce a transparent and detailed analysis. A powerful sign off on the UAT execution is critical to go live, and to ensure this, make sure to-\n\n- Generate a clear test exit report offering details of executed tests, bug raised, and existing status of all defects\n- Evaluate and accordingly take a call to officially close the UAT phase.\n\n#### User Acceptance Testing in Agile Environment\n\nThe agile environment is typically more dynamic, and in an agile world, business users will be fully involved throughout the project sprints, and the project would be accordingly enhanced based on the feedback loops from them.\n\nIn Agile teams, the entire responsibility of maximizing the product's value is with the product owner. The product owner here represents all stakeholders, including customers/users, and is the only authorized entity mentioned in the definition of user acceptance testing.\n\nThe product owner must work in close collaboration with stakeholders to understand their specific expectations and help the scrum team give feedback about the product.\n\nThe feedback that is received during sprint demo and UAT is collated and added back to the product backlog, which is reviewed and prioritized constantly. Overall, in an agile world, the business users are typically more close to the project, and unlike the traditional waterfall projects, they evaluate the same for its use on a more frequent basis.\n\nIf you are a Product Manager or Designer who would like to get a sneak peak at new features being built and need a trusted demo environment to perform User Acceptance Testing (UAT) without a hassle, you should take a look at 👉🏽 [_User Acceptance testing (UAT)_](https://release.com/user-acceptance-testing-with-ephemeral-environments) with Release [_Ephemeral Environments_](https://docs.releaseapp.io/release-overview#ephemeral-environments), With Release’s Ephemeral Environments, it’s possible to view a developer’s changes at any time or any place. You don’t need to worry about having the proper environment variables in place or running any code locally.\n\n### Conclusion\n\nFor user acceptance testing to be effective, it is important to view it as validation instead of verification. If approached right, user acceptance testing helps reduce the likelihood of issues that typically happen in web development projects, which reduces the amount of work and effort required in development and maintenance.\n",
          "code": "var Component=(()=>{var d=Object.create;var s=Object.defineProperty;var p=Object.getOwnPropertyDescriptor;var u=Object.getOwnPropertyNames;var g=Object.getPrototypeOf,m=Object.prototype.hasOwnProperty;var f=(n,e)=>()=>(e||n((e={exports:{}}).exports,e),e.exports),y=(n,e)=>{for(var i in e)s(n,i,{get:e[i],enumerable:!0})},r=(n,e,i,c)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let a of u(e))!m.call(n,a)&&a!==i&&s(n,a,{get:()=>e[a],enumerable:!(c=p(e,a))||c.enumerable});return n};var k=(n,e,i)=>(i=n!=null?d(g(n)):{},r(e||!n||!n.__esModule?s(i,\"default\",{value:n,enumerable:!0}):i,n)),b=n=>r(s({},\"__esModule\",{value:!0}),n);var l=f((x,o)=>{o.exports=_jsx_runtime});var w={};y(w,{default:()=>A,frontmatter:()=>v});var t=k(l()),v={title:\"User Acceptance Testing Checklist\",summary:\"User Acceptance Testing Checklist: Follow this checklist to get best results with your UAT testing.\",publishDate:\"Wed Jan 05 2022 03:07:39 GMT+0000 (Coordinated Universal Time)\",author:\"tommy-mcclung\",readingTime:5,categories:[\"platform-engineering\",\"product\"],mainImage:\"/blog-images/05c8957c0f0900c77a5b259adfc56e69.jpg\",imageAlt:\"a woman sitting at a table writing on a book\",showCTA:!0,ctaCopy:\"Explore Release for seamless UAT environment setup and testing efficiency, aligning with your UAT checklist needs.\",ctaLink:\"https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=user-acceptance-testing-checklist\",relatedPosts:[\"\"],ogImage:\"/blog-images/05c8957c0f0900c77a5b259adfc56e69.jpg\",excerpt:\"User Acceptance Testing Checklist: Follow this checklist to get best results with your UAT testing.\",tags:[\"platform-engineering\",\"product\"],ctaButton:\"Try Release for Free\"};function h(n){let e=Object.assign({p:\"p\",ul:\"ul\",li:\"li\",a:\"a\",strong:\"strong\",h3:\"h3\",span:\"span\",h4:\"h4\",em:\"em\"},n.components);return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(e.p,{children:\"This is a 4-part series on User Acceptance Testing (UAT)\"}),`\n`,(0,t.jsxs)(e.ul,{children:[`\n`,(0,t.jsxs)(e.li,{children:[\"Part 1: \",(0,t.jsx)(e.a,{href:\"https://release.com/blog/user-acceptance-testing-best-practices\",children:\"What is User Acceptance Testing and its Best Practices\"})]}),`\n`,(0,t.jsxs)(e.li,{children:[\"Part 2: \",(0,t.jsx)(e.a,{href:\"https://release.com/blog/how-do-you-prepare-for-user-acceptance-testing\",children:\"How to Prepare for User Acceptance Testing?\"})]}),`\n`,(0,t.jsxs)(e.li,{children:[\"Part 3: \",(0,t.jsx)(e.a,{href:\"https://release.com/blog/what-are-the-challenges-faced-during-uat-testing\",children:\"User Acceptance Testing Challenges & UAT Environment Examples\"})]}),`\n`,(0,t.jsx)(e.li,{children:(0,t.jsx)(e.strong,{children:\"Part 4: UAT Checklist\"})}),`\n`]}),`\n`,(0,t.jsxs)(e.h3,{id:\"uat-checklist\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#uat-checklist\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"UAT Checklist\"]}),`\n`,(0,t.jsx)(e.p,{children:\"To be able to perform seamless user acceptance testing, it is important to ensure that the mentioned test stages are thoroughly covered as part of the UAT for best results.\"}),`\n`,(0,t.jsxs)(e.h4,{id:\"initiating-the-user-acceptance-testing-project\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#initiating-the-user-acceptance-testing-project\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Initiating the user acceptance testing project\"]}),`\n`,(0,t.jsx)(e.p,{children:\"This is the first stage that helps to prevent any future issues. If there are several things to be kept in mind and you don\\u2019t know what to do initially, here are the stages/checklist of preparation:\"}),`\n`,(0,t.jsxs)(e.ul,{children:[`\n`,(0,t.jsx)(e.li,{children:\"Make a list and contact your future stakeholders\"}),`\n`,(0,t.jsx)(e.li,{children:\"Discuss with end-users the objectives, aims, and key deliverables of the project\"}),`\n`,(0,t.jsx)(e.li,{children:\"Pick a single point of contact for testing in the team\"}),`\n`,(0,t.jsx)(e.li,{children:\"Ascertain all the documents and the UAT resources\"}),`\n`,(0,t.jsx)(e.li,{children:\"Create the project template and gear up UAT training for the team\"}),`\n`]}),`\n`,(0,t.jsxs)(e.h4,{id:\"planning-the-user-acceptance-testing-and-define-the-methodology\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#planning-the-user-acceptance-testing-and-define-the-methodology\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Planning the User Acceptance Testing and define the methodology\"]}),`\n`,(0,t.jsx)(e.p,{children:\"At this point, make a strategy that holds all the information collected at the previous stage to help you plan the execution and formulate the final results. At this step, make sure to-\"}),`\n`,(0,t.jsxs)(e.ul,{children:[`\n`,(0,t.jsx)(e.li,{children:\"Recognize to the overall UAT methodology for evaluating the right UAT solution\"}),`\n`,(0,t.jsx)(e.li,{children:\"Explicitly define the specifications of business and clarify them with the team\"}),`\n`,(0,t.jsx)(e.li,{children:\"Assess various existing documentation to serve as a reference for test basis\"}),`\n`,(0,t.jsx)(e.li,{children:\"Ensure that various business requirements are included and documented.\"}),`\n`]}),`\n`,(0,t.jsxs)(e.h4,{id:\"user-acceptance-testing-design\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#user-acceptance-testing-design\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"User acceptance testing design\"]}),`\n`,(0,t.jsx)(e.p,{children:\"This stage clearly states the test points and verifies that the earlier stages are delivered successfully. At this step, make sure to-\"}),`\n`,(0,t.jsxs)(e.ul,{children:[`\n`,(0,t.jsx)(e.li,{children:\"Set clear expectations at the beginning of UAT\"}),`\n`,(0,t.jsx)(e.li,{children:\"List test conditions and approaches to kick off UAT\"}),`\n`,(0,t.jsx)(e.li,{children:\"Define the criteria as well as the test cases based on the existing one\"}),`\n`,(0,t.jsx)(e.li,{children:\"List testing scenarios and prepare test cases\"}),`\n`,(0,t.jsx)(e.li,{children:\"Ensure that the test cases consist of all the business requirements\"}),`\n`]}),`\n`,(0,t.jsxs)(e.h4,{id:\"user-acceptance-testing-execution\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#user-acceptance-testing-execution\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"User acceptance testing execution\"]}),`\n`,(0,t.jsx)(e.p,{children:\"While comprehensive preparation is the key, things can still go wrong during UAT kick-off. Therefore, the project has to be monitored, led, and tracked on all stages to achieve the UAT timeline. At this stage, make sure-\"}),`\n`,(0,t.jsxs)(e.ul,{children:[`\n`,(0,t.jsx)(e.li,{children:\"Users give enough time to execute the UAT test scripts\\xA0\"}),`\n`,(0,t.jsx)(e.li,{children:\"Your team executes the testing as per the defined test plan and strategy\"}),`\n`,(0,t.jsx)(e.li,{children:\"All defects are reported accurately and promptly\\xA0\"}),`\n`,(0,t.jsx)(e.li,{children:\"Conduct meetings daily to communicate status and address concerns as they come up\"}),`\n`,(0,t.jsx)(e.li,{children:\"Schedule dedicated time for defect resolution and re-testing of functionality\"}),`\n`]}),`\n`,(0,t.jsxs)(e.h4,{id:\"uat-exit\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#uat-exit\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"UAT exit\"]}),`\n`,(0,t.jsx)(e.p,{children:\"This is the final phase and helps produce a transparent and detailed analysis. A powerful sign off on the UAT execution is critical to go live, and to ensure this, make sure to-\"}),`\n`,(0,t.jsxs)(e.ul,{children:[`\n`,(0,t.jsx)(e.li,{children:\"Generate a clear test exit report offering details of executed tests, bug raised, and existing status of all defects\"}),`\n`,(0,t.jsx)(e.li,{children:\"Evaluate and accordingly take a call to officially close the UAT phase.\"}),`\n`]}),`\n`,(0,t.jsxs)(e.h4,{id:\"user-acceptance-testing-in-agile-environment\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#user-acceptance-testing-in-agile-environment\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"User Acceptance Testing in Agile Environment\"]}),`\n`,(0,t.jsx)(e.p,{children:\"The agile environment is typically more dynamic, and in an agile world, business users will be fully involved throughout the project sprints, and the project would be accordingly enhanced based on the feedback loops from them.\"}),`\n`,(0,t.jsx)(e.p,{children:\"In Agile teams, the entire responsibility of maximizing the product's value is with the product owner. The product owner here represents all stakeholders, including customers/users, and is the only authorized entity mentioned in the definition of user acceptance testing.\"}),`\n`,(0,t.jsx)(e.p,{children:\"The product owner must work in close collaboration with stakeholders to understand their specific expectations and help the scrum team give feedback about the product.\"}),`\n`,(0,t.jsx)(e.p,{children:\"The feedback that is received during sprint demo and UAT is collated and added back to the product backlog, which is reviewed and prioritized constantly. Overall, in an agile world, the business users are typically more close to the project, and unlike the traditional waterfall projects, they evaluate the same for its use on a more frequent basis.\"}),`\n`,(0,t.jsxs)(e.p,{children:[\"If you are a Product Manager or Designer who would like to get a sneak peak at new features being built and need a trusted demo environment to perform User Acceptance Testing (UAT) without a hassle, you should take a look at \\u{1F449}\\u{1F3FD} \",(0,t.jsx)(e.a,{href:\"https://release.com/user-acceptance-testing-with-ephemeral-environments\",children:(0,t.jsx)(e.em,{children:\"User Acceptance testing (UAT)\"})}),\" with Release \",(0,t.jsx)(e.a,{href:\"https://docs.releaseapp.io/release-overview#ephemeral-environments\",children:(0,t.jsx)(e.em,{children:\"Ephemeral Environments\"})}),\", With Release\\u2019s Ephemeral Environments, it\\u2019s possible to view a developer\\u2019s changes at any time or any place. You don\\u2019t need to worry about having the proper environment variables in place or running any code locally.\"]}),`\n`,(0,t.jsxs)(e.h3,{id:\"conclusion\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#conclusion\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Conclusion\"]}),`\n`,(0,t.jsx)(e.p,{children:\"For user acceptance testing to be effective, it is important to view it as validation instead of verification. If approached right, user acceptance testing helps reduce the likelihood of issues that typically happen in web development projects, which reduces the amount of work and effort required in development and maintenance.\"})]})}function T(n={}){let{wrapper:e}=n.components||{};return e?(0,t.jsx)(e,Object.assign({},n,{children:(0,t.jsx)(h,n)})):h(n)}var A=T;return b(w);})();\n;return Component;"
        },
        "_id": "blog/posts/user-acceptance-testing-checklist.mdx",
        "_raw": {
          "sourceFilePath": "blog/posts/user-acceptance-testing-checklist.mdx",
          "sourceFileName": "user-acceptance-testing-checklist.mdx",
          "sourceFileDir": "blog/posts",
          "contentType": "mdx",
          "flattenedPath": "blog/posts/user-acceptance-testing-checklist"
        },
        "type": "BlogPost",
        "computedSlug": "user-acceptance-testing-checklist"
      },
      "documentHash": "1739393595029",
      "hasWarnings": false,
      "documentTypeName": "BlogPost"
    },
    "blog/posts/using-docker-environment-variables-in-compose.mdx": {
      "document": {
        "title": "Using Docker Environment Variables in Compose",
        "summary": "This post will explain what Docker Compose variables are, their use, and the risks involved with environment variables.",
        "publishDate": "Mon Jan 30 2023 08:31:35 GMT+0000 (Coordinated Universal Time)",
        "author": "mercy-kibet",
        "readingTime": 6,
        "categories": [
          "platform-engineering",
          "product"
        ],
        "mainImage": "",
        "imageAlt": "",
        "showCTA": true,
        "ctaCopy": "Unlock streamlined environment management with Release.com for secure Docker Compose variables handling and efficient application configuration.",
        "ctaLink": "https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=using-docker-environment-variables-in-compose",
        "relatedPosts": [
          ""
        ],
        "ogImage": "/blog/default-og-image.png",
        "excerpt": "This post will explain what Docker Compose variables are, their use, and the risks involved with environment variables.",
        "tags": [
          "platform-engineering",
          "product"
        ],
        "ctaButton": "Try Release for Free",
        "body": {
          "raw": "\nAs a developer, you probably use Docker to run your application efficiently. Containerizing helps you avoid the \"but it works on my computer\" problem. You might even use Docker Compose to manage different services that run on different containers.\n\nSome services managed by Docker Compose, like backend services, may have sensitive information that should be kept secret. This is where you need to use environment variables to specify your configuration. But what are Docker environment variables in Compose?\n\nThis post will explain what Docker Compose variables are, how you can use them in Compose, and the risks associated with putting secrets in an environment variable.\n\n![](/blog-images/9c896058ea675ccb883dc3596b6c371d.png)\n\n### What is a Docker Environment Variable?\n\nEnvironment variables are used in programs to store values that the program checks at runtime. This means that the value is not stored in code but is instead stored in a separate file.\n\nA Docker environment variable is a variable that's passed to a Docker container when it's created. You can use environment variables to configure your application. Also, you can use them to store sensitive information like keys and passwords.\n\nYou can set environment variables in several ways, such as using the **ENV** instruction in a Dockerfile, using the **\\-e** flag when running the **docker run** command, or using environment files.\n\nWhen a container is created, the environment variables are passed to it. You can access them within the container. For example, you can access environment variables in a Linux-based container using the **$** notation like **$APP_ENV** or echo $APP_ENV.\n\nUsing Docker environment variables keeps your application configuration flexible.\n\n### What is Docker Compose?\n\n[Docker Compose](https://docs.docker.com/compose/) is a tool that spins up instances of your Dockerfile where your Dockerfile is the blueprint of your application. It helps you manage and configure your app's specific requirements. It also gives you the flexibility to define different services your app needs. For example, you may have different Dockerfiles for different services, such as the frontend and the backend.\n\nBy using Docker Compose, you can use one file to configure the relationship between the two services. This single file gives you the ability to use only a single command to build your entire application.\n\n![](/blog-images/9e2d3acac8f698700a5019ce789f41ca.png)\n\n### Can I use Environment Variables in a Docker Compose File?\n\nYes, you can use environment variables in a Docker Compose file.\n\nDocker and Compose work together to provide a way to manage and run containers. When using Compose, you define your application's services, networks, and volumes in a single **docker-compose.yml** file.\n\nUse environment variables to set specific options in the Compose file, such as image name, command, ports, volumes, and links. You can set these values in different ways, such as by using the **environment** key in the compose file, by using the **\\-e** flag when running the Docker run command, or by using environment files.\n\nWhen you run the **docker-compose up** command, Compose reads the **docker-compose.yml** file and creates the specified services, networks, and volumes. As part of this process, Compose also sets the environment variables for each service as specified in the Compose file.\n\nUsing environment variables in a Compose file can make your application more flexible and configurable. You can use different environment variables to set different values for different stages of your application, such as development, staging, and production.\n\n### How to use Docker Environment Variables in Compose\n\nYou can set and pass Docker environment variables in several ways in Compose. Some of these ways include the following:\n**Environment Key:** You can configure a container by setting environment variables in the Compose file. If you want to use your app in production mode, you can set the value of the APP_ENV variable like so:\n\n`services:   web:     environment:       - APP_ENV=production`\n\n**\\-e Flag:** You can also set environment variables when running a container by using the -e flag. For example, you can set the variable APP_ENV with a value of production when running a container like this:\n\n`docker compose run -e APP_ENV=production myimage`\n\n**Environment Files:** You can also use environment files to set environment variables. This can be useful when you have multiple environment variables that you want to set or when you want to keep your environment variables separate from your Compose file.\n\n`services:   web:     env_file:       - Docker/web/web.env`\n\nTo use environment files, you can pass the --env-file flag when running the Compose command: docker-compose --env-file /path/toenv.env up. This will override the default path.\n**.env:** The .env file is a simple text file containing key-value pairs, with one pair per line. The .env file should be in the same directory as the **docker-compose.yml** file. You don't need to pass any flag when running the Compose command. Compose will automatically pick the .env file. If you defined a version to your web app in your .env file, this is how you'll use it in Compose:\n\n`services:   web:     image: \"webapp:${VERSION}\"`\n\nAlways remember that environment variables passed to a container are only visible to the processes running in that container. If you need to share environment variables between containers, you can use a tool such as Docker Compose's environment key or a third-party tool like a key-value store.\n\n### How to Substitute Environment Variables\n\nUsing environment variables in Compose allows substituting values at runtime rather than hard coding them in the Compose file. This makes it easy to switch between different [environments](https://release.com/blog/environments-as-a-service-eaas-top-3-benefits), such as development, staging, and production, without modifying the Compose file.\n\nOne way to manage this is by using multiple environment files, each with its own values. For example, you can have a **development.env** file with development-specific values and a **production.env** file with production-specific values.\n\nWhen running the Compose command, you can specify which environment file to use with the **\\-f** flag. For example, docker-compose -f docker-compose.yml -f development.env will start the containers with the values specified in the **development.env** file.\n\nThis approach allows you to keep your environment-specific values separate from your Compose file, making it easy to switch between environments and maintain different configurations for different application stages.\n\n### The Security Risks of Putting Secrets in Environment Variables\n\nThere are risks associated with putting secrets such as passwords and API keys in environment variables. Here are a few examples:\n\n- Anyone with access to the host system can access environment variables. If attackers gain access to the host system, they might access any secrets stored in environment variables.\n- Any process running on the host system can access environment variables. Thus, an attacker can run a malicious process to access the sensitive information stored in the environment variable and gain control of your application.\n- If an environment variable contains a secret, it might be logged or displayed in plain text, allowing anyone with access to the logs or display to see the secrets.\n- Suppose a developer pushes code to a public repository with a file containing secret environment variables. In that case, the secret might be exposed to anyone with access to the repository.\n- To mitigate these risks, it's important to be careful when using environment variables to store secrets and to use other secure methods, such as encrypted secrets stores or secret management tools, whenever possible.\n\n![](/blog-images/905a0239a88ecb381cd171d5404bd2e5.png)\n\n### Conclusion\n\nDocker environment variables are useful for configuring and managing containerized applications with Compose. You can use them to pass information to the containers at runtime and to override the default values defined in the Compose file. Be careful when handling sensitive information as environment variables are stored in plain text and visible to any process inside the container. Having looked at how you can use environment variables in Compose, you should also look at the [benefits of having environments as a service.](https://release.com/blog/environments-as-a-service-eaas-top-3-benefits)\n\n_This post was written by Mercy Kibet._ [_Mercy_](https://hashnode.com/@eiMJay) _is a full-stack developer with a knack for learning and writing about new and intriguing tech stacks._\n",
          "code": "var Component=(()=>{var m=Object.create;var a=Object.defineProperty;var d=Object.getOwnPropertyDescriptor;var p=Object.getOwnPropertyNames;var u=Object.getPrototypeOf,v=Object.prototype.hasOwnProperty;var f=(i,e)=>()=>(e||i((e={exports:{}}).exports,e),e.exports),g=(i,e)=>{for(var o in e)a(i,o,{get:e[o],enumerable:!0})},r=(i,e,o,t)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let s of p(e))!v.call(i,s)&&s!==o&&a(i,s,{get:()=>e[s],enumerable:!(t=d(e,s))||t.enumerable});return i};var b=(i,e,o)=>(o=i!=null?m(u(i)):{},r(e||!i||!i.__esModule?a(o,\"default\",{value:i,enumerable:!0}):o,i)),y=i=>r(a({},\"__esModule\",{value:!0}),i);var l=f((T,c)=>{c.exports=_jsx_runtime});var D={};g(D,{default:()=>C,frontmatter:()=>k});var n=b(l()),k={title:\"Using Docker Environment Variables in Compose\",summary:\"This post will explain what Docker Compose variables are, their use, and the risks involved with environment variables.\",publishDate:\"Mon Jan 30 2023 08:31:35 GMT+0000 (Coordinated Universal Time)\",author:\"mercy-kibet\",readingTime:6,categories:[\"platform-engineering\",\"product\"],mainImage:\"\",imageAlt:\"\",showCTA:!0,ctaCopy:\"Unlock streamlined environment management with Release.com for secure Docker Compose variables handling and efficient application configuration.\",ctaLink:\"https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=using-docker-environment-variables-in-compose\",relatedPosts:[\"\"],ogImage:\"/blog/default-og-image.png\",excerpt:\"This post will explain what Docker Compose variables are, their use, and the risks involved with environment variables.\",tags:[\"platform-engineering\",\"product\"],ctaButton:\"Try Release for Free\"};function h(i){let e=Object.assign({p:\"p\",img:\"img\",h3:\"h3\",a:\"a\",span:\"span\",strong:\"strong\",code:\"code\",ul:\"ul\",li:\"li\",em:\"em\"},i.components);return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(e.p,{children:'As a developer, you probably use Docker to run your application efficiently. Containerizing helps you avoid the \"but it works on my computer\" problem. You might even use Docker Compose to manage different services that run on different containers.'}),`\n`,(0,n.jsx)(e.p,{children:\"Some services managed by Docker Compose, like backend services, may have sensitive information that should be kept secret. This is where you need to use environment variables to specify your configuration. But what are Docker environment variables in Compose?\"}),`\n`,(0,n.jsx)(e.p,{children:\"This post will explain what Docker Compose variables are, how you can use them in Compose, and the risks associated with putting secrets in an environment variable.\"}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/9c896058ea675ccb883dc3596b6c371d.png\",alt:\"\"})}),`\n`,(0,n.jsxs)(e.h3,{id:\"what-is-a-docker-environment-variable\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#what-is-a-docker-environment-variable\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"What is a Docker Environment Variable?\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Environment variables are used in programs to store values that the program checks at runtime. This means that the value is not stored in code but is instead stored in a separate file.\"}),`\n`,(0,n.jsx)(e.p,{children:\"A Docker environment variable is a variable that's passed to a Docker container when it's created. You can use environment variables to configure your application. Also, you can use them to store sensitive information like keys and passwords.\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"You can set environment variables in several ways, such as using the \",(0,n.jsx)(e.strong,{children:\"ENV\"}),\" instruction in a Dockerfile, using the \",(0,n.jsx)(e.strong,{children:\"-e\"}),\" flag when running the \",(0,n.jsx)(e.strong,{children:\"docker run\"}),\" command, or using environment files.\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"When a container is created, the environment variables are passed to it. You can access them within the container. For example, you can access environment variables in a Linux-based container using the \",(0,n.jsx)(e.strong,{children:\"$\"}),\" notation like \",(0,n.jsx)(e.strong,{children:\"$APP_ENV\"}),\" or echo $APP_ENV.\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Using Docker environment variables keeps your application configuration flexible.\"}),`\n`,(0,n.jsxs)(e.h3,{id:\"what-is-docker-compose\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#what-is-docker-compose\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"What is Docker Compose?\"]}),`\n`,(0,n.jsxs)(e.p,{children:[(0,n.jsx)(e.a,{href:\"https://docs.docker.com/compose/\",children:\"Docker Compose\"}),\" is a tool that spins up instances of your Dockerfile where your Dockerfile is the blueprint of your application. It helps you manage and configure your app's specific requirements. It also gives you the flexibility to define different services your app needs. For example, you may have different Dockerfiles for different services, such as the frontend and the backend.\"]}),`\n`,(0,n.jsx)(e.p,{children:\"By using Docker Compose, you can use one file to configure the relationship between the two services. This single file gives you the ability to use only a single command to build your entire application.\"}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/9e2d3acac8f698700a5019ce789f41ca.png\",alt:\"\"})}),`\n`,(0,n.jsxs)(e.h3,{id:\"can-i-use-environment-variables-in-a-docker-compose-file\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#can-i-use-environment-variables-in-a-docker-compose-file\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Can I use Environment Variables in a Docker Compose File?\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Yes, you can use environment variables in a Docker Compose file.\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"Docker and Compose work together to provide a way to manage and run containers. When using Compose, you define your application's services, networks, and volumes in a single \",(0,n.jsx)(e.strong,{children:\"docker-compose.yml\"}),\" file.\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"Use environment variables to set specific options in the Compose file, such as image name, command, ports, volumes, and links. You can set these values in different ways, such as by using the \",(0,n.jsx)(e.strong,{children:\"environment\"}),\" key in the compose file, by using the \",(0,n.jsx)(e.strong,{children:\"-e\"}),\" flag when running the Docker run command, or by using environment files.\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"When you run the \",(0,n.jsx)(e.strong,{children:\"docker-compose up\"}),\" command, Compose reads the \",(0,n.jsx)(e.strong,{children:\"docker-compose.yml\"}),\" file and creates the specified services, networks, and volumes. As part of this process, Compose also sets the environment variables for each service as specified in the Compose file.\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Using environment variables in a Compose file can make your application more flexible and configurable. You can use different environment variables to set different values for different stages of your application, such as development, staging, and production.\"}),`\n`,(0,n.jsxs)(e.h3,{id:\"how-to-use-docker-environment-variables-in-compose\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#how-to-use-docker-environment-variables-in-compose\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"How to use Docker Environment Variables in Compose\"]}),`\n`,(0,n.jsxs)(e.p,{children:[`You can set and pass Docker environment variables in several ways in Compose. Some of these ways include the following:\n`,(0,n.jsx)(e.strong,{children:\"Environment Key:\"}),\" You can configure a container by setting environment variables in the Compose file. If you want to use your app in production mode, you can set the value of the APP_ENV variable like so:\"]}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.code,{children:\"services:  \\xA0web:  \\xA0 \\xA0environment:  \\xA0 \\xA0 \\xA0- APP_ENV=production\"})}),`\n`,(0,n.jsxs)(e.p,{children:[(0,n.jsx)(e.strong,{children:\"-e Flag:\"}),\" You can also set environment variables when running a container by using the -e flag. For example, you can set the variable APP_ENV with a value of production when running a container like this:\"]}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.code,{children:\"docker compose run -e APP_ENV=production myimage\"})}),`\n`,(0,n.jsxs)(e.p,{children:[(0,n.jsx)(e.strong,{children:\"Environment Files:\"}),\" You can also use environment files to set environment variables. This can be useful when you have multiple environment variables that you want to set or when you want to keep your environment variables separate from your Compose file.\"]}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.code,{children:\"services:  \\xA0web:  \\xA0 \\xA0env_file:  \\xA0 \\xA0 \\xA0- Docker/web/web.env\"})}),`\n`,(0,n.jsxs)(e.p,{children:[`To use environment files, you can pass the --env-file flag when running the Compose command: docker-compose --env-file /path/toenv.env up. This will override the default path.\n`,(0,n.jsx)(e.strong,{children:\".env:\"}),\" The .env file is a simple text file containing key-value pairs, with one pair per line. The .env file should be in the same directory as the \",(0,n.jsx)(e.strong,{children:\"docker-compose.yml\"}),\" file. You don't need to pass any flag when running the Compose command. Compose will automatically pick the .env file. If you defined a version to your web app in your .env file, this is how you'll use it in Compose:\"]}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.code,{children:'services:  \\xA0web:  \\xA0 \\xA0image: \"webapp:${VERSION}\"'})}),`\n`,(0,n.jsx)(e.p,{children:\"Always remember that environment variables passed to a container are only visible to the processes running in that container. If you need to share environment variables between containers, you can use a tool such as Docker Compose's environment key or a third-party tool like a key-value store.\"}),`\n`,(0,n.jsxs)(e.h3,{id:\"how-to-substitute-environment-variables\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#how-to-substitute-environment-variables\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"How to Substitute Environment Variables\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"Using environment variables in Compose allows substituting values at runtime rather than hard coding them in the Compose file. This makes it easy to switch between different \",(0,n.jsx)(e.a,{href:\"https://release.com/blog/environments-as-a-service-eaas-top-3-benefits\",children:\"environments\"}),\", such as development, staging, and production, without modifying the Compose file.\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"One way to manage this is by using multiple environment files, each with its own values. For example, you can have a \",(0,n.jsx)(e.strong,{children:\"development.env\"}),\" file with development-specific values and a \",(0,n.jsx)(e.strong,{children:\"production.env\"}),\" file with production-specific values.\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"When running the Compose command, you can specify which environment file to use with the \",(0,n.jsx)(e.strong,{children:\"-f\"}),\" flag. For example, docker-compose -f docker-compose.yml -f development.env will start the containers with the values specified in the \",(0,n.jsx)(e.strong,{children:\"development.env\"}),\" file.\"]}),`\n`,(0,n.jsx)(e.p,{children:\"This approach allows you to keep your environment-specific values separate from your Compose file, making it easy to switch between environments and maintain different configurations for different application stages.\"}),`\n`,(0,n.jsxs)(e.h3,{id:\"the-security-risks-of-putting-secrets-in-environment-variables\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#the-security-risks-of-putting-secrets-in-environment-variables\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"The Security Risks of Putting Secrets in Environment Variables\"]}),`\n`,(0,n.jsx)(e.p,{children:\"There are risks associated with putting secrets such as passwords and API keys in environment variables. Here are a few examples:\"}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsx)(e.li,{children:\"Anyone with access to the host system can access environment variables. If attackers gain access to the host system, they might access any secrets stored in environment variables.\"}),`\n`,(0,n.jsx)(e.li,{children:\"Any process running on the host system can access environment variables. Thus, an attacker can run a malicious process to access the sensitive information stored in the environment variable and gain control of your application.\"}),`\n`,(0,n.jsx)(e.li,{children:\"If an environment variable contains a secret, it might be logged or displayed in plain text, allowing anyone with access to the logs or display to see the secrets.\"}),`\n`,(0,n.jsx)(e.li,{children:\"Suppose a developer pushes code to a public repository with a file containing secret environment variables. In that case, the secret might be exposed to anyone with access to the repository.\"}),`\n`,(0,n.jsx)(e.li,{children:\"To mitigate these risks, it's important to be careful when using environment variables to store secrets and to use other secure methods, such as encrypted secrets stores or secret management tools, whenever possible.\"}),`\n`]}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/905a0239a88ecb381cd171d5404bd2e5.png\",alt:\"\"})}),`\n`,(0,n.jsxs)(e.h3,{id:\"conclusion\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#conclusion\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Conclusion\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"Docker environment variables are useful for configuring and managing containerized applications with Compose. You can use them to pass information to the containers at runtime and to override the default values defined in the Compose file. Be careful when handling sensitive information as environment variables are stored in plain text and visible to any process inside the container. Having looked at how you can use environment variables in Compose, you should also look at the \",(0,n.jsx)(e.a,{href:\"https://release.com/blog/environments-as-a-service-eaas-top-3-benefits\",children:\"benefits of having environments as a service.\"})]}),`\n`,(0,n.jsxs)(e.p,{children:[(0,n.jsx)(e.em,{children:\"This post was written by Mercy Kibet.\"}),\" \",(0,n.jsx)(e.a,{href:\"https://hashnode.com/@eiMJay\",children:(0,n.jsx)(e.em,{children:\"Mercy\"})}),\" \",(0,n.jsx)(e.em,{children:\"is a full-stack developer with a knack for learning and writing about new and intriguing tech stacks.\"})]})]})}function w(i={}){let{wrapper:e}=i.components||{};return e?(0,n.jsx)(e,Object.assign({},i,{children:(0,n.jsx)(h,i)})):h(i)}var C=w;return y(D);})();\n;return Component;"
        },
        "_id": "blog/posts/using-docker-environment-variables-in-compose.mdx",
        "_raw": {
          "sourceFilePath": "blog/posts/using-docker-environment-variables-in-compose.mdx",
          "sourceFileName": "using-docker-environment-variables-in-compose.mdx",
          "sourceFileDir": "blog/posts",
          "contentType": "mdx",
          "flattenedPath": "blog/posts/using-docker-environment-variables-in-compose"
        },
        "type": "BlogPost",
        "computedSlug": "using-docker-environment-variables-in-compose"
      },
      "documentHash": "1739393595029",
      "hasWarnings": false,
      "documentTypeName": "BlogPost"
    },
    "blog/posts/using-environment-variables-in-angular-a-guide.mdx": {
      "document": {
        "title": "Using Environment Variables in Angular: A Guide",
        "summary": "What are environments? Why do you need them? And how do you correctly use environment variables in an Angular app?",
        "publishDate": "Mon Jan 23 2023 14:06:16 GMT+0000 (Coordinated Universal Time)",
        "author": "",
        "readingTime": 5,
        "categories": [
          "platform-engineering",
          "product"
        ],
        "mainImage": "/blog-images/9de6930e0d988897966633594a3cfe34.png",
        "imageAlt": "Using Environment Variables in Angular: A Guide",
        "showCTA": true,
        "ctaCopy": "By automating environment setup like Angular, Release simplifies managing different environments for seamless deployment and testing.",
        "ctaLink": "https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=using-environment-variables-in-angular-a-guide",
        "relatedPosts": [
          ""
        ],
        "ogImage": "/blog-images/9de6930e0d988897966633594a3cfe34.png",
        "excerpt": "Learn how to effectively use environment variables in your Angular applications for better configuration management.",
        "tags": [
          "platform-engineering",
          "product"
        ],
        "ctaButton": "Try Release for Free",
        "date": "2024-02-05",
        "body": {
          "raw": "\n![angular environment](/blog-images/c4635aef1048ce157456670de7326463.png)\n\nAngular is one of the most popular and preferred frontend frameworks today, especially among large engineering teams. It's backed by a robust open-source community that makes it an ideal choice for building scalable web applications.\n\nHowever, being a frontend framework, your Angular application at some point needs to communicate with an external API or a third-party service application.\n\nFor verifying authentic communication between your Angular application and the server, you'll need to use API keys and URLs that may change based on the environment of your Angular application.\n\nIn this post, I'll walk you through how to use environment variables in an Angular application.\n\n![](/blog-images/6e56bd2300ad21054eb696f30be767e8.png)\n\n## **What are Environments in Angular?**\n\nBefore we get into what environment variables are, let's quickly understand what an environment means for Angular. Angular by default provides a way to detect and modify some files in your application based on where the application is running.\n\nBroadly speaking, Angular considers two environments for any application. One is the development environment, which means you're running Angular locally on your own system. This is when you're actually writing code for your Angular application.\n\nThe other environment is production. This is where your Angular application is running for the end users on a domain or a server. This environment comes into the picture when you create a JavaScript bundle for your Angular application and deploy it to production.\n\n## **How Does Angular Know Which Environment?**\n\nNow that we understand what environments are in Angular, let's see how Angular maintains and keeps track of them.\n\nCreate a fresh Angular app by running the following:\n\nng new angular-environment-variables\n\nTo keep track of different environments, Angular maintains an environment directory in the src directory:\n\n![](/blog-images/f825bb67f75affcad84aca257f1ff2a4.png)\n\nNotice that Angular maintains two environment files called **environment.ts** and **environment.prod.ts**.\n\nNow, let's go to the **angular.json** file in the root directory. If you look closely at the **configurations** section, you'll notice it has some configurations declared for **production** and **development**.\n\n![angular environment](/blog-images/b7ee3bff372e7733bef1883ec4284b1f.png)\n\nIf you take a look at the **fileReplacements** array in this file, you'll see it instructs Angular to replace the **environment.ts** file with the **environment.prod.ts** file for production. So, when you run the **ng build** command and deploy your Angular application in production, Angular automatically switches these two files so that the production bundle uses the production environment.\n\n## **Detecting Environment in Angular**\n\nAngular makes it really easy for you to detect the environment you're running it in. There are also other ways you can do this, including by checking the URL of your application's domain. But Angular provides an out-of-the-box solution for this.\n\nLet's explore this a bit.\n\nIn the **src/app/app.component.ts** file, import the **isDevMode** from **@angular/core**:\n\n```typescript\nimport { Component, isDevMode } from \"@angular/core\";\n```\n\nThen, create a variable called **environment** inside the AppComponent:\n\n```typescript\n@Component({\n  selector: \"app-root\",\n  template: \"<h1>{{title}}</h1>\",\n})\nexport class AppComponent {\n  title = \"angular-environment-variables\";\n  environment = \"\";\n}\n```\n\nNow we can use the **isDevMode** to populate the **environment** variable above:\n\n```typescript\n@Component({\n  selector: \"app-root\",\n  template: \"<h1>{{title}}</h1>\",\n})\nexport class AppComponent {\n  title = \"angular-environment-variables\";\n  environment = \"\";\n\n  ngOnInit() {\n    if (isDevMode()) {\n      this.environment = \"Development\";\n    } else {\n      this.environment = \"Production\";\n    }\n  }\n}\n```\n\nLet's now render the **environment** variable in the component's HTML:\n\n```html\n<h1>This is Angular app running in {{environment}} Environment</h1>\n```\n\nIf you visit your Angular development server, it should tell you that your Angular app is running in a development environment:\n\n![](/blog-images/5edfe8892fb6dacc5f586f73cfba9ce5.png)\n\nGreat!\n\nNow let's look at how we can modify or update our environment files to use environment variables in our Angular application.\n\n## **Using Environment Variables in Angular**\n\nNow that you understand how Angular maintains and keeps track of the environment, let's play around with the environment directory that we have in the root directory.\n\nFirst, let's look at the default contents of each. The **/src/environments/environment.ts** file contains the following code by default:\n\n```typescript\n// This file can be replaced during build by using the `fileReplacements` array.\n// `ng build` replaces `environment.ts` with `environment.prod.ts`.\n// The list of file replacements can be found in `angular.json`.\nexport const environment = {\n  production: false,\n};\n\n/*\n * For easier debugging in development mode, you can import the following file\n * to ignore zone related error stack frames such as `zone.run`, `zoneDelegate.invokeTask`.\n *\n * This import should be commented out in production mode because it will have a negative impact\n * on performance if an error is thrown.\n */\n// import 'zone.js/plugins/zone-error';\n// Included with Angular CLI.\n```\n\nSimilarly, if we look at the **/src/environmnets/environment.prod.ts** file, we see this:\n\n```typescript\nexport const environment = {\n  production: true,\n};\n```\n\nNow, let's say you wish to add API URLs for the development and production environment. Here's how the **/src/environments/environment.ts** file could look:\n\n```typescript\nexport const environment = {\n  production: false,\n  apiUrl: \"local api url\",\n};\n```\n\nAnd your **/src/environments/environments.prod.ts** file could look like this:\n\n```typescript\nexport const environment = {\n  production: true,\n  apiUrl: \"production api url\",\n};\n```\n\nNow, let's go and use these environment variables in our app component. Update the **app.component.ts** file with the following:\n\n```typescript\n// app.component.ts\nimport { Component, isDevMode } from \"@angular/core\";\nimport { environment } from \"../environments/environment\";\n\n@Component({\n  selector: \"app-root\",\n  template: \"<h1>{{title}}</h1>\",\n})\nexport class AppComponent {\n  title = \"angular-environment-variables\";\n  environment = \"\";\n  apiUrl = environment.apiUrl;\n\n  ngOnInit() {\n    if (isDevMode()) {\n      this.environment = \"Development\";\n    } else {\n      this.environment = \"Production\";\n    }\n  }\n}\n```\n\nThen, we can update the template:\n\n```html\n<h1>\n  This is Angular app running in {% raw %}{{environment}}{% endraw %}\n  Environment\n</h1>\n<h3>API URL: {% raw %}{{apiUrl}}{% endraw %}</h3>\n```\n\nAnd we should now see the **apiUrl** referring to the development environment on the page:\n\n![](/blog-images/3fd86c11ab5c50104e9886c3487f5b28.png)\n\nBut if you close the Angular development server and run it in production mode using this command:\n\n```bash\nng serve --configuration=production\n```\n\nthat should render the template for the production environment as shown below:\n\n![](/blog-images/7d5165a066fd2b04a67c3cd389720232.png)\n\nGreat!\n\nBut what if we had another environment called staging or QA, where we occasionally test our applications?\n\n## **Using Staging Environment Variables**\n\nWe can add as many custom environments in Angular as we want. All we need to do is define the relevant environment configuration in the **angular.json** file and then create that environment file in the **/src/environments** directory.\n\nLet's say we were to add a new staging environment.\n\nFirst, under the **build** configurations, we'll add the following **fileReplacements** array for our staging environment:\n\n```json\n{\n  \"staging\": {\n    \"fileReplacements\": [\n      {\n        \"replace\": \"src/environments/environment.ts\",\n        \"with\": \"src/environments/environment.stage.ts\"\n      }\n    ]\n  }\n}\n```\n\nThen, under the **serve** configurations, we'll add the **browserTarget** configuration for our staging environment:\n\n```json\n{\n  \"staging\": {\n    \"browserTarget\": \"angular-environment-variables:build:staging\"\n  }\n}\n```\n\nAlmost there!\n\nNow, we'll create a new file called **environment.stage.ts** inside the **/src/environments** directory with the following contents:\n\n```typescript\nexport const environment = {\n  production: false,\n  apiUrl: \"staging api url\",\n};\n```\n\nAwesome!\n\nThen, all we need to do is run the following command:\n\nng serve --configuration=staging\n\nAnd you should see your Angular app running in the newly defined staging environment. The **apiUrl** will resolve to the value you specified in the **environment.stage.ts** file:\n\n![angular environment](/blog-images/3db3b8412406cdd6d8b7a470aff3dccd.png)\n\n## **Security Considerations**\n\nThe concept of environments we've explored in Angular helps us dynamically inject some variables based on the run-time environment of our application.\n\nHowever, this is quite different from the environment variables you use in a backend server or a system. Typically, environment variables are defined in the system or on a server where they can't be accessed by anyone else.\n\nIn this case, our environment variables are exported for other components and files to work with. This exposes these environment variables to anyone who's using the frontend application.\n\nFor this reason, you shouldn't store any sensitive credentials in your Angular app's environment variables. For instance, if you have an API key that, if exposed, could cause an attacker to use APIs on your behalf, you shouldn't store them here.\n\n![](/blog-images/2e2aac8a10bd455c521dbd4923699549.png)\n\n## **Conclusion**\n\nAngular's default support for environments makes it really convenient for developers and testers to build and test the application in different environments.\n\nYou can pretty much create your own custom environment and use it any way you like, as we did here for the staging environment.\n\nFinally, remember to keep security considerations in mind when using environment variables in any frontend application. If you don't want to manage environments on your own, you can also use an automated environment management service from Release. Learn more about it [here](https://releasehub.com/ebook/the-complete-guide-to-automated-software-environments).\n\nWe've explored what environments are, why you need them, and how to correctly use environment variables in an Angular application. Hopefully, this has given you the starting point you need to dive deeper into environments and explore further use cases for environment variables in your applications.\n",
          "code": "var Component=(()=>{var h=Object.create;var t=Object.defineProperty;var p=Object.getOwnPropertyDescriptor;var m=Object.getOwnPropertyNames;var u=Object.getPrototypeOf,g=Object.prototype.hasOwnProperty;var v=(r,n)=>()=>(n||r((n={exports:{}}).exports,n),n.exports),f=(r,n)=>{for(var i in n)t(r,i,{get:n[i],enumerable:!0})},l=(r,n,i,a)=>{if(n&&typeof n==\"object\"||typeof n==\"function\")for(let o of m(n))!g.call(r,o)&&o!==i&&t(r,o,{get:()=>n[o],enumerable:!(a=p(n,o))||a.enumerable});return r};var y=(r,n,i)=>(i=r!=null?h(u(r)):{},l(n||!r||!r.__esModule?t(i,\"default\",{value:r,enumerable:!0}):i,r)),w=r=>l(t({},\"__esModule\",{value:!0}),r);var c=v((x,s)=>{s.exports=_jsx_runtime});var N={};f(N,{default:()=>k,frontmatter:()=>b});var e=y(c()),b={title:\"Using Environment Variables in Angular: A Guide\",excerpt:\"Learn how to effectively use environment variables in your Angular applications for better configuration management.\",date:\"2024-02-05\",ogImage:\"/blog-images/9de6930e0d988897966633594a3cfe34.png\",summary:\"What are environments? Why do you need them? And how do you correctly use environment variables in an Angular app?\",publishDate:\"Mon Jan 23 2023 14:06:16 GMT+0000 (Coordinated Universal Time)\",author:\"\",readingTime:5,categories:[\"platform-engineering\",\"product\"],mainImage:\"/blog-images/9de6930e0d988897966633594a3cfe34.png\",imageAlt:\"Using Environment Variables in Angular: A Guide\",showCTA:!0,ctaCopy:\"By automating environment setup like Angular, Release simplifies managing different environments for seamless deployment and testing.\",ctaLink:\"https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=using-environment-variables-in-angular-a-guide\",relatedPosts:[\"\"],tags:[\"platform-engineering\",\"product\"],ctaButton:\"Try Release for Free\"};function d(r){let n=Object.assign({p:\"p\",img:\"img\",h2:\"h2\",a:\"a\",span:\"span\",strong:\"strong\",pre:\"pre\",code:\"code\"},r.components);return(0,e.jsxs)(e.Fragment,{children:[(0,e.jsx)(n.p,{children:(0,e.jsx)(n.img,{src:\"/blog-images/c4635aef1048ce157456670de7326463.png\",alt:\"angular environment\"})}),`\n`,(0,e.jsx)(n.p,{children:\"Angular is one of the most popular and preferred frontend frameworks today, especially among large engineering teams. It's backed by a robust open-source community that makes it an ideal choice for building scalable web applications.\"}),`\n`,(0,e.jsx)(n.p,{children:\"However, being a frontend framework, your Angular application at some point needs to communicate with an external API or a third-party service application.\"}),`\n`,(0,e.jsx)(n.p,{children:\"For verifying authentic communication between your Angular application and the server, you'll need to use API keys and URLs that may change based on the environment of your Angular application.\"}),`\n`,(0,e.jsx)(n.p,{children:\"In this post, I'll walk you through how to use environment variables in an Angular application.\"}),`\n`,(0,e.jsx)(n.p,{children:(0,e.jsx)(n.img,{src:\"/blog-images/6e56bd2300ad21054eb696f30be767e8.png\",alt:\"\"})}),`\n`,(0,e.jsxs)(n.h2,{id:\"what-are-environments-in-angular\",children:[(0,e.jsx)(n.a,{className:\"anchor\",href:\"#what-are-environments-in-angular\",children:(0,e.jsx)(n.span,{className:\"icon icon-link\"})}),(0,e.jsx)(n.strong,{children:\"What are Environments in Angular?\"})]}),`\n`,(0,e.jsx)(n.p,{children:\"Before we get into what environment variables are, let's quickly understand what an environment means for Angular. Angular by default provides a way to detect and modify some files in your application based on where the application is running.\"}),`\n`,(0,e.jsx)(n.p,{children:\"Broadly speaking, Angular considers two environments for any application. One is the development environment, which means you're running Angular locally on your own system. This is when you're actually writing code for your Angular application.\"}),`\n`,(0,e.jsx)(n.p,{children:\"The other environment is production. This is where your Angular application is running for the end users on a domain or a server. This environment comes into the picture when you create a JavaScript bundle for your Angular application and deploy it to production.\"}),`\n`,(0,e.jsxs)(n.h2,{id:\"how-does-angular-know-which-environment\",children:[(0,e.jsx)(n.a,{className:\"anchor\",href:\"#how-does-angular-know-which-environment\",children:(0,e.jsx)(n.span,{className:\"icon icon-link\"})}),(0,e.jsx)(n.strong,{children:\"How Does Angular Know Which Environment?\"})]}),`\n`,(0,e.jsx)(n.p,{children:\"Now that we understand what environments are in Angular, let's see how Angular maintains and keeps track of them.\"}),`\n`,(0,e.jsx)(n.p,{children:\"Create a fresh Angular app by running the following:\"}),`\n`,(0,e.jsx)(n.p,{children:\"ng new angular-environment-variables\"}),`\n`,(0,e.jsx)(n.p,{children:\"To keep track of different environments, Angular maintains an environment directory in the src directory:\"}),`\n`,(0,e.jsx)(n.p,{children:(0,e.jsx)(n.img,{src:\"/blog-images/f825bb67f75affcad84aca257f1ff2a4.png\",alt:\"\"})}),`\n`,(0,e.jsxs)(n.p,{children:[\"Notice that Angular maintains two environment files called \",(0,e.jsx)(n.strong,{children:\"environment.ts\"}),\" and \",(0,e.jsx)(n.strong,{children:\"environment.prod.ts\"}),\".\"]}),`\n`,(0,e.jsxs)(n.p,{children:[\"Now, let's go to the \",(0,e.jsx)(n.strong,{children:\"angular.json\"}),\" file in the root directory. If you look closely at the \",(0,e.jsx)(n.strong,{children:\"configurations\"}),\" section, you'll notice it has some configurations declared for \",(0,e.jsx)(n.strong,{children:\"production\"}),\" and \",(0,e.jsx)(n.strong,{children:\"development\"}),\".\"]}),`\n`,(0,e.jsx)(n.p,{children:(0,e.jsx)(n.img,{src:\"/blog-images/b7ee3bff372e7733bef1883ec4284b1f.png\",alt:\"angular environment\"})}),`\n`,(0,e.jsxs)(n.p,{children:[\"If you take a look at the \",(0,e.jsx)(n.strong,{children:\"fileReplacements\"}),\" array in this file, you'll see it instructs Angular to replace the \",(0,e.jsx)(n.strong,{children:\"environment.ts\"}),\" file with the \",(0,e.jsx)(n.strong,{children:\"environment.prod.ts\"}),\" file for production. So, when you run the \",(0,e.jsx)(n.strong,{children:\"ng build\"}),\" command and deploy your Angular application in production, Angular automatically switches these two files so that the production bundle uses the production environment.\"]}),`\n`,(0,e.jsxs)(n.h2,{id:\"detecting-environment-in-angular\",children:[(0,e.jsx)(n.a,{className:\"anchor\",href:\"#detecting-environment-in-angular\",children:(0,e.jsx)(n.span,{className:\"icon icon-link\"})}),(0,e.jsx)(n.strong,{children:\"Detecting Environment in Angular\"})]}),`\n`,(0,e.jsx)(n.p,{children:\"Angular makes it really easy for you to detect the environment you're running it in. There are also other ways you can do this, including by checking the URL of your application's domain. But Angular provides an out-of-the-box solution for this.\"}),`\n`,(0,e.jsx)(n.p,{children:\"Let's explore this a bit.\"}),`\n`,(0,e.jsxs)(n.p,{children:[\"In the \",(0,e.jsx)(n.strong,{children:\"src/app/app.component.ts\"}),\" file, import the \",(0,e.jsx)(n.strong,{children:\"isDevMode\"}),\" from \",(0,e.jsx)(n.strong,{children:\"@angular/core\"}),\":\"]}),`\n`,(0,e.jsx)(n.pre,{children:(0,e.jsx)(n.code,{className:\"language-typescript\",children:`import { Component, isDevMode } from \"@angular/core\";\n`})}),`\n`,(0,e.jsxs)(n.p,{children:[\"Then, create a variable called \",(0,e.jsx)(n.strong,{children:\"environment\"}),\" inside the AppComponent:\"]}),`\n`,(0,e.jsx)(n.pre,{children:(0,e.jsx)(n.code,{className:\"language-typescript\",children:`@Component({\n  selector: \"app-root\",\n  template: \"<h1>{{title}}</h1>\",\n})\nexport class AppComponent {\n  title = \"angular-environment-variables\";\n  environment = \"\";\n}\n`})}),`\n`,(0,e.jsxs)(n.p,{children:[\"Now we can use the \",(0,e.jsx)(n.strong,{children:\"isDevMode\"}),\" to populate the \",(0,e.jsx)(n.strong,{children:\"environment\"}),\" variable above:\"]}),`\n`,(0,e.jsx)(n.pre,{children:(0,e.jsx)(n.code,{className:\"language-typescript\",children:`@Component({\n  selector: \"app-root\",\n  template: \"<h1>{{title}}</h1>\",\n})\nexport class AppComponent {\n  title = \"angular-environment-variables\";\n  environment = \"\";\n\n  ngOnInit() {\n    if (isDevMode()) {\n      this.environment = \"Development\";\n    } else {\n      this.environment = \"Production\";\n    }\n  }\n}\n`})}),`\n`,(0,e.jsxs)(n.p,{children:[\"Let's now render the \",(0,e.jsx)(n.strong,{children:\"environment\"}),\" variable in the component's HTML:\"]}),`\n`,(0,e.jsx)(n.pre,{children:(0,e.jsx)(n.code,{className:\"language-html\",children:`<h1>This is Angular app running in {{environment}} Environment</h1>\n`})}),`\n`,(0,e.jsx)(n.p,{children:\"If you visit your Angular development server, it should tell you that your Angular app is running in a development environment:\"}),`\n`,(0,e.jsx)(n.p,{children:(0,e.jsx)(n.img,{src:\"/blog-images/5edfe8892fb6dacc5f586f73cfba9ce5.png\",alt:\"\"})}),`\n`,(0,e.jsx)(n.p,{children:\"Great!\"}),`\n`,(0,e.jsx)(n.p,{children:\"Now let's look at how we can modify or update our environment files to use environment variables in our Angular application.\"}),`\n`,(0,e.jsxs)(n.h2,{id:\"using-environment-variables-in-angular\",children:[(0,e.jsx)(n.a,{className:\"anchor\",href:\"#using-environment-variables-in-angular\",children:(0,e.jsx)(n.span,{className:\"icon icon-link\"})}),(0,e.jsx)(n.strong,{children:\"Using Environment Variables in Angular\"})]}),`\n`,(0,e.jsx)(n.p,{children:\"Now that you understand how Angular maintains and keeps track of the environment, let's play around with the environment directory that we have in the root directory.\"}),`\n`,(0,e.jsxs)(n.p,{children:[\"First, let's look at the default contents of each. The \",(0,e.jsx)(n.strong,{children:\"/src/environments/environment.ts\"}),\" file contains the following code by default:\"]}),`\n`,(0,e.jsx)(n.pre,{children:(0,e.jsx)(n.code,{className:\"language-typescript\",children:`// This file can be replaced during build by using the \\`fileReplacements\\` array.\n// \\`ng build\\` replaces \\`environment.ts\\` with \\`environment.prod.ts\\`.\n// The list of file replacements can be found in \\`angular.json\\`.\nexport const environment = {\n  production: false,\n};\n\n/*\n * For easier debugging in development mode, you can import the following file\n * to ignore zone related error stack frames such as \\`zone.run\\`, \\`zoneDelegate.invokeTask\\`.\n *\n * This import should be commented out in production mode because it will have a negative impact\n * on performance if an error is thrown.\n */\n// import 'zone.js/plugins/zone-error';\n// Included with Angular CLI.\n`})}),`\n`,(0,e.jsxs)(n.p,{children:[\"Similarly, if we look at the \",(0,e.jsx)(n.strong,{children:\"/src/environmnets/environment.prod.ts\"}),\" file, we see this:\"]}),`\n`,(0,e.jsx)(n.pre,{children:(0,e.jsx)(n.code,{className:\"language-typescript\",children:`export const environment = {\n  production: true,\n};\n`})}),`\n`,(0,e.jsxs)(n.p,{children:[\"Now, let's say you wish to add API URLs for the development and production environment. Here's how the \",(0,e.jsx)(n.strong,{children:\"/src/environments/environment.ts\"}),\" file could look:\"]}),`\n`,(0,e.jsx)(n.pre,{children:(0,e.jsx)(n.code,{className:\"language-typescript\",children:`export const environment = {\n  production: false,\n  apiUrl: \"local api url\",\n};\n`})}),`\n`,(0,e.jsxs)(n.p,{children:[\"And your \",(0,e.jsx)(n.strong,{children:\"/src/environments/environments.prod.ts\"}),\" file could look like this:\"]}),`\n`,(0,e.jsx)(n.pre,{children:(0,e.jsx)(n.code,{className:\"language-typescript\",children:`export const environment = {\n  production: true,\n  apiUrl: \"production api url\",\n};\n`})}),`\n`,(0,e.jsxs)(n.p,{children:[\"Now, let's go and use these environment variables in our app component. Update the \",(0,e.jsx)(n.strong,{children:\"app.component.ts\"}),\" file with the following:\"]}),`\n`,(0,e.jsx)(n.pre,{children:(0,e.jsx)(n.code,{className:\"language-typescript\",children:`// app.component.ts\nimport { Component, isDevMode } from \"@angular/core\";\nimport { environment } from \"../environments/environment\";\n\n@Component({\n  selector: \"app-root\",\n  template: \"<h1>{{title}}</h1>\",\n})\nexport class AppComponent {\n  title = \"angular-environment-variables\";\n  environment = \"\";\n  apiUrl = environment.apiUrl;\n\n  ngOnInit() {\n    if (isDevMode()) {\n      this.environment = \"Development\";\n    } else {\n      this.environment = \"Production\";\n    }\n  }\n}\n`})}),`\n`,(0,e.jsx)(n.p,{children:\"Then, we can update the template:\"}),`\n`,(0,e.jsx)(n.pre,{children:(0,e.jsx)(n.code,{className:\"language-html\",children:`<h1>\n  This is Angular app running in {% raw %}{{environment}}{% endraw %}\n  Environment\n</h1>\n<h3>API URL: {% raw %}{{apiUrl}}{% endraw %}</h3>\n`})}),`\n`,(0,e.jsxs)(n.p,{children:[\"And we should now see the \",(0,e.jsx)(n.strong,{children:\"apiUrl\"}),\" referring to the development environment on the page:\"]}),`\n`,(0,e.jsx)(n.p,{children:(0,e.jsx)(n.img,{src:\"/blog-images/3fd86c11ab5c50104e9886c3487f5b28.png\",alt:\"\"})}),`\n`,(0,e.jsx)(n.p,{children:\"But if you close the Angular development server and run it in production mode using this command:\"}),`\n`,(0,e.jsx)(n.pre,{children:(0,e.jsx)(n.code,{className:\"language-bash\",children:`ng serve --configuration=production\n`})}),`\n`,(0,e.jsx)(n.p,{children:\"that should render the template for the production environment as shown below:\"}),`\n`,(0,e.jsx)(n.p,{children:(0,e.jsx)(n.img,{src:\"/blog-images/7d5165a066fd2b04a67c3cd389720232.png\",alt:\"\"})}),`\n`,(0,e.jsx)(n.p,{children:\"Great!\"}),`\n`,(0,e.jsx)(n.p,{children:\"But what if we had another environment called staging or QA, where we occasionally test our applications?\"}),`\n`,(0,e.jsxs)(n.h2,{id:\"using-staging-environment-variables\",children:[(0,e.jsx)(n.a,{className:\"anchor\",href:\"#using-staging-environment-variables\",children:(0,e.jsx)(n.span,{className:\"icon icon-link\"})}),(0,e.jsx)(n.strong,{children:\"Using Staging Environment Variables\"})]}),`\n`,(0,e.jsxs)(n.p,{children:[\"We can add as many custom environments in Angular as we want. All we need to do is define the relevant environment configuration in the \",(0,e.jsx)(n.strong,{children:\"angular.json\"}),\" file and then create that environment file in the \",(0,e.jsx)(n.strong,{children:\"/src/environments\"}),\" directory.\"]}),`\n`,(0,e.jsx)(n.p,{children:\"Let's say we were to add a new staging environment.\"}),`\n`,(0,e.jsxs)(n.p,{children:[\"First, under the \",(0,e.jsx)(n.strong,{children:\"build\"}),\" configurations, we'll add the following \",(0,e.jsx)(n.strong,{children:\"fileReplacements\"}),\" array for our staging environment:\"]}),`\n`,(0,e.jsx)(n.pre,{children:(0,e.jsx)(n.code,{className:\"language-json\",children:`{\n  \"staging\": {\n    \"fileReplacements\": [\n      {\n        \"replace\": \"src/environments/environment.ts\",\n        \"with\": \"src/environments/environment.stage.ts\"\n      }\n    ]\n  }\n}\n`})}),`\n`,(0,e.jsxs)(n.p,{children:[\"Then, under the \",(0,e.jsx)(n.strong,{children:\"serve\"}),\" configurations, we'll add the \",(0,e.jsx)(n.strong,{children:\"browserTarget\"}),\" configuration for our staging environment:\"]}),`\n`,(0,e.jsx)(n.pre,{children:(0,e.jsx)(n.code,{className:\"language-json\",children:`{\n  \"staging\": {\n    \"browserTarget\": \"angular-environment-variables:build:staging\"\n  }\n}\n`})}),`\n`,(0,e.jsx)(n.p,{children:\"Almost there!\"}),`\n`,(0,e.jsxs)(n.p,{children:[\"Now, we'll create a new file called \",(0,e.jsx)(n.strong,{children:\"environment.stage.ts\"}),\" inside the \",(0,e.jsx)(n.strong,{children:\"/src/environments\"}),\" directory with the following contents:\"]}),`\n`,(0,e.jsx)(n.pre,{children:(0,e.jsx)(n.code,{className:\"language-typescript\",children:`export const environment = {\n  production: false,\n  apiUrl: \"staging api url\",\n};\n`})}),`\n`,(0,e.jsx)(n.p,{children:\"Awesome!\"}),`\n`,(0,e.jsx)(n.p,{children:\"Then, all we need to do is run the following command:\"}),`\n`,(0,e.jsx)(n.p,{children:\"ng serve --configuration=staging\"}),`\n`,(0,e.jsxs)(n.p,{children:[\"And you should see your Angular app running in the newly defined staging environment. The \",(0,e.jsx)(n.strong,{children:\"apiUrl\"}),\" will resolve to the value you specified in the \",(0,e.jsx)(n.strong,{children:\"environment.stage.ts\"}),\" file:\"]}),`\n`,(0,e.jsx)(n.p,{children:(0,e.jsx)(n.img,{src:\"/blog-images/3db3b8412406cdd6d8b7a470aff3dccd.png\",alt:\"angular environment\"})}),`\n`,(0,e.jsxs)(n.h2,{id:\"security-considerations\",children:[(0,e.jsx)(n.a,{className:\"anchor\",href:\"#security-considerations\",children:(0,e.jsx)(n.span,{className:\"icon icon-link\"})}),(0,e.jsx)(n.strong,{children:\"Security Considerations\"})]}),`\n`,(0,e.jsx)(n.p,{children:\"The concept of environments we've explored in Angular helps us dynamically inject some variables based on the run-time environment of our application.\"}),`\n`,(0,e.jsx)(n.p,{children:\"However, this is quite different from the environment variables you use in a backend server or a system. Typically, environment variables are defined in the system or on a server where they can't be accessed by anyone else.\"}),`\n`,(0,e.jsx)(n.p,{children:\"In this case, our environment variables are exported for other components and files to work with. This exposes these environment variables to anyone who's using the frontend application.\"}),`\n`,(0,e.jsx)(n.p,{children:\"For this reason, you shouldn't store any sensitive credentials in your Angular app's environment variables. For instance, if you have an API key that, if exposed, could cause an attacker to use APIs on your behalf, you shouldn't store them here.\"}),`\n`,(0,e.jsx)(n.p,{children:(0,e.jsx)(n.img,{src:\"/blog-images/2e2aac8a10bd455c521dbd4923699549.png\",alt:\"\"})}),`\n`,(0,e.jsxs)(n.h2,{id:\"conclusion\",children:[(0,e.jsx)(n.a,{className:\"anchor\",href:\"#conclusion\",children:(0,e.jsx)(n.span,{className:\"icon icon-link\"})}),(0,e.jsx)(n.strong,{children:\"Conclusion\"})]}),`\n`,(0,e.jsx)(n.p,{children:\"Angular's default support for environments makes it really convenient for developers and testers to build and test the application in different environments.\"}),`\n`,(0,e.jsx)(n.p,{children:\"You can pretty much create your own custom environment and use it any way you like, as we did here for the staging environment.\"}),`\n`,(0,e.jsxs)(n.p,{children:[\"Finally, remember to keep security considerations in mind when using environment variables in any frontend application. If you don't want to manage environments on your own, you can also use an automated environment management service from Release. Learn more about it \",(0,e.jsx)(n.a,{href:\"https://releasehub.com/ebook/the-complete-guide-to-automated-software-environments\",children:\"here\"}),\".\"]}),`\n`,(0,e.jsx)(n.p,{children:\"We've explored what environments are, why you need them, and how to correctly use environment variables in an Angular application. Hopefully, this has given you the starting point you need to dive deeper into environments and explore further use cases for environment variables in your applications.\"})]})}function A(r={}){let{wrapper:n}=r.components||{};return n?(0,e.jsx)(n,Object.assign({},r,{children:(0,e.jsx)(d,r)})):d(r)}var k=A;return w(N);})();\n;return Component;"
        },
        "_id": "blog/posts/using-environment-variables-in-angular-a-guide.mdx",
        "_raw": {
          "sourceFilePath": "blog/posts/using-environment-variables-in-angular-a-guide.mdx",
          "sourceFileName": "using-environment-variables-in-angular-a-guide.mdx",
          "sourceFileDir": "blog/posts",
          "contentType": "mdx",
          "flattenedPath": "blog/posts/using-environment-variables-in-angular-a-guide"
        },
        "type": "BlogPost",
        "computedSlug": "using-environment-variables-in-angular-a-guide"
      },
      "documentHash": "1739393595030",
      "hasWarnings": false,
      "documentTypeName": "BlogPost"
    },
    "blog/posts/webhook-authentication-learnings.mdx": {
      "document": {
        "title": "Webhook Authentication Learnings for GitHub, GitLab, and Bitbucket",
        "summary": "I was recently tasked with implementing GitLab support for Release and to complete that task I needed to implement authe",
        "publishDate": "Wed Jun 02 2021 17:13:56 GMT+0000 (Coordinated Universal Time)",
        "author": "jeremy-kreutzbender",
        "readingTime": 9,
        "categories": [
          "platform-engineering",
          "product"
        ],
        "mainImage": "/blog-images/f640a5f16852b36e83669568508fa7c2.jpg",
        "imageAlt": "Rope passing on rock climbing pitons conveying the idea of Webhook Authentication",
        "showCTA": true,
        "ctaCopy": "Looking to streamline webhook authentication like in the blog post? Try Release for seamless environment management.",
        "ctaLink": "https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=webhook-authentication-learnings",
        "relatedPosts": [
          ""
        ],
        "ogImage": "/blog-images/f640a5f16852b36e83669568508fa7c2.jpg",
        "excerpt": "I was recently tasked with implementing GitLab support for Release and to complete that task I needed to implement authe",
        "tags": [
          "platform-engineering",
          "product"
        ],
        "ctaButton": "Try Release for Free",
        "body": {
          "raw": "\nI was recently tasked with implementing GitLab support for Release and to complete that task I needed to implement authentication for GitLab and a way to handle their webhooks. I completed the authentication first and left the webhook implementation for another pull request as I wanted to refactor the way we were handling all webhooks.\n\nAs I started the webhook work, the state of how Release handled webhooks was that GitHub was using the [github_webhook](https://github.com/ssaunier/github_webhook) gem and Bitbucket was using some custom built code that lived in a Controller Concern. With the need to add a third client I wanted to align everything into a few classes that allow us to easily onboard more providers if the need ever arose.\n\nAs I finished up the work I thought it would be useful to share some things I found about the differences between the three providers and share some of the code that I wrote in case anyone else is trying to do a similar implementation. First I'll talk about a few things I came across and later, in the Technical Details section, I'll go over a few classes we're using in our Ruby on Rails project for handling the webhooks.\n\nI enjoyed doing this work because it spanned a lot different aspects of our codebase, including designing a refactor that would allow for processing webhooks from three different sources, reading documentation and understanding three different APIs, and writing tests to ensure that as we move forward we shouldn't need to every worry about breaking our webhook processing.\n\n### Authentication Methods\n\nFurther down in the Technical Details section, I'll show some code for the **_Authenticator_** class so you'll see the implementation around this topic, but I wanted to touch on how each of the three providers handles authenticating the webhooks. We'll start with GitHub.\n\n- **GitHub** - With a GitHub App, when you create the App, you can supply a secret key which will be used as the basis for authenticating the webhooks for all repositories. Their approach is to combine the payload of the webhook and the secret to generate a hash. The generated hash will be passed as a header when the request is sent to you. The documentation gives an example in Ruby on how to generate your own version of the hash. Then, if the comparison of the two hashes matches, you know the authenticity of the request is valid. I would rate this as the most secure way of authenticating the webhooks because if the request were intercepted and decoded, the secret used to generate the hash is not present anywhere in the request. You can read GitHub's [Securing your webhooks documentation](https://docs.github.com/en/developers/webhooks-and-events/webhooks/securing-your-webhooks) for yourself if you want to learn more.\n- **GitLab** - GitLab follows a similar approach to GitHub, except that a different webhook object must be created on GitLab for each Repository (as opposed to the singular GitHub App). Each webhook installation can take in a secret as it is being created and that secret is sent with the webhook request in the headers. There is no generating of a hash like GitHub, the secret is simply added to the request. Due to the secret being sent in the header, we decided to generate a different secret for every Repository as we save the Repository in the database. This means that if a request were to be intercepted and decoded, at most a single Repository would be compromised. You can read Gitlab's [Webhook documentation](https://docs.gitlab.com/ee/user/project/integrations/webhooks.html) for yourself if you want to learn more.\n- **Bitbucket Cloud** - Bitbucket is set up in a similar fashion to GitLab where we have to create a webhook object for each Repository. However, unlike Gitlab, Bitbucket Cloud does not offer a way to add a secret. I came across a [JIRA Ticket](https://jira.atlassian.com/browse/BCLOUD-14683) that was created in 2017 outlining this omission of a way to secure the requests but it is currently still open. It is unfortunate that Bitbucket doesn't offer a way to authenticate the webhook requests as it would allow someone to potentially send requests with bad information. They do offer an alternative solution in their documentation about whitelisting specific IPs that the requests could come from, but my opinion is that they should implement adding the secret and at least follow in GitLab's approach to send the secret in the headers. You can read Bitbucket Cloud's [Manage webhooks documentation](https://support.atlassian.com/bitbucket-cloud/docs/manage-webhooks/) for yourself if you want to learn more.\n\n### The Action Is Separated From The Event\n\nOne aspect I really like with Github and GitLab is that they differentiate their webhook request through an event and an action. The event is sent as a header in the request and an example would be _pull_request_ (on Github) or _merge_request_ (on GitLab). There are many different things that can happen with a Pull Request though: it might be one of opened, closed, merged, reopened, and so on. Those different actions that could happen on the Pull Request are sent over in the payload as the key _action_ with the value of the aforementioned states. From a coding perspective this event and action pattern allowed me to create a method, say _process_pull_request_ and inside of that method, handle the many different actions that could occur in another method, say _pull_request_opened_. I found that designing the code this way allowed for a good abstraction and thorough unit testing of all the different action methods.\n\nThe outlier is Bitbucket which added the event and the action together in the header. For example, when a Pull Request is created, the header contains _pullrequest:created_, when closed _pullrequest:rejected_, and when merged _pullrequest:fulfilled_. When using Release for ephemeral environments, closing and merging a Pull Request are considered the same type of action: we will destroy that ephemeral environment. But since the header contains two different values, I had to implement two different methods: _process_pullrequest_rejected_ and _process_pullrequest_fulfilled_ which simply call another method. While it is a pretty minor inconvenience, I like the code pattern of the action and event separated compared to having them combined.\n\n### Technical Details\n\nFirst and foremost I want to acknowledge the great work on the [github_webhook](https://github.com/ssaunier/github_webhook) gem as I used a good amount of what they had done to create the foundation for the **_Authenticator_** and **_Processor_** classes. What follows is the Ruby code I wrote to manage the webhooks from the three providers that Release currently supports.\n\n### The Authenticator\n\nFirst up, we'll look at the **_Authenticator_** class. Its purpose is to authenticate the webhooks that we are receiving to ensure that they're valid. You'll see that there is an optional parameter for the Repository and it is optional because as I mentioned in the Authentication Methods above, for GitHub we have a single secret, while for GitLab and Bitbucket a secret is generated for each Repository.\n\nAside from the initialization method, the class has a single public method, _authenticate_request!_ which does as it is named. It will raise an error if the authenticity of the request cannot be validated otherwise the call will return. The _expected_signature_ method follows the different providers implementations with GitHub needing to create a hash to compare, GitLab needing only the secret, and Bitbucket currently using a random string due to not offering an authentication method.\n\n```ruby\n\nmodule Webhooks\n  class Authenticator\n    class SignatureError < StandardError; end\n\n    def initialize(request:, vcs_type:, repository: nil)\n      @request = request\n      @vcs_type = vcs_type\n      @repository = repository\n    end\n\n    def authenticate_request!\n      secret = client_secret(@vcs_type, @repository)\n      request_signature = signature_header(@vcs_type, @request)\n      expected_signature = expected_signature(@vcs_type, secret, @request)\n\n      unless ActiveSupport::SecurityUtils.secure_compare(request_signature, expected_signature)\n        raise SignatureError\n      end\n    end\n\n    private\n\n    def request_body(request)\n      @request_body ||= (\n        request.body.rewind\n        request.body.read\n      )\n    end\n  \n    def signature_header(vcs_type, request)\n      @signature_header ||= (\n        case vcs_type\n        when :github\n          @request.headers['X-Hub-Signature-256']\n        when :gitlab\n          @request.headers['X-Gitlab-Token']\n        when :bitbucket\n          'bitbucket_cloud'\n        end\n      )\n    end\n\n    def expected_signature(vcs_type, secret, request)\n      digest = OpenSSL::Digest.new('sha256')\n\n      case vcs_type\n      when :github\n        \"sha256=#{OpenSSL::HMAC.hexdigest(digest, secret, request_body(request))}\"\n      when :gitlab\n        secret\n      when :bitbucket\n        'bitbucket_cloud'\n      end\n    end\n  \n    def client_secret(vcs_type, repository)\n      case vcs_type\n      when :github\n        Clients::Github.webhook_secret\n      when :gitlab, :bitbucket\n        repository.webhook_secret\n      end\n    end\n    \n  end\nend\n\n```\n\n### The Processor\n\nIf the request is authenticated, then we need to process the payload that comes with the request and the **_Processor_** class does just that. It will look through the payload and try to find the associated Repository in our database, if that Repository cannot be found, then an error occurs. To determine what event occurred, we look through the different headers in the request and parse the value into Ruby method declaration form by replacing any non-word character with an underscore. Based on the provider who sent the request, a service object is initialized and then we attempt to call a \\_process\\__ method. Some webhooks we receive are for things Release doesn't deal with, for example GitHub's \\_issues_ webhooks, so we safely _try_ the method as there may not be an implemented \\_process\\_\\_ method.\n\n```ruby\n\nmodule Webhooks\n  class Processor\n    def initialize(request, vcs_type)\n      @request = request\n      @payload = json_body(request)\n      @vcs_type = vcs_type\n      @repository = repository_from_payload(@vcs_type, @payload)\n      \n      @webhook_service = webhook_service(@vcs_type, @payload, @repository)\n    end\n\n    def repository\n      @repository\n    end\n\n    def process_webhook\n      process_method = \"process_#{event_method(@vcs_type, @request)}\"\n      @webhook_service.try(process_method)\n    end\n\n    private\n\n    def json_body(request)\n      payload = request.body.read\n      ActiveSupport::HashWithIndifferentAccess.new(JSON.load(payload))\n    end\n\n    def repository_from_payload(vcs_type, payload)\n      provider_repository_id = provider_repository_id(vcs_type, payload)\n      if provider_repository_id\n        Repository.find_by!(type: \"Repositories::#{vcs_type.capitalize}\", provider_repository_id: provider_repository_id)\n      end\n    rescue ActiveRecord::RecordNotFound => error\n      # Re-Raise the error with info from the payload so we know what the repository is\n      repository_info = payload.dig('repository', 'full_name')\n      new_error = ActiveRecord::RecordNotFound.new(error.message + \"Repository Info: #{repository_info}\")\n      new_error.set_backtrace(error.backtrace)\n      raise new_error\n    end\n\n    def provider_repository_id(vcs_type, payload)\n      case vcs_type\n      when :github\n        payload.dig('repository', 'id')\n      when :gitlab\n        payload.dig('project', 'id')\n      when :bitbucket\n        payload.dig('repository', 'uuid')\n      else\n        nil\n      end\n    end\n\n    def event_method(vcs_type, request)\n      @event_method ||=\n        (\n          case vcs_type\n          when :github\n            request.headers['X-GitHub-Event']\n          when :gitlab\n            request.headers['X-Gitlab-Event']\n          when :bitbucket\n            request.headers['X-Event-Key']\n          else\n            nil\n          end\n        )&.downcase&.gsub(/\\W/, '_')&.to_sym\n    end\n\n    def webhook_service(vcs_type, payload, repository)\n      service_class = \"Webhooks::#{vcs_type.to_s.capitalize}\"\n      service_class.constantize.new(payload, repository)\n    end\n  end\nend\n\n```\n\n### GitHub Webhook Service\n\nThe last method in **_Processor_**, _webhook_service_ returns a service class that goes through our internal business logic of what we want to do with the webhook. I'm going to provide a small snippet of the GitHub service when we receive a Pull Request webhook. If you recall, I mentioned this method in the \"The Action Is Separated From The Event\" section and how I liked this pattern of structuring the code. If someone else were to look at this code, I would hope they would find it easy to understand that anything to do with GitHub Pull Request webhooks happens inside of the _process_pull_request_ method and the _case_ statement handles all the different actions that can take place.\n\n```ruby\n\nmodule Webhooks\n  class Github\n    def initialize(payload, repository)\n      @payload = payload\n      @action = @payload.dig('action')\n\n      @repository = repository\n    end\n\n    def process_pull_request\n      if @action.nil?\n        error_message = \"ERROR: Pull Request no action received, payload : #{@payload}, do nothing\"\n        Rails.logger.error(error_message)\n      else            \n        message = \"Pull Request with action : *#{@action}*. Received Repository : #{@repository.name}.\"\n        Rails.logger.info(message)\n  \n        case @action\n        when 'opened', 'reopened'\n          pull_request_opened\n        when 'closed'\n          pull_request_closed\n        when 'labeled'\n          pull_request_labeled\n        else\n          message = \"Pull Request with action : *#{@action}*. Nothing to do for now.\"\n          Rails.logger.info(message)\n        end\n      end\n    end\n  end\nend\n\n```\n\n### The Controller\n\nThe final piece to tie everything together is the controller. **_WebhooksController_** is the base class and each subclass implements only the _vcs_type_ method. Our previous approach had custom code for each of the _Webhooks::GithubController_ and _Webhooks::BitbucketController_. This meant that each required a ton of specific tests to ensure that we were processing all the different webhooks correctly. My refactored approach moved all that logic out of the controller and aimed for the smallest footprint possible to make testing as simple as possible.\n\nThere is only one route in the controller, which is a _POST_ to _create_. I decided that since the Repository may be optional in the **_Authenticator_** that I will store it in the **_Processor_** and pass it into the **_Authenticator_**. Otherwise you can see that the public methods for each of the classes are called. If an error is raised by either, due to an unauthenticated webhook or possibly a webhook for a Repository we don't have in our database, we'll capture the error and log as much information as possible so that we can look into what went wrong.\n\n```ruby\n\nclass Webhooks::GithubController < WebhooksController\n  vcs_type(:github)\nend\n\n```\n\n```ruby\n\nclass WebhooksController < ActionController::Base\n  skip_before_action :verify_authenticity_token\n\n  def self.vcs_type(vcs_name)\n    define_method :vcs_type do\n      vcs_name\n    end\n  end\n\n  rescue_from StandardError do |error|\n    payload = @webhook_service&.payload\n\n    backtrace_cleaner = ActiveSupport::BacktraceCleaner.new\n    cleaned_backtrace = backtrace_cleaner.clean(error.backtrace)\n\n    error_message = \"Error in #{self}! Message : #{error.message}\\nPayload : #{payload}\\nBacktrace : #{cleaned_backtrace.join(\"\\n\")}\"\n    Rails.logger.error(error_message)\n    head :bad_request\n  end\n\n  def create\n    processor = Webhooks::Processor.new(request, vcs_type)\n    repository = processor.repository\n    \n    authenticator = Webhooks::Authenticator.new(request: request, vcs_type: vcs_type, repository: repository)\n    authenticator.authenticate_request!\n\n    processor.process_webhook\n\n    head :ok\n  end\nend\n\n```\n\n### Conclusion\n\nThat's a wrap on my stint in refactoring our webhook code to work with GitHub, Bitbucket, and GitLab. If we ever have to add another provider I think it will be quite straightforward and I hope you enjoyed taking a peek inside some development work at Release. If you're interested in having an ephemeral environment created whenever we receive a Pull Request webhook from your Repository, head on over to the [homepage](https://release.com) and sign up!\n\nPhoto by [Brook Anderson](https://unsplash.com/@brookanderson?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) on [Unsplash](https://unsplash.com/?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)\n",
          "code": "var Component=(()=>{var l=Object.create;var a=Object.defineProperty;var u=Object.getOwnPropertyDescriptor;var p=Object.getOwnPropertyNames;var m=Object.getPrototypeOf,b=Object.prototype.hasOwnProperty;var w=(n,e)=>()=>(e||n((e={exports:{}}).exports,e),e.exports),f=(n,e)=>{for(var o in e)a(n,o,{get:e[o],enumerable:!0})},i=(n,e,o,s)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let r of p(e))!b.call(n,r)&&r!==o&&a(n,r,{get:()=>e[r],enumerable:!(s=u(e,r))||s.enumerable});return n};var y=(n,e,o)=>(o=n!=null?l(m(n)):{},i(e||!n||!n.__esModule?a(o,\"default\",{value:n,enumerable:!0}):o,n)),g=n=>i(a({},\"__esModule\",{value:!0}),n);var c=w((R,h)=>{h.exports=_jsx_runtime});var q={};f(q,{default:()=>v,frontmatter:()=>_});var t=y(c()),_={title:\"Webhook Authentication Learnings for GitHub, GitLab, and Bitbucket\",summary:\"I was recently tasked with implementing GitLab support for Release and to complete that task I needed to implement authe\",publishDate:\"Wed Jun 02 2021 17:13:56 GMT+0000 (Coordinated Universal Time)\",author:\"jeremy-kreutzbender\",readingTime:9,categories:[\"platform-engineering\",\"product\"],mainImage:\"/blog-images/f640a5f16852b36e83669568508fa7c2.jpg\",imageAlt:\"Rope passing on rock climbing pitons conveying the idea of Webhook Authentication\",showCTA:!0,ctaCopy:\"Looking to streamline webhook authentication like in the blog post? Try Release for seamless environment management.\",ctaLink:\"https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=webhook-authentication-learnings\",relatedPosts:[\"\"],ogImage:\"/blog-images/f640a5f16852b36e83669568508fa7c2.jpg\",excerpt:\"I was recently tasked with implementing GitLab support for Release and to complete that task I needed to implement authe\",tags:[\"platform-engineering\",\"product\"],ctaButton:\"Try Release for Free\"};function d(n){let e=Object.assign({p:\"p\",a:\"a\",h3:\"h3\",span:\"span\",strong:\"strong\",em:\"em\",ul:\"ul\",li:\"li\",pre:\"pre\",code:\"code\"},n.components);return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(e.p,{children:\"I was recently tasked with implementing GitLab support for Release and to complete that task I needed to implement authentication for GitLab and a way to handle their webhooks. I completed the authentication first and left the webhook implementation for another pull request as I wanted to refactor the way we were handling all webhooks.\"}),`\n`,(0,t.jsxs)(e.p,{children:[\"As I started the webhook work, the state of how Release handled webhooks was that GitHub was using the \",(0,t.jsx)(e.a,{href:\"https://github.com/ssaunier/github_webhook\",children:\"github_webhook\"}),\" gem and Bitbucket was using some custom built code that lived in a Controller Concern. With the need to add a third client I wanted to align everything into a few classes that allow us to easily onboard more providers if the need ever arose.\"]}),`\n`,(0,t.jsx)(e.p,{children:\"As I finished up the work I thought it would be useful to share some things I found about the differences between the three providers and share some of the code that I wrote in case anyone else is trying to do a similar implementation. First I'll talk about a few things I came across and later, in the Technical Details section, I'll go over a few classes we're using in our Ruby on Rails project for handling the webhooks.\"}),`\n`,(0,t.jsx)(e.p,{children:\"I enjoyed doing this work because it spanned a lot different aspects of our codebase, including designing a refactor that would allow for processing webhooks from three different sources, reading documentation and understanding three different APIs, and writing tests to ensure that as we move forward we shouldn't need to every worry about breaking our webhook processing.\"}),`\n`,(0,t.jsxs)(e.h3,{id:\"authentication-methods\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#authentication-methods\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Authentication Methods\"]}),`\n`,(0,t.jsxs)(e.p,{children:[\"Further down in the Technical Details section, I'll show some code for the \",(0,t.jsx)(e.strong,{children:(0,t.jsx)(e.em,{children:\"Authenticator\"})}),\" class so you'll see the implementation around this topic, but I wanted to touch on how each of the three providers handles authenticating the webhooks. We'll start with GitHub.\"]}),`\n`,(0,t.jsxs)(e.ul,{children:[`\n`,(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:\"GitHub\"}),\" - With a GitHub App, when you create the App, you can supply a secret key which will be used as the basis for authenticating the webhooks for all repositories. Their approach is to combine the payload of the webhook and the secret to generate a hash. The generated hash will be passed as a header when the request is sent to you. The documentation gives an example in Ruby on how to generate your own version of the hash. Then, if the comparison of the two hashes matches, you know the authenticity of the request is valid. I would rate this as the most secure way of authenticating the webhooks because if the request were intercepted and decoded, the secret used to generate the hash is not present anywhere in the request. You can read GitHub's \",(0,t.jsx)(e.a,{href:\"https://docs.github.com/en/developers/webhooks-and-events/webhooks/securing-your-webhooks\",children:\"Securing your webhooks documentation\"}),\" for yourself if you want to learn more.\"]}),`\n`,(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:\"GitLab\"}),\" - GitLab follows a similar approach to GitHub, except that a different webhook object must be created on GitLab for each Repository (as opposed to the singular GitHub App). Each webhook installation can take in a secret as it is being created and that secret is sent with the webhook request in the headers. There is no generating of a hash like GitHub, the secret is simply added to the request. Due to the secret being sent in the header, we decided to generate a different secret for every Repository as we save the Repository in the database. This means that if a request were to be intercepted and decoded, at most a single Repository would be compromised. You can read Gitlab's \",(0,t.jsx)(e.a,{href:\"https://docs.gitlab.com/ee/user/project/integrations/webhooks.html\",children:\"Webhook documentation\"}),\" for yourself if you want to learn more.\"]}),`\n`,(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:\"Bitbucket Cloud\"}),\" - Bitbucket is set up in a similar fashion to GitLab where we have to create a webhook object for each Repository. However, unlike Gitlab, Bitbucket Cloud does not offer a way to add a secret. I came across a \",(0,t.jsx)(e.a,{href:\"https://jira.atlassian.com/browse/BCLOUD-14683\",children:\"JIRA Ticket\"}),\" that was created in 2017 outlining this omission of a way to secure the requests but it is currently still open. It is unfortunate that Bitbucket doesn't offer a way to authenticate the webhook requests as it would allow someone to potentially send requests with bad information. They do offer an alternative solution in their documentation about whitelisting specific IPs that the requests could come from, but my opinion is that they should implement adding the secret and at least follow in GitLab's approach to send the secret in the headers. You can read Bitbucket Cloud's \",(0,t.jsx)(e.a,{href:\"https://support.atlassian.com/bitbucket-cloud/docs/manage-webhooks/\",children:\"Manage webhooks documentation\"}),\" for yourself if you want to learn more.\"]}),`\n`]}),`\n`,(0,t.jsxs)(e.h3,{id:\"the-action-is-separated-from-the-event\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#the-action-is-separated-from-the-event\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"The Action Is Separated From The Event\"]}),`\n`,(0,t.jsxs)(e.p,{children:[\"One aspect I really like with Github and GitLab is that they differentiate their webhook request through an event and an action. The event is sent as a header in the request and an example would be \",(0,t.jsx)(e.em,{children:\"pull_request\"}),\" (on Github) or \",(0,t.jsx)(e.em,{children:\"merge_request\"}),\" (on GitLab). There are many different things that can happen with a Pull Request though: it might be one of opened, closed, merged, reopened, and so on. Those different actions that could happen on the Pull Request are sent over in the payload as the key \",(0,t.jsx)(e.em,{children:\"action\"}),\" with the value of the aforementioned states. From a coding perspective this event and action pattern allowed me to create a method, say \",(0,t.jsx)(e.em,{children:\"process_pull_request\"}),\" and inside of that method, handle the many different actions that could occur in another method, say \",(0,t.jsx)(e.em,{children:\"pull_request_opened\"}),\". I found that designing the code this way allowed for a good abstraction and thorough unit testing of all the different action methods.\"]}),`\n`,(0,t.jsxs)(e.p,{children:[\"The outlier is Bitbucket which added the event and the action together in the header. For example, when a Pull Request is created, the header contains \",(0,t.jsx)(e.em,{children:\"pullrequest:created\"}),\", when closed \",(0,t.jsx)(e.em,{children:\"pullrequest:rejected\"}),\", and when merged \",(0,t.jsx)(e.em,{children:\"pullrequest:fulfilled\"}),\". When using Release for ephemeral environments, closing and merging a Pull Request are considered the same type of action: we will destroy that ephemeral environment. But since the header contains two different values, I had to implement two different methods: \",(0,t.jsx)(e.em,{children:\"process_pullrequest_rejected\"}),\" and \",(0,t.jsx)(e.em,{children:\"process_pullrequest_fulfilled\"}),\" which simply call another method. While it is a pretty minor inconvenience, I like the code pattern of the action and event separated compared to having them combined.\"]}),`\n`,(0,t.jsxs)(e.h3,{id:\"technical-details\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#technical-details\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Technical Details\"]}),`\n`,(0,t.jsxs)(e.p,{children:[\"First and foremost I want to acknowledge the great work on the \",(0,t.jsx)(e.a,{href:\"https://github.com/ssaunier/github_webhook\",children:\"github_webhook\"}),\" gem as I used a good amount of what they had done to create the foundation for the \",(0,t.jsx)(e.strong,{children:(0,t.jsx)(e.em,{children:\"Authenticator\"})}),\" and \",(0,t.jsx)(e.strong,{children:(0,t.jsx)(e.em,{children:\"Processor\"})}),\" classes. What follows is the Ruby code I wrote to manage the webhooks from the three providers that Release currently supports.\"]}),`\n`,(0,t.jsxs)(e.h3,{id:\"the-authenticator\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#the-authenticator\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"The Authenticator\"]}),`\n`,(0,t.jsxs)(e.p,{children:[\"First up, we'll look at the \",(0,t.jsx)(e.strong,{children:(0,t.jsx)(e.em,{children:\"Authenticator\"})}),\" class. Its purpose is to authenticate the webhooks that we are receiving to ensure that they're valid. You'll see that there is an optional parameter for the Repository and it is optional because as I mentioned in the Authentication Methods above, for GitHub we have a single secret, while for GitLab and Bitbucket a secret is generated for each Repository.\"]}),`\n`,(0,t.jsxs)(e.p,{children:[\"Aside from the initialization method, the class has a single public method, \",(0,t.jsx)(e.em,{children:\"authenticate_request!\"}),\" which does as it is named. It will raise an error if the authenticity of the request cannot be validated otherwise the call will return. The \",(0,t.jsx)(e.em,{children:\"expected_signature\"}),\" method follows the different providers implementations with GitHub needing to create a hash to compare, GitLab needing only the secret, and Bitbucket currently using a random string due to not offering an authentication method.\"]}),`\n`,(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:\"language-ruby\",children:`\nmodule Webhooks\n \\xA0class Authenticator\n \\xA0 \\xA0class SignatureError < StandardError; end\n\n \\xA0 \\xA0def initialize(request:, vcs_type:, repository: nil)\n \\xA0 \\xA0 \\xA0@request = request\n \\xA0 \\xA0 \\xA0@vcs_type = vcs_type\n \\xA0 \\xA0 \\xA0@repository = repository\n \\xA0 \\xA0end\n\n \\xA0 \\xA0def authenticate_request!\n \\xA0 \\xA0 \\xA0secret = client_secret(@vcs_type, @repository)\n \\xA0 \\xA0 \\xA0request_signature = signature_header(@vcs_type, @request)\n \\xA0 \\xA0 \\xA0expected_signature = expected_signature(@vcs_type, secret, @request)\n\n \\xA0 \\xA0 \\xA0unless ActiveSupport::SecurityUtils.secure_compare(request_signature, expected_signature)\n \\xA0 \\xA0 \\xA0 \\xA0raise SignatureError\n \\xA0 \\xA0 \\xA0end\n \\xA0 \\xA0end\n\n \\xA0 \\xA0private\n\n \\xA0 \\xA0def request_body(request)\n \\xA0 \\xA0 \\xA0@request_body ||= (\n \\xA0 \\xA0 \\xA0 \\xA0request.body.rewind\n \\xA0 \\xA0 \\xA0 \\xA0request.body.read\n \\xA0 \\xA0 \\xA0)\n \\xA0 \\xA0end\n \\xA0\n \\xA0 \\xA0def signature_header(vcs_type, request)\n \\xA0 \\xA0 \\xA0@signature_header ||= (\n \\xA0 \\xA0 \\xA0 \\xA0case vcs_type\n \\xA0 \\xA0 \\xA0 \\xA0when :github\n \\xA0 \\xA0 \\xA0 \\xA0 \\xA0@request.headers['X-Hub-Signature-256']\n \\xA0 \\xA0 \\xA0 \\xA0when :gitlab\n \\xA0 \\xA0 \\xA0 \\xA0 \\xA0@request.headers['X-Gitlab-Token']\n \\xA0 \\xA0 \\xA0 \\xA0when :bitbucket\n \\xA0 \\xA0 \\xA0 \\xA0 \\xA0'bitbucket_cloud'\n \\xA0 \\xA0 \\xA0 \\xA0end\n \\xA0 \\xA0 \\xA0)\n \\xA0 \\xA0end\n\n \\xA0 \\xA0def expected_signature(vcs_type, secret, request)\n \\xA0 \\xA0 \\xA0digest = OpenSSL::Digest.new('sha256')\n\n \\xA0 \\xA0 \\xA0case vcs_type\n \\xA0 \\xA0 \\xA0when :github\n \\xA0 \\xA0 \\xA0 \\xA0\"sha256=#{OpenSSL::HMAC.hexdigest(digest, secret, request_body(request))}\"\n \\xA0 \\xA0 \\xA0when :gitlab\n \\xA0 \\xA0 \\xA0 \\xA0secret\n \\xA0 \\xA0 \\xA0when :bitbucket\n \\xA0 \\xA0 \\xA0 \\xA0'bitbucket_cloud'\n \\xA0 \\xA0 \\xA0end\n \\xA0 \\xA0end\n \\xA0\n \\xA0 \\xA0def client_secret(vcs_type, repository)\n \\xA0 \\xA0 \\xA0case vcs_type\n \\xA0 \\xA0 \\xA0when :github\n \\xA0 \\xA0 \\xA0 \\xA0Clients::Github.webhook_secret\n \\xA0 \\xA0 \\xA0when :gitlab, :bitbucket\n \\xA0 \\xA0 \\xA0 \\xA0repository.webhook_secret\n \\xA0 \\xA0 \\xA0end\n \\xA0 \\xA0end\n \\xA0 \\xA0\n \\xA0end\nend\n\n`})}),`\n`,(0,t.jsxs)(e.h3,{id:\"the-processor\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#the-processor\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"The Processor\"]}),`\n`,(0,t.jsxs)(e.p,{children:[\"If the request is authenticated, then we need to process the payload that comes with the request and the \",(0,t.jsx)(e.strong,{children:(0,t.jsx)(e.em,{children:\"Processor\"})}),\" class does just that. It will look through the payload and try to find the associated Repository in our database, if that Repository cannot be found, then an error occurs. To determine what event occurred, we look through the different headers in the request and parse the value into Ruby method declaration form by replacing any non-word character with an underscore. Based on the provider who sent the request, a service object is initialized and then we attempt to call a _process__ method. Some webhooks we receive are for things Release doesn't deal with, for example GitHub's _issues_ webhooks, so we safely \",(0,t.jsx)(e.em,{children:\"try\"}),\" the method as there may not be an implemented _process__ method.\"]}),`\n`,(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:\"language-ruby\",children:`\nmodule Webhooks\n \\xA0class Processor\n \\xA0 \\xA0def initialize(request, vcs_type)\n \\xA0 \\xA0 \\xA0@request = request\n \\xA0 \\xA0 \\xA0@payload = json_body(request)\n \\xA0 \\xA0 \\xA0@vcs_type = vcs_type\n \\xA0 \\xA0 \\xA0@repository = repository_from_payload(@vcs_type, @payload)\n \\xA0 \\xA0 \\xA0\n \\xA0 \\xA0 \\xA0@webhook_service = webhook_service(@vcs_type, @payload, @repository)\n \\xA0 \\xA0end\n\n \\xA0 \\xA0def repository\n \\xA0 \\xA0 \\xA0@repository\n \\xA0 \\xA0end\n\n \\xA0 \\xA0def process_webhook\n \\xA0 \\xA0 \\xA0process_method = \"process_#{event_method(@vcs_type, @request)}\"\n \\xA0 \\xA0 \\xA0@webhook_service.try(process_method)\n \\xA0 \\xA0end\n\n \\xA0 \\xA0private\n\n \\xA0 \\xA0def json_body(request)\n \\xA0 \\xA0 \\xA0payload = request.body.read\n \\xA0 \\xA0 \\xA0ActiveSupport::HashWithIndifferentAccess.new(JSON.load(payload))\n \\xA0 \\xA0end\n\n \\xA0 \\xA0def repository_from_payload(vcs_type, payload)\n \\xA0 \\xA0 \\xA0provider_repository_id = provider_repository_id(vcs_type, payload)\n \\xA0 \\xA0 \\xA0if provider_repository_id\n \\xA0 \\xA0 \\xA0 \\xA0Repository.find_by!(type: \"Repositories::#{vcs_type.capitalize}\", provider_repository_id: provider_repository_id)\n \\xA0 \\xA0 \\xA0end\n \\xA0 \\xA0rescue ActiveRecord::RecordNotFound => error\n \\xA0 \\xA0 \\xA0# Re-Raise the error with info from the payload so we know what the repository is\n \\xA0 \\xA0 \\xA0repository_info = payload.dig('repository', 'full_name')\n \\xA0 \\xA0 \\xA0new_error = ActiveRecord::RecordNotFound.new(error.message + \"Repository Info: #{repository_info}\")\n \\xA0 \\xA0 \\xA0new_error.set_backtrace(error.backtrace)\n \\xA0 \\xA0 \\xA0raise new_error\n \\xA0 \\xA0end\n\n \\xA0 \\xA0def provider_repository_id(vcs_type, payload)\n \\xA0 \\xA0 \\xA0case vcs_type\n \\xA0 \\xA0 \\xA0when :github\n \\xA0 \\xA0 \\xA0 \\xA0payload.dig('repository', 'id')\n \\xA0 \\xA0 \\xA0when :gitlab\n \\xA0 \\xA0 \\xA0 \\xA0payload.dig('project', 'id')\n \\xA0 \\xA0 \\xA0when :bitbucket\n \\xA0 \\xA0 \\xA0 \\xA0payload.dig('repository', 'uuid')\n \\xA0 \\xA0 \\xA0else\n \\xA0 \\xA0 \\xA0 \\xA0nil\n \\xA0 \\xA0 \\xA0end\n \\xA0 \\xA0end\n\n \\xA0 \\xA0def event_method(vcs_type, request)\n \\xA0 \\xA0 \\xA0@event_method ||=\n \\xA0 \\xA0 \\xA0 \\xA0(\n \\xA0 \\xA0 \\xA0 \\xA0 \\xA0case vcs_type\n \\xA0 \\xA0 \\xA0 \\xA0 \\xA0when :github\n \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0request.headers['X-GitHub-Event']\n \\xA0 \\xA0 \\xA0 \\xA0 \\xA0when :gitlab\n \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0request.headers['X-Gitlab-Event']\n \\xA0 \\xA0 \\xA0 \\xA0 \\xA0when :bitbucket\n \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0request.headers['X-Event-Key']\n \\xA0 \\xA0 \\xA0 \\xA0 \\xA0else\n \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0nil\n \\xA0 \\xA0 \\xA0 \\xA0 \\xA0end\n \\xA0 \\xA0 \\xA0 \\xA0)&.downcase&.gsub(/\\\\W/, '_')&.to_sym\n \\xA0 \\xA0end\n\n \\xA0 \\xA0def webhook_service(vcs_type, payload, repository)\n \\xA0 \\xA0 \\xA0service_class = \"Webhooks::#{vcs_type.to_s.capitalize}\"\n \\xA0 \\xA0 \\xA0service_class.constantize.new(payload, repository)\n \\xA0 \\xA0end\n \\xA0end\nend\n\n`})}),`\n`,(0,t.jsxs)(e.h3,{id:\"github-webhook-service\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#github-webhook-service\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"GitHub Webhook Service\"]}),`\n`,(0,t.jsxs)(e.p,{children:[\"The last method in \",(0,t.jsx)(e.strong,{children:(0,t.jsx)(e.em,{children:\"Processor\"})}),\", \",(0,t.jsx)(e.em,{children:\"webhook_service\"}),` returns a service class that goes through our internal business logic of what we want to do with the webhook. I'm going to provide a small snippet of the GitHub service when we receive a Pull Request webhook. If you recall, I mentioned this method in the \"The Action Is Separated From The Event\" section and how I liked this pattern of structuring the code. If someone else were to look at this code, I would hope they would find it easy to understand that anything to do with GitHub Pull Request webhooks happens inside of the `,(0,t.jsx)(e.em,{children:\"process_pull_request\"}),\" method and the \",(0,t.jsx)(e.em,{children:\"case\"}),\" statement handles all the different actions that can take place.\"]}),`\n`,(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:\"language-ruby\",children:`\nmodule Webhooks\n \\xA0class Github\n \\xA0 \\xA0def initialize(payload, repository)\n \\xA0 \\xA0 \\xA0@payload = payload\n \\xA0 \\xA0 \\xA0@action = @payload.dig('action')\n\n \\xA0 \\xA0 \\xA0@repository = repository\n \\xA0 \\xA0end\n\n \\xA0 \\xA0def process_pull_request\n \\xA0 \\xA0 \\xA0if @action.nil?\n \\xA0 \\xA0 \\xA0 \\xA0error_message = \"ERROR: Pull Request no action received, payload : #{@payload}, do nothing\"\n \\xA0 \\xA0 \\xA0 \\xA0Rails.logger.error(error_message)\n \\xA0 \\xA0 \\xA0else \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0\n \\xA0 \\xA0 \\xA0 \\xA0message = \"Pull Request with action : *#{@action}*. Received Repository : #{@repository.name}.\"\n \\xA0 \\xA0 \\xA0 \\xA0Rails.logger.info(message)\n \\xA0\n \\xA0 \\xA0 \\xA0 \\xA0case @action\n \\xA0 \\xA0 \\xA0 \\xA0when 'opened', 'reopened'\n \\xA0 \\xA0 \\xA0 \\xA0 \\xA0pull_request_opened\n \\xA0 \\xA0 \\xA0 \\xA0when 'closed'\n \\xA0 \\xA0 \\xA0 \\xA0 \\xA0pull_request_closed\n \\xA0 \\xA0 \\xA0 \\xA0when 'labeled'\n \\xA0 \\xA0 \\xA0 \\xA0 \\xA0pull_request_labeled\n \\xA0 \\xA0 \\xA0 \\xA0else\n \\xA0 \\xA0 \\xA0 \\xA0 \\xA0message = \"Pull Request with action : *#{@action}*. Nothing to do for now.\"\n \\xA0 \\xA0 \\xA0 \\xA0 \\xA0Rails.logger.info(message)\n \\xA0 \\xA0 \\xA0 \\xA0end\n \\xA0 \\xA0 \\xA0end\n \\xA0 \\xA0end\n \\xA0end\nend\n\n`})}),`\n`,(0,t.jsxs)(e.h3,{id:\"the-controller\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#the-controller\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"The Controller\"]}),`\n`,(0,t.jsxs)(e.p,{children:[\"The final piece to tie everything together is the controller. \",(0,t.jsx)(e.strong,{children:(0,t.jsx)(e.em,{children:\"WebhooksController\"})}),\" is the base class and each subclass implements only the \",(0,t.jsx)(e.em,{children:\"vcs_type\"}),\" method. Our previous approach had custom code for each of the \",(0,t.jsx)(e.em,{children:\"Webhooks::GithubController\"}),\" and \",(0,t.jsx)(e.em,{children:\"Webhooks::BitbucketController\"}),\". This meant that each required a ton of specific tests to ensure that we were processing all the different webhooks correctly. My refactored approach moved all that logic out of the controller and aimed for the smallest footprint possible to make testing as simple as possible.\"]}),`\n`,(0,t.jsxs)(e.p,{children:[\"There is only one route in the controller, which is a \",(0,t.jsx)(e.em,{children:\"POST\"}),\" to \",(0,t.jsx)(e.em,{children:\"create\"}),\". I decided that since the Repository may be optional in the \",(0,t.jsx)(e.strong,{children:(0,t.jsx)(e.em,{children:\"Authenticator\"})}),\" that I will store it in the \",(0,t.jsx)(e.strong,{children:(0,t.jsx)(e.em,{children:\"Processor\"})}),\" and pass it into the \",(0,t.jsx)(e.strong,{children:(0,t.jsx)(e.em,{children:\"Authenticator\"})}),\". Otherwise you can see that the public methods for each of the classes are called. If an error is raised by either, due to an unauthenticated webhook or possibly a webhook for a Repository we don't have in our database, we'll capture the error and log as much information as possible so that we can look into what went wrong.\"]}),`\n`,(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:\"language-ruby\",children:`\nclass Webhooks::GithubController < WebhooksController\n \\xA0vcs_type(:github)\nend\n\n`})}),`\n`,(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:\"language-ruby\",children:`\nclass WebhooksController < ActionController::Base\n \\xA0skip_before_action :verify_authenticity_token\n\n \\xA0def self.vcs_type(vcs_name)\n \\xA0 \\xA0define_method :vcs_type do\n \\xA0 \\xA0 \\xA0vcs_name\n \\xA0 \\xA0end\n \\xA0end\n\n \\xA0rescue_from StandardError do |error|\n \\xA0 \\xA0payload = @webhook_service&.payload\n\n \\xA0 \\xA0backtrace_cleaner = ActiveSupport::BacktraceCleaner.new\n \\xA0 \\xA0cleaned_backtrace = backtrace_cleaner.clean(error.backtrace)\n\n \\xA0 \\xA0error_message = \"Error in #{self}! Message : #{error.message}\\\\nPayload : #{payload}\\\\nBacktrace : #{cleaned_backtrace.join(\"\\\\n\")}\"\n \\xA0 \\xA0Rails.logger.error(error_message)\n \\xA0 \\xA0head :bad_request\n \\xA0end\n\n \\xA0def create\n \\xA0 \\xA0processor = Webhooks::Processor.new(request, vcs_type)\n \\xA0 \\xA0repository = processor.repository\n \\xA0 \\xA0\n \\xA0 \\xA0authenticator = Webhooks::Authenticator.new(request: request, vcs_type: vcs_type, repository: repository)\n \\xA0 \\xA0authenticator.authenticate_request!\n\n \\xA0 \\xA0processor.process_webhook\n\n \\xA0 \\xA0head :ok\n \\xA0end\nend\n\n`})}),`\n`,(0,t.jsxs)(e.h3,{id:\"conclusion\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#conclusion\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Conclusion\"]}),`\n`,(0,t.jsxs)(e.p,{children:[\"That's a wrap on my stint in refactoring our webhook code to work with GitHub, Bitbucket, and GitLab. If we ever have to add another provider I think it will be quite straightforward and I hope you enjoyed taking a peek inside some development work at Release. If you're interested in having an ephemeral environment created whenever we receive a Pull Request webhook from your Repository, head on over to the \",(0,t.jsx)(e.a,{href:\"https://release.com\",children:\"homepage\"}),\" and sign up!\"]}),`\n`,(0,t.jsxs)(e.p,{children:[\"Photo by \",(0,t.jsx)(e.a,{href:\"https://unsplash.com/@brookanderson?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText\",children:\"Brook Anderson\"}),\" on \",(0,t.jsx)(e.a,{href:\"https://unsplash.com/?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText\",children:\"Unsplash\"})]})]})}function k(n={}){let{wrapper:e}=n.components||{};return e?(0,t.jsx)(e,Object.assign({},n,{children:(0,t.jsx)(d,n)})):d(n)}var v=k;return g(q);})();\n;return Component;"
        },
        "_id": "blog/posts/webhook-authentication-learnings.mdx",
        "_raw": {
          "sourceFilePath": "blog/posts/webhook-authentication-learnings.mdx",
          "sourceFileName": "webhook-authentication-learnings.mdx",
          "sourceFileDir": "blog/posts",
          "contentType": "mdx",
          "flattenedPath": "blog/posts/webhook-authentication-learnings"
        },
        "type": "BlogPost",
        "computedSlug": "webhook-authentication-learnings"
      },
      "documentHash": "1739393595030",
      "hasWarnings": false,
      "documentTypeName": "BlogPost"
    },
    "blog/posts/what-are-the-challenges-faced-during-uat-testing.mdx": {
      "document": {
        "title": "What are the Challenges Faced During UAT Testing?",
        "summary": "In this blog post you'll learn the challenges in UAT testing and examples of user acceptance testing environment.",
        "publishDate": "Wed Jan 05 2022 02:31:00 GMT+0000 (Coordinated Universal Time)",
        "author": "tommy-mcclung",
        "readingTime": 5,
        "categories": [
          "platform-engineering",
          "product"
        ],
        "mainImage": "/blog-images/21f37447ab5490033e2c49ad4a80b4e1.jpg",
        "imageAlt": "a woman jumping in the air with a beautiful view as background",
        "showCTA": true,
        "ctaCopy": "Improve UAT by replicating production environments with Release's ephemeral environments for accurate testing.",
        "ctaLink": "https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=what-are-the-challenges-faced-during-uat-testing",
        "relatedPosts": [
          ""
        ],
        "ogImage": "/blog-images/21f37447ab5490033e2c49ad4a80b4e1.jpg",
        "excerpt": "In this blog post you'll learn the challenges in UAT testing and examples of user acceptance testing environment.",
        "tags": [
          "platform-engineering",
          "product"
        ],
        "ctaButton": "Try Release for Free",
        "body": {
          "raw": "\nThis is a 4-part series on User Acceptance Testing (UAT)\n\n- Part 1: [What is User Acceptance Testing and its Best Practices](https://release.com/blog/user-acceptance-testing-best-practices)\n- Part 2: [How to Prepare for User Acceptance Testing?](https://release.com/blog/how-do-you-prepare-for-user-acceptance-testing)\n- **Part 3: User Acceptance Testing Challenges & UAT Environment Examples**\n- Part 4: [UAT Checklist](https://release.com/blog/user-acceptance-testing-checklist)\n\n### User Acceptance Testing Challenges\n\nUser acceptance testing often uncovers various challenges and clarifies requirements for the software. In some cases, the users might also find issues specific to the aspects of their platform that were not tested in other environments.\n\nHere are some of these challenges that you might face during UAT:\n\n#### Mimicking the right testing environment\n\nUAT is often conducted in the same environment used by the design team, potentially bypassing most of the real-world issues that will likely arise for the end-user. We highly recommend [replicating your production environment](https://releasehub.com/user-acceptance-testing-with-ephemeral-environments) and making sure you perform UAT on a replica that is as close to production as possible including the right data, services, infrastructure to test both the functionality, user experience and performance. Given the complexity of today’s software architecture, too often organizations settle for an environment that is limited thus many issues slip to production.\n\nMany times UAT requires testing different types of users and different states hence in some cases multiple environments might be required to properly run UAT. \n\n#### Determining time frames\n\nThis is another challenge that you might face when you're defining the UAT project plan at the beginning of a project. It is recommended to always include criteria for the standard time frame that the organization expects. While most places accept two weeks, it should be ideally defined for each UAT project.\n\n#### Reviewing your test plans\n\nUAT test plans can have errors similar to any other type of software project documentation. To navigate this effectively, UAT plans can be reviewed by the UAT team, QA team, project manager, facilitator, or anyone else with knowledge of testing and the project.\n\n#### Ambiguous requirements\n\nAmbiguous requirements will typically bubble up during UAT as the tester needs to decide whether a certain experience meets the requirements or not. If requirements are not well defined, it would be up to the tester’s own judgement to check the box or not on certain requirements. \n\nIf the requirements are not well defined log it as a defect. The end-user/customer then expects these errors to be fixed in the current release without considering the time for any change requests and impact on the release plan. \n\n#### Asking functional test team to perform user acceptance testing\n\nAsking the functional test team to perform UAT just to offload the responsibility to the test team for reasons such as lack of resources can. The purpose of UAT testing gets compromised in such cases, and you also run the risk of the end-users quickly spotting the issues that are not considered real-world scenarios by the functional testers.\n\n### User Acceptance Testing Environment Examples\n\nDepending on what exactly you are evaluating, there can be various user acceptance test scripts that may need a variety of UAT templates or UAT environments. UAT environment template is primarily a data and information collection tool that helps testers gather feedback so they can improve their end product.\n\nBelow are some of the UAT environment examples:\n\n#### Single-purpose UAT environment\n\nWhen a developer wishes to test a particular aspect of their product or software, the best option is a single-purpose UAT environment as it clearly outlines the test and its description along with different parameters.\n\n#### Priority-based UAT environment\n\nWhen there is a range of aspects to be evaluated and assessed, it is best to use an environment that offers the option to prioritize different testing criteria. This allows developers to address various critical issues, followed by focusing on small bugs and fixes.\n\n#### Multi-purpose UAT environment\n\nIn case a tester or developer is looking to evaluate a range of different applications, it is always best to rely on a flexible user acceptance testing environment that also enables them to accumulate data regarding their product.\n\n#### Customer-focused UAT scenario\n\nTesters and developers use this UAT environment when they wish to involve customers in the testing process as it enables them to engage the target audience and also collect relevant data when it comes to addressing various customer-oriented issues.\n\n### Disadvantages of Acceptance Testing\n\nWhile there are several benefits of UAT, there are some disadvantages too. For instance, as per the testing plan, the customer has to write their requirements in their own words and by themselves. However, there are two main problems here-\n\n- Customers are not willing to do this, and it defeats the entire purpose of acceptance testing.\n- In case the test cases are written by someone else, the customer does not understand them. The tester then has to perform the inspections by themselves only.\n\nIf the process of UAT is done in this manner, it completely defeats the very purpose of the acceptance testing.\n",
          "code": "var Component=(()=>{var m=Object.create;var a=Object.defineProperty;var d=Object.getOwnPropertyDescriptor;var p=Object.getOwnPropertyNames;var u=Object.getPrototypeOf,g=Object.prototype.hasOwnProperty;var f=(n,e)=>()=>(e||n((e={exports:{}}).exports,e),e.exports),v=(n,e)=>{for(var s in e)a(n,s,{get:e[s],enumerable:!0})},o=(n,e,s,r)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let i of p(e))!g.call(n,i)&&i!==s&&a(n,i,{get:()=>e[i],enumerable:!(r=d(e,i))||r.enumerable});return n};var y=(n,e,s)=>(s=n!=null?m(u(n)):{},o(e||!n||!n.__esModule?a(s,\"default\",{value:n,enumerable:!0}):s,n)),b=n=>o(a({},\"__esModule\",{value:!0}),n);var l=f((x,c)=>{c.exports=_jsx_runtime});var U={};v(U,{default:()=>A,frontmatter:()=>T});var t=y(l()),T={title:\"What are the Challenges Faced During UAT Testing?\",summary:\"In this blog post you'll learn the challenges in UAT testing and examples of user acceptance testing environment.\",publishDate:\"Wed Jan 05 2022 02:31:00 GMT+0000 (Coordinated Universal Time)\",author:\"tommy-mcclung\",readingTime:5,categories:[\"platform-engineering\",\"product\"],mainImage:\"/blog-images/21f37447ab5490033e2c49ad4a80b4e1.jpg\",imageAlt:\"a woman jumping in the air with a beautiful view as background\",showCTA:!0,ctaCopy:\"Improve UAT by replicating production environments with Release's ephemeral environments for accurate testing.\",ctaLink:\"https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=what-are-the-challenges-faced-during-uat-testing\",relatedPosts:[\"\"],ogImage:\"/blog-images/21f37447ab5490033e2c49ad4a80b4e1.jpg\",excerpt:\"In this blog post you'll learn the challenges in UAT testing and examples of user acceptance testing environment.\",tags:[\"platform-engineering\",\"product\"],ctaButton:\"Try Release for Free\"};function h(n){let e=Object.assign({p:\"p\",ul:\"ul\",li:\"li\",a:\"a\",strong:\"strong\",h3:\"h3\",span:\"span\",h4:\"h4\"},n.components);return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(e.p,{children:\"This is a 4-part series on User Acceptance Testing (UAT)\"}),`\n`,(0,t.jsxs)(e.ul,{children:[`\n`,(0,t.jsxs)(e.li,{children:[\"Part 1: \",(0,t.jsx)(e.a,{href:\"https://release.com/blog/user-acceptance-testing-best-practices\",children:\"What is User Acceptance Testing and its Best Practices\"})]}),`\n`,(0,t.jsxs)(e.li,{children:[\"Part 2: \",(0,t.jsx)(e.a,{href:\"https://release.com/blog/how-do-you-prepare-for-user-acceptance-testing\",children:\"How to Prepare for User Acceptance Testing?\"})]}),`\n`,(0,t.jsx)(e.li,{children:(0,t.jsx)(e.strong,{children:\"Part 3: User Acceptance Testing Challenges & UAT Environment Examples\"})}),`\n`,(0,t.jsxs)(e.li,{children:[\"Part 4: \",(0,t.jsx)(e.a,{href:\"https://release.com/blog/user-acceptance-testing-checklist\",children:\"UAT Checklist\"})]}),`\n`]}),`\n`,(0,t.jsxs)(e.h3,{id:\"user-acceptance-testing-challenges\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#user-acceptance-testing-challenges\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"User Acceptance Testing Challenges\"]}),`\n`,(0,t.jsx)(e.p,{children:\"User acceptance testing often uncovers various challenges and clarifies requirements for the software. In some cases, the users might also find issues specific to the aspects of their platform that were not tested in other environments.\"}),`\n`,(0,t.jsx)(e.p,{children:\"Here are some of these challenges that you might face during UAT:\"}),`\n`,(0,t.jsxs)(e.h4,{id:\"mimicking-the-right-testing-environment\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#mimicking-the-right-testing-environment\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Mimicking the right testing environment\"]}),`\n`,(0,t.jsxs)(e.p,{children:[\"UAT is often conducted in the same environment used by the design team, potentially bypassing most of the real-world issues that will likely arise for the end-user. We highly recommend \",(0,t.jsx)(e.a,{href:\"https://releasehub.com/user-acceptance-testing-with-ephemeral-environments\",children:\"replicating your production environment\"}),\" and making sure you perform UAT on a replica that is as close to production as possible including the right data, services, infrastructure to test both the functionality, user experience and performance. Given the complexity of today\\u2019s software architecture, too often organizations settle for an environment that is limited thus many issues slip to production.\"]}),`\n`,(0,t.jsx)(e.p,{children:\"Many times UAT requires testing different types of users and different states hence in some cases multiple environments might be required to properly run UAT.\\xA0\"}),`\n`,(0,t.jsxs)(e.h4,{id:\"determining-time-frames\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#determining-time-frames\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Determining time frames\"]}),`\n`,(0,t.jsx)(e.p,{children:\"This is another challenge that you might face when you're defining the UAT project plan at the beginning of a project. It is recommended to always include criteria for the standard time frame that the organization expects. While most places accept two weeks, it should be ideally defined for each UAT project.\"}),`\n`,(0,t.jsxs)(e.h4,{id:\"reviewing-your-test-plans\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#reviewing-your-test-plans\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Reviewing your test plans\"]}),`\n`,(0,t.jsx)(e.p,{children:\"UAT test plans can have errors similar to any other type of software project documentation. To navigate this effectively, UAT plans can be reviewed by the UAT team, QA team, project manager, facilitator, or anyone else with knowledge of testing and the project.\"}),`\n`,(0,t.jsxs)(e.h4,{id:\"ambiguous-requirements\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#ambiguous-requirements\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Ambiguous requirements\"]}),`\n`,(0,t.jsx)(e.p,{children:\"Ambiguous requirements will typically bubble up during UAT as the tester needs to decide whether a certain experience meets the requirements or not. If requirements are not well defined, it would be up to the tester\\u2019s own judgement to check the box or not on certain requirements.\\xA0\"}),`\n`,(0,t.jsx)(e.p,{children:\"If the requirements are not well defined log it as a defect. The end-user/customer then expects these errors to be fixed in the current release without considering the time for any change requests and impact on the release plan.\\xA0\"}),`\n`,(0,t.jsxs)(e.h4,{id:\"asking-functional-test-team-to-perform-user-acceptance-testing\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#asking-functional-test-team-to-perform-user-acceptance-testing\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Asking functional test team to perform user acceptance testing\"]}),`\n`,(0,t.jsx)(e.p,{children:\"Asking the functional test team to perform UAT just to offload the responsibility to the test team for reasons such as lack of resources can. The purpose of UAT testing gets compromised in such cases, and you also run the risk of the end-users quickly spotting the issues that are not considered real-world scenarios by the functional testers.\"}),`\n`,(0,t.jsxs)(e.h3,{id:\"user-acceptance-testing-environment-examples\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#user-acceptance-testing-environment-examples\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"User Acceptance Testing Environment Examples\"]}),`\n`,(0,t.jsx)(e.p,{children:\"Depending on what exactly you are evaluating, there can be various user acceptance test scripts that may need a variety of UAT templates or UAT environments. UAT environment template is primarily a data and information collection tool that helps testers gather feedback so they can improve their end product.\"}),`\n`,(0,t.jsx)(e.p,{children:\"Below are some of the UAT environment examples:\"}),`\n`,(0,t.jsxs)(e.h4,{id:\"single-purpose-uat-environment\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#single-purpose-uat-environment\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Single-purpose UAT environment\"]}),`\n`,(0,t.jsx)(e.p,{children:\"When a developer wishes to test a particular aspect of their product or software, the best option is a single-purpose UAT environment as it clearly outlines the test and its description along with different parameters.\"}),`\n`,(0,t.jsxs)(e.h4,{id:\"priority-based-uat-environment\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#priority-based-uat-environment\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Priority-based UAT environment\"]}),`\n`,(0,t.jsx)(e.p,{children:\"When there is a range of aspects to be evaluated and assessed, it is best to use an environment that offers the option to prioritize different testing criteria. This allows developers to address various critical issues, followed by focusing on small bugs and fixes.\"}),`\n`,(0,t.jsxs)(e.h4,{id:\"multi-purpose-uat-environment\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#multi-purpose-uat-environment\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Multi-purpose UAT environment\"]}),`\n`,(0,t.jsx)(e.p,{children:\"In case a tester or developer is looking to evaluate a range of different applications, it is always best to rely on a flexible user acceptance testing environment that also enables them to accumulate data regarding their product.\"}),`\n`,(0,t.jsxs)(e.h4,{id:\"customer-focused-uat-scenario\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#customer-focused-uat-scenario\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Customer-focused UAT scenario\"]}),`\n`,(0,t.jsx)(e.p,{children:\"Testers and developers use this UAT environment when they wish to involve customers in the testing process as it enables them to engage the target audience and also collect relevant data when it comes to addressing various customer-oriented issues.\"}),`\n`,(0,t.jsxs)(e.h3,{id:\"disadvantages-of-acceptance-testing\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#disadvantages-of-acceptance-testing\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Disadvantages of Acceptance Testing\"]}),`\n`,(0,t.jsx)(e.p,{children:\"While there are several benefits of UAT, there are some disadvantages too. For instance, as per the testing plan, the customer has to write their requirements in their own words and by themselves. However, there are two main problems here-\"}),`\n`,(0,t.jsxs)(e.ul,{children:[`\n`,(0,t.jsx)(e.li,{children:\"Customers are not willing to do this, and it defeats the entire purpose of acceptance testing.\"}),`\n`,(0,t.jsx)(e.li,{children:\"In case the test cases are written by someone else, the customer does not understand them. The tester then has to perform the inspections by themselves only.\"}),`\n`]}),`\n`,(0,t.jsx)(e.p,{children:\"If the process of UAT is done in this manner, it completely defeats the very purpose of the acceptance testing.\"})]})}function w(n={}){let{wrapper:e}=n.components||{};return e?(0,t.jsx)(e,Object.assign({},n,{children:(0,t.jsx)(h,n)})):h(n)}var A=w;return b(U);})();\n;return Component;"
        },
        "_id": "blog/posts/what-are-the-challenges-faced-during-uat-testing.mdx",
        "_raw": {
          "sourceFilePath": "blog/posts/what-are-the-challenges-faced-during-uat-testing.mdx",
          "sourceFileName": "what-are-the-challenges-faced-during-uat-testing.mdx",
          "sourceFileDir": "blog/posts",
          "contentType": "mdx",
          "flattenedPath": "blog/posts/what-are-the-challenges-faced-during-uat-testing"
        },
        "type": "BlogPost",
        "computedSlug": "what-are-the-challenges-faced-during-uat-testing"
      },
      "documentHash": "1739393595030",
      "hasWarnings": false,
      "documentTypeName": "BlogPost"
    },
    "blog/posts/what-is-a-production-environment-and-why-is-it-unique.mdx": {
      "document": {
        "title": "What Is a Production Environment and Why Is It Unique?",
        "summary": "What is a production environment? What makes it special? Learn to make your production environment better!",
        "publishDate": "Tue Mar 14 2023 18:15:32 GMT+0000 (Coordinated Universal Time)",
        "author": "eric-goebelbecker",
        "readingTime": 7,
        "categories": [
          "platform-engineering",
          "product"
        ],
        "mainImage": "/blog-images/4ec490497814724ca55f37f02e79e548.jpg",
        "imageAlt": "What Is a Production Environment and Why Is It Unique?",
        "showCTA": true,
        "ctaCopy": "Enhance production environment efficiency with Release's on-demand environments. Simplify testing, streamline workflows, and ensure consistent deployments.",
        "ctaLink": "https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=what-is-a-production-environment-and-why-is-it-unique",
        "relatedPosts": [
          ""
        ],
        "ogImage": "/blog-images/4ec490497814724ca55f37f02e79e548.jpg",
        "excerpt": "What is a production environment? What makes it special? Learn to make your production environment better!",
        "tags": [
          "platform-engineering",
          "product"
        ],
        "ctaButton": "Try Release for Free",
        "body": {
          "raw": "\nIs the code ready for production? Can we test that in production? How will this configuration change affect production? These questions, and ones like them, are issues that developers and engineers hear and ask nearly every day. But what is a production environment? What makes it different from development, user-acceptance testing, or [staging](https://release.com/staging-environments)? What does the name \"production\" imply, and what do you need to keep in mind when using it? \n\nLet's take a close look at production environments. We'll talk about what an environment is, what makes an environment \"production,\" what can go wrong in a production environment, and how to improve it. \n\n![](/blog-images/1e7476e4e0b3e9a0a643189ed9059958.jpeg)\n\n### Environments\n\nBefore we delve into what makes a production environment \"production,\" let's discuss what environments are and how to think about them. \n\n### What Is an Environment?\n\nAn environment, or more formally a [deployment environment](https://en.wikipedia.org/wiki/Deployment_environment), is a set of computer systems and services that host and support a software application. The size and scope of applications vary, and environments have to scale to support them. So, it’s useful to define the term application before talking about environments. \n\nWhen we're talking about an application deployed to an environment, we're usually talking about an application composed of several, if not dozens of programs that work together to serve a business need. For example, the application might be an online store, a trading platform, or software as a service (SaaS) that provides capabilities to client businesses. \n\nThe requirements for these applications directly affect the size and scope of the environments that support them. A development environment could be a single system running in a few containers. A production environment might span 100s of virtual machines. Two different environments can support the same application in two very different contexts. A developer can run a containerized application on their laptop for testing purposes, while the production environment requires ten or more virtual machines. \n\n![](/blog-images/14b5f95f951d008e2d0eb92e4ccccad5.png)\n\nYou build environments to satisfy the [release management cycle](https://en.wikipedia.org/wiki/Release_management). You do your coding, troubleshooting, and initial testing in a development environment. Then you conduct testing in User Acceptance Testing (UAT) environments. You might test your final configuration and perform more intense testing in a staging environment. Finally, you serve your customers from the production environment. \n\n### What Are Environments Composed of?\n\nWhen we talk about environments, especially production, we tend to focus on the systems that run the core software stack. A typical scenario might include a database server (or cluster), [microservices](https://docs.release.com/guides-and-examples/advanced-guides/microservices-architecture), customer-facing web server, and network gear. These may be managed resources, cloud native services, or ephemeral systems in a cloud environment, but they tend to be \"what we talk about when we talk about environments.\"\n\nBut environments are more than the systems and services they run on. An application has code, but it has data in the form of configurations, user information, and other resources, too. In most cases, that data differs between environments. That data is part of the environment. When the developer runs the environment on their laptop, they create a development environment. Their system configuration is very different from what's required to support the production environment and they are usually not using live user information. \n\nIn today’s complex software architecture environments also include cloud native services, and security policies governing access requirements to various systems and services.\n\n![](/blog-images/0558487bbdd0a363a0007e56f18e4d1a.jpg)\n\n### What Is a Production Environment?\n\nSimply put, your production environment is where your application meets your primary users. This applies to external (public, customers consuming your services) or internal (enterprise, users within the organizations for whom the application is built). Let's look at what that means and how it affects how you treat this environment. \n\n### What Makes Production Unique?\n\n#### Revenue and Customer Satisfaction\n\nFor almost all businesses, the fact that production supports their users means that it represents their primary source of revenue. In many cases, clients are paying for access to the application, (or internal users are using it to do business directly). Other clients may pay for \"offline\" services, but production is how they interface with the company. In all cases, if the production environment isn't functioning correctly, revenue is in danger. \n\nIf clients can't access your application, they can't give (or make) you money. Every minute of downtime means lost revenue. When a major internet service provider reports downtime, the headlines usually contain \"lost\" revenue numbers. \n\nBut a production outage has a long-term effect, too. While your application is down, there's an excellent chance your customers will take their business to someone else. If they have a good experience with your competitor, they may not come back. \n\n#### Security\n\nIf users use your production information, it's available to the public, which means it's in constant danger from attack. Production environments have extraordinary security requirements. \n\nAttackers can cause problems in many ways. One is extortion via ransomware. If they can compromise even one system, they can install code that spreads throughout your production environment and forces you to either pay them and hope they follow through or start from scratch and build a new environment. \n\nAnother attacker may break in to steal user information. This attack exposes you to legal and financial liability. It can have a fatal impact on your reputation, too. \n\n### What Are the Risks to Production?\n\nWe've talked about what can go wrong if your production environment is broken or compromised. What are the common causes of these issues? \n\n### Insufficient or Incorrect Testing\n\nIn many organizations, testing is like the weather; everyone talks about testing, but no one does anything about it. A test plan missed an issue because it didn't exist, or it wasn't executed, or the test plan failed to account for the issue. \n\nThe first two scenarios are probably more common than many of us would like to admit. \n\nThe third scenario points back to the definition of an environment. User acceptance testing doesn't work if your [environment doesn't look like production.](https://release.com/blog/what-are-the-challenges-faced-during-uat-testing) If, for example, your production environment has more than 30 virtual machines, but your development environment only has four, testing in development doesn't cover all of your cases. It's too expensive to reproduce your production environment for every developer or development team. But you still need to find a cost-effective way to [build a staging environment](https://release.com/usecase/on-demand-staging-environments) that looks like production. \n\n### Security and Access Control\n\nSecurity is an ongoing concern for everyone. We already covered the risks that a breach represents above. Proper access control goes a long way toward preventing these issues. Access control can also act as a check against incorrect configuration changes and premature releases.\n\n![](/blog-images/ac685788143302558873d9f8bf99cd27.png)\n\nAnother way to tighten security is via automation. Automation is an effective tool for enforcing the \"[principle of least privilege](https://release.com/usecase/on-demand-staging-environments)\" because you can delegate deployment and configuration tasks to entities with limited privileges. \n\nTo protect yourself from data loss, recognize that any system storing user information is part of the production environment. If you don't want to extend production into that system, don't let the data go there. If the system is storing data that you can't afford to lose, confine it to the strictest level of access control possible. \n\n### How Can Release Help?\n\nDeveloping and testing for today’s architectures can get complex. Things work well in development and staging, then they break down in pre-production and you have to rollback. Sound familiar?\n\nCreating an up-to-date production replica will significantly reduce your rollback rate. When you test with the right dataset, security policies, and services you remove the differences between your environments, and testing is more effective. \n\nBetter user acceptance testing makes your production environment better, and the [best way to accomplish that](https://release.com/blog/how-do-you-prepare-for-user-acceptance-testing) is with an environment that looks like production. What if you could create a replica of your production environment, perform your testing, and then [tear it down until you need it next time](https://release.com/ephemeral-environments)? What if your user acceptance testing could help you find shortcomings in your production environment and not just in your code? \n\nDo you need a large dataset to test or code against? Don't risk your production data; use an [instant dataset](https://docs.release.com/reference-documentation/instant-datasets-aws) and throw it away when you're finished. \n\n### Multi-Cloud Deployments of Private Applications\n\nSoftware companies who, for security reasons, deploy their application on customers' private clouds often find it challenging to ensure that application works on every cloud vendor.\n\nRelease makes it easy to deploy your private application in AWS or GCP without refactoring. We create an abstraction of the cloud native services, making it easier to support and maintain your application without limiting yourself to one cloud vendor or another.\n\n### Build a Better Production Environment\n\nThis post talked about the essential parts of an environment and how you need to examine those parts when comparing one domain to another. Then we discussed production and what makes it unique when compared to other environments. Finally, we talked about how you take positive steps to improve the reliability of your production environment. \n\nRelease has the tools you need to build better environments. Discover [ways](https://release.com/use-cases) to improve production development speed and flexibility, and equip your testing and engineering teams with better tools today, [start for free](https://release.com/get-started)!\n",
          "code": "var Component=(()=>{var d=Object.create;var a=Object.defineProperty;var u=Object.getOwnPropertyDescriptor;var m=Object.getOwnPropertyNames;var p=Object.getPrototypeOf,f=Object.prototype.hasOwnProperty;var v=(t,e)=>()=>(e||t((e={exports:{}}).exports,e),e.exports),y=(t,e)=>{for(var o in e)a(t,o,{get:e[o],enumerable:!0})},s=(t,e,o,r)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let i of m(e))!f.call(t,i)&&i!==o&&a(t,i,{get:()=>e[i],enumerable:!(r=u(e,i))||r.enumerable});return t};var g=(t,e,o)=>(o=t!=null?d(p(t)):{},s(e||!t||!t.__esModule?a(o,\"default\",{value:t,enumerable:!0}):o,t)),w=t=>s(a({},\"__esModule\",{value:!0}),t);var l=v((N,c)=>{c.exports=_jsx_runtime});var W={};y(W,{default:()=>x,frontmatter:()=>b});var n=g(l()),b={title:\"What Is a Production Environment and Why Is It Unique?\",summary:\"What is a production environment? What makes it special? Learn to make your production environment better!\",publishDate:\"Tue Mar 14 2023 18:15:32 GMT+0000 (Coordinated Universal Time)\",author:\"eric-goebelbecker\",readingTime:7,categories:[\"platform-engineering\",\"product\"],mainImage:\"/blog-images/4ec490497814724ca55f37f02e79e548.jpg\",imageAlt:\"What Is a Production Environment and Why Is It Unique?\",showCTA:!0,ctaCopy:\"Enhance production environment efficiency with Release's on-demand environments. Simplify testing, streamline workflows, and ensure consistent deployments.\",ctaLink:\"https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=what-is-a-production-environment-and-why-is-it-unique\",relatedPosts:[\"\"],ogImage:\"/blog-images/4ec490497814724ca55f37f02e79e548.jpg\",excerpt:\"What is a production environment? What makes it special? Learn to make your production environment better!\",tags:[\"platform-engineering\",\"product\"],ctaButton:\"Try Release for Free\"};function h(t){let e=Object.assign({p:\"p\",a:\"a\",img:\"img\",h3:\"h3\",span:\"span\",h4:\"h4\"},t.components);return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsxs)(e.p,{children:[\"Is the code ready for production? Can we test that in production? How will this configuration change affect production? These questions, and ones like them, are issues that developers and engineers hear and ask nearly every day. But what is a production environment? What makes it different from development, user-acceptance testing, or \",(0,n.jsx)(e.a,{href:\"https://release.com/staging-environments\",children:\"staging\"}),'? What does the name \"production\" imply, and what do you need to keep in mind when using it?\\xA0']}),`\n`,(0,n.jsx)(e.p,{children:`Let's take a close look at production environments. We'll talk about what an environment is, what makes an environment \"production,\" what can go wrong in a production environment, and how to improve it.\\xA0`}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/1e7476e4e0b3e9a0a643189ed9059958.jpeg\",alt:\"\"})}),`\n`,(0,n.jsxs)(e.h3,{id:\"environments\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#environments\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Environments\"]}),`\n`,(0,n.jsx)(e.p,{children:`Before we delve into what makes a production environment \"production,\" let's discuss what environments are and how to think about them.\\xA0`}),`\n`,(0,n.jsxs)(e.h3,{id:\"what-is-an-environment\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#what-is-an-environment\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"What Is an Environment?\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"An environment, or more formally a \",(0,n.jsx)(e.a,{href:\"https://en.wikipedia.org/wiki/Deployment_environment\",children:\"deployment environment\"}),\", is a set of computer systems and services that host and support a software application. The size and scope of applications vary, and environments have to scale to support them. So, it\\u2019s useful to define the term application before talking about environments.\\xA0\"]}),`\n`,(0,n.jsx)(e.p,{children:\"When we're talking about an application deployed to an environment, we're usually talking about an application composed of several, if not dozens of programs that work together to serve a business need. For example, the application might be an online store, a trading platform, or software as a service (SaaS) that provides capabilities to client businesses.\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:\"The requirements for these applications directly affect the size and scope of the environments that support them. A development environment could be a single system running in a few containers. A production environment might span 100s of virtual machines. Two different environments can support the same application in two very different contexts. A developer can run a containerized application on their laptop for testing purposes, while the production environment requires ten or more virtual machines.\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/14b5f95f951d008e2d0eb92e4ccccad5.png\",alt:\"\"})}),`\n`,(0,n.jsxs)(e.p,{children:[\"You build environments to satisfy the \",(0,n.jsx)(e.a,{href:\"https://en.wikipedia.org/wiki/Release_management\",children:\"release management cycle\"}),\". You do your coding, troubleshooting, and initial testing in a development environment. Then you conduct testing in User Acceptance Testing (UAT) environments. You might test your final configuration and perform more intense testing in a staging environment. Finally, you serve your customers from the production environment.\\xA0\"]}),`\n`,(0,n.jsxs)(e.h3,{id:\"what-are-environments-composed-of\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#what-are-environments-composed-of\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"What Are Environments Composed of?\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"When we talk about environments, especially production, we tend to focus on the systems that run the core software stack. A typical scenario might include a database server (or cluster), \",(0,n.jsx)(e.a,{href:\"https://docs.release.com/guides-and-examples/advanced-guides/microservices-architecture\",children:\"microservices\"}),', customer-facing web server, and network gear. These may be managed resources, cloud native services, or ephemeral systems in a cloud environment, but they tend to be \"what we talk about when we talk about environments.\"']}),`\n`,(0,n.jsx)(e.p,{children:\"But environments are more than the systems and services they run on. An application has code, but it has data in the form of configurations, user information, and other resources, too. In most cases, that data differs between environments. That data is part of the environment. When the developer runs the environment on their laptop, they create a development environment. Their system configuration is very different from what's required to support the production environment and they are usually not using live user information.\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:\"In today\\u2019s complex software architecture environments also include cloud native services, and security policies governing access requirements to various systems and services.\"}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/0558487bbdd0a363a0007e56f18e4d1a.jpg\",alt:\"\"})}),`\n`,(0,n.jsxs)(e.h3,{id:\"what-is-a-production-environment\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#what-is-a-production-environment\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"What Is a Production Environment?\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Simply put, your production environment is where your application meets your primary users. This applies to external (public, customers consuming your services) or internal (enterprise, users within the organizations for whom the application is built). Let's look at what that means and how it affects how you treat this environment.\\xA0\"}),`\n`,(0,n.jsxs)(e.h3,{id:\"what-makes-production-unique\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#what-makes-production-unique\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"What Makes Production Unique?\"]}),`\n`,(0,n.jsxs)(e.h4,{id:\"revenue-and-customer-satisfaction\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#revenue-and-customer-satisfaction\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Revenue and Customer Satisfaction\"]}),`\n`,(0,n.jsx)(e.p,{children:`For almost all businesses, the fact that production supports their users means that it represents their primary source of revenue. In many cases, clients are paying for access to the application, (or internal users are using it to do business directly). Other clients may pay for \"offline\" services, but production is how they interface with the company. In all cases, if the production environment isn't functioning correctly, revenue is in danger.\\xA0`}),`\n`,(0,n.jsx)(e.p,{children:`If clients can't access your application, they can't give (or make) you money. Every minute of downtime means lost revenue. When a major internet service provider reports downtime, the headlines usually contain \"lost\" revenue numbers.\\xA0`}),`\n`,(0,n.jsx)(e.p,{children:\"But a production outage has a long-term effect, too. While your application is down, there's an excellent chance your customers will take their business to someone else. If they have a good experience with your competitor, they may not come back.\\xA0\"}),`\n`,(0,n.jsxs)(e.h4,{id:\"security\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#security\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Security\"]}),`\n`,(0,n.jsx)(e.p,{children:\"If users use your production information, it's available to the public, which means it's in constant danger from attack. Production environments have extraordinary security requirements.\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:\"Attackers can cause problems in many ways. One is extortion via ransomware. If they can compromise even one system, they can install code that spreads throughout your production environment and forces you to either pay them and hope they follow through or start from scratch and build a new environment.\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:\"Another attacker may break in to steal user information. This attack exposes you to legal and financial liability. It can have a fatal impact on your reputation, too.\\xA0\"}),`\n`,(0,n.jsxs)(e.h3,{id:\"what-are-the-risks-to-production\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#what-are-the-risks-to-production\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"What Are the Risks to Production?\"]}),`\n`,(0,n.jsx)(e.p,{children:\"We've talked about what can go wrong if your production environment is broken or compromised. What are the common causes of these issues?\\xA0\"}),`\n`,(0,n.jsxs)(e.h3,{id:\"insufficient-or-incorrect-testing\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#insufficient-or-incorrect-testing\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Insufficient or Incorrect Testing\"]}),`\n`,(0,n.jsx)(e.p,{children:\"In many organizations, testing is like the weather; everyone talks about testing, but no one does anything about it. A test plan missed an issue because it didn't exist, or it wasn't executed, or the test plan failed to account for the issue.\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:\"The first two scenarios are probably more common than many of us would like to admit.\\xA0\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"The third scenario points back to the definition of an environment. User acceptance testing doesn't work if your \",(0,n.jsx)(e.a,{href:\"https://release.com/blog/what-are-the-challenges-faced-during-uat-testing\",children:\"environment doesn't look like production.\"}),\" If, for example, your production environment has more than 30 virtual machines, but your development environment only has four, testing in development doesn't cover all of your cases. It's too expensive to reproduce your production environment for every developer or development team. But you still need to find a cost-effective way to \",(0,n.jsx)(e.a,{href:\"https://release.com/usecase/on-demand-staging-environments\",children:\"build a staging environment\"}),\" that looks like production.\\xA0\"]}),`\n`,(0,n.jsxs)(e.h3,{id:\"security-and-access-control\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#security-and-access-control\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Security and Access Control\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Security is an ongoing concern for everyone. We already covered the risks that a breach represents above. Proper access control goes a long way toward preventing these issues. Access control can also act as a check against incorrect configuration changes and premature releases.\"}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/ac685788143302558873d9f8bf99cd27.png\",alt:\"\"})}),`\n`,(0,n.jsxs)(e.p,{children:['Another way to tighten security is via automation. Automation is an effective tool for enforcing the \"',(0,n.jsx)(e.a,{href:\"https://release.com/usecase/on-demand-staging-environments\",children:\"principle of least privilege\"}),'\" because you can delegate deployment and configuration tasks to entities with limited privileges.\\xA0']}),`\n`,(0,n.jsx)(e.p,{children:\"To protect yourself from data loss, recognize that any system storing user information is part of the production environment. If you don't want to extend production into that system, don't let the data go there. If the system is storing data that you can't afford to lose, confine it to the strictest level of access control possible.\\xA0\"}),`\n`,(0,n.jsxs)(e.h3,{id:\"how-can-release-help\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#how-can-release-help\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"How Can Release Help?\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Developing and testing for today\\u2019s architectures can get complex. Things work well in development and staging, then they break down in pre-production and you have to rollback. Sound familiar?\"}),`\n`,(0,n.jsx)(e.p,{children:\"Creating an up-to-date production replica will significantly reduce your rollback rate. When you test with the right dataset, security policies, and services you remove the differences between your environments, and testing is more effective.\\xA0\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"Better user acceptance testing makes your production environment better, and the \",(0,n.jsx)(e.a,{href:\"https://release.com/blog/how-do-you-prepare-for-user-acceptance-testing\",children:\"best way to accomplish that\"}),\" is with an environment that looks like production. What if you could create a replica of your production environment, perform your testing, and then \",(0,n.jsx)(e.a,{href:\"https://release.com/ephemeral-environments\",children:\"tear it down until you need it next time\"}),\"? What if your user acceptance testing could help you find shortcomings in your production environment and not just in your code?\\xA0\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"Do you need a large dataset to test or code against? Don't risk your production data; use an \",(0,n.jsx)(e.a,{href:\"https://docs.release.com/reference-documentation/instant-datasets-aws\",children:\"instant dataset\"}),\" and throw it away when you're finished.\\xA0\"]}),`\n`,(0,n.jsxs)(e.h3,{id:\"multi-cloud-deployments-of-private-applications\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#multi-cloud-deployments-of-private-applications\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Multi-Cloud Deployments of Private Applications\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Software companies who, for security reasons, deploy their application on customers' private clouds often find it challenging to ensure that application works on every cloud vendor.\"}),`\n`,(0,n.jsx)(e.p,{children:\"Release makes it easy to deploy your private application in AWS or GCP without refactoring. We create an abstraction of the cloud native services, making it easier to support and maintain your application without limiting yourself to one cloud vendor or another.\"}),`\n`,(0,n.jsxs)(e.h3,{id:\"build-a-better-production-environment\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#build-a-better-production-environment\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Build a Better Production Environment\"]}),`\n`,(0,n.jsx)(e.p,{children:\"This post talked about the essential parts of an environment and how you need to examine those parts when comparing one domain to another. Then we discussed production and what makes it unique when compared to other environments. Finally, we talked about how you take positive steps to improve the reliability of your production environment.\\xA0\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"Release has the tools you need to build better environments. Discover \",(0,n.jsx)(e.a,{href:\"https://release.com/use-cases\",children:\"ways\"}),\" to improve production development speed and flexibility, and equip your testing and engineering teams with better tools today, \",(0,n.jsx)(e.a,{href:\"https://release.com/get-started\",children:\"start for free\"}),\"!\"]})]})}function k(t={}){let{wrapper:e}=t.components||{};return e?(0,n.jsx)(e,Object.assign({},t,{children:(0,n.jsx)(h,t)})):h(t)}var x=k;return w(W);})();\n;return Component;"
        },
        "_id": "blog/posts/what-is-a-production-environment-and-why-is-it-unique.mdx",
        "_raw": {
          "sourceFilePath": "blog/posts/what-is-a-production-environment-and-why-is-it-unique.mdx",
          "sourceFileName": "what-is-a-production-environment-and-why-is-it-unique.mdx",
          "sourceFileDir": "blog/posts",
          "contentType": "mdx",
          "flattenedPath": "blog/posts/what-is-a-production-environment-and-why-is-it-unique"
        },
        "type": "BlogPost",
        "computedSlug": "what-is-a-production-environment-and-why-is-it-unique"
      },
      "documentHash": "1739393595030",
      "hasWarnings": false,
      "documentTypeName": "BlogPost"
    },
    "blog/posts/what-is-a-qa-environment-and-how-do-you-manage-it.mdx": {
      "document": {
        "title": "What Is a QA Environment and How Do You Manage It?",
        "summary": "This article will discuss the importance of a robust QA environment and give some practical tips on how to manage it.",
        "publishDate": "Wed Dec 21 2022 10:30:22 GMT+0000 (Coordinated Universal Time)",
        "author": "regis-wilson",
        "readingTime": 5,
        "categories": [
          "platform-engineering",
          "product"
        ],
        "mainImage": "/blog-images/20f503180a7b38d49d464ff967861e06.jpg",
        "imageAlt": "What Is a QA Environment and How Do You Manage It?",
        "showCTA": true,
        "ctaCopy": "Enhance QA environments with Release for isolated testing, faster bug resolution, and consistent deployments. Optimize your software quality assurance process today!",
        "ctaLink": "https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=what-is-a-qa-environment-and-how-do-you-manage-it",
        "relatedPosts": [
          ""
        ],
        "ogImage": "/blog-images/20f503180a7b38d49d464ff967861e06.jpg",
        "excerpt": "This article will discuss the importance of a robust QA environment and give some practical tips on how to manage it.",
        "tags": [
          "platform-engineering",
          "product"
        ],
        "ctaButton": "Try Release for Free",
        "body": {
          "raw": "\n\"QA\" is an umbrella term that covers many different areas of a company's operations. It can describe various tasks, such as the testing and quality assurance process, test methods, and processes. These terms are often used interchangeably but should not be interpreted literally. The term \"[QA](https://en.wikipedia.org/wiki/Quality_assurance)\" describes how software was tested during its production process before it was released. \n\nSoftware testing is critical to the software development life cycle. It helps ensure that the final product meets its requirements before being released into production. Software testers perform various tasks, such as verifying that the application functions correctly, identifying bugs, and making sure the application complies with relevant regulations. They also test the user interface and usability of the application.  \n\nManaging quality assurance is a crucial aspect of software development. Unfortunately, it's often overlooked due to time constraints. And many companies use manual testing or simply don't test at all. A QA environment is a type of test environment specifically designed to support software quality. In other words, it's a test environment with a set of requirements containing elements specifically intended to support software verification.  \n\nToday, organizations are using QA environments more and more frequently as part of the development process. This article will discuss the importance of a robust QA environment and give some practical tips on how to manage it. Read on to learn more! \n\n‍\n\n![](/blog-images/7bbc0ec7aadc845ae8eb16c541c5376c.jpeg)\n\n### What Is a QA Environment, and What Is Its Purpose?\n\nA QA environment is a testing environment used to validate a software application's quality before it is deployed to production, and to optimize software development processes so that the software works. This environment is typically isolated from other environments, such as development and staging, to ensure that any issues that are found in the QA environment do not impact the live production system. \n\nQA environments can monitor software development processes periodically or continuously. Continuous monitoring means your software developer continuously monitors and records data about how their project is going, how much time they spend on each process, and how many bugs they find during tests and feature additions. \n\nPeriodic testing is where you have one set of developers doing one type of testing for one period of time. This way, you have more consistent quality throughout your entire team. You can also ensure that all updates and additions are properly accounted for. Additionally, you don't have to constantly monitor and record data about each process in your organization. \n\n![](/blog-images/5fef15034ad91124e9c9f91678d26747.png)\n\nInstead, a developer manually checks in with their manager when they're done with their process check-in. They then perform another check-in after every feature addition or process update. This is because the outlying cases are so widespread that it would be too difficult for them to catch up if they were constantly monitoring everything. \n\n### Why Do We Need a QA Environment?\n\nQA environments are often referred to as \"[production ready](https://dzone.com/articles/5-principles-of-production-readiness).\" This means they meet specific minimum requirements before they can be rolled out to production for general consumption. The goal of QA is to ensure that bugs don't slip through during development. It helps ensure that the final product meets its specifications and performs flawlessly once released into production. \n\nA realistic test environment—the production equivalent of an actual system deployment—is what you need to manage a realistic QA environment. There may be no specific reason why your QA environment should meet these minimum requirements, but it might be more effective if you have one. \n\n### How Do You Create a QA Environment?\n\nHere is what you need to know about creating and running a quality-aware QA environment. To set up a QA environment, you'll need to determine the kind of infrastructure to test and which testing technique to use. \n\nYou'll also need to choose the testing tools and methodologies. Once the infrastructure is set up and configured, you'll need to [create a test environment](https://release.com/blog/setup-test-environment). This will be the environment in which your application will be tested. \n\nOnce the environment is set up, you'll need to create and run tests. The tests will verify that your application works as expected and that the infrastructure underneath can run your application smoothly. \n\nWhen the tests are complete, you'll need to analyze the results and make changes to your application as necessary.  \n\n### Understanding the QA Environment Using an Example\n\nLet's take an example of a SaaS product. The application side is for existing users, and there's a set of landing pages as a part of the marketing campaign. The testing that needs to be done and the scope of QA could be, say, the performance of the application and the static pages. \n\nMany tools analyze the performance of web applications and landing pages that organizations widely use. Lighthouse is one such open-source, automated tool for improving the quality and performance of web applications. It runs audits for performance, accessibility, progressive web apps, SEO, and much more. You can run Lighthouse using Chrome DevTools, from the command line, or as a standalone Node.js module. \n\n![](/blog-images/cacaefdd2e23633f2018e991d1c98b3c.png)\n\nWith Lighthouse, you can test static HTML pages against dozens of performance and modern development best practices. It produces a report that includes a score for each category, actionable advice, and a list of resources for further reading. This is helpful for those who want to dive deeper into making their landing pages top-class in terms of performance. \n\nOn the other hand, Selenium is a tool that developers can use to automate web browsers. You can use it to simulate user actions on your web application, such as filling out a form or clicking a button. Selenium can also automatically take screenshots of webpages or record a user's browsing session. \n\nWith Selenium, you can also test dynamic web applications whose state changes based on user actions or depending on the user's life cycle to ensure that the web application is working as expected. \n\n### What Are QA and UAT?\n\nPeople often confuse QA environments with user acceptance testing (UAT) environments since they both involve testing. But they have different objectives. \n\n[User acceptance testing](https://release.com/blog/user-acceptance-testing-best-practices) is a process of designing a test that helps make sure the functionality of a software application is supported by the users. In contrast, QA testing is a process of validating software application functionality against specific requirements (to ensure the accuracy and completeness of the software). Software engineers often use it to validate a software application's functionality mathematically. \n\nSimply put, the difference is that QA aims for error-free software, whereas UAT ensures that users get the product they want. Quality assurance teams work hard to make the user acceptance testing process as smooth and customer-friendly as possible. \n\n![](/blog-images/9179465a025e7f1feff67b4025bc0b2c.png)\n\n### Summary\n\nQA environments are essential in making sure your application runs smoothly. They are also helpful for testing new features or bug fixes before deployment.  Testing typically involves: \n\n- Reviewing code for errors\n- Checking functionality\n- Running tests against sample data\n- Conducting usability studies\n\nQuality assurance takes place throughout the entire life cycle of the project—from planning and research to design, coding, testing, deployment, support, maintenance, and even after retirement. Therefore, you need to manage your QA environment efficiently to get the maximum return on investment. This means managing your time well, having a good plan, and keeping track of tasks. It's important to remember that every piece of software has bugs. But you want to ensure these bugs don't impact your application's performance or functioning. \n\nWe hope you liked the post. If you want to learn more about automated software environments, check out our [ultimate guide](https://release.com/ebook/the-complete-guide-to-automated-software-environments).  \n\nThe bottom line is that a QA environment is an integral part of the software development process, and it's essential for ensuring the quality of your application. By following the tips in this article, you can create a QA environment that is efficient and effective.\n",
          "code": "var Component=(()=>{var d=Object.create;var i=Object.defineProperty;var u=Object.getOwnPropertyDescriptor;var p=Object.getOwnPropertyNames;var m=Object.getPrototypeOf,f=Object.prototype.hasOwnProperty;var g=(n,e)=>()=>(e||n((e={exports:{}}).exports,e),e.exports),y=(n,e)=>{for(var a in e)i(n,a,{get:e[a],enumerable:!0})},r=(n,e,a,s)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let o of p(e))!f.call(n,o)&&o!==a&&i(n,o,{get:()=>e[o],enumerable:!(s=u(e,o))||s.enumerable});return n};var w=(n,e,a)=>(a=n!=null?d(m(n)):{},r(e||!n||!n.__esModule?i(a,\"default\",{value:n,enumerable:!0}):a,n)),v=n=>r(i({},\"__esModule\",{value:!0}),n);var l=g((q,c)=>{c.exports=_jsx_runtime});var Q={};y(Q,{default:()=>k,frontmatter:()=>b});var t=w(l()),b={title:\"What Is a QA Environment and How Do You Manage It?\",summary:\"This article will discuss the importance of a robust QA environment and give some practical tips on how to manage it.\",publishDate:\"Wed Dec 21 2022 10:30:22 GMT+0000 (Coordinated Universal Time)\",author:\"regis-wilson\",readingTime:5,categories:[\"platform-engineering\",\"product\"],mainImage:\"/blog-images/20f503180a7b38d49d464ff967861e06.jpg\",imageAlt:\"What Is a QA Environment and How Do You Manage It?\",showCTA:!0,ctaCopy:\"Enhance QA environments with Release for isolated testing, faster bug resolution, and consistent deployments. Optimize your software quality assurance process today!\",ctaLink:\"https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=what-is-a-qa-environment-and-how-do-you-manage-it\",relatedPosts:[\"\"],ogImage:\"/blog-images/20f503180a7b38d49d464ff967861e06.jpg\",excerpt:\"This article will discuss the importance of a robust QA environment and give some practical tips on how to manage it.\",tags:[\"platform-engineering\",\"product\"],ctaButton:\"Try Release for Free\"};function h(n){let e=Object.assign({p:\"p\",a:\"a\",img:\"img\",h3:\"h3\",span:\"span\",ul:\"ul\",li:\"li\"},n.components);return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsxs)(e.p,{children:[`\"QA\" is an umbrella term that covers many different areas of a company's operations. It can describe various tasks, such as the testing and quality assurance process, test methods, and processes. These terms are often used interchangeably but should not be interpreted literally. The term \"`,(0,t.jsx)(e.a,{href:\"https://en.wikipedia.org/wiki/Quality_assurance\",children:\"QA\"}),'\" describes how software was tested during its production process before it was released.\\xA0']}),`\n`,(0,t.jsx)(e.p,{children:\"Software testing is critical to the software development life cycle. It helps ensure that the final product meets its requirements before being released into production. Software testers perform various tasks, such as verifying that the application functions correctly, identifying bugs, and making sure the application complies with relevant regulations. They also test the user interface and usability of the application. \\xA0\"}),`\n`,(0,t.jsx)(e.p,{children:\"Managing quality assurance is a crucial aspect of software development. Unfortunately, it's often overlooked due to time constraints. And many companies use manual testing or simply don't test at all. A QA environment is a type of test environment specifically designed to support software quality. In other words, it's a test environment with a set of requirements containing elements specifically intended to support software verification. \\xA0\"}),`\n`,(0,t.jsx)(e.p,{children:\"Today, organizations are using QA environments more and more frequently as part of the development process. This article will discuss the importance of a robust QA environment and give some practical tips on how to manage it. Read on to learn more!\\xA0\"}),`\n`,(0,t.jsx)(e.p,{children:\"\\u200D\"}),`\n`,(0,t.jsx)(e.p,{children:(0,t.jsx)(e.img,{src:\"/blog-images/7bbc0ec7aadc845ae8eb16c541c5376c.jpeg\",alt:\"\"})}),`\n`,(0,t.jsxs)(e.h3,{id:\"what-is-a-qa-environment-and-what-is-its-purpose\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#what-is-a-qa-environment-and-what-is-its-purpose\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"What Is a QA Environment, and What Is Its Purpose?\"]}),`\n`,(0,t.jsx)(e.p,{children:\"A QA environment is a testing environment used to validate a software application's quality before it is deployed to production, and to optimize software development processes so that the software works. This environment is typically isolated from other environments, such as development and staging, to ensure that any issues that are found in the QA environment do not impact the live production system.\\xA0\"}),`\n`,(0,t.jsx)(e.p,{children:\"QA environments can monitor software development processes periodically or continuously. Continuous monitoring means your software developer continuously monitors and records data about how their project is going, how much time they spend on each process, and how many bugs they find during tests and feature additions.\\xA0\"}),`\n`,(0,t.jsx)(e.p,{children:\"Periodic testing is where you have one set of developers doing one type of testing for one period of time. This way, you have more consistent quality throughout your entire team. You can also ensure that all updates and additions are properly accounted for. Additionally, you don't have to constantly monitor and record data about each process in your organization.\\xA0\"}),`\n`,(0,t.jsx)(e.p,{children:(0,t.jsx)(e.img,{src:\"/blog-images/5fef15034ad91124e9c9f91678d26747.png\",alt:\"\"})}),`\n`,(0,t.jsx)(e.p,{children:\"Instead, a developer manually checks in with their manager when they're done with their process check-in. They then perform another check-in after every feature addition or process update. This is because the outlying cases are so widespread that it would be too difficult for them to catch up if they were constantly monitoring everything.\\xA0\"}),`\n`,(0,t.jsxs)(e.h3,{id:\"why-do-we-need-a-qa-environment\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#why-do-we-need-a-qa-environment\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Why Do We Need a QA Environment?\"]}),`\n`,(0,t.jsxs)(e.p,{children:['QA environments are often referred to as \"',(0,t.jsx)(e.a,{href:\"https://dzone.com/articles/5-principles-of-production-readiness\",children:\"production ready\"}),`.\" This means they meet specific minimum requirements before they can be rolled out to production for general consumption. The goal of QA is to ensure that bugs don't slip through during development. It helps ensure that the final product meets its specifications and performs flawlessly once released into production.\\xA0`]}),`\n`,(0,t.jsx)(e.p,{children:\"A realistic test environment\\u2014the production equivalent of an actual system deployment\\u2014is what you need to manage a realistic QA environment. There may be no specific reason why your QA environment should meet these minimum requirements, but it might be more effective if you have one.\\xA0\"}),`\n`,(0,t.jsxs)(e.h3,{id:\"how-do-you-create-a-qa-environment\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#how-do-you-create-a-qa-environment\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"How Do You Create a QA Environment?\"]}),`\n`,(0,t.jsx)(e.p,{children:\"Here is what you need to know about creating and running a quality-aware QA environment. To set up a QA environment, you'll need to determine the kind of infrastructure to test and which testing technique to use.\\xA0\"}),`\n`,(0,t.jsxs)(e.p,{children:[\"You'll also need to choose the testing tools and methodologies. Once the infrastructure is set up and configured, you'll need to \",(0,t.jsx)(e.a,{href:\"https://release.com/blog/setup-test-environment\",children:\"create a test environment\"}),\". This will be the environment in which your application will be tested.\\xA0\"]}),`\n`,(0,t.jsx)(e.p,{children:\"Once the environment is set up, you'll need to create and run tests. The tests will verify that your application works as expected and that the infrastructure underneath can run your application smoothly.\\xA0\"}),`\n`,(0,t.jsx)(e.p,{children:\"When the tests are complete, you'll need to analyze the results and make changes to your application as necessary. \\xA0\"}),`\n`,(0,t.jsxs)(e.h3,{id:\"understanding-the-qa-environment-using-an-example\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#understanding-the-qa-environment-using-an-example\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Understanding the QA Environment Using an Example\"]}),`\n`,(0,t.jsx)(e.p,{children:\"Let's take an example of a SaaS product. The application side is for existing users, and there's a set of landing pages as a part of the marketing campaign. The testing that needs to be done and the scope of QA could be, say, the performance of the application and the static pages.\\xA0\"}),`\n`,(0,t.jsx)(e.p,{children:\"Many tools analyze the performance of web applications and landing pages that organizations widely use. Lighthouse is one such open-source, automated tool for improving the quality and performance of web applications. It runs audits for performance, accessibility, progressive web apps, SEO, and much more. You can run Lighthouse using Chrome DevTools, from the command line, or as a standalone Node.js module.\\xA0\"}),`\n`,(0,t.jsx)(e.p,{children:(0,t.jsx)(e.img,{src:\"/blog-images/cacaefdd2e23633f2018e991d1c98b3c.png\",alt:\"\"})}),`\n`,(0,t.jsx)(e.p,{children:\"With Lighthouse, you can test static HTML pages against dozens of performance and modern development best practices. It produces a report that includes a score for each category, actionable advice, and a list of resources for further reading. This is helpful for those who want to dive deeper into making their landing pages top-class in terms of performance.\\xA0\"}),`\n`,(0,t.jsx)(e.p,{children:\"On the other hand, Selenium is a tool that developers can use to automate web browsers. You can use it to simulate user actions on your web application, such as filling out a form or clicking a button. Selenium can also automatically take screenshots of webpages or record a user's browsing session.\\xA0\"}),`\n`,(0,t.jsx)(e.p,{children:\"With Selenium, you can also test dynamic web applications whose state changes based on user actions or depending on the user's life cycle to ensure that the web application is working as expected.\\xA0\"}),`\n`,(0,t.jsxs)(e.h3,{id:\"what-are-qa-and-uat\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#what-are-qa-and-uat\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"What Are QA and UAT?\"]}),`\n`,(0,t.jsx)(e.p,{children:\"People often confuse QA environments with user acceptance testing (UAT) environments since they both involve testing. But they have different objectives.\\xA0\"}),`\n`,(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.a,{href:\"https://release.com/blog/user-acceptance-testing-best-practices\",children:\"User acceptance testing\"}),\" is a process of designing a test that helps make sure the functionality of a software application is supported by the users. In contrast, QA testing is a process of validating software application functionality against specific requirements (to ensure the accuracy and completeness of the software). Software engineers often use it to validate a software application's functionality mathematically.\\xA0\"]}),`\n`,(0,t.jsx)(e.p,{children:\"Simply put, the difference is that QA aims for error-free software, whereas UAT ensures that users get the product they want. Quality assurance teams work hard to make the user acceptance testing process as smooth and customer-friendly as possible.\\xA0\"}),`\n`,(0,t.jsx)(e.p,{children:(0,t.jsx)(e.img,{src:\"/blog-images/9179465a025e7f1feff67b4025bc0b2c.png\",alt:\"\"})}),`\n`,(0,t.jsxs)(e.h3,{id:\"summary\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#summary\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Summary\"]}),`\n`,(0,t.jsx)(e.p,{children:\"QA environments are essential in making sure your application runs smoothly. They are also helpful for testing new features or bug fixes before deployment. \\xA0Testing typically involves:\\xA0\"}),`\n`,(0,t.jsxs)(e.ul,{children:[`\n`,(0,t.jsx)(e.li,{children:\"Reviewing code for errors\"}),`\n`,(0,t.jsx)(e.li,{children:\"Checking functionality\"}),`\n`,(0,t.jsx)(e.li,{children:\"Running tests against sample data\"}),`\n`,(0,t.jsx)(e.li,{children:\"Conducting usability studies\"}),`\n`]}),`\n`,(0,t.jsx)(e.p,{children:\"Quality assurance takes place throughout the entire life cycle of the project\\u2014from planning and research to design, coding, testing, deployment, support, maintenance, and even after retirement. Therefore, you need to manage your QA environment efficiently to get the maximum return on investment. This means managing your time well, having a good plan, and keeping track of tasks. It's important to remember that every piece of software has bugs. But you want to ensure these bugs don't impact your application's performance or functioning.\\xA0\"}),`\n`,(0,t.jsxs)(e.p,{children:[\"We hope you liked the post. If you want to learn more about automated software environments, check out our \",(0,t.jsx)(e.a,{href:\"https://release.com/ebook/the-complete-guide-to-automated-software-environments\",children:\"ultimate guide\"}),\". \\xA0\"]}),`\n`,(0,t.jsx)(e.p,{children:\"The bottom line is that a QA environment is an integral part of the software development process, and it's essential for ensuring the quality of your application. By following the tips in this article, you can create a QA environment that is efficient and effective.\"})]})}function A(n={}){let{wrapper:e}=n.components||{};return e?(0,t.jsx)(e,Object.assign({},n,{children:(0,t.jsx)(h,n)})):h(n)}var k=A;return v(Q);})();\n;return Component;"
        },
        "_id": "blog/posts/what-is-a-qa-environment-and-how-do-you-manage-it.mdx",
        "_raw": {
          "sourceFilePath": "blog/posts/what-is-a-qa-environment-and-how-do-you-manage-it.mdx",
          "sourceFileName": "what-is-a-qa-environment-and-how-do-you-manage-it.mdx",
          "sourceFileDir": "blog/posts",
          "contentType": "mdx",
          "flattenedPath": "blog/posts/what-is-a-qa-environment-and-how-do-you-manage-it"
        },
        "type": "BlogPost",
        "computedSlug": "what-is-a-qa-environment-and-how-do-you-manage-it"
      },
      "documentHash": "1739393595030",
      "hasWarnings": false,
      "documentTypeName": "BlogPost"
    },
    "blog/posts/what-is-an-internal-developer-platform-and-why-should-i-have-one.mdx": {
      "document": {
        "title": "What Is an Internal Developer Platform and Why Should I Have One?",
        "summary": "IDPs automate self-service for developers, streamlining software practices, infrastructure, environments, and operations",
        "publishDate": "Wed Aug 02 2023 20:49:49 GMT+0000 (Coordinated Universal Time)",
        "author": "sylvia-fronczak",
        "readingTime": 10,
        "categories": [
          "platform-engineering",
          "product"
        ],
        "mainImage": "/blog-images/b22d39624eadf32e5019d4f55aa45252.jpg",
        "imageAlt": "What Is an Internal Developer Platform and Why Should I Have One?",
        "showCTA": true,
        "ctaCopy": "Automate environment provisioning and streamline workflows with Release's platform to enhance your internal developer platform.",
        "ctaLink": "https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=what-is-an-internal-developer-platform-and-why-should-i-have-one",
        "relatedPosts": [
          ""
        ],
        "ogImage": "/blog-images/b22d39624eadf32e5019d4f55aa45252.jpg",
        "excerpt": "IDPs automate self-service for developers, streamlining software practices, infrastructure, environments, and operations",
        "tags": [
          "platform-engineering",
          "product"
        ],
        "ctaButton": "Try Release for Free",
        "body": {
          "raw": "\nIt seems like everyone is talking about internal developer platforms (IDPs). But what is an IDP, really, and how can it help your team?\n\nLet’s explore what an IDP is from a few angles. We’ll look at the definition, how the need for the IDPs developed, and the goals and outcomes of IDPs. With this information, you can assess whether IDPs can help your teams and organization and briefly look at next steps.\n\nNow let’s get started with a definition.\n\n### A Simple Definition\n\nAn internal developer platform provides automated self-service solutions for developers to simplify and standardize software practices, infrastructure, environments, and operations.\n\nThat’s it.\n\nDoes it not seem like it’s specific enough?\n\nWell, that open-ended definition can be interpreted and built in many different ways into many forms. Is that OK? Yes, absolutely. That’s deliberate.\n\nLet’s note what this definition does not specify.\n\nFirst, perhaps you noticed that the definition does not specify any particular tool. An IDP consists of multiple tools and technologies that are integrated together. These tools vary from company to company and stack to stack. It really depends on the use case and should be built in accordance with the needs of the particular organization that wants the IDP.\n\nAdditionally, this definition doesn’t mention [continuous integration and continuous deployment (CI/CD) tools](https://release.com/blog/11-continuous-deployment-tools-and-how-to-choose-one), microservice catalogs, or environment provisioning. Though those capabilities are often provided through the IDP, they are not an IDP on their own.\n\nAnd finally, the definition doesn’t mention a single pane of glass. In recent years, there’s been a lot of focus on bringing all the tools together in one place. Unfortunately, a central location isn’t enough. The IDP should be integrated throughout the development workflow in various ways based on need. There may be common UIs in an IDP, but there could also instead be CLIs, APIs, and other integration points between tools. It’s more about understanding the development process and adding the right automation in the right place at the right time.\n\nNow that we’ve covered the definition, let's talk about how the need for IDPs developed.\n\n### How the Need for IDPs Developed\n\nIn the distant past, many developers handled both infrastructure and software. The environments were simpler and didn’t require a lot of specialization.\n\nOf course, the landscape changed. It was less about one C++ app running on a server somewhere and more about a collage of technologies working together. Tasks like deploying and operating the infrastructure moved to specialized operations or release management teams.\n\nUnfortunately, and typically due to a lack of automation and increased bureaucracy, the development teams felt held back, unable to get their changes into [the production environment](https://release.com/blog/what-is-a-production-environment-and-why-is-it-unique) quickly enough. And operations folks guarded their environment against changes in production to reduce incidents and outages. This took many forms, some more automated than others. As an example of less automation and more bureaucracy, some organizations required multiple requests, forms, or even Excel docs that specified what changes and what files were supposed to move to what environment. Even when there was more automation, the tools tended to be focused on the needs of administrators and not developers. There was a lot of friction and pain, and developers were not able to get their changes to production quickly.\n\nFrom that pain, DevOps was born. In order to liberate developers and allow them to deploy more frequently, they were also given the keys to the CI/CD pipeline and entrusted to handle infrastructure and operations for their applications as well. This made sense in theory, but in practice, various interpretations of DevOps resulted in every single team in an organization implementing their own CI/CD pipeline, every team applying the same patches to their pipeline, and learning all the same lessons of proper CI/CD and operations as each other. What started as an exception became the norm. And once again, development teams across organizations spent countless hours on work that had very little to do with building new products and features.\n\nThere has to be a better way. How can we take the learnings from DevOps, but also make the development life cycle simpler to manage from the development team’s perspective? That’s where IDPs can help.\n\n‍\n\n![](/blog-images/64c5e84ac5ebf4f4f9d37f4b24f256ac.png)\n\n### Goals/Outcome of an IDP\n\nWhen we begin to look at what an IDP can do for an organization, we typically try to meet one or more of the following goals:\n\n1.  Reduce toil\n2.  Reduce context switching\n3.  Allow for technical specialization\n4.  Drive standardization\n5.  Increase efficiency\n6.  Improve developer experience\n7.  Improve time to market\n\nThese goals all tie together well and overlap frequently.\n\nFirst, **reducing toil** involves reducing repetitive work that can be automated away. This could involve often-used tactics like automating CI/CD and tests. Reducing toil can also include automating dependency upgrades, audits, and access requests.\n\nSecond, an IDP should reduce **context switching**. In part, when we reduce toil, we automatically reduce some amount of context switching. Additionally, with the appropriate automations and integrations, developers should not have to jump context between application development, environment management, infrastructure concerns, and whatever else is necessary.\n\nA proper IDP will automate and standardize many of the tools and technologies required by the development team to get their features out to production, which then also allows them to **specialize** in their product domain and in software engineering skills.\n\nAdditionally, IDPs can drive **standardization** by providing out-of-the-box integration with static code analysis and enforced coding standards. And the teams themselves do not configure this each time for their new service or product. Instead, it’s baked in from the start and just works.\n\nWhen you **increase efficiency**, you’re making it easier for the team to focus on the job they do best—write software for their domain. They spend less time on work that doesn’t involve their craft or domain, making them more efficient overall. This, as well as previous points, also improves the **developer experience.** The development team has more time to focus on their craft and domain and worries less about the peripheral needs of their software stack.\n\nAnd finally, all of these combined result in improving **time to market**. When you reduce toil, reduce context switching, and increase efficiency, your development team can focus on building the product for your customer and not spend time on efforts that dilute their focus. They are able to prioritize the most important work with increased specialization for your target audience.\n\n### Common Problems IDPs Can Solve\n\nWe’ve covered the definition, history, and goals of IDPs, but sometimes it’s still difficult to see how this applies to your organization. You need more to understand how an IDP can help you.\n\nLet’s look at common problems that IDPs can solve.\n\nFirst, let’s consider the attention sprawl and security risks example mentioned earlier. If teams have to manage their own infrastructure and CI/CD pipeline, they’re spending time on tasks that do not deliver working software to customers. More so, they are spending time doing tasks that they’re not always knowledgeable in. This can result in security vulnerabilities, like leaving certain ports and directories open or not utilizing proper deployment strategies and configuration management.\n\nAdding on to the security aspect, when teams manage their own infrastructure and CI/CD, they each must learn how to integrate your new security scanning software into their pipeline. At best, this will lead to repetitive work for each team that should have been centralized. At worst, teams may not configure the scans correctly, leaving you more vulnerable to security exploits.\n\n![](/blog-images/67f6d49acf6a72ffec4120ab21f58391.png)\n\nAs another example, perhaps your teams struggle with environment setup. This can manifest as inconsistent environment setup or, more commonly, as a limitation in the number of environments available. Many teams are tied to a [small number of environments](https://release.com/blog/testing-environment-types-and-what-theyre-used-for)—staging, QA, UAT, and production, for example. It’s been historically difficult to replicate [everything that goes into an environment](https://release.com/blog/a-simple-guide-to-software-environments), and so organizations often stick to these static environments. This can cause [bottlenecks](https://release.com/blog/increase-developer-velocity-by-removing-environment-bottlenecks) for testing and deployments with multiple devs working in the same environment or waiting on others to complete their testing. With static environments like this, you may also end up with a need to do large data refreshes or resets periodically to clean up odd testing scenarios. With proper IDP integration, we could utilize tools like Release to [provide ephemeral environments instead](https://release.com/blog/improving-developer-productivity-with-ephemeral-environments).\n\nFor our final example, let’s talk about waiting on tickets. Perhaps your organization utilizes ticket queues and request systems. If your teams are often waiting on requests for infrastructure changes, blocking their ability to build or ship code, then you’ve found an opportunity for automation in your IDP.\n\nThis is a short and general list. Wherever there’s friction in the development process, there’s opportunity to improve the process through an IDP.\n\n### Next Steps\n\nAt this point, you may think that an IDP will easily solve everyone’s problems. However, that’s not always the case. You need to carefully design your IDP to fit your specific needs, so that it can reduce toil, reduce context switching, improve the developer experience, and improve time to market. And if you don’t focus on the right aspects of your IDP, you could waste time and make the problems you’re trying to solve even worse.\n\nThe investment in an IDP will require investigation, prioritization, and a product-driven perspective. Your IT will require a dedicated team, support, and governance. To begin, take a look at **how** your development teams work. What tools do they use? How often do they spend on tasks outside of their domain or primary technologies? Once you’ve assessed your starting point, you can begin to build the components of an IDP that will benefit your particular organization the most.\n\nStay tuned to learn more about the components of a successful IDP in the next chapter on this IDP series.  \n\n‍*This post was written by Sylvia Fronczak.* [_Sylvia_](https://sylviafronczak.com/) _is a software developer that has worked in various industries with various software methodologies. She’s currently focused on design practices that the whole team can own, understand, and evolve over time._\n",
          "code": "var Component=(()=>{var c=Object.create;var a=Object.defineProperty;var p=Object.getOwnPropertyDescriptor;var m=Object.getOwnPropertyNames;var u=Object.getPrototypeOf,f=Object.prototype.hasOwnProperty;var g=(n,e)=>()=>(e||n((e={exports:{}}).exports,e),e.exports),v=(n,e)=>{for(var o in e)a(n,o,{get:e[o],enumerable:!0})},s=(n,e,o,r)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let i of m(e))!f.call(n,i)&&i!==o&&a(n,i,{get:()=>e[i],enumerable:!(r=p(e,i))||r.enumerable});return n};var y=(n,e,o)=>(o=n!=null?c(u(n)):{},s(e||!n||!n.__esModule?a(o,\"default\",{value:n,enumerable:!0}):o,n)),w=n=>s(a({},\"__esModule\",{value:!0}),n);var d=g((x,l)=>{l.exports=_jsx_runtime});var D={};v(D,{default:()=>I,frontmatter:()=>b});var t=y(d()),b={title:\"What Is an Internal Developer Platform and Why Should I Have One?\",summary:\"IDPs automate self-service for developers, streamlining software practices, infrastructure, environments, and operations\",publishDate:\"Wed Aug 02 2023 20:49:49 GMT+0000 (Coordinated Universal Time)\",author:\"sylvia-fronczak\",readingTime:10,categories:[\"platform-engineering\",\"product\"],mainImage:\"/blog-images/b22d39624eadf32e5019d4f55aa45252.jpg\",imageAlt:\"What Is an Internal Developer Platform and Why Should I Have One?\",showCTA:!0,ctaCopy:\"Automate environment provisioning and streamline workflows with Release's platform to enhance your internal developer platform.\",ctaLink:\"https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=what-is-an-internal-developer-platform-and-why-should-i-have-one\",relatedPosts:[\"\"],ogImage:\"/blog-images/b22d39624eadf32e5019d4f55aa45252.jpg\",excerpt:\"IDPs automate self-service for developers, streamlining software practices, infrastructure, environments, and operations\",tags:[\"platform-engineering\",\"product\"],ctaButton:\"Try Release for Free\"};function h(n){let e=Object.assign({p:\"p\",h3:\"h3\",a:\"a\",span:\"span\",img:\"img\",ol:\"ol\",li:\"li\",strong:\"strong\",em:\"em\"},n.components);return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(e.p,{children:\"It seems like everyone is talking about internal developer platforms (IDPs). But what is an IDP, really, and how can it help your team?\"}),`\n`,(0,t.jsx)(e.p,{children:\"Let\\u2019s explore what an IDP is from a few angles. We\\u2019ll look at the definition, how the need for the IDPs developed, and the goals and outcomes of IDPs. With this information, you can assess whether IDPs can help your teams and organization and briefly look at next steps.\"}),`\n`,(0,t.jsx)(e.p,{children:\"Now let\\u2019s get started with a definition.\"}),`\n`,(0,t.jsxs)(e.h3,{id:\"a-simple-definition\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#a-simple-definition\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"A Simple Definition\"]}),`\n`,(0,t.jsx)(e.p,{children:\"An internal developer platform provides automated self-service solutions for developers to simplify and standardize software practices, infrastructure, environments, and operations.\"}),`\n`,(0,t.jsx)(e.p,{children:\"That\\u2019s it.\"}),`\n`,(0,t.jsx)(e.p,{children:\"Does it not seem like it\\u2019s specific enough?\"}),`\n`,(0,t.jsx)(e.p,{children:\"Well, that open-ended definition can be interpreted and built in many different ways into many forms. Is that OK? Yes, absolutely. That\\u2019s deliberate.\"}),`\n`,(0,t.jsx)(e.p,{children:\"Let\\u2019s note what this definition does not specify.\"}),`\n`,(0,t.jsx)(e.p,{children:\"First, perhaps you noticed that the definition does not specify any particular tool. An IDP consists of multiple tools and technologies that are integrated together. These tools vary from company to company and stack to stack. It really depends on the use case and should be built in accordance with the needs of the particular organization that wants the IDP.\"}),`\n`,(0,t.jsxs)(e.p,{children:[\"Additionally, this definition doesn\\u2019t mention \",(0,t.jsx)(e.a,{href:\"https://release.com/blog/11-continuous-deployment-tools-and-how-to-choose-one\",children:\"continuous integration and continuous deployment (CI/CD) tools\"}),\", microservice catalogs, or environment provisioning. Though those capabilities are often provided through the IDP, they are not an IDP on their own.\"]}),`\n`,(0,t.jsx)(e.p,{children:\"And finally, the definition doesn\\u2019t mention a single pane of glass. In recent years, there\\u2019s been a lot of focus on bringing all the tools together in one place. Unfortunately, a central location isn\\u2019t enough. The IDP should be integrated throughout the development workflow in various ways based on need. There may be common UIs in an IDP, but there could also instead be CLIs, APIs, and other integration points between tools. It\\u2019s more about understanding the development process and adding the right automation in the right place at the right time.\"}),`\n`,(0,t.jsx)(e.p,{children:\"Now that we\\u2019ve covered the definition, let's talk about how the need for IDPs developed.\"}),`\n`,(0,t.jsxs)(e.h3,{id:\"how-the-need-for-idps-developed\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#how-the-need-for-idps-developed\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"How the Need for IDPs Developed\"]}),`\n`,(0,t.jsx)(e.p,{children:\"In the distant past, many developers handled both infrastructure and software. The environments were simpler and didn\\u2019t require a lot of specialization.\"}),`\n`,(0,t.jsx)(e.p,{children:\"Of course, the landscape changed. It was less about one C++ app running on a server somewhere and more about a collage of technologies working together. Tasks like deploying and operating the infrastructure moved to specialized operations or release management teams.\"}),`\n`,(0,t.jsxs)(e.p,{children:[\"Unfortunately, and typically due to a lack of automation and increased bureaucracy, the development teams felt held back, unable to get their changes into \",(0,t.jsx)(e.a,{href:\"https://release.com/blog/what-is-a-production-environment-and-why-is-it-unique\",children:\"the production environment\"}),\" quickly enough. And operations folks guarded their environment against changes in production to reduce incidents and outages. This took many forms, some more automated than others. As an example of less automation and more bureaucracy, some organizations required multiple requests, forms, or even Excel docs that specified what changes and what files were supposed to move to what environment. Even when there was more automation, the tools tended to be focused on the needs of administrators and not developers. There was a lot of friction and pain, and developers were not able to get their changes to production quickly.\"]}),`\n`,(0,t.jsx)(e.p,{children:\"From that pain, DevOps was born. In order to liberate developers and allow them to deploy more frequently, they were also given the keys to the CI/CD pipeline and entrusted to handle infrastructure and operations for their applications as well. This made sense in theory, but in practice, various interpretations of DevOps resulted in every single team in an organization implementing their own CI/CD pipeline, every team applying the same patches to their pipeline, and learning all the same lessons of proper CI/CD and operations as each other. What started as an exception became the norm. And once again, development teams across organizations spent countless hours on work that had very little to do with building new products and features.\"}),`\n`,(0,t.jsx)(e.p,{children:\"There has to be a better way. How can we take the learnings from DevOps, but also make the development life cycle simpler to manage from the development team\\u2019s perspective? That\\u2019s where IDPs can help.\"}),`\n`,(0,t.jsx)(e.p,{children:\"\\u200D\"}),`\n`,(0,t.jsx)(e.p,{children:(0,t.jsx)(e.img,{src:\"/blog-images/64c5e84ac5ebf4f4f9d37f4b24f256ac.png\",alt:\"\"})}),`\n`,(0,t.jsxs)(e.h3,{id:\"goalsoutcome-of-an-idp\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#goalsoutcome-of-an-idp\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Goals/Outcome of an IDP\"]}),`\n`,(0,t.jsx)(e.p,{children:\"When we begin to look at what an IDP can do for an organization, we typically try to meet one or more of the following goals:\"}),`\n`,(0,t.jsxs)(e.ol,{children:[`\n`,(0,t.jsx)(e.li,{children:\"Reduce toil\"}),`\n`,(0,t.jsx)(e.li,{children:\"Reduce context switching\"}),`\n`,(0,t.jsx)(e.li,{children:\"Allow for technical specialization\"}),`\n`,(0,t.jsx)(e.li,{children:\"Drive standardization\"}),`\n`,(0,t.jsx)(e.li,{children:\"Increase efficiency\"}),`\n`,(0,t.jsx)(e.li,{children:\"Improve developer experience\"}),`\n`,(0,t.jsx)(e.li,{children:\"Improve time to market\"}),`\n`]}),`\n`,(0,t.jsx)(e.p,{children:\"These goals all tie together well and overlap frequently.\"}),`\n`,(0,t.jsxs)(e.p,{children:[\"First, \",(0,t.jsx)(e.strong,{children:\"reducing toil\"}),\" involves reducing repetitive work that can be automated away. This could involve often-used tactics like automating CI/CD and tests. Reducing toil can also include automating dependency upgrades, audits, and access requests.\"]}),`\n`,(0,t.jsxs)(e.p,{children:[\"Second, an IDP should reduce \",(0,t.jsx)(e.strong,{children:\"context switching\"}),\". In part, when we reduce toil, we automatically reduce some amount of context switching. Additionally, with the appropriate automations and integrations, developers should not have to jump context between application development, environment management, infrastructure concerns, and whatever else is necessary.\"]}),`\n`,(0,t.jsxs)(e.p,{children:[\"A proper IDP will automate and standardize many of the tools and technologies required by the development team to get their features out to production, which then also allows them to \",(0,t.jsx)(e.strong,{children:\"specialize\"}),\" in their product domain and in software engineering skills.\"]}),`\n`,(0,t.jsxs)(e.p,{children:[\"Additionally, IDPs can drive \",(0,t.jsx)(e.strong,{children:\"standardization\"}),\" by providing out-of-the-box integration with static code analysis and enforced coding standards. And the teams themselves do not configure this each time for their new service or product. Instead, it\\u2019s baked in from the start and just works.\"]}),`\n`,(0,t.jsxs)(e.p,{children:[\"When you \",(0,t.jsx)(e.strong,{children:\"increase efficiency\"}),\", you\\u2019re making it easier for the team to focus on the job they do best\\u2014write software for their domain. They spend less time on work that doesn\\u2019t involve their craft or domain, making them more efficient overall. This, as well as previous points, also improves the \",(0,t.jsx)(e.strong,{children:\"developer experience.\"}),\" The development team has more time to focus on their craft and domain and worries less about the peripheral needs of their software stack.\"]}),`\n`,(0,t.jsxs)(e.p,{children:[\"And finally, all of these combined result in improving \",(0,t.jsx)(e.strong,{children:\"time to market\"}),\". When you reduce toil, reduce context switching, and increase efficiency, your development team can focus on building the product for your customer and not spend time on efforts that dilute their focus. They are able to prioritize the most important work with increased specialization for your target audience.\"]}),`\n`,(0,t.jsxs)(e.h3,{id:\"common-problems-idps-can-solve\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#common-problems-idps-can-solve\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Common Problems IDPs Can Solve\"]}),`\n`,(0,t.jsx)(e.p,{children:\"We\\u2019ve covered the definition, history, and goals of IDPs, but sometimes it\\u2019s still difficult to see how this applies to your organization. You need more to understand how an IDP can help you.\"}),`\n`,(0,t.jsx)(e.p,{children:\"Let\\u2019s look at common problems that IDPs can solve.\"}),`\n`,(0,t.jsx)(e.p,{children:\"First, let\\u2019s consider the attention sprawl and security risks example mentioned earlier. If teams have to manage their own infrastructure and CI/CD pipeline, they\\u2019re spending time on tasks that do not deliver working software to customers. More so, they are spending time doing tasks that they\\u2019re not always knowledgeable in. This can result in security vulnerabilities, like leaving certain ports and directories open or not utilizing proper deployment strategies and configuration management.\"}),`\n`,(0,t.jsx)(e.p,{children:\"Adding on to the security aspect, when teams manage their own infrastructure and CI/CD, they each must learn how to integrate your new security scanning software into their pipeline. At best, this will lead to repetitive work for each team that should have been centralized. At worst, teams may not configure the scans correctly, leaving you more vulnerable to security exploits.\"}),`\n`,(0,t.jsx)(e.p,{children:(0,t.jsx)(e.img,{src:\"/blog-images/67f6d49acf6a72ffec4120ab21f58391.png\",alt:\"\"})}),`\n`,(0,t.jsxs)(e.p,{children:[\"As another example, perhaps your teams struggle with environment setup. This can manifest as inconsistent environment setup or, more commonly, as a limitation in the number of environments available. Many teams are tied to a \",(0,t.jsx)(e.a,{href:\"https://release.com/blog/testing-environment-types-and-what-theyre-used-for\",children:\"small number of environments\"}),\"\\u2014staging, QA, UAT, and production, for example. It\\u2019s been historically difficult to replicate \",(0,t.jsx)(e.a,{href:\"https://release.com/blog/a-simple-guide-to-software-environments\",children:\"everything that goes into an environment\"}),\", and so organizations often stick to these static environments. This can cause \",(0,t.jsx)(e.a,{href:\"https://release.com/blog/increase-developer-velocity-by-removing-environment-bottlenecks\",children:\"bottlenecks\"}),\" for testing and deployments with multiple devs working in the same environment or waiting on others to complete their testing. With static environments like this, you may also end up with a need to do large data refreshes or resets periodically to clean up odd testing scenarios. With proper IDP integration, we could utilize tools like Release to \",(0,t.jsx)(e.a,{href:\"https://release.com/blog/improving-developer-productivity-with-ephemeral-environments\",children:\"provide ephemeral environments instead\"}),\".\"]}),`\n`,(0,t.jsx)(e.p,{children:\"For our final example, let\\u2019s talk about waiting on tickets. Perhaps your organization utilizes ticket queues and request systems. If your teams are often waiting on requests for infrastructure changes, blocking their ability to build or ship code, then you\\u2019ve found an opportunity for automation in your IDP.\"}),`\n`,(0,t.jsx)(e.p,{children:\"This is a short and general list. Wherever there\\u2019s friction in the development process, there\\u2019s opportunity to improve the process through an IDP.\"}),`\n`,(0,t.jsxs)(e.h3,{id:\"next-steps\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#next-steps\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Next Steps\"]}),`\n`,(0,t.jsx)(e.p,{children:\"At this point, you may think that an IDP will easily solve everyone\\u2019s problems. However, that\\u2019s not always the case. You need to carefully design your IDP to fit your specific needs, so that it can reduce toil, reduce context switching, improve the developer experience, and improve time to market. And if you don\\u2019t focus on the right aspects of your IDP, you could waste time and make the problems you\\u2019re trying to solve even worse.\"}),`\n`,(0,t.jsxs)(e.p,{children:[\"The investment in an IDP will require investigation, prioritization, and a product-driven perspective. Your IT will require a dedicated team, support, and governance. To begin, take a look at \",(0,t.jsx)(e.strong,{children:\"how\"}),\" your development teams work. What tools do they use? How often do they spend on tasks outside of their domain or primary technologies? Once you\\u2019ve assessed your starting point, you can begin to build the components of an IDP that will benefit your particular organization the most.\"]}),`\n`,(0,t.jsx)(e.p,{children:\"Stay tuned to learn more about the components of a successful IDP in the next chapter on this IDP series. \\xA0\"}),`\n`,(0,t.jsxs)(e.p,{children:[\"\\u200D\",(0,t.jsx)(e.em,{children:\"This post was written by Sylvia Fronczak.\"}),\" \",(0,t.jsx)(e.a,{href:\"https://sylviafronczak.com/\",children:(0,t.jsx)(e.em,{children:\"Sylvia\"})}),\" \",(0,t.jsx)(e.em,{children:\"is a software developer that has worked in various industries with various software methodologies. She\\u2019s currently focused on design practices that the whole team can own, understand, and evolve over time.\"})]})]})}function k(n={}){let{wrapper:e}=n.components||{};return e?(0,t.jsx)(e,Object.assign({},n,{children:(0,t.jsx)(h,n)})):h(n)}var I=k;return w(D);})();\n;return Component;"
        },
        "_id": "blog/posts/what-is-an-internal-developer-platform-and-why-should-i-have-one.mdx",
        "_raw": {
          "sourceFilePath": "blog/posts/what-is-an-internal-developer-platform-and-why-should-i-have-one.mdx",
          "sourceFileName": "what-is-an-internal-developer-platform-and-why-should-i-have-one.mdx",
          "sourceFileDir": "blog/posts",
          "contentType": "mdx",
          "flattenedPath": "blog/posts/what-is-an-internal-developer-platform-and-why-should-i-have-one"
        },
        "type": "BlogPost",
        "computedSlug": "what-is-an-internal-developer-platform-and-why-should-i-have-one"
      },
      "documentHash": "1739393595030",
      "hasWarnings": false,
      "documentTypeName": "BlogPost"
    },
    "blog/posts/what-is-gitops.mdx": {
      "document": {
        "title": "What is GitOps and how to get started with GitOps",
        "summary": "We recently joined a panel on Gitops. This blog will summarize the key nuggets and share tips on where to start",
        "publishDate": "Wed Jan 26 2022 02:47:33 GMT+0000 (Coordinated Universal Time)",
        "author": "regis-wilson",
        "readingTime": 7,
        "categories": [
          "platform-engineering",
          "product"
        ],
        "mainImage": "/blog-images/c3d3cfa735e4839ea2755b862e3e913f.jpg",
        "imageAlt": "GitOps Panel",
        "showCTA": true,
        "ctaCopy": "Unlock streamlined GitOps workflows with Release's ephemeral environments. Ensure consistent deployments and zero configuration drift.",
        "ctaLink": "https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=what-is-gitops",
        "relatedPosts": [
          ""
        ],
        "ogImage": "/blog-images/c3d3cfa735e4839ea2755b862e3e913f.jpg",
        "excerpt": "We recently joined a panel on Gitops. This blog will summarize the key nuggets and share tips on where to start",
        "tags": [
          "platform-engineering",
          "product"
        ],
        "ctaButton": "Try Release for Free",
        "body": {
          "raw": "\n**We recently joined Kong, Stackhawk, Fairwinds and Devops.com to discuss GitOps. You can click** [**here**](https://www.youtube.com/watch?v=m1VN-RjTHHo&feature=youtu.be) **to watch the panel discussion. If you prefer reading, we summarized some of the nuggets for you.**  \n**Does GitOps replace DevOps or is it just an opinionated instance using more declarative tools?**\n\nIn short, \n\n1.  GitOps is a theory, a way to operate: the philosophy that DevOps people use in response to the challenges of operating distributed applications.\n2.  GitOps = XaC (X=infrastructure, configuration or any dependencies) + DevOps\n\nMore in length, GitOps is a tool in the DevOps toolbox. If you want to stretch it a bit, you could say that GitOps is a philosophical bent of DevOps practitioners. The mantra for several years has been “X as code” where “X” could be infrastructure, configuration, policy, and so forth. The meaning of GitOps is usually understood as “check in X and Y happens”.\n\nUnfortunately, as is usually the case with such broad strokes, there are a few places where this falls down. For example, the most oft-cited example of Infrastructure as Code is usually declarative configuration files that are not code. These “Infrastructure as Code” imposters (to use a negative term) are actually static text files that are copy-pasted and littered about the repository as if they were code. Often, these configuration files are whole directories and subtrees full of hardcoded boilerplate that need to be copy-paste-edited.\n\nEven worse, these configuration files often get stale and freeze-dried into branches and can’t be merged due to overlapping changes or schema adaptations that lag behind current versions or upstream fixes.\n\nThe actual practice of GitOps should not be reduced to an overly simple saying like, “X as code,” but rather should be restated as “X in version control with good templating and deployment strategies”. What is the difference? Well, consider the usual case of configuration files sitting in a repository with names like “env.production”, “env.staging”, “env.qa”, and so on. Is this a good GitOps practice?\n\nWhat happens when we need a new environment like “env.qa2”, or even worse, “env.alice_dev”, “env.bob_dev”, “env.caden_dev”, etc.? Instead, you would prefer there be one, maybe two configurations with names like “production” and “testing” where “production” is instantiated many times as staging, qa1, qa2, and so on; and “testing” is instantiated N number of times by anyone and any environment that needs to be created. The “diff” between production and testing configurations should approach zero as time advances to infinity.\n\nThis is why GitOps should not be the only philosophy or tool in the DevOps practitioner’s toolbox. It should be balanced as well by the [twelve factors](https://12factor.net/) of good application development, including “one codebase, many deploys,” “store config in the environment,” and “keep dev, stage, and prod as similar as possible”.\n**Is GitOps it just about managing infrastructure as code or is that just one pain point?**\n\nGitOps is not just about infrastructure. It’s about all dependencies, internal and external. However, infrastructure is the main pain point that people address first.\n\n- More of a means to an end. IaaC gives you the option to automate your infrastructure. DevOps teams can then do whatever automation they need. \n- Leverage the GitFlow to apply to infrastructure. Rigor of dev process to your infrastructure methodology.\n- IaaC is the biggest and most. Don’t have data as code, security policy as code. As infra gets checked in, you’ll end up with needs outside of infra.\n\n‍**Is GitOps a response to continuous delivery obstacles?**\n\nGitOps is an extension of CD. Automating your release process requires many ingredients in place. One such example is automated testing. The same way continuous testing is an extension of CI, GitOps an extension of CD. Automating version control, code reviews, testing and many other aspects of CI/CD relay on the premise that you have the right environments available at the right time in the release cycle.\n\n- It is a response in part, and a necessary part. Because infra is needed, when you make a code change that needs an S3 bucket, you can define it in IaaC alongside your code. Making sure all of that works adds another layer of complexity.\n- It is part of the process to get to CD. Let’s get our config into Git. \n- You don’t have to do GitOps to get to CD. \n- It’s a philosophy… Other tools can be used: Ansible, Puppet, Chef.\n- Build additional tooling is really required (Release, ah hem)\n\nIt would almost be impossible to accomplish continuous delivery without something like GitOps.\n\nBy the way, CD is probably the most challenging part of GitOps and we’re still figuring it out. Testing code we’ve been doing for years. Testing infrastructure is more challenging.\n**How to get started with GitOps? Where do you learn and find the right communities?**\n\nThe two most common tools are [Argo](https://argoproj.github.io/cd/) and Flask. Each supports different applications. If you want to get your hands dirty, try them out and see which one better fits your needs. If you don’t have a Kubernetes cluster, [Release](https://release.com) is the place to go to get started.  \n‍  \nFor the full panel discussion click [here](https://www.youtube.com/watch?v=m1VN-RjTHHo&feature=youtu.be).\n\n### Additional Resources\n\n- [Increase Developer Velocity by Removing Environment Bottlenecks](https://release.com/blog/increase-developer-velocity-by-removing-environment-bottlenecks)\n- [What is an Ephemeral Environment?](https://release.com/ephemeral-environments)\n- [Using Release GitOps](https://docs.releasehub.com/reference-guide/gitops)\n",
          "code": "var Component=(()=>{var h=Object.create;var a=Object.defineProperty;var p=Object.getOwnPropertyDescriptor;var u=Object.getOwnPropertyNames;var g=Object.getPrototypeOf,m=Object.prototype.hasOwnProperty;var f=(n,e)=>()=>(e||n((e={exports:{}}).exports,e),e.exports),y=(n,e)=>{for(var o in e)a(n,o,{get:e[o],enumerable:!0})},r=(n,e,o,s)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let i of u(e))!m.call(n,i)&&i!==o&&a(n,i,{get:()=>e[i],enumerable:!(s=p(e,i))||s.enumerable});return n};var b=(n,e,o)=>(o=n!=null?h(g(n)):{},r(e||!n||!n.__esModule?a(o,\"default\",{value:n,enumerable:!0}):o,n)),w=n=>r(a({},\"__esModule\",{value:!0}),n);var c=f((x,l)=>{l.exports=_jsx_runtime});var k={};y(k,{default:()=>G,frontmatter:()=>v});var t=b(c()),v={title:\"What is GitOps and how to get started with GitOps\",summary:\"We recently joined a panel on Gitops. This blog will summarize the key nuggets and share tips on where to start\",publishDate:\"Wed Jan 26 2022 02:47:33 GMT+0000 (Coordinated Universal Time)\",author:\"regis-wilson\",readingTime:7,categories:[\"platform-engineering\",\"product\"],mainImage:\"/blog-images/c3d3cfa735e4839ea2755b862e3e913f.jpg\",imageAlt:\"GitOps Panel\",showCTA:!0,ctaCopy:\"Unlock streamlined GitOps workflows with Release's ephemeral environments. Ensure consistent deployments and zero configuration drift.\",ctaLink:\"https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=what-is-gitops\",relatedPosts:[\"\"],ogImage:\"/blog-images/c3d3cfa735e4839ea2755b862e3e913f.jpg\",excerpt:\"We recently joined a panel on Gitops. This blog will summarize the key nuggets and share tips on where to start\",tags:[\"platform-engineering\",\"product\"],ctaButton:\"Try Release for Free\"};function d(n){let e=Object.assign({p:\"p\",strong:\"strong\",a:\"a\",br:\"br\",ol:\"ol\",li:\"li\",ul:\"ul\",h3:\"h3\",span:\"span\"},n.components);return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:\"We recently joined Kong, Stackhawk, Fairwinds and Devops.com to discuss GitOps. You can click\"}),\" \",(0,t.jsx)(e.a,{href:\"https://www.youtube.com/watch?v=m1VN-RjTHHo&feature=youtu.be\",children:(0,t.jsx)(e.strong,{children:\"here\"})}),\" \",(0,t.jsx)(e.strong,{children:\"to watch the panel discussion. If you prefer reading, we summarized some of the nuggets for you.\"}),(0,t.jsx)(e.br,{}),`\n`,(0,t.jsx)(e.strong,{children:\"Does GitOps replace DevOps or is it just an opinionated instance using more declarative tools?\"})]}),`\n`,(0,t.jsx)(e.p,{children:\"In short,\\xA0\"}),`\n`,(0,t.jsxs)(e.ol,{children:[`\n`,(0,t.jsx)(e.li,{children:\"GitOps is a theory, a way to operate: the philosophy that DevOps people use in response to the challenges of operating distributed applications.\"}),`\n`,(0,t.jsx)(e.li,{children:\"GitOps = XaC (X=infrastructure, configuration or any dependencies) + DevOps\"}),`\n`]}),`\n`,(0,t.jsx)(e.p,{children:\"More in length, GitOps is a tool in the DevOps toolbox. If you want to stretch it a bit, you could say that GitOps is a philosophical bent of DevOps practitioners. The mantra for several years has been \\u201CX as code\\u201D where \\u201CX\\u201D could be infrastructure, configuration, policy, and so forth. The meaning of GitOps is usually understood as \\u201Ccheck in X and Y happens\\u201D.\"}),`\n`,(0,t.jsx)(e.p,{children:\"Unfortunately, as is usually the case with such broad strokes, there are a few places where this falls down. For example, the most oft-cited example of Infrastructure as Code is usually declarative configuration files that are not code. These \\u201CInfrastructure as Code\\u201D imposters (to use a negative term) are actually static text files that are copy-pasted and littered about the repository as if they were code. Often, these configuration files are whole directories and subtrees full of hardcoded boilerplate that need to be copy-paste-edited.\"}),`\n`,(0,t.jsx)(e.p,{children:\"Even worse, these configuration files often get stale and freeze-dried into branches and can\\u2019t be merged due to overlapping changes or schema adaptations that lag behind current versions or upstream fixes.\"}),`\n`,(0,t.jsx)(e.p,{children:\"The actual practice of GitOps should not be reduced to an overly simple saying like, \\u201CX as code,\\u201D but rather should be restated as \\u201CX in version control with good templating and deployment strategies\\u201D. What is the difference? Well, consider the usual case of configuration files sitting in a repository with names like \\u201Cenv.production\\u201D, \\u201Cenv.staging\\u201D, \\u201Cenv.qa\\u201D, and so on. Is this a good GitOps practice?\"}),`\n`,(0,t.jsx)(e.p,{children:\"What happens when we need a new environment like \\u201Cenv.qa2\\u201D, or even worse, \\u201Cenv.alice_dev\\u201D, \\u201Cenv.bob_dev\\u201D, \\u201Cenv.caden_dev\\u201D, etc.? Instead, you would prefer there be one, maybe two configurations with names like \\u201Cproduction\\u201D and \\u201Ctesting\\u201D where \\u201Cproduction\\u201D is instantiated many times as staging, qa1, qa2, and so on; and \\u201Ctesting\\u201D is instantiated N number of times by anyone and any environment that needs to be created. The \\u201Cdiff\\u201D between production and testing configurations should approach zero as time advances to infinity.\"}),`\n`,(0,t.jsxs)(e.p,{children:[\"This is why GitOps should not be the only philosophy or tool in the DevOps practitioner\\u2019s toolbox. It should be balanced as well by the \",(0,t.jsx)(e.a,{href:\"https://12factor.net/\",children:\"twelve factors\"}),` of good application development, including \\u201Cone codebase, many deploys,\\u201D \\u201Cstore config in the environment,\\u201D and \\u201Ckeep dev, stage, and prod as similar as possible\\u201D.\n`,(0,t.jsx)(e.strong,{children:\"Is GitOps it just about managing infrastructure as code or is that just one pain point?\"})]}),`\n`,(0,t.jsx)(e.p,{children:\"GitOps is not just about infrastructure. It\\u2019s about all dependencies, internal and external. However, infrastructure is the main pain point that people address first.\"}),`\n`,(0,t.jsxs)(e.ul,{children:[`\n`,(0,t.jsx)(e.li,{children:\"More of a means to an end. IaaC gives you the option to automate your infrastructure. DevOps teams can then do whatever automation they need.\\xA0\"}),`\n`,(0,t.jsx)(e.li,{children:\"Leverage the GitFlow to apply to infrastructure. Rigor of dev process to your infrastructure methodology.\"}),`\n`,(0,t.jsx)(e.li,{children:\"IaaC is the biggest and most. Don\\u2019t have data as code, security policy as code. As infra gets checked in, you\\u2019ll end up with needs outside of infra.\"}),`\n`]}),`\n`,(0,t.jsxs)(e.p,{children:[\"\\u200D\",(0,t.jsx)(e.strong,{children:\"Is GitOps a response to continuous delivery obstacles?\"})]}),`\n`,(0,t.jsx)(e.p,{children:\"GitOps is an extension of CD. Automating your release process requires many ingredients in place. One such example is automated testing. The same way continuous testing is an extension of CI, GitOps an extension of CD. Automating version control, code reviews, testing and many other aspects of CI/CD relay on the premise that you have the right environments available at the right time in the release cycle.\"}),`\n`,(0,t.jsxs)(e.ul,{children:[`\n`,(0,t.jsx)(e.li,{children:\"It is a response in part, and a necessary part. Because infra is needed, when you make a code change that needs an S3 bucket, you can define it in IaaC alongside your code. Making sure all of that works adds another layer of complexity.\"}),`\n`,(0,t.jsx)(e.li,{children:\"It is part of the process to get to CD. Let\\u2019s get our config into Git.\\xA0\"}),`\n`,(0,t.jsx)(e.li,{children:\"You don\\u2019t have to do GitOps to get to CD.\\xA0\"}),`\n`,(0,t.jsx)(e.li,{children:\"It\\u2019s a philosophy\\u2026 Other tools can be used: Ansible, Puppet, Chef.\"}),`\n`,(0,t.jsx)(e.li,{children:\"Build additional tooling is really required (Release, ah hem)\"}),`\n`]}),`\n`,(0,t.jsx)(e.p,{children:\"It would almost be impossible to accomplish continuous delivery without something like GitOps.\"}),`\n`,(0,t.jsxs)(e.p,{children:[`By the way, CD is probably the most challenging part of GitOps and we\\u2019re still figuring it out. Testing code we\\u2019ve been doing for years. Testing infrastructure is more challenging.\n`,(0,t.jsx)(e.strong,{children:\"How to get started with GitOps? Where do you learn and find the right communities?\"})]}),`\n`,(0,t.jsxs)(e.p,{children:[\"The two most common tools are \",(0,t.jsx)(e.a,{href:\"https://argoproj.github.io/cd/\",children:\"Argo\"}),\" and Flask. Each supports different applications. If you want to get your hands dirty, try them out and see which one better fits your needs. If you don\\u2019t have a Kubernetes cluster, \",(0,t.jsx)(e.a,{href:\"https://release.com\",children:\"Release\"}),\" is the place to go to get started.\",(0,t.jsx)(e.br,{}),`\n`,\"\\u200D\",(0,t.jsx)(e.br,{}),`\n`,\"For the full panel discussion click \",(0,t.jsx)(e.a,{href:\"https://www.youtube.com/watch?v=m1VN-RjTHHo&feature=youtu.be\",children:\"here\"}),\".\"]}),`\n`,(0,t.jsxs)(e.h3,{id:\"additional-resources\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#additional-resources\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Additional Resources\"]}),`\n`,(0,t.jsxs)(e.ul,{children:[`\n`,(0,t.jsx)(e.li,{children:(0,t.jsx)(e.a,{href:\"https://release.com/blog/increase-developer-velocity-by-removing-environment-bottlenecks\",children:\"Increase Developer Velocity by Removing Environment Bottlenecks\"})}),`\n`,(0,t.jsx)(e.li,{children:(0,t.jsx)(e.a,{href:\"https://release.com/ephemeral-environments\",children:\"What is an Ephemeral Environment?\"})}),`\n`,(0,t.jsx)(e.li,{children:(0,t.jsx)(e.a,{href:\"https://docs.releasehub.com/reference-guide/gitops\",children:\"Using Release GitOps\"})}),`\n`]})]})}function O(n={}){let{wrapper:e}=n.components||{};return e?(0,t.jsx)(e,Object.assign({},n,{children:(0,t.jsx)(d,n)})):d(n)}var G=O;return w(k);})();\n;return Component;"
        },
        "_id": "blog/posts/what-is-gitops.mdx",
        "_raw": {
          "sourceFilePath": "blog/posts/what-is-gitops.mdx",
          "sourceFileName": "what-is-gitops.mdx",
          "sourceFileDir": "blog/posts",
          "contentType": "mdx",
          "flattenedPath": "blog/posts/what-is-gitops"
        },
        "type": "BlogPost",
        "computedSlug": "what-is-gitops"
      },
      "documentHash": "1739393595030",
      "hasWarnings": false,
      "documentTypeName": "BlogPost"
    },
    "blog/posts/why-i-joined-release-bryce-fehmel.mdx": {
      "document": {
        "title": "Why I Joined Release: Bryce Fehmel",
        "summary": "Bryce Fehmel shares why he is excited to join the team at Release",
        "publishDate": "Thu Aug 25 2022 19:50:03 GMT+0000 (Coordinated Universal Time)",
        "author": "bryce",
        "readingTime": 2,
        "categories": [
          "platform-engineering",
          "product"
        ],
        "mainImage": "/blog-images/e84c81f1030dc3e0429892a5d20ca0c8.jpg",
        "imageAlt": "Why I Joined Release: Bryce Fehmel",
        "showCTA": true,
        "ctaCopy": "Automate your development workflows like Bryce's transition to tech with Release's ephemeral environments for faster testing and collaboration.",
        "ctaLink": "https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=why-i-joined-release-bryce-fehmel",
        "relatedPosts": [
          ""
        ],
        "ogImage": "/blog-images/e84c81f1030dc3e0429892a5d20ca0c8.jpg",
        "excerpt": "Bryce Fehmel shares why he is excited to join the team at Release",
        "tags": [
          "platform-engineering",
          "product"
        ],
        "ctaButton": "Try Release for Free",
        "body": {
          "raw": "\nMy name is Bryce Fehmel, a Sales Development Representative with Release. Contrary to many of my colleagues, I have spent the entirety of my career playing professional baseball. I grew up dreaming of playing Major League Baseball, but realized how unlikely it would be after multiple injuries. I began to take an interest in the analytics side of baseball, and how those analytics have an impact on each game and years to come for the organization.\n\nAfter my collegiate career and being drafted in the 21st round in 2019, I began my professional career in Arizona where I had enough success to be promoted to what was known as short season (Single A), in Salem, Oregon. During Spring Training of 2020, I found out I had torn a ligament in my elbow which would require Tommy John surgery, a procedure that would take a year to get back on the mound. Throughout the rehab process, I recognized that the ligament in my elbow still had not healed. After months of rehab with physical therapists, strength coaches and many more appointments with specialty doctors, I was told that the same ligament had torn once again. I quickly realized the chances of my dreams of playing in the Major Leagues had become significantly more slim than when I was first drafted.\n\nAfter recovery from the second surgery, I considered my possible options due to the amount of time I spent away from playing and the hours of rehab that would continue to be required.. There were long and difficult conversations about my potential future in baseball and what could be next. After attending Spring Training in 2022 and finally pitching in a game for the first time in 2+ years, it was clear that my injury would cause a third consecutive missed season. In that moment, I decided to retire gracefully, with my head held high leaving the professional baseball world with nothing but love and excitement for anyone else who has the opportunity to chase their dreams.\n\nThat is where Release and CEO of the company, Tommy McClung come into the picture. Getting to know Tommy, a former Oregon State alumni himself, and his children over the years through their baseball experience, I gained an immense amount of curiosity around the tech industry and different positions where I could bring my expertise. As I step into a new journey with Release, I am eager to learn and grow in the software industry to make an impact in a variety of different ways. I look forward to the opportunity of helping others reach their goals and to bring their ideas to life faster and more freely. Most importantly, I am excited to work with my teammates to carry out the company mission of allowing the best ideas to emerge through efficiency and collaboration.\n\nRelease is a complex software, in its own way, similar to the game of baseball. To be able to use Release and fully automate development more efficiently with code testing and ephemeral environments, there are many pieces that tie together. Similarly, baseball takes a group of people to come together, (even further than the players on the field) to create a winning team that remains memorable for years to come. As winning teams are memorable, so are features that end users benefit from to drive revenue and company growth. With Release, it is the easiest way to develop, manage and deploy full-stack complex environments on-demand.\n",
          "code": "var Component=(()=>{var c=Object.create;var r=Object.defineProperty;var d=Object.getOwnPropertyDescriptor;var g=Object.getOwnPropertyNames;var u=Object.getPrototypeOf,y=Object.prototype.hasOwnProperty;var f=(t,e)=>()=>(e||t((e={exports:{}}).exports,e),e.exports),p=(t,e)=>{for(var n in e)r(t,n,{get:e[n],enumerable:!0})},s=(t,e,n,i)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let o of g(e))!y.call(t,o)&&o!==n&&r(t,o,{get:()=>e[o],enumerable:!(i=d(e,o))||i.enumerable});return t};var w=(t,e,n)=>(n=t!=null?c(u(t)):{},s(e||!t||!t.__esModule?r(n,\"default\",{value:t,enumerable:!0}):n,t)),b=t=>s(r({},\"__esModule\",{value:!0}),t);var h=f((T,l)=>{l.exports=_jsx_runtime});var j={};p(j,{default:()=>x,frontmatter:()=>I});var a=w(h()),I={title:\"Why I Joined Release: Bryce Fehmel\",summary:\"Bryce Fehmel shares why he is excited to join the team at Release\",publishDate:\"Thu Aug 25 2022 19:50:03 GMT+0000 (Coordinated Universal Time)\",author:\"bryce\",readingTime:2,categories:[\"platform-engineering\",\"product\"],mainImage:\"/blog-images/e84c81f1030dc3e0429892a5d20ca0c8.jpg\",imageAlt:\"Why I Joined Release: Bryce Fehmel\",showCTA:!0,ctaCopy:\"Automate your development workflows like Bryce's transition to tech with Release's ephemeral environments for faster testing and collaboration.\",ctaLink:\"https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=why-i-joined-release-bryce-fehmel\",relatedPosts:[\"\"],ogImage:\"/blog-images/e84c81f1030dc3e0429892a5d20ca0c8.jpg\",excerpt:\"Bryce Fehmel shares why he is excited to join the team at Release\",tags:[\"platform-engineering\",\"product\"],ctaButton:\"Try Release for Free\"};function m(t){let e=Object.assign({p:\"p\"},t.components);return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(e.p,{children:\"My name is Bryce Fehmel, a Sales Development Representative with Release. Contrary to many of my colleagues, I have spent the entirety of my career playing professional baseball. I grew up dreaming of playing Major League Baseball, but realized how unlikely it would be after multiple injuries. I began to take an interest in the analytics side of baseball, and how those analytics have an impact on each game and years to come for the organization.\"}),`\n`,(0,a.jsx)(e.p,{children:\"After my collegiate career and being drafted in the 21st round in 2019, I began my professional career in Arizona where I had enough success to be promoted to what was known as short season (Single A), in Salem, Oregon. During Spring Training of 2020, I found out I had torn a ligament in my elbow which would require Tommy John surgery, a procedure that would take a year to get back on the mound. Throughout the rehab process, I recognized that the ligament in my elbow still had not healed. After months of rehab with physical therapists, strength coaches and many more appointments with specialty doctors, I was told that the same ligament had torn once again. I quickly realized the chances of my dreams of playing in the Major Leagues had become significantly more slim than when I was first drafted.\"}),`\n`,(0,a.jsx)(e.p,{children:\"After recovery from the second surgery, I considered my possible options due to the amount of time I spent away from playing and the hours of rehab that would continue to be required.. There were long and difficult conversations about my potential future in baseball and what could be next. After attending Spring Training in 2022 and finally pitching in a game for the first time in 2+ years, it was clear that my injury would cause a third consecutive missed season. In that moment, I decided to retire gracefully, with my head held high leaving the professional baseball world with nothing but love and excitement for anyone else who has the opportunity to chase their dreams.\"}),`\n`,(0,a.jsx)(e.p,{children:\"That is where Release and CEO of the company, Tommy McClung come into the picture. Getting to know Tommy, a former Oregon State alumni himself, and his children over the years through their baseball experience, I gained an immense amount of curiosity around the tech industry and different positions where I could bring my expertise. As I step into a new journey with Release, I am eager to learn and grow in the software industry to make an impact in a variety of different ways. I look forward to the opportunity of helping others reach their goals and to bring their ideas to life faster and more freely. Most importantly, I am excited to work with my teammates to carry out the company mission of allowing the best ideas to emerge through efficiency and collaboration.\"}),`\n`,(0,a.jsx)(e.p,{children:\"Release is a complex software, in its own way, similar to the game of baseball. To be able to use Release and fully automate development more efficiently with code testing and ephemeral environments, there are many pieces that tie together. Similarly, baseball takes a group of people to come together, (even further than the players on the field) to create a winning team that remains memorable for years to come. As winning teams are memorable, so are features that end users benefit from to drive revenue and company growth. With Release, it is the easiest way to develop, manage and deploy full-stack complex environments on-demand.\"})]})}function v(t={}){let{wrapper:e}=t.components||{};return e?(0,a.jsx)(e,Object.assign({},t,{children:(0,a.jsx)(m,t)})):m(t)}var x=v;return b(j);})();\n;return Component;"
        },
        "_id": "blog/posts/why-i-joined-release-bryce-fehmel.mdx",
        "_raw": {
          "sourceFilePath": "blog/posts/why-i-joined-release-bryce-fehmel.mdx",
          "sourceFileName": "why-i-joined-release-bryce-fehmel.mdx",
          "sourceFileDir": "blog/posts",
          "contentType": "mdx",
          "flattenedPath": "blog/posts/why-i-joined-release-bryce-fehmel"
        },
        "type": "BlogPost",
        "computedSlug": "why-i-joined-release-bryce-fehmel"
      },
      "documentHash": "1739393595030",
      "hasWarnings": false,
      "documentTypeName": "BlogPost"
    },
    "blog/posts/why-i-joined-release-matt-carter.mdx": {
      "document": {
        "title": "Why I Joined Release: Matt Carter",
        "summary": "Read Matt's story on why he joined Release as Chief Marketing Officer and learn about his experience.",
        "publishDate": "Sun Oct 23 2022 14:53:34 GMT+0000 (Coordinated Universal Time)",
        "author": "matt-carter",
        "readingTime": 2,
        "categories": [
          "platform-engineering",
          "product"
        ],
        "mainImage": "/blog-images/7dd4e287eb56c9854d414e79aa9b6008.jpg",
        "imageAlt": "Why I Joined Release: Matt Carter",
        "showCTA": true,
        "ctaCopy": "Simplify testing and deployment with Release's automated environments. Speed up feedback loops and enhance innovation velocity.",
        "ctaLink": "https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=why-i-joined-release-matt-carter",
        "relatedPosts": [
          ""
        ],
        "ogImage": "/blog-images/7dd4e287eb56c9854d414e79aa9b6008.jpg",
        "excerpt": "Read Matt's story on why he joined Release as Chief Marketing Officer and learn about his experience.",
        "tags": [
          "platform-engineering",
          "product"
        ],
        "ctaButton": "Try Release for Free",
        "body": {
          "raw": "\nEarlier this month, I joined the team at Release to build out their marketing team as part of the company’s next chapter of growth. After spending time with the Release leadership team, Tommy, Erik, David, and Kelsey, I could see my values and vision were clearly aligned with a group who understood the needs of developers and app dev teams, saw the creation of the Environments as a Service (EaaS) category will help teams go faster with better results, and most importantly wanted to build a company where people could do their best work and thrive.\n\nDeveloper obsession\n\nFor most of my career, I’ve been fortunate to build products and communities that help developers get their jobs done better and faster. Microsoft’s Visual Studio, Chef, and Docker all revolutionized how millions of developers could use technology as a force multiplier to not only innovate faster but get greater joy from their craft. The work Tommy, Erik, and David have done to date at Release comes directly from their experiences building startups and large company IT systems. Release came directly from their frustrations and aspirations–the chance to take a product with this vision in mind and help it grow is truly exciting, and matters in a meaningful way to devs. The Release developer experience creates the “flow” developers love that is the result of this deep understanding and real-life experience of the founders.\n\nGreat developer experiences are just the beginning. As I looked more closely at what was possible through the Release vision, the power of EaaS became apparent. Apps are getting more complicated every day. Multiple dependencies, microservices, massive datasets, and different deployment options make rich testing more complicated. With EaaS, automated immediate environments are available to dev and ops teams throughout the software development lifecycle. This week’s announcement of Remote Development Environments let developers use the power of automated environments to get instant feedback from the earliest stages of coding. And as apps get ready to move to testing and review, Release automatically creates ephemeral or permanent environments with every code pull request, complete with dependencies, namespaces, and data sources in your cloud environment. This eliminates the need for teams to build bespoke solutions and for devs to wait in line for their turn to deploy. This gives every team the chance to release as quickly as desired. More features delivered faster equals better customer experiences and improved innovation velocity.\n\nFinally, the people at Release and the commitment to deliver tools to solve tangible problems for dev teams made this decision easy. From the founders and investors to each individual on the team, every individual is committed to shaping a company that delivers great tools and delightful experiences. I’ve been lucky to be part of teams building awesome companies in the past, and I see in Release a chance to help build another great organization. The opportunity, products, and people Tommy Erik, and David have brought together create the chance to make a big difference to app development teams. I’m excited to be along for the ride!\n",
          "code": "var Component=(()=>{var c=Object.create;var i=Object.defineProperty;var h=Object.getOwnPropertyDescriptor;var p=Object.getOwnPropertyNames;var u=Object.getPrototypeOf,g=Object.prototype.hasOwnProperty;var f=(t,e)=>()=>(e||t((e={exports:{}}).exports,e),e.exports),v=(t,e)=>{for(var o in e)i(t,o,{get:e[o],enumerable:!0})},s=(t,e,o,r)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let n of p(e))!g.call(t,n)&&n!==o&&i(t,n,{get:()=>e[n],enumerable:!(r=h(e,n))||r.enumerable});return t};var y=(t,e,o)=>(o=t!=null?c(u(t)):{},s(e||!t||!t.__esModule?i(o,\"default\",{value:t,enumerable:!0}):o,t)),b=t=>s(i({},\"__esModule\",{value:!0}),t);var l=f((T,d)=>{d.exports=_jsx_runtime});var R={};v(R,{default:()=>k,frontmatter:()=>w});var a=y(l()),w={title:\"Why I Joined Release: Matt Carter\",summary:\"Read Matt's story on why he joined Release as Chief Marketing Officer and learn about his experience.\",publishDate:\"Sun Oct 23 2022 14:53:34 GMT+0000 (Coordinated Universal Time)\",author:\"matt-carter\",readingTime:2,categories:[\"platform-engineering\",\"product\"],mainImage:\"/blog-images/7dd4e287eb56c9854d414e79aa9b6008.jpg\",imageAlt:\"Why I Joined Release: Matt Carter\",showCTA:!0,ctaCopy:\"Simplify testing and deployment with Release's automated environments. Speed up feedback loops and enhance innovation velocity.\",ctaLink:\"https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=why-i-joined-release-matt-carter\",relatedPosts:[\"\"],ogImage:\"/blog-images/7dd4e287eb56c9854d414e79aa9b6008.jpg\",excerpt:\"Read Matt's story on why he joined Release as Chief Marketing Officer and learn about his experience.\",tags:[\"platform-engineering\",\"product\"],ctaButton:\"Try Release for Free\"};function m(t){let e=Object.assign({p:\"p\"},t.components);return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(e.p,{children:\"Earlier this month, I joined the team at Release to build out their marketing team as part of the company\\u2019s next chapter of growth. After spending time with the Release leadership team, Tommy, Erik, David, and Kelsey, I could see my values and vision were clearly aligned with a group who understood the needs of developers and app dev teams, saw the creation of the Environments as a Service (EaaS) category will help teams go faster with better results, and most importantly wanted to build a company where people could do their best work and thrive.\"}),`\n`,(0,a.jsx)(e.p,{children:\"Developer obsession\"}),`\n`,(0,a.jsx)(e.p,{children:\"For most of my career, I\\u2019ve been fortunate to build products and communities that help developers get their jobs done better and faster. Microsoft\\u2019s Visual Studio, Chef, and Docker all revolutionized how millions of developers could use technology as a force multiplier to not only innovate faster but get greater joy from their craft. The work Tommy, Erik, and David have done to date at Release comes directly from their experiences building startups and large company IT systems. Release came directly from their frustrations and aspirations\\u2013the chance to take a product with this vision in mind and help it grow is truly exciting, and matters in a meaningful way to devs. The Release developer experience creates the \\u201Cflow\\u201D developers love that is the result of this deep understanding and real-life experience of the founders.\"}),`\n`,(0,a.jsx)(e.p,{children:\"Great developer experiences are just the beginning. As I looked more closely at what was possible through the Release vision, the power of EaaS became apparent. Apps are getting more complicated every day. Multiple dependencies, microservices, massive datasets, and different deployment options make rich testing more complicated. With EaaS, automated immediate environments are available to dev and ops teams throughout the software development lifecycle. This week\\u2019s announcement of Remote Development Environments let developers use the power of automated environments to get instant feedback from the earliest stages of coding. And as apps get ready to move to testing and review, Release automatically creates ephemeral or permanent environments with every code pull request, complete with dependencies, namespaces, and data sources in your cloud environment. This eliminates the need for teams to build bespoke solutions and for devs to wait in line for their turn to deploy. This gives every team the chance to release as quickly as desired. More features delivered faster equals better customer experiences and improved innovation velocity.\"}),`\n`,(0,a.jsx)(e.p,{children:\"Finally, the people at Release and the commitment to deliver tools to solve tangible problems for dev teams made this decision easy. From the founders and investors to each individual on the team, every individual is committed to shaping a company that delivers great tools and delightful experiences. I\\u2019ve been lucky to be part of teams building awesome companies in the past, and I see in Release a chance to help build another great organization. The opportunity, products, and people Tommy Erik, and David have brought together create the chance to make a big difference to app development teams. I\\u2019m excited to be along for the ride!\"})]})}function x(t={}){let{wrapper:e}=t.components||{};return e?(0,a.jsx)(e,Object.assign({},t,{children:(0,a.jsx)(m,t)})):m(t)}var k=x;return b(R);})();\n;return Component;"
        },
        "_id": "blog/posts/why-i-joined-release-matt-carter.mdx",
        "_raw": {
          "sourceFilePath": "blog/posts/why-i-joined-release-matt-carter.mdx",
          "sourceFileName": "why-i-joined-release-matt-carter.mdx",
          "sourceFileDir": "blog/posts",
          "contentType": "mdx",
          "flattenedPath": "blog/posts/why-i-joined-release-matt-carter"
        },
        "type": "BlogPost",
        "computedSlug": "why-i-joined-release-matt-carter"
      },
      "documentHash": "1739393595031",
      "hasWarnings": false,
      "documentTypeName": "BlogPost"
    },
    "blog/posts/why-i-joined-release-matt-riley.mdx": {
      "document": {
        "title": "Why I Joined Release",
        "summary": "Matt Riley joins the Release team and shares his enthusiasm about \"Environments-as-a-Service\"",
        "publishDate": "Wed Mar 23 2022 14:24:34 GMT+0000 (Coordinated Universal Time)",
        "author": "matt-riley",
        "readingTime": 2,
        "categories": [
          "platform-engineering",
          "product"
        ],
        "mainImage": "/blog-images/0ac1cd96fcaf7a09644adf07134f09d2.jpg",
        "imageAlt": "Why I Joined Release",
        "showCTA": true,
        "ctaCopy": "Simplify environment setup, focus on revenue-generating code. Release's EaaS automates infrastructure for faster development.",
        "ctaLink": "https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=why-i-joined-release-matt-riley",
        "relatedPosts": [
          ""
        ],
        "ogImage": "/blog-images/0ac1cd96fcaf7a09644adf07134f09d2.jpg",
        "excerpt": "Matt Riley joins the Release team and shares his enthusiasm about \"Environments-as-a-Service\"",
        "tags": [
          "platform-engineering",
          "product"
        ],
        "ctaButton": "Try Release for Free",
        "body": {
          "raw": "\nWhen learning about Release, I immediately recognized the magnitude of the problem EaaS solves. My 9-year professional career in technical program management, business development, and account management has been centered around helping customers answer \"how can we streamline processes\" and \"how can we create better experiences for customers.\" Release’s EaaS solution answered both of these and ignited my passion to connect this tool with DevOps leaders to bring their ideas to the world faster.\n\nThe goal of DevOps is to accelerate the process of application production. This involves increasing innovation, collaboration, improving quality, and being more responsive to business needs through rapid delivery and continuity between software developers and operations. Ultimately, this allows businesses to be more consistent with enhancing their competitive position and customer demand. Automation plays a critical role in accomplishing these goals by saving time and increasing velocity. Today, DevOps teams are automating infrastructure, code testing, and workflows. However, conventional environments for development, testing, staging, and production are difficult to manage. Engineering resources are distracted from the core product offering due to efforts required to create, maintain, and administrate environments. These efforts are undifferentiated, including configuring VPCs and networking, load balancing, creating Kubernetes manifests, and building end-to-end pipelines to turn your code into a running application in your environment. Valuable engineering resources should be focused on revenue-generating code, which Release is here to solve.\n\nRelease is an out-of-the-box automation tool to spin up even the most complicated environments to enable software development uniformity and velocity. EaaS can be viewed as an automated template or package of all infrastructure as code and configuration settings to minimize prep work so software engineers can focus on their code. Outside of the common shared environment types (staging, QA, etc.), Release offers ephemeral environments. These are short-lived production replications where you can input a feature to receive instant results and share with stakeholders without disrupting other environments. No more waiting in line for a shared environment, either.\n\nEnvironments provide customers: \n\n- **Faster time-to-market:** Improve product velocity with the ability to parallelize development. \n- **Quality:** Enables less rework by shifting testing closer to the developer, allowing bugs to be caught earlier before they hit production.\n- **Innovation:** Eliminates internal friction and bottlenecks to easily implement team members' ideas.\n- **Collaboration:** Enables real-time visualization and sharing of code changes. \n- **Flexibility:** Automatically generate ephemeral, development, testing, staging, and production environments on-demand. Spin them down just as quickly and only pay for what you use. \n- **Scalability:** Scale faster by using EaaS to automatically configure Infrastructure as Code, VPC, DNS, load balancing, integration pipelines, and more. \n\nIf you’re in Development or DevOps and would like to learn more, please reach out directly ([matthew@release.com](mailto:matthew@release.com)).\n",
          "code": "var Component=(()=>{var m=Object.create;var o=Object.defineProperty;var u=Object.getOwnPropertyDescriptor;var h=Object.getOwnPropertyNames;var g=Object.getPrototypeOf,p=Object.prototype.hasOwnProperty;var f=(t,e)=>()=>(e||t((e={exports:{}}).exports,e),e.exports),v=(t,e)=>{for(var a in e)o(t,a,{get:e[a],enumerable:!0})},s=(t,e,a,r)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let i of h(e))!p.call(t,i)&&i!==a&&o(t,i,{get:()=>e[i],enumerable:!(r=u(e,i))||r.enumerable});return t};var y=(t,e,a)=>(a=t!=null?m(g(t)):{},s(e||!t||!t.__esModule?o(a,\"default\",{value:t,enumerable:!0}):a,t)),b=t=>s(o({},\"__esModule\",{value:!0}),t);var c=f((S,l)=>{l.exports=_jsx_runtime});var R={};v(R,{default:()=>x,frontmatter:()=>w});var n=y(c()),w={title:\"Why I Joined Release\",summary:'Matt Riley joins the Release team and shares his enthusiasm about \"Environments-as-a-Service\"',publishDate:\"Wed Mar 23 2022 14:24:34 GMT+0000 (Coordinated Universal Time)\",author:\"matt-riley\",readingTime:2,categories:[\"platform-engineering\",\"product\"],mainImage:\"/blog-images/0ac1cd96fcaf7a09644adf07134f09d2.jpg\",imageAlt:\"Why I Joined Release\",showCTA:!0,ctaCopy:\"Simplify environment setup, focus on revenue-generating code. Release's EaaS automates infrastructure for faster development.\",ctaLink:\"https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=why-i-joined-release-matt-riley\",relatedPosts:[\"\"],ogImage:\"/blog-images/0ac1cd96fcaf7a09644adf07134f09d2.jpg\",excerpt:'Matt Riley joins the Release team and shares his enthusiasm about \"Environments-as-a-Service\"',tags:[\"platform-engineering\",\"product\"],ctaButton:\"Try Release for Free\"};function d(t){let e=Object.assign({p:\"p\",ul:\"ul\",li:\"li\",strong:\"strong\",a:\"a\"},t.components);return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(e.p,{children:'When learning about Release, I immediately recognized the magnitude of the problem EaaS solves. My 9-year professional career in technical program management, business development, and account management has been centered around helping customers answer \"how can we streamline processes\" and \"how can we create better experiences for customers.\" Release\\u2019s EaaS solution answered both of these and ignited my passion to connect this tool with DevOps leaders to bring their ideas to the world faster.'}),`\n`,(0,n.jsx)(e.p,{children:\"The goal of DevOps is to accelerate the process of application production. This involves increasing innovation, collaboration, improving quality, and being more responsive to business needs through rapid delivery and continuity between software developers and operations. Ultimately, this allows businesses to be more consistent with enhancing their competitive position and customer demand. Automation plays a critical role in accomplishing these goals by saving time and increasing velocity. Today, DevOps teams are automating infrastructure, code testing, and workflows. However, conventional environments for development, testing, staging, and production are difficult to manage. Engineering resources are distracted from the core product offering due to efforts required to create, maintain, and administrate environments. These efforts are undifferentiated, including configuring VPCs and networking, load balancing, creating Kubernetes manifests, and building end-to-end pipelines to turn your code into a running application in your environment. Valuable engineering resources should be focused on revenue-generating code, which Release is here to solve.\"}),`\n`,(0,n.jsx)(e.p,{children:\"Release is an out-of-the-box automation tool to spin up even the most complicated environments to enable software development uniformity and velocity. EaaS can be viewed as an automated template or package of all infrastructure as code and configuration settings to minimize prep work so software engineers can focus on their code. Outside of the common shared environment types (staging, QA, etc.), Release offers ephemeral environments. These are short-lived production replications where you can input a feature to receive instant results and share with stakeholders without disrupting other environments. No more waiting in line for a shared environment, either.\"}),`\n`,(0,n.jsx)(e.p,{children:\"Environments provide customers:\\xA0\"}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"Faster time-to-market:\"}),\" Improve product velocity with the ability to parallelize development.\\xA0\"]}),`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"Quality:\"}),\" Enables less rework by shifting testing closer to the developer, allowing bugs to be caught earlier before they hit production.\"]}),`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"Innovation:\"}),\" Eliminates internal friction and bottlenecks to easily implement team members' ideas.\"]}),`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"Collaboration:\"}),\" Enables real-time visualization and sharing of code changes.\\xA0\"]}),`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"Flexibility:\"}),\" Automatically generate ephemeral, development, testing, staging, and production environments on-demand. Spin them down just as quickly and only pay for what you use.\\xA0\"]}),`\n`,(0,n.jsxs)(e.li,{children:[(0,n.jsx)(e.strong,{children:\"Scalability:\"}),\" Scale faster by using EaaS to automatically configure Infrastructure as Code, VPC, DNS, load balancing, integration pipelines, and more.\\xA0\"]}),`\n`]}),`\n`,(0,n.jsxs)(e.p,{children:[\"If you\\u2019re in Development or DevOps and would like to learn more, please reach out directly (\",(0,n.jsx)(e.a,{href:\"mailto:matthew@release.com\",children:\"matthew@release.com\"}),\").\"]})]})}function j(t={}){let{wrapper:e}=t.components||{};return e?(0,n.jsx)(e,Object.assign({},t,{children:(0,n.jsx)(d,t)})):d(t)}var x=j;return b(R);})();\n;return Component;"
        },
        "_id": "blog/posts/why-i-joined-release-matt-riley.mdx",
        "_raw": {
          "sourceFilePath": "blog/posts/why-i-joined-release-matt-riley.mdx",
          "sourceFileName": "why-i-joined-release-matt-riley.mdx",
          "sourceFileDir": "blog/posts",
          "contentType": "mdx",
          "flattenedPath": "blog/posts/why-i-joined-release-matt-riley"
        },
        "type": "BlogPost",
        "computedSlug": "why-i-joined-release-matt-riley"
      },
      "documentHash": "1739393595031",
      "hasWarnings": false,
      "documentTypeName": "BlogPost"
    },
    "blog/posts/why-i-joined-release.mdx": {
      "document": {
        "title": "Why I Joined Release",
        "summary": "Why the mission of solving development environment availability & private application deployment led me to join Release",
        "publishDate": "Tue Jan 04 2022 20:52:58 GMT+0000 (Coordinated Universal Time)",
        "author": "jon-burns",
        "readingTime": 3,
        "categories": [
          "platform-engineering",
          "product"
        ],
        "mainImage": "/blog-images/e9fe349824521727834cac1f58cc4df4.jpg",
        "imageAlt": "Jon-Burns-Release",
        "showCTA": true,
        "ctaCopy": "Looking to streamline your development workflows and eliminate bottlenecks? Explore how Release's EaaS simplifies environment management for high-quality, frequent releases.",
        "ctaLink": "https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=why-i-joined-release",
        "relatedPosts": [
          ""
        ],
        "ogImage": "/blog-images/e9fe349824521727834cac1f58cc4df4.jpg",
        "excerpt": "Why the mission of solving development environment availability & private application deployment led me to join Release",
        "tags": [
          "platform-engineering",
          "product"
        ],
        "ctaButton": "Try Release for Free",
        "body": {
          "raw": "\nI’m excited to start 2022 by sharing that I've joined the sales team at [Release](https://releasehub.com/). We provide Environments-as-a-Service (EaaS). High performing organizations require development environments that are a close replica of their production environment. It’s the secret sauce to high quality frequent releases. Readily available [ephemeral environments](https://releasehub.com/ephemeral-environments) enable developers to remove bottlenecks and internal friction, eliminate unnecessary complexity, and improve velocity. It’s a way for them to focus on ideas, code, and collaboration by eliminating the operational and political burden of environment management.\n\nBut Release is more than just ephemeral environments. Permanent environments play a key role in testing and continuous deployment, especially with the emergence of multi-cloud.  They’re also critical in the deployment of private applications inside your customer cloud, be it GCP, AWS, Azure, Govcloud, on-prem, etc.    \n\nI found out about the company through Y Combinator’s [_Work at a Startup_](http://workatastartup.com/) and struck up a conversation with CEO Tommy McClung. He began sharing the team’s [mission](https://releasehub.com/blog/the-release-mission); to enable anyone to bring their best ideas to the world more effectively. He was steadfast in his belief that too many ideas never see the light of day due to resource constraints, lack of seniority, and the hassle of creating and managing the necessary environments. In previous roles I’d see just how challenging infrastructure and DevOps management can be, especially as companies scale and their architecture gets complex. It became clear to me this is a massive universal problem and Release has the team and tech to achieve its mission and make customers wildly successful.  \nIf you’re a developer and would like to learn more please reach out. Release is backed by investors like Y Combinator, Sequoia, and CRV and [raised a $20M Series A](https://releasehub.com/blog/releasehub-20-million-series-a-led-by-crv) in Oct 2021. PS [we’re hiring!](https://releasehub.com/company#hire)\n",
          "code": "var Component=(()=>{var h=Object.create;var i=Object.defineProperty;var d=Object.getOwnPropertyDescriptor;var u=Object.getOwnPropertyNames;var p=Object.getPrototypeOf,g=Object.prototype.hasOwnProperty;var f=(t,e)=>()=>(e||t((e={exports:{}}).exports,e),e.exports),b=(t,e)=>{for(var n in e)i(t,n,{get:e[n],enumerable:!0})},s=(t,e,n,r)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let o of u(e))!g.call(t,o)&&o!==n&&i(t,o,{get:()=>e[o],enumerable:!(r=d(e,o))||r.enumerable});return t};var v=(t,e,n)=>(n=t!=null?h(p(t)):{},s(e||!t||!t.__esModule?i(n,\"default\",{value:t,enumerable:!0}):n,t)),y=t=>s(i({},\"__esModule\",{value:!0}),t);var m=f((R,l)=>{l.exports=_jsx_runtime});var x={};b(x,{default:()=>k,frontmatter:()=>j});var a=v(m()),j={title:\"Why I Joined Release\",summary:\"Why the mission of solving development environment availability & private application deployment led me to join Release\",publishDate:\"Tue Jan 04 2022 20:52:58 GMT+0000 (Coordinated Universal Time)\",author:\"jon-burns\",readingTime:3,categories:[\"platform-engineering\",\"product\"],mainImage:\"/blog-images/e9fe349824521727834cac1f58cc4df4.jpg\",imageAlt:\"Jon-Burns-Release\",showCTA:!0,ctaCopy:\"Looking to streamline your development workflows and eliminate bottlenecks? Explore how Release's EaaS simplifies environment management for high-quality, frequent releases.\",ctaLink:\"https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=why-i-joined-release\",relatedPosts:[\"\"],ogImage:\"/blog-images/e9fe349824521727834cac1f58cc4df4.jpg\",excerpt:\"Why the mission of solving development environment availability & private application deployment led me to join Release\",tags:[\"platform-engineering\",\"product\"],ctaButton:\"Try Release for Free\"};function c(t){let e=Object.assign({p:\"p\",a:\"a\",em:\"em\",br:\"br\"},t.components);return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsxs)(e.p,{children:[\"I\\u2019m excited to start 2022 by sharing that I've joined the sales team at \",(0,a.jsx)(e.a,{href:\"https://releasehub.com/\",children:\"Release\"}),\". We provide Environments-as-a-Service (EaaS). High performing organizations require development environments that are a close replica of their production environment. It\\u2019s the secret sauce to high quality frequent releases. Readily available \",(0,a.jsx)(e.a,{href:\"https://releasehub.com/ephemeral-environments\",children:\"ephemeral environments\"}),\" enable developers to remove bottlenecks and internal friction, eliminate unnecessary complexity, and improve velocity. It\\u2019s a way for them to focus on ideas, code, and collaboration by eliminating the operational and political burden of environment management.\"]}),`\n`,(0,a.jsx)(e.p,{children:\"But Release is more than just ephemeral environments. Permanent environments play a key role in testing and continuous deployment, especially with the emergence of multi-cloud. \\xA0They\\u2019re also critical in the deployment of private applications inside your customer cloud, be it GCP, AWS, Azure, Govcloud, on-prem, etc. \\xA0 \\xA0\"}),`\n`,(0,a.jsxs)(e.p,{children:[\"I found out about the company through Y Combinator\\u2019s \",(0,a.jsx)(e.a,{href:\"http://workatastartup.com/\",children:(0,a.jsx)(e.em,{children:\"Work at a Startup\"})}),\" and struck up a conversation with CEO Tommy McClung. He began sharing the team\\u2019s \",(0,a.jsx)(e.a,{href:\"https://releasehub.com/blog/the-release-mission\",children:\"mission\"}),\"; to enable anyone to bring their best ideas to the world more effectively. He was steadfast in his belief that too many ideas never see the light of day due to resource constraints, lack of seniority, and the hassle of creating and managing the necessary environments. In previous roles I\\u2019d see just how challenging infrastructure and DevOps management can be, especially as companies scale and their architecture gets complex. It became clear to me this is a massive universal problem and Release has the team and tech to achieve its mission and make customers wildly successful.\",(0,a.jsx)(e.br,{}),`\n`,\"If you\\u2019re a developer and would like to learn more please reach out. Release is backed by investors like Y Combinator, Sequoia, and CRV and \",(0,a.jsx)(e.a,{href:\"https://releasehub.com/blog/releasehub-20-million-series-a-led-by-crv\",children:\"raised a $20M Series A\"}),\" in Oct 2021. PS \",(0,a.jsx)(e.a,{href:\"https://releasehub.com/company#hire\",children:\"we\\u2019re hiring!\"})]})]})}function w(t={}){let{wrapper:e}=t.components||{};return e?(0,a.jsx)(e,Object.assign({},t,{children:(0,a.jsx)(c,t)})):c(t)}var k=w;return y(x);})();\n;return Component;"
        },
        "_id": "blog/posts/why-i-joined-release.mdx",
        "_raw": {
          "sourceFilePath": "blog/posts/why-i-joined-release.mdx",
          "sourceFileName": "why-i-joined-release.mdx",
          "sourceFileDir": "blog/posts",
          "contentType": "mdx",
          "flattenedPath": "blog/posts/why-i-joined-release"
        },
        "type": "BlogPost",
        "computedSlug": "why-i-joined-release"
      },
      "documentHash": "1739393595031",
      "hasWarnings": false,
      "documentTypeName": "BlogPost"
    },
    "blog/posts/why-im-excited-about-environment-as-a-service-and-release.mdx": {
      "document": {
        "title": "Why I'm excited about environment-as-a-service and Release",
        "summary": "Jimmy recently joined Release from AWS. This post summarizes why he is excited about environment-as-a-service.",
        "publishDate": "Thu Feb 03 2022 18:52:52 GMT+0000 (Coordinated Universal Time)",
        "author": "jimmy-bried",
        "readingTime": 4,
        "categories": [
          "platform-engineering",
          "product"
        ],
        "mainImage": "/blog-images/608de72bf2f20433c6fb4dd6232b2ffa.jpg",
        "imageAlt": "Jimmy Bried headshot",
        "showCTA": true,
        "ctaCopy": "Simplify environment management, accelerate deployment cycles, and reduce operational costs with Release's environment-as-a-service platform.",
        "ctaLink": "https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=why-im-excited-about-environment-as-a-service-and-release",
        "relatedPosts": [
          ""
        ],
        "ogImage": "/blog-images/608de72bf2f20433c6fb4dd6232b2ffa.jpg",
        "excerpt": "Jimmy recently joined Release from AWS. This post summarizes why he is excited about environment-as-a-service.",
        "tags": [
          "platform-engineering",
          "product"
        ],
        "ctaButton": "Try Release for Free",
        "body": {
          "raw": "\nI recently concluded an incredible four-year journey at Amazon Web Services. I partnered with hundreds of customers across virtually every industry, from early seed startups to the 100+ year old enterprise companies, helping them leverage AWS to bring their ideas to reality. While differences in company size, funding, and industry brought their own unique requirements, it was their shared and seemingly universal challenges that really caught my attention:\n\n1.  Increase revenue with new features and faster time to market\n2.  Reduce cost (whether it be operational, fixed, or variable)\n3.  Develop a work environment that attracts the best technical talent\n4.  Automate and simplify business processes to become agile and effective\n\nAfter 2 years of working with some of the most innovative and disruptive startups, I decided it was time to embark on the startup journey myself. I was extremely thoughtful in my selection. I knew I wanted to get in early and have a hand in building out the sales organization while wearing many other hats. I also knew that I wanted to work for a company that helps customers solve all the above business challenges / goals. Outside of that, I considered the experience of founders / leadership, credibility of investors, total addressable market, unique or disruptive offering, and the company’s culture. [Release](https://releasehub.com/) checked every single box for me.\n\n[Release](https://releasehub.com/) is a YC, Sequoia and CRV-backed environment-as-a-service startup. It creates, runs, and manages infrastructure environments of any complexity on demand. Release [raised a $20M Series A](https://releasehub.com/blog/releasehub-20-million-series-a-led-by-crv) in October 2021 and they are solving a unique challenge that every customer with cloud infrastructure faces in some capacity.\n\n#### Why Do Environments Matter?\n\nEvery member of product development is dependent on [environments](https://releasehub.com/ephemeral-environments). With applications becoming increasingly complex, creating and managing environments at scale becomes difficult. I saw this with customers at AWS. The cloud helped them with their infrastructure, but their IT teams needed something to operationalize their cloud resources to be effective and fast. I worked with legacy enterprise customers who tried to build an internal solution to meet this need but frequently hit walls in the process and eventually gave up. There wasn’t anything in the market to my knowledge that completely solved this. When developers (or their teams) have to manage the undifferentiated heavy lifting of building and maintaining an environment in order to test code, it results in slower time to market, lower software quality, and exorbitant operational costs. Alternative solutions are either ill-matched (CI/CD utilities, container tools, IDEs) or require building home-grown solutions that require dedicated employees, and might take years to build.\n\n#### What is Environment-as-a-service?\n\nRelease makes it easy to create ephemeral or permanent environments around any code commit for performance testing, [QA](https://releasehub.com/staging-environments), migrations, [sales demos](https://releasehub.com/usecase/sales-demo-environments), [VPC deployments](https://releasehub.com/usecase/running-your-production-environments) and more. Customers can integrate with their source control system of choice and Release will create temporary or permanent branch-based environments with every code push or pull request. You can automate creation of complex environments, replicate copies of that environment in minutes and continually update environments whenever a change is made. We do all this in an EKS cluster built and managed by us which allows you to benefit from Kubernetes even if your team is unfamiliar with the orchestration system.\n\n#### How EaaS Impacts Release Cycles?\n\nDuring the due diligence process I spoke to multiple Release customers, but one customer’s testimony told me all I needed to know about Release. _“ It deployed the environment super quick and it literally just works. I don’t even know how to come up with a previous metric compared to what I just witnessed.”._ Release is giving customers faster time to market in an increasingly competitive environment,  and cost reduction both from removing the undifferentiated heavy lifting and from making the best use of existing dev employees. Customers get best practice DevOps out of the box, up to date instant data sets for realistic experiences, isolated environments, and they are creating an exciting work environment where developers can do what they do best.. Make cool applications.\n\nIf you are looking to join an incredible organization with experienced founders, top-tier investors, and a solution that is disrupting the cloud industry, please feel free to reach out to me directly ([jimmy@release.com](mailto:jimmy@release.com)) or apply to any of the various open roles we have [Here](https://releasehub.com/company#hire).\n",
          "code": "var Component=(()=>{var h=Object.create;var r=Object.defineProperty;var d=Object.getOwnPropertyDescriptor;var u=Object.getOwnPropertyNames;var p=Object.getPrototypeOf,f=Object.prototype.hasOwnProperty;var y=(n,e)=>()=>(e||n((e={exports:{}}).exports,e),e.exports),g=(n,e)=>{for(var a in e)r(n,a,{get:e[a],enumerable:!0})},s=(n,e,a,o)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let i of u(e))!f.call(n,i)&&i!==a&&r(n,i,{get:()=>e[i],enumerable:!(o=d(e,i))||o.enumerable});return n};var v=(n,e,a)=>(a=n!=null?h(p(n)):{},s(e||!n||!n.__esModule?r(a,\"default\",{value:n,enumerable:!0}):a,n)),b=n=>s(r({},\"__esModule\",{value:!0}),n);var c=y((R,l)=>{l.exports=_jsx_runtime});var I={};g(I,{default:()=>x,frontmatter:()=>w});var t=v(c()),w={title:\"Why I'm excited about environment-as-a-service and Release\",summary:\"Jimmy recently joined Release from AWS. This post summarizes why he is excited about environment-as-a-service.\",publishDate:\"Thu Feb 03 2022 18:52:52 GMT+0000 (Coordinated Universal Time)\",author:\"jimmy-bried\",readingTime:4,categories:[\"platform-engineering\",\"product\"],mainImage:\"/blog-images/608de72bf2f20433c6fb4dd6232b2ffa.jpg\",imageAlt:\"Jimmy Bried headshot\",showCTA:!0,ctaCopy:\"Simplify environment management, accelerate deployment cycles, and reduce operational costs with Release's environment-as-a-service platform.\",ctaLink:\"https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=why-im-excited-about-environment-as-a-service-and-release\",relatedPosts:[\"\"],ogImage:\"/blog-images/608de72bf2f20433c6fb4dd6232b2ffa.jpg\",excerpt:\"Jimmy recently joined Release from AWS. This post summarizes why he is excited about environment-as-a-service.\",tags:[\"platform-engineering\",\"product\"],ctaButton:\"Try Release for Free\"};function m(n){let e=Object.assign({p:\"p\",ol:\"ol\",li:\"li\",a:\"a\",h4:\"h4\",span:\"span\",em:\"em\"},n.components);return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(e.p,{children:\"I recently concluded an incredible four-year journey at Amazon Web Services. I partnered with hundreds of customers across virtually every industry, from early seed startups to the 100+ year old enterprise companies, helping them leverage AWS to bring their ideas to reality. While differences in company size, funding, and industry brought their own unique requirements, it was their shared and seemingly universal challenges that really caught my attention:\"}),`\n`,(0,t.jsxs)(e.ol,{children:[`\n`,(0,t.jsx)(e.li,{children:\"Increase revenue with new features and faster time to market\"}),`\n`,(0,t.jsx)(e.li,{children:\"Reduce cost (whether it be operational, fixed, or variable)\"}),`\n`,(0,t.jsx)(e.li,{children:\"Develop a work environment that attracts the best technical talent\"}),`\n`,(0,t.jsx)(e.li,{children:\"Automate and simplify business processes to become agile and effective\"}),`\n`]}),`\n`,(0,t.jsxs)(e.p,{children:[\"After 2 years of working with some of the most innovative and disruptive startups, I decided it was time to embark on the startup journey myself. I was extremely thoughtful in my selection. I knew I wanted to get in early and have a hand in building out the sales organization while wearing many other hats. I also knew that I wanted to work for a company that helps customers solve all the above business challenges / goals. Outside of that, I considered the experience of founders / leadership, credibility of investors, total addressable market, unique or disruptive offering, and the company\\u2019s culture. \",(0,t.jsx)(e.a,{href:\"https://releasehub.com/\",children:\"Release\"}),\" checked every single box for me.\"]}),`\n`,(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.a,{href:\"https://releasehub.com/\",children:\"Release\"}),\" is a YC, Sequoia and CRV-backed environment-as-a-service startup. It creates, runs, and manages infrastructure environments of any complexity on demand. Release \",(0,t.jsx)(e.a,{href:\"https://releasehub.com/blog/releasehub-20-million-series-a-led-by-crv\",children:\"raised a $20M Series A\"}),\" in October 2021 and they are solving a unique challenge that every customer with cloud infrastructure faces in some capacity.\"]}),`\n`,(0,t.jsxs)(e.h4,{id:\"why-do-environments-matter\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#why-do-environments-matter\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Why Do Environments Matter?\"]}),`\n`,(0,t.jsxs)(e.p,{children:[\"Every member of product development is dependent on \",(0,t.jsx)(e.a,{href:\"https://releasehub.com/ephemeral-environments\",children:\"environments\"}),\". With applications becoming increasingly complex, creating and managing environments at scale becomes difficult. I saw this with customers at AWS. The cloud helped them with their infrastructure, but their IT teams needed something to operationalize their cloud resources to be effective and fast. I worked with legacy enterprise customers who tried to build an internal solution to meet this need but frequently hit walls in the process and eventually gave up. There wasn\\u2019t anything in the market to my knowledge that completely solved this. When developers (or their teams) have to manage the undifferentiated heavy lifting of building and maintaining an environment in order to test code, it results in slower time to market, lower software quality, and exorbitant operational costs. Alternative solutions are either ill-matched (CI/CD utilities, container tools, IDEs) or require building home-grown solutions that require dedicated employees, and might take years to build.\"]}),`\n`,(0,t.jsxs)(e.h4,{id:\"what-is-environment-as-a-service\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#what-is-environment-as-a-service\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"What is Environment-as-a-service?\"]}),`\n`,(0,t.jsxs)(e.p,{children:[\"Release makes it easy to create ephemeral or permanent environments around any code commit for performance testing, \",(0,t.jsx)(e.a,{href:\"https://releasehub.com/staging-environments\",children:\"QA\"}),\", migrations, \",(0,t.jsx)(e.a,{href:\"https://releasehub.com/usecase/sales-demo-environments\",children:\"sales demos\"}),\", \",(0,t.jsx)(e.a,{href:\"https://releasehub.com/usecase/running-your-production-environments\",children:\"VPC deployments\"}),\" and more. Customers can integrate with their source control system of choice and Release will create temporary or permanent branch-based environments with every code push or pull request. You can automate creation of complex environments, replicate copies of that environment in minutes and continually update environments whenever a change is made. We do all this in an EKS cluster built and managed by us which allows you to benefit from Kubernetes even if your team is unfamiliar with the orchestration system.\"]}),`\n`,(0,t.jsxs)(e.h4,{id:\"how-eaas-impacts-release-cycles\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#how-eaas-impacts-release-cycles\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"How EaaS Impacts Release Cycles?\"]}),`\n`,(0,t.jsxs)(e.p,{children:[\"During the due diligence process I spoke to multiple Release customers, but one customer\\u2019s testimony told me all I needed to know about Release. \",(0,t.jsx)(e.em,{children:\"\\u201C It deployed the environment super quick and it literally just works. I don\\u2019t even know how to come up with a previous metric compared to what I just witnessed.\\u201D.\"}),\" Release is giving customers faster time to market in an increasingly competitive environment, \\xA0and cost reduction both from removing the undifferentiated heavy lifting and from making the best use of existing dev employees. Customers get best practice DevOps out of the box, up to date instant data sets for realistic experiences, isolated environments, and they are creating an exciting work environment where developers can do what they do best.. Make cool applications.\"]}),`\n`,(0,t.jsxs)(e.p,{children:[\"If you are looking to join an incredible organization with experienced founders, top-tier investors, and a solution that is disrupting the cloud industry, please feel free to reach out to me directly (\",(0,t.jsx)(e.a,{href:\"mailto:jimmy@release.com\",children:\"jimmy@release.com\"}),\") or apply to any of the various open roles we have \",(0,t.jsx)(e.a,{href:\"https://releasehub.com/company#hire\",children:\"Here\"}),\".\"]})]})}function k(n={}){let{wrapper:e}=n.components||{};return e?(0,t.jsx)(e,Object.assign({},n,{children:(0,t.jsx)(m,n)})):m(n)}var x=k;return b(I);})();\n;return Component;"
        },
        "_id": "blog/posts/why-im-excited-about-environment-as-a-service-and-release.mdx",
        "_raw": {
          "sourceFilePath": "blog/posts/why-im-excited-about-environment-as-a-service-and-release.mdx",
          "sourceFileName": "why-im-excited-about-environment-as-a-service-and-release.mdx",
          "sourceFileDir": "blog/posts",
          "contentType": "mdx",
          "flattenedPath": "blog/posts/why-im-excited-about-environment-as-a-service-and-release"
        },
        "type": "BlogPost",
        "computedSlug": "why-im-excited-about-environment-as-a-service-and-release"
      },
      "documentHash": "1739393595031",
      "hasWarnings": false,
      "documentTypeName": "BlogPost"
    },
    "blog/posts/why-kubernetes-is-so-hard.mdx": {
      "document": {
        "title": "Why Is Kubernetes So Hard - 4 Reasons Why And What to do About it",
        "summary": "Kubernetes (k8s) has been all the rage for the last few years because application orchestration has become a de facto",
        "publishDate": "Wed Mar 03 2021 03:03:48 GMT+0000 (Coordinated Universal Time)",
        "author": "regis-wilson",
        "readingTime": 9,
        "categories": [
          "kubernetes",
          "platform-engineering"
        ],
        "mainImage": "/blog-images/0de40b472cdcd421f32d1dd75c78815d.jpg",
        "imageAlt": "Two wrestling athletes fighting representing why Kubernetes infrastructure is hard",
        "showCTA": true,
        "ctaCopy": "Simplify Kubernetes infrastructure management with Release's ephemeral environments for seamless testing and deployment workflows.",
        "ctaLink": "https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=why-kubernetes-is-so-hard",
        "relatedPosts": [
          ""
        ],
        "ogImage": "/blog-images/0de40b472cdcd421f32d1dd75c78815d.jpg",
        "excerpt": "Kubernetes (k8s) has been all the rage for the last few years because application orchestration has become a de facto",
        "tags": [
          "kubernetes",
          "platform-engineering"
        ],
        "ctaButton": "Try Release for Free",
        "body": {
          "raw": "\n### Overview\n\n- [Introduction to Kubernetes](#intro)\n- [Kubernetes Infrastructure Is Hard](#k8sishard)\n- [YAML is not a Markup Language](#yaml)\n- [You Can't Just Copy-Paste Code in Kubernetes](#copypaste)\n- [Kubernetes Debugging](#debug)\n- [Release Hub Solution](#solution)\n\n[‍](#solution)\n\n### Introduction to Kubernetes\n\nKubernetes (k8s) has been all the rage for the last few years because application orchestration has become a de facto table-stakes requirement for production workloads running containers. “Containerising” applications is _relatively_ straightforward, and most DevOps engineers worth their salt can create a few Dockerfiles and build images in a pipeline that are ready to run. But where do you “run” your Docker containers? And which versions do you deploy? And how do all the containers talk to each other? This is where orchestration comes into play and where a few options are proposed by large vendors. There are two main options available at the time of this writing: Elastic Container Service (ECS) from Amazon Web Services (AWS) and Kubernetes which is offered by all the Infrastructure As A Service (IaaS) providers, including even AWS.\n\nThe hope is that orchestration will allow companies to deliver their containerised applications to test and integration environments quickly and painlessly. The ideal scenario is that it “just works”, that is, you would snap your fingers and wait a few minutes before you see your application running in front of you. Ideally, you’d only need to specify the minimum information necessary to run your application: name, framework, dependencies, and so forth, preferably read out from existing configuration files you already have available. That’s the hope anyway.\n\nObviously, from the title, we are focusing on Kubernetes, mainly because it is available everywhere and mainly because the only _other_ option is ECS which only proves the point of our thesis that Kubernetes is hard to use because AWS came up with their own solution that is _supposed to be_ easier. But why is it so hard to use? How long would it reasonably take to get your application(s) running in K8s?? And why isn’t it easier?\n\n### Kubernetes Infrastructure Is Hard\n\nWe always start with the infrastructure even though most companies wouldn’t build their own Kubernetes clusters themselves. If you were ever going to use a managed infrastructure service, K8s would be at the top of the list. I’m sure there are a few people who start up [minikube](https://minikube.sigs.k8s.io/docs/start/) on their laptops and say to themselves, “Wow, this is easy! I can do this myself!!” This reminds me of people who start up an [Elasticsearch](https://www.elastic.co/guide/en/elastic-stack-get-started/current/get-started-elastic-stack.html) container on their laptop and say, “Wow, we should implement this for our website!!” Fast forward to a production launch six months or a year later and the simple “We can do this ourselves” mantra turns into “I wish we didn’t have to do this anymore.”\n\nIf you were truly going to build your own Kubernetes cluster, you’d need to build all the control plane servers and services on your bare-metal or Virtual Machines (VMs) from an IaaS of choice, and then tie them all together with some fancy networking configuration to separate control-plane traffic from container traffic. You’d need to configure and run all of the control plane software and get them all talking to each other, running stably and monitored properly. Perversely, you’d be orchestrating the containers that orchestrate the application, but without a lot of orchestration! The fancy mirage that’s presented when you run minikube or Docker Desktop on Windows hides all the inception of running a container orchestration system using containers.\n\nWe haven’t even gotten to the complications of setting up ingresses (which are just nginx instances, usually) and load balancers that sit on top of or next to the control plane stack. A lot of the time, you’ll feel like you are creating a whole infrastructure just for your infrastructure to run (which isn’t unusual, but definitely doesn’t feel better than trying to orchestrate things yourself). We also haven’t gotten into the Role Based Authentication Controls and network policies that need to be set to support more than a single application or stack running in one cluster. The number of configuration points and server-side setups start to mount quickly and we haven’t even started orchestrating applications yet, which is the whole point of the orchestration system we’re supposed to be setting up.\n\nAnd let’s suppose that you really do wade out into this deep North Atlantic Ocean of huge waves and death-inducing freezing waters, and build yourself a production-worthy ship that can orchestrate your containers into an actual application. You look back up at your calendar and it’s been six months or a year since you started, and you’re just now deploying a control plane that says, “Hello World!” You think you’re successful and you’re about to celebrate when you check the releases section on the website and now you have a new version of k8s to deploy!!!\n\nI hear what you’re saying, “We’re large company and we have lots of DevOps engineers who are top decile of engineering talent in the whole world. We can handle all the heavy lifting. You’re just a whining, jealous baby.” I see you, [Datadog](https://www.youtube.com/watch?v=HDm9iNkLyPI) and [Ticketmaster](https://www.youtube.com/watch?v=wqXVKneP0Hg). (By the way, your accusations of jealousy might be correct. At the end of my good friend Justin Dean’s keynote speech where he shows the slides with all the team members, my picture should have been up there — but I had left the team two years earlier.) For everyone else, we all just decide to not spend six months or a year trying to build our control plane and start up our IaaS provider’s managed service and cross our fingers and pray.\n\n### K8s YAML Ain’t Markup Language\n\nIf you’ve skipped ahead and just started up a managed k8s cluster, you’re still in for a long and tedious journey wading into a deep sea of confusing YAML. YAML is to text what James Joyce’s [_Finnegan’s Wake_](https://www.finwake.com/1024chapter1/fw01.htm) is to English. If you close one eye, use only your left pinky and right thumb to follow some brail, put your feet into ballet’s fifth position, and then recite World War II codes under your breath, then you will easily see that YAML is quite a breeze to comprehend. Once you get the hang of it, it’s like riding a bicycle over a frozen lake on centimeter-thin ice with rabid wolves chasing you. It’s as easy as trying to crash the Ancient Aliens cocktail party held in Fort Knox on gold smuggling days.\n\nLook, it’s not actually that hard, right? Let’s say a guy walks up to you on the street. He’s a k8s expert and he’s going to show you how easy the “hello world” web service deployment is. The conversation goes like this:\n\n```line-numbers\n\nHim: “kind: Deployment”\n\nYou: “Oh, I see. Yes, I like it.”\n\nHim: “apiVersion: apps/v1beta1.”\n\nYou: “Uh, okay. Isn’t v1beta1 out of date? You can use v1 as of k8s 1.9.\nIt’s actually removed in 1.16, but I wonder how many people have never\nupdated.”\n\nHim: “Start over.”\n\nYou: “Wat.”\n\nHim: “kind: Deployment”\n\nYou: “Stop with the Kinds everywhere!”\n\nHim: “apiVersion: apps/v1”\n\nYou: “This again.”\n\nHim: “spec:”\n\nYou: “Huh??”\n\nHim: “selector:”\n\nYou: “No.”\n\nHim: “matchLabels:”\n\nYou: “Wat.”\n\nHim: “app: nginx”\n\nYou: “That’s nearly the first thing I’ve understood about this so far.”\n\nHim: “spec:”\n\nYou: “Again?”\n\nHim: “containers:”\n\nYou: “Okay, now we’re getting somewhere.”\n\nHim: “image: nginx:1.14.2”\n\nYou: “Hmm.”\n\nHim: “ports:”\n\nYou: “Aiiiiieeee.”\n\nHim: “containerPort: 80”\n\nYou: “I’m going home. I quit. There must be a devops job I can get where\nI work on [Gatsby blogs](https://www.gatsbyjs.com/ \"Gatsby nodejs frontend\")\nall day.”\n\n```\n\nAnd that’s just trying to read and understand the file. Try reading two k8s yaml examples and then generate one yourself from scratch. Even better, every day try a [code kata](http://codekata.com/) practice of writing working and deployable configurations to Kubernetes.\n\nI DARE YOU.\n\n### Copy Paste Ain’t Code\n\nI’m still chuckling over the previous section. I have to chuckle because this is the daily pain of my day-to-day existence and facing that pain directly is like standing in front of a bus being driven by Keanu Reeves on the freeway. The only thing that keeps my nose going back to the grindstone is the realization that working with NodeJs would be worse. The problem is that the Kubernetes docs are pretty good. You copy-paste some hello world examples and the outputs look like they work. You start to get pretty good at using kubectl. You can see vague shapes and outlines in YAML. You’re starting to gain confidence that you might be able to do something useful.\n\n“Let’s try to move our application into Kubernetes!” you yell into the air as you emerge dripping wet from your bathtub wrapped only in a towel, like [Archimedes](https://www.scientificamerican.com/article/fact-or-fiction-archimede/) sprinting through the streets of Syracuse. “We’ll just copy-paste some sections from here and here and put them there and there, and we’ll have our app running in no time,” you breathlessly explain to your coworkers. “Does it work?!” they excitedly ask. “Not yet. I mean, no. I need to indent the section and remove one piece that is not used in this spec. Then I need to decide if we use a deployment or a daemonset, but it’s almost there. I swear!”\n\nFirst of all, put on some clothes. I’m all for taking a bath while thinking about kubernetes YAML files, but you need to get dressed afterward. Also, if you drop your Macbook Air into the bath with you, the results can be electrifying. I know. Second, here’s a riddle for you: how many YAML files do you think you need to run and deploy your application? Good thing that some people have ten fingers and ten toes because that’s probably how many you’ll need. And they’re all related but not really. You can copy-paste sections around if you’re adventurous and gullible, but you have no idea if the sections are compatible. There are only four required fields, all of which are gibberish, and everything goes under spec: (including spec:). Most of the sections are duplicated but only slightly. They vary microscopically in ways that matter macroscopically.\n\nCopying and pasting is a wonderful art, and I’ve personally worked my entire adult career that way. I gleefully admit my whole output in life is like a [ransom note](https://tvtropes.org/pmwiki/pmwiki.php/Main/CutAndPasteNote) cut from stack overflow and documentation examples. But piecing together this fragile web of text to do what really should be quite simple and obvious is tedious, error-prone, and too trial-and-error-y. It would be much better to express what you want and be able to actually emit workable, executable code that produces a result you want: namely your application running.\n\nAll this complaining about YAML is quite amusing, but really it’s the symptom of the cause: Kubernetes is so difficult to use because the interface has to be completely rigid. K8s configurations are not living, majestic trees, they are a bunch of dead chopped wood. They are worse than chopped wood, they are whole petrified forests, vast piles of rocks with the imprint of thousands of years of growth rings imprinted on them and preserved for millions of years.\n\nNo, they are worse than petrified wood forests! Kubernetes manifests are the punch cards of the twenty-first century. Each YAML is a collection of holes poked into chopped up wooden cards that we can’t read and understand, that we shove blindly into the kubectl apply -f command and hope that we put them in the correct order and didn’t make a single-hole mistake anywhere in the stack. Then, just like the machines of yesteryear, we try to gain insight into what’s happening by looking at the blinking lights and obscure output of ticker tape, hoping to glean insight.\n\nJust like trying to reproduce Mozart or Beethoven on a [pianola](https://en.wikipedia.org/wiki/Player_piano#Music_rolls) is tedious, laborious, error prone, and ultimately unfulfilling, similarly k8s manifests are frozen forever in time, impossible to write expressively, and playing the same tune _ad infinitum_. The reason people still use v1beta1 even though v1 has been available for _two years_ is because nobody has generated new k8s configurations since then.\n\n### Doctor, Heal Thyself; or Debugging Yourself Is Hard\n\nThe great thing about k8s is that when something goes wrong, nobody knows. I can’t count the number of times I’ve deployed something, worked on something else for a few hours, came back and realised that the deployment had just silently failed and nothing ever notified me. The error message was available somewhere: was it in the deployment logs or the pod logs? Is the ingress or ingress deployment running? Where in the ten or dozens of Kinds files did the log entry appear? And the root cause was often some unrelated issue: an errant and invisible whitespace, not using double quotation marks when I should have, not using single quotation marks when I should have, or getting the brunt end of the indent from a copy-paste issue from three weeks ago.\n\nThere are, of course, tools and techniques and monitoring tools that help out; it’s like Elon Musk’s [Mars orbiter](https://youtu.be/aBr2kKAHN6M?t=837) MVP: “Does it work?” “Absolutely!! A thousand times, yes!” “What does it do?” “Almost anything you want!” You have to know what to look for and where to look for it, then you have to know how to figure out what to do about it, then you have to figure out where in the ten or dozens of files which line or lines to fix, and then you have to know how to fix it.\n\nThe other great thing about k8s is that you own the whole thing. Listen: friendo, pal, buddy, you chose this existence. You copy pasted the “code”. The documentation examples work. I can run “Hello World!” on my laptop so it’s clearly all on you. You’re the one who ran through the office dripping wet in a towel shouting “Kubernetes!” If the Hippocratic oath is “Do no harm” then maybe the Devops oath is “Do no more harm than that which will get you fired.”\n\nAnd the last great thing about k8s is there are tons of people and companies who claim to know what is going on and what to do, and they’ll gladly take your money to show you whether that’s true or not. Type Kubernetes into the search engines and see all the ads that pop up. This article is part of the problem, and also the solution, so stay with me.\n\n### The Solution, Finally\n\nThere are several ways to make Kubernetes easier to use:\n\n1.  Don’t use k8s: run, screaming for your lives\n2.  Train all your people to figure it out (come back to me when you’re done; I still might be alive. Probably not.)\n3.  Hire more people for your team to figure it out (I’m available, hit me up. Ha ha, just kidding.)\n4.  Hire someone else to do it for you\n5.  Wait longer for results, do more with less, eventually settle on something that isn’t horrible\n6.  Find a solution that deploys your applications to environments for you and get on with your actual business of, well, whatever business it is you actually do. Automation tools and services can help you get your application running without investing in the activities described above. Someone has to do it, but it better not be you.\n\nAt [Release](https://releasehub.com/) we work tirelessly to bring your application to life in an orchestrated, human interface. We write software to deal with all the complexity, difficulty, and strain so that no one else has to (unless they want to!) We create the engine that drives the Kubernetes vehicle, and we deliver solutions that our customers can use to get on with their business of doing business.\n\n### Additional Resources\n\n- [Kubernetes Pods Advanced Concepts Explained](https://releasehub.com/blog/kubernetes-pods-advanced-concepts-explained)\n- [How Do You Make Kubernetes Config Files Not Suck?](https://releasehub.com/blog/how-to-make-kubernetes-config-files-not-suck)\n- [Kubernetes - How to Debug CrashLoopBackOff in a Container](https://releasehub.com/blog/kubernetes-how-to-debug-crashloopbackoff-in-a-container)\n- [Cutting Build Time In Half with Docker’s Buildx Kubernetes Driver](https://releasehub.com/blog/cutting-build-time-in-half-docker-buildx-kubernetes)\n- [Kubernetes Health Checks - 2 Ways to Improve Stability in Your Production Applications](https://releasehub.com/blog/kubernetes-health-checks-2-ways-to-improve-stability)\n",
          "code": "var Component=(()=>{var c=Object.create;var i=Object.defineProperty;var d=Object.getOwnPropertyDescriptor;var p=Object.getOwnPropertyNames;var y=Object.getPrototypeOf,m=Object.prototype.hasOwnProperty;var g=(n,e)=>()=>(e||n((e={exports:{}}).exports,e),e.exports),f=(n,e)=>{for(var o in e)i(n,o,{get:e[o],enumerable:!0})},s=(n,e,o,r)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let a of p(e))!m.call(n,a)&&a!==o&&i(n,a,{get:()=>e[a],enumerable:!(r=d(e,a))||r.enumerable});return n};var w=(n,e,o)=>(o=n!=null?c(y(n)):{},s(e||!n||!n.__esModule?i(o,\"default\",{value:n,enumerable:!0}):o,n)),b=n=>s(i({},\"__esModule\",{value:!0}),n);var h=g((K,l)=>{l.exports=_jsx_runtime});var A={};f(A,{default:()=>I,frontmatter:()=>k});var t=w(h()),k={title:\"Why Is Kubernetes So Hard - 4 Reasons Why And What to do About it\",summary:\"Kubernetes (k8s) has been all the rage for the last few years because application orchestration has become a de facto\",publishDate:\"Wed Mar 03 2021 03:03:48 GMT+0000 (Coordinated Universal Time)\",author:\"regis-wilson\",readingTime:9,categories:[\"kubernetes\",\"platform-engineering\"],mainImage:\"/blog-images/0de40b472cdcd421f32d1dd75c78815d.jpg\",imageAlt:\"Two wrestling athletes fighting representing why Kubernetes infrastructure is hard\",showCTA:!0,ctaCopy:\"Simplify Kubernetes infrastructure management with Release's ephemeral environments for seamless testing and deployment workflows.\",ctaLink:\"https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=why-kubernetes-is-so-hard\",relatedPosts:[\"\"],ogImage:\"/blog-images/0de40b472cdcd421f32d1dd75c78815d.jpg\",excerpt:\"Kubernetes (k8s) has been all the rage for the last few years because application orchestration has become a de facto\",tags:[\"kubernetes\",\"platform-engineering\"],ctaButton:\"Try Release for Free\"};function u(n){let e=Object.assign({h3:\"h3\",a:\"a\",span:\"span\",ul:\"ul\",li:\"li\",p:\"p\",em:\"em\",pre:\"pre\",code:\"code\",ol:\"ol\"},n.components);return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsxs)(e.h3,{id:\"overview\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#overview\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Overview\"]}),`\n`,(0,t.jsxs)(e.ul,{children:[`\n`,(0,t.jsx)(e.li,{children:(0,t.jsx)(e.a,{href:\"#intro\",children:\"Introduction to Kubernetes\"})}),`\n`,(0,t.jsx)(e.li,{children:(0,t.jsx)(e.a,{href:\"#k8sishard\",children:\"Kubernetes Infrastructure Is Hard\"})}),`\n`,(0,t.jsx)(e.li,{children:(0,t.jsx)(e.a,{href:\"#yaml\",children:\"YAML is not a Markup Language\"})}),`\n`,(0,t.jsx)(e.li,{children:(0,t.jsx)(e.a,{href:\"#copypaste\",children:\"You Can't Just Copy-Paste Code in Kubernetes\"})}),`\n`,(0,t.jsx)(e.li,{children:(0,t.jsx)(e.a,{href:\"#debug\",children:\"Kubernetes Debugging\"})}),`\n`,(0,t.jsx)(e.li,{children:(0,t.jsx)(e.a,{href:\"#solution\",children:\"Release Hub Solution\"})}),`\n`]}),`\n`,(0,t.jsx)(e.p,{children:(0,t.jsx)(e.a,{href:\"#solution\",children:\"\\u200D\"})}),`\n`,(0,t.jsxs)(e.h3,{id:\"introduction-to-kubernetes\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#introduction-to-kubernetes\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Introduction to Kubernetes\"]}),`\n`,(0,t.jsxs)(e.p,{children:[\"Kubernetes (k8s) has been all the rage for the last few years because application orchestration has become a de facto table-stakes requirement for production workloads running containers. \\u201CContainerising\\u201D applications is \",(0,t.jsx)(e.em,{children:\"relatively\"}),\" straightforward, and most DevOps engineers worth their salt can create a few Dockerfiles and build images in a pipeline that are ready to run. But where do you \\u201Crun\\u201D your Docker containers? And which versions do you deploy? And how do all the containers talk to each other? This is where orchestration comes into play and where a few options are proposed by large vendors. There are two main options available at the time of this writing: Elastic Container Service (ECS) from Amazon Web Services (AWS) and Kubernetes which is offered by all the Infrastructure As A Service (IaaS) providers, including even AWS.\"]}),`\n`,(0,t.jsx)(e.p,{children:\"The hope is that orchestration will allow companies to deliver their containerised applications to test and integration environments quickly and painlessly. The ideal scenario is that it \\u201Cjust works\\u201D, that is, you would snap your fingers and wait a few minutes before you see your application running in front of you. Ideally, you\\u2019d only need to specify the minimum information necessary to run your application: name, framework, dependencies, and so forth, preferably read out from existing configuration files you already have available. That\\u2019s the hope anyway.\"}),`\n`,(0,t.jsxs)(e.p,{children:[\"Obviously, from the title, we are focusing on Kubernetes, mainly because it is available everywhere and mainly because the only \",(0,t.jsx)(e.em,{children:\"other\"}),\" option is ECS which only proves the point of our thesis that Kubernetes is hard to use because AWS came up with their own solution that is \",(0,t.jsx)(e.em,{children:\"supposed to be\"}),\" easier. But why is it so hard to use? How long would it reasonably take to get your application(s) running in K8s?? And why isn\\u2019t it easier?\"]}),`\n`,(0,t.jsxs)(e.h3,{id:\"kubernetes-infrastructure-is-hard\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#kubernetes-infrastructure-is-hard\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Kubernetes Infrastructure Is Hard\"]}),`\n`,(0,t.jsxs)(e.p,{children:[\"We always start with the infrastructure even though most companies wouldn\\u2019t build their own Kubernetes clusters themselves. If you were ever going to use a managed infrastructure service, K8s would be at the top of the list. I\\u2019m sure there are a few people who start up \",(0,t.jsx)(e.a,{href:\"https://minikube.sigs.k8s.io/docs/start/\",children:\"minikube\"}),\" on their laptops and say to themselves, \\u201CWow, this is easy! I can do this myself!!\\u201D This reminds me of people who start up an \",(0,t.jsx)(e.a,{href:\"https://www.elastic.co/guide/en/elastic-stack-get-started/current/get-started-elastic-stack.html\",children:\"Elasticsearch\"}),\" container on their laptop and say, \\u201CWow, we should implement this for our website!!\\u201D Fast forward to a production launch six months or a year later and the simple \\u201CWe can do this ourselves\\u201D mantra turns into \\u201CI wish we didn\\u2019t have to do this anymore.\\u201D\"]}),`\n`,(0,t.jsx)(e.p,{children:\"If you were truly going to build your own Kubernetes cluster, you\\u2019d need to build all the control plane servers and services on your bare-metal or Virtual Machines (VMs) from an IaaS of choice, and then tie them all together with some fancy networking configuration to separate control-plane traffic from container traffic. You\\u2019d need to configure and run all of the control plane software and get them all talking to each other, running stably and monitored properly. Perversely, you\\u2019d be orchestrating the containers that orchestrate the application, but without a lot of orchestration! The fancy mirage that\\u2019s presented when you run minikube or Docker Desktop on Windows hides all the inception of running a container orchestration system using containers.\"}),`\n`,(0,t.jsx)(e.p,{children:\"We haven\\u2019t even gotten to the complications of setting up ingresses (which are just nginx instances, usually) and load balancers that sit on top of or next to the control plane stack. A lot of the time, you\\u2019ll feel like you are creating a whole infrastructure just for your infrastructure to run (which isn\\u2019t unusual, but definitely doesn\\u2019t feel better than trying to orchestrate things yourself). We also haven\\u2019t gotten into the Role Based Authentication Controls and network policies that need to be set to support more than a single application or stack running in one cluster. The number of configuration points and server-side setups start to mount quickly and we haven\\u2019t even started orchestrating applications yet, which is the whole point of the orchestration system we\\u2019re supposed to be setting up.\"}),`\n`,(0,t.jsx)(e.p,{children:\"And let\\u2019s suppose that you really do wade out into this deep North Atlantic Ocean of huge waves and death-inducing freezing waters, and build yourself a production-worthy ship that can orchestrate your containers into an actual application. You look back up at your calendar and it\\u2019s been six months or a year since you started, and you\\u2019re just now deploying a control plane that says, \\u201CHello World!\\u201D You think you\\u2019re successful and you\\u2019re about to celebrate when you check the releases section on the website and now you have a new version of k8s to deploy!!!\"}),`\n`,(0,t.jsxs)(e.p,{children:[\"I hear what you\\u2019re saying, \\u201CWe\\u2019re large company and we have lots of DevOps engineers who are top decile of engineering talent in the whole world. We can handle all the heavy lifting. You\\u2019re just a whining, jealous baby.\\u201D I see you, \",(0,t.jsx)(e.a,{href:\"https://www.youtube.com/watch?v=HDm9iNkLyPI\",children:\"Datadog\"}),\" and \",(0,t.jsx)(e.a,{href:\"https://www.youtube.com/watch?v=wqXVKneP0Hg\",children:\"Ticketmaster\"}),\". (By the way, your accusations of jealousy might be correct. At the end of my good friend Justin Dean\\u2019s keynote speech where he shows the slides with all the team members, my picture should have been up there \\u2014 but I had left the team two years earlier.) For everyone else, we all just decide to not spend six months or a year trying to build our control plane and start up our IaaS provider\\u2019s managed service and cross our fingers and pray.\"]}),`\n`,(0,t.jsxs)(e.h3,{id:\"k8s-yaml-aint-markup-language\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#k8s-yaml-aint-markup-language\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"K8s YAML Ain\\u2019t Markup Language\"]}),`\n`,(0,t.jsxs)(e.p,{children:[\"If you\\u2019ve skipped ahead and just started up a managed k8s cluster, you\\u2019re still in for a long and tedious journey wading into a deep sea of confusing YAML. YAML is to text what James Joyce\\u2019s \",(0,t.jsx)(e.a,{href:\"https://www.finwake.com/1024chapter1/fw01.htm\",children:(0,t.jsx)(e.em,{children:\"Finnegan\\u2019s Wake\"})}),\" is to English. If you close one eye, use only your left pinky and right thumb to follow some brail, put your feet into ballet\\u2019s fifth position, and then recite World War II codes under your breath, then you will easily see that YAML is quite a breeze to comprehend. Once you get the hang of it, it\\u2019s like riding a bicycle over a frozen lake on centimeter-thin ice with rabid wolves chasing you. It\\u2019s as easy as trying to crash the Ancient Aliens cocktail party held in Fort Knox on gold smuggling days.\"]}),`\n`,(0,t.jsx)(e.p,{children:\"Look, it\\u2019s not actually that hard, right? Let\\u2019s say a guy walks up to you on the street. He\\u2019s a k8s expert and he\\u2019s going to show you how easy the \\u201Chello world\\u201D web service deployment is. The conversation goes like this:\"}),`\n`,(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:\"language-line-numbers\",children:`\nHim: \\u201Ckind: Deployment\\u201D\n\nYou: \\u201COh, I see. Yes, I like it.\\u201D\n\nHim: \\u201CapiVersion: apps/v1beta1.\\u201D\n\nYou: \\u201CUh, okay. Isn\\u2019t v1beta1 out of date? You can use v1 as of k8s 1.9.\nIt\\u2019s actually removed in 1.16, but I wonder how many people have never\nupdated.\\u201D\n\nHim: \\u201CStart over.\\u201D\n\nYou: \\u201CWat.\\u201D\n\nHim: \\u201Ckind: Deployment\\u201D\n\nYou: \\u201CStop with the Kinds everywhere!\\u201D\n\nHim: \\u201CapiVersion: apps/v1\\u201D\n\nYou: \\u201CThis again.\\u201D\n\nHim: \\u201Cspec:\\u201D\n\nYou: \\u201CHuh??\\u201D\n\nHim: \\u201Cselector:\\u201D\n\nYou: \\u201CNo.\\u201D\n\nHim: \\u201CmatchLabels:\\u201D\n\nYou: \\u201CWat.\\u201D\n\nHim: \\u201Capp: nginx\\u201D\n\nYou: \\u201CThat\\u2019s nearly the first thing I\\u2019ve understood about this so far.\\u201D\n\nHim: \\u201Cspec:\\u201D\n\nYou: \\u201CAgain?\\u201D\n\nHim: \\u201Ccontainers:\\u201D\n\nYou: \\u201COkay, now we\\u2019re getting somewhere.\\u201D\n\nHim: \\u201Cimage: nginx:1.14.2\\u201D\n\nYou: \\u201CHmm.\\u201D\n\nHim: \\u201Cports:\\u201D\n\nYou: \\u201CAiiiiieeee.\\u201D\n\nHim: \\u201CcontainerPort: 80\\u201D\n\nYou: \\u201CI\\u2019m going home. I quit. There must be a devops job I can get where\nI work on [Gatsby blogs](https://www.gatsbyjs.com/ \"Gatsby nodejs frontend\")\nall day.\\u201D\n\n`})}),`\n`,(0,t.jsxs)(e.p,{children:[\"And that\\u2019s just trying to read and understand the file. Try reading two k8s yaml examples and then generate one yourself from scratch. Even better, every day try a \",(0,t.jsx)(e.a,{href:\"http://codekata.com/\",children:\"code kata\"}),\" practice of writing working and deployable configurations to Kubernetes.\"]}),`\n`,(0,t.jsx)(e.p,{children:\"I DARE YOU.\"}),`\n`,(0,t.jsxs)(e.h3,{id:\"copy-paste-aint-code\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#copy-paste-aint-code\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Copy Paste Ain\\u2019t Code\"]}),`\n`,(0,t.jsx)(e.p,{children:\"I\\u2019m still chuckling over the previous section. I have to chuckle because this is the daily pain of my day-to-day existence and facing that pain directly is like standing in front of a bus being driven by Keanu Reeves on the freeway. The only thing that keeps my nose going back to the grindstone is the realization that working with NodeJs would be worse. The problem is that the Kubernetes docs are pretty good. You copy-paste some hello world examples and the outputs look like they work. You start to get pretty good at using kubectl. You can see vague shapes and outlines in YAML. You\\u2019re starting to gain confidence that you might be able to do something useful.\"}),`\n`,(0,t.jsxs)(e.p,{children:[\"\\u201CLet\\u2019s try to move our application into Kubernetes!\\u201D you yell into the air as you emerge dripping wet from your bathtub wrapped only in a towel, like \",(0,t.jsx)(e.a,{href:\"https://www.scientificamerican.com/article/fact-or-fiction-archimede/\",children:\"Archimedes\"}),\" sprinting through the streets of Syracuse. \\u201CWe\\u2019ll just copy-paste some sections from here and here and put them there and there, and we\\u2019ll have our app running in no time,\\u201D you breathlessly explain to your coworkers. \\u201CDoes it work?!\\u201D they excitedly ask. \\u201CNot yet. I mean, no. I need to indent the section and remove one piece that is not used in this spec. Then I need to decide if we use a deployment or a daemonset, but it\\u2019s almost there. I swear!\\u201D\"]}),`\n`,(0,t.jsx)(e.p,{children:\"First of all, put on some clothes. I\\u2019m all for taking a bath while thinking about kubernetes YAML files, but you need to get dressed afterward. Also, if you drop your Macbook Air into the bath with you, the results can be electrifying. I know. Second, here\\u2019s a riddle for you: how many YAML files do you think you need to run and deploy your application? Good thing that some people have ten fingers and ten toes because that\\u2019s probably how many you\\u2019ll need. And they\\u2019re all related but not really. You can copy-paste sections around if you\\u2019re adventurous and gullible, but you have no idea if the sections are compatible. There are only four required fields, all of which are gibberish, and everything goes under spec: (including spec:). Most of the sections are duplicated but only slightly. They vary microscopically in ways that matter macroscopically.\"}),`\n`,(0,t.jsxs)(e.p,{children:[\"Copying and pasting is a wonderful art, and I\\u2019ve personally worked my entire adult career that way. I gleefully admit my whole output in life is like a \",(0,t.jsx)(e.a,{href:\"https://tvtropes.org/pmwiki/pmwiki.php/Main/CutAndPasteNote\",children:\"ransom note\"}),\" cut from stack overflow and documentation examples. But piecing together this fragile web of text to do what really should be quite simple and obvious is tedious, error-prone, and too trial-and-error-y. It would be much better to express what you want and be able to actually emit workable, executable code that produces a result you want: namely your application running.\"]}),`\n`,(0,t.jsx)(e.p,{children:\"All this complaining about YAML is quite amusing, but really it\\u2019s the symptom of the cause: Kubernetes is so difficult to use because the interface has to be completely rigid. K8s configurations are not living, majestic trees, they are a bunch of dead chopped wood. They are worse than chopped wood, they are whole petrified forests, vast piles of rocks with the imprint of thousands of years of growth rings imprinted on them and preserved for millions of years.\"}),`\n`,(0,t.jsx)(e.p,{children:\"No, they are worse than petrified wood forests! Kubernetes manifests are the punch cards of the twenty-first century. Each YAML is a collection of holes poked into chopped up wooden cards that we can\\u2019t read and understand, that we shove blindly into the kubectl apply -f command and hope that we put them in the correct order and didn\\u2019t make a single-hole mistake anywhere in the stack. Then, just like the machines of yesteryear, we try to gain insight into what\\u2019s happening by looking at the blinking lights and obscure output of ticker tape, hoping to glean insight.\"}),`\n`,(0,t.jsxs)(e.p,{children:[\"Just like trying to reproduce Mozart or Beethoven on a \",(0,t.jsx)(e.a,{href:\"https://en.wikipedia.org/wiki/Player_piano#Music_rolls\",children:\"pianola\"}),\" is tedious, laborious, error prone, and ultimately unfulfilling, similarly k8s manifests are frozen forever in time, impossible to write expressively, and playing the same tune \",(0,t.jsx)(e.em,{children:\"ad infinitum\"}),\". The reason people still use v1beta1 even though v1 has been available for \",(0,t.jsx)(e.em,{children:\"two years\"}),\" is because nobody has generated new k8s configurations since then.\"]}),`\n`,(0,t.jsxs)(e.h3,{id:\"doctor-heal-thyself-or-debugging-yourself-is-hard\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#doctor-heal-thyself-or-debugging-yourself-is-hard\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Doctor, Heal Thyself; or Debugging Yourself Is Hard\"]}),`\n`,(0,t.jsx)(e.p,{children:\"The great thing about k8s is that when something goes wrong, nobody knows. I can\\u2019t count the number of times I\\u2019ve deployed something, worked on something else for a few hours, came back and realised that the deployment had just silently failed and nothing ever notified me. The error message was available somewhere: was it in the deployment logs or the pod logs? Is the ingress or ingress deployment running? Where in the ten or dozens of Kinds files did the log entry appear? And the root cause was often some unrelated issue: an errant and invisible whitespace, not using double quotation marks when I should have, not using single quotation marks when I should have, or getting the brunt end of the indent from a copy-paste issue from three weeks ago.\"}),`\n`,(0,t.jsxs)(e.p,{children:[\"There are, of course, tools and techniques and monitoring tools that help out; it\\u2019s like Elon Musk\\u2019s \",(0,t.jsx)(e.a,{href:\"https://youtu.be/aBr2kKAHN6M?t=837\",children:\"Mars orbiter\"}),\" MVP: \\u201CDoes it work?\\u201D \\u201CAbsolutely!! A thousand times, yes!\\u201D \\u201CWhat does it do?\\u201D \\u201CAlmost anything you want!\\u201D You have to know what to look for and where to look for it, then you have to know how to figure out what to do about it, then you have to figure out where in the ten or dozens of files which line or lines to fix, and then you have to know how to fix it.\"]}),`\n`,(0,t.jsx)(e.p,{children:\"The other great thing about k8s is that you own the whole thing. Listen: friendo, pal, buddy, you chose this existence. You copy pasted the \\u201Ccode\\u201D. The documentation examples work. I can run \\u201CHello World!\\u201D on my laptop so it\\u2019s clearly all on you. You\\u2019re the one who ran through the office dripping wet in a towel shouting \\u201CKubernetes!\\u201D If the Hippocratic oath is \\u201CDo no harm\\u201D then maybe the Devops oath is \\u201CDo no more harm than that which will get you fired.\\u201D\"}),`\n`,(0,t.jsx)(e.p,{children:\"And the last great thing about k8s is there are tons of people and companies who claim to know what is going on and what to do, and they\\u2019ll gladly take your money to show you whether that\\u2019s true or not. Type Kubernetes into the search engines and see all the ads that pop up. This article is part of the problem, and also the solution, so stay with me.\"}),`\n`,(0,t.jsxs)(e.h3,{id:\"the-solution-finally\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#the-solution-finally\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"The Solution, Finally\"]}),`\n`,(0,t.jsx)(e.p,{children:\"There are several ways to make Kubernetes easier to use:\"}),`\n`,(0,t.jsxs)(e.ol,{children:[`\n`,(0,t.jsx)(e.li,{children:\"Don\\u2019t use k8s: run, screaming for your lives\"}),`\n`,(0,t.jsx)(e.li,{children:\"Train all your people to figure it out (come back to me when you\\u2019re done; I still might be alive. Probably not.)\"}),`\n`,(0,t.jsx)(e.li,{children:\"Hire more people for your team to figure it out (I\\u2019m available, hit me up. Ha ha, just kidding.)\"}),`\n`,(0,t.jsx)(e.li,{children:\"Hire someone else to do it for you\"}),`\n`,(0,t.jsx)(e.li,{children:\"Wait longer for results, do more with less, eventually settle on something that isn\\u2019t horrible\"}),`\n`,(0,t.jsx)(e.li,{children:\"Find a solution that deploys your applications to environments for you and get on with your actual business of, well, whatever business it is you actually do. Automation tools and services can help you get your application running without investing in the activities described above. Someone has to do it, but it better not be you.\"}),`\n`]}),`\n`,(0,t.jsxs)(e.p,{children:[\"At \",(0,t.jsx)(e.a,{href:\"https://releasehub.com/\",children:\"Release\"}),\" we work tirelessly to bring your application to life in an orchestrated, human interface. We write software to deal with all the complexity, difficulty, and strain so that no one else has to (unless they want to!) We create the engine that drives the Kubernetes vehicle, and we deliver solutions that our customers can use to get on with their business of doing business.\"]}),`\n`,(0,t.jsxs)(e.h3,{id:\"additional-resources\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#additional-resources\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Additional Resources\"]}),`\n`,(0,t.jsxs)(e.ul,{children:[`\n`,(0,t.jsx)(e.li,{children:(0,t.jsx)(e.a,{href:\"https://releasehub.com/blog/kubernetes-pods-advanced-concepts-explained\",children:\"Kubernetes Pods Advanced Concepts Explained\"})}),`\n`,(0,t.jsx)(e.li,{children:(0,t.jsx)(e.a,{href:\"https://releasehub.com/blog/how-to-make-kubernetes-config-files-not-suck\",children:\"How Do You Make Kubernetes Config Files Not Suck?\"})}),`\n`,(0,t.jsx)(e.li,{children:(0,t.jsx)(e.a,{href:\"https://releasehub.com/blog/kubernetes-how-to-debug-crashloopbackoff-in-a-container\",children:\"Kubernetes - How to Debug CrashLoopBackOff in a Container\"})}),`\n`,(0,t.jsx)(e.li,{children:(0,t.jsx)(e.a,{href:\"https://releasehub.com/blog/cutting-build-time-in-half-docker-buildx-kubernetes\",children:\"Cutting Build Time In Half with Docker\\u2019s Buildx Kubernetes Driver\"})}),`\n`,(0,t.jsx)(e.li,{children:(0,t.jsx)(e.a,{href:\"https://releasehub.com/blog/kubernetes-health-checks-2-ways-to-improve-stability\",children:\"Kubernetes Health Checks - 2 Ways to Improve Stability in Your Production Applications\"})}),`\n`]})]})}function v(n={}){let{wrapper:e}=n.components||{};return e?(0,t.jsx)(e,Object.assign({},n,{children:(0,t.jsx)(u,n)})):u(n)}var I=v;return b(A);})();\n;return Component;"
        },
        "_id": "blog/posts/why-kubernetes-is-so-hard.mdx",
        "_raw": {
          "sourceFilePath": "blog/posts/why-kubernetes-is-so-hard.mdx",
          "sourceFileName": "why-kubernetes-is-so-hard.mdx",
          "sourceFileDir": "blog/posts",
          "contentType": "mdx",
          "flattenedPath": "blog/posts/why-kubernetes-is-so-hard"
        },
        "type": "BlogPost",
        "computedSlug": "why-kubernetes-is-so-hard"
      },
      "documentHash": "1739393595031",
      "hasWarnings": false,
      "documentTypeName": "BlogPost"
    },
    "blog/posts/you-need-agile-devops-methodology.mdx": {
      "document": {
        "title": "You don't need to know what you’re doing, you need an agile DevOps methodology",
        "summary": "Do you ever feel like you have no idea what you’re doing? Like you’re just kind of going along with things, doing your b",
        "publishDate": "Tue Jan 26 2021 22:22:08 GMT+0000 (Coordinated Universal Time)",
        "author": "vicky-koblinski",
        "readingTime": 5,
        "categories": [
          "platform-engineering",
          "product"
        ],
        "mainImage": "/blog-images/a4778940b15c7370ac6df7eeb05d221d.jpg",
        "imageAlt": "White light on dark background",
        "showCTA": true,
        "ctaCopy": "Looking to embrace agile DevOps like a pro? Streamline your workflows with Release's ephemeral environments for faster deployments.",
        "ctaLink": "https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=you-need-agile-devops-methodology",
        "relatedPosts": [
          ""
        ],
        "ogImage": "/blog-images/a4778940b15c7370ac6df7eeb05d221d.jpg",
        "excerpt": "Do you ever feel like you have no idea what you’re doing? Like you’re just kind of going along with things, doing your b",
        "tags": [
          "platform-engineering",
          "product"
        ],
        "ctaButton": "Try Release for Free",
        "body": {
          "raw": "\nDo you ever feel like you have no idea what you’re doing? Like you’re just kind of going along with things, doing your _best_ but not really sure if you’re doing it _right_?\n\nHere’s a little secret I’ve learned over the years: _Nobody knows what they’re doing._\n\nDenis, with his steadfast approach and surgeon-like precision? Hannah, with her pen and notebook, who writes down every detail to later review? Kendall, who is quick on the trigger to any bizarre question that upper management tosses her way and always leaves them with good laughs and big smiles?\n\nAll of them? _They have no idea what they’re doing._ They don’t **have** to know what they’re doing. They’ve unlocked the biggest secret that formal education has desperately tried to unteach us: Failing is _fine_! Failing is **good**! Failing is **_the fastest way to success_**!\n\n![](/blog-images/fb957b595a7ec1e99fa11afb77151430.gif)\n\n_Brainless, single-cell slime hunting for food_\n\nIn the world of software development, we’ve already accepted this as scientific fact. We embrace this and weave it into the foundation of our methodologies and systems. We know, with absolute certainty, that nothing is certain. The winner will be the little boat whose chart is scribbled on the back of a napkin and can pivot on a dime, not the monolithic Titanic who, despite the captain’s best efforts, is going to collide with that iceberg.\n\nHowever, even though we recognize the advantages and the _need_ to be **agile** in this industry, that does not mean that we’ve mastered all the ways to optimize this. While different Software Development methodologies can have their place for different problem spaces (just as different programming languages are better suited for some problems over others), one particular approach to **_failing fast_** has gained a lot of traction over the past decade. The **DevOps** methodology was forged from the fires of Agile, and today DevOps has been crowned the champion of how to build great software, quickly and wicked fast. Modern technology companies that thrive to compete place DevOps on the forefront of their mind.\n\n![](/blog-images/6fdb6e842b8756cc2f0722f8608dd966.gif)\n\n_Twining motion of vines trying to find something to climb_\n\n### What is DevOps?\n\nIf you’re not already familiar with DevOps, the term can be a little confusing. DevOps began as a cultural movement within companies. Rather than Developer teams taking their code and throwing it over the fence for the Operations team to deploy and monitor (while the Security team haughtily throws their arms in the air over any concern—be it major or minor—introduced by the other teams), DevOps works to tear down these artificial walls.\n\nThe way this works in practice is through tight feedback loops and blurring the edges of responsibility. “DevOps” has turned from an idea into a career where you build a racetrack for product development. This enables the idea-to-deployment cycle to hasten. No longer do we need to take months to plan, build, test, release, deploy, evaluate. No longer do we need to make sure every release is perfect “because there’s no going back”. Now we can do what we do best: _Make mistakes._\n\nYou’ve heard of (or worked with) companies that deploy their code tens to thousands of times a day. This is incredibly powerful. Ideas always look a little different in practice and sometimes they turn out to be bad ideas. But sometimes those silly ideas that would have you laughed out of a boardroom turn out to be the ones worth their weight in gold. With the ability to experiment and quickly reset, we can fractal our way to the perfect solution for any problem.\n\n### Why do I need Environments?\n\nIf your company is big enough, has the capital, and understands the need, you may be lucky enough to have a team of DevOps Engineers who work to help make sure everyone has the environments they need, and the tools to build and deploy code.You have your build and deploy pipelines. Your code only takes one push, merge, and a couple button clicks to make its way into production. You’re. Living. The. Life.\n\nOkay, sure, there are rough edges. The Developers and QA might be a little agitated that they have “bad data” in their databases. This bad data causes weird shadow bugs that wouldn’t exist “for a real user”. You have Product Managers and UX/UI Engineers digging around your QA and Staging Environments to make sure the feature matches the requirements, but they run into these shadow bugs and in a panic, hold a meeting to discuss “why the application is broken”.\n\n_C’est la vie._ Your environments are starting to drift apart, but it’s okay. QA finds a bug, but they can’t be sure when it was introduced. The Developer isn’t sure either, there are a few features that others were working on that have all been merged into here. One major feature is ready, but the others are blocked. Hours tick into the evening as the team scrambles to fix the concerns, introducing more in their haste, until finally QA calls to cancel today’s deployment. Disappointed and mentally taxed, everyone finally goes home frustrated.\n\nThis happens regularly. It’s a clog in the system, but it is manageable. However, what you don’t know yet is that your competitor has committed to allocating resources to addressing this problem directly. It took them twelve months (a bit off from their original six-month estimate), but now the tooling is built out and QA can create a build from any branch, on the fly. Now the data is fresh, every time, freeing QA to get their hands dirty in this sandbox. The Product team is excited to be able to look at new features side by side before they’re released, and features can even be put on hold or tinkered with in isolation.\n\nMeanwhile, your company is falling behind. The only way you’d be able to keep up is to spend the money _and the time_ to build this for yourselves. It’s a big investment, a big time commitment, and your company is worried. What if the project fails? There’s a lot at stake here.\n\nEnvironments are the key to rapid prototyping and quick feature releases, while maintaining a solid, battle-tested product. But environments are also expensive. The upfront and maintenance cost put them outside the scope for many companies and by the time a strong need rears its head, the architecture has evolved into a technical labyrinth.\n\nAt Release, we understand this problem in depth. We have customers who use Release to give themselves a competitive advantage after they realized environments were holding them back. A new concept in DevOps has emerged called [Ephemeral Environments](https://releasehub.com/ephemeral-environments) which eliminates the bottleneck of shared staging environments. An Ephemeral Environment is automatically created when a developer does a pull request and has just their changes on their branch. This environment spins up for UAT testing and when the branch is merged, it disappears. Developers never wait for access to environments as they appear as part of their development workflow.\n\nWe’re using Ephemeral Environments which has put us on fast-track to shipping deliverables that we can stand behind. The unfortunate truth is that building this infrastructure is necessary to have a fighting chance against the heavyweights. But the problem is that the cost and time required can be astronomical. The [Release](https://releasehub.com) platform specifically aims to solve this problem directly by providing Environments-as-a-Service. This way, you get all the advantages of environments, without the costs or headache of Doing It Yourself, freeing you up to focus on the business and the product needs.\n\n### In short…\n\nRemember: Nobody knows what they’re doing. And that’s okay. Nobody’s ever known what they’re doing; we’re all just stumbling around. But if we stumble with purpose, we can fall into something that works. If we fail fast, we get to success faster. Our methodologies in software development reflect this, but in practice, building supportive infrastructure is costly. If we can get to a place where we optimtimize failing fast by deploying early and often, we can more quickly find what our product needs to be. We can find success _without ever having to know_ what we’re doing.\n\n### Ephemeral Environments\n\nCurious about Ephemeral Environments and what they can do for you? Check out [this article](https://release.com/ephemeral-environments) on what Ephemeral Environments are and what they can do for you.\n",
          "code": "var Component=(()=>{var c=Object.create;var i=Object.defineProperty;var m=Object.getOwnPropertyDescriptor;var u=Object.getOwnPropertyNames;var g=Object.getPrototypeOf,p=Object.prototype.hasOwnProperty;var f=(n,e)=>()=>(e||n((e={exports:{}}).exports,e),e.exports),y=(n,e)=>{for(var o in e)i(n,o,{get:e[o],enumerable:!0})},s=(n,e,o,r)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let a of u(e))!p.call(n,a)&&a!==o&&i(n,a,{get:()=>e[a],enumerable:!(r=m(e,a))||r.enumerable});return n};var w=(n,e,o)=>(o=n!=null?c(g(n)):{},s(e||!n||!n.__esModule?i(o,\"default\",{value:n,enumerable:!0}):o,n)),b=n=>s(i({},\"__esModule\",{value:!0}),n);var d=f((x,h)=>{h.exports=_jsx_runtime});var D={};y(D,{default:()=>T,frontmatter:()=>v});var t=w(d()),v={title:\"You don't need to know what you\\u2019re doing, you need an agile DevOps methodology\",summary:\"Do you ever feel like you have no idea what you\\u2019re doing? Like you\\u2019re just kind of going along with things, doing your b\",publishDate:\"Tue Jan 26 2021 22:22:08 GMT+0000 (Coordinated Universal Time)\",author:\"vicky-koblinski\",readingTime:5,categories:[\"platform-engineering\",\"product\"],mainImage:\"/blog-images/a4778940b15c7370ac6df7eeb05d221d.jpg\",imageAlt:\"White light on dark background\",showCTA:!0,ctaCopy:\"Looking to embrace agile DevOps like a pro? Streamline your workflows with Release's ephemeral environments for faster deployments.\",ctaLink:\"https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=you-need-agile-devops-methodology\",relatedPosts:[\"\"],ogImage:\"/blog-images/a4778940b15c7370ac6df7eeb05d221d.jpg\",excerpt:\"Do you ever feel like you have no idea what you\\u2019re doing? Like you\\u2019re just kind of going along with things, doing your b\",tags:[\"platform-engineering\",\"product\"],ctaButton:\"Try Release for Free\"};function l(n){let e=Object.assign({p:\"p\",em:\"em\",strong:\"strong\",img:\"img\",h3:\"h3\",a:\"a\",span:\"span\"},n.components);return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsxs)(e.p,{children:[\"Do you ever feel like you have no idea what you\\u2019re doing? Like you\\u2019re just kind of going along with things, doing your \",(0,t.jsx)(e.em,{children:\"best\"}),\" but not really sure if you\\u2019re doing it \",(0,t.jsx)(e.em,{children:\"right\"}),\"?\"]}),`\n`,(0,t.jsxs)(e.p,{children:[\"Here\\u2019s a little secret I\\u2019ve learned over the years: \",(0,t.jsx)(e.em,{children:\"Nobody knows what they\\u2019re doing.\"})]}),`\n`,(0,t.jsx)(e.p,{children:\"Denis, with his steadfast approach and surgeon-like precision? Hannah, with her pen and notebook, who writes down every detail to later review? Kendall, who is quick on the trigger to any bizarre question that upper management tosses her way and always leaves them with good laughs and big smiles?\"}),`\n`,(0,t.jsxs)(e.p,{children:[\"All of them? \",(0,t.jsx)(e.em,{children:\"They have no idea what they\\u2019re doing.\"}),\" They don\\u2019t \",(0,t.jsx)(e.strong,{children:\"have\"}),\" to know what they\\u2019re doing. They\\u2019ve unlocked the biggest secret that formal education has desperately tried to unteach us: Failing is \",(0,t.jsx)(e.em,{children:\"fine\"}),\"! Failing is \",(0,t.jsx)(e.strong,{children:\"good\"}),\"! Failing is \",(0,t.jsx)(e.strong,{children:(0,t.jsx)(e.em,{children:\"the fastest way to success\"})}),\"!\"]}),`\n`,(0,t.jsx)(e.p,{children:(0,t.jsx)(e.img,{src:\"/blog-images/fb957b595a7ec1e99fa11afb77151430.gif\",alt:\"\"})}),`\n`,(0,t.jsx)(e.p,{children:(0,t.jsx)(e.em,{children:\"Brainless, single-cell slime hunting for food\"})}),`\n`,(0,t.jsx)(e.p,{children:\"In the world of software development, we\\u2019ve already accepted this as scientific fact. We embrace this and weave it into the foundation of our methodologies and systems. We know, with absolute certainty, that nothing is certain. The winner will be the little boat whose chart is scribbled on the back of a napkin and can pivot on a dime, not the monolithic Titanic who, despite the captain\\u2019s best efforts, is going to collide with that iceberg.\"}),`\n`,(0,t.jsxs)(e.p,{children:[\"However, even though we recognize the advantages and the \",(0,t.jsx)(e.em,{children:\"need\"}),\" to be \",(0,t.jsx)(e.strong,{children:\"agile\"}),\" in this industry, that does not mean that we\\u2019ve mastered all the ways to optimize this. While different Software Development methodologies can have their place for different problem spaces (just as different programming languages are better suited for some problems over others), one particular approach to \",(0,t.jsx)(e.strong,{children:(0,t.jsx)(e.em,{children:\"failing fast\"})}),\" has gained a lot of traction over the past decade. The \",(0,t.jsx)(e.strong,{children:\"DevOps\"}),\" methodology was forged from the fires of Agile, and today DevOps has been crowned the champion of how to build great software, quickly and wicked fast. Modern technology companies that thrive to compete place DevOps on the forefront of their mind.\"]}),`\n`,(0,t.jsx)(e.p,{children:(0,t.jsx)(e.img,{src:\"/blog-images/6fdb6e842b8756cc2f0722f8608dd966.gif\",alt:\"\"})}),`\n`,(0,t.jsx)(e.p,{children:(0,t.jsx)(e.em,{children:\"Twining motion of vines trying to find something to climb\"})}),`\n`,(0,t.jsxs)(e.h3,{id:\"what-is-devops\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#what-is-devops\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"What is DevOps?\"]}),`\n`,(0,t.jsx)(e.p,{children:\"If you\\u2019re not already familiar with DevOps, the term can be a little confusing. DevOps began as a cultural movement within companies. Rather than Developer teams taking their code and throwing it over the fence for the Operations team to deploy and monitor (while the Security team haughtily throws their arms in the air over any concern\\u2014be it major or minor\\u2014introduced by the other teams), DevOps works to tear down these artificial walls.\"}),`\n`,(0,t.jsxs)(e.p,{children:[\"The way this works in practice is through tight feedback loops and blurring the edges of responsibility. \\u201CDevOps\\u201D has turned from an idea into a career where you build a racetrack for product development. This enables the idea-to-deployment cycle to hasten. No longer do we need to take months to plan, build, test, release, deploy, evaluate. No longer do we need to make sure every release is perfect \\u201Cbecause there\\u2019s no going back\\u201D. Now we can do what we do best: \",(0,t.jsx)(e.em,{children:\"Make mistakes.\"})]}),`\n`,(0,t.jsx)(e.p,{children:\"You\\u2019ve heard of (or worked with) companies that deploy their code tens to thousands of times a day. This is incredibly powerful. Ideas always look a little different in practice and sometimes they turn out to be bad ideas. But sometimes those silly ideas that would have you laughed out of a boardroom turn out to be the ones worth their weight in gold. With the ability to experiment and quickly reset, we can fractal our way to the perfect solution for any problem.\"}),`\n`,(0,t.jsxs)(e.h3,{id:\"why-do-i-need-environments\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#why-do-i-need-environments\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Why do I need Environments?\"]}),`\n`,(0,t.jsx)(e.p,{children:\"If your company is big enough, has the capital, and understands the need, you may be lucky enough to have a team of DevOps Engineers who work to help make sure everyone has the environments they need, and the tools to build and deploy code.You have your build and deploy pipelines. Your code only takes one push, merge, and a couple button clicks to make its way into production. You\\u2019re. Living. The. Life.\"}),`\n`,(0,t.jsx)(e.p,{children:\"Okay, sure, there are rough edges. The Developers and QA might be a little agitated that they have \\u201Cbad data\\u201D in their databases. This bad data causes weird shadow bugs that wouldn\\u2019t exist \\u201Cfor a real user\\u201D. You have Product Managers and UX/UI Engineers digging around your QA and Staging Environments to make sure the feature matches the requirements, but they run into these shadow bugs and in a panic, hold a meeting to discuss \\u201Cwhy the application is broken\\u201D.\"}),`\n`,(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.em,{children:\"C\\u2019est la vie.\"}),\" Your environments are starting to drift apart, but it\\u2019s okay. QA finds a bug, but they can\\u2019t be sure when it was introduced. The Developer isn\\u2019t sure either, there are a few features that others were working on that have all been merged into here. One major feature is ready, but the others are blocked. Hours tick into the evening as the team scrambles to fix the concerns, introducing more in their haste, until finally QA calls to cancel today\\u2019s deployment. Disappointed and mentally taxed, everyone finally goes home frustrated.\"]}),`\n`,(0,t.jsx)(e.p,{children:\"This happens regularly. It\\u2019s a clog in the system, but it is manageable. However, what you don\\u2019t know yet is that your competitor has committed to allocating resources to addressing this problem directly. It took them twelve months (a bit off from their original six-month estimate), but now the tooling is built out and QA can create a build from any branch, on the fly. Now the data is fresh, every time, freeing QA to get their hands dirty in this sandbox. The Product team is excited to be able to look at new features side by side before they\\u2019re released, and features can even be put on hold or tinkered with in isolation.\"}),`\n`,(0,t.jsxs)(e.p,{children:[\"Meanwhile, your company is falling behind. The only way you\\u2019d be able to keep up is to spend the money \",(0,t.jsx)(e.em,{children:\"and the time\"}),\" to build this for yourselves. It\\u2019s a big investment, a big time commitment, and your company is worried. What if the project fails? There\\u2019s a lot at stake here.\"]}),`\n`,(0,t.jsx)(e.p,{children:\"Environments are the key to rapid prototyping and quick feature releases, while maintaining a solid, battle-tested product. But environments are also expensive. The upfront and maintenance cost put them outside the scope for many companies and by the time a strong need rears its head, the architecture has evolved into a technical labyrinth.\"}),`\n`,(0,t.jsxs)(e.p,{children:[\"At Release, we understand this problem in depth. We have customers who use Release to give themselves a competitive advantage after they realized environments were holding them back. A new concept in DevOps has emerged called \",(0,t.jsx)(e.a,{href:\"https://releasehub.com/ephemeral-environments\",children:\"Ephemeral Environments\"}),\" which eliminates the bottleneck of shared staging environments. An Ephemeral Environment is automatically created when a developer does a pull request and has just their changes on their branch. This environment spins up for UAT testing and when the branch is merged, it disappears. Developers never wait for access to environments as they appear as part of their development workflow.\"]}),`\n`,(0,t.jsxs)(e.p,{children:[\"We\\u2019re using Ephemeral Environments which has put us on fast-track to shipping deliverables that we can stand behind. The unfortunate truth is that building this infrastructure is necessary to have a fighting chance against the heavyweights. But the problem is that the cost and time required can be astronomical. The \",(0,t.jsx)(e.a,{href:\"https://releasehub.com\",children:\"Release\"}),\" platform specifically aims to solve this problem directly by providing Environments-as-a-Service. This way, you get all the advantages of environments, without the costs or headache of Doing It Yourself, freeing you up to focus on the business and the product needs.\"]}),`\n`,(0,t.jsxs)(e.h3,{id:\"in-short\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#in-short\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"In short\\u2026\"]}),`\n`,(0,t.jsxs)(e.p,{children:[\"Remember: Nobody knows what they\\u2019re doing. And that\\u2019s okay. Nobody\\u2019s ever known what they\\u2019re doing; we\\u2019re all just stumbling around. But if we stumble with purpose, we can fall into something that works. If we fail fast, we get to success faster. Our methodologies in software development reflect this, but in practice, building supportive infrastructure is costly. If we can get to a place where we optimtimize failing fast by deploying early and often, we can more quickly find what our product needs to be. We can find success \",(0,t.jsx)(e.em,{children:\"without ever having to know\"}),\" what we\\u2019re doing.\"]}),`\n`,(0,t.jsxs)(e.h3,{id:\"ephemeral-environments\",children:[(0,t.jsx)(e.a,{className:\"anchor\",href:\"#ephemeral-environments\",children:(0,t.jsx)(e.span,{className:\"icon icon-link\"})}),\"Ephemeral Environments\"]}),`\n`,(0,t.jsxs)(e.p,{children:[\"Curious about Ephemeral Environments and what they can do for you? Check out \",(0,t.jsx)(e.a,{href:\"https://release.com/ephemeral-environments\",children:\"this article\"}),\" on what Ephemeral Environments are and what they can do for you.\"]})]})}function k(n={}){let{wrapper:e}=n.components||{};return e?(0,t.jsx)(e,Object.assign({},n,{children:(0,t.jsx)(l,n)})):l(n)}var T=k;return b(D);})();\n;return Component;"
        },
        "_id": "blog/posts/you-need-agile-devops-methodology.mdx",
        "_raw": {
          "sourceFilePath": "blog/posts/you-need-agile-devops-methodology.mdx",
          "sourceFileName": "you-need-agile-devops-methodology.mdx",
          "sourceFileDir": "blog/posts",
          "contentType": "mdx",
          "flattenedPath": "blog/posts/you-need-agile-devops-methodology"
        },
        "type": "BlogPost",
        "computedSlug": "you-need-agile-devops-methodology"
      },
      "documentHash": "1739393595031",
      "hasWarnings": false,
      "documentTypeName": "BlogPost"
    },
    "blog/posts/youre-invited-join-the-release-ai-technical-preview-program.mdx": {
      "document": {
        "title": "You’re Invited! Join the Release.ai Technical Preview Program",
        "summary": "We're excited to launch the Release.ai Technical Preview Program, aimed at simplifying infrastructure management.",
        "publishDate": "Wed Mar 13 2024 22:26:33 GMT+0000 (Coordinated Universal Time)",
        "author": "michael-poon",
        "readingTime": 3,
        "categories": [
          "ai",
          "kubernetes",
          "nvidia",
          "platform-engineering"
        ],
        "mainImage": "/blog-images/7073398697121b238d1a791ceb8ca403.jpg",
        "imageAlt": "You’re Invited! Join the Release.ai Technical Preview Program",
        "showCTA": true,
        "ctaCopy": "Simplify AI infrastructure like in the Release.ai Technical Preview Program with Release's ephemeral environments.",
        "ctaLink": "https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=youre-invited-join-the-release-ai-technical-preview-program",
        "relatedPosts": [
          ""
        ],
        "ogImage": "/blog-images/7073398697121b238d1a791ceb8ca403.jpg",
        "excerpt": "We're excited to launch the Release.ai Technical Preview Program, aimed at simplifying infrastructure management.",
        "tags": [
          "ai",
          "kubernetes",
          "nvidia",
          "platform-engineering"
        ],
        "ctaButton": "Try Release for Free",
        "body": {
          "raw": "\nToday we are excited to launch the Release.ai Technical Preview Program, aimed at simplifying infrastructure management for AI workloads. We’ve spoken with teams doing incredible work in AI, from building new foundational models to integrating custom trained models into products, and one thing is clear, the infrastructure is lagging behind. All across the industry we see amazing innovations pop up every week (as I’m writing this [Cognition AI is blowing up the internet with their Devin assistant](https://www.bloomberg.com/news/articles/2024-03-12/cognition-ai-is-a-peter-thiel-backed-coding-assistant)), and underneath the surface to make all this happen are infrastructure teams toiling away at accumulating GPUs, managing finicky clusters, investigating the latest timeouts, and making their finance folks bug-eyed at the bills being generated.\n\n[Release](http://release.com) is a leader in abstracting away infrastructure complexity for conventional SDLC processes, so that engineering teams can focus on shipping high quality products, faster. Now we are bringing that expertise to support AI engineering teams. To begin, we partnered with NVIDIA to simplify the setup and management of the NeMo framework infrastructure. Now you can use NeMo and Release to build and deploy models in a portable and cloud agnostic way on top of Kubernetes clusters we manage for you.\n\nWhile Release.ai simplifies AI infrastructure, you don’t have to choose between simplicity and control - everything on Release.ai is orchestrated in your cloud account on hardware that you own, simplifying toil while maintaining control. Currently we support training, fine-tuning, and inference workflows with more workflows and supported frameworks to come in the near future. \n\nWe opened access to our beta program for teams who build, fine tune and train AI models. Come build a new AI Infrastructure platform with us!\n\n[Sign up for Release.ai today!](https://release.ai/)\n",
          "code": "var Component=(()=>{var m=Object.create;var r=Object.defineProperty;var g=Object.getOwnPropertyDescriptor;var h=Object.getOwnPropertyNames;var d=Object.getPrototypeOf,p=Object.prototype.hasOwnProperty;var f=(t,e)=>()=>(e||t((e={exports:{}}).exports,e),e.exports),w=(t,e)=>{for(var i in e)r(t,i,{get:e[i],enumerable:!0})},s=(t,e,i,o)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let n of h(e))!p.call(t,n)&&n!==i&&r(t,n,{get:()=>e[n],enumerable:!(o=g(e,n))||o.enumerable});return t};var b=(t,e,i)=>(i=t!=null?m(d(t)):{},s(e||!t||!t.__esModule?r(i,\"default\",{value:t,enumerable:!0}):i,t)),y=t=>s(r({},\"__esModule\",{value:!0}),t);var l=f((R,c)=>{c.exports=_jsx_runtime});var x={};w(x,{default:()=>I,frontmatter:()=>v});var a=b(l()),v={title:\"You\\u2019re Invited! Join the Release.ai Technical Preview Program\",summary:\"We're excited to launch the Release.ai Technical Preview Program, aimed at simplifying infrastructure management.\",publishDate:\"Wed Mar 13 2024 22:26:33 GMT+0000 (Coordinated Universal Time)\",author:\"michael-poon\",readingTime:3,categories:[\"ai\",\"kubernetes\",\"nvidia\",\"platform-engineering\"],mainImage:\"/blog-images/7073398697121b238d1a791ceb8ca403.jpg\",imageAlt:\"You\\u2019re Invited! Join the Release.ai Technical Preview Program\",showCTA:!0,ctaCopy:\"Simplify AI infrastructure like in the Release.ai Technical Preview Program with Release's ephemeral environments.\",ctaLink:\"https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=youre-invited-join-the-release-ai-technical-preview-program\",relatedPosts:[\"\"],ogImage:\"/blog-images/7073398697121b238d1a791ceb8ca403.jpg\",excerpt:\"We're excited to launch the Release.ai Technical Preview Program, aimed at simplifying infrastructure management.\",tags:[\"ai\",\"kubernetes\",\"nvidia\",\"platform-engineering\"],ctaButton:\"Try Release for Free\"};function u(t){let e=Object.assign({p:\"p\",a:\"a\"},t.components);return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsxs)(e.p,{children:[\"Today we are excited to launch the Release.ai Technical Preview Program, aimed at simplifying infrastructure management for AI workloads. We\\u2019ve spoken with teams doing incredible work in AI, from building new foundational models to integrating custom trained models into products, and one thing is clear, the infrastructure is lagging behind. All across the industry we see amazing innovations pop up every week (as I\\u2019m writing this \",(0,a.jsx)(e.a,{href:\"https://www.bloomberg.com/news/articles/2024-03-12/cognition-ai-is-a-peter-thiel-backed-coding-assistant\",children:\"Cognition AI is blowing up the internet with their Devin assistant\"}),\"), and underneath the surface to make all this happen are infrastructure teams toiling away at accumulating GPUs, managing finicky clusters, investigating the latest timeouts, and making their finance folks bug-eyed at the bills being generated.\"]}),`\n`,(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.a,{href:\"http://release.com\",children:\"Release\"}),\" is a leader in abstracting away infrastructure complexity for conventional SDLC processes, so that engineering teams can focus on shipping high quality products, faster. Now we are bringing that expertise to support AI engineering teams. To begin, we partnered with NVIDIA to simplify the setup and management of the NeMo framework infrastructure. Now you can use NeMo and Release to build and deploy models in a portable and cloud agnostic way on top of Kubernetes clusters we manage for you.\"]}),`\n`,(0,a.jsx)(e.p,{children:\"While Release.ai simplifies AI infrastructure, you don\\u2019t have to choose between simplicity and control - everything on Release.ai is orchestrated in your cloud account on hardware that you own, simplifying toil while maintaining control. Currently we support training, fine-tuning, and inference workflows with more workflows and supported frameworks to come in the near future.\\xA0\"}),`\n`,(0,a.jsx)(e.p,{children:\"We opened access to our beta program for teams who build, fine tune and train AI models. Come build a new AI Infrastructure platform with us!\"}),`\n`,(0,a.jsx)(e.p,{children:(0,a.jsx)(e.a,{href:\"https://release.ai/\",children:\"Sign up for Release.ai today!\"})})]})}function k(t={}){let{wrapper:e}=t.components||{};return e?(0,a.jsx)(e,Object.assign({},t,{children:(0,a.jsx)(u,t)})):u(t)}var I=k;return y(x);})();\n;return Component;"
        },
        "_id": "blog/posts/youre-invited-join-the-release-ai-technical-preview-program.mdx",
        "_raw": {
          "sourceFilePath": "blog/posts/youre-invited-join-the-release-ai-technical-preview-program.mdx",
          "sourceFileName": "youre-invited-join-the-release-ai-technical-preview-program.mdx",
          "sourceFileDir": "blog/posts",
          "contentType": "mdx",
          "flattenedPath": "blog/posts/youre-invited-join-the-release-ai-technical-preview-program"
        },
        "type": "BlogPost",
        "computedSlug": "youre-invited-join-the-release-ai-technical-preview-program"
      },
      "documentHash": "1739393595031",
      "hasWarnings": false,
      "documentTypeName": "BlogPost"
    },
    "legal/content/privacy-policy.mdx": {
      "document": {
        "title": "Privacy Policy",
        "publishDate": "2024-02-26",
        "slug": "privacy-policy",
        "body": {
          "raw": "\n# Privacy Policy\n\n_Effective date: 26 Feb 2020_\n\n## 1. Introduction\n\nWelcome to TED Technology, Inc.  \n**TED Technology, Inc. (\"us\", \"we\", or \"our\")** operates https://release.com (hereinafter referred to as \"**Service**\").  \nOur Privacy Policy governs your visit to https://release.com, and explains how we collect, safeguard and disclose information that results from your use of our Service.  \nWe use your data to provide and improve Service. By using Service, you agree to the collection and use of information in accordance with this policy. Unless otherwise defined in this Privacy Policy, the terms used in this Privacy Policy have the same meanings as in our Terms and Conditions.  \nOur Terms and Conditions (\"**Terms**\") govern all use of our Service and together with the Privacy Policy constitutes your agreement with us (\"**agreement**\").\n\n## 2. Definitions\n\n**SERVICE** means the https://release.com website operated by TED Technology, Inc.  \n**PERSONAL DATA** means data about a living individual who can be identified from those data (or from those and other information either in our possession or likely to come into our possession).  \n**USAGE DATA** is data collected automatically either generated by the use of Service or from Service infrastructure itself (for example, the duration of a page visit).  \n**COOKIES** are small files stored on your device (computer or mobile device).  \n**DATA CONTROLLER** means a natural or legal person who (either alone or jointly or in common with other persons) determines the purposes for which and the manner in which any personal data are, or are to be, processed. For the purpose of this Privacy Policy, we are a Data Controller of your data.  \n**DATA PROCESSORS (OR SERVICE PROVIDERS)** means any natural or legal person who processes the data on behalf of the Data Controller. We may use the services of various Service Providers in order to process your data more effectively.  \n**DATA SUBJECT** is any living individual who is the subject of Personal Data.  \n**THE USER** is the individual using our Service. The User corresponds to the Data Subject, who is the subject of Personal Data.\n\n## 3. Information Collection and Use\n\nWe collect several different types of information for various purposes to provide and improve our Service to you.\n\n## 4. Types of Data Collected\n\n**Personal Data**  \nWhile using our Service, we may ask you to provide us with certain personally identifiable information that can be used to contact or identify you (\"**Personal Data**\"). Personally identifiable information may include, but is not limited to:  \n(a) Email address  \n(b) First name and last name  \n(c) Cookies and Usage Data\n\nWe may use your Personal Data to contact you with newsletters, marketing or promotional materials and other information that may be of interest to you. You may opt out of receiving any, or all, of these communications from us by following the unsubscribe link or by emailing at support@release.com.\n\n**Usage Data**  \nWe may also collect information that your browser sends whenever you visit our Service or when you access Service by or through a mobile device (\"**Usage Data**\").\n\nThis Usage Data may include information such as your computer's Internet Protocol address (e.g. IP address), browser type, browser version, the pages of our Service that you visit, the time and date of your visit, the time spent on those pages, unique device identifiers and other diagnostic data.\n\nWhen you access Service with a mobile device, this Usage Data may include information such as the type of mobile device you use, your mobile device unique ID, the IP address of your mobile device, your mobile operating system, the type of mobile Internet browser you use, unique device identifiers and other diagnostic data.\n\n**Tracking Cookies Data**  \nWe use cookies and similar tracking technologies to track the activity on our Service and we hold certain information.\n\nCookies are files with a small amount of data which may include an anonymous unique identifier. Cookies are sent to your browser from a website and stored on your device. Other tracking technologies are also used such as beacons, tags and scripts to collect and track information and to improve and analyze our Service.\n\nYou can instruct your browser to refuse all cookies or to indicate when a cookie is being sent. However, if you do not accept cookies, you may not be able to use some portions of our Service.\n\nExamples of Cookies we use:  \n(a) **Session Cookies**: We use Session Cookies to operate our Service.  \n(b) **Preference Cookies**: We use Preference Cookies to remember your preferences and various settings.  \n(c) **Security Cookies**: We use Security Cookies for security purposes.  \n(d) **Advertising Cookies**: Advertising Cookies are used to serve you with advertisements that may be relevant to you and your interests.\n\n## 5. Use of Data\n\nTED Technology, Inc. uses the collected data for various purposes:  \n(a) to provide and maintain our Service;  \n(b) to notify you about changes to our Service;  \n(c) to allow you to participate in interactive features of our Service when you choose to do so;  \n(d) to provide customer support;  \n(e) to gather analysis or valuable information so that we can improve our Service;  \n(f) to monitor the usage of our Service;  \n(g) to detect, prevent and address technical issues;  \n(h) to fulfill any other purpose for which you provide it;  \n(i) to carry out our obligations and enforce our rights arising from any contracts entered into between you and us, including for billing and collection;  \n(j) to provide you with notices about your account and/or subscription, including expiration and renewal notices, email-instructions, etc.;  \n(k) to provide you with news, special offers and general information about other goods, services and events which we offer that are similar to those that you have already purchased or enquired about unless you have opted not to receive such information;  \n(l) in any other way we may describe when you provide the information;  \n(m) for any other purpose with your consent.\n\n## 6. Retention of Data\n\nWe will retain your Personal Data only for as long as is necessary for the purposes set out in this Privacy Policy. We will retain and use your Personal Data to the extent necessary to comply with our legal obligations (for example, if we are required to retain your data to comply with applicable laws), resolve disputes, and enforce our legal agreements and policies.\n\nWe will also retain Usage Data for internal analysis purposes. Usage Data is generally retained for a shorter period, except when this data is used to strengthen the security or to improve the functionality of our Service, or we are legally obligated to retain this data for longer time periods.\n\n## 7. Transfer of Data\n\nYour information, including Personal Data, may be transferred to – and maintained on – computers located outside of your state, province, country or other governmental jurisdiction where the data protection laws may differ from those of your jurisdiction.\n\nIf you are located outside United States and choose to provide information to us, please note that we transfer the data, including Personal Data, to United States and process it there.\n\nYour consent to this Privacy Policy followed by your submission of such information represents your agreement to that transfer.\n\nTED Technology, Inc. will take all the steps reasonably necessary to ensure that your data is treated securely and in accordance with this Privacy Policy and no transfer of your Personal Data will take place to an organization or a country unless there are adequate controls in place including the security of your data and other personal information.\n\n## 8. Disclosure of Data\n\n**Business Transaction**  \nIf TED Technology, Inc. is involved in a merger, acquisition or asset sale, your Personal Data may be transferred. We will provide notice before your Personal Data is transferred and becomes subject to a different Privacy Policy.\n\n**Disclosure for Law Enforcement**  \nUnder certain circumstances, TED Technology, Inc. may be required to disclose your Personal Data if required to do so by law or in response to valid requests by public authorities (e.g. a court or a government agency).\n\n**Legal Requirements**  \nTED Technology, Inc. may disclose your Personal Data in the good faith belief that such action is necessary to:  \n(a) to comply with a legal obligation;  \n(b) to protect and defend the rights or property of TED Technology, Inc.;  \n(c) to prevent or investigate possible wrongdoing in connection with the Service;  \n(d) to protect the personal safety of users of the Service or the public;  \n(e) to protect against legal liability.\n\n## 9. Security of Data\n\nThe security of your data is important to us but remember that no method of transmission over the Internet or method of electronic storage is 100% secure. While we strive to use commercially acceptable means to protect your Personal Data, we cannot guarantee its absolute security.\n\n## 10. Your Data Protection Rights Under General Data Protection Regulation (GDPR)\n\nIf you are a resident of the European Economic Area (EEA), you have certain data protection rights. TED Technology, Inc. aims to take reasonable steps to allow you to correct, amend, delete or limit the use of your Personal Data.\n\nIf you wish to be informed about what Personal Data we hold about you and if you want it to be removed from our systems, please contact us.\n\nIn certain circumstances, you have the following data protection rights:  \n(a) the right to access, update or delete the information we have on you;  \n(b) the right of rectification. You have the right to have your information rectified if that information is inaccurate or incomplete;  \n(c) the right to object. You have the right to object to our processing of your Personal Data;  \n(d) the right of restriction. You have the right to request that we restrict the processing of your personal information;  \n(e) the right to data portability. You have the right to be provided with a copy of your Personal Data in a structured, machine-readable and commonly used format;  \n(f) the right to withdraw consent. You also have the right to withdraw your consent at any time where we rely on your consent to process your personal information;\n\nPlease note that we may ask you to verify your identity before responding to such requests. Please note, we may not able to provide Service without some necessary data.\n\nYou have the right to complain to a Data Protection Authority about our collection and use of your Personal Data. For more information, please contact your local data protection authority in the European Economic Area (EEA).\n\n## 11. Your Data Protection Rights under the California Privacy Protection Act (CalOPPA)\n\nCalOPPA is the first state law in the nation to require commercial websites and online services to post a privacy policy. The law's reach stretches well beyond California to require a person or company in the United States (and conceivable the world) that operates websites collecting personally identifiable information from California consumers to post a conspicuous privacy policy on its website stating exactly the information being collected and those individuals with whom it is being shared, and to comply with this policy.\n\nAccording to CalOPPA we agree to the following:  \n(a) users can visit our site anonymously;  \n(b) our Privacy Policy link includes the word \"Privacy\", and can easily be found on the page specified above on the home page of our website;  \n(c) users will be notified of any privacy policy changes on our Privacy Policy Page;  \n(d) users are able to change their personal information by emailing us at support@release.com.\n\nOur Policy on \"Do Not Track\" Signals:  \nWe honor Do Not Track signals and do not track, plant cookies, or use advertising when a Do Not Track browser mechanism is in place. Do Not Track is a preference you can set in your web browser to inform websites that you do not want to be tracked.\n\nYou can enable or disable Do Not Track by visiting the Preferences or Settings page of your web browser.\n\n## 12. Your Data Protection Rights under the California Consumer Privacy Act (CCPA)\n\nIf you are a California resident, you are entitled to learn what data we collect about you, ask to delete your data and not to sell (share) it. To exercise your data protection rights, you can make certain requests and ask us:\n\n(a) **What personal information we have about you**. If you make this request, we will return to you:  \n(i) The categories of personal information we have collected about you.  \n(ii) The categories of sources from which we collect your personal information.  \n(iii) The business or commercial purpose for collecting or selling your personal information.  \n(iv) The categories of third parties with whom we share personal information.  \n(v) The specific pieces of personal information we have collected about you.  \n(vi) A list of categories of personal information that we have sold, along with the category of any other company we sold it to. If we have not sold your personal information, we will inform you of that fact.  \n(vii) A list of categories of personal information that we have disclosed for a business purpose, along with the category of any other company we shared it with.\n\nPlease note, you are entitled to ask us to provide you with this information up to two times in a rolling twelve-month period. When you make this request, the information provided may be limited to the personal information we collected about you in the previous 12 months.\n\n(b) **To delete your personal information**. If you make this request, we will delete the personal information we hold about you as of the date of your request from our records and direct any service providers to do the same. In some cases, deletion may be accomplished through de-identification of the information. If you choose to delete your personal information, you may not be able to use certain functions that require your personal information to operate.\n\n(c) **To stop selling your personal information**. We don't sell or rent your personal information to any third parties for any purpose. You are the only owner of your Personal Data and can request disclosure or deletion at any time.\n\nPlease note, if you ask us to delete or stop selling your data, it may impact your experience with us, and you may not be able to participate in certain programs or membership services which require the usage of your personal information to function. But in no circumstances, we will discriminate against you for exercising your rights.\n\nTo exercise your California data protection rights described above, please send your request(s) by one of the following means:  \nBy email: support@release.com\n\nYour data protection rights, described above, are covered by the CCPA, short for the California Consumer Privacy Act. To find out more, visit the official California Legislative Information website. The CCPA took effect on 01/01/2020.\n\n## 13. Service Providers\n\nWe may employ third party companies and individuals to facilitate our Service (\"**Service Providers**\"), provide Service on our behalf, perform Service-related services or assist us in analysing how our Service is used.\n\nThese third parties have access to your Personal Data only to perform these tasks on our behalf and are obligated not to disclose or use it for any other purpose.\n\n## 14. Analytics\n\nWe may use third-party Service Providers to monitor and analyze the use of our Service.\n\n**Mixpanel**  \nMixpanel is provided by Mixpanel Inc.  \nYou can prevent Mixpanel from using your information for analytics purposes by opting-out. To opt-out of Mixpanel service, please visit this page: https://mixpanel.com/optout/  \nFor more information on what type of information Mixpanel collects, please visit the Terms of Use page of Mixpanel: https://mixpanel.com/terms/\n\n## 15. CI/CD tools\n\nWe may use third-party Service Providers to automate the development process of our Service.\n\n**GitHub**  \nGitHub is provided by GitHub, Inc.  \nGitHub is a development platform to host and review code, manage projects, and build software.  \nFor more information on what data GitHub collects for what purpose and how the protection of the data is ensured, please visit GitHub Privacy Policy page: https://help.github.com/en/articles/github-privacy-statement.\n\n## 16. Payments\n\nWe may provide paid products and/or services within Service. In that case, we use third-party services for payment processing (e.g. payment processors).\n\nWe will not store or collect your payment card details. That information is provided directly to our third-party payment processors whose use of your personal information is governed by their Privacy Policy. These payment processors adhere to the standards set by PCI-DSS as managed by the PCI Security Standards Council, which is a joint effort of brands like Visa, Mastercard, American Express and Discover. PCI-DSS requirements help ensure the secure handling of payment information.\n\nThe payment processors we work with are:  \n**Stripe**: Their Privacy Policy can be viewed at: https://stripe.com/us/privacy\n\n## 17. Links to Other Sites\n\nOur Service may contain links to other sites that are not operated by us. If you click a third party link, you will be directed to that third party's site. We strongly advise you to review the Privacy Policy of every site you visit.\n\nWe have no control over and assume no responsibility for the content, privacy policies or practices of any third party sites or services.\n\n## 18. Children's Privacy\n\nOur Services are not intended for use by children under the age of 13 (\"**Children**\").\n\nWe do not knowingly collect personally identifiable information from Children under 13. If you become aware that a Child has provided us with Personal Data, please contact us. If we become aware that we have collected Personal Data from Children without verification of parental consent, we take steps to remove that information from our servers.\n\n## 19. Changes to This Privacy Policy\n\nWe may update our Privacy Policy from time to time. We will notify you of any changes by posting the new Privacy Policy on this page.\n\nWe will let you know via email and/or a prominent notice on our Service, prior to the change becoming effective and update \"effective date\" at the top of this Privacy Policy.\n\nYou are advised to review this Privacy Policy periodically for any changes. Changes to this Privacy Policy are effective when they are posted on this page.\n\n## 20. Contact Us\n\nIf you have any questions about this Privacy Policy, please contact us at support@release.com.\n",
          "code": "var Component=(()=>{var d=Object.create;var n=Object.defineProperty;var u=Object.getOwnPropertyDescriptor;var p=Object.getOwnPropertyNames;var y=Object.getPrototypeOf,m=Object.prototype.hasOwnProperty;var f=(r,e)=>()=>(e||r((e={exports:{}}).exports,e),e.exports),v=(r,e)=>{for(var t in e)n(r,t,{get:e[t],enumerable:!0})},s=(r,e,t,a)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let i of p(e))!m.call(r,i)&&i!==t&&n(r,i,{get:()=>e[i],enumerable:!(a=u(e,i))||a.enumerable});return r};var b=(r,e,t)=>(t=r!=null?d(y(r)):{},s(e||!r||!r.__esModule?n(t,\"default\",{value:r,enumerable:!0}):t,r)),g=r=>s(n({},\"__esModule\",{value:!0}),r);var l=f((T,c)=>{c.exports=_jsx_runtime});var D={};v(D,{default:()=>k,frontmatter:()=>w});var o=b(l()),w={title:\"Privacy Policy\",publishDate:\"2024-02-26\",slug:\"privacy-policy\"};function h(r){let e=Object.assign({h1:\"h1\",a:\"a\",span:\"span\",p:\"p\",em:\"em\",h2:\"h2\",br:\"br\",strong:\"strong\"},r.components);return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsxs)(e.h1,{id:\"privacy-policy\",children:[(0,o.jsx)(e.a,{className:\"anchor\",href:\"#privacy-policy\",children:(0,o.jsx)(e.span,{className:\"icon icon-link\"})}),\"Privacy Policy\"]}),`\n`,(0,o.jsx)(e.p,{children:(0,o.jsx)(e.em,{children:\"Effective date: 26 Feb 2020\"})}),`\n`,(0,o.jsxs)(e.h2,{id:\"1-introduction\",children:[(0,o.jsx)(e.a,{className:\"anchor\",href:\"#1-introduction\",children:(0,o.jsx)(e.span,{className:\"icon icon-link\"})}),\"1. Introduction\"]}),`\n`,(0,o.jsxs)(e.p,{children:[\"Welcome to TED Technology, Inc.\",(0,o.jsx)(e.br,{}),`\n`,(0,o.jsx)(e.strong,{children:'TED Technology, Inc. (\"us\", \"we\", or \"our\")'}),\" operates \",(0,o.jsx)(e.a,{href:\"https://release.com\",children:\"https://release.com\"}),' (hereinafter referred to as \"',(0,o.jsx)(e.strong,{children:\"Service\"}),'\").',(0,o.jsx)(e.br,{}),`\n`,\"Our Privacy Policy governs your visit to \",(0,o.jsx)(e.a,{href:\"https://release.com\",children:\"https://release.com\"}),\", and explains how we collect, safeguard and disclose information that results from your use of our Service.\",(0,o.jsx)(e.br,{}),`\n`,\"We use your data to provide and improve Service. By using Service, you agree to the collection and use of information in accordance with this policy. Unless otherwise defined in this Privacy Policy, the terms used in this Privacy Policy have the same meanings as in our Terms and Conditions.\",(0,o.jsx)(e.br,{}),`\n`,'Our Terms and Conditions (\"',(0,o.jsx)(e.strong,{children:\"Terms\"}),'\") govern all use of our Service and together with the Privacy Policy constitutes your agreement with us (\"',(0,o.jsx)(e.strong,{children:\"agreement\"}),'\").']}),`\n`,(0,o.jsxs)(e.h2,{id:\"2-definitions\",children:[(0,o.jsx)(e.a,{className:\"anchor\",href:\"#2-definitions\",children:(0,o.jsx)(e.span,{className:\"icon icon-link\"})}),\"2. Definitions\"]}),`\n`,(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:\"SERVICE\"}),\" means the \",(0,o.jsx)(e.a,{href:\"https://release.com\",children:\"https://release.com\"}),\" website operated by TED Technology, Inc.\",(0,o.jsx)(e.br,{}),`\n`,(0,o.jsx)(e.strong,{children:\"PERSONAL DATA\"}),\" means data about a living individual who can be identified from those data (or from those and other information either in our possession or likely to come into our possession).\",(0,o.jsx)(e.br,{}),`\n`,(0,o.jsx)(e.strong,{children:\"USAGE DATA\"}),\" is data collected automatically either generated by the use of Service or from Service infrastructure itself (for example, the duration of a page visit).\",(0,o.jsx)(e.br,{}),`\n`,(0,o.jsx)(e.strong,{children:\"COOKIES\"}),\" are small files stored on your device (computer or mobile device).\",(0,o.jsx)(e.br,{}),`\n`,(0,o.jsx)(e.strong,{children:\"DATA CONTROLLER\"}),\" means a natural or legal person who (either alone or jointly or in common with other persons) determines the purposes for which and the manner in which any personal data are, or are to be, processed. For the purpose of this Privacy Policy, we are a Data Controller of your data.\",(0,o.jsx)(e.br,{}),`\n`,(0,o.jsx)(e.strong,{children:\"DATA PROCESSORS (OR SERVICE PROVIDERS)\"}),\" means any natural or legal person who processes the data on behalf of the Data Controller. We may use the services of various Service Providers in order to process your data more effectively.\",(0,o.jsx)(e.br,{}),`\n`,(0,o.jsx)(e.strong,{children:\"DATA SUBJECT\"}),\" is any living individual who is the subject of Personal Data.\",(0,o.jsx)(e.br,{}),`\n`,(0,o.jsx)(e.strong,{children:\"THE USER\"}),\" is the individual using our Service. The User corresponds to the Data Subject, who is the subject of Personal Data.\"]}),`\n`,(0,o.jsxs)(e.h2,{id:\"3-information-collection-and-use\",children:[(0,o.jsx)(e.a,{className:\"anchor\",href:\"#3-information-collection-and-use\",children:(0,o.jsx)(e.span,{className:\"icon icon-link\"})}),\"3. Information Collection and Use\"]}),`\n`,(0,o.jsx)(e.p,{children:\"We collect several different types of information for various purposes to provide and improve our Service to you.\"}),`\n`,(0,o.jsxs)(e.h2,{id:\"4-types-of-data-collected\",children:[(0,o.jsx)(e.a,{className:\"anchor\",href:\"#4-types-of-data-collected\",children:(0,o.jsx)(e.span,{className:\"icon icon-link\"})}),\"4. Types of Data Collected\"]}),`\n`,(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:\"Personal Data\"}),(0,o.jsx)(e.br,{}),`\n`,'While using our Service, we may ask you to provide us with certain personally identifiable information that can be used to contact or identify you (\"',(0,o.jsx)(e.strong,{children:\"Personal Data\"}),'\"). Personally identifiable information may include, but is not limited to:',(0,o.jsx)(e.br,{}),`\n`,\"(a) Email address\",(0,o.jsx)(e.br,{}),`\n`,\"(b) First name and last name\",(0,o.jsx)(e.br,{}),`\n`,\"(c) Cookies and Usage Data\"]}),`\n`,(0,o.jsxs)(e.p,{children:[\"We may use your Personal Data to contact you with newsletters, marketing or promotional materials and other information that may be of interest to you. You may opt out of receiving any, or all, of these communications from us by following the unsubscribe link or by emailing at \",(0,o.jsx)(e.a,{href:\"mailto:support@release.com\",children:\"support@release.com\"}),\".\"]}),`\n`,(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:\"Usage Data\"}),(0,o.jsx)(e.br,{}),`\n`,'We may also collect information that your browser sends whenever you visit our Service or when you access Service by or through a mobile device (\"',(0,o.jsx)(e.strong,{children:\"Usage Data\"}),'\").']}),`\n`,(0,o.jsx)(e.p,{children:\"This Usage Data may include information such as your computer's Internet Protocol address (e.g. IP address), browser type, browser version, the pages of our Service that you visit, the time and date of your visit, the time spent on those pages, unique device identifiers and other diagnostic data.\"}),`\n`,(0,o.jsx)(e.p,{children:\"When you access Service with a mobile device, this Usage Data may include information such as the type of mobile device you use, your mobile device unique ID, the IP address of your mobile device, your mobile operating system, the type of mobile Internet browser you use, unique device identifiers and other diagnostic data.\"}),`\n`,(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:\"Tracking Cookies Data\"}),(0,o.jsx)(e.br,{}),`\n`,\"We use cookies and similar tracking technologies to track the activity on our Service and we hold certain information.\"]}),`\n`,(0,o.jsx)(e.p,{children:\"Cookies are files with a small amount of data which may include an anonymous unique identifier. Cookies are sent to your browser from a website and stored on your device. Other tracking technologies are also used such as beacons, tags and scripts to collect and track information and to improve and analyze our Service.\"}),`\n`,(0,o.jsx)(e.p,{children:\"You can instruct your browser to refuse all cookies or to indicate when a cookie is being sent. However, if you do not accept cookies, you may not be able to use some portions of our Service.\"}),`\n`,(0,o.jsxs)(e.p,{children:[\"Examples of Cookies we use:\",(0,o.jsx)(e.br,{}),`\n`,\"(a) \",(0,o.jsx)(e.strong,{children:\"Session Cookies\"}),\": We use Session Cookies to operate our Service.\",(0,o.jsx)(e.br,{}),`\n`,\"(b) \",(0,o.jsx)(e.strong,{children:\"Preference Cookies\"}),\": We use Preference Cookies to remember your preferences and various settings.\",(0,o.jsx)(e.br,{}),`\n`,\"(c) \",(0,o.jsx)(e.strong,{children:\"Security Cookies\"}),\": We use Security Cookies for security purposes.\",(0,o.jsx)(e.br,{}),`\n`,\"(d) \",(0,o.jsx)(e.strong,{children:\"Advertising Cookies\"}),\": Advertising Cookies are used to serve you with advertisements that may be relevant to you and your interests.\"]}),`\n`,(0,o.jsxs)(e.h2,{id:\"5-use-of-data\",children:[(0,o.jsx)(e.a,{className:\"anchor\",href:\"#5-use-of-data\",children:(0,o.jsx)(e.span,{className:\"icon icon-link\"})}),\"5. Use of Data\"]}),`\n`,(0,o.jsxs)(e.p,{children:[\"TED Technology, Inc. uses the collected data for various purposes:\",(0,o.jsx)(e.br,{}),`\n`,\"(a) to provide and maintain our Service;\",(0,o.jsx)(e.br,{}),`\n`,\"(b) to notify you about changes to our Service;\",(0,o.jsx)(e.br,{}),`\n`,\"(c) to allow you to participate in interactive features of our Service when you choose to do so;\",(0,o.jsx)(e.br,{}),`\n`,\"(d) to provide customer support;\",(0,o.jsx)(e.br,{}),`\n`,\"(e) to gather analysis or valuable information so that we can improve our Service;\",(0,o.jsx)(e.br,{}),`\n`,\"(f) to monitor the usage of our Service;\",(0,o.jsx)(e.br,{}),`\n`,\"(g) to detect, prevent and address technical issues;\",(0,o.jsx)(e.br,{}),`\n`,\"(h) to fulfill any other purpose for which you provide it;\",(0,o.jsx)(e.br,{}),`\n`,\"(i) to carry out our obligations and enforce our rights arising from any contracts entered into between you and us, including for billing and collection;\",(0,o.jsx)(e.br,{}),`\n`,\"(j) to provide you with notices about your account and/or subscription, including expiration and renewal notices, email-instructions, etc.;\",(0,o.jsx)(e.br,{}),`\n`,\"(k) to provide you with news, special offers and general information about other goods, services and events which we offer that are similar to those that you have already purchased or enquired about unless you have opted not to receive such information;\",(0,o.jsx)(e.br,{}),`\n`,\"(l) in any other way we may describe when you provide the information;\",(0,o.jsx)(e.br,{}),`\n`,\"(m) for any other purpose with your consent.\"]}),`\n`,(0,o.jsxs)(e.h2,{id:\"6-retention-of-data\",children:[(0,o.jsx)(e.a,{className:\"anchor\",href:\"#6-retention-of-data\",children:(0,o.jsx)(e.span,{className:\"icon icon-link\"})}),\"6. Retention of Data\"]}),`\n`,(0,o.jsx)(e.p,{children:\"We will retain your Personal Data only for as long as is necessary for the purposes set out in this Privacy Policy. We will retain and use your Personal Data to the extent necessary to comply with our legal obligations (for example, if we are required to retain your data to comply with applicable laws), resolve disputes, and enforce our legal agreements and policies.\"}),`\n`,(0,o.jsx)(e.p,{children:\"We will also retain Usage Data for internal analysis purposes. Usage Data is generally retained for a shorter period, except when this data is used to strengthen the security or to improve the functionality of our Service, or we are legally obligated to retain this data for longer time periods.\"}),`\n`,(0,o.jsxs)(e.h2,{id:\"7-transfer-of-data\",children:[(0,o.jsx)(e.a,{className:\"anchor\",href:\"#7-transfer-of-data\",children:(0,o.jsx)(e.span,{className:\"icon icon-link\"})}),\"7. Transfer of Data\"]}),`\n`,(0,o.jsx)(e.p,{children:\"Your information, including Personal Data, may be transferred to \\u2013 and maintained on \\u2013 computers located outside of your state, province, country or other governmental jurisdiction where the data protection laws may differ from those of your jurisdiction.\"}),`\n`,(0,o.jsx)(e.p,{children:\"If you are located outside United States and choose to provide information to us, please note that we transfer the data, including Personal Data, to United States and process it there.\"}),`\n`,(0,o.jsx)(e.p,{children:\"Your consent to this Privacy Policy followed by your submission of such information represents your agreement to that transfer.\"}),`\n`,(0,o.jsx)(e.p,{children:\"TED Technology, Inc. will take all the steps reasonably necessary to ensure that your data is treated securely and in accordance with this Privacy Policy and no transfer of your Personal Data will take place to an organization or a country unless there are adequate controls in place including the security of your data and other personal information.\"}),`\n`,(0,o.jsxs)(e.h2,{id:\"8-disclosure-of-data\",children:[(0,o.jsx)(e.a,{className:\"anchor\",href:\"#8-disclosure-of-data\",children:(0,o.jsx)(e.span,{className:\"icon icon-link\"})}),\"8. Disclosure of Data\"]}),`\n`,(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:\"Business Transaction\"}),(0,o.jsx)(e.br,{}),`\n`,\"If TED Technology, Inc. is involved in a merger, acquisition or asset sale, your Personal Data may be transferred. We will provide notice before your Personal Data is transferred and becomes subject to a different Privacy Policy.\"]}),`\n`,(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:\"Disclosure for Law Enforcement\"}),(0,o.jsx)(e.br,{}),`\n`,\"Under certain circumstances, TED Technology, Inc. may be required to disclose your Personal Data if required to do so by law or in response to valid requests by public authorities (e.g. a court or a government agency).\"]}),`\n`,(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:\"Legal Requirements\"}),(0,o.jsx)(e.br,{}),`\n`,\"TED Technology, Inc. may disclose your Personal Data in the good faith belief that such action is necessary to:\",(0,o.jsx)(e.br,{}),`\n`,\"(a) to comply with a legal obligation;\",(0,o.jsx)(e.br,{}),`\n`,\"(b) to protect and defend the rights or property of TED Technology, Inc.;\",(0,o.jsx)(e.br,{}),`\n`,\"(c) to prevent or investigate possible wrongdoing in connection with the Service;\",(0,o.jsx)(e.br,{}),`\n`,\"(d) to protect the personal safety of users of the Service or the public;\",(0,o.jsx)(e.br,{}),`\n`,\"(e) to protect against legal liability.\"]}),`\n`,(0,o.jsxs)(e.h2,{id:\"9-security-of-data\",children:[(0,o.jsx)(e.a,{className:\"anchor\",href:\"#9-security-of-data\",children:(0,o.jsx)(e.span,{className:\"icon icon-link\"})}),\"9. Security of Data\"]}),`\n`,(0,o.jsx)(e.p,{children:\"The security of your data is important to us but remember that no method of transmission over the Internet or method of electronic storage is 100% secure. While we strive to use commercially acceptable means to protect your Personal Data, we cannot guarantee its absolute security.\"}),`\n`,(0,o.jsxs)(e.h2,{id:\"10-your-data-protection-rights-under-general-data-protection-regulation-gdpr\",children:[(0,o.jsx)(e.a,{className:\"anchor\",href:\"#10-your-data-protection-rights-under-general-data-protection-regulation-gdpr\",children:(0,o.jsx)(e.span,{className:\"icon icon-link\"})}),\"10. Your Data Protection Rights Under General Data Protection Regulation (GDPR)\"]}),`\n`,(0,o.jsx)(e.p,{children:\"If you are a resident of the European Economic Area (EEA), you have certain data protection rights. TED Technology, Inc. aims to take reasonable steps to allow you to correct, amend, delete or limit the use of your Personal Data.\"}),`\n`,(0,o.jsx)(e.p,{children:\"If you wish to be informed about what Personal Data we hold about you and if you want it to be removed from our systems, please contact us.\"}),`\n`,(0,o.jsxs)(e.p,{children:[\"In certain circumstances, you have the following data protection rights:\",(0,o.jsx)(e.br,{}),`\n`,\"(a) the right to access, update or delete the information we have on you;\",(0,o.jsx)(e.br,{}),`\n`,\"(b) the right of rectification. You have the right to have your information rectified if that information is inaccurate or incomplete;\",(0,o.jsx)(e.br,{}),`\n`,\"(c) the right to object. You have the right to object to our processing of your Personal Data;\",(0,o.jsx)(e.br,{}),`\n`,\"(d) the right of restriction. You have the right to request that we restrict the processing of your personal information;\",(0,o.jsx)(e.br,{}),`\n`,\"(e) the right to data portability. You have the right to be provided with a copy of your Personal Data in a structured, machine-readable and commonly used format;\",(0,o.jsx)(e.br,{}),`\n`,\"(f) the right to withdraw consent. You also have the right to withdraw your consent at any time where we rely on your consent to process your personal information;\"]}),`\n`,(0,o.jsx)(e.p,{children:\"Please note that we may ask you to verify your identity before responding to such requests. Please note, we may not able to provide Service without some necessary data.\"}),`\n`,(0,o.jsx)(e.p,{children:\"You have the right to complain to a Data Protection Authority about our collection and use of your Personal Data. For more information, please contact your local data protection authority in the European Economic Area (EEA).\"}),`\n`,(0,o.jsxs)(e.h2,{id:\"11-your-data-protection-rights-under-the-california-privacy-protection-act-caloppa\",children:[(0,o.jsx)(e.a,{className:\"anchor\",href:\"#11-your-data-protection-rights-under-the-california-privacy-protection-act-caloppa\",children:(0,o.jsx)(e.span,{className:\"icon icon-link\"})}),\"11. Your Data Protection Rights under the California Privacy Protection Act (CalOPPA)\"]}),`\n`,(0,o.jsx)(e.p,{children:\"CalOPPA is the first state law in the nation to require commercial websites and online services to post a privacy policy. The law's reach stretches well beyond California to require a person or company in the United States (and conceivable the world) that operates websites collecting personally identifiable information from California consumers to post a conspicuous privacy policy on its website stating exactly the information being collected and those individuals with whom it is being shared, and to comply with this policy.\"}),`\n`,(0,o.jsxs)(e.p,{children:[\"According to CalOPPA we agree to the following:\",(0,o.jsx)(e.br,{}),`\n`,\"(a) users can visit our site anonymously;\",(0,o.jsx)(e.br,{}),`\n`,'(b) our Privacy Policy link includes the word \"Privacy\", and can easily be found on the page specified above on the home page of our website;',(0,o.jsx)(e.br,{}),`\n`,\"(c) users will be notified of any privacy policy changes on our Privacy Policy Page;\",(0,o.jsx)(e.br,{}),`\n`,\"(d) users are able to change their personal information by emailing us at \",(0,o.jsx)(e.a,{href:\"mailto:support@release.com\",children:\"support@release.com\"}),\".\"]}),`\n`,(0,o.jsxs)(e.p,{children:['Our Policy on \"Do Not Track\" Signals:',(0,o.jsx)(e.br,{}),`\n`,\"We honor Do Not Track signals and do not track, plant cookies, or use advertising when a Do Not Track browser mechanism is in place. Do Not Track is a preference you can set in your web browser to inform websites that you do not want to be tracked.\"]}),`\n`,(0,o.jsx)(e.p,{children:\"You can enable or disable Do Not Track by visiting the Preferences or Settings page of your web browser.\"}),`\n`,(0,o.jsxs)(e.h2,{id:\"12-your-data-protection-rights-under-the-california-consumer-privacy-act-ccpa\",children:[(0,o.jsx)(e.a,{className:\"anchor\",href:\"#12-your-data-protection-rights-under-the-california-consumer-privacy-act-ccpa\",children:(0,o.jsx)(e.span,{className:\"icon icon-link\"})}),\"12. Your Data Protection Rights under the California Consumer Privacy Act (CCPA)\"]}),`\n`,(0,o.jsx)(e.p,{children:\"If you are a California resident, you are entitled to learn what data we collect about you, ask to delete your data and not to sell (share) it. To exercise your data protection rights, you can make certain requests and ask us:\"}),`\n`,(0,o.jsxs)(e.p,{children:[\"(a) \",(0,o.jsx)(e.strong,{children:\"What personal information we have about you\"}),\". If you make this request, we will return to you:\",(0,o.jsx)(e.br,{}),`\n`,\"(i) The categories of personal information we have collected about you.\",(0,o.jsx)(e.br,{}),`\n`,\"(ii) The categories of sources from which we collect your personal information.\",(0,o.jsx)(e.br,{}),`\n`,\"(iii) The business or commercial purpose for collecting or selling your personal information.\",(0,o.jsx)(e.br,{}),`\n`,\"(iv) The categories of third parties with whom we share personal information.\",(0,o.jsx)(e.br,{}),`\n`,\"(v) The specific pieces of personal information we have collected about you.\",(0,o.jsx)(e.br,{}),`\n`,\"(vi) A list of categories of personal information that we have sold, along with the category of any other company we sold it to. If we have not sold your personal information, we will inform you of that fact.\",(0,o.jsx)(e.br,{}),`\n`,\"(vii) A list of categories of personal information that we have disclosed for a business purpose, along with the category of any other company we shared it with.\"]}),`\n`,(0,o.jsx)(e.p,{children:\"Please note, you are entitled to ask us to provide you with this information up to two times in a rolling twelve-month period. When you make this request, the information provided may be limited to the personal information we collected about you in the previous 12 months.\"}),`\n`,(0,o.jsxs)(e.p,{children:[\"(b) \",(0,o.jsx)(e.strong,{children:\"To delete your personal information\"}),\". If you make this request, we will delete the personal information we hold about you as of the date of your request from our records and direct any service providers to do the same. In some cases, deletion may be accomplished through de-identification of the information. If you choose to delete your personal information, you may not be able to use certain functions that require your personal information to operate.\"]}),`\n`,(0,o.jsxs)(e.p,{children:[\"(c) \",(0,o.jsx)(e.strong,{children:\"To stop selling your personal information\"}),\". We don't sell or rent your personal information to any third parties for any purpose. You are the only owner of your Personal Data and can request disclosure or deletion at any time.\"]}),`\n`,(0,o.jsx)(e.p,{children:\"Please note, if you ask us to delete or stop selling your data, it may impact your experience with us, and you may not be able to participate in certain programs or membership services which require the usage of your personal information to function. But in no circumstances, we will discriminate against you for exercising your rights.\"}),`\n`,(0,o.jsxs)(e.p,{children:[\"To exercise your California data protection rights described above, please send your request(s) by one of the following means:\",(0,o.jsx)(e.br,{}),`\n`,\"By email: \",(0,o.jsx)(e.a,{href:\"mailto:support@release.com\",children:\"support@release.com\"})]}),`\n`,(0,o.jsx)(e.p,{children:\"Your data protection rights, described above, are covered by the CCPA, short for the California Consumer Privacy Act. To find out more, visit the official California Legislative Information website. The CCPA took effect on 01/01/2020.\"}),`\n`,(0,o.jsxs)(e.h2,{id:\"13-service-providers\",children:[(0,o.jsx)(e.a,{className:\"anchor\",href:\"#13-service-providers\",children:(0,o.jsx)(e.span,{className:\"icon icon-link\"})}),\"13. Service Providers\"]}),`\n`,(0,o.jsxs)(e.p,{children:['We may employ third party companies and individuals to facilitate our Service (\"',(0,o.jsx)(e.strong,{children:\"Service Providers\"}),'\"), provide Service on our behalf, perform Service-related services or assist us in analysing how our Service is used.']}),`\n`,(0,o.jsx)(e.p,{children:\"These third parties have access to your Personal Data only to perform these tasks on our behalf and are obligated not to disclose or use it for any other purpose.\"}),`\n`,(0,o.jsxs)(e.h2,{id:\"14-analytics\",children:[(0,o.jsx)(e.a,{className:\"anchor\",href:\"#14-analytics\",children:(0,o.jsx)(e.span,{className:\"icon icon-link\"})}),\"14. Analytics\"]}),`\n`,(0,o.jsx)(e.p,{children:\"We may use third-party Service Providers to monitor and analyze the use of our Service.\"}),`\n`,(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:\"Mixpanel\"}),(0,o.jsx)(e.br,{}),`\n`,\"Mixpanel is provided by Mixpanel Inc.\",(0,o.jsx)(e.br,{}),`\n`,\"You can prevent Mixpanel from using your information for analytics purposes by opting-out. To opt-out of Mixpanel service, please visit this page: \",(0,o.jsx)(e.a,{href:\"https://mixpanel.com/optout/\",children:\"https://mixpanel.com/optout/\"}),(0,o.jsx)(e.br,{}),`\n`,\"For more information on what type of information Mixpanel collects, please visit the Terms of Use page of Mixpanel: \",(0,o.jsx)(e.a,{href:\"https://mixpanel.com/terms/\",children:\"https://mixpanel.com/terms/\"})]}),`\n`,(0,o.jsxs)(e.h2,{id:\"15-cicd-tools\",children:[(0,o.jsx)(e.a,{className:\"anchor\",href:\"#15-cicd-tools\",children:(0,o.jsx)(e.span,{className:\"icon icon-link\"})}),\"15. CI/CD tools\"]}),`\n`,(0,o.jsx)(e.p,{children:\"We may use third-party Service Providers to automate the development process of our Service.\"}),`\n`,(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:\"GitHub\"}),(0,o.jsx)(e.br,{}),`\n`,\"GitHub is provided by GitHub, Inc.\",(0,o.jsx)(e.br,{}),`\n`,\"GitHub is a development platform to host and review code, manage projects, and build software.\",(0,o.jsx)(e.br,{}),`\n`,\"For more information on what data GitHub collects for what purpose and how the protection of the data is ensured, please visit GitHub Privacy Policy page: \",(0,o.jsx)(e.a,{href:\"https://help.github.com/en/articles/github-privacy-statement\",children:\"https://help.github.com/en/articles/github-privacy-statement\"}),\".\"]}),`\n`,(0,o.jsxs)(e.h2,{id:\"16-payments\",children:[(0,o.jsx)(e.a,{className:\"anchor\",href:\"#16-payments\",children:(0,o.jsx)(e.span,{className:\"icon icon-link\"})}),\"16. Payments\"]}),`\n`,(0,o.jsx)(e.p,{children:\"We may provide paid products and/or services within Service. In that case, we use third-party services for payment processing (e.g. payment processors).\"}),`\n`,(0,o.jsx)(e.p,{children:\"We will not store or collect your payment card details. That information is provided directly to our third-party payment processors whose use of your personal information is governed by their Privacy Policy. These payment processors adhere to the standards set by PCI-DSS as managed by the PCI Security Standards Council, which is a joint effort of brands like Visa, Mastercard, American Express and Discover. PCI-DSS requirements help ensure the secure handling of payment information.\"}),`\n`,(0,o.jsxs)(e.p,{children:[\"The payment processors we work with are:\",(0,o.jsx)(e.br,{}),`\n`,(0,o.jsx)(e.strong,{children:\"Stripe\"}),\": Their Privacy Policy can be viewed at: \",(0,o.jsx)(e.a,{href:\"https://stripe.com/us/privacy\",children:\"https://stripe.com/us/privacy\"})]}),`\n`,(0,o.jsxs)(e.h2,{id:\"17-links-to-other-sites\",children:[(0,o.jsx)(e.a,{className:\"anchor\",href:\"#17-links-to-other-sites\",children:(0,o.jsx)(e.span,{className:\"icon icon-link\"})}),\"17. Links to Other Sites\"]}),`\n`,(0,o.jsx)(e.p,{children:\"Our Service may contain links to other sites that are not operated by us. If you click a third party link, you will be directed to that third party's site. We strongly advise you to review the Privacy Policy of every site you visit.\"}),`\n`,(0,o.jsx)(e.p,{children:\"We have no control over and assume no responsibility for the content, privacy policies or practices of any third party sites or services.\"}),`\n`,(0,o.jsxs)(e.h2,{id:\"18-childrens-privacy\",children:[(0,o.jsx)(e.a,{className:\"anchor\",href:\"#18-childrens-privacy\",children:(0,o.jsx)(e.span,{className:\"icon icon-link\"})}),\"18. Children's Privacy\"]}),`\n`,(0,o.jsxs)(e.p,{children:['Our Services are not intended for use by children under the age of 13 (\"',(0,o.jsx)(e.strong,{children:\"Children\"}),'\").']}),`\n`,(0,o.jsx)(e.p,{children:\"We do not knowingly collect personally identifiable information from Children under 13. If you become aware that a Child has provided us with Personal Data, please contact us. If we become aware that we have collected Personal Data from Children without verification of parental consent, we take steps to remove that information from our servers.\"}),`\n`,(0,o.jsxs)(e.h2,{id:\"19-changes-to-this-privacy-policy\",children:[(0,o.jsx)(e.a,{className:\"anchor\",href:\"#19-changes-to-this-privacy-policy\",children:(0,o.jsx)(e.span,{className:\"icon icon-link\"})}),\"19. Changes to This Privacy Policy\"]}),`\n`,(0,o.jsx)(e.p,{children:\"We may update our Privacy Policy from time to time. We will notify you of any changes by posting the new Privacy Policy on this page.\"}),`\n`,(0,o.jsx)(e.p,{children:'We will let you know via email and/or a prominent notice on our Service, prior to the change becoming effective and update \"effective date\" at the top of this Privacy Policy.'}),`\n`,(0,o.jsx)(e.p,{children:\"You are advised to review this Privacy Policy periodically for any changes. Changes to this Privacy Policy are effective when they are posted on this page.\"}),`\n`,(0,o.jsxs)(e.h2,{id:\"20-contact-us\",children:[(0,o.jsx)(e.a,{className:\"anchor\",href:\"#20-contact-us\",children:(0,o.jsx)(e.span,{className:\"icon icon-link\"})}),\"20. Contact Us\"]}),`\n`,(0,o.jsxs)(e.p,{children:[\"If you have any questions about this Privacy Policy, please contact us at \",(0,o.jsx)(e.a,{href:\"mailto:support@release.com\",children:\"support@release.com\"}),\".\"]})]})}function P(r={}){let{wrapper:e}=r.components||{};return e?(0,o.jsx)(e,Object.assign({},r,{children:(0,o.jsx)(h,r)})):h(r)}var k=P;return g(D);})();\n;return Component;"
        },
        "_id": "legal/content/privacy-policy.mdx",
        "_raw": {
          "sourceFilePath": "legal/content/privacy-policy.mdx",
          "sourceFileName": "privacy-policy.mdx",
          "sourceFileDir": "legal/content",
          "contentType": "mdx",
          "flattenedPath": "legal/content/privacy-policy"
        },
        "type": "Legal",
        "computedSlug": "privacy-policy"
      },
      "documentHash": "1739393595039",
      "hasWarnings": false,
      "documentTypeName": "Legal"
    },
    "legal/content/security.mdx": {
      "document": {
        "title": "Security",
        "publishDate": "2024-02-26",
        "slug": "security",
        "body": {
          "raw": "\n# Security at Release\n\n<CertificationBadge />\n\n## Our Commitment\n\nAt Release, security is not just a feature - it's a fundamental principle that guides everything we do. We understand that our customers trust us with their development environments and data, and we take this responsibility seriously.\n\n## Infrastructure Security\n\n### Secure Architecture\n\nOur platform is built on a foundation of security-first principles, utilizing industry best practices and modern security architectures to protect your data and environments.\n\n### Data Isolation\n\nEach customer's environments and data are completely isolated using advanced containerization and networking technologies. This ensures that there is no cross-contamination between different customer environments.\n\n### Cloud Security\n\nWe leverage enterprise-grade cloud infrastructure with:\n\n- Automated security patches and updates\n- Regular security audits and penetration testing\n- 24/7 infrastructure monitoring\n- Comprehensive logging and alerting systems\n\n## Data Protection\n\n### Data Encryption\n\n- All data is encrypted in transit using TLS 1.2+\n- Data at rest is encrypted using industry-standard AES-256 encryption\n- Secure key management using AWS KMS\n\n### Access Controls\n\n- Role-based access control (RBAC)\n- Multi-factor authentication (MFA)\n- Regular access reviews and audit logging\n- Principle of least privilege enforcement\n\n## Compliance\n\n### SOC 2 Type 2 Certified\n\nWe maintain SOC 2 Type 2 certification, demonstrating our commitment to:\n\n- Security\n- Availability\n- Processing Integrity\n- Confidentiality\n- Privacy\n\n### Regular Audits\n\nOur security practices are regularly audited by independent third parties to ensure compliance with industry standards and best practices.\n\n## Security Features\n\n### Environment Security\n\n- Isolated development environments\n- Secure access controls\n- Network security policies\n- Regular security scanning\n\n### Application Security\n\n- Automated vulnerability scanning\n- Dependency security monitoring\n- Container security\n- Code security analysis\n\n## Incident Response\n\n### 24/7 Monitoring\n\nOur security team monitors our infrastructure around the clock for any potential security issues.\n\n### Rapid Response\n\nWe maintain a comprehensive incident response plan that includes:\n\n- Immediate threat containment\n- Thorough investigation\n- Customer notification\n- Post-incident analysis and improvements\n\n## Security Practices\n\n### Employee Security\n\n- Background checks for all employees\n- Regular security training\n- Security awareness programs\n- Access management procedures\n\n### Vendor Management\n\n- Thorough vendor security assessments\n- Regular vendor security reviews\n- Contractual security requirements\n- Vendor access monitoring\n\n## Reporting Security Issues\n\nWe take security issues seriously. If you discover a potential security issue, please notify us immediately at security@release.com.\n\n## Updates and Notifications\n\nWe continuously update our security measures to address new threats and vulnerabilities. Customers are notified of significant security updates through:\n\n- Email notifications\n- Platform announcements\n- Security advisories\n- Regular security bulletins\n\n## Contact Us\n\nFor security-related questions or concerns, please contact our security team:\n\n- Email: security@release.com\n- Response time: Within 24 hours for non-critical issues\n- Immediate response for critical security issues\n",
          "code": "var Component=(()=>{var h=Object.create;var a=Object.defineProperty;var u=Object.getOwnPropertyDescriptor;var m=Object.getOwnPropertyNames;var p=Object.getPrototypeOf,y=Object.prototype.hasOwnProperty;var f=(i,e)=>()=>(e||i((e={exports:{}}).exports,e),e.exports),g=(i,e)=>{for(var r in e)a(i,r,{get:e[r],enumerable:!0})},t=(i,e,r,s)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let c of m(e))!y.call(i,c)&&c!==r&&a(i,c,{get:()=>e[c],enumerable:!(s=u(e,c))||s.enumerable});return i};var N=(i,e,r)=>(r=i!=null?h(p(i)):{},t(e||!i||!i.__esModule?a(r,\"default\",{value:i,enumerable:!0}):r,i)),k=i=>t(a({},\"__esModule\",{value:!0}),i);var o=f((A,l)=>{l.exports=_jsx_runtime});var b={};g(b,{default:()=>C,frontmatter:()=>v});var n=N(o()),v={title:\"Security\",publishDate:\"2024-02-26\",slug:\"security\"};function d(i){let e=Object.assign({h1:\"h1\",a:\"a\",span:\"span\",h2:\"h2\",p:\"p\",h3:\"h3\",ul:\"ul\",li:\"li\"},i.components),{CertificationBadge:r}=e;return r||R(\"CertificationBadge\",!0),(0,n.jsxs)(n.Fragment,{children:[(0,n.jsxs)(e.h1,{id:\"security-at-release\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#security-at-release\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Security at Release\"]}),`\n`,(0,n.jsx)(r,{}),`\n`,(0,n.jsxs)(e.h2,{id:\"our-commitment\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#our-commitment\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Our Commitment\"]}),`\n`,(0,n.jsx)(e.p,{children:\"At Release, security is not just a feature - it's a fundamental principle that guides everything we do. We understand that our customers trust us with their development environments and data, and we take this responsibility seriously.\"}),`\n`,(0,n.jsxs)(e.h2,{id:\"infrastructure-security\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#infrastructure-security\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Infrastructure Security\"]}),`\n`,(0,n.jsxs)(e.h3,{id:\"secure-architecture\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#secure-architecture\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Secure Architecture\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Our platform is built on a foundation of security-first principles, utilizing industry best practices and modern security architectures to protect your data and environments.\"}),`\n`,(0,n.jsxs)(e.h3,{id:\"data-isolation\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#data-isolation\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Data Isolation\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Each customer's environments and data are completely isolated using advanced containerization and networking technologies. This ensures that there is no cross-contamination between different customer environments.\"}),`\n`,(0,n.jsxs)(e.h3,{id:\"cloud-security\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#cloud-security\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Cloud Security\"]}),`\n`,(0,n.jsx)(e.p,{children:\"We leverage enterprise-grade cloud infrastructure with:\"}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsx)(e.li,{children:\"Automated security patches and updates\"}),`\n`,(0,n.jsx)(e.li,{children:\"Regular security audits and penetration testing\"}),`\n`,(0,n.jsx)(e.li,{children:\"24/7 infrastructure monitoring\"}),`\n`,(0,n.jsx)(e.li,{children:\"Comprehensive logging and alerting systems\"}),`\n`]}),`\n`,(0,n.jsxs)(e.h2,{id:\"data-protection\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#data-protection\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Data Protection\"]}),`\n`,(0,n.jsxs)(e.h3,{id:\"data-encryption\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#data-encryption\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Data Encryption\"]}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsx)(e.li,{children:\"All data is encrypted in transit using TLS 1.2+\"}),`\n`,(0,n.jsx)(e.li,{children:\"Data at rest is encrypted using industry-standard AES-256 encryption\"}),`\n`,(0,n.jsx)(e.li,{children:\"Secure key management using AWS KMS\"}),`\n`]}),`\n`,(0,n.jsxs)(e.h3,{id:\"access-controls\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#access-controls\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Access Controls\"]}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsx)(e.li,{children:\"Role-based access control (RBAC)\"}),`\n`,(0,n.jsx)(e.li,{children:\"Multi-factor authentication (MFA)\"}),`\n`,(0,n.jsx)(e.li,{children:\"Regular access reviews and audit logging\"}),`\n`,(0,n.jsx)(e.li,{children:\"Principle of least privilege enforcement\"}),`\n`]}),`\n`,(0,n.jsxs)(e.h2,{id:\"compliance\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#compliance\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Compliance\"]}),`\n`,(0,n.jsxs)(e.h3,{id:\"soc-2-type-2-certified\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#soc-2-type-2-certified\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"SOC 2 Type 2 Certified\"]}),`\n`,(0,n.jsx)(e.p,{children:\"We maintain SOC 2 Type 2 certification, demonstrating our commitment to:\"}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsx)(e.li,{children:\"Security\"}),`\n`,(0,n.jsx)(e.li,{children:\"Availability\"}),`\n`,(0,n.jsx)(e.li,{children:\"Processing Integrity\"}),`\n`,(0,n.jsx)(e.li,{children:\"Confidentiality\"}),`\n`,(0,n.jsx)(e.li,{children:\"Privacy\"}),`\n`]}),`\n`,(0,n.jsxs)(e.h3,{id:\"regular-audits\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#regular-audits\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Regular Audits\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Our security practices are regularly audited by independent third parties to ensure compliance with industry standards and best practices.\"}),`\n`,(0,n.jsxs)(e.h2,{id:\"security-features\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#security-features\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Security Features\"]}),`\n`,(0,n.jsxs)(e.h3,{id:\"environment-security\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#environment-security\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Environment Security\"]}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsx)(e.li,{children:\"Isolated development environments\"}),`\n`,(0,n.jsx)(e.li,{children:\"Secure access controls\"}),`\n`,(0,n.jsx)(e.li,{children:\"Network security policies\"}),`\n`,(0,n.jsx)(e.li,{children:\"Regular security scanning\"}),`\n`]}),`\n`,(0,n.jsxs)(e.h3,{id:\"application-security\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#application-security\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Application Security\"]}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsx)(e.li,{children:\"Automated vulnerability scanning\"}),`\n`,(0,n.jsx)(e.li,{children:\"Dependency security monitoring\"}),`\n`,(0,n.jsx)(e.li,{children:\"Container security\"}),`\n`,(0,n.jsx)(e.li,{children:\"Code security analysis\"}),`\n`]}),`\n`,(0,n.jsxs)(e.h2,{id:\"incident-response\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#incident-response\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Incident Response\"]}),`\n`,(0,n.jsxs)(e.h3,{id:\"247-monitoring\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#247-monitoring\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"24/7 Monitoring\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Our security team monitors our infrastructure around the clock for any potential security issues.\"}),`\n`,(0,n.jsxs)(e.h3,{id:\"rapid-response\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#rapid-response\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Rapid Response\"]}),`\n`,(0,n.jsx)(e.p,{children:\"We maintain a comprehensive incident response plan that includes:\"}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsx)(e.li,{children:\"Immediate threat containment\"}),`\n`,(0,n.jsx)(e.li,{children:\"Thorough investigation\"}),`\n`,(0,n.jsx)(e.li,{children:\"Customer notification\"}),`\n`,(0,n.jsx)(e.li,{children:\"Post-incident analysis and improvements\"}),`\n`]}),`\n`,(0,n.jsxs)(e.h2,{id:\"security-practices\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#security-practices\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Security Practices\"]}),`\n`,(0,n.jsxs)(e.h3,{id:\"employee-security\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#employee-security\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Employee Security\"]}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsx)(e.li,{children:\"Background checks for all employees\"}),`\n`,(0,n.jsx)(e.li,{children:\"Regular security training\"}),`\n`,(0,n.jsx)(e.li,{children:\"Security awareness programs\"}),`\n`,(0,n.jsx)(e.li,{children:\"Access management procedures\"}),`\n`]}),`\n`,(0,n.jsxs)(e.h3,{id:\"vendor-management\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#vendor-management\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Vendor Management\"]}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsx)(e.li,{children:\"Thorough vendor security assessments\"}),`\n`,(0,n.jsx)(e.li,{children:\"Regular vendor security reviews\"}),`\n`,(0,n.jsx)(e.li,{children:\"Contractual security requirements\"}),`\n`,(0,n.jsx)(e.li,{children:\"Vendor access monitoring\"}),`\n`]}),`\n`,(0,n.jsxs)(e.h2,{id:\"reporting-security-issues\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#reporting-security-issues\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Reporting Security Issues\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"We take security issues seriously. If you discover a potential security issue, please notify us immediately at \",(0,n.jsx)(e.a,{href:\"mailto:security@release.com\",children:\"security@release.com\"}),\".\"]}),`\n`,(0,n.jsxs)(e.h2,{id:\"updates-and-notifications\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#updates-and-notifications\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Updates and Notifications\"]}),`\n`,(0,n.jsx)(e.p,{children:\"We continuously update our security measures to address new threats and vulnerabilities. Customers are notified of significant security updates through:\"}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsx)(e.li,{children:\"Email notifications\"}),`\n`,(0,n.jsx)(e.li,{children:\"Platform announcements\"}),`\n`,(0,n.jsx)(e.li,{children:\"Security advisories\"}),`\n`,(0,n.jsx)(e.li,{children:\"Regular security bulletins\"}),`\n`]}),`\n`,(0,n.jsxs)(e.h2,{id:\"contact-us\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#contact-us\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Contact Us\"]}),`\n`,(0,n.jsx)(e.p,{children:\"For security-related questions or concerns, please contact our security team:\"}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsxs)(e.li,{children:[\"Email: \",(0,n.jsx)(e.a,{href:\"mailto:security@release.com\",children:\"security@release.com\"})]}),`\n`,(0,n.jsx)(e.li,{children:\"Response time: Within 24 hours for non-critical issues\"}),`\n`,(0,n.jsx)(e.li,{children:\"Immediate response for critical security issues\"}),`\n`]})]})}function S(i={}){let{wrapper:e}=i.components||{};return e?(0,n.jsx)(e,Object.assign({},i,{children:(0,n.jsx)(d,i)})):d(i)}var C=S;function R(i,e){throw new Error(\"Expected \"+(e?\"component\":\"object\")+\" `\"+i+\"` to be defined: you likely forgot to import, pass, or provide it.\")}return k(b);})();\n;return Component;"
        },
        "_id": "legal/content/security.mdx",
        "_raw": {
          "sourceFilePath": "legal/content/security.mdx",
          "sourceFileName": "security.mdx",
          "sourceFileDir": "legal/content",
          "contentType": "mdx",
          "flattenedPath": "legal/content/security"
        },
        "type": "Legal",
        "computedSlug": "security"
      },
      "documentHash": "1739393595039",
      "hasWarnings": false,
      "documentTypeName": "Legal"
    },
    "legal/content/terms-of-service.mdx": {
      "document": {
        "title": "Terms of Service",
        "publishDate": "2021-12-14",
        "slug": "terms-of-service",
        "body": {
          "raw": "\n# Terms of Service\n\n_Effective date: 14 DEC 2021_\n\n## ARTICLE I. INTRODUCTION\n\nWelcome to **Release Technologies, Inc. (\"Release Technologies\" or \"we\" or \"us\")**!\n\nBy accessing or using our website at www.release.com (the \"Site\") you agree to these Terms of Service (\"ToS\") which are a legally binding agreement between you and Release Technologies regarding your access and use of our Site, Product and Services. If you do not wish to accept these ToS then do not access or use the Site, Product or Services. The Site is intended for access and use by individuals age 18 or older. If you are not at least 18 years old you are prohibited from accessing or using the Site.\n\nArticle II of these ToS describes the terms that apply to all visitors to our Site.\n\nArticle III of these ToS describes the terms that apply to our customers who license a free version or a paid version of our Product and Services which are accessed through the Site.\n\n## ARTICLE II. TERMS AND CONDITIONS FOR SITE VISITORS\n\n### 1. Site Purpose; Age\n\nThe Site provides certain information about us and about our products and services and provides you with opportunities to contact us.\n\n### 2. Marketing Communications\n\nIf you submit your email address to the Site to create a beta account on the Site then you agree to subscribe to newsletters, marketing and promotional materials we may send you. However, you may opt out of receiving these communications from us in the future by following the unsubscribe link in any communication or by emailing us at support@release.com. We describe this further in our Privacy Policy.\n\n### 3. Credit Card Automatic Renewals\n\nWhen you visit the Site you may choose to subscribe to a free subscription or subscribe to a paid monthly or annual subscription to our Services. If you subscribe for a subscription which is paid via your credit card, or in the case of a trial subscription which may become paid via your credit card, then you must submit your credit card and you agree to an initial and recurring subscription fee (in the case of a trial subscription, beginning when the trial period ends). Such charges will be made in advance, either monthly or annually, and you accept responsibility for all recurring charges until you cancel your subscription. By subscribing for any paid subscription (including a paid subscription automatically entered into at the end of free trial period) and providing us with your credit card information, you authorize us to charge your credit card for all Services licensed by you, including for any renewal term, until such time as you cancel your subscription.\n\n**AUTOMATIC RENEWAL AND CANCELLATION TERMS:** Subscription fees are non-refundable. You may cancel your monthly or annual subscription renewal at any time after you are billed for the then-current month or year (as applicable) and before you are billed for the next month or year (as applicable), by notifying us in writing at support@release.com or, to the extent the functionality is available, by logging into your account and following the cancellation procedures.\n\n### 4. Site Ownership; Proprietary Rights\n\nThe Site is owned and operated by Release Technologies. The look and feel of the Site, the visual interfaces, custom fonts, graphics, designs and button designs, the compilation, information, data, computer code, and all other elements of the Site and Services (collectively, \"Materials\") provided by Release Technologies are protected by intellectual property and other laws. All Materials associated with the Site are the property of Release Technologies or our third-party licensors. You are authorized to view the Materials on the Site in accordance with these ToS; but you may not make any other use of the Materials without the prior express written permission of Release Technologies. For example, you may not copy or distribute the Materials, or prepare derivative works based on the Materials, without our written consent in advance.\n\n### 5. Privacy Policy\n\nPlease read the Release Technologies Privacy Policy for information relating to our collection, use, storage and disclosure of information we collect about you. The Release Technologies Privacy Policy is incorporated by reference into, and made part of, these ToS. Release Technologies will post notices of updates to the Privacy Policy on the Site.\n\n### 6. Prohibited Conduct\n\nYOU AGREE NOT TO:\n\n6.1 use the Site for any illegal purpose or in violation of any local, state, national, or international law;\n\n6.2 interfere with security-related features of the Site, including by disabling or circumventing features that prevent or limit access to or use of any content, or by using any account credentials that are not your own;\n\n6.3 interfere with the operation of the Site or any visitor's enjoyment of the Site, including by interfering with, intruding into, disrupting, or making repeated accesses or requests that cause performance degradation to any network, equipment, server, or software system used to host or otherwise implement the Site;\n\n6.4 use any means of automated data collection (\"scraping\") or indexing (\"crawling\"), including by use of data mining tools, scripts, repeated data requests, automated systems (\"robots\"), or any other method to collect, copy, or aggregate information hosted on the Site; or\n\n6.5 attempt to do any of the acts described in this Article II, Section 6, or assist, encourage, request, or permit any person to engage in any of the acts described in this Article II, Section 6.\n\n### 7. Linked Websites\n\nThe Site may contain links to third-party websites such as social media websites. We provide these links only as a convenience and are not responsible for the content, products or services on or available from those websites or resources or links displayed on such sites. You acknowledge sole responsibility for, and assume all risk arising from, your use of any third-party websites or resources.\n\n### 8. Modification of the Site\n\nWe reserve the right to modify or discontinue the Site at any time, temporarily or permanently, without notice to you. We will have no liability whatsoever on account of any change to the Site or termination of your access to or use of the Site.\n\n### 9. Modification of Article II of these ToS\n\nWe reserve the right, at our discretion, to change this Article II on a going-forward basis at any time. Please check these ToS periodically for changes. We will post notices of changes on the Site.\n\n### 10. Site Feedback\n\nIf you choose to provide us with input and suggestions regarding problems with or proposed modifications or improvements to the Site (\"Site Feedback\"), then you hereby grant Release Technologies an unrestricted, perpetual, irrevocable, non-exclusive, fully-paid, royalty-free right to exploit the Site Feedback in any manner and for any purpose, including to improve the Site and create other products and services.\n\n### 11. No Site Warranty\n\n11.1 THE SITE AND ALL MATERIALS AND CONTENT AVAILABLE THROUGH THE SITE ARE PROVIDED \"AS IS\" AND ON AN \"AS AVAILABLE\" BASIS. RELEASE TECHNOLOGIES DISCLAIMS ALL WARRANTIES AND CONDITIONS OF ANY KIND, WHETHER EXPRESS OR IMPLIED, RELATING TO THE SITE AND ALL MATERIALS AVAILABLE THROUGH THE SITE, INCLUDING WITHOUT LIMITATION ANY IMPLIED WARRANTY OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, QUIET ENJOYMENT, OR NON-INFRINGEMENT. RELEASE TECHNOLOGIES DOES NOT WARRANT THAT THE ACCESS TO SITE, OR ANY MATERIALS OR CONTENT OFFERED THROUGH THE SITE, WILL BE UNINTERRUPTED, SECURE, OR FREE OF ERRORS, VIRUSES, OR OTHER HARMFUL COMPONENTS.\n\n11.2 No advice or information, whether oral or written, obtained by you from the Site or Release Technologies or any Materials available through the Site will create any warranty regarding Release Technologies or the Site that is not expressly stated in these ToS. You assume all risk for any damage that may result from your use of or access to the Site and any Materials available through the Site. You understand and agree that you use the Site and use, access, download, or otherwise obtain Materials through the Site and any associated sites or services at your own discretion and risk, and that you are solely responsible for any damage to your property (including your computer system or mobile device used in connection with the Site), or the loss of data that results from the use of the Site or the download or use of such Materials.\n\n### 12. Limitation of Liability\n\nIN NO EVENT WILL RELEASE TECHNOLOGIES BE LIABLE TO YOU FOR ANY INDIRECT, INCIDENTAL, SPECIAL, CONSEQUENTIAL OR PUNITIVE DAMAGES (INCLUDING DAMAGES FOR LOSS OF PROFITS, GOODWILL, OR ANY OTHER INTANGIBLE LOSS) ARISING OUT OF OR RELATING TO YOUR ACCESS TO OR USE OF, OR YOUR INABILITY TO ACCESS OR USE, THE SITE OR ANY MATERIALS AVAILABLE THROUGH THE SITE, WHETHER BASED ON WARRANTY, CONTRACT, TORT (INCLUDING NEGLIGENCE), STATUTE, OR ANY OTHER LEGAL THEORY, AND WHETHER OR NOT RELEASE TECHNOLOGIES HAS BEEN INFORMED OF THE POSSIBILITY OF DAMAGE.\n\n## ARTICLE III. TERMS AND CONDITIONS FOR CUSTOMERS\n\n### 1. Product and Services\n\nRelease Technologies provides a software-as-a-service platform (the \"Product\") and related services (the \"Services\") that enable customers to create and manage ephemeral environments for their software development and testing needs. The Product and Services are accessed through the Site.\n\n### 2. License Grant\n\nSubject to your compliance with these ToS, Release Technologies grants you a limited, non-exclusive, non-transferable license to access and use the Product and Services for your internal business purposes during the term of your subscription.\n\n### 3. Usage Restrictions\n\nYou agree not to:\n\n- Use the Product or Services for any unlawful purpose\n- Attempt to gain unauthorized access to the Product or Services\n- Copy, modify, or create derivative works of the Product or Services\n- Reverse engineer, decompile, or disassemble the Product\n- Remove or alter any proprietary notices on the Product or Services\n\n### 4. Customer Data\n\nYou retain all rights to any data, content, or materials that you upload to or process through the Product (\"Customer Data\"). You grant Release Technologies a license to host, copy, transmit, and display Customer Data as necessary to provide the Product and Services.\n\n### 5. Security\n\nRelease Technologies implements reasonable security measures to protect Customer Data. However, you acknowledge that no security measures are perfect and that Release Technologies cannot guarantee the security of Customer Data.\n\n### 6. Support\n\nRelease Technologies will provide reasonable technical support for the Product and Services in accordance with our standard support policies.\n\n### 7. Fees and Payment\n\nYou agree to pay all fees specified in your subscription plan. All fees are non-refundable except as required by law or as explicitly stated in these ToS.\n\n### 8. Term and Termination\n\nYour subscription will continue until terminated by either party. Either party may terminate these ToS upon 30 days written notice. Upon termination, you will immediately cease all use of the Product and Services.\n\n## ARTICLE IV. GENERAL TERMS\n\n### 1. Governing Law\n\nThese ToS are governed by the laws of the State of California without regard to conflict of law principles.\n\n### 2. Dispute Resolution\n\nAny dispute arising from these ToS will be resolved through binding arbitration in San Francisco, California, under the rules of the American Arbitration Association.\n\n### 3. Entire Agreement\n\nThese ToS constitute the entire agreement between you and Release Technologies regarding the use of the Site, Product, and Services.\n\n### 4. Contact Information\n\nFor questions about these ToS, please contact us at:\n\nRelease Technologies, Inc.  \nsupport@release.com\n",
          "code": "var Component=(()=>{var d=Object.create;var t=Object.defineProperty;var u=Object.getOwnPropertyDescriptor;var p=Object.getOwnPropertyNames;var m=Object.getPrototypeOf,y=Object.prototype.hasOwnProperty;var f=(n,e)=>()=>(e||n((e={exports:{}}).exports,e),e.exports),S=(n,e)=>{for(var o in e)t(n,o,{get:e[o],enumerable:!0})},s=(n,e,o,a)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let r of p(e))!y.call(n,r)&&r!==o&&t(n,r,{get:()=>e[r],enumerable:!(a=u(e,r))||a.enumerable});return n};var g=(n,e,o)=>(o=n!=null?d(m(n)):{},s(e||!n||!n.__esModule?t(o,\"default\",{value:n,enumerable:!0}):o,n)),T=n=>s(t({},\"__esModule\",{value:!0}),n);var l=f((A,c)=>{c.exports=_jsx_runtime});var I={};S(I,{default:()=>b,frontmatter:()=>N});var i=g(l()),N={title:\"Terms of Service\",publishDate:\"2021-12-14\",slug:\"terms-of-service\"};function h(n){let e=Object.assign({h1:\"h1\",a:\"a\",span:\"span\",p:\"p\",em:\"em\",h2:\"h2\",strong:\"strong\",h3:\"h3\",ul:\"ul\",li:\"li\",br:\"br\"},n.components);return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsxs)(e.h1,{id:\"terms-of-service\",children:[(0,i.jsx)(e.a,{className:\"anchor\",href:\"#terms-of-service\",children:(0,i.jsx)(e.span,{className:\"icon icon-link\"})}),\"Terms of Service\"]}),`\n`,(0,i.jsx)(e.p,{children:(0,i.jsx)(e.em,{children:\"Effective date: 14 DEC 2021\"})}),`\n`,(0,i.jsxs)(e.h2,{id:\"article-i-introduction\",children:[(0,i.jsx)(e.a,{className:\"anchor\",href:\"#article-i-introduction\",children:(0,i.jsx)(e.span,{className:\"icon icon-link\"})}),\"ARTICLE I. INTRODUCTION\"]}),`\n`,(0,i.jsxs)(e.p,{children:[\"Welcome to \",(0,i.jsx)(e.strong,{children:'Release Technologies, Inc. (\"Release Technologies\" or \"we\" or \"us\")'}),\"!\"]}),`\n`,(0,i.jsxs)(e.p,{children:[\"By accessing or using our website at \",(0,i.jsx)(e.a,{href:\"http://www.release.com\",children:\"www.release.com\"}),' (the \"Site\") you agree to these Terms of Service (\"ToS\") which are a legally binding agreement between you and Release Technologies regarding your access and use of our Site, Product and Services. If you do not wish to accept these ToS then do not access or use the Site, Product or Services. The Site is intended for access and use by individuals age 18 or older. If you are not at least 18 years old you are prohibited from accessing or using the Site.']}),`\n`,(0,i.jsx)(e.p,{children:\"Article II of these ToS describes the terms that apply to all visitors to our Site.\"}),`\n`,(0,i.jsx)(e.p,{children:\"Article III of these ToS describes the terms that apply to our customers who license a free version or a paid version of our Product and Services which are accessed through the Site.\"}),`\n`,(0,i.jsxs)(e.h2,{id:\"article-ii-terms-and-conditions-for-site-visitors\",children:[(0,i.jsx)(e.a,{className:\"anchor\",href:\"#article-ii-terms-and-conditions-for-site-visitors\",children:(0,i.jsx)(e.span,{className:\"icon icon-link\"})}),\"ARTICLE II. TERMS AND CONDITIONS FOR SITE VISITORS\"]}),`\n`,(0,i.jsxs)(e.h3,{id:\"1-site-purpose-age\",children:[(0,i.jsx)(e.a,{className:\"anchor\",href:\"#1-site-purpose-age\",children:(0,i.jsx)(e.span,{className:\"icon icon-link\"})}),\"1. Site Purpose; Age\"]}),`\n`,(0,i.jsx)(e.p,{children:\"The Site provides certain information about us and about our products and services and provides you with opportunities to contact us.\"}),`\n`,(0,i.jsxs)(e.h3,{id:\"2-marketing-communications\",children:[(0,i.jsx)(e.a,{className:\"anchor\",href:\"#2-marketing-communications\",children:(0,i.jsx)(e.span,{className:\"icon icon-link\"})}),\"2. Marketing Communications\"]}),`\n`,(0,i.jsxs)(e.p,{children:[\"If you submit your email address to the Site to create a beta account on the Site then you agree to subscribe to newsletters, marketing and promotional materials we may send you. However, you may opt out of receiving these communications from us in the future by following the unsubscribe link in any communication or by emailing us at \",(0,i.jsx)(e.a,{href:\"mailto:support@release.com\",children:\"support@release.com\"}),\". We describe this further in our Privacy Policy.\"]}),`\n`,(0,i.jsxs)(e.h3,{id:\"3-credit-card-automatic-renewals\",children:[(0,i.jsx)(e.a,{className:\"anchor\",href:\"#3-credit-card-automatic-renewals\",children:(0,i.jsx)(e.span,{className:\"icon icon-link\"})}),\"3. Credit Card Automatic Renewals\"]}),`\n`,(0,i.jsx)(e.p,{children:\"When you visit the Site you may choose to subscribe to a free subscription or subscribe to a paid monthly or annual subscription to our Services. If you subscribe for a subscription which is paid via your credit card, or in the case of a trial subscription which may become paid via your credit card, then you must submit your credit card and you agree to an initial and recurring subscription fee (in the case of a trial subscription, beginning when the trial period ends). Such charges will be made in advance, either monthly or annually, and you accept responsibility for all recurring charges until you cancel your subscription. By subscribing for any paid subscription (including a paid subscription automatically entered into at the end of free trial period) and providing us with your credit card information, you authorize us to charge your credit card for all Services licensed by you, including for any renewal term, until such time as you cancel your subscription.\"}),`\n`,(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:\"AUTOMATIC RENEWAL AND CANCELLATION TERMS:\"}),\" Subscription fees are non-refundable. You may cancel your monthly or annual subscription renewal at any time after you are billed for the then-current month or year (as applicable) and before you are billed for the next month or year (as applicable), by notifying us in writing at \",(0,i.jsx)(e.a,{href:\"mailto:support@release.com\",children:\"support@release.com\"}),\" or, to the extent the functionality is available, by logging into your account and following the cancellation procedures.\"]}),`\n`,(0,i.jsxs)(e.h3,{id:\"4-site-ownership-proprietary-rights\",children:[(0,i.jsx)(e.a,{className:\"anchor\",href:\"#4-site-ownership-proprietary-rights\",children:(0,i.jsx)(e.span,{className:\"icon icon-link\"})}),\"4. Site Ownership; Proprietary Rights\"]}),`\n`,(0,i.jsx)(e.p,{children:'The Site is owned and operated by Release Technologies. The look and feel of the Site, the visual interfaces, custom fonts, graphics, designs and button designs, the compilation, information, data, computer code, and all other elements of the Site and Services (collectively, \"Materials\") provided by Release Technologies are protected by intellectual property and other laws. All Materials associated with the Site are the property of Release Technologies or our third-party licensors. You are authorized to view the Materials on the Site in accordance with these ToS; but you may not make any other use of the Materials without the prior express written permission of Release Technologies. For example, you may not copy or distribute the Materials, or prepare derivative works based on the Materials, without our written consent in advance.'}),`\n`,(0,i.jsxs)(e.h3,{id:\"5-privacy-policy\",children:[(0,i.jsx)(e.a,{className:\"anchor\",href:\"#5-privacy-policy\",children:(0,i.jsx)(e.span,{className:\"icon icon-link\"})}),\"5. Privacy Policy\"]}),`\n`,(0,i.jsx)(e.p,{children:\"Please read the Release Technologies Privacy Policy for information relating to our collection, use, storage and disclosure of information we collect about you. The Release Technologies Privacy Policy is incorporated by reference into, and made part of, these ToS. Release Technologies will post notices of updates to the Privacy Policy on the Site.\"}),`\n`,(0,i.jsxs)(e.h3,{id:\"6-prohibited-conduct\",children:[(0,i.jsx)(e.a,{className:\"anchor\",href:\"#6-prohibited-conduct\",children:(0,i.jsx)(e.span,{className:\"icon icon-link\"})}),\"6. Prohibited Conduct\"]}),`\n`,(0,i.jsx)(e.p,{children:\"YOU AGREE NOT TO:\"}),`\n`,(0,i.jsx)(e.p,{children:\"6.1 use the Site for any illegal purpose or in violation of any local, state, national, or international law;\"}),`\n`,(0,i.jsx)(e.p,{children:\"6.2 interfere with security-related features of the Site, including by disabling or circumventing features that prevent or limit access to or use of any content, or by using any account credentials that are not your own;\"}),`\n`,(0,i.jsx)(e.p,{children:\"6.3 interfere with the operation of the Site or any visitor's enjoyment of the Site, including by interfering with, intruding into, disrupting, or making repeated accesses or requests that cause performance degradation to any network, equipment, server, or software system used to host or otherwise implement the Site;\"}),`\n`,(0,i.jsx)(e.p,{children:'6.4 use any means of automated data collection (\"scraping\") or indexing (\"crawling\"), including by use of data mining tools, scripts, repeated data requests, automated systems (\"robots\"), or any other method to collect, copy, or aggregate information hosted on the Site; or'}),`\n`,(0,i.jsx)(e.p,{children:\"6.5 attempt to do any of the acts described in this Article II, Section 6, or assist, encourage, request, or permit any person to engage in any of the acts described in this Article II, Section 6.\"}),`\n`,(0,i.jsxs)(e.h3,{id:\"7-linked-websites\",children:[(0,i.jsx)(e.a,{className:\"anchor\",href:\"#7-linked-websites\",children:(0,i.jsx)(e.span,{className:\"icon icon-link\"})}),\"7. Linked Websites\"]}),`\n`,(0,i.jsx)(e.p,{children:\"The Site may contain links to third-party websites such as social media websites. We provide these links only as a convenience and are not responsible for the content, products or services on or available from those websites or resources or links displayed on such sites. You acknowledge sole responsibility for, and assume all risk arising from, your use of any third-party websites or resources.\"}),`\n`,(0,i.jsxs)(e.h3,{id:\"8-modification-of-the-site\",children:[(0,i.jsx)(e.a,{className:\"anchor\",href:\"#8-modification-of-the-site\",children:(0,i.jsx)(e.span,{className:\"icon icon-link\"})}),\"8. Modification of the Site\"]}),`\n`,(0,i.jsx)(e.p,{children:\"We reserve the right to modify or discontinue the Site at any time, temporarily or permanently, without notice to you. We will have no liability whatsoever on account of any change to the Site or termination of your access to or use of the Site.\"}),`\n`,(0,i.jsxs)(e.h3,{id:\"9-modification-of-article-ii-of-these-tos\",children:[(0,i.jsx)(e.a,{className:\"anchor\",href:\"#9-modification-of-article-ii-of-these-tos\",children:(0,i.jsx)(e.span,{className:\"icon icon-link\"})}),\"9. Modification of Article II of these ToS\"]}),`\n`,(0,i.jsx)(e.p,{children:\"We reserve the right, at our discretion, to change this Article II on a going-forward basis at any time. Please check these ToS periodically for changes. We will post notices of changes on the Site.\"}),`\n`,(0,i.jsxs)(e.h3,{id:\"10-site-feedback\",children:[(0,i.jsx)(e.a,{className:\"anchor\",href:\"#10-site-feedback\",children:(0,i.jsx)(e.span,{className:\"icon icon-link\"})}),\"10. Site Feedback\"]}),`\n`,(0,i.jsx)(e.p,{children:'If you choose to provide us with input and suggestions regarding problems with or proposed modifications or improvements to the Site (\"Site Feedback\"), then you hereby grant Release Technologies an unrestricted, perpetual, irrevocable, non-exclusive, fully-paid, royalty-free right to exploit the Site Feedback in any manner and for any purpose, including to improve the Site and create other products and services.'}),`\n`,(0,i.jsxs)(e.h3,{id:\"11-no-site-warranty\",children:[(0,i.jsx)(e.a,{className:\"anchor\",href:\"#11-no-site-warranty\",children:(0,i.jsx)(e.span,{className:\"icon icon-link\"})}),\"11. No Site Warranty\"]}),`\n`,(0,i.jsx)(e.p,{children:'11.1 THE SITE AND ALL MATERIALS AND CONTENT AVAILABLE THROUGH THE SITE ARE PROVIDED \"AS IS\" AND ON AN \"AS AVAILABLE\" BASIS. RELEASE TECHNOLOGIES DISCLAIMS ALL WARRANTIES AND CONDITIONS OF ANY KIND, WHETHER EXPRESS OR IMPLIED, RELATING TO THE SITE AND ALL MATERIALS AVAILABLE THROUGH THE SITE, INCLUDING WITHOUT LIMITATION ANY IMPLIED WARRANTY OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, QUIET ENJOYMENT, OR NON-INFRINGEMENT. RELEASE TECHNOLOGIES DOES NOT WARRANT THAT THE ACCESS TO SITE, OR ANY MATERIALS OR CONTENT OFFERED THROUGH THE SITE, WILL BE UNINTERRUPTED, SECURE, OR FREE OF ERRORS, VIRUSES, OR OTHER HARMFUL COMPONENTS.'}),`\n`,(0,i.jsx)(e.p,{children:\"11.2 No advice or information, whether oral or written, obtained by you from the Site or Release Technologies or any Materials available through the Site will create any warranty regarding Release Technologies or the Site that is not expressly stated in these ToS. You assume all risk for any damage that may result from your use of or access to the Site and any Materials available through the Site. You understand and agree that you use the Site and use, access, download, or otherwise obtain Materials through the Site and any associated sites or services at your own discretion and risk, and that you are solely responsible for any damage to your property (including your computer system or mobile device used in connection with the Site), or the loss of data that results from the use of the Site or the download or use of such Materials.\"}),`\n`,(0,i.jsxs)(e.h3,{id:\"12-limitation-of-liability\",children:[(0,i.jsx)(e.a,{className:\"anchor\",href:\"#12-limitation-of-liability\",children:(0,i.jsx)(e.span,{className:\"icon icon-link\"})}),\"12. Limitation of Liability\"]}),`\n`,(0,i.jsx)(e.p,{children:\"IN NO EVENT WILL RELEASE TECHNOLOGIES BE LIABLE TO YOU FOR ANY INDIRECT, INCIDENTAL, SPECIAL, CONSEQUENTIAL OR PUNITIVE DAMAGES (INCLUDING DAMAGES FOR LOSS OF PROFITS, GOODWILL, OR ANY OTHER INTANGIBLE LOSS) ARISING OUT OF OR RELATING TO YOUR ACCESS TO OR USE OF, OR YOUR INABILITY TO ACCESS OR USE, THE SITE OR ANY MATERIALS AVAILABLE THROUGH THE SITE, WHETHER BASED ON WARRANTY, CONTRACT, TORT (INCLUDING NEGLIGENCE), STATUTE, OR ANY OTHER LEGAL THEORY, AND WHETHER OR NOT RELEASE TECHNOLOGIES HAS BEEN INFORMED OF THE POSSIBILITY OF DAMAGE.\"}),`\n`,(0,i.jsxs)(e.h2,{id:\"article-iii-terms-and-conditions-for-customers\",children:[(0,i.jsx)(e.a,{className:\"anchor\",href:\"#article-iii-terms-and-conditions-for-customers\",children:(0,i.jsx)(e.span,{className:\"icon icon-link\"})}),\"ARTICLE III. TERMS AND CONDITIONS FOR CUSTOMERS\"]}),`\n`,(0,i.jsxs)(e.h3,{id:\"1-product-and-services\",children:[(0,i.jsx)(e.a,{className:\"anchor\",href:\"#1-product-and-services\",children:(0,i.jsx)(e.span,{className:\"icon icon-link\"})}),\"1. Product and Services\"]}),`\n`,(0,i.jsx)(e.p,{children:'Release Technologies provides a software-as-a-service platform (the \"Product\") and related services (the \"Services\") that enable customers to create and manage ephemeral environments for their software development and testing needs. The Product and Services are accessed through the Site.'}),`\n`,(0,i.jsxs)(e.h3,{id:\"2-license-grant\",children:[(0,i.jsx)(e.a,{className:\"anchor\",href:\"#2-license-grant\",children:(0,i.jsx)(e.span,{className:\"icon icon-link\"})}),\"2. License Grant\"]}),`\n`,(0,i.jsx)(e.p,{children:\"Subject to your compliance with these ToS, Release Technologies grants you a limited, non-exclusive, non-transferable license to access and use the Product and Services for your internal business purposes during the term of your subscription.\"}),`\n`,(0,i.jsxs)(e.h3,{id:\"3-usage-restrictions\",children:[(0,i.jsx)(e.a,{className:\"anchor\",href:\"#3-usage-restrictions\",children:(0,i.jsx)(e.span,{className:\"icon icon-link\"})}),\"3. Usage Restrictions\"]}),`\n`,(0,i.jsx)(e.p,{children:\"You agree not to:\"}),`\n`,(0,i.jsxs)(e.ul,{children:[`\n`,(0,i.jsx)(e.li,{children:\"Use the Product or Services for any unlawful purpose\"}),`\n`,(0,i.jsx)(e.li,{children:\"Attempt to gain unauthorized access to the Product or Services\"}),`\n`,(0,i.jsx)(e.li,{children:\"Copy, modify, or create derivative works of the Product or Services\"}),`\n`,(0,i.jsx)(e.li,{children:\"Reverse engineer, decompile, or disassemble the Product\"}),`\n`,(0,i.jsx)(e.li,{children:\"Remove or alter any proprietary notices on the Product or Services\"}),`\n`]}),`\n`,(0,i.jsxs)(e.h3,{id:\"4-customer-data\",children:[(0,i.jsx)(e.a,{className:\"anchor\",href:\"#4-customer-data\",children:(0,i.jsx)(e.span,{className:\"icon icon-link\"})}),\"4. Customer Data\"]}),`\n`,(0,i.jsx)(e.p,{children:'You retain all rights to any data, content, or materials that you upload to or process through the Product (\"Customer Data\"). You grant Release Technologies a license to host, copy, transmit, and display Customer Data as necessary to provide the Product and Services.'}),`\n`,(0,i.jsxs)(e.h3,{id:\"5-security\",children:[(0,i.jsx)(e.a,{className:\"anchor\",href:\"#5-security\",children:(0,i.jsx)(e.span,{className:\"icon icon-link\"})}),\"5. Security\"]}),`\n`,(0,i.jsx)(e.p,{children:\"Release Technologies implements reasonable security measures to protect Customer Data. However, you acknowledge that no security measures are perfect and that Release Technologies cannot guarantee the security of Customer Data.\"}),`\n`,(0,i.jsxs)(e.h3,{id:\"6-support\",children:[(0,i.jsx)(e.a,{className:\"anchor\",href:\"#6-support\",children:(0,i.jsx)(e.span,{className:\"icon icon-link\"})}),\"6. Support\"]}),`\n`,(0,i.jsx)(e.p,{children:\"Release Technologies will provide reasonable technical support for the Product and Services in accordance with our standard support policies.\"}),`\n`,(0,i.jsxs)(e.h3,{id:\"7-fees-and-payment\",children:[(0,i.jsx)(e.a,{className:\"anchor\",href:\"#7-fees-and-payment\",children:(0,i.jsx)(e.span,{className:\"icon icon-link\"})}),\"7. Fees and Payment\"]}),`\n`,(0,i.jsx)(e.p,{children:\"You agree to pay all fees specified in your subscription plan. All fees are non-refundable except as required by law or as explicitly stated in these ToS.\"}),`\n`,(0,i.jsxs)(e.h3,{id:\"8-term-and-termination\",children:[(0,i.jsx)(e.a,{className:\"anchor\",href:\"#8-term-and-termination\",children:(0,i.jsx)(e.span,{className:\"icon icon-link\"})}),\"8. Term and Termination\"]}),`\n`,(0,i.jsx)(e.p,{children:\"Your subscription will continue until terminated by either party. Either party may terminate these ToS upon 30 days written notice. Upon termination, you will immediately cease all use of the Product and Services.\"}),`\n`,(0,i.jsxs)(e.h2,{id:\"article-iv-general-terms\",children:[(0,i.jsx)(e.a,{className:\"anchor\",href:\"#article-iv-general-terms\",children:(0,i.jsx)(e.span,{className:\"icon icon-link\"})}),\"ARTICLE IV. GENERAL TERMS\"]}),`\n`,(0,i.jsxs)(e.h3,{id:\"1-governing-law\",children:[(0,i.jsx)(e.a,{className:\"anchor\",href:\"#1-governing-law\",children:(0,i.jsx)(e.span,{className:\"icon icon-link\"})}),\"1. Governing Law\"]}),`\n`,(0,i.jsx)(e.p,{children:\"These ToS are governed by the laws of the State of California without regard to conflict of law principles.\"}),`\n`,(0,i.jsxs)(e.h3,{id:\"2-dispute-resolution\",children:[(0,i.jsx)(e.a,{className:\"anchor\",href:\"#2-dispute-resolution\",children:(0,i.jsx)(e.span,{className:\"icon icon-link\"})}),\"2. Dispute Resolution\"]}),`\n`,(0,i.jsx)(e.p,{children:\"Any dispute arising from these ToS will be resolved through binding arbitration in San Francisco, California, under the rules of the American Arbitration Association.\"}),`\n`,(0,i.jsxs)(e.h3,{id:\"3-entire-agreement\",children:[(0,i.jsx)(e.a,{className:\"anchor\",href:\"#3-entire-agreement\",children:(0,i.jsx)(e.span,{className:\"icon icon-link\"})}),\"3. Entire Agreement\"]}),`\n`,(0,i.jsx)(e.p,{children:\"These ToS constitute the entire agreement between you and Release Technologies regarding the use of the Site, Product, and Services.\"}),`\n`,(0,i.jsxs)(e.h3,{id:\"4-contact-information\",children:[(0,i.jsx)(e.a,{className:\"anchor\",href:\"#4-contact-information\",children:(0,i.jsx)(e.span,{className:\"icon icon-link\"})}),\"4. Contact Information\"]}),`\n`,(0,i.jsx)(e.p,{children:\"For questions about these ToS, please contact us at:\"}),`\n`,(0,i.jsxs)(e.p,{children:[\"Release Technologies, Inc.\",(0,i.jsx)(e.br,{}),`\n`,(0,i.jsx)(e.a,{href:\"mailto:support@release.com\",children:\"support@release.com\"})]})]})}function E(n={}){let{wrapper:e}=n.components||{};return e?(0,i.jsx)(e,Object.assign({},n,{children:(0,i.jsx)(h,n)})):h(n)}var b=E;return T(I);})();\n;return Component;"
        },
        "_id": "legal/content/terms-of-service.mdx",
        "_raw": {
          "sourceFilePath": "legal/content/terms-of-service.mdx",
          "sourceFileName": "terms-of-service.mdx",
          "sourceFileDir": "legal/content",
          "contentType": "mdx",
          "flattenedPath": "legal/content/terms-of-service"
        },
        "type": "Legal",
        "computedSlug": "terms-of-service"
      },
      "documentHash": "1739393595039",
      "hasWarnings": false,
      "documentTypeName": "Legal"
    },
    "partners/content/aws.mdx": {
      "document": {
        "title": "Release + AWS",
        "slug": "aws",
        "description": "Production, development and preview environments in your AWS Cloud, your way.",
        "category": "Cloud partner",
        "thumbnail": "/partner-images/c11adc344c44671ee91d8a36a0f16185.svg",
        "mainImage": "/partner-images/e4726fd14e77afc1e303d4d18c216dd4.svg",
        "buttonCopy": "Get Started",
        "buttonLink": "/signup",
        "betterTitle": "Better Together",
        "betterDescription": "Release and AWS partner to help organizations of all sizes deploy predictable, scalable and isolated environments throughout the whole application lifecycle.",
        "betterCards": [
          "<figure class=\"w-richtext-figure-type-image w-richtext-align-normal\" data-rt-type=\"image\" data-rt-align=\"normal\"><div><img src=\"https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/65ca4b217375e35a5653f46f_Seamless%20Integration%20-%20icon.svg\" loading=\"lazy\"></div></figure><h5>Seamless Integration</h5><p>Release runs in your AWS account and automatically provisions and de-provisions necessary resources for your environments.</p>",
          "<figure id=\"\" class=\"w-richtext-figure-type-image w-richtext-align-normal\" data-rt-type=\"image\" data-rt-align=\"normal\"><div id=\"\"><img src=\"https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/65ca4b4003e629bd11e72326_Optimize%20cloud%20spend%20-%20icon.svg\" loading=\"lazy\" width=\"auto\" height=\"auto\" alt=\"\" id=\"\"></div></figure><h5 id=\"\">Ease of Billing</h5><p id=\"\">Draw down on your spend commitments and get a consolidated bill for all AWS and Release services. Available on <a href=\"https://aws.amazon.com/marketplace/pp/prodview-faxqlvzq65fea?sr=0-1&ref_=beagle&applicationId=AWSMPContessa\" target=\"_blank\">Marketplace.</a></p>",
          "<figure class=\"w-richtext-figure-type-image w-richtext-align-normal\" data-rt-type=\"image\" data-rt-align=\"normal\"><div><img src=\"https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/65ca543bc6170a3e068ced2c_Production-like%20and%20Data-rich%20logo.svg\" loading=\"lazy\"></div></figure><h5>Production-like and Data-rich</h5><p>All your environments in AWS are as close to production as you need them, with instant access to production-like data.</p>",
          "<figure id=\"\" class=\"w-richtext-figure-type-image w-richtext-align-normal\" data-rt-type=\"image\" data-rt-align=\"normal\"><div id=\"\"><img src=\"https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/65ca547729ce90bfbb3da98f_Cloud%20Optimized%20logo.svg\" loading=\"lazy\" width=\"auto\" height=\"auto\" alt=\"\" id=\"\"></div></figure><h5 id=\"\">Cloud Optimized</h5><p id=\"\">Release optimizes all your environments, &nbsp;so you pay for only what you need, when you need it. Now you can spend the freed-up resources on more interesting projects.</p>",
          "<figure class=\"w-richtext-figure-type-image w-richtext-align-normal\" data-rt-type=\"image\" data-rt-align=\"normal\"><div><img src=\"https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/65ca4b904ae9bed8a538fa6f_Cloud%20on%20your%20terms%20icon.svg\" loading=\"lazy\"></div></figure><h5>Cloud on your terms</h5><p>Consistent cloud configuration experience when running in multiple clouds. Simplified day-to-day operations, while maintaining full control over the advanced settings.</p>",
          "<figure id=\"\" class=\"w-richtext-figure-type-image w-richtext-align-normal\" data-rt-type=\"image\" data-rt-align=\"normal\"><div id=\"\"><img src=\"https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/65ca4bacb6b468d0df7a6f32_Preview%20Programs%20icon.svg\" loading=\"lazy\" width=\"auto\" height=\"auto\" alt=\"\" id=\"\"></div></figure><h5 id=\"\">Preview Programs</h5><p id=\"\">Joint customers get access to preview programs of new features and products. Be at the forefront of product development and help shape the solutions that work for you.</p>"
        ],
        "publishDate": "2024-02-14T17:55:49.000Z",
        "body": {
          "raw": "",
          "code": "var Component=(()=>{var g=Object.create;var r=Object.defineProperty;var p=Object.getOwnPropertyDescriptor;var u=Object.getOwnPropertyNames;var m=Object.getPrototypeOf,h=Object.prototype.hasOwnProperty;var f=(e,a)=>()=>(a||e((a={exports:{}}).exports,a),a.exports),w=(e,a)=>{for(var t in a)r(e,t,{get:a[t],enumerable:!0})},d=(e,a,t,n)=>{if(a&&typeof a==\"object\"||typeof a==\"function\")for(let o of u(a))!h.call(e,o)&&o!==t&&r(e,o,{get:()=>a[o],enumerable:!(n=p(a,o))||n.enumerable});return e};var b=(e,a,t)=>(t=e!=null?g(m(e)):{},d(a||!e||!e.__esModule?r(t,\"default\",{value:e,enumerable:!0}):t,e)),y=e=>d(r({},\"__esModule\",{value:!0}),e);var l=f((S,s)=>{s.exports=_jsx_runtime});var C={};w(C,{default:()=>z,frontmatter:()=>v});var i=b(l()),v={title:\"Release + AWS\",slug:\"aws\",description:\"Production, development and preview environments in your AWS Cloud, your way.\",category:\"Cloud partner\",thumbnail:\"/partner-images/c11adc344c44671ee91d8a36a0f16185.svg\",mainImage:\"/partner-images/e4726fd14e77afc1e303d4d18c216dd4.svg\",buttonCopy:\"Get Started\",buttonLink:\"/signup\",betterTitle:\"Better Together\",betterDescription:\"Release and AWS partner to help organizations of all sizes deploy predictable, scalable and isolated environments throughout the whole application lifecycle.\",betterCards:['<figure class=\"w-richtext-figure-type-image w-richtext-align-normal\" data-rt-type=\"image\" data-rt-align=\"normal\"><div><img src=\"https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/65ca4b217375e35a5653f46f_Seamless%20Integration%20-%20icon.svg\" loading=\"lazy\"></div></figure><h5>Seamless Integration</h5><p>Release runs in your AWS account and automatically provisions and de-provisions necessary resources for your environments.</p>','<figure id=\"\" class=\"w-richtext-figure-type-image w-richtext-align-normal\" data-rt-type=\"image\" data-rt-align=\"normal\"><div id=\"\"><img src=\"https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/65ca4b4003e629bd11e72326_Optimize%20cloud%20spend%20-%20icon.svg\" loading=\"lazy\" width=\"auto\" height=\"auto\" alt=\"\" id=\"\"></div></figure><h5 id=\"\">Ease of Billing</h5><p id=\"\">Draw down on your spend commitments and get a consolidated bill for all AWS and Release services. Available on <a href=\"https://aws.amazon.com/marketplace/pp/prodview-faxqlvzq65fea?sr=0-1&ref_=beagle&applicationId=AWSMPContessa\" target=\"_blank\">Marketplace.</a></p>','<figure class=\"w-richtext-figure-type-image w-richtext-align-normal\" data-rt-type=\"image\" data-rt-align=\"normal\"><div><img src=\"https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/65ca543bc6170a3e068ced2c_Production-like%20and%20Data-rich%20logo.svg\" loading=\"lazy\"></div></figure><h5>Production-like and Data-rich</h5><p>All your environments in AWS are as close to production as you need them, with instant access to production-like data.</p>','<figure id=\"\" class=\"w-richtext-figure-type-image w-richtext-align-normal\" data-rt-type=\"image\" data-rt-align=\"normal\"><div id=\"\"><img src=\"https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/65ca547729ce90bfbb3da98f_Cloud%20Optimized%20logo.svg\" loading=\"lazy\" width=\"auto\" height=\"auto\" alt=\"\" id=\"\"></div></figure><h5 id=\"\">Cloud Optimized</h5><p id=\"\">Release optimizes all your environments, &nbsp;so you pay for only what you need, when you need it. Now you can spend the freed-up resources on more interesting projects.</p>','<figure class=\"w-richtext-figure-type-image w-richtext-align-normal\" data-rt-type=\"image\" data-rt-align=\"normal\"><div><img src=\"https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/65ca4b904ae9bed8a538fa6f_Cloud%20on%20your%20terms%20icon.svg\" loading=\"lazy\"></div></figure><h5>Cloud on your terms</h5><p>Consistent cloud configuration experience when running in multiple clouds. Simplified day-to-day operations, while maintaining full control over the advanced settings.</p>','<figure id=\"\" class=\"w-richtext-figure-type-image w-richtext-align-normal\" data-rt-type=\"image\" data-rt-align=\"normal\"><div id=\"\"><img src=\"https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/65ca4bacb6b468d0df7a6f32_Preview%20Programs%20icon.svg\" loading=\"lazy\" width=\"auto\" height=\"auto\" alt=\"\" id=\"\"></div></figure><h5 id=\"\">Preview Programs</h5><p id=\"\">Joint customers get access to preview programs of new features and products. Be at the forefront of product development and help shape the solutions that work for you.</p>'],publishDate:\"Wed Feb 14 2024 17:55:49 GMT+0000 (Coordinated Universal Time)\"};function c(e){return(0,i.jsx)(i.Fragment,{})}function x(e={}){let{wrapper:a}=e.components||{};return a?(0,i.jsx)(a,Object.assign({},e,{children:(0,i.jsx)(c,e)})):c(e)}var z=x;return y(C);})();\n;return Component;"
        },
        "_id": "partners/content/aws.mdx",
        "_raw": {
          "sourceFilePath": "partners/content/aws.mdx",
          "sourceFileName": "aws.mdx",
          "sourceFileDir": "partners/content",
          "contentType": "mdx",
          "flattenedPath": "partners/content/aws"
        },
        "type": "Partner",
        "computedSlug": "aws"
      },
      "documentHash": "1739648351149",
      "hasWarnings": false,
      "documentTypeName": "Partner"
    },
    "partners/content/codingscape.mdx": {
      "document": {
        "title": "Release + Codingscape",
        "slug": "codingscape",
        "description": "Expert migration and implementation services for Release environments.",
        "category": "Migration partner",
        "thumbnail": "/partner-images/64e4b363acf9e6098a85773d2e239d26.svg",
        "mainImage": "/partner-images/64e4b363acf9e6098a85773d2e239d26.svg",
        "buttonCopy": "Contact Us",
        "buttonLink": "mailto:support@release.com",
        "betterTitle": "Better Together",
        "betterDescription": "Release and Codingscape partner to provide organizations with expert guidance and implementation services for their cloud environment needs.",
        "betterCards": [
          "<figure class=\"w-richtext-figure-type-image w-richtext-align-normal\"><div><img src=\"https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/65ca4b217375e35a5653f46f_Seamless%20Integration%20-%20icon.svg\" loading=\"lazy\"></div></figure><h5>Expert Migration</h5><p>Get professional assistance in migrating your existing environments to Release's platform with minimal disruption.</p>",
          "<figure class=\"w-richtext-figure-type-image w-richtext-align-normal\"><div><img src=\"https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/65ca4b4003e629bd11e72326_Optimize%20cloud%20spend%20-%20icon.svg\" loading=\"lazy\"></div></figure><h5>Implementation Support</h5><p>Receive dedicated support in implementing Release's features and best practices for your specific use cases.</p>",
          "<figure class=\"w-richtext-figure-type-image w-richtext-align-normal\"><div><img src=\"https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/65ca4bacb6b468d0df7a6f32_Preview%20Programs%20icon.svg\" loading=\"lazy\"></div></figure><h5>Custom Solutions</h5><p>Get tailored solutions and integrations that match your organization's unique requirements and workflows.</p>"
        ],
        "publishDate": "2024-02-14T15:49:13.000Z",
        "body": {
          "raw": "",
          "code": "var Component=(()=>{var g=Object.create;var r=Object.defineProperty;var p=Object.getOwnPropertyDescriptor;var m=Object.getOwnPropertyNames;var u=Object.getPrototypeOf,f=Object.prototype.hasOwnProperty;var b=(e,t)=>()=>(t||e((t={exports:{}}).exports,t),t.exports),h=(e,t)=>{for(var i in t)r(e,i,{get:t[i],enumerable:!0})},o=(e,t,i,s)=>{if(t&&typeof t==\"object\"||typeof t==\"function\")for(let n of m(t))!f.call(e,n)&&n!==i&&r(e,n,{get:()=>t[n],enumerable:!(s=p(t,n))||s.enumerable});return e};var v=(e,t,i)=>(i=e!=null?g(u(e)):{},o(t||!e||!e.__esModule?r(i,\"default\",{value:e,enumerable:!0}):i,e)),w=e=>o(r({},\"__esModule\",{value:!0}),e);var c=b((M,d)=>{d.exports=_jsx_runtime});var _={};h(_,{default:()=>C,frontmatter:()=>x});var a=v(c()),x={title:\"Release + Codingscape\",slug:\"codingscape\",description:\"Expert migration and implementation services for Release environments.\",category:\"Migration partner\",thumbnail:\"/partner-images/64e4b363acf9e6098a85773d2e239d26.svg\",mainImage:\"/partner-images/64e4b363acf9e6098a85773d2e239d26.svg\",buttonCopy:\"Contact Us\",buttonLink:\"mailto:support@release.com\",betterTitle:\"Better Together\",betterDescription:\"Release and Codingscape partner to provide organizations with expert guidance and implementation services for their cloud environment needs.\",betterCards:[`<figure class=\"w-richtext-figure-type-image w-richtext-align-normal\"><div><img src=\"https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/65ca4b217375e35a5653f46f_Seamless%20Integration%20-%20icon.svg\" loading=\"lazy\"></div></figure><h5>Expert Migration</h5><p>Get professional assistance in migrating your existing environments to Release's platform with minimal disruption.</p>`,`<figure class=\"w-richtext-figure-type-image w-richtext-align-normal\"><div><img src=\"https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/65ca4b4003e629bd11e72326_Optimize%20cloud%20spend%20-%20icon.svg\" loading=\"lazy\"></div></figure><h5>Implementation Support</h5><p>Receive dedicated support in implementing Release's features and best practices for your specific use cases.</p>`,`<figure class=\"w-richtext-figure-type-image w-richtext-align-normal\"><div><img src=\"https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/65ca4bacb6b468d0df7a6f32_Preview%20Programs%20icon.svg\" loading=\"lazy\"></div></figure><h5>Custom Solutions</h5><p>Get tailored solutions and integrations that match your organization's unique requirements and workflows.</p>`],publishDate:\"Wed Feb 14 2024 15:49:13 GMT+0000 (Coordinated Universal Time)\"};function l(e){return(0,a.jsx)(a.Fragment,{})}function y(e={}){let{wrapper:t}=e.components||{};return t?(0,a.jsx)(t,Object.assign({},e,{children:(0,a.jsx)(l,e)})):l(e)}var C=y;return w(_);})();\n;return Component;"
        },
        "_id": "partners/content/codingscape.mdx",
        "_raw": {
          "sourceFilePath": "partners/content/codingscape.mdx",
          "sourceFileName": "codingscape.mdx",
          "sourceFileDir": "partners/content",
          "contentType": "mdx",
          "flattenedPath": "partners/content/codingscape"
        },
        "type": "Partner",
        "computedSlug": "codingscape"
      },
      "documentHash": "1739648608317",
      "hasWarnings": false,
      "documentTypeName": "Partner"
    },
    "partners/content/docker.mdx": {
      "document": {
        "title": "Release + Docker",
        "slug": "docker",
        "description": "Streamline your container development workflow with Release's Docker integration.",
        "category": "Docker Verified Publisher",
        "thumbnail": "/partner-images/565afee7ff72c86a0e763ff9e097ed7d.svg",
        "mainImage": "/partner-images/565afee7ff72c86a0e763ff9e097ed7d.svg",
        "buttonCopy": "Learn More",
        "buttonLink": "/product/docker-extension",
        "betterTitle": "Better Together",
        "betterDescription": "Release and Docker partner to provide developers with seamless container management and deployment capabilities directly from their development environment.",
        "betterCards": [
          "<figure class=\"w-richtext-figure-type-image w-richtext-align-normal\"><div><img src=\"https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/65ca4b217375e35a5653f46f_Seamless%20Integration%20-%20icon.svg\" loading=\"lazy\"></div></figure><h5>Docker Extension</h5><p>Access Release's environment management capabilities directly from Docker Desktop with our official extension.</p>",
          "<figure class=\"w-richtext-figure-type-image w-richtext-align-normal\"><div><img src=\"https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/65ca547729ce90bfbb3da98f_Cloud%20Optimized%20logo.svg\" loading=\"lazy\"></div></figure><h5>Container Optimization</h5><p>Automatically optimize container configurations for different environments while maintaining consistency across your deployment pipeline.</p>",
          "<figure class=\"w-richtext-figure-type-image w-richtext-align-normal\"><div><img src=\"https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/65ca4b904ae9bed8a538fa6f_Cloud%20on%20your%20terms%20icon.svg\" loading=\"lazy\"></div></figure><h5>Simplified Workflow</h5><p>Manage your containerized applications and environments directly from your local development workflow.</p>"
        ],
        "publishDate": "2024-02-14T15:49:13.000Z",
        "body": {
          "raw": "",
          "code": "var Component=(()=>{var m=Object.create;var r=Object.defineProperty;var f=Object.getOwnPropertyDescriptor;var p=Object.getOwnPropertyNames;var g=Object.getPrototypeOf,u=Object.prototype.hasOwnProperty;var b=(e,t)=>()=>(t||e((t={exports:{}}).exports,t),t.exports),h=(e,t)=>{for(var i in t)r(e,i,{get:t[i],enumerable:!0})},s=(e,t,i,a)=>{if(t&&typeof t==\"object\"||typeof t==\"function\")for(let n of p(t))!u.call(e,n)&&n!==i&&r(e,n,{get:()=>t[n],enumerable:!(a=f(t,n))||a.enumerable});return e};var w=(e,t,i)=>(i=e!=null?m(g(e)):{},s(t||!e||!e.__esModule?r(i,\"default\",{value:e,enumerable:!0}):i,e)),v=e=>s(r({},\"__esModule\",{value:!0}),e);var c=b((_,l)=>{l.exports=_jsx_runtime});var D={};h(D,{default:()=>k,frontmatter:()=>y});var o=w(c()),y={title:\"Release + Docker\",slug:\"docker\",description:\"Streamline your container development workflow with Release's Docker integration.\",category:\"Docker Verified Publisher\",thumbnail:\"/partner-images/565afee7ff72c86a0e763ff9e097ed7d.svg\",mainImage:\"/partner-images/565afee7ff72c86a0e763ff9e097ed7d.svg\",buttonCopy:\"Learn More\",buttonLink:\"/product/docker-extension\",betterTitle:\"Better Together\",betterDescription:\"Release and Docker partner to provide developers with seamless container management and deployment capabilities directly from their development environment.\",betterCards:[`<figure class=\"w-richtext-figure-type-image w-richtext-align-normal\"><div><img src=\"https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/65ca4b217375e35a5653f46f_Seamless%20Integration%20-%20icon.svg\" loading=\"lazy\"></div></figure><h5>Docker Extension</h5><p>Access Release's environment management capabilities directly from Docker Desktop with our official extension.</p>`,'<figure class=\"w-richtext-figure-type-image w-richtext-align-normal\"><div><img src=\"https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/65ca547729ce90bfbb3da98f_Cloud%20Optimized%20logo.svg\" loading=\"lazy\"></div></figure><h5>Container Optimization</h5><p>Automatically optimize container configurations for different environments while maintaining consistency across your deployment pipeline.</p>','<figure class=\"w-richtext-figure-type-image w-richtext-align-normal\"><div><img src=\"https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/65ca4b904ae9bed8a538fa6f_Cloud%20on%20your%20terms%20icon.svg\" loading=\"lazy\"></div></figure><h5>Simplified Workflow</h5><p>Manage your containerized applications and environments directly from your local development workflow.</p>'],publishDate:\"Wed Feb 14 2024 15:49:13 GMT+0000 (Coordinated Universal Time)\"};function d(e){return(0,o.jsx)(o.Fragment,{})}function x(e={}){let{wrapper:t}=e.components||{};return t?(0,o.jsx)(t,Object.assign({},e,{children:(0,o.jsx)(d,e)})):d(e)}var k=x;return v(D);})();\n;return Component;"
        },
        "_id": "partners/content/docker.mdx",
        "_raw": {
          "sourceFilePath": "partners/content/docker.mdx",
          "sourceFileName": "docker.mdx",
          "sourceFileDir": "partners/content",
          "contentType": "mdx",
          "flattenedPath": "partners/content/docker"
        },
        "type": "Partner",
        "computedSlug": "docker"
      },
      "documentHash": "1739648663392",
      "hasWarnings": false,
      "documentTypeName": "Partner"
    },
    "partners/content/google.mdx": {
      "document": {
        "title": "Release + Google Cloud",
        "slug": "google",
        "description": "Production, development and preview environments in your Google Cloud, your way.",
        "category": "Cloud partner",
        "thumbnail": "/partner-images/576355d56ed563cac01f5af8f75f1f1b.svg",
        "mainImage": "/partner-images/9aa078c6f27efce8a6943a20bc4edb00.svg",
        "buttonCopy": "Get Started",
        "buttonLink": "/signup",
        "betterTitle": "Better Together",
        "betterDescription": "Release and Google Cloud partner to help organizations of all sizes deploy predictable, scalable and isolated environments throughout the whole application lifecycle.",
        "betterCards": [
          "<figure id=\"\" class=\"w-richtext-figure-type-image w-richtext-align-normal\" style=\"max-width:60px\" data-rt-type=\"image\" data-rt-align=\"normal\" data-rt-max-width=\"60px\"><div id=\"\"><img src=\"https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/65ca4b217375e35a5653f46f_Seamless%20Integration%20-%20icon.svg\" loading=\"lazy\" width=\"auto\" height=\"auto\" alt=\"\" id=\"\"></div></figure><h5 id=\"\">Seamless Integration</h5><p id=\"\">Release runs in your Google Cloud and simplifies infrastructure deployment for even most complex applications.</p>",
          "<figure id=\"\" class=\"w-richtext-figure-type-image w-richtext-align-normal\" style=\"max-width:60px\" data-rt-type=\"image\" data-rt-align=\"normal\" data-rt-max-width=\"60px\"><div id=\"\"><img src=\"https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/65ca4b4003e629bd11e72326_Optimize%20cloud%20spend%20-%20icon.svg\" loading=\"lazy\" width=\"auto\" height=\"auto\" alt=\"\" id=\"\"></div></figure><h5 id=\"\">Optimize cloud spend</h5><p id=\"\">With environment pausing and auto-teardown capabilities, you use the cloud resources only when you need them, no wasted spend.</p>",
          "<figure class=\"w-richtext-figure-type-image w-richtext-align-normal\" style=\"max-width:60px\" data-rt-type=\"image\" data-rt-align=\"normal\" data-rt-max-width=\"60px\"><div><img src=\"https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/65ca4b5e49e0bcb7b93e9c8a_Production-like%20and%20Data-rich%20-%20icon.svg\" loading=\"lazy\"></div></figure><h5 id=\"\">Production-like and Data-rich</h5><p id=\"\">All your environments in Google Cloud are as close to production as you need them, with instant access to production-like data.</p>",
          "<figure class=\"w-richtext-figure-type-image w-richtext-align-normal\" style=\"max-width:60px\" data-rt-type=\"image\" data-rt-align=\"normal\" data-rt-max-width=\"60px\"><div><img src=\"https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/65ca4b76b6b468d0df7a570a_Kubernetes%20made%20simpler%20icon.svg\" loading=\"lazy\"></div></figure><h5 id=\"\">Kubernetes made simpler</h5><p id=\"\">Abstract away the complexities of Kubernetes so developers can focus on your application, not manual cluster setup or writing low-level YAML.</p>",
          "<figure class=\"w-richtext-figure-type-image w-richtext-align-normal\" style=\"max-width:60px\" data-rt-type=\"image\" data-rt-align=\"normal\" data-rt-max-width=\"60px\"><div><img src=\"https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/65ca4b904ae9bed8a538fa6f_Cloud%20on%20your%20terms%20icon.svg\" loading=\"lazy\"></div></figure><h5 id=\"\">Cloud on your terms</h5><p id=\"\">Consistent cloud configuration experience when running in multiple clouds. Simplified day-to-day operations, while maintaining full control over the advanced settings.</p>",
          "<figure class=\"w-richtext-figure-type-image w-richtext-align-normal\" style=\"max-width:60px\" data-rt-type=\"image\" data-rt-align=\"normal\" data-rt-max-width=\"60px\"><div><img src=\"https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/65ca4bacb6b468d0df7a6f32_Preview%20Programs%20icon.svg\" loading=\"lazy\"></div></figure><h5 id=\"\">Preview Programs</h5><p id=\"\">Joint customers get access to preview programs of new features and products. Be at the forefront of product development and help shape the solutions that work for you.</p>"
        ],
        "publishDate": "2024-02-14T17:03:21.000Z",
        "body": {
          "raw": "",
          "code": "var Component=(()=>{var p=Object.create;var r=Object.defineProperty;var g=Object.getOwnPropertyDescriptor;var u=Object.getOwnPropertyNames;var m=Object.getPrototypeOf,h=Object.prototype.hasOwnProperty;var f=(e,t)=>()=>(t||e((t={exports:{}}).exports,t),t.exports),w=(e,t)=>{for(var a in t)r(e,a,{get:t[a],enumerable:!0})},n=(e,t,a,d)=>{if(t&&typeof t==\"object\"||typeof t==\"function\")for(let o of u(t))!h.call(e,o)&&o!==a&&r(e,o,{get:()=>t[o],enumerable:!(d=g(t,o))||d.enumerable});return e};var b=(e,t,a)=>(a=e!=null?p(m(e)):{},n(t||!e||!e.__esModule?r(a,\"default\",{value:e,enumerable:!0}):a,e)),x=e=>n(r({},\"__esModule\",{value:!0}),e);var s=f((G,l)=>{l.exports=_jsx_runtime});var _={};w(_,{default:()=>C,frontmatter:()=>y});var i=b(s()),y={title:\"Release + Google Cloud\",slug:\"google\",description:\"Production, development and preview environments in your Google Cloud, your way.\",category:\"Cloud partner\",thumbnail:\"/partner-images/576355d56ed563cac01f5af8f75f1f1b.svg\",mainImage:\"/partner-images/9aa078c6f27efce8a6943a20bc4edb00.svg\",buttonCopy:\"Get Started\",buttonLink:\"/signup\",betterTitle:\"Better Together\",betterDescription:\"Release and Google Cloud partner to help organizations of all sizes deploy predictable, scalable and isolated environments throughout the whole application lifecycle.\",betterCards:['<figure id=\"\" class=\"w-richtext-figure-type-image w-richtext-align-normal\" style=\"max-width:60px\" data-rt-type=\"image\" data-rt-align=\"normal\" data-rt-max-width=\"60px\"><div id=\"\"><img src=\"https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/65ca4b217375e35a5653f46f_Seamless%20Integration%20-%20icon.svg\" loading=\"lazy\" width=\"auto\" height=\"auto\" alt=\"\" id=\"\"></div></figure><h5 id=\"\">Seamless Integration</h5><p id=\"\">Release runs in your Google Cloud and simplifies infrastructure deployment for even most complex applications.</p>','<figure id=\"\" class=\"w-richtext-figure-type-image w-richtext-align-normal\" style=\"max-width:60px\" data-rt-type=\"image\" data-rt-align=\"normal\" data-rt-max-width=\"60px\"><div id=\"\"><img src=\"https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/65ca4b4003e629bd11e72326_Optimize%20cloud%20spend%20-%20icon.svg\" loading=\"lazy\" width=\"auto\" height=\"auto\" alt=\"\" id=\"\"></div></figure><h5 id=\"\">Optimize cloud spend</h5><p id=\"\">With environment pausing and auto-teardown capabilities, you use the cloud resources only when you need them, no wasted spend.</p>','<figure class=\"w-richtext-figure-type-image w-richtext-align-normal\" style=\"max-width:60px\" data-rt-type=\"image\" data-rt-align=\"normal\" data-rt-max-width=\"60px\"><div><img src=\"https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/65ca4b5e49e0bcb7b93e9c8a_Production-like%20and%20Data-rich%20-%20icon.svg\" loading=\"lazy\"></div></figure><h5 id=\"\">Production-like and Data-rich</h5><p id=\"\">All your environments in Google Cloud are as close to production as you need them, with instant access to production-like data.</p>','<figure class=\"w-richtext-figure-type-image w-richtext-align-normal\" style=\"max-width:60px\" data-rt-type=\"image\" data-rt-align=\"normal\" data-rt-max-width=\"60px\"><div><img src=\"https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/65ca4b76b6b468d0df7a570a_Kubernetes%20made%20simpler%20icon.svg\" loading=\"lazy\"></div></figure><h5 id=\"\">Kubernetes made simpler</h5><p id=\"\">Abstract away the complexities of Kubernetes so developers can focus on your application, not manual cluster setup or writing low-level YAML.</p>','<figure class=\"w-richtext-figure-type-image w-richtext-align-normal\" style=\"max-width:60px\" data-rt-type=\"image\" data-rt-align=\"normal\" data-rt-max-width=\"60px\"><div><img src=\"https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/65ca4b904ae9bed8a538fa6f_Cloud%20on%20your%20terms%20icon.svg\" loading=\"lazy\"></div></figure><h5 id=\"\">Cloud on your terms</h5><p id=\"\">Consistent cloud configuration experience when running in multiple clouds. Simplified day-to-day operations, while maintaining full control over the advanced settings.</p>','<figure class=\"w-richtext-figure-type-image w-richtext-align-normal\" style=\"max-width:60px\" data-rt-type=\"image\" data-rt-align=\"normal\" data-rt-max-width=\"60px\"><div><img src=\"https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/65ca4bacb6b468d0df7a6f32_Preview%20Programs%20icon.svg\" loading=\"lazy\"></div></figure><h5 id=\"\">Preview Programs</h5><p id=\"\">Joint customers get access to preview programs of new features and products. Be at the forefront of product development and help shape the solutions that work for you.</p>'],publishDate:\"Wed Feb 14 2024 17:03:21 GMT+0000 (Coordinated Universal Time)\"};function c(e){return(0,i.jsx)(i.Fragment,{})}function v(e={}){let{wrapper:t}=e.components||{};return t?(0,i.jsx)(t,Object.assign({},e,{children:(0,i.jsx)(c,e)})):c(e)}var C=v;return x(_);})();\n;return Component;"
        },
        "_id": "partners/content/google.mdx",
        "_raw": {
          "sourceFilePath": "partners/content/google.mdx",
          "sourceFileName": "google.mdx",
          "sourceFileDir": "partners/content",
          "contentType": "mdx",
          "flattenedPath": "partners/content/google"
        },
        "type": "Partner",
        "computedSlug": "google"
      },
      "documentHash": "1739648462180",
      "hasWarnings": false,
      "documentTypeName": "Partner"
    },
    "partners/content/nvidia.mdx": {
      "document": {
        "title": "Welcome to the Future of AI with Release and NVIDIA",
        "slug": "nvidia",
        "description": "Bringing self-service AI environments tailored for NVIDIA's powerful GPUs to your fingertips.",
        "category": "AI stack partner",
        "thumbnail": "/partner-images/fc0911e0f23fd4e98d43f0b3c83c894c.svg",
        "mainImage": "/partner-images/5bd77296623aa78e839aa0e43f44e1a0.svg",
        "featureImage": "/partner-images/1784daf3cc8c84a01fce45ec27577072.svg",
        "buttonCopy": "Start Building Now",
        "buttonLink": "/signup",
        "featureTitle": "Democratizing AI Training, Testing and Deployment",
        "featureList": "<ul id=\"\"><li id=\"\">Release is thrilled to be an NVIDIA Inception Program Partner working on cutting edge technologies and solutions for our joint customers.</li><li id=\"\">Together we unlock the value in your data by automating the creation of full-stack AI applications for NVIDIA GPUs.</li><li id=\"\">Want to join the preview program for new products we're building? Email us at <a href=\"mailto:partner@release.com\">partner@release.com</a></li></ul>",
        "betterTitle": "Better Together",
        "betterDescription": "Release and NVIDIA partner to help organizations deploy and manage AI environments with ease, bringing powerful GPU capabilities to your development workflow.",
        "betterCards": [
          "<figure class=\"w-richtext-figure-type-image w-richtext-align-normal\"><div><img src=\"https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/65ca4b217375e35a5653f46f_Seamless%20Integration%20-%20icon.svg\" loading=\"lazy\"></div></figure><h5>Automated AI Environments</h5><p>Release automates the creation of AI environments on the NVIDIA stack, offering a seamless, cost-effective solution for accessing the computational power of GPUs.</p>",
          "<figure class=\"w-richtext-figure-type-image w-richtext-align-normal\"><div><img src=\"https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/65ca4b4003e629bd11e72326_Optimize%20cloud%20spend%20-%20icon.svg\" loading=\"lazy\"></div></figure><h5>Tailored for Your Needs</h5><p>Customize your AI technology stack with everything from frameworks and foundation models to tools and inference engines, all deployed in your AI infrastructure automatically.</p>",
          "<figure class=\"w-richtext-figure-type-image w-richtext-align-normal\"><div><img src=\"https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/65ca4bacb6b468d0df7a6f32_Preview%20Programs%20icon.svg\" loading=\"lazy\"></div></figure><h5>Ephemeral AI Environments</h5><p>Optimize costs and enhance security with ephemeral AI environments that are activated only when needed, and equipped with guardrails to prevent data mishaps.</p>"
        ],
        "publishDate": "2024-02-14T17:24:50.000Z",
        "body": {
          "raw": "",
          "code": "var Component=(()=>{var g=Object.create;var n=Object.defineProperty;var u=Object.getOwnPropertyDescriptor;var m=Object.getOwnPropertyNames;var f=Object.getPrototypeOf,p=Object.prototype.hasOwnProperty;var h=(e,t)=>()=>(t||e((t={exports:{}}).exports,t),t.exports),w=(e,t)=>{for(var a in t)n(e,a,{get:t[a],enumerable:!0})},s=(e,t,a,o)=>{if(t&&typeof t==\"object\"||typeof t==\"function\")for(let r of m(t))!p.call(e,r)&&r!==a&&n(e,r,{get:()=>t[r],enumerable:!(o=u(t,r))||o.enumerable});return e};var b=(e,t,a)=>(a=e!=null?g(f(e)):{},s(t||!e||!e.__esModule?n(a,\"default\",{value:e,enumerable:!0}):a,e)),v=e=>s(n({},\"__esModule\",{value:!0}),e);var c=h((k,l)=>{l.exports=_jsx_runtime});var x={};w(x,{default:()=>A,frontmatter:()=>I});var i=b(c()),I={title:\"Welcome to the Future of AI with Release and NVIDIA\",slug:\"nvidia\",description:\"Bringing self-service AI environments tailored for NVIDIA's powerful GPUs to your fingertips.\",category:\"AI stack partner\",thumbnail:\"/partner-images/fc0911e0f23fd4e98d43f0b3c83c894c.svg\",mainImage:\"/partner-images/5bd77296623aa78e839aa0e43f44e1a0.svg\",featureImage:\"/partner-images/1784daf3cc8c84a01fce45ec27577072.svg\",buttonCopy:\"Start Building Now\",buttonLink:\"/signup\",featureTitle:\"Democratizing AI Training, Testing and Deployment\",featureList:`<ul id=\"\"><li id=\"\">Release is thrilled to be an NVIDIA Inception Program Partner working on cutting edge technologies and solutions for our joint customers.</li><li id=\"\">Together we unlock the value in your data by automating the creation of full-stack AI applications for NVIDIA GPUs.</li><li id=\"\">Want to join the preview program for new products we're building? Email us at <a href=\"mailto:partner@release.com\">partner@release.com</a></li></ul>`,betterTitle:\"Better Together\",betterDescription:\"Release and NVIDIA partner to help organizations deploy and manage AI environments with ease, bringing powerful GPU capabilities to your development workflow.\",betterCards:['<figure class=\"w-richtext-figure-type-image w-richtext-align-normal\"><div><img src=\"https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/65ca4b217375e35a5653f46f_Seamless%20Integration%20-%20icon.svg\" loading=\"lazy\"></div></figure><h5>Automated AI Environments</h5><p>Release automates the creation of AI environments on the NVIDIA stack, offering a seamless, cost-effective solution for accessing the computational power of GPUs.</p>','<figure class=\"w-richtext-figure-type-image w-richtext-align-normal\"><div><img src=\"https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/65ca4b4003e629bd11e72326_Optimize%20cloud%20spend%20-%20icon.svg\" loading=\"lazy\"></div></figure><h5>Tailored for Your Needs</h5><p>Customize your AI technology stack with everything from frameworks and foundation models to tools and inference engines, all deployed in your AI infrastructure automatically.</p>','<figure class=\"w-richtext-figure-type-image w-richtext-align-normal\"><div><img src=\"https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/65ca4bacb6b468d0df7a6f32_Preview%20Programs%20icon.svg\" loading=\"lazy\"></div></figure><h5>Ephemeral AI Environments</h5><p>Optimize costs and enhance security with ephemeral AI environments that are activated only when needed, and equipped with guardrails to prevent data mishaps.</p>'],publishDate:\"Wed Feb 14 2024 17:24:50 GMT+0000 (Coordinated Universal Time)\"};function d(e){return(0,i.jsx)(i.Fragment,{})}function y(e={}){let{wrapper:t}=e.components||{};return t?(0,i.jsx)(t,Object.assign({},e,{children:(0,i.jsx)(d,e)})):d(e)}var A=y;return v(x);})();\n;return Component;"
        },
        "_id": "partners/content/nvidia.mdx",
        "_raw": {
          "sourceFilePath": "partners/content/nvidia.mdx",
          "sourceFileName": "nvidia.mdx",
          "sourceFileDir": "partners/content",
          "contentType": "mdx",
          "flattenedPath": "partners/content/nvidia"
        },
        "type": "Partner",
        "computedSlug": "nvidia"
      },
      "documentHash": "1739648477659",
      "hasWarnings": false,
      "documentTypeName": "Partner"
    },
    "partners/content/tonic.mdx": {
      "document": {
        "title": "Release + Tonic",
        "slug": "tonic",
        "description": "Generate high-quality synthetic data for your development and preview environments.",
        "category": "Technology partner",
        "thumbnail": "/partner-images/a0f62858c524db6b963463d8d14d2593.svg",
        "mainImage": "/partner-images/a0f62858c524db6b963463d8d14d2593.svg",
        "buttonCopy": "Get Started",
        "buttonLink": "/signup",
        "betterTitle": "Better Together",
        "betterDescription": "Release and Tonic partner to help organizations maintain data privacy while enabling developers to work with realistic data in their environments.",
        "betterCards": [
          "<figure class=\"w-richtext-figure-type-image w-richtext-align-normal\"><div><img src=\"https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/65ca4b217375e35a5653f46f_Seamless%20Integration%20-%20icon.svg\" loading=\"lazy\"></div></figure><h5>Synthetic Data Generation</h5><p>Automatically generate and refresh synthetic data in your Release environments, maintaining data privacy while preserving data relationships and characteristics.</p>",
          "<figure class=\"w-richtext-figure-type-image w-richtext-align-normal\"><div><img src=\"https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/65ca4b4003e629bd11e72326_Optimize%20cloud%20spend%20-%20icon.svg\" loading=\"lazy\"></div></figure><h5>Privacy by Design</h5><p>Ensure compliance and protect sensitive information while giving developers access to realistic data for testing and development.</p>",
          "<figure class=\"w-richtext-figure-type-image w-richtext-align-normal\"><div><img src=\"https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/65ca543bc6170a3e068ced2c_Production-like%20and%20Data-rich%20logo.svg\" loading=\"lazy\"></div></figure><h5>Production-like Data</h5><p>Work with data that maintains the same statistical properties and relationships as production data, ensuring reliable testing and development.</p>"
        ],
        "publishDate": "2024-02-14T15:49:13.000Z",
        "body": {
          "raw": "",
          "code": "var Component=(()=>{var g=Object.create;var r=Object.defineProperty;var p=Object.getOwnPropertyDescriptor;var m=Object.getOwnPropertyNames;var h=Object.getPrototypeOf,u=Object.prototype.hasOwnProperty;var b=(e,t)=>()=>(t||e((t={exports:{}}).exports,t),t.exports),v=(e,t)=>{for(var a in t)r(e,a,{get:t[a],enumerable:!0})},o=(e,t,a,s)=>{if(t&&typeof t==\"object\"||typeof t==\"function\")for(let n of m(t))!u.call(e,n)&&n!==a&&r(e,n,{get:()=>t[n],enumerable:!(s=p(t,n))||s.enumerable});return e};var f=(e,t,a)=>(a=e!=null?g(h(e)):{},o(t||!e||!e.__esModule?r(a,\"default\",{value:e,enumerable:!0}):a,e)),y=e=>o(r({},\"__esModule\",{value:!0}),e);var c=b((k,d)=>{d.exports=_jsx_runtime});var _={};v(_,{default:()=>D,frontmatter:()=>w});var i=f(c()),w={title:\"Release + Tonic\",slug:\"tonic\",description:\"Generate high-quality synthetic data for your development and preview environments.\",category:\"Technology partner\",thumbnail:\"/partner-images/a0f62858c524db6b963463d8d14d2593.svg\",mainImage:\"/partner-images/a0f62858c524db6b963463d8d14d2593.svg\",buttonCopy:\"Get Started\",buttonLink:\"/signup\",betterTitle:\"Better Together\",betterDescription:\"Release and Tonic partner to help organizations maintain data privacy while enabling developers to work with realistic data in their environments.\",betterCards:['<figure class=\"w-richtext-figure-type-image w-richtext-align-normal\"><div><img src=\"https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/65ca4b217375e35a5653f46f_Seamless%20Integration%20-%20icon.svg\" loading=\"lazy\"></div></figure><h5>Synthetic Data Generation</h5><p>Automatically generate and refresh synthetic data in your Release environments, maintaining data privacy while preserving data relationships and characteristics.</p>','<figure class=\"w-richtext-figure-type-image w-richtext-align-normal\"><div><img src=\"https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/65ca4b4003e629bd11e72326_Optimize%20cloud%20spend%20-%20icon.svg\" loading=\"lazy\"></div></figure><h5>Privacy by Design</h5><p>Ensure compliance and protect sensitive information while giving developers access to realistic data for testing and development.</p>','<figure class=\"w-richtext-figure-type-image w-richtext-align-normal\"><div><img src=\"https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/65ca543bc6170a3e068ced2c_Production-like%20and%20Data-rich%20logo.svg\" loading=\"lazy\"></div></figure><h5>Production-like Data</h5><p>Work with data that maintains the same statistical properties and relationships as production data, ensuring reliable testing and development.</p>'],publishDate:\"Wed Feb 14 2024 15:49:13 GMT+0000 (Coordinated Universal Time)\"};function l(e){return(0,i.jsx)(i.Fragment,{})}function x(e={}){let{wrapper:t}=e.components||{};return t?(0,i.jsx)(t,Object.assign({},e,{children:(0,i.jsx)(l,e)})):l(e)}var D=x;return y(_);})();\n;return Component;"
        },
        "_id": "partners/content/tonic.mdx",
        "_raw": {
          "sourceFilePath": "partners/content/tonic.mdx",
          "sourceFileName": "tonic.mdx",
          "sourceFileDir": "partners/content",
          "contentType": "mdx",
          "flattenedPath": "partners/content/tonic"
        },
        "type": "Partner",
        "computedSlug": "tonic"
      },
      "documentHash": "1739648484124",
      "hasWarnings": false,
      "documentTypeName": "Partner"
    }
  }
}
