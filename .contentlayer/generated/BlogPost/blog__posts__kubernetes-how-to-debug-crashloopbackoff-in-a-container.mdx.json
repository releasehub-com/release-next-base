{
  "title": "Kubernetes - How to Debug CrashLoopBackOff in a Container",
  "summary": "If you've used Kubernetes (k8s), you've probably bumped into the dreaded CrashLoopBackOff. A CrashLoopBackOff is possibl",
  "publishDate": "Thu Feb 04 2021 19:23:56 GMT+0000 (Coordinated Universal Time)",
  "author": "david-giffin",
  "readingTime": 3,
  "categories": [
    "kubernetes",
    "platform-engineering"
  ],
  "mainImage": "/blog-images/4e91e41da26a361f3123c6c7a68815ee.jpg",
  "imageAlt": "Storage containers stacked representing a CrashLoopBackOff in a container",
  "showCTA": true,
  "ctaCopy": "Struggling with CrashLoopBackOff in Kubernetes? Use Release for streamlined debugging environments and faster issue resolution.",
  "ctaLink": "https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=kubernetes-how-to-debug-crashloopbackoff-in-a-container",
  "relatedPosts": [
    ""
  ],
  "ogImage": "/blog-images/4e91e41da26a361f3123c6c7a68815ee.jpg",
  "excerpt": "If you've used Kubernetes (k8s), you've probably bumped into the dreaded CrashLoopBackOff. A CrashLoopBackOff is possibl",
  "tags": [
    "kubernetes",
    "platform-engineering"
  ],
  "ctaButton": "Try Release for Free",
  "body": {
    "raw": "\nIf you've used Kubernetes (k8s), you've probably bumped into the dreaded CrashLoopBackOff. A CrashLoopBackOff is possible for several types of k8s misconfigurations (not able to connect to persistent volumes, init-container misconfiguration, etc). We aren't going to cover how to configure k8s properly in this article, but instead will focus on the harder problem of debugging your code or, even worse, someone else's code ðŸ˜±\n\nHere is the output from kubectl describe pod for a CrashLoopBackOff:\n\n```ruby\n\nName:             frontend-5c49b595fc-sjzkg\nNamespace:        tedbf02-ac-david-nginx-golang-tmcclung-nginx-golang\nPriority:         0\nStart Time:       Wed, 23 Dec 2020 14:55:49 -0500\nLabels:           app=frontend\n                  pod-template-hash=5c49b595fc\n                  tier=frontend\nStatus:           Running\nIP:              10.1.31.0\nIPs:\nControlled By:   ReplicaSet/frontend-5c49b595fc\nContainers:\n  frontend:\n    Container ID:   docker://a4ed7efcaaa87fe36342cf7532ff1de5cd51b62d3d681dfb9857999300f6c587\n    Image:          .amazonaws.com/tommyrelease/awesome-compose/frontend@sha256:dfd762c\n    Image ID:       docker-pullable://.amazonaws.com/tommyrelease/awesome-compose/frontend@sha256:dfd762c\n    Port:           80/TCP\n    Host Port:      0/TCP\n    State:          Waiting\n      Reason:       CrashLoopBackOff\n    Last State:     Terminated\n      Reason:       Error\n      Exit Code:    1\n      Started:      Sun, 24 Jan 2021 20:25:26 -0500\n      Finished:     Sun, 24 Jan 2021 20:25:26 -0500\n    Ready:          False\n    Restart Count:  9043\n\n```\n\nTwo common problems when starting a container are OCI runtime create failed (which means you are referencing a binary or script that doesn't exist on the container) and container \"Completed\" or \"Error\" which both mean that the code executing on the container failed to run a service and stay running.\n\nHere's an example of an OCI runtime error, trying to execute: \"hello crashloop\":\n\n```ruby\n\nPort:           80/TCP\n    Host Port:    0/TCP\n    Command:\n      hello\n      crashloop\n    State:          Waiting\n      Reason:       CrashLoopBackOff\n    Last State:     Terminated\n      Reason:       ContainerCannotRun\n      Message:      OCI runtime create failed: container_linux.go:370: starting container process caused: exec: \"hello\": executable file not found in $PATH: unknown\n      Exit Code:    127\n      Started:      Mon, 25 Jan 2021 22:20:04 -0500\n      Finished:     Mon, 25 Jan 2021 22:20:04 -0500\n\n```\n\nK8s gives you the exit status of the process in the container when you look at a pod using kubectl or [k9s](https://github.com/derailed/k9s). Common exit statuses from unix processes include 1-125. Each unix command usually has a man page, which provides more details around the various exit codes. Exit code (128 + SIGKILL 9) 137 means that k8s hit the memory limit for your pod and killed your container for you.\n\nHere is the output from kubectl describe pod, showing the container exit code:\n\n```ruby\n\nLast State:     Terminated\n      Reason:       Error\n      Exit Code:    1\n      Started:      Sun, 24 Jan 2021 20:25:26 -0500\n      Finished:     Sun, 24 Jan 2021 20:25:26 -0500\n    Ready:          False\n    Restart Count:  9043\n\n```\n\n### All containers are not created equally.\n\nDocker allows you to define an Entrypoint and Cmd which you can mix and match in a Dockerfile. Entrypoint is the executable, and Cmd are the arguments passed to the Entrypoint. The Dockerfile schema is quite lenient and allows users to set Cmd without Entrypoint, which means that the first argument in Cmd will be the executable to run.\n\nNote: k8s uses a different naming convention for Docker Entrypoint and Cmd. In Kubernetes command is Docker Entrypoint and Kubernetes args is Docker Cmd.\n\n| Description           | The command run by the container | Arguments passed to the command |\n| --------------------- | -------------------------------- | ------------------------------- |\n| Docker field name     | Entrypoint                       | Cmd                             |\n| Kubernetes field name | Command                          | args                            |\n\nThere are a few tricks to understanding how the container you're working with starts up. In order to get the startup command when you're dealing with someone else's container, we need to know the intended Docker Entrypoint and Cmd of the Docker image. If you have the Dockerfile that created the Docker image, then you likely already know the Entrypoint and Cmd, unless you aren't defining them and inheriting from a base image that has them set.\n\nWhen dealing with either off the shelf containers, using someone else's container and you don't have the Dockerfile, or you're inheriting from a base image that you don't have the Dockerfile for, you can use the following steps to get the values you need. First, we pull the container locally using docker pull, then we inspect the container image to get the Entrypoint and Cmd:\n\n- docker pull <image id=\"\"></image>\n- docker inspect <image id=\"\"></image>\n\nHere we use jq to filter the JSON response from docker inspect:\n\n```ruby\n\ndavid@sega:~: docker pull docker.elastic.co/elasticsearch/elasticsearch:7.10.2\n7.10.2: Pulling from elasticsearch/elasticsearch\nddf49b9115d7: Pull complete\ne736878e27ad: Pull complete\n7487c9dcefbe: Pull complete\n9ccb7e6e1f0c: Pull complete\ndcec6dec98db: Pull complete\n8a10b4854661: Pull complete\n1e595aee1b7d: Pull complete\n06cc198dbf22: Pull complete\n55b9b1b50ed8: Pull complete\nDigest: sha256:d528cec81720266974fdfe7a0f12fee928dc02e5a2c754b45b9a84c84695bfd9\nStatus: Downloaded newer image for docker.elastic.co/elasticsearch/elasticsearch:7.10.2\ndocker.elastic.co/elasticsearch/elasticsearch:7.10.2\ndavid@sega:~: docker inspect docker.elastic.co/elasticsearch/elasticsearch:7.10.2 | jq '.[0] .ContainerConfig .Entrypoint'\n[\n  \"/tini\",\n  \"--\",\n  \"/usr/local/bin/docker-entrypoint.sh\"\n]\ndavid@sega:~: docker inspect docker.elastic.co/elasticsearch/elasticsearch:7.10.2 | jq '.[0] .ContainerConfig .Cmd'\n[\n  \"/bin/sh\",\n  \"-c\",\n  \"#(nop) \",\n  \"CMD [\\\"eswrapper\\\"]\"\n]\n\n```\n\n### The Dreaded CrashLoopBackOff\n\nNow that you have all that background, let's get to debugging the CrashLoopBackOff.\n\nIn order to understand what's happening, it's important to be able to inspect the container inside of k8s so the application has all the environment variables and dependent services. Updating the deployment and setting the container Entrypoint or k8s command temporarily to tail -f /dev/null or sleep infinity will give you an opportunity to debug why the service doesn't stay running.\n\nHere's how to configure k8s to override the container Entrypoint:\n\n```ruby\n\napiVersion: extensions/v1beta1\nkind: Deployment\nmetadata:\n  name: elasticsearch\n  namespace: elasticsearch\nspec:\n  progressDeadlineSeconds: 600\n  replicas: 1\n  revisionHistoryLimit: 3\n  selector:\n    matchLabels:\n      app: backend\n      tier: backend\n  strategy:\n    rollingUpdate:\n      maxSurge: 25%\n      maxUnavailable: 25%\n    type: RollingUpdate\n  template:\n    metadata:\n      creationTimestamp: null\n      labels:\n        app: backend\n        tier: backend\n    spec:\n      containers:\n      - command:\n        - tail\n        - \"-f\"\n        - /dev/null\n\n```\n\nHere's the configuration in Release:\n\n```ruby\n\nservices:\n- name: elasticsearch\n  image: docker.elastic.co/elasticsearch/elasticsearch:7.10.2\n  command:\n  - tail\n  - \"-f\"\n  - /dev/null\n\n```\n\nYou can now use kubectl or k9s to exec into the container and take a look around. Using the Entrypoint and Cmd you discovered earlier, you can execute the intended startup command and see how the application is failing.\n\nDepending on the container you're running, it may be missing many of the tools necessary to debug your problem like: curl, lsof, vim; and if it's someone else's code, you probably don't know which version of linux was used to create the image. We typically try all of the common package managers until we find the right one. Most containers these days use Alpine Linux (apk package manager) or a Debian, Ubuntu (apt-get package manager) based image. In some cases we've seen Centos and Fedora, which both use the yum package manager.\n\nOne of the following commands should work depending on the operating system:\n\n- apk\n- apt-get\n- yum\n\nDockerfile maintainers often remove the cache from the package manager to shrink the size of the image, so you may also need to run one of the following:\n\n- apk update\n- apt-get update\n- yum makecache\n\nNow you need to add the necessary tools to help with debugging. Depending on the package manager you found, use one of the following commands to add useful debugging tools:\n\n- apt-get install -y curl vim procps inetutils-tools net-tools lsof\n- apk add curl vim procps net-tools lsof\n- yum install curl vim procps lsof\n\nAt this point, it's up to you to figure out the problem. You can edit files using vim to tweak the container until you understand what's going on. If you forget all of the files you've touched on the container, you can alway kill the pod and the container will restart without your changes. Always remember to write down the steps taken to get the container working. You'll want to use your notes to alter the Dockerfile or add commands to the container startup scripts.\n\n### Debugging Your Containers\n\nWe have created a simple script to get all of the debuging tools, as long as you are working with a container that has curl pre-installed:\n\n```ruby\n\n# install debugging tools on a container with curl pre-installed\n\n/bin/sh -c \"$(curl -fsSL https://raw.githubusercontent.com/releaseapp-io/container-debug/main/install.sh)\"\n\n```\n\n### Conclusion\n\nIn this article, we've learnt how to spot and investigate the CrashLoopBackOff errors in containers. We walked you through how to inspect and investigate the container image itself. We've listed and shown some tools that we use to spot problems and investigate issues. We got several useful and basic tools installed on the image, hopefully regardless of base image. With these steps in mind and all the tools ready at your disposal, go forth and fix all the things!\n",
    "code": "var Component=(()=>{var h=Object.create;var i=Object.defineProperty;var u=Object.getOwnPropertyDescriptor;var p=Object.getOwnPropertyNames;var m=Object.getPrototypeOf,g=Object.prototype.hasOwnProperty;var f=(t,e)=>()=>(e||t((e={exports:{}}).exports,e),e.exports),b=(t,e)=>{for(var a in e)i(t,a,{get:e[a],enumerable:!0})},s=(t,e,a,r)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let o of p(e))!g.call(t,o)&&o!==a&&i(t,o,{get:()=>e[o],enumerable:!(r=u(e,o))||r.enumerable});return t};var y=(t,e,a)=>(a=t!=null?h(m(t)):{},s(e||!t||!t.__esModule?i(a,\"default\",{value:t,enumerable:!0}):a,t)),k=t=>s(i({},\"__esModule\",{value:!0}),t);var l=f((L,c)=>{c.exports=_jsx_runtime});var x={};b(x,{default:()=>v,frontmatter:()=>w});var n=y(l()),w={title:\"Kubernetes - How to Debug CrashLoopBackOff in a Container\",summary:\"If you've used Kubernetes (k8s), you've probably bumped into the dreaded CrashLoopBackOff. A CrashLoopBackOff is possibl\",publishDate:\"Thu Feb 04 2021 19:23:56 GMT+0000 (Coordinated Universal Time)\",author:\"david-giffin\",readingTime:3,categories:[\"kubernetes\",\"platform-engineering\"],mainImage:\"/blog-images/4e91e41da26a361f3123c6c7a68815ee.jpg\",imageAlt:\"Storage containers stacked representing a CrashLoopBackOff in a container\",showCTA:!0,ctaCopy:\"Struggling with CrashLoopBackOff in Kubernetes? Use Release for streamlined debugging environments and faster issue resolution.\",ctaLink:\"https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=kubernetes-how-to-debug-crashloopbackoff-in-a-container\",relatedPosts:[\"\"],ogImage:\"/blog-images/4e91e41da26a361f3123c6c7a68815ee.jpg\",excerpt:\"If you've used Kubernetes (k8s), you've probably bumped into the dreaded CrashLoopBackOff. A CrashLoopBackOff is possibl\",tags:[\"kubernetes\",\"platform-engineering\"],ctaButton:\"Try Release for Free\"};function d(t){let e=Object.assign({p:\"p\",pre:\"pre\",code:\"code\",a:\"a\",h3:\"h3\",span:\"span\",table:\"table\",thead:\"thead\",tr:\"tr\",th:\"th\",tbody:\"tbody\",td:\"td\",ul:\"ul\",li:\"li\"},t.components);return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(e.p,{children:\"If you've used Kubernetes (k8s), you've probably bumped into the dreaded CrashLoopBackOff. A CrashLoopBackOff is possible for several types of k8s misconfigurations (not able to connect to persistent volumes, init-container misconfiguration, etc). We aren't going to cover how to configure k8s properly in this article, but instead will focus on the harder problem of debugging your code or, even worse, someone else's code \\u{1F631}\"}),`\n`,(0,n.jsx)(e.p,{children:\"Here is the output from kubectl describe pod for a CrashLoopBackOff:\"}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-ruby\",children:`\nName:             frontend-5c49b595fc-sjzkg\nNamespace:        tedbf02-ac-david-nginx-golang-tmcclung-nginx-golang\nPriority:         0\nStart Time:       Wed, 23 Dec 2020 14:55:49 -0500\nLabels:           app=frontend\n                  pod-template-hash=5c49b595fc\n                  tier=frontend\nStatus:           Running\nIP:              10.1.31.0\nIPs:\nControlled By:   ReplicaSet/frontend-5c49b595fc\nContainers:\n  frontend:\n    Container ID:   docker://a4ed7efcaaa87fe36342cf7532ff1de5cd51b62d3d681dfb9857999300f6c587\n    Image:          .amazonaws.com/tommyrelease/awesome-compose/frontend@sha256:dfd762c\n    Image ID:       docker-pullable://.amazonaws.com/tommyrelease/awesome-compose/frontend@sha256:dfd762c\n    Port:           80/TCP\n    Host Port:      0/TCP\n    State:          Waiting\n      Reason:       CrashLoopBackOff\n    Last State:     Terminated\n      Reason:       Error\n      Exit Code:    1\n      Started:      Sun, 24 Jan 2021 20:25:26 -0500\n      Finished:     Sun, 24 Jan 2021 20:25:26 -0500\n    Ready:          False\n    Restart Count:  9043\n\n`})}),`\n`,(0,n.jsx)(e.p,{children:`Two common problems when starting a container are OCI runtime create failed (which means you are referencing a binary or script that doesn't exist on the container) and container \"Completed\" or \"Error\" which both mean that the code executing on the container failed to run a service and stay running.`}),`\n`,(0,n.jsx)(e.p,{children:`Here's an example of an OCI runtime error, trying to execute: \"hello crashloop\":`}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-ruby\",children:`\nPort:           80/TCP\n    Host Port:    0/TCP\n    Command:\n      hello\n      crashloop\n    State:          Waiting\n      Reason:       CrashLoopBackOff\n    Last State:     Terminated\n      Reason:       ContainerCannotRun\n      Message:      OCI runtime create failed: container_linux.go:370: starting container process caused: exec: \"hello\": executable file not found in $PATH: unknown\n      Exit Code:    127\n      Started:      Mon, 25 Jan 2021 22:20:04 -0500\n      Finished:     Mon, 25 Jan 2021 22:20:04 -0500\n\n`})}),`\n`,(0,n.jsxs)(e.p,{children:[\"K8s gives you the exit status of the process in the container when you look at a pod using kubectl or \",(0,n.jsx)(e.a,{href:\"https://github.com/derailed/k9s\",children:\"k9s\"}),\". Common exit statuses from unix processes include 1-125. Each unix command usually has a man page, which provides more details around the various exit codes. Exit code (128 + SIGKILL 9) 137 means that k8s hit the memory limit for your pod and killed your container for you.\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Here is the output from kubectl describe pod, showing the container exit code:\"}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-ruby\",children:`\nLast State:     Terminated\n      Reason:       Error\n      Exit Code:    1\n      Started:      Sun, 24 Jan 2021 20:25:26 -0500\n      Finished:     Sun, 24 Jan 2021 20:25:26 -0500\n    Ready:          False\n    Restart Count:  9043\n\n`})}),`\n`,(0,n.jsxs)(e.h3,{id:\"all-containers-are-not-created-equally\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#all-containers-are-not-created-equally\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"All containers are not created equally.\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Docker allows you to define an Entrypoint and Cmd which you can mix and match in a Dockerfile. Entrypoint is the executable, and Cmd are the arguments passed to the Entrypoint. The Dockerfile schema is quite lenient and allows users to set Cmd without Entrypoint, which means that the first argument in Cmd will be the executable to run.\"}),`\n`,(0,n.jsx)(e.p,{children:\"Note: k8s uses a different naming convention for Docker Entrypoint and Cmd. In Kubernetes command is Docker Entrypoint and Kubernetes args is Docker Cmd.\"}),`\n`,(0,n.jsxs)(e.table,{children:[(0,n.jsx)(e.thead,{children:(0,n.jsxs)(e.tr,{children:[(0,n.jsx)(e.th,{children:\"Description\"}),(0,n.jsx)(e.th,{children:\"The command run by the container\"}),(0,n.jsx)(e.th,{children:\"Arguments passed to the command\"})]})}),(0,n.jsxs)(e.tbody,{children:[(0,n.jsxs)(e.tr,{children:[(0,n.jsx)(e.td,{children:\"Docker field name\"}),(0,n.jsx)(e.td,{children:\"Entrypoint\"}),(0,n.jsx)(e.td,{children:\"Cmd\"})]}),(0,n.jsxs)(e.tr,{children:[(0,n.jsx)(e.td,{children:\"Kubernetes field name\"}),(0,n.jsx)(e.td,{children:\"Command\"}),(0,n.jsx)(e.td,{children:\"args\"})]})]})]}),`\n`,(0,n.jsx)(e.p,{children:\"There are a few tricks to understanding how the container you're working with starts up. In order to get the startup command when you're dealing with someone else's container, we need to know the intended Docker Entrypoint and Cmd of the Docker image. If you have the Dockerfile that created the Docker image, then you likely already know the Entrypoint and Cmd, unless you aren't defining them and inheriting from a base image that has them set.\"}),`\n`,(0,n.jsx)(e.p,{children:\"When dealing with either off the shelf containers, using someone else's container and you don't have the Dockerfile, or you're inheriting from a base image that you don't have the Dockerfile for, you can use the following steps to get the values you need. First, we pull the container locally using docker pull, then we inspect the container image to get the Entrypoint and Cmd:\"}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsxs)(e.li,{children:[\"docker pull \",(0,n.jsx)(\"image\",{id:\"\"})]}),`\n`,(0,n.jsxs)(e.li,{children:[\"docker inspect \",(0,n.jsx)(\"image\",{id:\"\"})]}),`\n`]}),`\n`,(0,n.jsx)(e.p,{children:\"Here we use jq to filter the JSON response from docker inspect:\"}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-ruby\",children:`\ndavid@sega:~: docker pull docker.elastic.co/elasticsearch/elasticsearch:7.10.2\n7.10.2: Pulling from elasticsearch/elasticsearch\nddf49b9115d7: Pull complete\ne736878e27ad: Pull complete\n7487c9dcefbe: Pull complete\n9ccb7e6e1f0c: Pull complete\ndcec6dec98db: Pull complete\n8a10b4854661: Pull complete\n1e595aee1b7d: Pull complete\n06cc198dbf22: Pull complete\n55b9b1b50ed8: Pull complete\nDigest: sha256:d528cec81720266974fdfe7a0f12fee928dc02e5a2c754b45b9a84c84695bfd9\nStatus: Downloaded newer image for docker.elastic.co/elasticsearch/elasticsearch:7.10.2\ndocker.elastic.co/elasticsearch/elasticsearch:7.10.2\ndavid@sega:~: docker inspect docker.elastic.co/elasticsearch/elasticsearch:7.10.2 | jq '.[0] .ContainerConfig .Entrypoint'\n[\n  \"/tini\",\n  \"--\",\n  \"/usr/local/bin/docker-entrypoint.sh\"\n]\ndavid@sega:~: docker inspect docker.elastic.co/elasticsearch/elasticsearch:7.10.2 | jq '.[0] .ContainerConfig .Cmd'\n[\n  \"/bin/sh\",\n  \"-c\",\n  \"#(nop) \",\n  \"CMD [\\\\\"eswrapper\\\\\"]\"\n]\n\n`})}),`\n`,(0,n.jsxs)(e.h3,{id:\"the-dreaded-crashloopbackoff\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#the-dreaded-crashloopbackoff\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"The Dreaded CrashLoopBackOff\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Now that you have all that background, let's get to debugging the CrashLoopBackOff.\"}),`\n`,(0,n.jsx)(e.p,{children:\"In order to understand what's happening, it's important to be able to inspect the container inside of k8s so the application has all the environment variables and dependent services. Updating the deployment and setting the container Entrypoint or k8s command temporarily to tail -f /dev/null or sleep infinity will give you an opportunity to debug why the service doesn't stay running.\"}),`\n`,(0,n.jsx)(e.p,{children:\"Here's how to configure k8s to override the container Entrypoint:\"}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-ruby\",children:`\napiVersion: extensions/v1beta1\nkind: Deployment\nmetadata:\n  name: elasticsearch\n  namespace: elasticsearch\nspec:\n  progressDeadlineSeconds: 600\n  replicas: 1\n  revisionHistoryLimit: 3\n  selector:\n    matchLabels:\n      app: backend\n      tier: backend\n  strategy:\n    rollingUpdate:\n      maxSurge: 25%\n      maxUnavailable: 25%\n    type: RollingUpdate\n  template:\n    metadata:\n      creationTimestamp: null\n      labels:\n        app: backend\n        tier: backend\n    spec:\n      containers:\n      - command:\n        - tail\n        - \"-f\"\n        - /dev/null\n\n`})}),`\n`,(0,n.jsx)(e.p,{children:\"Here's the configuration in Release:\"}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-ruby\",children:`\nservices:\n- name: elasticsearch\n  image: docker.elastic.co/elasticsearch/elasticsearch:7.10.2\n  command:\n  - tail\n  - \"-f\"\n  - /dev/null\n\n`})}),`\n`,(0,n.jsx)(e.p,{children:\"You can now use kubectl or k9s to exec into the container and take a look around. Using the Entrypoint and Cmd you discovered earlier, you can execute the intended startup command and see how the application is failing.\"}),`\n`,(0,n.jsx)(e.p,{children:\"Depending on the container you're running, it may be missing many of the tools necessary to debug your problem like: curl, lsof, vim; and if it's someone else's code, you probably don't know which version of linux was used to create the image. We typically try all of the common package managers until we find the right one. Most containers these days use Alpine Linux (apk package manager) or a Debian, Ubuntu (apt-get package manager) based image. In some cases we've seen Centos and Fedora, which both use the yum package manager.\"}),`\n`,(0,n.jsx)(e.p,{children:\"One of the following commands should work depending on the operating system:\"}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsx)(e.li,{children:\"apk\"}),`\n`,(0,n.jsx)(e.li,{children:\"apt-get\"}),`\n`,(0,n.jsx)(e.li,{children:\"yum\"}),`\n`]}),`\n`,(0,n.jsx)(e.p,{children:\"Dockerfile maintainers often remove the cache from the package manager to shrink the size of the image, so you may also need to run one of the following:\"}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsx)(e.li,{children:\"apk update\"}),`\n`,(0,n.jsx)(e.li,{children:\"apt-get update\"}),`\n`,(0,n.jsx)(e.li,{children:\"yum makecache\"}),`\n`]}),`\n`,(0,n.jsx)(e.p,{children:\"Now you need to add the necessary tools to help with debugging. Depending on the package manager you found, use one of the following commands to add useful debugging tools:\"}),`\n`,(0,n.jsxs)(e.ul,{children:[`\n`,(0,n.jsx)(e.li,{children:\"apt-get install -y curl vim procps inetutils-tools net-tools lsof\"}),`\n`,(0,n.jsx)(e.li,{children:\"apk add curl vim procps net-tools lsof\"}),`\n`,(0,n.jsx)(e.li,{children:\"yum install curl vim procps lsof\"}),`\n`]}),`\n`,(0,n.jsx)(e.p,{children:\"At this point, it's up to you to figure out the problem. You can edit files using vim to tweak the container until you understand what's going on. If you forget all of the files you've touched on the container, you can alway kill the pod and the container will restart without your changes. Always remember to write down the steps taken to get the container working. You'll want to use your notes to alter the Dockerfile or add commands to the container startup scripts.\"}),`\n`,(0,n.jsxs)(e.h3,{id:\"debugging-your-containers\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#debugging-your-containers\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Debugging Your Containers\"]}),`\n`,(0,n.jsx)(e.p,{children:\"We have created a simple script to get all of the debuging tools, as long as you are working with a container that has curl pre-installed:\"}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-ruby\",children:`\n# install debugging tools on a container with curl pre-installed\n\n/bin/sh -c \"$(curl -fsSL https://raw.githubusercontent.com/releaseapp-io/container-debug/main/install.sh)\"\n\n`})}),`\n`,(0,n.jsxs)(e.h3,{id:\"conclusion\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#conclusion\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Conclusion\"]}),`\n`,(0,n.jsx)(e.p,{children:\"In this article, we've learnt how to spot and investigate the CrashLoopBackOff errors in containers. We walked you through how to inspect and investigate the container image itself. We've listed and shown some tools that we use to spot problems and investigate issues. We got several useful and basic tools installed on the image, hopefully regardless of base image. With these steps in mind and all the tools ready at your disposal, go forth and fix all the things!\"})]})}function C(t={}){let{wrapper:e}=t.components||{};return e?(0,n.jsx)(e,Object.assign({},t,{children:(0,n.jsx)(d,t)})):d(t)}var v=C;return k(x);})();\n;return Component;"
  },
  "_id": "blog/posts/kubernetes-how-to-debug-crashloopbackoff-in-a-container.mdx",
  "_raw": {
    "sourceFilePath": "blog/posts/kubernetes-how-to-debug-crashloopbackoff-in-a-container.mdx",
    "sourceFileName": "kubernetes-how-to-debug-crashloopbackoff-in-a-container.mdx",
    "sourceFileDir": "blog/posts",
    "contentType": "mdx",
    "flattenedPath": "blog/posts/kubernetes-how-to-debug-crashloopbackoff-in-a-container"
  },
  "type": "BlogPost",
  "computedSlug": "kubernetes-how-to-debug-crashloopbackoff-in-a-container"
}