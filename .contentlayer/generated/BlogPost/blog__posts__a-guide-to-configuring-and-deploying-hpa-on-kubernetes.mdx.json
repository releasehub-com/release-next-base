{
  "title": "A Guide to Configuring and Deploying HPA on Kubernetes",
  "summary": "HPA in Kubernetes will make your Kubernetes cluster more self-sustainable and will offload you from repetitive tasks.",
  "publishDate": "Wed Sep 14 2022 16:07:00 GMT+0000 (Coordinated Universal Time)",
  "author": "ashley-penney",
  "readingTime": 5,
  "categories": [
    "kubernetes",
    "platform-engineering"
  ],
  "mainImage": "/blog-images/6939a8efb4225dcbacab89ad2bf333d3.jpg",
  "imageAlt": "a row of computer screens with a glass in front of them",
  "showCTA": true,
  "ctaCopy": "Automate scaling with Release's environments like HPA on Kubernetes for efficient resource management.",
  "ctaLink": "https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=a-guide-to-configuring-and-deploying-hpa-on-kubernetes",
  "relatedPosts": [
    ""
  ],
  "ogImage": "/blog-images/6939a8efb4225dcbacab89ad2bf333d3.jpg",
  "excerpt": "HPA in Kubernetes will make your Kubernetes cluster more self-sustainable and will offload you from repetitive tasks.",
  "tags": [
    "kubernetes",
    "platform-engineering"
  ],
  "ctaButton": "Try Release for Free",
  "body": {
    "raw": "\nOne of the main advantages of Kubernetes is that it can take care of your containers for you. This means, for example, that it will move containers around to distribute the load on the cluster evenly, automatically restart failed pods, and kill those that misbehave and try to eat too many resources. Another nice feature is Horizontal Pod Autoscaler. As the name suggests, HPA can automatically scale your pods. But how? I'm glad you asked, because that's exactly what this post is about. \n\n### What Is HPA in Kubernetes?\n\nNormally when you create a deployment in Kubernetes, you need to specify how many pods you want to run. This number is static. Therefore, every time you want to increase or decrease the number of pods, you need to edit the deployment. \n\nIf you only need to do that once or twice a year, it's not that big of a deal. But it's very unlikely that your traffic will be at the same exact level for the whole year. And the more spikes of traffic you have, the more time you'll have to spend editing your deployment to cope with the traffic. This also works the other way around: If you're running a very big cluster with lots of applications, you could save a lot of money by decreasing the number of replicas for each deployment during periods of less traffic, like during the night. But again, it would be a lot of work to make these adjustments manually all the time. And that brings us to HPA. \n\nThe main purpose of HPA is to automatically scale your deployments based on the load to match the demand. [Horizontal](https://en.wikipedia.org/wiki/Autoscaling#Kubernetes_Horizontal_Pod_Autoscaler:~:text=3%5D%5B33%5D-,Kubernetes%20Horizontal%20Pod%20Autoscaler,-%5Bedit%5D), in this case, means that we're talking about scaling the number of pods. You can specify the minimum and the maximum number of pods per deployment and a condition such as CPU or memory usage. Kubernetes will constantly monitor your deployment, and based on the condition you specified, it will increase or decrease the number of pods accordingly. \n\n‍\n\n![](/blog-images/e9b0bdf47e53fda5243b490cba26a58f.png)\n\n### What Is VPA in Kubernetes?\n\nIn Kubernetes, there is also a Vertical Pod Autoscaler (VPA). As you may guess by the name, it works contrary to Horizontal Pod Autoscaler. Instead of adjusting the number of pods up or down, as HPA does, VPA scales up or down the resource requests and limits for the pods. \n\nSo, in theory, VPA tries to achieve the same thing that HPA does, but in practice, they serve very different purposes, and you shouldn’t use them interchangeably. But to understand the difference, we need to take a step back and talk about how autoscaler knows when to scale your pods in the first place. \n\n### A Few Words About Resource Requests\n\nWe mentioned before that you need to provide a condition for HPA, such as CPU usage. So, for example, you can specify that you want your HPA to add an additional pod to the deployment when current pods have average CPU usage higher than 80%. But what does 80% mean? 80% of what? That's a very good question, and the answer will help you understand the main difference between HPA and VPA. \n\nYou see, if you create a very basic Kubernetes deployment just by specifying its name and which docker image to use, you won't be able to add HPA to it. Why is that? It's because you didn't specify resource requests and limits for it. Long story short, specifying resource requests and limits for your [pods](https://release.com/blog/kubernetes-pods-advanced-concepts-explained) isn't just a good practice, but also drastically helps Kubernetes do its job more efficiently. And that brings us back to the question of \"What does 80% usage mean?\" when setting up HPA thresholds. This percentage value is related to the pods' resource requests. And that's how HPA knows when to scale your pods up or down. \n\nIf you say that your application is using around 2GB of RAM under normal load, you set the resource requests accordingly: for example, to 2.5GB (you should always set the request to a little bit more than average). Then your HPA will know that it needs to schedule an additional pod for your deployment when the current one is using more than ~2.4GB (this value will depend on the target value that you specify when creating your HPA). \n\n### Figuring Correct Values for Resource Requests\n\nBut how do you know what resource requests to set in the first place? You could, of course, run your pods without requests first and check how many resources they normally use. But that's quite a time-consuming process, especially on big clusters with multiple applications. \n\nThat brings us back to the VPA. You see, the point of VPA isn't to scale your deployments up or down in order to keep up with sudden spikes in traffic. That's the HPA's job. VPA should be used to get you a good baseline of resource requests and limits for your pods. This way, you're free from that time-consuming task of monitoring pods' typical usage and setting requests and limits accordingly. When the traffic goes up and your pods can't keep up, the HPA should add one or more pods to the pack to get resource usage back to the \"average.\" At that point, VPA doesn't need to do anything. \n\nYou can think of it as VPA working much slower and more long-term, looking at patterns of usage, while HPA responds quicker and provides short-term solutions for load spikes. \n\n‍\n\n![](/blog-images/ea9f60b7a898e8d43fd9c3f87c85cf72.png)\n\n### How Do I Configure HPA in Kubernetes?\n\nNow that you know all the theory, let's create some HPAs. We'll start with creating an example deployment with a resource request set: \n\n```yaml\napiVersion: v1\nkind: Service\nmetadata:\n   name: nginx-svc\n   labels:\n     app: nginx\nspec:\n   ports:\n   - port: 80\n   selector:\n     app: nginx\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n   name: nginx-deployment\n   labels:\n     app: nginx\nspec:\n   replicas: 1\n   selector:\n     matchLabels:\n       app: nginx\n   template:\n     metadata:\n       labels:\n         app: nginx\n     spec:\n       containers:\n       - name: nginx\n         image: nginx\n         ports:\n         - containerPort: 80\n         resources:\n           requests:\n             memory: \"64Mi\"\n             cpu: \"100m\"\n```\n\nI'll save it as nginx.yaml and apply with **kubectl apply**: \n\n```yaml\n$ kubectl apply -f nginx.yaml\ndeployment.apps/nginx-deployment created\n\n$ kubectl get deploy\nNAME               READY   UP-TO-DATE   AVAILABLE   AGE\nnginx-deployment   1/1     1            1           115s\n```\n\nOur deployment is up and running, so we can now add HPA to it. Just like anything in Kubernetes, you can add HPA by creating and applying the YAML definition, just like we did with this deployment. Another option is to use the **kubectl autoscale** command. Let's start with the latter. To create HPA with **kubectl** **autoscale,** you need to execute the following command: \n\n```yaml\n$ kubectl autoscale deploy nginx-deployment --min=1 --max=5 --cpu-percent=80\nhorizontalpodautoscaler.autoscaling/nginx-deployment autoscaled\n```\n\nAfter **kubectl autoscale,** we need to specify the resource type we want to autoscale. In our case, it's **deploy** (short for deployment), and then we specify the deployment name. After that, we pass the minimum and maximum amount of pods HPA can create and the condition on which to scale. \n\n### Validating If HPA Works\n\nIn our example, we tell HPA to scale out pods when their CPU usage goes over 80%. And since in our deployment earlier we specified CPU requests of 100m, this means our HPA will start scaling out nginx-deployment when its average CPU usage goes over 80m. Let's validate that. First, we double-check if HPA is working using **kubectl get hpa:** \n\n```yaml\n$ kubectl get hpa\nNAME               REFERENCE                     TARGETS   MINPODS   MAXPODS   REPLICAS   AGE\nnginx-deployment   Deployment/nginx-deployment   0%/80%    1         5         1          54s\n```\n\nWe can see it's working, and our nginx deployment is currently running one pod. There is no traffic on it, so it makes sense. But let's see if HPA will do its job when we put some traffic on nginx. For that, I'll deploy a simple load generator: \n\n```yaml\n\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: load-generator\n  labels:\n    app: load\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: load\n  template:\n    metadata:\n      labels:\n        app: load\n    spec:\n      containers:\n      - command:\n            - \"/bin/sh\"\n            - \"-c\"\n            - \"while true; do wget -q -O /dev/null nginx-svc; done\"\n        name: load\n        image: busybox\n\n```\n\nOnce deployed (using **kubectl apply -f load-generator.yaml**) we can monitor your nginx deployment CPU usage with **kubectl top pods:** \n\n```yaml\n$ kubectl top pods\nNAME                               CPU(cores)   MEMORY(bytes)\nnginx-deployment-bc54c744b-tlpds   112m         4Mi\n```\n\nAnd when we see the CPU usage goes over 80m, we can check the status of HPA again: \n\n```yaml\n$  ~ kubectl get hpa\nNAME               REFERENCE                     TARGETS    MINPODS   MAXPODS   REPLICAS   AGE\nnginx-deployment   Deployment/nginx-deployment   138%/80%   1         5         2          16m\n```\n\nAnd shortly after that, the target CPU usage should also drop (since the traffic is now distributed to two pods): \n\n```yaml\n$ kubectl get hpa\nNAME               REFERENCE                     TARGETS   MINPODS   MAXPODS   REPLICAS   AGE\nnginx-deployment   Deployment/nginx-deployment   59%/80%   1         5         2          20m\n\n$ kubectl top pods\nNAME                               CPU(cores)   MEMORY(bytes)\nnginx-deployment-bc54c744b-qshtz   52m          3Mi\nnginx-deployment-bc54c744b-tlpds   59m          3Mi\n```\n\nSo, HPA is working as expected. It increased the number of pods for our deployment based on the load. Just keep in mind that HPA doesn't work instantly. It usually takes a few seconds before it will take any action, just to avoid unnecessary scaling actions on very short load spikes. \n\nYou now know how to create HPA with **kubectl,** and this is how you do the same with YAML definition: \n\n```yaml\napiVersion: autoscaling/v2\nkind: HorizontalPodAutoscaler\nmetadata:\n   name: nginx-hpa\nspec:\n   scaleTargetRef:\n     apiVersion: apps/v1\n     kind: Deployment\n     name: nginx-deployment\n   minReplicas: 1\n   maxReplicas: 5\n   metrics:\n   - type: Resource\n     resource:\n       name: cpu\n       target:\n         type: Utilization\n         averageUtilization: 80\n```\n\nI showed you the basic usage of HPA using only CPU metrics. But you can also use memory usage instead. For that, you only need to change **name:** to **memory** in your HPA YAML definition. \n\n### Custom Metrics\n\nHPA can also be configured using custom metrics. For example, take the average HTTP response time. For that, HPA offers either pod-related metrics or so-called object metrics, which can be related to anything other than pods, like networking. \n\nLet's see an example. If your application exposes a metric called \"orders-pending,\" we can use the AverageValue of that metric to configure HPA as follows: \n\n```yaml\napiVersion: autoscaling/v2\nkind: HorizontalPodAutoscaler\nmetadata:\n   name: nginx-hpa-custom\nspec:\n   scaleTargetRef:\n     apiVersion: apps/v1\n     kind: Deployment\n     name: nginx-deployment\n   minReplicas: 1\n   maxReplicas: 5\n   metrics:\n   - type: Pods\n     pods:\n       metric:\n         name: orders-pending\n       target:\n         type: AverageValue\n         averageValue: 10\n```\n\nCustom metrics are quite a broad topic and will depend on your use case. For more information on custom metrics, you can refer to Kubernetes documentation [here](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/#autoscaling-on-multiple-metrics-and-custom-metrics). \n\n### Summary\n\nAs you can see, HPA is relatively easy to set up. It only takes one kubectl command or a few lines of YAML file, and you can get even more benefits by instructing it to monitor custom metrics from your application. Clearly, HPA is a very nice thing to know. It will make your Kubernetes cluster even more self-sustainable and will offload you from repetitive tasks. \n\nIf you want to learn more about Kubernetes, feel free to take a look at [our blog here](https://release.com/blog).\n",
    "code": "var Component=(()=>{var u=Object.create;var s=Object.defineProperty;var h=Object.getOwnPropertyDescriptor;var p=Object.getOwnPropertyNames;var m=Object.getPrototypeOf,g=Object.prototype.hasOwnProperty;var y=(t,e)=>()=>(e||t((e={exports:{}}).exports,e),e.exports),f=(t,e)=>{for(var o in e)s(t,o,{get:e[o],enumerable:!0})},r=(t,e,o,i)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let a of p(e))!g.call(t,a)&&a!==o&&s(t,a,{get:()=>e[a],enumerable:!(i=h(e,a))||i.enumerable});return t};var b=(t,e,o)=>(o=t!=null?u(m(t)):{},r(e||!t||!t.__esModule?s(o,\"default\",{value:t,enumerable:!0}):o,t)),w=t=>r(s({},\"__esModule\",{value:!0}),t);var c=y((H,l)=>{l.exports=_jsx_runtime});var v={};f(v,{default:()=>P,frontmatter:()=>k});var n=b(c()),k={title:\"A Guide to Configuring and Deploying HPA on Kubernetes\",summary:\"HPA in Kubernetes will make your Kubernetes cluster more self-sustainable and will offload you from repetitive tasks.\",publishDate:\"Wed Sep 14 2022 16:07:00 GMT+0000 (Coordinated Universal Time)\",author:\"ashley-penney\",readingTime:5,categories:[\"kubernetes\",\"platform-engineering\"],mainImage:\"/blog-images/6939a8efb4225dcbacab89ad2bf333d3.jpg\",imageAlt:\"a row of computer screens with a glass in front of them\",showCTA:!0,ctaCopy:\"Automate scaling with Release's environments like HPA on Kubernetes for efficient resource management.\",ctaLink:\"https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=a-guide-to-configuring-and-deploying-hpa-on-kubernetes\",relatedPosts:[\"\"],ogImage:\"/blog-images/6939a8efb4225dcbacab89ad2bf333d3.jpg\",excerpt:\"HPA in Kubernetes will make your Kubernetes cluster more self-sustainable and will offload you from repetitive tasks.\",tags:[\"kubernetes\",\"platform-engineering\"],ctaButton:\"Try Release for Free\"};function d(t){let e=Object.assign({p:\"p\",h3:\"h3\",a:\"a\",span:\"span\",img:\"img\",pre:\"pre\",code:\"code\",strong:\"strong\"},t.components);return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(e.p,{children:\"One of the main advantages of Kubernetes is that it can take care of your containers for you. This means, for example, that it will move containers around to distribute the load on the cluster evenly, automatically restart failed pods, and kill those that misbehave and try to eat too many resources. Another nice feature is Horizontal Pod Autoscaler. As the name suggests, HPA can automatically scale your pods. But how? I'm glad you asked, because that's exactly what this post is about.\\xA0\"}),`\n`,(0,n.jsxs)(e.h3,{id:\"what-is-hpa-in-kubernetes\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#what-is-hpa-in-kubernetes\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"What Is HPA in Kubernetes?\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Normally when you create a deployment in Kubernetes, you need to specify how many pods you want to run. This number is static. Therefore, every time you want to increase or decrease the number of pods, you need to edit the deployment.\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:\"If you only need to do that once or twice a year, it's not that big of a deal. But it's very unlikely that your traffic will be at the same exact level for the whole year. And the more spikes of traffic you have, the more time you'll have to spend editing your deployment to cope with the traffic. This also works the other way around: If you're running a very big cluster with lots of applications, you could save a lot of money by decreasing the number of replicas for each deployment during periods of less traffic, like during the night. But again, it would be a lot of work to make these adjustments manually all the time. And that brings us to HPA.\\xA0\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"The main purpose of HPA is to automatically scale your deployments based on the load to match the demand. \",(0,n.jsx)(e.a,{href:\"https://en.wikipedia.org/wiki/Autoscaling#Kubernetes_Horizontal_Pod_Autoscaler:~:text=3%5D%5B33%5D-,Kubernetes%20Horizontal%20Pod%20Autoscaler,-%5Bedit%5D\",children:\"Horizontal\"}),\", in this case, means that we're talking about scaling the number of pods. You can specify the minimum and the maximum number of pods per deployment and a condition such as CPU or memory usage. Kubernetes will constantly monitor your deployment, and based on the condition you specified, it will increase or decrease the number of pods accordingly.\\xA0\"]}),`\n`,(0,n.jsx)(e.p,{children:\"\\u200D\"}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/e9b0bdf47e53fda5243b490cba26a58f.png\",alt:\"\"})}),`\n`,(0,n.jsxs)(e.h3,{id:\"what-is-vpa-in-kubernetes\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#what-is-vpa-in-kubernetes\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"What Is VPA in Kubernetes?\"]}),`\n`,(0,n.jsx)(e.p,{children:\"In Kubernetes, there is also a Vertical Pod Autoscaler (VPA). As you may guess by the name, it works contrary to Horizontal Pod Autoscaler. Instead of adjusting the number of pods up or down, as HPA does, VPA scales up or down the resource requests and limits for the pods.\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:\"So, in theory, VPA tries to achieve the same thing that HPA does, but in practice, they serve very different purposes, and you shouldn\\u2019t use them interchangeably. But to understand the difference, we need to take a step back and talk about how autoscaler knows when to scale your pods in the first place.\\xA0\"}),`\n`,(0,n.jsxs)(e.h3,{id:\"a-few-words-about-resource-requests\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#a-few-words-about-resource-requests\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"A Few Words About Resource Requests\"]}),`\n`,(0,n.jsx)(e.p,{children:\"We mentioned before that you need to provide a condition for HPA, such as CPU usage. So, for example, you can specify that you want your HPA to add an additional pod to the deployment when current pods have average CPU usage higher than 80%. But what does 80% mean? 80% of what? That's a very good question, and the answer will help you understand the main difference between HPA and VPA.\\xA0\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"You see, if you create a very basic Kubernetes deployment just by specifying its name and which docker image to use, you won't be able to add HPA to it. Why is that? It's because you didn't specify resource requests and limits for it. Long story short, specifying resource requests and limits for your \",(0,n.jsx)(e.a,{href:\"https://release.com/blog/kubernetes-pods-advanced-concepts-explained\",children:\"pods\"}),` isn't just a good practice, but also drastically helps Kubernetes do its job more efficiently. And that brings us back to the question of \"What does 80% usage mean?\" when setting up HPA thresholds. This percentage value is related to the pods' resource requests. And that's how HPA knows when to scale your pods up or down.\\xA0`]}),`\n`,(0,n.jsx)(e.p,{children:\"If you say that your application is using around 2GB of RAM under normal load, you set the resource requests accordingly: for example, to 2.5GB (you should always set the request to a little bit more than average). Then your HPA will know that it needs to schedule an additional pod for your deployment when the current one is using more than ~2.4GB (this value will depend on the target value that you specify when creating your HPA).\\xA0\"}),`\n`,(0,n.jsxs)(e.h3,{id:\"figuring-correct-values-for-resource-requests\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#figuring-correct-values-for-resource-requests\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Figuring Correct Values for Resource Requests\"]}),`\n`,(0,n.jsx)(e.p,{children:\"But how do you know what resource requests to set in the first place? You could, of course, run your pods without requests first and check how many resources they normally use. But that's quite a time-consuming process, especially on big clusters with multiple applications.\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:`That brings us back to the VPA. You see, the point of VPA isn't to scale your deployments up or down in order to keep up with sudden spikes in traffic. That's the HPA's job. VPA should be used to get you a good baseline of resource requests and limits for your pods. This way, you're free from that time-consuming task of monitoring pods' typical usage and setting requests and limits accordingly. When the traffic goes up and your pods can't keep up, the HPA should add one or more pods to the pack to get resource usage back to the \"average.\" At that point, VPA doesn't need to do anything.\\xA0`}),`\n`,(0,n.jsx)(e.p,{children:\"You can think of it as VPA working much slower and more long-term, looking at patterns of usage, while HPA responds quicker and provides short-term solutions for load spikes.\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:\"\\u200D\"}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.img,{src:\"/blog-images/ea9f60b7a898e8d43fd9c3f87c85cf72.png\",alt:\"\"})}),`\n`,(0,n.jsxs)(e.h3,{id:\"how-do-i-configure-hpa-in-kubernetes\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#how-do-i-configure-hpa-in-kubernetes\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"How Do I Configure HPA in Kubernetes?\"]}),`\n`,(0,n.jsx)(e.p,{children:\"Now that you know all the theory, let's create some HPAs. We'll start with creating an example deployment with a resource request set:\\xA0\"}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-yaml\",children:`apiVersion: v1\nkind: Service\nmetadata:\n  \\xA0name: nginx-svc\n  \\xA0labels:\n  \\xA0 \\xA0app: nginx\nspec:\n  \\xA0ports:\n  \\xA0- port: 80\n  \\xA0selector:\n  \\xA0 \\xA0app: nginx\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  \\xA0name: nginx-deployment\n  \\xA0labels:\n  \\xA0 \\xA0app: nginx\nspec:\n  \\xA0replicas: 1\n  \\xA0selector:\n  \\xA0 \\xA0matchLabels:\n  \\xA0 \\xA0 \\xA0app: nginx\n  \\xA0template:\n  \\xA0 \\xA0metadata:\n  \\xA0 \\xA0 \\xA0labels:\n  \\xA0 \\xA0 \\xA0 \\xA0app: nginx\n  \\xA0 \\xA0spec:\n  \\xA0 \\xA0 \\xA0containers:\n  \\xA0 \\xA0 \\xA0- name: nginx\n  \\xA0 \\xA0 \\xA0 \\xA0image: nginx\n  \\xA0 \\xA0 \\xA0 \\xA0ports:\n  \\xA0 \\xA0 \\xA0 \\xA0- containerPort: 80\n  \\xA0 \\xA0 \\xA0 \\xA0resources:\n  \\xA0 \\xA0 \\xA0 \\xA0 \\xA0requests:\n  \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0memory: \"64Mi\"\n  \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0cpu: \"100m\"\n`})}),`\n`,(0,n.jsxs)(e.p,{children:[\"I'll save it as nginx.yaml and apply with \",(0,n.jsx)(e.strong,{children:\"kubectl apply\"}),\":\\xA0\"]}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-yaml\",children:`$ kubectl apply -f nginx.yaml\ndeployment.apps/nginx-deployment created\n\n$ kubectl get deploy\nNAME \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 READY \\xA0 UP-TO-DATE \\xA0 AVAILABLE \\xA0 AGE\nnginx-deployment \\xA0 1/1 \\xA0 \\xA0 1 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA01 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 115s\n`})}),`\n`,(0,n.jsxs)(e.p,{children:[\"Our deployment is up and running, so we can now add HPA to it. Just like anything in Kubernetes, you can add HPA by creating and applying the YAML definition, just like we did with this deployment. Another option is to use the \",(0,n.jsx)(e.strong,{children:\"kubectl autoscale\"}),\" command. Let's start with the latter. To create HPA with \",(0,n.jsx)(e.strong,{children:\"kubectl\"}),\" \",(0,n.jsx)(e.strong,{children:\"autoscale,\"}),\" you need to execute the following command:\\xA0\"]}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-yaml\",children:`$ kubectl autoscale deploy nginx-deployment --min=1 --max=5 --cpu-percent=80\nhorizontalpodautoscaler.autoscaling/nginx-deployment autoscaled\n`})}),`\n`,(0,n.jsxs)(e.p,{children:[\"After \",(0,n.jsx)(e.strong,{children:\"kubectl autoscale,\"}),\" we need to specify the resource type we want to autoscale. In our case, it's \",(0,n.jsx)(e.strong,{children:\"deploy\"}),\" (short for deployment), and then we specify the deployment name. After that, we pass the minimum and maximum amount of pods HPA can create and the condition on which to scale.\\xA0\"]}),`\n`,(0,n.jsxs)(e.h3,{id:\"validating-if-hpa-works\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#validating-if-hpa-works\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Validating If HPA Works\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"In our example, we tell HPA to scale out pods when their CPU usage goes over 80%. And since in our deployment earlier we specified CPU requests of 100m, this means our HPA will start scaling out nginx-deployment when its average CPU usage goes over 80m. Let's validate that. First, we double-check if HPA is working using \",(0,n.jsx)(e.strong,{children:\"kubectl get hpa:\"}),\"\\xA0\"]}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-yaml\",children:`$ kubectl get hpa\nNAME \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 REFERENCE \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 TARGETS \\xA0 MINPODS \\xA0 MAXPODS \\xA0 REPLICAS \\xA0 AGE\nnginx-deployment \\xA0 Deployment/nginx-deployment \\xA0 0%/80% \\xA0 \\xA01 \\xA0 \\xA0 \\xA0 \\xA0 5 \\xA0 \\xA0 \\xA0 \\xA0 1 \\xA0 \\xA0 \\xA0 \\xA0 \\xA054s\n`})}),`\n`,(0,n.jsx)(e.p,{children:\"We can see it's working, and our nginx deployment is currently running one pod. There is no traffic on it, so it makes sense. But let's see if HPA will do its job when we put some traffic on nginx. For that, I'll deploy a simple load generator:\\xA0\"}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-yaml\",children:`\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n \\xA0name: load-generator\n \\xA0labels:\n \\xA0 \\xA0app: load\nspec:\n \\xA0replicas: 3\n \\xA0selector:\n \\xA0 \\xA0matchLabels:\n \\xA0 \\xA0 \\xA0app: load\n \\xA0template:\n \\xA0 \\xA0metadata:\n \\xA0 \\xA0 \\xA0labels:\n \\xA0 \\xA0 \\xA0 \\xA0app: load\n \\xA0 \\xA0spec:\n \\xA0 \\xA0 \\xA0containers:\n \\xA0 \\xA0 \\xA0- command:\n \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0- \"/bin/sh\"\n \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0- \"-c\"\n \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0- \"while true; do wget -q -O /dev/null nginx-svc; done\"\n \\xA0 \\xA0 \\xA0 \\xA0name: load\n \\xA0 \\xA0 \\xA0 \\xA0image: busybox\n\n`})}),`\n`,(0,n.jsxs)(e.p,{children:[\"Once deployed (using \",(0,n.jsx)(e.strong,{children:\"kubectl apply -f load-generator.yaml\"}),\") we can monitor your nginx deployment CPU usage with \",(0,n.jsx)(e.strong,{children:\"kubectl top pods:\"}),\"\\xA0\"]}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-yaml\",children:`$ kubectl top pods\nNAME \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 CPU(cores) \\xA0 MEMORY(bytes)\nnginx-deployment-bc54c744b-tlpds \\xA0 112m \\xA0 \\xA0 \\xA0 \\xA0 4Mi\n`})}),`\n`,(0,n.jsx)(e.p,{children:\"And when we see the CPU usage goes over 80m, we can check the status of HPA again:\\xA0\"}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-yaml\",children:`$ \\xA0~ kubectl get hpa\nNAME \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 REFERENCE \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 TARGETS \\xA0 \\xA0MINPODS \\xA0 MAXPODS \\xA0 REPLICAS \\xA0 AGE\nnginx-deployment \\xA0 Deployment/nginx-deployment \\xA0 138%/80% \\xA0 1 \\xA0 \\xA0 \\xA0 \\xA0 5 \\xA0 \\xA0 \\xA0 \\xA0 2 \\xA0 \\xA0 \\xA0 \\xA0 \\xA016m\n`})}),`\n`,(0,n.jsx)(e.p,{children:\"And shortly after that, the target CPU usage should also drop (since the traffic is now distributed to two pods):\\xA0\"}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-yaml\",children:`$ kubectl get hpa\nNAME \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 REFERENCE \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 TARGETS \\xA0 MINPODS \\xA0 MAXPODS \\xA0 REPLICAS \\xA0 AGE\nnginx-deployment \\xA0 Deployment/nginx-deployment \\xA0 59%/80% \\xA0 1 \\xA0 \\xA0 \\xA0 \\xA0 5 \\xA0 \\xA0 \\xA0 \\xA0 2 \\xA0 \\xA0 \\xA0 \\xA0 \\xA020m\n\n$ kubectl top pods\nNAME \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 \\xA0 CPU(cores) \\xA0 MEMORY(bytes)\nnginx-deployment-bc54c744b-qshtz \\xA0 52m \\xA0 \\xA0 \\xA0 \\xA0 \\xA03Mi\nnginx-deployment-bc54c744b-tlpds \\xA0 59m \\xA0 \\xA0 \\xA0 \\xA0 \\xA03Mi\n`})}),`\n`,(0,n.jsx)(e.p,{children:\"So, HPA is working as expected. It increased the number of pods for our deployment based on the load. Just keep in mind that HPA doesn't work instantly. It usually takes a few seconds before it will take any action, just to avoid unnecessary scaling actions on very short load spikes.\\xA0\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"You now know how to create HPA with \",(0,n.jsx)(e.strong,{children:\"kubectl,\"}),\" and this is how you do the same with YAML definition:\\xA0\"]}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-yaml\",children:`apiVersion: autoscaling/v2\nkind: HorizontalPodAutoscaler\nmetadata:\n  \\xA0name: nginx-hpa\nspec:\n  \\xA0scaleTargetRef:\n  \\xA0 \\xA0apiVersion: apps/v1\n  \\xA0 \\xA0kind: Deployment\n  \\xA0 \\xA0name: nginx-deployment\n  \\xA0minReplicas: 1\n  \\xA0maxReplicas: 5\n  \\xA0metrics:\n  \\xA0- type: Resource\n  \\xA0 \\xA0resource:\n  \\xA0 \\xA0 \\xA0name: cpu\n  \\xA0 \\xA0 \\xA0target:\n  \\xA0 \\xA0 \\xA0 \\xA0type: Utilization\n  \\xA0 \\xA0 \\xA0 \\xA0averageUtilization: 80\n`})}),`\n`,(0,n.jsxs)(e.p,{children:[\"I showed you the basic usage of HPA using only CPU metrics. But you can also use memory usage instead. For that, you only need to change \",(0,n.jsx)(e.strong,{children:\"name:\"}),\" to \",(0,n.jsx)(e.strong,{children:\"memory\"}),\" in your HPA YAML definition.\\xA0\"]}),`\n`,(0,n.jsxs)(e.h3,{id:\"custom-metrics\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#custom-metrics\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Custom Metrics\"]}),`\n`,(0,n.jsx)(e.p,{children:\"HPA can also be configured using custom metrics. For example, take the average HTTP response time. For that, HPA offers either pod-related metrics or so-called object metrics, which can be related to anything other than pods, like networking.\\xA0\"}),`\n`,(0,n.jsx)(e.p,{children:`Let's see an example. If your application exposes a metric called \"orders-pending,\" we can use the AverageValue of that metric to configure HPA as follows:\\xA0`}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-yaml\",children:`apiVersion: autoscaling/v2\nkind: HorizontalPodAutoscaler\nmetadata:\n  \\xA0name: nginx-hpa-custom\nspec:\n  \\xA0scaleTargetRef:\n  \\xA0 \\xA0apiVersion: apps/v1\n  \\xA0 \\xA0kind: Deployment\n  \\xA0 \\xA0name: nginx-deployment\n  \\xA0minReplicas: 1\n  \\xA0maxReplicas: 5\n  \\xA0metrics:\n  \\xA0- type: Pods\n  \\xA0 \\xA0pods:\n  \\xA0 \\xA0 \\xA0metric:\n  \\xA0 \\xA0 \\xA0 \\xA0name: orders-pending\n  \\xA0 \\xA0 \\xA0target:\n  \\xA0 \\xA0 \\xA0 \\xA0type: AverageValue\n  \\xA0 \\xA0 \\xA0 \\xA0averageValue: 10\n`})}),`\n`,(0,n.jsxs)(e.p,{children:[\"Custom metrics are quite a broad topic and will depend on your use case. For more information on custom metrics, you can refer to Kubernetes documentation \",(0,n.jsx)(e.a,{href:\"https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/#autoscaling-on-multiple-metrics-and-custom-metrics\",children:\"here\"}),\".\\xA0\"]}),`\n`,(0,n.jsxs)(e.h3,{id:\"summary\",children:[(0,n.jsx)(e.a,{className:\"anchor\",href:\"#summary\",children:(0,n.jsx)(e.span,{className:\"icon icon-link\"})}),\"Summary\"]}),`\n`,(0,n.jsx)(e.p,{children:\"As you can see, HPA is relatively easy to set up. It only takes one kubectl command or a few lines of YAML file, and you can get even more benefits by instructing it to monitor custom metrics from your application. Clearly, HPA is a very nice thing to know. It will make your Kubernetes cluster even more self-sustainable and will offload you from repetitive tasks.\\xA0\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"If you want to learn more about Kubernetes, feel free to take a look at \",(0,n.jsx)(e.a,{href:\"https://release.com/blog\",children:\"our blog here\"}),\".\"]})]})}function A(t={}){let{wrapper:e}=t.components||{};return e?(0,n.jsx)(e,Object.assign({},t,{children:(0,n.jsx)(d,t)})):d(t)}var P=A;return w(v);})();\n;return Component;"
  },
  "_id": "blog/posts/a-guide-to-configuring-and-deploying-hpa-on-kubernetes.mdx",
  "_raw": {
    "sourceFilePath": "blog/posts/a-guide-to-configuring-and-deploying-hpa-on-kubernetes.mdx",
    "sourceFileName": "a-guide-to-configuring-and-deploying-hpa-on-kubernetes.mdx",
    "sourceFileDir": "blog/posts",
    "contentType": "mdx",
    "flattenedPath": "blog/posts/a-guide-to-configuring-and-deploying-hpa-on-kubernetes"
  },
  "type": "BlogPost",
  "computedSlug": "a-guide-to-configuring-and-deploying-hpa-on-kubernetes"
}