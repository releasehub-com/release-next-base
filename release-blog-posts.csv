Name,Slug,Collection ID,Item ID,Created On,Updated On,Published On,Post Summary,Post Body Top,Post Body CTA | Switch,Post Body CTA | Copy,Post Body CTA | Link,Post Body Bottom,Main Image,Alt text for Main image,Author,Reading time (in minutes),Publish Date,Categories,You might also like
10 Best Practices When Using Feature Flags,10-best-practices-when-using-feature-flags,62aa5a70cd5ba27d9d0d718a,63bbcaff076d751d137b34ca,Mon Jan 09 2023 08:06:23 GMT+0000 (Coordinated Universal Time),Thu Apr 20 2023 21:01:25 GMT+0000 (Coordinated Universal Time),Thu Apr 20 2023 21:44:19 GMT+0000 (Coordinated Universal Time),"This post will explore what feature flags are, answer common questions, and discuss the best practices for using them.","<p id="""">Feature flags are a powerful tool for software development teams to control the release of new features. By using feature flags, teams can easily turn features on or off in real time. This allows for the quick and safe release of new functionality to users.&nbsp;</p><p id="""">In this post, we'll explore what feature flags are. We'll also cover best practices for using them and discuss common challenges and questions surrounding their use.&nbsp;</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1000px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1000px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63bbc92d9d70bb70b6ed72fa_sd9ReIcDAFAhzdAeG4GsWJ2j0EGRuEwRH6vTjt559Xm9w9HmkDnZVYYMC6yo9SraeWZoE2l3iax7ekxc4jqSt2j5lJ9QwlJHEzxUSE0wM9efgHand0ca2P9mwgnbrqXRmuhJrKw7mVXVhFZmvR37DckUM1r3IuDZms8Y9DLJetE5OSFOhdoxZb199Ltp.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><h3 id="""">What are Feature Flags?</h3><p id="""">Feature flags, also known as <a href=""https://en.wikipedia.org/wiki/Feature_toggle"" target=""_blank"" id="""">feature toggle</a> or feature switch, are a software development technique. They allow developers to enable or disable certain features or functionality in their application without deploying new code. This allows for more flexible and efficient software development. Using feature flags, teams can quickly and easily test and roll out new features. They can also manage these features without impacting the stability and reliability of the application.&nbsp;</p><p id="""">Feature flags provide a number of benefits, including the ability to:&nbsp;</p><ul id=""""><li id="""">test new features with a small group of users before rolling them out to the entire user base;</li><li id="""">roll out new features gradually, rather than all at once;</li><li id="""">quickly disable or roll back features that are causing problems or are not performing as expected; and</li><li id="""">customize user experience for different groups of users or for specific regions or markets.</li></ul><h3 id="""">Best Practices When Using Feature Flags</h3><p id="""">To fully realize the benefits of feature flags, it's important to follow best practices when using them.&nbsp;</p><h4 id="""">1. Clean up unused feature flags</h4><p id="""">You should regularly remove temporary feature flags, such as release flags and experiment flags, when they are no longer needed. This helps prevent the accumulation of technical debt in the code. It also keeps the feature flag management system organized and easy to understand. Each flag should have a specific, independent purpose, and the code should be modular enough to allow different features to be turned on in any combination.&nbsp;</p><p id="""">However, if multiple flags are required or may conflict with each other, using flags can become confusing and may have a negative impact on user experience. To avoid these problems, you must carefully plan and manage the use of feature flags.&nbsp;</p><h4 id="""">2. Use a feature flag management platform</h4><p id="""">It's important to choose a feature flag management system that's easy to understand and use. These characteristics are essential whether it's a specialized tool, a config file, or a database table. When introducing feature flags to a team, take the time to carefully consider the best system for your needs. That way, the solution you pick can be used long term.&nbsp;</p><p id="""">A few popular feature flag management platforms to consider are Harness and LaunchDarkly.&nbsp;</p><h4 id="""">3. Establish naming conventions</h4><p id="""">Make your feature flags easy to understand and use by establishing a naming convention. Good, descriptive naming conventions and clear documentation can help ensure that all software engineers understand the purpose and use of each flag.&nbsp;</p><p id="""">When creating a naming convention, consider including a prefix with the project or team name, indicating whether the flag is temporary or permanent, and including a creation date. It may also be helpful to include the word ""flag"" in the name if using a homegrown solution, as this can clarify the purpose of the code.&nbsp;</p><p id="""">In general, it's a good idea to follow a style guide for code that includes conventions for things like camelCase and indentation, as this can make it easier to read and understand the code. By establishing clear naming conventions and style guidelines for feature flags, you can improve the maintainability and readability of your codebase.&nbsp;</p><h4 id="""">4. Use feature flags for small test releases</h4><p id="""">Big, new feature releases can be stressful and risky, as they can potentially impact the stability and reliability of the application. However, you can reduce this risk by using feature flags to expose your new feature to a small audience first, monitor the effects, and roll back if necessary.&nbsp;</p><p id="""">One of the key benefits of feature flags is the ability to perform canary releases, or the gradual rollout of a new feature to a small group of users before making it available to the entire user base. This allows you to test the functionality of the new feature and gather feedback from a small group of users before exposing it to the entire user base.&nbsp;</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1431px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1431px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63bbc92d9d5ae761cfacbb55_r_RDpT1UPZso1UlHBqEFEVqTSJSjnIRPJiYLRKUQGroPRNg5U59SjplrHIhDlaPY4e4EGa8hdNW-eBZFE8ULjsOvLz4FD2Awuh1ld4ySCMJfTfXF37ruljzMdUmXrIGaIZLGI3Vh7YVg4drjqgBjk0YEKVv0HCthV59K8GywDrTO47Ouu7OOwnOxpJQX.jpeg"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><h4 id="""">5. Avoid dependencies between flags</h4><p id="""">Ensure that each feature flag serves a specific, independent purpose. When multiple flags are required for a single release or the state of the flag conflicts with another flag, it can lead to confusion and make it difficult to maintain the code. This can also have a negative impact on user experience. To avoid these issues, you must carefully plan and manage the use of feature flags, ensuring that they're used effectively and efficiently.&nbsp;</p><h4 id="""">6. Use targeted feature flags</h4><p id="""">Use feature flags to customize the user experience. Feature flags can be used to customize user experience for different groups of users or for specific regions or markets. By using targeted feature flags, developers can enable or disable features for specific users or groups based on various criteria such as location, language, or user role.&nbsp;</p><p id="""">One popular use of feature flags is to manage styling, themes, and personalized content. For example, a feature flag could be used to enable or disable dark mode for a website or app. This allows developers to test the new feature with a subset of users before rolling it out to the entire user base.&nbsp;</p><h4 id="""">7. Use feature flags to enable feature branches</h4><p id="""">Feature branches allow developers to work on new features in a separate branch of the codebase without impacting the stability and reliability of the main branch. This can be useful for developing and testing new features without disrupting the main development process.&nbsp;</p><h4 id="""">8. Track changes with audit logs</h4><p id="""">An audit log can be a useful tool for tracking and managing changes to feature flags. This log can provide a record of all changes made to each flag, including the identity of the person making the change and the date and time of the change.&nbsp;</p><p id="""">An audit log can help ensure transparency and visibility in the implementation of feature flag changes. Changes to feature flags can be particularly important in regulated industries such as finance and healthcare. By restricting access to the audit log to a limited number of authorized individuals, it's also possible to enhance the security of the process and protect sensitive flags from unauthorized changes.&nbsp;</p><h4 id="""">9. Control access based on policies</h4><p id="""">You may want to control access to feature flags based on policies. One of the benefits of feature flags is that they can be accessed by nontechnical team members, such as the product team, who can use them to assist with A/B testing. Another way to limit access is to allow only administrators to toggle a feature flag in a production-related environment.&nbsp;</p><p id="""">However, it's important to carefully control access to feature flags and to track changes made to them. This can involve locking changes in the production environment or maintaining a log of who's modified which flags. By taking these precautions, you can ensure that feature flags are used effectively and that changes are made by authorized personnel.&nbsp;</p><h4 id="""">10. Plan ahead for feature flags</h4><p id="""">Proper planning is key to the successful implementation of feature flags. Rather than treating feature flags as an afterthought, it's important to consider them during the design process. This will help you determine whether a flag should be temporary or permanent. You'll then be able to plan accordingly for things like naming conventions, configuration settings, review and removal processes, and access control and safety checks. By planning carefully for all flags upfront, you can increase the chances of success and ensure that your feature flags are implemented effectively.&nbsp;</p><h3 id="""">Challenges of Using Feature Flags</h3><p id="""">Feature flags can be a useful tool in software development, but they also come with their own set of challenges. Some common challenges of using feature flags include:&nbsp;</p><ul id=""""><li id=""""><strong id="""">Technical debt:</strong> Using feature flags can lead to technical debt if the flags are not maintained and updated over time. This can result in increased maintenance costs and may require dedicated resources to address.</li><li id=""""><strong id="""">Feature creep:</strong> Feature flags can also lead to feature creep, where more and more features are added over time without a clear plan or goal. This can result in a cluttered and confusing user experience.</li><li id=""""><strong id="""">Complexity:</strong> Managing multiple feature flags can be complex, especially as the number of flags increases. Difficulty in keeping track of enabled and disabled flags can lead to potential issues with code changes and deployments.</li><li id=""""><strong id="""">Overuse:</strong> It can be tempting to use feature flags as a catch-all solution, but this can lead to code complexity and maintenance issues. It's important to carefully consider whether a feature flag is the appropriate solution for a given situation, rather than relying on them as a default.</li><li id=""""><strong id="""">Lack of documentation:</strong> If a feature flag management solution does not track data such as the owner and purpose of a specific flag, it can be challenging to identify and document this information. When employees change or time passes, teams may need to rediscover the original purpose of the flag in order to determine whether it is still needed or risk leaving it in the code without understanding its purpose. This can lead to confusion and the potential for unintended consequences.</li></ul><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1430px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1430px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63bbc92df11099f14c7149fa_XzEUIrZbr4gWtviNl8szmcTLBJNupHDrwvt898M1dyqpttECu8E_LH5W-vwwznwAOrCeTlNLnvKeZpvSTe6EPJKsx10batI_oP8ew-ITWdDd7KL6Ozefm_E2VpTKkvqQQp2YQgb9zwJTc3JYJbZ2HwsVoZsvETTLc8380YRZoAvKc3dlrIdAt0QmRW2y.jpeg"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><h3 id=""""><strong id=""""><sup>Frequently Asked Questions (FAQs)</sup></strong></h3><h4 id="""">When should you use feature flags?</h4><p id="""">Use feature flags when you want to release new features to a subset of users or when you need to quickly turn off a feature that's causing issues. They can also be useful for A/B testing or for gradually rolling out features.&nbsp;</p><h4 id="""">Where do you store feature flags?</h4><p id="""">Your team and development process will determine where you store feature flags, which can include configuration files, databases, or a feature flag platform.&nbsp;</p><h4 id="""">Should feature flags be removed?</h4><p id="""">Once you've tested a feature and it's ready for widespread release, it's generally a good idea to remove the feature flag and clean up any related code. This can help to reduce code complexity and improve maintainability.&nbsp;</p><h3 id="""">Conclusion</h3><p id="""">Overall, feature flags are a useful tool for modern software development teams to control the release of new features. By following best practices and carefully considering the challenges surrounding their use, teams can effectively leverage feature flags to enable more agile and responsive development, as well as to better control the user experience.&nbsp;</p><p id="""">Now you've learned about feature flags and best practices for using them to control the release of new features in software development. You also got answers to common questions and discovered how to effectively leverage feature flags to improve your team's workflow. Check out <a href=""https://release.com/ebook/the-complete-guide-to-automated-software-environments"">the complete guide to automated software environments</a> by Release to help speed up your workflow and get your apps and projects running smoothly.&nbsp;</p><p id=""""><em id="""">This post was written by Israel Oyetunji. </em><a href=""https://twitter.com/israelmitolu"" target=""_blank""><em id="""">Israel</em></a><em id=""""> is a frontend developer with a knack for creating engaging UI and interactive experiences. He has proven experience developing consumer-focused websites using HTML, CSS, JavaScript, React JS, SASS, and relevant technologies. He loves writing about tech and creating how-to tutorials for developers.</em></p>",false,,,,https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63e69b7b3612c397c84ffaf6_63c1bebeca87e7ad127d4ef2_Logo%20-%20Light%20on%20Dark%201.svg,,israel-oyetunji,9,Mon Mar 01 2021 22:44:00 GMT+0000 (Coordinated Universal Time),,
10 Kubernetes Namespace Best Practices to Start Following,10-kubernetes-namespace-best-practices-to-start-following,62aa5a70cd5ba27d9d0d718a,6328bab6ab1fad745e8e4c9e,Mon Sep 19 2022 18:53:42 GMT+0000 (Coordinated Universal Time),Wed Oct 30 2024 19:55:21 GMT+0000 (Coordinated Universal Time),Wed Oct 30 2024 19:55:21 GMT+0000 (Coordinated Universal Time),This post will discuss how you can use kubernetes namespace to achieve even more efficiency by following best practices.,,true,<p>Get kubernetes namespace management out of the box with Release.</p>,https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=k8s-namespaces,"<p id="""">Namespaces are the most efficient resource segregation and allocation method in the Kubernetes cluster.</p><p id="""">This post will discuss how you can use namespaces to improve efficiency by following best practices.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1000px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1000px""><div id=""""><img src=""https://cdn.prod.website-files.com/603dd147c5b0a4221d1bd360/6328b82a1e647e09a99ac4d0_FiOMYcjAXL68x9oQt3VBBTzXtItLonau1et7HlTCSDxSPKV4e7Vt7f765s45Hsw2qzU8knRwv6pUgZ_VGaiNwa6Mz4_EuflwVLZypUFMEGnYOxVWYP8GLSZ5jYlBpEhcMgvvf39_OYIvWcHjKPjB3cEgkTdyA7bFKEeH89f-b1oFgGxx0-eTNbeh.png"" id="""" width=""auto"" height=""auto"" loading=""auto"" alt=""""></div></figure><h3 id="""">What are Namespaces in Kubernetes, and Why are They Important?</h3><p id="""">Before moving to best practices, let’s briefly look into what namespaces are and why you should use them.</p><p id="""">Suppose you want to use a single cluster as both your performance and dev clusters in order to save cost on computational resources. To do this in Kubernetes, you can use namespaces to segregate <a href=""https://release.com/blog/kubernetes-pod-a-beginners-guide-to-an-essential-resource"" id="""">pod</a> services while running them on a single cluster.</p><p id="""">The namespace system is not new to computing; almost all programming languages use namespaces. Wherever you have encountered namespaces, the fundamental purpose is the same: They are used for logical grouping.</p><p id="""">Namespaces are a feature of the Linux kernel, and containers use namespaces extensively. Each container has its own storage namespace and network namespace for the segregation and allocation of resources.</p><p id="""">The <a href=""https://release.com/blog/kubernetes-namespaces-the-ultimate-guide"" id="""">Kubernetes namespace</a> refers to virtual clusters that are backed by the same physical cluster. This option is designed for use in environments with multiple users spread across multiple work teams or projects.</p><h3 id="""">Types of Kubernetes Namespaces</h3><p id="""">These are the four initial namespaces that Kubernetes starts with:</p><ul id=""""><li id=""""><strong id="""">""default""—</strong>The default namespace set by the system. It's intended for objects that don't specify any of the namespaces.</li><li id=""""><strong id="""">""kube-system""—</strong>This namespace is assigned to resources that are created by the Kubernetes system.</li><li id=""""><strong id="""">""kube-public""—</strong>This namespace is created by the system and is visible to all users, even users that aren't authenticated. Usually, this namespace is focused on the internal use of the platform cluster in situations where some of the resources need to be publicly visible and readable for the entire cluster.</li><li id="""">‍<strong id="""">""kube-node-lease ""—</strong>This namespace holds lease objects associated with each node. These leases allow the kubelet to send heartbeats so that you can determine node availability.</li></ul><p id="""">In addition to these four namespaces, you can create custom namespaces.</p><h3 id="""">Naming Convention of Kubernetes Namespaces</h3><p id="""">Namespaces in Kubernetes follow the same naming convention as other objects created in Kubernetes.</p><p id="""">You can create a name with a maximum length of 253 characters using only alphanumeric characters and hyphens. Names cannot start with a hyphen and the alpha characters can only be lowercase.</p><h3 id="""">10 Kubernetes Namespace Best Practices</h3><p id="""">Let's take a look at 10 Kubernetes namespace best practices so that you can get the most out of this feature.</p><p id="""">It's important to note that the actual utility of these practices depends on your particular needs and the nature of the project.</p><h4 id="""">1. Use Convenient and Scalable Names</h4><p id="""">Naming is at the root of programming and is one of its basic building blocks. Names should be meaningful and provide context. Therefore, it’s recommended to use names that are expressive and scalable.<br><br></p><p id="""">For example, if you're working on a streaming application, you can name the namespace ""stream"". For the different development environments, you can scale this name by adding a suffix, for example,<strong id=""""> </strong>""stream-dev"" for the development environment, ""stream-test"" for testing, and ""stream-prod"" for production.</p><h4 id="""">2. Attach Labels to Namespaces</h4><p id="""">Labels in Kubernetes are not just a way to distinguish resources, but they're also a major source of metadata that can be used to log, analyze, and audit resources.</p><p id="""">Though it’s considered a best practice to use labels throughout Kubernetes, using them in a namespace is essential when you have a large team. Here is an example:</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yaml"">
kubectl create namespace namespace_name
kubectl label namespaces namespace_name labelname=value --overwrite=true
</code>
</pre></div><h4 id="""">3. Use RBAC to Allocate Resources</h4><p id="""">Using role-based access control (RBAC), you can authorize and limit users’ access to certain resources. You can manage access locally within a cluster and globally to the entire cluster.</p><p id="""">To use RBAC for a specific namespace, you can use the <em id="""">Role</em> resource type while the <em id="""">ClusterRole</em> resource type can be used globally.</p><p id="""">Using RBAC helps you to secure clusters and manage resources by defining permissions based on roles.</p><h4 id="""">4. Use <em id="""">ResourceQuota</em> and <em id="""">LimitRange</em></h4><p id="""">The namespaces in a cluster don't all need the same resources. Giving all namespaces equal resources can compromise the performance of key namespaces. Use a resource quota to limit the resource usage of particular namespaces.</p><p id="""">Use Kubernetes <em id="""">ResourceQuota</em> to control the number of resources that can be created in a namespace and <em id="""">LimitRange</em> to restrict the consumption of resources by pods.</p><p id="""">Here's an example of how to use <em id="""">ResourceQuota</em>:</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yaml"">
apiVersion: v1
      kind: ResourceQuota
      name: default-resourcequota
      synchronize: true
      namespace: stream
      data:
        spec:
          hard:
            requests.cpu: '4'
            requests.memory: ‘16Gi’
            limits.cpu: '4'
            limits.memory: ‘16Gi’
</code>
</pre></div><p id="""">And here's an example of how to use <em id="""">LimitRange</em>:</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yaml"">
apiVersion: v1
      kind: LimitRange
      name: default-limitrange
      synchronize: true
      namespace: stream
      data:
        spec:
          limits:
          - default:
              cpu: 500m
              memory: 1Gi
            defaultRequest:
              cpu: 200m
              memory: 256Mi
</code>
</pre></div><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1000px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1000px""><div id=""""><img src=""https://cdn.prod.website-files.com/603dd147c5b0a4221d1bd360/6328b82a0804671d64b5b76d_l6sTIJ4kLfFXQSl80LHOfbaI6PPgIAXUYczz5UCYMD0vC4ayWWRFRHdGNyCNI0mB0RFbL1Hqv3sam5sFuWnko_iJCn2WPvw-A1phJVkn24zkvJXUC0FDu3Z52JP6PTE3KdDXMWH0aycPb2kk3RGaryrTXkHdpeyEogDj9Zr2BlZf9EWb5MIruAM6.png"" id="""" width=""auto"" height=""auto"" loading=""auto"" alt=""""></div></figure><h4 id="""">5. Use a NetworkPolicy</h4><p id="""">Kubernetes allows different pods across clusters to communicate. To secure the pods and only allow the desired traffic to pods from selected sources, it's necessary to use a NetworkPolicy for each namespace along with a CNI plugin to restrict communications. </p><p id="""">Using a NetworkPolicy will allow you to deny ingress, egress, or any unwanted traffic coming into pods through the namespace. &nbsp;</p><p id="""">Here's an example of how to use a NetworkPolicy:</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yaml"">
apiVersion: networking.k8s.io/v1
      kind: NetworkPolicy
      name: default-deny
      namespace: stream
      synchronize: true
      data:
        spec:
          # select all pods in the Namespace
          podSelector: {}
          # deny selected or all traffic
          policyTypes:
          - Ingress
          - Egress
</code>
</pre></div><h4 id="""">6. Don’t Create Too Many Namespaces</h4><p id="""">Even though there's no restriction on how many namespaces you can create and how many namespaces Kubernetes can handle, it's best to avoid creating too many namespaces.</p><p id="""">Creating namespaces without any definite function can become difficult to manage and too many namespaces can affect the efficient consumption of resources. </p><h4 id="""">7. Don’t Shy Away From Creating a Cluster</h4><p id="""">Namespaces are used to create virtual clusters to segregate resources and reduce costs. However, it's important to understand that as your team grows, the better FinOps approach is to create additional clusters rather than creating namespaces so that you don't to compromise on performance. </p><h4 id="""">8. Don’t Use the Default Namespace</h4><p id="""">All objects created without a specified namespace are placed in the Kubernetes ""default"" namespace. If you use the ""default"" namespace, it can become difficult to segregate objects in it or implement RBAC and NetworkPolicies.</p><h4 id="""">9. Have an Idea of What’s Inside</h4><p id="""">For better management of the Kubernetes cluster, it's important to understand which objects and resources are located in namespaces. This includes objects such as pods, replication controllers managed by the Kubernetes controller manager, and others.</p><p id="""">However, some elements are responsible for representing these resources are found outside Kubernetes namespaces. Additionally, low-level resources, such as persistent volumes and nodes, aren't found within namespaces. Services like<a href=""https://release.com/"" id=""""> Release</a> use dynamic provisioning in Kubernetes to provide on-demand environments that reduce the management overhead required to create persistent volumes.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1000px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1000px""><div id=""""><img src=""https://cdn.prod.website-files.com/603dd147c5b0a4221d1bd360/6328b82a68e63ed6cd3b1c74_fmORD1YcZQGY0Uhok8YFXA8Dw80UYWrgTy4gVodPPogK2kQRSdvH6XIzeEUe7usSKJJ9AVPvZO3VneD4zUXFS5rRupoJqbWiqZqiQK5lpybN2h47VkWocRVNo9LZxLevB5LtN8dX6TXXq3PuZfhceCDaNFKxAuYcWbT1agzkg7JzWZzS1mdxN_hL.png"" id="""" width=""auto"" height=""auto"" loading=""auto"" alt=""""></div></figure><h4 id="""">10. Sync Secrets</h4><p id="""">Secrets in Kubernetes often need to exist in multiple namespaces in a cluster so pods can access them. Registry credentials, for example, need to exist in all namespaces in a cluster. If you have many namespaces, managing registry credentials manually can be tricky.</p><p id="""">Syncing secrets allows you to copy ""regcred"" to all new namespaces when they are created and pushes updates to the copied secrets.</p><h3 id="""">Conclusion</h3><p id="""">The use of namespaces in Kubernetes is a convenient way to organize and manage resources. If you know how to use namespaces effectively, it can make a significant difference in the performance of your Kubernetes workflows and help you enhance the operability of your clusters while saving on cost.</p><p>‍</p>",https://cdn.prod.website-files.com/603dd147c5b0a4221d1bd360/63e4211070f4a92d7ecb2c01_110922%20(1).jpg,a person writing on a book,marion-newman,5,Wed Nov 09 2022 19:00:00 GMT+0000 (Coordinated Universal Time),,
11 Continuous Deployment Tools and How to Choose One,11-continuous-deployment-tools-and-how-to-choose-one,62aa5a70cd5ba27d9d0d718a,6320d9bcb25c810de79993d1,Tue Sep 13 2022 19:27:56 GMT+0000 (Coordinated Universal Time),Fri Apr 21 2023 17:42:36 GMT+0000 (Coordinated Universal Time),Fri Apr 21 2023 18:22:03 GMT+0000 (Coordinated Universal Time),Create a reliable application with little downtime with the right continuous deployment tools for the DevOps pipeline,"<p id="""">The popularity of agile methodologies among businesses has escalated and resulted in the mass adoption of continuous software development processes. This has given rise to many tools that make implementing your DevOps pipeline and application development easier. Choosing which tool to use to produce a robust system is difficult because of the market surge in continuous deployment tools.&nbsp;</p><p id="""">Continuous deployment (CD) is the procedure whereby updates to software code are pipelined, automated, tested, and then made available in the production environment. It is a crucial stage in the DevOps life cycle and assists companies in quickly and efficiently deploying changes, upgrades, and updates at the app level.&nbsp;</p><p id="""">In this article, I'll outline 11 continuous deployment tools that you should consider for deployment processes in your DevOps pipeline. We’ll look at the features and highlight the advantages and disadvantages of each tool. I'll talk about how these technologies may be utilized in DevOps, and finally, we’ll look at a comparison of these tools in detail.&nbsp;</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1430px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1430px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/6320d78e6050a3611221967f_roJfF9yluU4smz3y21zNIEhiiDT0mAmNAdUsD19dAL1YGmh5YfZxr16CfzcsMbAc3X2yBnt3dyopB41qV6PXy0uCrIP_QMrXIJlZwLS4bOvcgqBEXZE0CzJaIR1g_rJzl28ytXDVRdbuFX4WzGWR_Bu8nmeK7_hZ16fqHVvWtGRgS2BD0z9NI4SNTQ.jpeg"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><h3 id="""">Top 11 Continuous Deployment Tools for Software Development</h3><p id="""">Tools for DevOps automation make it easier, simpler, and faster for teams to manage these operations at scale. Using CI/CD tools, DevOps teams and engineers can offer continuous software upgrades of any size, platform, and environment. The top eleven CI/CD tools covered in this list all provide essential options.&nbsp;</p><ul id=""""><li id="""">AWS CodeDeploy</li><li id="""">Buddy</li><li id="""">Bamboo</li><li id="""">CircleCI</li><li id="""">DeployBot</li><li id="""">CodeShip</li><li id="""">GitLab CI/CD</li><li id="""">Jenkins</li><li id="""">Octopus Deploy</li><li id="""">TeamCity</li><li id="""">Travis CI</li></ul><h4 id="""">AWS CodeDeploy</h4><p id="""">A deployment solution called <a href=""https://aws.amazon.com/codedeploy/"" target=""_blank"" id="""">AWS CodeDeploy</a> automates the deployment of applications computing services like Amazon EC2 instances, on-premises instances, serverless Lambda functions, or Amazon Web Services.&nbsp;</p><ul id=""""><li id=""""><strong id="""">Features:</strong></li></ul><ul id=""""><li id="""">Can be deployed automatically</li><li id="""">Provides centralized control and surveillance</li><li id="""">Provides QA tracking for your deployment</li><li id="""">Is compatible with different architectures and languages</li><li id="""">Has an effective notification system</li></ul><ul id=""""><li id=""""><strong id="""">Strengths: </strong></li></ul><ul id=""""><li id="""">Offers a command-line interface (CLI) or an online administration console, which can be viewed or edited in any environment</li><li id="""">Offers code as configuration features</li><li id="""">Integrates with other AWS services</li><li id="""">Can repeat the application deployment process to other instance groups</li></ul><ul id=""""><li id=""""><strong id="""">Weakness:</strong>&nbsp;</li></ul><ul id=""""><li id="""">AWS CodeDeploy does not integrate with GitHub</li></ul><ul id=""""><li id=""""><strong id="""">Pricing:</strong>&nbsp;</li></ul><ul id=""""><li id="""">This tool does not require deployment costs. However, you are required to pay $0.02 per update for on-premises instances.</li></ul><ul id=""""><li id=""""><strong id="""">Suitable for:</strong> Small or large-scale businesses</li></ul><ul id=""""><li id=""""><strong id="""">Website: </strong><a href=""https://aws.amazon.com/codedeploy/?nc=sn&loc=0"" target=""_blank"" id="""">aws.amazon.com/codedeploy</a></li></ul><h4 id="""">Buddy</h4><p id="""">Buddy is an automation platform that makes DevOps easy for developers. It's a commercial CI/CD platform that enables rapid development, testing, and deployment of websites and applications.&nbsp;</p><ul id=""""><li id=""""><strong id="""">Features:</strong>&nbsp;</li></ul><ul id=""""><li id="""">Support for all widely used deployment protocols, IaaS/PaaS services, custom tools, and scripts</li><li id="""">One-click rollbacks that restore the server to its prior condition</li><li id="""">Parallel test execution and simultaneous deployment to several servers</li><li id="""">Multithreaded transfers for FTP/SFTP, AWS, Google Cloud, DigitalOcean, Azure, and others</li><li id="""">An exclusive list of Docker/Kubernetes activities</li></ul><ul id=""""><li id=""""><strong id="""">Strengths:</strong>&nbsp;</li></ul><ul id=""""><li id="""">Ease of setup and use</li><li id="""">Quality support</li><li id="""">Accessible for on-premises and cloud deployment</li><li id="""">Free commercial tool</li></ul><ul id=""""><li id=""""><strong id="""">Weakness:</strong>&nbsp;</li></ul><ul id=""""><li id="""">A little daunting learning curve for first-time users of CI/CD tools</li></ul><ul id=""""><li id=""""><strong id="""">Pricing:&nbsp;</strong></li></ul><ul id=""""><li id="""">Buddy offers four price tiers, ranging from $0 to $200. Its plans include a free account ($0/month for five projects), Pro ($75/month for 20 projects), Hyper ($200/month unlimited), and On-premises ($35/month per user).</li></ul><ul id=""""><li id=""""><strong id="""">Suitable for: </strong>Small or large-scale businesses</li></ul><ul id=""""><li id=""""><strong id="""">Website: </strong><a href=""https://buddy.works/"" target=""_blank"" id="""">https://buddy.works/</a></li></ul><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:300px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""300px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/6320d78dc85cca0318345b06_o__RZ-1sqCxjoRHoUD6wstBuzuTSLbpAPCGdNPcjeaPqI_VTH-PjWBHVRT7ZaUYpYDhFfNABxnSvi0BukhGUSTpr1IzflAcL2YtGGVEXs5L9pc4GEW2sfsDCM-tbwvdANL4chc1CBcFID7xxqRP3fa5pooUJ6PFQdciXQJPOpQtJzI3t_NGueZJ0DA.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><h4 id="""">Bamboo</h4><p id="""">Bamboo is a CI/CD tool from Atlassian that integrates and sets up automated builds, tests, and releases in a single DevOps flow. It seamlessly integrates with multiple projects like Bitbucket and JIRA.&nbsp;</p><ul id=""""><li id=""""><strong id="""">Features:</strong>&nbsp;</li></ul><ul id=""""><li id="""">Flexibility with languages and technologies such as CodeDeploy, Mercurial, Docker, AWS, Git, SVN, and Amazon S3 buckets</li><li id="""">Available both in hosted and on-premises versions</li><li id="""">Git workflows and branching built in, so Bamboo automatically merges branches</li><li id="""">Simple management for enterprise CI scaling</li></ul><ul id=""""><li id=""""><strong id="""">Strengths:</strong></li></ul><ul id=""""><li id="""">Has a user-friendly graphical user interface</li><li id="""">Has built-in disaster recovery features that deliver high availability</li></ul><ul id=""""><li id=""""><strong id="""">Weaknesses:</strong>&nbsp;</li></ul><ul id=""""><li id="""">Is high priced</li><li id="""">Doesn't have a cloud version</li></ul><ul id=""""><li id=""""><strong id="""">Suitable for: </strong>Small or large-scale businesses</li></ul><ul id=""""><li id=""""><strong id="""">Pricing:</strong>&nbsp;</li></ul><ul id=""""><li id="""">In Bamboo, pricing changes according to agents rather than consumers. It provides two packages for small and growing teams, from $10 to $1,100.</li></ul><ul id=""""><li id=""""><strong id="""">Website: </strong><a href=""https://www.atlassian.com/software/bamboo"" target=""_blank"" id="""">https://www.atlassian.com/software/bamboo</a></li></ul><h4 id="""">CircleCI</h4><p id="""">CircleCI is a CI tool for optimizing software development procedures. It combines with other third-party tools to automate the end-to-end deployment process in several languages. &nbsp;</p><ul id=""""><li id=""""><strong id="""">Features:</strong></li></ul><ul id=""""><li id="""">Runs on any environment, on-premises, public and private cloud</li><li id="""">Functions well with Docker and lets you configure a customized environment</li><li id="""">Automates parallelization, branch-specific and continuous deployments for quick speed</li><li id="""">Supports multiple languages like C++, JavaScript, .NET, PHP, Python, and Ruby</li><li id="""">Splits, shares, and reuses builds across multiple containers to reduce overall build time</li><li id="""">Supports all of the popular repositories, including GitHub, Bitbucket, and GitHub Enterprise</li></ul><ul id=""""><li id=""""><strong id="""">Strengths:</strong></li></ul><ul id=""""><li id="""">Is a compact CI/CD platform</li><li id="""">Provides unrivaled security</li></ul><ul id=""""><li id=""""><strong id="""">Weaknesses:</strong>&nbsp;</li></ul><ul id=""""><li id="""">It doesn't consolidate all workflows into a single platform</li><li id="""">It doesn't have an intuitive dashboard and lacks a comprehensive picture of the statistics for builds across the organization</li><li id="""">When the CI process is complicated, it can occasionally be exceedingly slow</li></ul><ul id=""""><li id=""""><strong id="""">Suitable for: </strong>Small or large-scale businesses</li></ul><ul id=""""><li id=""""><strong id="""">Pricing:</strong>&nbsp;</li></ul><ul id=""""><li id="""">CircleCI offers four paid plans, including a free plan of $0/month, $15/month, $2,000/month for enterprise-level deployments, and custom pricing for self-hosted.</li></ul><ul id=""""><li id=""""><strong id="""">Website: </strong><a href=""https://circleci.com/"" target=""_blank"" id="""">https://circleci.com/</a></li></ul><h4 id="""">DeployBot</h4><p id="""">DeployBot is a solution for cloud-based code distribution to assist businesses in creating and deploying code through a single interface<strong id="""">.</strong>&nbsp;</p><ul id=""""><li id=""""><strong id="""">Features:</strong>&nbsp;</li></ul><ul id=""""><li id="""">Allows you to roll back a release</li><li id="""">Provides real-time deployment tracking</li><li id="""">Allows for easy deployment in an open interface protocol or integration</li><li id="""">Can run shell scripts on your server before, after, or during deployment</li></ul><ul id=""""><li id=""""><strong id="""">Strengths:</strong>&nbsp;</li></ul><ul id=""""><li id="""">Allows code deployment without requiring server connection</li><li id="""">Handles permission management for clients and teams</li><li id="""">Is easy to use and integrate</li></ul><ul id=""""><li id=""""><strong id="""">Weakness:</strong>&nbsp;</li></ul><ul id=""""><li id="""">UI is not intuitive and often has too much information on one screen</li></ul><ul id=""""><li id=""""><strong id="""">Suitable for: </strong>Start-ups and scale-up businesses</li><li id=""""><strong id="""">Pricing:&nbsp;</strong></li></ul><ul id=""""><li id="""">It offers three payment tiers: the free plan ($0/month), Plus ($25/month), and Premium ($50/month).</li></ul><ul id=""""><li id=""""><strong id="""">Website:</strong> <a href=""https://deploybot.com/"" target=""_blank"" id="""">https://deploybot.com/</a></li></ul><h4 id="""">CodeShip</h4><p id="""">CodeShip is a cloud-based application development platform for continuous integration and delivery. It resides between your source code repository and the hosting environment and automatically tests and delivers any modification to your platform.&nbsp;</p><ul id=""""><li id=""""><strong id="""">Features:</strong>&nbsp;</li></ul><ul id=""""><li id="""">Includes access regulations and permits</li><li id="""">Has a builds log</li><li id="""">Offers automated and parallel testing</li><li id="""">Has configuration management for deployment pipelines</li><li id="""">Offers native Docker support</li></ul><ul id=""""><li id=""""><strong id="""">Strengths:</strong>&nbsp;</li></ul><ul id=""""><li id="""">Gives users the ability to choose containers for their production environment</li><li id="""">Integrates with any tool</li><li id="""">Has an easy-to-use web interface</li></ul><ul id=""""><li id=""""><strong id="""">Weaknesses:</strong>&nbsp;</li></ul><ul id=""""><li id="""">It lacks integrations</li><li id="""">Upgrades can cause certain plugins and integrations to stop working</li></ul><ul id=""""><li id=""""><strong id="""">Suitable for: </strong>Any team or project </li></ul><ul id=""""><li id=""""><strong id="""">Pricing:</strong>&nbsp;</li></ul><ul id=""""><li id="""">CodeShip offers pricing tiers: Starter ($49/month), Essential ($99/month), and Power ($399/month).</li></ul><ul id=""""><li id=""""><strong id="""">Website: </strong><a href=""https://www.cloudbees.com/codeship/features-pricing"" target=""_blank"" id="""">https://www.cloudbees.com/codeship/features-pricing</a></li></ul><h4 id="""">GitLab CI/CD</h4><p id="""">GitLab is a popular DevOps/CI/CD tool that offers continuous integration, delivery, and deployment all within a single interface that integrates with the Git source control system. It enables you to implement planning, source code management, tracking, and security into your development life cycle.&nbsp;</p><ul id=""""><li id=""""><strong id="""">Features:</strong>&nbsp;</li></ul><ul id=""""><li id="""">Handles performance evaluation for your server and applications life cycle</li><li id="""">Can be used for producing, displaying, and managing branches of code and project data</li><li id="""">Stores Docker images securely in the GitLab Container Registry</li><li id="""">Allows you to automatically create, test, and publish software releases with the aid of GitLab Auto DevOps</li><li id="""">Offers more features, like support for Docker, parallel builds, and real-time logging</li></ul><ul id=""""><li id=""""><strong id="""">Strengths:</strong>&nbsp;</li></ul><ul id=""""><li id="""">Is an open-source solution that is simple to use and scalable and will help you get results more quickly</li><li id="""">Offers self-hosted capability or GitLab’s SaaS option</li><li id="""">Supports multiple languages.</li></ul><ul id=""""><li id=""""><strong id="""">Weaknesses:</strong>&nbsp;</li></ul><ul id=""""><li id="""">GitLab's UI and UX are quite overwhelming and complex for a first-time user</li><li id="""">For larger projects, it can get pricy</li><li id="""">Integration with third parties can be confusing</li></ul><ul id=""""><li id=""""><strong id="""">Suitable for: </strong>Small or large-scale businesses</li></ul><ul id=""""><li id=""""><strong id="""">Pricing:</strong>&nbsp;</li></ul><ul id=""""><li id="""">GitLab offers three payment options that include free ($0), Premium ($19/month), and Ultimate ($99/month). </li></ul><ul id=""""><li id=""""><strong id="""">Website: </strong><a href=""https://docs.gitlab.com/ee/ci/introduction/index.html#continuous-deployment"" target=""_blank"" id="""">https://docs.gitlab.com/ee/ci/introduction/index.html#continuous-deployment</a></li></ul><h4 id="""">Jenkins</h4><p id="""">Jenkins is a popular open-source automation server that provides programmers with a reliable way to build, test, and deploy applications.&nbsp;</p><ul id=""""><li id=""""><strong id="""">Features:</strong>&nbsp;</li></ul><ul id=""""><li id="""">Supports multiple OSs like Mac, Windows, and others</li><li id="""">Distributes workloads across multiple machines</li><li id="""">Offers over 1,000 plugins</li><li id="""">Enables easy installation and configuration with web interface</li></ul><ul id=""""><li id=""""><strong id="""">Strengths:</strong>&nbsp;</li></ul><ul id=""""><li id="""">Has extensible plugin architecture</li><li id="""">Integrates with every language tool</li><li id="""">Is open source and free</li></ul><ul id=""""><li id=""""><strong id="""">Weaknesses:</strong>&nbsp;</li></ul><ul id=""""><li id="""">UI is not intuitive and has a learning curve</li><li id="""">Sudden failure can occur from updating processes</li><li id="""">Configuration and integration process for some plugins is not properly documented</li></ul><ul id=""""><li id=""""><strong id="""">Suitable for: </strong>Small or large-scale businesses</li></ul><ul id=""""><li id=""""><strong id="""">Pricing:</strong>&nbsp;</li></ul><ul id=""""><li id="""">Available to all users for free.</li></ul><ul id=""""><li id=""""><strong id="""">Website: </strong><a href=""https://www.jenkins.io/"" target=""_blank"" id="""">https://www.jenkins.io/</a></li></ul><h4 id="""">Octopus Deploy</h4><p id="""">Octopus Deploy is a cloud-based and on-premises DevOps automation server that enables developers to manage, audit, and automate deployments and operational runbooks from a single platform.&nbsp;</p><ul id=""""><li id=""""><strong id="""">Features:</strong>&nbsp;</li></ul><ul id=""""><li id="""">Allows you to monitor releases and deployments</li><li id="""">Supports custom scripts and controls critical data</li><li id="""">Helps you to build and manage your CI/CD pipeline</li><li id="""">Allows teams to manage runbooks from a single location while planning, monitoring, inspecting, and scheduling them</li></ul><ul id=""""><li id=""""><strong id="""">Strengths:</strong>&nbsp;</li></ul><ul id=""""><li id="""">Has built-in features that offer flexibility for automating the software deployment pipeline</li><li id="""">Offers multi-tenancy support</li><li id="""">Aids with certificate management</li><li id="""">Has support for the same deployment steps across all environments</li></ul><ul id=""""><li id=""""><strong id="""">Weaknesses:</strong>&nbsp;</li></ul><ul id=""""><li id="""">Lacks relevant reports or metrics for tracking active deployments or historical data</li><li id="""">Can sometimes have complicated integration</li><li id="""">Requires a bit of a learning curve</li></ul><ul id=""""><li id=""""><strong id="""">Suitable for: </strong>Small or large-scale businesses</li></ul><ul id=""""><li id=""""><strong id="""">Pricing:</strong>&nbsp;</li></ul><ul id=""""><li id="""">Octopus Deploy is free for up to 10 deployment targets but offers two payment options, which include Cloud ($50/month) and Server ($600/annually).</li></ul><ul id=""""><li id=""""><strong id="""">Website: </strong><a href=""https://octopus.com/"" target=""_blank"" id="""">https://octopus.com/</a></li></ul><h4 id="""">TeamCity</h4><p id="""">DevOps teams use TeamCity, a continuous integration tool, to deploy apps, packages, and containers and run automated tests.&nbsp;</p><ul id=""""><li id=""""><strong id="""">Features:</strong>&nbsp;</li></ul><ul id=""""><li id="""">Integrates with well-known version control programs</li><li id="""">Improves the quality of the code</li><li id="""">Offers cloud connections with Kubernetes clusters, Microsoft Azure, or Amazon EC2</li><li id="""">Allows you to set up builds using DSL</li><li id="""">Has detailed VCS integration</li><li id="""">Remotely executes and tests commits</li><li id="""">Keeps track of builds for rollbacks</li></ul><ul id=""""><li id=""""><strong id="""">Strengths:</strong>&nbsp;</li></ul><ul id=""""><li id="""">Runs on all OSs</li><li id="""">Integrates with .NET technologies</li><li id="""">Supports multiple clouds, multiple platforms, and multiple languages</li></ul><ul id=""""><li id=""""><strong id="""">Weaknesses:</strong>&nbsp;</li></ul><ul id=""""><li id="""">Has a steep learning curve</li><li id="""">Requires manual upgrading</li></ul><ul id=""""><li id=""""><strong id="""">Suitable for: </strong>Small or large-scale businesses</li></ul><ul id=""""><li id=""""><strong id="""">Pricing:</strong>&nbsp;</li></ul><ul id=""""><li id="""">TeamCity is available across three pricing tiers. Small teams can use a free Professional Server License, the Build Agent License costs $299, and Enterprise Server Licenses start at $1,999.</li></ul><ul id=""""><li id=""""><strong id="""">Website: </strong><a href=""https://www.jetbrains.com/teamcity/"" target=""_blank"" id="""">https://www.jetbrains.com/teamcity/</a></li></ul><h4 id="""">Travis CI</h4><p id="""">Travis CI is a cloud-hosted open-source continuous integration and deployment system. It's used to create and test projects on Bitbucket and GitHub.&nbsp;</p><ul id=""""><li id=""""><strong id="""">Features:</strong>&nbsp;</li></ul><ul id=""""><li id="""">Is available to both open-source and private projects </li><li id="""">Includes authentication, testing management, change management</li><li id="""">Issues permissions based on roles</li><li id="""">Handles data synchronization, continuous deployment, and bespoke development</li></ul><ul id=""""><li id=""""><strong id="""">Strengths:</strong>&nbsp;</li></ul><ul id=""""><li id="""">Is easy to use and set up</li><li id="""">Is open source</li><li id="""">Takes less time to configure and can be hosted without a hosting server</li></ul><ul id=""""><li id=""""><strong id="""">Weaknesses:</strong>&nbsp;</li></ul><ul id=""""><li id="""">Can occasionally be slow </li><li id="""">Lacks scalability</li><li id="""">Has no flexibility in customization </li><li id="""">Has limited integration with third-party tools</li><li id="""">Lacks plugins</li></ul><ul id=""""><li id=""""><strong id="""">Suitable for:</strong> Small- to medium-scale projects</li></ul><ul id=""""><li id=""""><strong id="""">Pricing:</strong>&nbsp;</li></ul><ul id=""""><li id="""">Travis CI offers two pricing tiers, Core and Enterprise, starting at $69/month. It also offers a 30-day free trial plan.</li></ul><ul id=""""><li id=""""><strong id="""">Website: </strong><a href=""https://www.travis-ci.com/"" target=""_blank"" id="""">https://www.travis-ci.com/ </a></li></ul><h4 id="""">Which Tools Can Be Used for Continuous Deployment in DevOps?</h4><p id="""">The final phase in a DevOps pipeline is deployment, and to accomplish CD (continuous deployment), the whole pipeline must have been correctly automated (build, testing, and staging). Deciding a preferred CD tool for the system depends on the scale and sensitivity of the project, as well as what features you look to achieve in your DevOps pipeline.&nbsp;</p><p id="""">Here is a comparison of these top 11 continuous deployment tools discussed in this article.&nbsp;</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:913px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""913px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63220434c4f3493f6a8c4fce_Screen%20Shot%202022-09-14%20at%209.38.51%20AM.png"" loading=""lazy"" id="""" width=""auto"" height=""auto""></div></figure><h3 id="""">What to Consider When Choosing a CD Tool</h3><p id="""">Despite the inclination, you may have to utilize specific tools because of their popularity. Before choosing a continuous deployment technology, consider the following essential points. Your continuous deployment tool should meet these needs.&nbsp;</p><ul id=""""><li id="""">It should be user-friendly with little or no substantial technical obstacles or a high learning curve due to the device.</li><li id="""">The DevOps teams should have a seamless experience with the configuration.</li><li id="""">Pricing needs to be fair for both small and large teams.</li><li id="""">It should be compatible with your existing integrations and technologies.</li><li id="""">The tool should accommodate multiple languages and work on different OSs.</li><li id="""">It should offer flexibility based on the environment.</li><li id="""">The tool should give you a robust software development pipeline.</li><li id="""">And it should have support for multiple integrations and plugins.</li></ul><h3 id="""">Conclusion</h3><p id="""">This post has thoroughly compared and reviewed the best continuous deployment tools currently available on the market. As was said above, automating software development processes is the key to enhancing team communication and ensuring that code and features are delivered and deployed efficiently with little to no human intervention. In conclusion, you can create a reliable application with little downtime with the right continuous deployment tool.</p>",false,,,,https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63e41fde081ee1406d195c9b_101922%20(1).jpg,A bunch of tools,kevin-luu,6,Wed Oct 19 2022 16:00:00 GMT+0000 (Coordinated Universal Time),,
12 Things You Didn’t Know You Could Do With Release (Part 1),12-things-you-didnt-know-you-could-do-with-release-part-1,62aa5a70cd5ba27d9d0d718a,6464008ed04eb6554f1dc148,Tue May 16 2023 22:15:42 GMT+0000 (Coordinated Universal Time),Thu Apr 25 2024 20:29:53 GMT+0000 (Coordinated Universal Time),Thu May 02 2024 20:19:24 GMT+0000 (Coordinated Universal Time),Tips and tricks to elevate your experience with Release (Part 1),"<p id="""">‍<em id="""">This is Part 1 of a 2 part series on things you didn't know about using Release. Stay tuned for </em><a href=""https://release.com/blog/12-things-you-didnt-know-you-could-do-with-release-part-2"" id=""""><em id="""">Part 2 </em></a><em id="""">next week.</em></p><p id="""">You may know that Release makes environments easy by allowing anyone to create full stack, isolated environments for development, testing, quality assurance, user acceptance testing, or even production. But did you know that there are many other things you could use Release for that you may not have thought about? This guide will help you find some really cool hidden tips and tricks for maximising your enjoyment and delight using Release!</p><h3 id="""">#1: Install and Manage Your Kubernetes Helm Charts</h3><p id="""">Release natively supports Helm charts that can be deployed to your clusters. Deploying and managing Helm charts can be difficult, error prone, and tedious. For example, setting up a GitOps workflow with Argo or Flux can be quite daunting to get started. And once installed, how safe is it to test new features and changes in existing deployments?</p><p id="""">With Release, you can create an application that contains a Helm chart and deploy it to a namespace. If you want to test a new version of your Helm chart(s), simply create a branch on your repository and create a new environment. You will now be able to test a full version of the Helm chart in complete isolation without breaking any existing installations. When you are finished testing your changes, merge the new branch into the main branch to roll out the tested changes to your existing environment(s) automatically – or on demand, however you prefer.</p><p id="""">Danger: Do not try this with <a href=""https://hackmd.io/@carvel/rJKraqlDD"" id="""">CRDs</a>.</p><h3 id="""">#2: Test and Update Advanced Terraform and Serverless Code</h3><p id="""">Running <a href=""https://www.terraform.io/"" id="""">Terraform</a>, <a href=""https://www.pulumi.com/"" id="""">Pulumi</a>, or<a href=""https://en.wikipedia.org/wiki/Serverless_computing"" id=""""> Serverless code</a> is amazing but the development and testing for the cloud-native services they touch can be extremely challenging, to the point where testing in production is a real activity people resort to all the time. In some cases, you have no choice but to test in production and pray. If you are a DevOps engineer with full access to cloud resources, testing accounts, an/or infrastructure, the ability to develop and test automation scripts and code is a little easier. But what if you are a developer with little-to-no access to cloud credentials or environments to test in? And how closely similar are these environments to the real production environment?</p><p id="""">With Release, you can create a new environment based on a template that can deploy a new copy of your infrastructure code (or your serverless function code – usually you’ll need both) to a new environment (and even a separate testing cluster and/or cloud account as needed) to be able to deploy and test changes in isolation. Each commit and push to your branch will be deployed and executed in this safe environment so you can test and verify the functionality of the infrastructure you are deploying as code. You can be sure your changes are being tested in a high-fidelity environment that closely mimics your target environment because Release manages the templating and deployments to keep you safe and secure.</p><p id="""">All of this without requiring extra authentication, access to resources, and credentials, while still operating with the safety and guidelines set up and enforced by your policies.</p><h3 id="""">#3: Save on AWS Costs Using Instant Dataset Pause Schedules</h3><p id="""">With the Release <a href=""https://docs.release.com/reference-documentation/instant-datasets-aws"">Instant Datasets</a>, you can instantly access a full version of your database snapshot in your own sandbox environment to test against. It’s a popular functionality, since it allows developers to test more realistic scenarios, that using fake data. But did you know that you can save money by pausing idle Instant Dataset instances on a schedule? This adds up to significant cost savings in database and cloud bills, depending on the dataset size and number of instances. Currently, this feature only applies to AWS, where RDS and Aurora support the concept of pausing database instances and clusters, and only charged for storage during the paused state.</p><p id="""">The simplest and easiest way to save money with Instant Datasets is to pause the datasets during the weekend when most of our customers are not actively building, testing, and running their development cycles. However, you can also expand the schedule to include hours when your teams may not be active, for example during off hours of the week. For example, your team may only be actively developing and utilising environments during the morning and afternoon hours. You can set up a schedule to pause your Instant Datasets (and save a lot of money!) during the off hours of the week and also all day on the weekends.</p><p id="""">Even if you have teams working in multiple locations and time zones, most of our customers are able to find 8 or 12 hours per weekday when their environment datasets are not needed and can be paused, which adds up overtime. </p><h3 id="""">#4: Manipulate and Test Data Safely in Isolation</h3><p id="""">With the Release <a href=""https://docs.release.com/reference-documentation/instant-datasets-aws"">Instant Datasets</a>, you can get a full version of your database snapshot from recent copies of data into your own sandbox environment. From there, you can perform any number of tasks and administrative commands on the database. For example, you could take a full production snapshot with sensitive data in it and run “sanitising” or pruning scripts to create snapshots that would be suitable for testing purposes. You could even script this to automatically create and update test database snapshots for other environments to use as an Instant Dataset.</p><p id="""">Another use case is to perform potentially dangerous operations on your database in the safety of an isolated environment. You could perform dangerous operations like updating security patches, upgrading versions, or changing configuration values without altering the source dataset. You could also test less invasive, but potentially blocking operations, like changing various configuration settings on the database to see how performance is affected. For example, you could take an Instant Dataset and change the instance type or size and run load testing from the safety and comfort of an isolated environment to test how the application performs under the new configuration. With Release, environments aren’t just code!</p><h3 id="""">#5: Move Your Applications Across Cloud Providers</h3><p id="""">You probably already understand the power and advantages of deploying code in testing environments and then promoting changes to production environments. You may already know that Release makes this fantastically easy and if you have a production environment already hosted in Release, how comfortable you are that your environments are high-fidelity versions of the actual production environment. You may also know that you can now deploy your applications and environments between clusters (for example, a testing cluster and a production cluster, or a primary region and a backup or alternate region). Promoting changes between testing, QA, staging, and production environments, or from primary to secondary is as simple as merging a pull request!</p><p id="""">But did you also know that your clusters need not merely be separated by region or type (like preproduction and production), but also across cloud providers AWS and GCP? Because we keep our deployment features in parity (as much as humanly possible) with cloud providers, you can almost certainly move an application template between two clusters in two different cloud providers and have a nearly identical copy running in both! If you are using one of our supported databases via <a href=""https://docs.release.com/reference-documentation/instant-datasets-aws"">Instant Datasets</a>, you will be able to take snapshots from each cloud provider and check in a full set of data no matter whether you are using AWS or GCP. Combined with your code and application template, it is eminently possible to move an application across cloud providers, or even to run in a multi-cloud scenario.</p><p id="""">Of course, if you use cloud-native resources outside of Release constructs with Terraform or other IAAC, you will need to adjust your code and probably need to be extremely clever with your infrastructure. But this is easy to do on our platform and you now have as close a shot as ever in the history of cloud computing to pull this stunt off and make it a reality.</p><h3 id="""">#6: Connect to Full-Stack Cloud Environments Remotely</h3><p id="""">There are entire companies whose only product is to allow you to access a cloud environment and work seamlessly with your local environment to test and develop code in the cloud as easily and quickly as you would if everything were completely local. At Release, this dream is just one side feature of our product around environments. Because your environments based on a branch, feature, or pull request are available securely in your own cloud, we can easily flip your environment into a “developer mode” where you can have complete access to containers running in the cloud environment on local ports and local filesystems.</p><p id="""">This means it really is as easy as editing a local file, hitting refresh in your browser’s “localhost” and seeing the results live from the cloud environment. You also don’t need to lose remote access to your cloud environment because you can still share the links and environments publicly with customers and end users or privately with colleagues and coworkers while you still update and test changes, fully live, and nearly instantly without waiting for builds and deployments! You can even connect to remote resources in your environment, like databases and services that are deployed in the cloud that might be too large or too complicated to run locally.</p><p id="""">When you are done, simply turn off “developer mode” and your environment will go back to the latest commit on the branch or pull request you are tracking to continue where you started.</p><h3 id="""">Conclusion</h3><p id="""">Read the other six things you may not have known about using Release in <a href=""https://release.com/blog/12-things-you-didnt-know-you-could-do-with-release-part-2"" id="""">Part 2</a> next week. We hope you have enjoyed these tips and hope they inspire you to try a few on your own, or contact us to get a demonstration of how these features work. If you found other creative ways to use Release or have ideas for new features, drop us a line at <a href=""mailto:hello@release.com"" id="""">hello@release.com</a> We’d love to hear from you!</p>",false,,,,https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/6463feab46fdcbe10ba1c7e0_12%20things%20about%20Release%20-%20part%201.jpg,multiple hot air balloons ,regis-wilson,8,Wed May 17 2023 18:00:00 GMT+0000 (Coordinated Universal Time),product,
12 Things You Didn’t Know You Could Do With Release (Part 2),12-things-you-didnt-know-you-could-do-with-release-part-2,62aa5a70cd5ba27d9d0d718a,646400d18ad862267b191a43,Tue May 16 2023 22:16:49 GMT+0000 (Coordinated Universal Time),Thu Apr 25 2024 20:29:47 GMT+0000 (Coordinated Universal Time),Thu May 02 2024 20:19:24 GMT+0000 (Coordinated Universal Time),Tips and tricks to elevate your experience with Release (Part 2),"<p id="""">‍<em id="""">This is part 2 of a 2 part series on things you didn't know about using Release. Read </em><a href=""https://release.com/blog/12-things-you-didnt-know-you-could-do-with-release-part-1"" id=""""><em id="""">Part 1</em></a><em id=""""> to get started.</em></p><p id="""">You may know that Release makes environments easy by allowing anyone to create full stack, isolated environments for development, testing, quality assurance, user acceptance testing, or even production. But did you know that there are a lot of other things you could use Release for that you may not have thought about? This guide will help you find some really cool hidden tips and tricks for maximising your enjoyment and delight using Release!</p><h3 id="""">#7: Enable Your Sales Teams and Customers to Quickly Demo or Proof-of-Concept Test Your Product</h3><p id="""">With Release, it’s fantastically easy to spin up environments for any purpose. You can share your latest user acceptance tests with customers or your product team. But environments can be used by almost anyone in your company: for example, you can let your sales team spin up the latest preview version of your product and demonstrate it to a customer live: not a mock up or demonstration in a shared account, but a fully-fledged production-capable environment. This environment could be ahead of or behind the full production release cycle. The demonstration environment could even be customised for each customer and given full access to the potential or existing customer to “play with” a live demonstration for a limited period of time. Once the demonstration or proof of concept period is over, the environment can be turned down, deleted, or recycled without any intervention from DevOps or other technical teams.</p><h3 id="""">#8: Host a Hackathon</h3><p id="""">One of my favourite things to participate in at work that I wouldn’t even consider work is joining a Hackathon and banging out a demonstration or proof of concept of a minimal viable product idea. Joining with my team to deliver an idea starting from nothing to tangible demonstration, spending all night, or several days and nights in a row if needed, is an amazing thing to do and I recommend this experience to every engineer, regardless of field or industry.</p><p id="""">In terms of software development however, the truly special fruit of this endeavour is being able to see the results of your efforts running live within minutes of each change. Simply have all your teams start with a “hello world” template (even better, start off with your whole product stack as the initial “hello world” demonstration!) and turn them loose in their own pull requests and branches to write amazing code and unleash their ideas instantly.</p><p id="""">The judges can easily see the results of the code by scanning the environments and testing the code live. May the best team win! :heart_emoji: :starry_eyes_emoji:</p><h3 id="""">#9: Stay Secure and Up-to-Date With Latest Code Dependencies</h3><p id="""">At Release, we are very keen to keep our code secure and constantly up to date with all the latest updates and dependencies. Every time a Dependabot or Renovate issue is opened, we know all too well the sinking feeling of having to test and verify every update. Simply merging in all the changes can potentially break your application in the most insidiously small or sometimes the biggest, most horrendous ways possible. Testing every single change is a daunting task to say the least. The easiest path is to avoid these notifications and keep ploughing ahead. But you create a real risk in ignoring updates and patches that might make your code vulnerable to attacks or exposure of important data.</p><p id="""">With Release, you can create a full-stack high-fidelity environment for every pull request or via labels when Dependabot or Renovate scans your repository for updates. Testing each application stack is straightforward because the links to the environment are ready and available in the comments or the Release UI. You can also run automated tests as well or in lieu of manual testing to verify functionality of your application. You can then simply approve and merge the PR to immediately promote the latest (hopefully up-to-date and secure) version out to production with a minimal amount of toil and labour for each proposed change.</p><h3 id="""">#10: Control Public or Private Access to Your Services</h3><p id="""">Did you know that you can hide your services behind a VPN or internal tunnelling tool to your cloud provider? For AWS customers, we support Transit Gateway out of the box to handle routing, but you can also use VPC peering or Client VPN to connect to internal services. GCP customers can also use Cloud VPN to reach internal and private services securely and safely. You can even mix public services (like a frontend and backend API) and private services (like an internal administrative interface) in one environment. You merely need to specify the &lt;code inline&gt;visibility&lt;/code&gt; parameter for each service as shown <a href=""https://docs.release.com/reference-documentation/application-settings/application-template/schema-definition#rules-1"" id="""">in our docs</a> and below:</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yaml"">
rules:
  - service: admin
    hostnames:
      - admin-${env_id}.internal.example.com
    path: ""/""
    visibility: private
  - service: backend
    hostnames:
      - backend-${env_id}.${domain}
    path: ""/auth/""
    visibility: public-direct
  - service: frontend
    hostnames:
      - frontend-${env_id}.${domain}
    path: ""/""
    visibility: public
</code>
</pre></div><h3 id="""">#11: Run Your Own Application Infrastructure and Supporting Services</h3><p id="""">You may need supporting “infrastructure” in your application stack that are actually shared services, like ElasticSearch, Kafka, RabbitMQ, Prometheus, PostHog, Solr, and on and on the list goes. You can easily create an application based on any open-source Helm chart (or your own, obviously) and deploy shared permanent environments (like QA, staging, etc.) or ephemeral environments only used for a short period of time for testing and integration. You can also use the Release <a href=""https://docs.release.com/reference-documentation/application-settings/application-template/schema-definition#app-imports"" id="""">App Imports</a> feature to include an application into your environment stack with one additional line of configuration.</p><p id="""">Similarly, almost any open-source application that has a &lt;code inline&gt;docker-compose&lt;/code&gt; file (or one that you make) can be imported in literally minutes to create a full application stack that can be either used as a stand-alone shared environment or imported into your own application stack environment. In this way, virtually any application repository in GitHub, GitLab, or Bitbucket can become part of your own internal hosted application stack in minutes rather than hours or days of trying to build and install software in your application stacks. Plus, you can create as many environments as you like, both permanent and ephemeral.</p><p id="""">We have several customers who were able to build these supporting infrastructure applications live in their own accounts faster than they could deploy their own code due to how easy off-the-shelf open-source applications can be installed via Release.</p><h3 id="""">#12: Reference Secrets in Your Services, Jobs, and Build Arguments</h3><p id="""">Everyone loves secrets and if you believe the band U2, a secret is something you tell someone else. Well, not on our watch at Release! We currently have support for several secrets sources: own own internal encrypted store (where all our built-in secrets and environment variables are stored) which is published to your Kubernetes cluster as a secret and encrypted there too, AWS Parameter Store (SSM), and AWS Secrets Manager (support for GCP Secrets Manager is coming soon, let us know if you are interested).</p><p id="""">If you have an API token or an application ID you need to keep covered up but not necessarily secret, you can use our built-in secret store and reference it in either build arguments or environment variables like this:</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yaml"">
service:
  backend:
      - key: API_TOKEN
        value: 123456
        secret: true
</code>
</pre></div><p id="""">Once you save the value it will be swallowed up in our vault and never see the light of day again until your application accesses it at runtime.</p><p id="""">Another option is to pull from either SSM or Secrets Manager (AWS, today) by reference:</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yaml"">
service:
  backend:
      - key: API_TOKEN
        value: $secrets.ssm./path/to/some_token
      - key: DATABASE_PASSWORD
        value: $secrets.secretsmanager./prod/db/main_password
</code>
</pre></div><p id="""">In this way, Release gives you access to your own secrets in your own account so a secret is truly something you never have to tell someone else. You can read our top-secret <a href=""https://docs.release.com/reference-documentation/environment-settings/environment-specific-environment-variables/secrets"" id="""">documentation about secrets here</a>.</p><h3 id="""">Conclusion</h3><p id="""">This completes our series on 12 things you may not have known about using Release, see <a href=""https://release.com/blog/12-things-you-didnt-know-you-could-do-with-release-part-1"">Part 1 here</a>. We hope you have enjoyed these tips and they inspire you to try a few on your own, or contact us to get a demonstration of how these tips work. If you have any ideas or ways that you have implemented something similar, drop us a line at <a href=""mailto:hello@release.com"" id="""">hello@release.com</a>, we’d love to hear from you!</p>",false,,,,https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/646402d70a3c7803acdfb03f_12%20things%20about%20Release%20-%20part%202.jpg,hot air balloons,regis-wilson,8,Tue May 23 2023 18:00:00 GMT+0000 (Coordinated Universal Time),product,
5 Best Practices in a Staging Environment,5-best-practices-in-a-staging-environment,62aa5a70cd5ba27d9d0d718a,63c670097f2f1919d7b4afaa,Tue Jan 17 2023 09:53:13 GMT+0000 (Coordinated Universal Time),Tue Jan 17 2023 09:53:13 GMT+0000 (Coordinated Universal Time),,In this post we will discuss best practices for staging environments and how to effectively manage staging environments.,"<figure id="""" class=""w-richtext-figure-type-image w-richtext-align-center"" data-rt-type=""image"" data-rt-align=""center""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63c66f56a9bbed2ae3ee62d6_oGuYbBr0Ru6Eh5gAjPSdqV0bsie2zK11ZcXtNtciVB2RJgrPKy__dV7whaT3UcyToEib420YvRwz8_NWdlyjUYlh_Xc9Kx4hfEyIohFdqnbORh5IBTy99HyxCqHDEKNi8AyE_M2vRE8OTOeWF-vxWJri0v1XzFbInhQoH5XtZVFZt1UyvCrGLb2dgtgC.jpeg"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><p id="""">""Staging"" is a term used to describe the process of preparing a web page or application for public viewing. This usually involves creating a copy of a live site, testing it out, and then deploying it to a production server. The goal is to ensure that the final version looks good and performs well. Staging is necessary because it allows you to test new features before they go live. If something goes wrong during the deployment process, you can fix it before it affects visitors.</p><p id="""">""Staging environments are essential for any developer who wants to deploy their code into production."" - Paul Irish</p><p id="""">If you've ever worked on a project where you had to wait until the last minute to get something working, you'll appreciate the benefits of a staging environment. It helps identify bugs early and prevent downtime.</p><h2 id=""""><strong id="""">What's a Staging Environment?</strong></h2><p id="""">A staging environment replicates the production environment, and it's a place where new code changes are first deployed and tested before being pushed to production. This allows for a safer and more controlled testing process. Any issues that arise can be fixed before they impact your live site or web application.</p><p id="""">Developers can also use the staging environment to test new features or changes before they are released to the public, ensuring they're ready for production. There are several ways to implement a staging environment, such as using a local server or a cloud service.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-center"" data-rt-type=""image"" data-rt-align=""center""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63c66f5662d933d5885ae7e5_IuHhSSjdLLt9DSu2l9FdTbpmqseHaKrlq3mOBn_JZUJrmimbOE8gD95TOnxw2-qg3IcJ_0SsEWAdUbP7CqnkpNtRMsNuB5VP7PAD-JzfSGEki0CviIrqaOCxGLlN8tpWXPXKtWKEMwbA4zeMvQ7oaUuVLmPo0NOMPRDzK-3lcA_ceD1PyWIFyrBj3kT6.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><h2 id=""""><strong id="""">What is the Difference Between UAT and a Staging Environment?</strong></h2><p id="""">User Acceptance Testing (UAT) is the process of testing a software application with real users to ensure that it meets their needs and expectations. This testing is essential to ensure that the software is fit for purpose and user-friendly before it's released to the public. It typically includes a range of tests, such as functional testing, usability testing, and performance testing.</p><p id="""">On the other hand, a staging environment allows developers and engineers to test their new code for breaking changes. Thus, it solves a different problem than UAT does. UAT testers aim for usability, whereas developers aim for a bug-free release. These can be done in parallel or independently of each other, and the cumulative outcome of both these crucial stages impacts the overall performance of your application and users' experience of it.</p><h2 id=""""><strong id="""">Best Practices for Staging Environments</strong></h2><p id="""">Here are some best practices for implementing and using a staging environment.</p><h3 id=""""><strong id="""">Staging Should be a Copy of the Production Environment</strong></h3><p id="""">As staging is to serve as a copy of production, keep your staging environment as similar to your live environment as possible. This will ensure that any issues arising in the staging environment represent what would happen on the site.&nbsp;</p><p id="""">Small changes can sometimes have unexpected consequences and testing them in the staging environment can help you avoid them. An identical replica of production will ensure accurate error reports.</p><p id="""">For example, suppose you're planning to add chat functionality to your product, for which availability and the ability to handle concurrent connections simultaneously are of the utmost importance. A production environment would be able to handle it, as organizations spend a lot of resources on ensuring the scalability of their production applications.&nbsp;</p><p id="""">It's generally assumed that if production can handle the requirements of new functionality, staging can too. But the practical advice is to look at it the other way around: If staging can handle it, production can. This ensures that teams focus their efforts on making staging an equivalent environment to production to achieve this.</p><h3 id=""""><strong id="""">Use Version Control for Configurations and Databases</strong></h3><p id="""">Using version control will make it easier to keep track of changes and roll back changes if necessary. You might consider pairing version control with automatic backups of your website's configurations and database. This way, you have a backup if something goes wrong and you can further investigate the commit history to pinpoint the issue while the production site doesn't suffer from the error.</p><p id="""">For example, let's say a team of developers is working on a module and you're using a database to store your data. With version control, you can each make changes to the database independently and then use the version control system to merge your changes. By doing this, developers can avoid conflicts and ensure that everyone is working with the most up-to-date version of the database. Additionally, if you need to revert to a previous database version for any reason, you can easily do so using the version control system.</p><h3 id=""""><strong id="""">Don't use the Staging Site for Production Work</strong></h3><p id="""">The staging site should be used solely for testing and making changes, not publishing content or conducting business.&nbsp;</p><p id="""">Regularly update the staging site with the latest content and data from the live site. You can automate updates with some DevOps effort. Automated processes ensure that the staging environment is synced to production, but not the other way around. This will help ensure that the staging site accurately represents the live site.</p><p id="""">For example, let's say you're working on a web application and you've set up a staging environment to test new features and ensure everything is working correctly before deploying to production. However, instead of deploying the latest application version to the production environment, you accidentally deploy it to the staging environment.&nbsp;</p><p id="""">Since the staging environment is not intended for production use, the application may not be stable or reliable, so you should never direct users to your staging environment. This could lead to dissatisfaction among your users and damage to your reputation. Any changes intended for user consumption should always be made in production. So in the example above, you'd need to redeploy the application to the correct location.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-center"" data-rt-type=""image"" data-rt-align=""center""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63c66f56f0c3ca879523cbb9_AsZqTnmYhxZodZ2cNGC6l56jSfeHWapsKIiukgSBCHskxRv0ihfn9-VR216ZyN85qABLEkE1UfBnaWjeONM06-VZxdRqfdjIOEL-AHiryTmqJjsbLGBzONsi69mJ8FKJDFtxEnBYrkD3fLWfGozT-d1gEV7-gCoSVRdazI5YV9KRqez4MWWVOtroPsTz.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><h3 id=""""><strong id="""">Keep a Separate Domain for Staging</strong></h3><p id="""">A separate domain or subdomain, instead of your production domain, is recommended for your staging environment. If things go south, those changes don't negatively impact the SEO of the domain you use for your production site.&nbsp;</p><p id="""">Keeping the staging domain separate from production will also prevent users from accidentally accessing the staging version of your application. In cases of prolonged testing or experimentations with SEO-related experiments, it'll also avoid being crawled and indexed by search engines. Additionally, using a separate domain for the staging environment makes it easier to manage and maintain the two environments independently.</p><p id="""">For example, suppose you've set up a staging environment to test new features before deploying them to production and the staging environment uses the same domain as the production environment. Users may accidentally access the staging version of the site instead of the production version without even knowing, due to the same domain. This could lead to confusion and dissatisfaction among your users, as their accounts wouldn't be on the staging database. Thus, they wouldn't be able to access their work.&nbsp;</p><p id="""">Using a separate subdomain for the staging environment (such as ""staging.example.com"") is a better idea. Users will know that they're accessing the staging version of the site, and they can easily identify it and switch to the production version. This can help avoid confusion and ensure that users always access the correct version of your site.</p><h3 id=""""><strong id="""">Test, Test, Test</strong></h3><p id="""">Test all changes thoroughly in the staging environment before deploying them to the live site. Thorough testing will help you catch any potential issues before they affect your live site, even if the changes seem minor. The scope of this testing usually tests the changes a particular release will introduce into the application. However, to utilize a staging environment effectively, you should set up automated testing with the widest possible coverage. That way, the possibility of new changes breaking existing functionality can be invalidated and ruled out.</p><p id="""">Additionally, thorough testing on a staging environment can help improve the overall quality of the application. Furthermore, it can provide confidence that it'll function as expected in the production environment. You can even mitigate some costs and wasted time through automating your testing.</p><p id="""">For example, let's say you're working on an e-commerce website. In the staging environment, you test the website's checkout process, payment processing, and other critical features to ensure they work correctly. You also simulate high-traffic scenarios to ensure that the website can handle many users without crashing or experiencing other problems.&nbsp;</p><p id="""">This allows you to identify and fix any potential issues or bugs before they affect users in the production environment. As a result, when you deploy the website to production, it's stable and reliable, and users can use it to make purchases without encountering any problems. This can help improve the overall user experience and satisfaction with your website. And if you've automated this testing, you can often save time, money, and effort on the development side.</p><h2 id=""""><strong id="""">Managing Environments</strong></h2><p id="""">There are several ways to create and manage staging environments, and the best option for you depends on the infrastructure already in place at your company. Some companies choose to set up multiple on-premises servers to get both staging and production environments. Others use virtual machines (VMs) or cloud instances. Whatever you choose, be sure to replicate all aspects of your production environment in staging. This means servers, databases, networking, storage, configurations, and so on.</p><p id="""">To do that, you may want to consider a tool to help manage your environments.<a href=""https://releasehub.com/"" id=""""> Release</a> is one company offering<a href=""https://releasehub.com/whitepaper/easy-environments-management"" id=""""> easy environment management</a>, enabling developers to test in a sandboxed environment, where they can unleash their creativity without worrying about breaking things.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-center"" data-rt-type=""image"" data-rt-align=""center""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63c66f5603ef9caec36c2ed4_c7-GIvuyfqw1cbr-rYfTA75QnAjjYORyNPz4y5K7sEdlwf74_kMJI_mJ4TT_z1C_9VufFOnnt-nskpeEJvKncNfs3E-nHuaSaqSgjg8dKxCRgH7MVtraWKKM9IZlMoiYDClF5XaQM6pTDBDYMisMWw5xvpudX2KpWMLdk1EjpycoXJw0VnF3La4S8QSO.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><h2 id=""""><strong id="""">Summary</strong></h2><p id="""">A staging environment replicates the production environment, but it's used to deploy and test new code changes before they're pushed to production. Staging is necessary because it allows you to test new features before they go live. This offers more control and a safe testing process, as any issues that arise can be fixed before they impact your live site or application.</p><p id="""">We discussed best practices for staging environments, as well as how to effectively manage staging environments. These tips can help you derive the most value from your staging environment.</p><p id="""">Ultimately, it's essential to test everything before deploying to production. That way, you don't waste time and money when something goes wrong.</p>",false,,,,https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63c66fab65ab0c30bf2eec19_writing_person.jpeg,,,8,,,
6 Best EaaS Product Alternatives to Gitpod,6-best-eaas-product-alternatives-to-gitpod,62aa5a70cd5ba27d9d0d718a,63ce94d5da2bff272404d4c2,Mon Jan 23 2023 14:08:21 GMT+0000 (Coordinated Universal Time),Mon Jan 23 2023 14:11:39 GMT+0000 (Coordinated Universal Time),,Learn about the top alternatives to Gitpod in this article and find the best tool to meet your needs and preferences.,"<p>Are you looking for an alternative to Gitpod for your code? Many options offer similar functionality as cloud-based integrated development environments (IDEs). These alternatives can help you develop, collaborate on, and manage your projects more efficiently. By exploring some of the top alternatives to Gitpod, you can find the best tool to meet your specific needs and preferences. In this post, we'll introduce you to some of the top Gitpod alternatives and help you decide which one is right for you.&nbsp;</p><h2 id=""""><strong id="""">What is EaaS?</strong></h2><p id="""">Environment as a service (EaaS) is a cloud computing model that provides users access to a fully configured and managed computing environment over the internet. With EaaS, users can quickly and easily create, deploy, and manage applications and services without purchasing, configuring, and maintaining hardware and software infrastructure.&nbsp;</p><p id="""">EaaS providers offer a range of preconfigured environments that users can choose from, including different operating systems, programming languages, and frameworks. Users can customize their environments to meet their specific needs.&nbsp;</p><p id="""">EaaS provides many benefits over traditional IT infrastructure, including lower up-front costs, faster deployment times, and greater scalability and flexibility. It allows users to focus on developing and deploying their applications and services without worrying about managing and maintaining the underlying infrastructure.&nbsp;</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-center"" data-rt-type=""image"" data-rt-align=""center""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63ce946882f35795e8d9ebd3_2oL8F8mQegyg05p1QmB-bfvfbks5h2JGuV4ejK-rz33cwu4BLJnToEX34boxc2x1GnluvDboK4UTmjVTgCnSTskX04zD_JisP9V22GbCnB243oOl6EEJP1mdq7g_b9gWG58c-g53p7g4Ji6ioJW8Hxbx8TIZ9kgKt-f12xV2aRXMXctPzOauuavXtWN3.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><h2 id=""""><strong id="""">What Is Gitpod?</strong></h2><p id=""""><a href=""https://www.gitpod.io/"" id="""">Gitpod</a> is a cloud-based IDE that allows developers to work on code from any device with an internet connection. It enables collaboration on code projects with features like real-time code sharing and version control.&nbsp;</p><p id="""">One of Gitpod's critical features is its seamless integration with Git, a popular version control system. This allows developers to use Gitpod to manage their code repositories and collaborate with others on code projects hosted on Git.&nbsp;</p><p id="""">Gitpod includes several other tools and features helpful for developers. These features are code completion, debugging, testing, and deployment. Gitpod is available as a cloud service or can be installed on-premises.&nbsp;</p><h2 id=""""><strong id="""">What are the Gitpod Alternatives?</strong></h2><p id="""">Many alternatives to Gitpod offer similar functionality for cloud-based IDEs. Some of the top alternatives include the following:&nbsp;</p><ul id=""""><li id="""">Cloud9</li><li id="""">Eclipse Che</li><li id="""">CodeAnywhere</li><li id="""">JupyterLab</li><li id="""">Code Sandbox</li><li id="""">Replit</li></ul><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-center"" data-rt-type=""image"" data-rt-align=""center""><div id=""""><img alt=""ApplicationDescription automatically generated with medium confidence"" src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63ce9468fc37df6fb400e424_mG2O3dLSKgSEZcBV0IdzcGPYO8JX84c9IbXF6wOCTgujibyTBWreeSZDe1GMtut7mHqq4zrOMLV841lmbE-j-a1kTKS4KYRSjOl3-CgU3hx-vjr7DLfLz-l19_V4xtRukxH4k17gPchJv1shmPT2M1UXLWyfPm0n5m5Lk5VJCUZVIihTCXBMn3TxPqua.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><h2 id=""""><strong id="""">Cloud9</strong></h2><p id=""""><a href=""https://aws.amazon.com/cloud9/"" id="""">Cloud9</a> is an online development environment by Amazon Web Services (AWS) that enables developers to write, run, and debug code from any web browser. Its features include collaboration, version control, and integration with popular programming languages and frameworks.&nbsp;</p><h3 id=""""><strong id="""">Pros</strong></h3><ul id=""""><li id="""">Cloud9 allows developers to work from anywhere with an internet connection, enabling teams to collaborate in real time.</li><li id="""">Cloud9 provides a quick and easy set-up process, allowing developers to spin up a workspace in minutes without needing local installations.</li><li id="""">Workspaces are protected with built-in security measures, such as identity management and access control.</li><li id="""">Workspaces can be easily scaled up or down to meet the demands of a project.</li></ul><h3 id=""""><strong id="""">Cons</strong></h3><ul id=""""><li id="""">Cloud9 doesn't offer extensive customer support, so developers may have difficulty getting help if they encounter problems.</li><li id="""">Cloud9 lacks features such as built-in code completion, a feature that can make coding more efficient.</li></ul><h3 id=""""><strong id="""">Best Practices</strong></h3><p id="""">Leverage the Cloud9 features. Cloud9 IDE offers a wide range of features designed to help you work more efficiently, such as quick file search, code completion, and autocompletion of code snippets.&nbsp;</p><p id="""">Integrate with external services. Cloud9 IDE can be integrated with external services, such as code repositories and databases, to help streamline the development process.&nbsp;</p><p id="""">Use the Cloud9 Marketplace. The Cloud9 Marketplace offers a variety of tools and services that can be integrated into Cloud9 IDE, providing you with access to additional features and capabilities.&nbsp;</p><p id="""">Stay up to date on security patches. This will help ensure that your environment remains secure.&nbsp;</p><h2 id=""""><strong id="""">Eclipse Che</strong></h2><p id=""""><a href=""https://www.eclipse.org/che/"" id="""">Eclipse Che</a> is an open-source cloud IDE and developer workspace server that runs on Kubernetes. It offers a revolutionary developer workspace server that enables developers to create, edit, collaborate on, and debug applications in the cloud. It provides a single workspace server with a powerful IDE that can be accessed anywhere, anytime.&nbsp;</p><h3 id=""""><strong id="""">Pros</strong></h3><ul id=""""><li id="""">Eclipse Che enables collaboration between developers with an integrated development environment, version control system, task tracking, and more.</li><li id="""">Developers can scale their applications from small teams to large enterprise projects.</li><li id="""">Developers can customize the platform to their needs.</li></ul><h3 id=""""><strong id="""">Cons</strong></h3><ul id=""""><li id="""">Eclipse Che only offers limited plugins and integrations with other tools.</li><li id="""">Eclipse Che requires a complex set-up process and can be difficult for beginners.</li></ul><h3 id=""""><strong id="""">Best Practices</strong></h3><p id="""">Make sure to use strong authentication and access control to prevent unauthorized access. Use the built-in version control system to keep track of changes in your code. Test your applications in a staging environment before deploying.&nbsp;</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-center"" data-rt-type=""image"" data-rt-align=""center""><div id=""""><img alt=""A picture containing outdoor, nature, cloudsDescription automatically generated"" src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63ce9469b06dcec2fd9a16d0_orREGC66tNz2wE9CiOcL4Ax2fyM2jPPqADt9iOQ18A2j4gBYGb3fUxNoi5Qg_csfbawoMaROonCTwDubmAWMfZMKd4xnd3bXt93XmrxQXUUPiQPrY1087Ljg5-ICcdEwIVDT7Q4-zsXtakbvWSilFtGYMtzPo733_yyKfOIQD3So1DR0xQCJtx1oH23Q.jpeg"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><h2 id=""""><strong id="""">Codeanywhere</strong></h2><p id="""">Codeanywhere is a cloud-based IDE that allows developers to write, edit, and collaborate on code from anywhere. It has an intuitive interface that offers extensive features, including version control, debugging, and integration with popular cloud-based services.&nbsp;</p><h3 id=""""><strong id="""">Pros</strong></h3><ul id=""""><li id="""">Codeanywhere is easy to use with an intuitive interface.</li><li id="""">It's portable and accessible from any device.</li><li id="""">Supports a wide range of languages.</li><li id="""">Includes debugging tools.</li><li id="""">Codeanywhere supports version control.</li></ul><h3 id=""""><strong id="""">Cons</strong></h3><ul id=""""><li id="""">There is a potential lag when working on large projects.</li><li id="""">Codeanywhere's customization options are limited.</li><li id="""">There is limited support for third-party integrations.</li></ul><h3 id=""""><strong id="""">Best Practices</strong></h3><p id="""">Use version control to track changes and keep a backup of your code. Monitor and manage resources to optimize performance. Use linting and debuggers to identify and fix errors.&nbsp;</p><h2 id=""""><strong id="""">JupyterLab</strong></h2><p id=""""><a href=""https://jupyter.org/try-jupyter/lab/"" id="""">JupyterLab</a> is a web-based interactive computing environment for programming, data exploration, and visualization. It provides a set of tools for data science and machine learning.&nbsp;</p><p id="""">A data scientist working on a machine learning project could use JupyterLab to quickly explore different algorithms, visualize data, and debug code.&nbsp;</p><h3 id=""""><strong id="""">Pros</strong></h3><ul id=""""><li id="""">JupyterLab gives you access to powerful tools and libraries for data analysis.</li><li id="""">It supports multiple programming languages like Python, Julia, and R.</li><li id="""">JupyterLab is flexible, allowing users to create and share their notebooks.</li><li id="""">JupyterLab allows for big data integration and manipulation.</li><li id="""">It supports interactive widgets and provides visualization capabilities.</li></ul><h3 id=""""><strong id="""">Cons</strong></h3><ul id=""""><li id="""">JupyterLab has limited memory and CPU resources.</li><li id="""">It's slower than other IDEs.</li><li id="""">Error reporting with JupyterLab is poor.</li></ul><h3 id=""""><strong id="""">Best Practices</strong></h3><p id="""">Use version control to keep track of changes to your code.&nbsp;</p><p id="""">Monitor the performance and stability of your code.&nbsp;</p><p id="""">Set up your environment with the necessary libraries and packages.&nbsp;</p><p id="""">Break your code into smaller, more manageable chunks.&nbsp;</p><h2 id=""""><strong id="""">CodeSandbox</strong></h2><p id=""""><a href=""https://codesandbox.io/"" id="""">CodeSandbox</a> is an online development environment that allows developers to write and test code without installing software. You can use it for server-side scripting.&nbsp;</p><h3 id=""""><strong id="""">Pros</strong></h3><ul id=""""><li id="""">CodeSandbox is easy to access.</li><li id="""">There's no need to install any software.</li><li id="""">It allows developers to collaborate and share code easily.</li><li id="""">CodeSandbox can be used to test code without having to deploy it.</li></ul><h3 id=""""><strong id="""">Cons</strong></h3><ul id=""""><li id="""">CodeSandbox has limited access to system resources.</li><li id="""">There is limited support for libraries and frameworks.</li><li id="""">Security is a concern if the code is shared publicly.</li></ul><h3 id=""""><strong id="""">Best Practices</strong></h3><p id="""">Utilize the built-in tools and resources to maximize development efficiency.&nbsp;</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-center"" data-rt-type=""image"" data-rt-align=""center""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63ce9468e546a51647910ea0_8QONAYjj_nAQYzoL33B5Xc60PCp74W94QcmbN3X4mMxovDRPC9FcO9Z2KlVzDaNqOHaTlIaeAHIMIjr6sv7zYXSRTZlydnrpEcabjqpmYMnq54KosjzB_V1enojp7T1NMeZHV4g6RcKYVXn0DVrh8If3OR2wmpD9dm2dSvczQMQPXVyxhSRbnz0dzctb.jpeg"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><h2 id=""""><strong id="""">Replit</strong></h2><p id="""">Repl stands for Read-Eval-Print-Loop, a type of interactive programming environment. <a href=""https://replit.com/"" id="""">Replit</a> allows developers to quickly execute code snippets and instantly view the results, allowing them to quickly iterate and improve their code.&nbsp;</p><h3 id=""""><strong id="""">Pros</strong></h3><ul id=""""><li id="""">Replit is a flexible environment for experimentation.</li><li id="""">It's a useful tool for teaching and learning programming languages.</li></ul><h3 id=""""><strong id="""">Cons</strong></h3><ul id=""""><li id="""">Replit is not suitable for complex projects.</li><li id="""">It can be slow for large datasets.</li></ul><h3 id=""""><strong id="""">Best Practices</strong></h3><p id="""">Make sure you're familiar with the language you're using in the Repl.&nbsp;</p><p id="""">Document your code and keep notes on what you’ve tested and debugged.&nbsp;</p><p id="""">Use Repls as a teaching tool to help others learn to program.&nbsp;</p><h2 id=""""><strong id="""">Is Gitpod VS Code?</strong></h2><p id="""">Gitpod is not Visual Studio Code (VS Code), though both are development environments. Gitpod is a cloud-based IDE that's integrated with GitHub, while VS Code is a desktop-based IDE. Both provide features like syntax highlighting, autocomplete, linting, and debugging. However, Gitpod also includes features like a built-in terminal, Docker support, and a built-in CI/CD pipeline.&nbsp;</p><h2 id=""""><strong id="""">Is Gitpod Part of GitHub?</strong></h2><p id="""">Gitpod is not directly affiliated with GitHub. It allows developers to use GitHub repositories and collaborate with others on code projects hosted on GitHub using its cloud-based IDE. Gitpod is available as a cloud service.&nbsp;</p><h2 id=""""><strong id="""">Conclusion</strong></h2><p id="""">Many alternatives to Gitpod are available for those looking for a cloud-based IDE. These alternatives offer a range of features and tools to help developers collaborate on, develop, and manage code projects efficiently. Some popular Gitpod alternatives include Cloud9, CodeAnywhere, Eclipse Che, and CodeSandbox, each of which has its strengths and capabilities.&nbsp;</p><p id="""">Try out the Release <a href=""https://releasehub.com/ebook/the-complete-guide-to-automated-software-environments"" id="""">automated software development environment</a> today to see how quickly and easily you can move your development projects forward. Automate your software development environment and eliminate manual steps to accelerate development timelines and reduce costs.&nbsp;</p>",false,,,,https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63ce94d082f357a155d9f36d_cloud-header.jpeg,,,6,,,
6 Docker Compose Best Practices for Dev and Prod,6-docker-compose-best-practices-for-dev-and-prod,62aa5a70cd5ba27d9d0d718a,62f4173bc7f4a580d9775776,Wed Aug 10 2022 20:38:19 GMT+0000 (Coordinated Universal Time),Wed Oct 30 2024 19:55:56 GMT+0000 (Coordinated Universal Time),Wed Oct 30 2024 19:55:56 GMT+0000 (Coordinated Universal Time),Docker Compose is an excellent optimization tool for development and production. Learn best practices for Docker Compose,,true,<p>Optimize Docker container orchestration and streamline your dev-to-prod workflows with Release.</p>,https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=6-docker-compose-tips,"<p id="""">Docker solves the ""but it runs on my machine"" problem by introducing containerization. However, with a multifaceted code base, you must simultaneously run several containers like the back and front end. Further, this will require you to leverage tools such as Docker Compose. </p><p id="""">Docker Compose is an excellent tool for optimizing the process of creating development, testing, staging, and production environments. With Docker Compose, you'll use a single file to build your environment instead of several files with complex scripts with a lot of branching logic. You can also share this single file with other developers on your team, making it easy to work from the same baseline environment. </p><p id="""">This post is about the best practices of Docker Compose for development and production. </p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1429px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1429px""><div id=""""><img alt=""A picture containing text, stationaryDescription automatically generated"" src=""https://cdn.prod.website-files.com/603dd147c5b0a4221d1bd360/62f4168426c6981d1f86f087_tuX6EcJdbXt5b8OZj9Mf56jga_q10URv-j4Ao4sC3roRtCMADNow4YIDmSYwr4qvCcPhykOkZThLg4SS-DF4Ot_3jXMK8aLCNRNsqcgftRDMwF_e5-bPP0h2ps-JWrgGCtwkeqrsCcJXh5kJmy8wfNM.jpeg"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><h3 id="""">What Is Docker Compose Good for?</h3><p id="""">Docker Compose is a tool for defining and running multi-container Docker applications. It allows you to bring up and link multiple containers into one logical unit. If you want to use Docker containers, you create one container that listens on an unused port on your machine. All other containers will connect to this server container on the same machine. Linking is where the different Docker services are connected and communicate with each other through a central node. This enables them to share data like configuration or databases. </p><p id="""">Docker Compose allows you to deploy your application's services as containers and lets you manage these containers as an organized and working whole in a single place—without having to worry about configuring your application's dependencies. For instance, if your app depends on three other services—like a database, an email server, and a messaging server—using Compose means you won't have to manage them individually. </p><p id="""">Instead, Docker handles that part for you so that all four services are available within one cohesive environment. This significantly <a href=""https://release.com/blog/cutting-build-time-in-half-docker-buildx-kubernetes"" id="""">reduces the time</a> needed to get a service up and running. You can make changes simultaneously across all services. Therefore, Docker Compose is an excellent tool for building complex applications that utilize several services. </p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1000px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1000px""><div id=""""><img alt=""Graphical user interface, applicationDescription automatically generated with medium confidence"" src=""https://cdn.prod.website-files.com/603dd147c5b0a4221d1bd360/62f41684d0cfd5e76cb21315_GcBi_Fs-Hq6LDOHBq569nGf6dLTRvWhoJYkfH_YFVdfi58oXk9-15frp4aDWXlTF_99F_BwYY_LJrJI3jHSNFh0hQmhD-R6IXUNld_60LL-Kwn28aPuZw9DZ25ig-5H0lsCfqXgGcEpLD72RGQLwEAA.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><h3 id="""">Docker Compose Best Practices for Development</h3><p id="""">During development, you may have the advantage of leveraging local storage, which is not the case during production. In production, resources like storage are costly; thus, you must carefully structure the <strong id="""">docker-compose</strong> file. Essentially, the configuration in development and production slightly differ, and the best practices also differ. Below are the best practices you should employ when using Docker Compose during development. </p><h3 id="""">Mount Your Code as Volume to Avoid Unnecessary Rebuilds</h3><p id="""">By mounting the project directory (current directory) on the host to code within the container using the <strong id="""">new volumes</strong> key, you may make changes to the code as you go without having to recompile the image. This also means you do not have to rebuild and push your image to change between development and production environments. You must delete or stop your local Docker machine and start it again. </p><p id="""">Note: You can also use a link in your code instead of the bind mount, but the downside to this approach is that you'll have to rebuild and repush your Docker image each time you change. With a bind mount, all you need to do is restart the container. </p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1430px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1430px""><div id=""""><img alt=""LogoDescription automatically generated with low confidence"" src=""https://cdn.prod.website-files.com/603dd147c5b0a4221d1bd360/62f4168437dc737ca568939a_r2bLESWetpi5eBXUfOKM4ZLpGIHBXvchWYT6yxWbXKOLO0wbUNmd23jwXxtF5w5uSG3GForO66Fb-xeiJ-1RhTtIPIdDAIDZbH3shU7rMUX2KJfi9oz2VxY9Gwe9INM68jq5L0iWcwCjbtojL_XdMD4.jpeg"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><h3 id="""">Use an Override File</h3><p id="""">Some files are only necessary during development and not production. For example, developing a JavaScript application using any of its frameworks needs <a href=""https://webpack.js.org/"" id="""">webpack</a>. Thus, an override file will mimic the compose file but with webpack as a service. When you spin up the container, the compose files will bundle together during development. Therefore, when you make changes in the code base, you'll see the changes in real time. This allows you to have separate settings for the production and development environment by avoiding redundancy. Your <strong id="""">docker-compose.override.yml </strong>file will have the following: </p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yaml"">
services:
  webpack:
    build:
      context: "".""
      target: ""webpack""
    command: ""yarn run watch""
</code>
</pre></div><h3 id="""">Use YAML Anchors</h3><p id="""">YAML anchors let you reuse parts of your YAML file-like functions. You can use them to share default settings between services. For example, let's say you want to create two services: <strong id="""">api</strong> and <strong id="""">web</strong>. Both of them will use Redis as a cache and a database, but the <strong id="""">web</strong> service will need additional volumes mounted. It would be cumbersome to set all these things in the web service's <strong id="""">docker-compose.yml</strong> file because it would have to duplicate those lines in the <strong id="""">api</strong> service's file. </p><p id="""">Using YAML anchors to share those settings, you can set the volumes section in the <strong id="""">web</strong> service's file like this: </p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yaml"">
x-app: &default-app
  build:
    context: "".""
    target: ""app""
  depends_on:
    - ""postgres""
    - ""redis""
  env_file:
    - "".env""
  restart: ""${DOCKER_RESTART_POLICY:-unless-stopped}""
  api:
    <<: *default-app
    ports:
      - ""8000:8000""
  web:
    <<: *default-app
    ports:
      - ""8000:5000""
</code>
</pre></div><p id="""">Additionally, you may alter an aliased property by overriding it in a particular service. If you wanted to, you could set <strong id="""">port:5000</strong> in the example above to only the <strong id="""">web</strong> service. The alias will give it precedence. This method is beneficial when two services could share a Dockerfile and a code base but have some slight variations. </p><h3 id="""">Docker Compose Best Practices for Production</h3><p id="""">As mentioned, dev and production may have slight configuration differences. So now, let's look at some best practices to help your app be production ready. </p><h3 id="""">Leverage the Docker Restart Policy</h3><p id="""">Occasionally, you'll face a scenario when a service fails to start. A common reason is that another service on your host machine has changed, and Docker Compose uses the old environment variables. To ensure this doesn't happen, set the restart behavior to <strong id="""">restart: always</strong> and configure your services with <strong id="""">update_config: true</strong>. This will refresh the environment variables for each run. However, if your app relies on other services (MySQL, Redis, etc.) outside of Docker Compose, then you should take extra precautions. Make sure they are configured correctly. </p><h3 id="""">Correct Cleanup Order of Docker Images</h3><p id="""">You need to clean up the order of your images during production. Do not use <strong id="""">docker rm -f</strong> as it may destroy useful images. Always run docker<strong id=""""> rm -f --remove-orphans</strong>. If you're working in the dev stage, this is not an issue because Docker Compose builds images only once, then exposes them. Thus there's no need to worry about removing old images. However, in production, Docker loops through all images when the container stops and restarts. </p><p id="""">Consequently, there's no way for you to be sure that an image wasn't destroyed, even when <strong id="""">docker-compose down</strong> is called. If a container is stopped and restarted, then the exposed images can change, and you can't be sure they're still in use. Using <strong id="""">docker rm -f </strong>to delete containers is a mistake. Docker Compose reuses port bindings, so an old service is still available, even though its container was destroyed. </p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1000px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1000px""><div id=""""><img alt=""Graphical user interface, text, applicationDescription automatically generated with medium confidence"" src=""https://cdn.prod.website-files.com/603dd147c5b0a4221d1bd360/62f4168459b4fc90802e8b57_h6E9YjVogsMOkdZ6L4aUDd2domKi8l5MzB0YsfxGAelGqMxlBq6bFGgPWwxGKcv9zS7LmMjqF7AiUgTCNVNu0RffYStMvs0qyF9ZgkZbw6c7EaYdSyzqaKEJrCZBBXLGKD86A60ukLYURWetOJ-GDhY.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><p id="""">Since you cannot tell which containers might be potentially in use, you must delete all of them using the <strong id="""">--remove-orphans</strong> flag. If a container is restarted by Docker Compose (or something else) and it reuses the same port, the new image will have the same image ID as the old one. </p><p id="""">Notice we've added the <strong id="""">--remove-orphans</strong> flag because that ensures Docker Compose only deletes containers and images that are no longer in use, regardless of whether we or a running container uses them. This is crucial if you have services restarting. </p><h3 id="""">Setting Your Containers' CPU and Memory Limits</h3><p id="""">You can configure Docker to limit the CPU and memory of your containers by passing arguments into the <strong id="""">docker-compose.yml</strong> file before starting your container. For example, the following command will start a web service with one CPU: </p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yaml"">
web:
    deploy:
      resources:
        limits:
          cpus: ""1""
</code>
</pre></div><p id="""">If you set a specific number of CPUs in the <strong id="""">multi_cpu</strong> key, it will only be used when available. If you fail to set the limit, the service will use the maximum resources it requires. </p><p id="""">Tip: If you want to run multiple containers with different memory limits on the same machine, ensure that all your containers have different memory limits. This is because each container views how much memory it needs. </p><p id="""">Note: You can use this technique for multiple services if you'd like. Docker Compose will automatically get the values from the <strong id="""">env</strong> file for each container when it starts up. </p><p id="""">Consequently, you need to understand the resource requirements of your service. This will prevent you from wasting resources and minimize production costs. </p><h3 id="""">Conclusion</h3><p id="""">Hopefully, these tips will help you use Docker Compose more effectively in development and production. After trying out the above configuration and optimization, you should be able to build your containers efficiently. If you feel that the above approach reduces the complexity of your Docker composition setup, don't worry. There are much easier ways to organize your containerized services for development and production at <a href=""https://release.com/"" id="""">Release</a>.</p><p id="""">‍</p>",https://cdn.prod.website-files.com/603dd147c5b0a4221d1bd360/63e41b38a2e78b2cc2991100_081622%20(1).jpg,a person holding a pen,regis-wilson,5,Tue Aug 16 2022 16:10:00 GMT+0000 (Coordinated Universal Time),docker,
6 Software Development Environment Best Practices,6-software-development-environment-best-practices,62aa5a70cd5ba27d9d0d718a,63bbc80fa09a605ff86b564d,Mon Jan 09 2023 07:53:51 GMT+0000 (Coordinated Universal Time),Thu Apr 20 2023 21:04:51 GMT+0000 (Coordinated Universal Time),Thu Apr 20 2023 21:44:19 GMT+0000 (Coordinated Universal Time),This post will cover a range of best practices to improve the productivity and quality of your software development proc,"<p id="""">A well-designed development environment is essential for efficient and effective software development. It provides developers with the tools and resources to write, test, and debug their code.&nbsp;</p><p id="""">By following best practices for setting up and maintaining a development environment, you can improve the productivity and quality of your software development process.&nbsp;</p><p id="""">This post will cover a range of best practices, including using version control, writing automated tests, and collaborating with other developers. So, let's dive in and explore the best practices for a software development environment.&nbsp;</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1430px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1430px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63bbc6d49d70bb303bed4437_Tl43rhW7YS1LJPR96MZmkHqgPb2txjrRNwp6o0odedtcd9nqShcOGPgnAAaKqO5DumMpje7J93HewMAvBVZjZvAIfG-xEMtbhzp8iVw8BbS-Yxn3OXGwgd2K-8-6kLZiyUAEEyNSkMNpsOxiI3zWNfaOKmGcl5HBncKt2wCmhBv2neVZ9ehH3vD4rA_L.jpeg"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><h3 id="""">What is a Software Development Environment?</h3><p id="""">A software development <a href=""https://release.com/blog/remote-development-environments"">environment</a> is a set of tools and processes to create and develop software applications.&nbsp;</p><p id="""">A software development environment may include a code editor or integrated development environment (IDE), a compiler or interpreter, and other tools such as version control and testing frameworks.&nbsp;</p><p id="""">The software development environment is the workspace in which developers write, test, and debug their code. It provides all the tools and resources for you as a developer to be productive and create high-quality software.&nbsp;</p><h3 id="""">How Many Environments Should You Have for Software Development?</h3><p id="""">The number of <a href=""https://release.com/blog/remote-development-environments"">environments</a> you should have for software development depends on your specific needs and the size and complexity of your project.&nbsp;</p><p id="""">However, you can have at least two environments: a development environment for writing and testing code and a production environment for running the final version of your software.&nbsp;</p><p id="""">Having separate development and production environments allows you to test your code in a controlled environment before deploying it to users. This can help ensure that your software is reliable and free of bugs.&nbsp;</p><p id="""">Besides these two environments, you may also want a staging environment for testing code before deploying to production. This environment can help test new features or conduct performance or load testing.&nbsp;</p><p id="""">Having the right balance of environments is essential to support your software's efficient and effective development and deployment.&nbsp;</p><h3 id="""">6 Software Development Environment Best Practices</h3><p id="""">There are many best practices for developing software effectively and efficiently. Some of these best practices include:&nbsp;</p><h4 id="""">1. Organized File Structure</h4><p id="""">As a first step toward creating a consistent and productive development flow, you need to get organized around your project's file structure. The file structure should be based on the application's needs but not too rigid.&nbsp;</p><p id="""">Structure maps can help establish consistent naming conventions for all source files and the directories and files for each module. This is how you'll break down your application into modules and feature sets.&nbsp;</p><p id="""">Many development environments have a file structure variation of their own, making it easier to disentangle your legacy code into different modules that you can reuse or extract as libraries. Source control will help you keep track of all versions of implemented features, which helps with code maintenance.&nbsp;</p><h4 id="""">2. Using Version/Source Control</h4><p id="""">The two key terms here are version control and source code control. Version control is tracking changes to your source code. The goal of source code control is to keep track of every change made to the source files. Version control saves the previous versions of your code so that you can easily roll back if necessary.&nbsp;</p><p id="""">Source control allows multiple developers to work collaboratively on the same codebase without worrying about overwriting each other’s changes. By having version and source control in place, developers can ensure that their code is always up to date and that their team is working on the same version of the code.&nbsp;</p><p id="""">Source code control will ensure you're ready for the next step if you want to use a different development tool or paradigm. Using a version control system such as Subversion (SVN), you can quickly generate a report on how different branches affect performance.&nbsp;</p><p id="""">Examples of version control tools include Git, SVN, Mercurial, and Team Foundation Server (TFS). Git is the most popular version control tool developers use to store and track changes to their code on platforms such as GitHub and GitLab.&nbsp;</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1430px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1430px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63bbc6d49d70bb609fed4436_uKlpdG6sTN-vPnb48ZFa50laYxe2H1tZTSSo_TaPx_0Wqmhh8ruvnwLoelseVTSnXxSYqwXQisl_z-T-eI0Lgy6V4JyBprlSWK7eOSoyTeGZOAuyeFqh2Eo7L5iDF5uiDVgxVtHmr72bCan44nXvhNBb722L1FBOL-SzBIa7ZUDN4BSXbUpaGnmdVK9T.jpeg"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><h4 id="""">3. Testing</h4><p id="""">To ensure that applications function as designed, developers must test their code thoroughly before implementing it into an actual production environment.&nbsp;</p><p id="""">Code coverage reports can provide developers with valuable information about the percentage of their code that has been tested. This will serve as a guide to the portion of code that needs to be fixed.&nbsp;</p><p id="""">Here, you can employ different paradigms, including test-driven development (TDD).&nbsp;</p><p id="""">With TDD, you go through a red-green-refactor process. First, write some tests that will fail on the first run (red). Next, you'll write enough code to pass the test (green). Finally, you make changes to optimize your code (refactor).&nbsp;</p><h4 id="""">4. Performance Monitoring</h4><p id="""">Performance monitoring can help you identify areas for improvement and inform decisions regarding the allocation of resources. It can also help you diagnose and identify bugs that may have appeared during the development process.&nbsp;</p><p id="""">Being proactive about addressing and monitoring issues related to the performance of your applications can help you meet your users' expectations.&nbsp;</p><p id="""">Performance monitoring and cloud computing go hand in hand. You can use tools like <a href=""https://aws.amazon.com/cloudwatch/"" target=""_blank"">CloudWatch</a> and New Relic to help you understand application performance in real-time.&nbsp;</p><p id="""">If you're facing significant problems while developing software applications, there are a few things you can do to help improve your workflow. It's all about keeping track of release milestones, showing each set of changes separately, generating reports on the various impacts of different feature sets on an application's performance and code quality, and ensuring that everything works as planned when it goes out the door.&nbsp;</p><h4 id="""">5. Documenting</h4><p id="""">Documentation is a best practice for software development <a href=""https://release.com/blog/remote-development-environments"">environments</a> because it helps developers understand the code they're writing and how it interacts with other pieces of code. A good documentation process will help to ensure that changes to the code are captured.&nbsp;</p><p id="""">Documentation makes it easier for other developers to understand your code and work with it. It provides a reference point for developers when they need to debug or troubleshoot problems.&nbsp;</p><p id="""">Finally, documentation can help prevent errors by providing instructions that developers can refer to when writing code.&nbsp;</p><h4 id="""">6. Containerization</h4><p id="""">Containers provide a consistent and isolated environment for applications to run, so developers can be sure that their applications will run the same way in any environment. This is especially important for distributed applications, as <a href=""https://releasehub.com/blog/6-docker-compose-best-practices-for-dev-and-prod"" id="""">containers</a> make it easier to deploy the same application to multiple environments.&nbsp;</p><p id="""">Containers also offer portability, allowing applications to be moved from one environment to another quickly and easily. Containers are lightweight and efficient, reducing the overhead of running multiple applications in a single environment.&nbsp;</p><p id="""">Finally, containers offer scalability, making it easier to scale applications as needed.&nbsp;</p><h3 id="""">What are Best Practices According to Different Roles?</h3><p id="""">Below are some best practices depending on the role you hold.&nbsp;</p><h4 id="""">Leadership Best Practices</h4><ol id=""""><li id="""">Establish clear goals and objectives for the development team.</li><li id="""">Foster a culture of innovation and collaboration.</li><li id="""">Encourage developers to be innovative and develop creative solutions.</li><li id="""">Provide continual training and support for developers.</li><li id="""">Stay up to date on the latest development technologies and trends.</li><li id="""">Be an effective communicator and listener.</li></ol><h4 id="""">Secondary Employee Best Practices</h4><ol id=""""><li id="""">Follow established development protocols and procedures.</li><li id="""">Monitor project timelines and ensure deadlines are met.</li><li id="""">Implement quality assurance protocols to ensure software stability.</li><li id="""">Use debugging tools to identify and fix software issues.</li><li id="""">Document development details and specifications for future reference.</li><li id="""">Create user-friendly interfaces for the software.</li></ol><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1000px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1000px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63bbc6d4e3ca2ace05f5f744_0TqicG_4KXWqbjb_sylOvcaPIHCFK-9TV5t4JjggeibZckotYPStf9rtIjlK9xGG695CoaksAvPs1MCkhL95dMwKrY4NeWceJ_zey-nUJwP0F0M9jTMYIi4DcGdNC2Uy4gdmyQddcfoJDBuSy38K5cEL4nK6fqQn42YEDTuBRNsHr5B644bDAWNV5Ti3.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><h3 id="""">Conclusion</h3><p id="""">A well-designed software development environment is essential for efficient and effective software development. By following best practices for setting up and maintaining your development environment, you can improve the productivity and quality of your software development process.&nbsp;</p><p id="""">Some critical best practices include using version/source control, testing, documenting, and containerization.&nbsp;</p><p id="""">If you want to learn more about best practices for a software development environment, check out <a href=""https://release.com/blog/remote-development-environments"">Release</a>. Release is a comprehensive resource for software development best practices, including tips and tools for setting up and maintaining a development environment.&nbsp;</p><p id="""">Visit Release today to learn more, and take your software development to the next level.&nbsp;</p><p id="""">Want to improve your software development skills? Check out our latest blog post on the six best practices for creating a productive and efficient development environment.&nbsp;</p><p id=""""><em id="""">This post was written by Mercy Kibet. </em><a href=""https://hashnode.com/@eiMJay"" target=""_blank""><em id="""">Mercy</em></a><em id=""""> is a full-stack developer with a knack for learning and writing about new and intriguing tech stacks.</em></p>",false,,,,https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63e3fa2c326731a29e31c30a_012223_6%20Software%20Development%20(1).jpg,A group of people sitting at a table with laptops,mercy-kibet,7,Sun Jan 22 2023 22:43:00 GMT+0000 (Coordinated Universal Time),,
A Guide to Configuring and Deploying HPA on Kubernetes,a-guide-to-configuring-and-deploying-hpa-on-kubernetes,62aa5a70cd5ba27d9d0d718a,6321fc24316e4f198134ab98,Wed Sep 14 2022 16:07:00 GMT+0000 (Coordinated Universal Time),Fri Apr 21 2023 17:38:57 GMT+0000 (Coordinated Universal Time),Fri Apr 21 2023 18:22:03 GMT+0000 (Coordinated Universal Time),HPA in Kubernetes will make your Kubernetes cluster more self-sustainable and will offload you from repetitive tasks.,"<p id="""">One of the main advantages of Kubernetes is that it can take care of your containers for you. This means, for example, that it will move containers around to distribute the load on the cluster evenly, automatically restart failed pods, and kill those that misbehave and try to eat too many resources. Another nice feature is Horizontal Pod Autoscaler. As the name suggests, HPA can automatically scale your pods. But how? I'm glad you asked, because that's exactly what this post is about.&nbsp;</p><h3 id="""">What Is HPA in Kubernetes?</h3><p id="""">Normally when you create a deployment in Kubernetes, you need to specify how many pods you want to run. This number is static. Therefore, every time you want to increase or decrease the number of pods, you need to edit the deployment.&nbsp;</p><p id="""">If you only need to do that once or twice a year, it's not that big of a deal. But it's very unlikely that your traffic will be at the same exact level for the whole year. And the more spikes of traffic you have, the more time you'll have to spend editing your deployment to cope with the traffic. This also works the other way around: If you're running a very big cluster with lots of applications, you could save a lot of money by decreasing the number of replicas for each deployment during periods of less traffic, like during the night. But again, it would be a lot of work to make these adjustments manually all the time. And that brings us to HPA.&nbsp;</p><p id="""">The main purpose of HPA is to automatically scale your deployments based on the load to match the demand. <a href=""https://en.wikipedia.org/wiki/Autoscaling#Kubernetes_Horizontal_Pod_Autoscaler:~:text=3%5D%5B33%5D-,Kubernetes%20Horizontal%20Pod%20Autoscaler,-%5Bedit%5D"" target=""_blank"" id="""">Horizontal</a>, in this case, means that we're talking about scaling the number of pods. You can specify the minimum and the maximum number of pods per deployment and a condition such as CPU or memory usage. Kubernetes will constantly monitor your deployment, and based on the condition you specified, it will increase or decrease the number of pods accordingly.&nbsp;</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1000px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1000px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/6321f88a15975741d04c1914_aM4PaZfUxc61bLf-6p5RzbrFjT8Z-yGfeB5gAUmCNvGgLGlzAD6HLfomUH34aKMO7Qv62-TSm5kg36N-DdcAiBMcQROe_rnH1N79guvi1WyitvazcwjVMhrHOfZ_2kiOMlIlaXs7vwFQn9mSHmyFSeS_uplG4viLneAIHa3zQKlDHOTYecdEdAqYtQ.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><h3 id="""">What Is VPA in Kubernetes?</h3><p id="""">In Kubernetes, there is also a Vertical Pod Autoscaler (VPA). As you may guess by the name, it works contrary to Horizontal Pod Autoscaler. Instead of adjusting the number of pods up or down, as HPA does, VPA scales up or down the resource requests and limits for the pods.&nbsp;</p><p id="""">So, in theory, VPA tries to achieve the same thing that HPA does, but in practice, they serve very different purposes, and you shouldn’t use them interchangeably. But to understand the difference, we need to take a step back and talk about how autoscaler knows when to scale your pods in the first place.&nbsp;</p><h3 id="""">A Few Words About Resource Requests</h3><p id="""">We mentioned before that you need to provide a condition for HPA, such as CPU usage. So, for example, you can specify that you want your HPA to add an additional pod to the deployment when current pods have average CPU usage higher than 80%. But what does 80% mean? 80% of what? That's a very good question, and the answer will help you understand the main difference between HPA and VPA.&nbsp;</p><p id="""">You see, if you create a very basic Kubernetes deployment just by specifying its name and which docker image to use, you won't be able to add HPA to it. Why is that? It's because you didn't specify resource requests and limits for it. Long story short, specifying resource requests and limits for your <a href=""https://release.com/blog/kubernetes-pods-advanced-concepts-explained"" id="""">pods</a> isn't just a good practice, but also drastically helps Kubernetes do its job more efficiently. And that brings us back to the question of ""What does 80% usage mean?"" when setting up HPA thresholds. This percentage value is related to the pods' resource requests. And that's how HPA knows when to scale your pods up or down.&nbsp;</p><p id="""">If you say that your application is using around 2GB of RAM under normal load, you set the resource requests accordingly: for example, to 2.5GB (you should always set the request to a little bit more than average). Then your HPA will know that it needs to schedule an additional pod for your deployment when the current one is using more than ~2.4GB (this value will depend on the target value that you specify when creating your HPA).&nbsp;</p><h3 id="""">Figuring Correct Values for Resource Requests</h3><p id="""">But how do you know what resource requests to set in the first place? You could, of course, run your pods without requests first and check how many resources they normally use. But that's quite a time-consuming process, especially on big clusters with multiple applications.&nbsp;</p><p id="""">That brings us back to the VPA. You see, the point of VPA isn't to scale your deployments up or down in order to keep up with sudden spikes in traffic. That's the HPA's job. VPA should be used to get you a good baseline of resource requests and limits for your pods. This way, you're free from that time-consuming task of monitoring pods' typical usage and setting requests and limits accordingly. When the traffic goes up and your pods can't keep up, the HPA should add one or more pods to the pack to get resource usage back to the ""average."" At that point, VPA doesn't need to do anything.&nbsp;</p><p id="""">You can think of it as VPA working much slower and more long-term, looking at patterns of usage, while HPA responds quicker and provides short-term solutions for load spikes.&nbsp;</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1000px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1000px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/6321f88a1e9b9a057ef10aa6_CMoTKCVPnR52NHDJ_q8BM83o_WMCghBP5aAL4gAteuDJjw1Ar78ACm9wi_0H8mKlSLdTI5tV8zSgsFLFm1hIveff26v16LjlxmK6AykzOgGdn6yEChDgbShMVu0qoMzEcOuNmqO2oSEbFM8HZ2wg-Yv_MWHBD7JQyTa0Vc-3rJwqPJuphRwyfZvY9w.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><h3 id="""">How Do I Configure HPA in Kubernetes?</h3><p id="""">Now that you know all the theory, let's create some HPAs. We'll start with creating an example deployment with a resource request set:&nbsp;</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yaml"">
apiVersion: v1
kind: Service
metadata:
  name: nginx-svc
  labels:
    app: nginx
spec:
  ports:
  - port: 80
  selector:
    app: nginx
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
  labels:
    app: nginx
spec:
  replicas: 1
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx
        ports:
        - containerPort: 80
        <strong>resources:
          requests:
            memory: ""64Mi""
            cpu: ""100m""</strong>
</code>
</pre></div><p id="""">I'll save it as nginx.yaml and apply with <strong id="""">kubectl apply</strong>:&nbsp;</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yaml"">
$ kubectl apply -f nginx.yaml
deployment.apps/nginx-deployment created

$ kubectl get deploy
NAME               READY   UP-TO-DATE   AVAILABLE   AGE
nginx-deployment   1/1     1            1           115s
</code>
</pre></div><p id="""">Our deployment is up and running, so we can now add HPA to it. Just like anything in Kubernetes, you can add HPA by creating and applying the YAML definition, just like we did with this deployment. Another option is to use the <strong id="""">kubectl autoscale</strong> command. Let's start with the latter. To create HPA with <strong id="""">kubectl</strong> <strong id="""">autoscale, </strong>you need to execute the following command:&nbsp;</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yaml"">
$ kubectl autoscale deploy nginx-deployment --min=1 --max=5 --cpu-percent=80
horizontalpodautoscaler.autoscaling/nginx-deployment autoscaled
</code>
</pre></div><p id="""">After <strong id="""">kubectl autoscale,</strong> we need to specify the resource type we want to autoscale. In our case, it's <strong id="""">deploy</strong> (short for deployment), and then we specify the deployment name. After that, we pass the minimum and maximum amount of pods HPA can create and the condition on which to scale.&nbsp;</p><h3 id="""">Validating If HPA Works</h3><p id="""">In our example, we tell HPA to scale out pods when their CPU usage goes over 80%. And since in our deployment earlier we specified CPU requests of 100m, this means our HPA will start scaling out nginx-deployment when its average CPU usage goes over 80m. Let's validate that. First, we double-check if HPA is working using <strong id="""">kubectl get hpa:</strong>&nbsp;</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yaml"">
$ kubectl get hpa
NAME               REFERENCE                     TARGETS   MINPODS   MAXPODS   REPLICAS   AGE
nginx-deployment   Deployment/nginx-deployment   0%/80%    1         5         1          54s
</code>
</pre></div><p id="""">We can see it's working, and our nginx deployment is currently running one pod. There is no traffic on it, so it makes sense. But let's see if HPA will do its job when we put some traffic on nginx. For that, I'll deploy a simple load generator:&nbsp;</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yaml"">
apiVersion: apps/v1
kind: Deployment
metadata:
  name: load-generator
  labels:
    app: load
spec:
  replicas: 3
  selector:
    matchLabels:
      app: load
  template:
    metadata:
      labels:
        app: load
    spec:
      containers:
      - command:
            - ""/bin/sh""
            - ""-c""
            - ""while true; do wget -q -O /dev/null nginx-svc; done""
        name: load
        image: busybox
</code>
</pre></div><p id="""">Once deployed (using <strong id="""">kubectl apply -f load-generator.yaml</strong>) we can monitor your nginx deployment CPU usage with <strong id="""">kubectl top pods:</strong>&nbsp;</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yaml"">
$ kubectl top pods
NAME                               CPU(cores)   MEMORY(bytes)
nginx-deployment-bc54c744b-tlpds   112m         4Mi
</code>
</pre></div><p id="""">And when we see the CPU usage goes over 80m, we can check the status of HPA again:&nbsp;</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yaml"">
$  ~ kubectl get hpa
NAME               REFERENCE                     TARGETS    MINPODS   MAXPODS   REPLICAS   AGE
nginx-deployment   Deployment/nginx-deployment   138%/80%   1         5         2          16m
</code>
</pre></div><p id="""">And shortly after that, the target CPU usage should also drop (since the traffic is now distributed to two pods):&nbsp;</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yaml"">
$ kubectl get hpa
NAME               REFERENCE                     TARGETS   MINPODS   MAXPODS   REPLICAS   AGE
nginx-deployment   Deployment/nginx-deployment   59%/80%   1         5         2          20m

$ kubectl top pods
NAME                               CPU(cores)   MEMORY(bytes)
nginx-deployment-bc54c744b-qshtz   52m          3Mi
nginx-deployment-bc54c744b-tlpds   59m          3Mi
</code>
</pre></div><p id="""">So, HPA is working as expected. It increased the number of pods for our deployment based on the load. Just keep in mind that HPA doesn't work instantly. It usually takes a few seconds before it will take any action, just to avoid unnecessary scaling actions on very short load spikes.&nbsp;</p><p id="""">You now know how to create HPA with <strong id="""">kubectl,</strong> and this is how you do the same with YAML definition:&nbsp;</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yaml"">
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: nginx-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: nginx-deployment
  minReplicas: 1
  maxReplicas: 5
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 80
</code>
</pre></div><p id="""">I showed you the basic usage of HPA using only CPU metrics. But you can also use memory usage instead. For that, you only need to change <strong id="""">name:</strong> to <strong id="""">memory </strong>in your HPA YAML definition.&nbsp;</p><h3 id="""">Custom Metrics</h3><p id="""">HPA can also be configured using custom metrics. For example, take the average HTTP response time. For that, HPA offers either pod-related metrics or so-called object metrics, which can be related to anything other than pods, like networking.&nbsp;</p><p id="""">Let's see an example. If your application exposes a metric called ""orders-pending,"" we can use the AverageValue of that metric to configure HPA as follows:&nbsp;</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yaml"">
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: nginx-hpa-custom
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: nginx-deployment
  minReplicas: 1
  maxReplicas: 5
  metrics:
  - type: Pods
    pods:
      metric:
        name: orders-pending
      target:
        type: AverageValue
        averageValue: 10
</code>
</pre></div><p id="""">Custom metrics are quite a broad topic and will depend on your use case. For more information on custom metrics, you can refer to Kubernetes documentation <a href=""https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/#autoscaling-on-multiple-metrics-and-custom-metrics"" target=""_blank"">here</a>.&nbsp;</p><h3 id="""">Summary</h3><p id="""">As you can see, HPA is relatively easy to set up. It only takes one kubectl command or a few lines of YAML file, and you can get even more benefits by instructing it to monitor custom metrics from your application. Clearly, HPA is a very nice thing to know. It will make your Kubernetes cluster even more self-sustainable and will offload you from repetitive tasks.&nbsp;</p><p id="""">If you want to learn more about Kubernetes, feel free to take a look at <a href=""https://release.com/blog"">our blog here</a>.</p>",false,,,,https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63e420656c206d027422d279_102622_2%20(1).jpg,a row of computer screens with a glass in front of them,ashley-penney,5,Wed Oct 26 2022 16:00:00 GMT+0000 (Coordinated Universal Time),,
A Manager's Guide to Release Cycles,a-managers-guide-to-release-cycles,62aa5a70cd5ba27d9d0d718a,632a15e73e8563575ec8258c,Tue Sep 20 2022 19:35:03 GMT+0000 (Coordinated Universal Time),Fri Apr 21 2023 17:31:32 GMT+0000 (Coordinated Universal Time),Fri Apr 21 2023 18:22:03 GMT+0000 (Coordinated Universal Time),You will learn what a release cycle is and a manager's role within it. Receive some tips on how to manage a release. ,"<p id="""">If you're a project manager, engineering manager, or executive (like a CTO), you're probably involved in a lot of tedious planning and meetings in the lead-up to a new product release—if you're a new manager in your company or team, the lead-up is even more overwhelming.</p><p id="""">The formula for your success as a manager doesn't rely solely on your tech knowledge or skills. It also relies on your ability to lead, plan, and release the right product at the right time.</p><p id="""">Knowledge of the product release cycle and your role as manager in it will help you get a handle on this key process. Read on for some tips on how to measure and manage a <a href=""https://en.wikipedia.org/wiki/Software_release_life_cycle"" target=""_blank"" id="""">release</a> based on some best practices.</p><h3 id="""">What is a Release Cycle?</h3><p id="""">A <strong id="""">release cycle</strong>, also called <strong id="""">release management</strong>, is the process of planning, scheduling, managing, and controlling the progress of a software build through the various stages of development to the deployment of the product.</p><p id="""">Release management requires some important skills, like technical knowledge of the software requirements and a firm grasp of risk management because at each stage of the cycle, you're placing the reputation of your organization on the line. You'll also want to master stakeholder engagement&nbsp;so that you can keep stakeholders informed and coordinate collaboration.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1000px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1000px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/632a10a2748219c8fa63240d_k7a3insdlIy2AEspnV6jSYtTA7B88LrNdfvz73Q_A8Pi_EYZXeHZbKPhogeBIMnbL5egadHy2iQZp6RyLRwtJPRGdweEYzrA8CuKqxcx0qrXUbbyah7yvYuDRvj7nuwRwhzOpuVKfsJtr7Yow9XyvM2L-GZ4-91GnmZ6t866Svgbg70YiUhA16C5ag.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><h4 id="""">What is a Software Release Life Cycle?</h4><p id="""">A software release life cycle (SRLC) is the sum of the stages of software development from its initial conception phase to its release. This includes the final, updated version of the release version to improve the software or fix bugs still present in the software. The cycle includes:&nbsp;</p><ul id=""""><li id=""""><strong id="""">Pre-alpha phase:</strong> In this stage, developers are building the software but have not formally started testing it yet.</li><li id=""""><strong id="""">Alpha phase:</strong> Here, developers begin formal testing of the software by doing white-box testing like unit testing or integration testing.</li><li id=""""><strong id="""">Beta phase:</strong> The beta stage comes after the requirements in the alpha phase of the software are completed. In this stage, users test the software, and user acceptance testing can be done. The beta test can be closed (limited to a certain number of users) or open to anyone (publicly available).</li><li id=""""><strong id="""">Release candidate stage:</strong> This stage is an iteration on the beta phase, fixing or improving the software based on issues discovered in beta. Beta testing is carried out to confirm that the software works correctly.</li><li id=""""><strong id="""">Production or stable stage:</strong> This is the final stage of software development, when the software is released to the market for the users. This version of the software is usually stable and free from crashes or system failures.</li></ul><h4 id="""">What is a Manager's Role in a Release Cycle?</h4><p id="""">The release cycle includes numerous activities, and as manager, you need to make sure that everything goes as planned and the goals are achieved. One of your key responsibilities during a release cycle is effectively communicating with stakeholders and getting updates on the progress of the project. You also need to do the following:</p><ul id=""""><li id="""">Assess the risks involved in carrying out the project, or identifying the pitfalls.</li><li id="""">Make scheduling decisions based on the progress of the project.</li><li id="""">Assign tasks to stakeholders or developers involved in building the project.</li><li id="""">Remove blocking issues that may hinder the success or progress of the project.</li><li id="""">Run regular team meetings to get updates on the project.</li><li id="""">Update project status and progress so things stay on track.</li></ul><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1000px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1000px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/632a10a25710ad79a2cb3a85_FBNIP3awn6h4KpX3aMwbecOK7_ffhN7u_JQrPW4q8kEuveR38LHMAJCWn0pww3bTrDtughl2KGn6eeO0qd1NjqXURL2JvVAlKPOJ3a45KgVQ48mvWyB0v-dAq8VSwjqAxAswFV35ZzwvNP6KjMn30-WxnZHP0hH23RBNSdlYxwioy7iFbkTBayMs7A.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><h4 id="""">What are the Steps in a Release Cycle?</h4><p id="""">To successfully manage the release cycle of your project, you'll want to time and schedule activities properly. You'll need to guide, collaborate, and communicate with your team members and colleagues at every stage of the release cycle to ensure that you release effectively and on time.</p><p id="""">Let's take a look at those stages below.</p><h5 id="""">Specification Stage</h5><p id="""">In the specification stage of the release cycle, the software specifications are laid out and the requirements to build it are planned. The manager ensures that the requirements and specifications for the software are achievable and executable.</p><h5 id="""">Development Stage</h5><p id="""">The development or execution stage of the release cycle involves the developers and other stakeholders responsible for building the software engaged in developing the product according to the specifications and requirements laid out in the specification stage.</p><h5 id="""">Testing Stage</h5><p id="""">The testing stage follows the successful build of the software or application, when a series of manual or automated testing is performed on the product. Some of this testing includes:</p><ul id=""""><li id=""""><strong id="""">Unit testing: </strong>Testing different units or functional components of the software or application.</li><li id=""""><strong id="""">Integration testing: </strong>Testing the different integrations used across the software.</li><li id=""""><strong id="""">System testing: </strong>Testing the entire system for bugs or vulnerabilities.</li><li id=""""><strong id="""">User acceptance testing: </strong>To measure users' responses to the software, and how they interact with it.</li></ul><h5 id="""">Sign-off Stage</h5><p id="""">In the sign-off stage, the manager performs a final set of checks. Once the software has passed through the previous stages of the release cycle to make sure it's ready for deployment or rollout to its end users, the manager can ensure the users are comfortable with how it works.</p><h5 id="""">Deployment Stage</h5><p id="""">This is the final stage of the release cycle. At this point, the finished product should be ready for the end users at the scheduled time. Also, this last phase should be free of issues to prevent production rollback, which can ‌lead to a terrible impression of the company—and ultimately, the manager.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1000px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1000px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/632a10a28897a879b33d50e3__xJqKOMdjVLGInoEQVow0TKXPiRN8b8SA2DHm-RieqsE0pQKiCbFbqRwq6ygg-5UuOMXfGPm_KwZKg-0WJTwkJiPG7EH4iesA2NMgYN3rspjhBz6NMy3u1FHPMPcvGqJfAXX7hg61UbUJ9BwA5nFvz6D_eKS9XOl3ogsUB_iw-bCBWKaMK1DMPoo6A.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><h4 id="""">How to Measure a Release Cycle</h4><p id="""">Just knowing the stages in the software release life cycle isn't enough. As a manager, it's also important that you know how to manage the release cycle and measure the success of each stage. Here are some tips on how to measure and manage your release cycle.</p><h4 id="""">Tips and Best Practices for Release Cycles</h4><ul id=""""><li id=""""><strong id="""">Set realistic goals and KPIs.</strong> Be truthful when setting goals and KPIs (Key Performance Indicators) for you and your teams during a release cycle so that you're guided by the deadlines you have set, and how much work you can do. Use these goals and KPIs to track what the stakeholders expect, and what you can accomplish on the project.<strong id="""">‍</strong></li><li id=""""><strong id="""">Track and record all progress.</strong> Enlist someone to help you track the daily or weekly progress of the project to ensure that you don't miss important events during the release cycle and ease your stress.<strong id="""">‍</strong></li><li id=""""><strong id="""">Use a version control system.</strong> In software development, version control systems don't only help in organizing the whole product development. They also serve as a source of truth when issues or conflicting errors arise. Some popular version control tools include Git, CVS, SVN, and Mercurial.<strong id="""">‍</strong></li><li id=""""><strong id="""">Decide on framework adoption.</strong> Consider adopting a management framework that fits your team to make releases more frequent and easy, so that you can deliver value to your organization faster. Consider breaking your workflow into sprints.<strong id="""">‍</strong></li><li id=""""><strong id="""">Decide on testing.</strong> Think about testing early in the process, and test your software as part of a routine throughout the lifecycle.<strong id="""">‍</strong></li><li id=""""><strong id="""">Don't forget about culture.</strong> Create a culture that encourages and invites others in the team to collaborate and make genuine reviews of the product release.<strong id="""">‍</strong></li><li id=""""><strong id="""">Be communicative.</strong> Communicate with your team members and stakeholders regularly about release sprints and schedules.<strong id="""">‍</strong></li><li id=""""><strong id="""">Use productivity software.</strong> Use productivity software to integrate your workflow and track the progress of the release.<strong id="""">‍</strong></li><li id=""""><strong id="""">Limit post-production changes.</strong> Avoid pushing changes directly to production, especially changes that haven't been reviewed or tested, as this can affect the business if something goes wrong.<strong id="""">‍</strong></li><li id=""""><strong id="""">Have a rollback strategy to protect your organization.</strong> This will help you roll back your software without affecting the user experience or the business.</li></ul><h4 id="""">Conclusion</h4><p id="""">How disciplined, knowledgeable, and collaborative you are as a manager determines the success of your team or company. Releasing a product to the market takes a lot of effort and commitment and every release cycle comes with its share of difficulties, but a good manager knows when and how to handle these challenges. If you want to know more about how you can get an on demand environment for development, staging and production, check out<a href=""https://release.com/""> Release</a>.</p>",false,,,,https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63e422581fc65b4423a38027_113022%20(1).jpg,,regis-wilson,3,Thu Dec 01 2022 06:05:00 GMT+0000 (Coordinated Universal Time),,
A Simple Guide to Software Environments,a-simple-guide-to-software-environments,62aa5a70cd5ba27d9d0d718a,6435cccf9a808aea399eea37,Tue Apr 11 2023 21:10:39 GMT+0000 (Coordinated Universal Time),Thu Apr 25 2024 20:30:37 GMT+0000 (Coordinated Universal Time),Thu May 02 2024 20:19:24 GMT+0000 (Coordinated Universal Time),Learn about tools to scope and limit instances of your software as it progresses from development to a finished product.,"<h3 id="""">Introduction</h3><p id="""">Building software is hard. It takes a lot of time and effort and it’s a somewhat cumbersome process. Software development passes through various stages in the product lifecycle before it reaches the end user. A spec must be defined, developers have to code the product, QA has to test it, and DevOps engineers have to deploy it. During this process, the code passes through several environments, from the developers’ local PC to containers in the cloud.&nbsp;</p><h3 id="""">What Are Software Environments</h3><p id="""">Software environments are tools to scope and limit instances of your software as it progresses from development to a customer facing product. Each environment contains the following:&nbsp;</p><ul id=""""><li id="""">Copy of your software code</li><li id="""">Copy of the database tables you use with (possibly) environment-specific data</li><li id="""">Internal network so the different microservices your code uses can communicate with each other and with external services</li><li id="""">Global values (environment variables)</li><li id="""">Access control to limit who can view the application and make changes to it in the specific environment</li><li id="""">Infrastructure to support it (servers and such)</li><li id="""">Testing software to test the code</li></ul><p id="""">Let's elaborate by giving specific examples of different environments.&nbsp;</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1430px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1430px""><div id=""""><img alt=""A computer sits on a window sillDescription automatically generated with medium confidence"" src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/6435cad7c79096f701b5a8b4_OHH_2hZexA0xRsY9P8vCPYblte10zNEPP_zGLZwyYF-mIIhVkwBp2IAheZlFEcR60oV7xrqvcEDPC1c-QWTGVhRXX-6SVW5HsfNko3KTn55QtlWbKXkGVWDma7q63VzNt8lp0id4azxGPRyNg-wGMw.jpeg"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><h3 id="""">Local Environment</h3><p id="""">The local environment is where a developer writes software code. They may be writing the whole application or just a specific feature for the application. The developer usually has a local database that works with the application code, and sometimes access to external microservices needed by the code. In other scenarios, where direct access from the developer’s laptop to external services is not possible, we use software mocks instead. When the app runs locally on the developer’s laptop, performance of the app in terms of CPU and RAM is low, compared with running it on a dedicated server or in other environments. In addition, there’s no access control for this environment. Anyone who can access the laptop has access to the application. On the other hand, it's the simplest and most immediate way to develop and test code.&nbsp;</p><h3 id="""">Development Environment</h3><p id="""">The development environment is where multiple developers upload their code to. It’s meant to be the first place where other stakeholders can test the code and the application’s different features. Developers merge the code and run a CI/CD pipeline here. The <a href=""https://en.wikipedia.org/wiki/CI/CD"" id="""">CI/CD pipeline</a> usually consists of the following steps:&nbsp;</p><ul id=""""><li id="""">Changes to the database (database migrations)</li><li id="""">Static code analysis</li><li id="""">Vulnerability scanning</li><li id="""">Deployment to the development environment of the end artifact (JAR in Java or a binary)</li></ul><p id="""">In addition, this environment has more robust and thorough tests than those a developer runs on their laptop (e.g., integration tests). Developers apply the latest updates to the application in the development environment. Because it’s constantly being updated, the application is not stable in the development environment. It has a dedicated URL, environment variables, and better performance than versions run on individual developers’ local PCs. Usually all the developers on the team have access to this the development environment and to its database, which are not accessible outside of the organization (to clients, for instance).&nbsp;</p><h3 id="""">Staging Environment</h3><p id="""">The staging environment is the next environment the software passes through before it’s made accessible to end users. The staging environment should be very similar to the production environment, mirroring it as accurately as possible, because its purpose is to be the last checkpoint before making the latest software publicly available. It should run on the same infrastructure, have the same database content, and the same environment variables (or as similar as possible) to those in the production environment.&nbsp;</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1000px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1000px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/6435cad71dbcc571ce7d6cc6_Zfku1n4N1tuSijMG5b4VuqtEGhrfc7mQldrPcq69QqlCtXCLe5bnf9BrA6GR3NZcTee8I4SfqSph2Zu6Eqv4Sl8zvTBVUqDkt2ucNv1giD9xksnUNiooY1Poi3U1bOTN5JcFAF-hbw_cJQvAaq1_Og.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><p id="""">Traditionally, the CI/CD pipeline that runs in the staging environment involves the most extensive testing of all the environments. All stakeholders in the organization have access to view this environment, but only a handful of key employees have direct access to modify it (IT team, DevOps, senior developers). It's a stable environment and after the code passes the tests, it will be deployed to production. However, recently companies started shifting testing to the left and conducting robust testing earlier in the cycle.&nbsp;</p><h3 id="""">Production Environment</h3><p id="""">The production environment runs the code that serves end users — your customers. It's a stable environment with powerful infrastructure that can handle spikes in demand and has extensive monitoring and logging. Unlike in other environments, down time here results in an immediate service outage. To protect the infrastructure, environment variables, database, and code, access to the production environment is usually restricted to only system administrators. Because automatic tests can reduce performance and change the database, no tests are run in the production environment.&nbsp;</p><h3 id="""">Customer Demo / Customer Specific Environment</h3><p id="""">It sometimes happens that you want to create a version of your software specific to a certain client or to demonstrate a certain feature before it’s available to customers. This happens in the <a href=""https://release.com/blog/great-saas-sales-demos"" id="""">customer demo environment</a>. It has all the bells and whistles of the production environment, but it might have a superset or subset of features from the production environment and a client-specific GUI or dedicated access control, relevant to one client only.&nbsp;</p><h3 id="""">Environment Pain Points</h3><p id="""">Provisioning environments is difficult and time consuming. Provisioning new environments requires servers, a deployment mechanism, design of CI/CD pipelines, database servers, access control definitions and so forth. In addition, every environment consumes significant resources to maintain it. For instance, the IT team needs to make sure that connectivity works as expected, make snapshots of the database, edit the list of stakeholders who have access to each environment, and so on.&nbsp;</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1430px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1430px""><div id=""""><img alt=""A person holding a cigaretteDescription automatically generated with low confidence"" src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/6435cad7e217a54410a93081_mSHxY5DX8xU9MSoE1honJY57vqbPYRX3vwS1cywdvZwor8oekBiSC3Olbh3La6JBPkemfTAonLxEXVqXeaGt-oR3Q_ehgNeLN-3R9qjIirbgO2cmf1GoQfnsWYDjlgPc_M7bxbXP5lms5MJzPVwdow.jpeg"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><h3 id="""">Easing Pain Points</h3><p id="""">Recently, new products that provide <a href=""https://release.com/blog/environments-as-a-service-eaas-top-3-benefits"" id="""">environments as a service (EaaS)</a> appeared on the market to address those pain points. They provide an unlimited number of automated environments with the click of a button, centralized management of environments, and a lower overhead for making changes to existing environments.&nbsp;</p><h3 id="""">Increased Velocity</h3><p id="""">In addition to simplifying and speeding up environment deployment, EaaS solutions facilitate increased velocity of the R&amp;D process as a whole because developers can spend less time in each environment and can push more code to production faster. For example, they don’t have to pause pushing code for monthly maintenance. Basically, EaaS is another building block every organization should have in their DevOps tool belt to allow the R&amp;D department to deliver more without requiring additional resources.&nbsp;</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1000px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1000px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/6435cad919107870f64f41c1_mXqkTW-uoKD3J4a0jZ4ungTvMprvSRzg9iVdCYimoZmlw9pLHTCp_xZrn1c8XBHlAfTzNH6s73PlnKA-vOt3NgAheVfqmmy_1GS4cL_7wPM7CO1Mld4aLLOfSQ0l7I5I21Z3eKezC15Q54SMI2ruSw.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><h3 id="""">Conclusion</h3><p id="""">Environments are an integral part of software development. As software progresses from development to production, it goes through different environments along the way, each with its own purpose. Per its purpose, each environment offers a certain level of performance, access control, code version, and database content. Provisioning and managing environments manually is time consuming and resource intensive. So are creating VMs or containers, setting up networking, adding SSL certificates, providing access control and so forth. EaaS tools have emerged to solve those issues. They provide environment provisioning with a click, an unlimited number of environments, central management, and more. <a href=""https://release.com/blog/increase-developer-velocity-by-removing-environment-bottlenecks"" id="""">Eliminating the bottleneck of environment management</a> will allow every R&amp;D team to increase development velocity and produce more with the same amount of resources.</p>",false,,,,https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/6435d2743b2aa58c0585898d_041123.png,,alexander-fridman,5,Tue Apr 11 2023 19:00:00 GMT+0000 (Coordinated Universal Time),product,
"How to Make an Agile Release Plan, With Examples",agile-release-plan,62aa5a70cd5ba27d9d0d718a,62aa5a70cd5ba2d1cb0d7307,Tue Feb 15 2022 22:18:07 GMT+0000 (Coordinated Universal Time),Tue Apr 04 2023 22:05:44 GMT+0000 (Coordinated Universal Time),,"How do you create an Agile release plan? What are the steps, and what's it like in action? Here's how to get started and","<p id=""""><em id="""">""Change is the only constant in life"" - </em><a href=""https://www.worldhistory.org/Heraclitus_of_Ephesos/#:~:text=expected%20him%20to.-,Life%20is%20Flux,-Following%20in%20the"" id=""""><em id="""">Heraclitus</em></a></p><p id="""">Was Heraclitus a developer, or did the Oracle of Delphi show him where things were going? Either way, his most famous quote is an apt description of the world we work in. We have to navigate constant transitions while maintaining forward progress. The Agile process helps us cope with that chaos. It's right there in the <a href=""https://www.merriam-webster.com/dictionary/agile_"" id="""">name</a>: <em id="""">having a quick, resourceful, and adaptable character</em>. So when it's time to plan the first or next increment in your product, you need an Agile release plan.&nbsp;<br></p><p id="""">Let's look at what an Agile release plan is and how you can go about creating one.&nbsp;</p><h2 id=""""><strong id="""">What's an Agile Release Plan?</strong></h2><p id="""">Many bloggers have spilled a great deal of digital ink over the topic of Agile, where it comes from, and how to implement it. But we have the <a href=""https://www.agilealliance.org/agile101/12-principles-behind-the-agile-manifesto/"" id="""">Agile Principles.</a> They're the essential guide on what it means and why we need it. So, before we delve into creating an Agile release plan, let's level set.&nbsp;</p><h3 id=""""><strong id="""">The Agile Principles</strong></h3><p id="""">&nbsp;cover the four principles and how they relate to an agile release plan.&nbsp;</p><p id=""""><em id="""">#1 - Our highest priority is to satisfy the customer through early and continuous delivery of valuable software.</em></p><p id="""">Any successful software development team focuses on customer satisfaction, but this principle introduces the concepts of <strong id="""">early</strong> and <strong id="""">continuous</strong>.&nbsp;</p><p id=""""><br></p><p id="""">Early and continuous means releasing features and fixes as you build and test. It's Agile's alternative to holding onto new code or rushing new features in the service of meeting an arbitrary date.&nbsp;</p><p id=""""><em id="""">#2 - Welcome changing requirements, even late in development. Agile processes harness change for the customer’s competitive advantage.</em></p><p id="""">Here's the hard part. Your release plan needs to keep your development efforts on target, but you still need to address changing requirements.&nbsp;</p><p id=""""><em id="""">#3 - Deliver working software frequently, from a couple of weeks to a couple of months, with a preference to the shorter timescale.</em></p><p id="""">Principle #3 echoes the salient points in #1 but helps us out by establishing a solid time range. Agile works in increments of weeks.&nbsp;</p><p id="""">#4 - Business people and developers must work together daily throughout the project.</p><p id="""">Finally, we get a hint on how to formulate and execute an Agile release plan; by getting developers and business people to work together. Moreover, they don't work together on the project and head back to their respective departments. They work together <em id="""">daily</em>.&nbsp;</p><p id=""""><br></p><p id="""">Before we move on, it's worth paying <a href=""https://www.agilealliance.org/agile101/12-principles-behind-the-agile-manifesto/"" id="""">Principle #6</a> a brief visit. It refines and reinforces #4 when it calls face-to-face conversations the most effective form of communication. You don't do Agile Release Planning over emails and Jira tickets. You do it by sitting down as a group and formulating a plan.&nbsp;</p><h3 id=""""><strong id="""">What's an Agile Release?</strong></h3><p id="""">Based on the Agile principles, we can infer what makes a release <strong id="""">agile</strong>. A release is a set of features you deliver to customers in a single increment. It's <strong id="""">agile</strong> when you bake in the ability to adjust.&nbsp;</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1000px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1000px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/620c26f6f42fa783402fd1da_How%20to%20Make%20an%20Agile%20Release%20Plan%2C%20With%20Examples%20pq01.png"" loading=""lazy"" width=""auto"" height=""auto"" id=""""></div></figure><p id=""""><br>Agile release plans adapt to changing requirements and conditions. They don't pick a date and decide what fits in or set a date and drive mercilessly to meet it. They set priorities, estimate the effort required, do the work, and make adjustments based on updated conditions and results.&nbsp;<br></p><h2 id=""""><strong id="""">Agile Release Planning Process</strong></h2><p id="""">The release planning process helps your team determine how to channel their efforts into the next increment of features and fixes.&nbsp;</p><h3 id=""""><strong id="""">Step 1: Establish the Release Vision</strong></h3><p id="""">Before planning a release, you need a vision for your product and its next step. This vision guides you as you decide which features to prioritize, which to slip if the situation changes, and which to set aside for the next pass.&nbsp;</p><p id=""""><br></p><p id="""">This is the first step in the process and the first opportunity for business people and developers to collaborate. The release vision must align with business goals, market conditions, and engineering reality.&nbsp;</p><h3 id=""""><strong id="""">Step 2: Evaluate Your Backlog</strong></h3><p id="""">Now, it's time to look at your product backlog and sort the features by priority. This is where you put your vision to work.&nbsp;</p><p id=""""><br></p><p id="""">Here again, everyone is involved; engineers and stakeholders apply a shared vision to the product backlog and product roadmap to determine what the priorities are for your product.&nbsp;</p><p id=""""><br></p><p id="""">Finally, create a basic release plan that outlines the goal, a target release date, and a complete set of ranked user stories with story points. This is the output you'll need for the next step.&nbsp;</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1000px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1000px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/620c26c8aa4698f6a1f922b5_How%20to%20Make%20an%20Agile%20Release%20Plan%2C%20With%20Examples%20pq02.png"" loading=""lazy"" width=""auto"" height=""auto"" id=""""></div></figure><h3 id=""""><strong id="""">Step 3: Review the Agile Release Plan</strong></h3><p id="""">Next, take your draft release plan, assemble all stakeholders, and hold a release planning session. Developers hate meetings, but this one is essential. As we covered above, Agile places great importance on face-to-face interaction and alignment between the business and engineering. This meeting will ensure that everyone understands and agrees with the plan.</p><p id="""">The meeting agenda should cover the following items.&nbsp;</p><h4 id=""""><strong id="""">Review Roadmap</strong></h4><p id="""">Review the vision and the product roadmap. If it seems like these steps include a lot of repetition and review, that means you're paying attention! This is another opportunity to ensure that the stakeholders are on the same page regarding the product and the release.&nbsp;</p><h4 id=""""><strong id="""">Review Design and Architecture</strong></h4><p id="""">Next, review the technical details and design for the release. Are there dependencies or gaps that can affect the release schedule? How will the plan be adjusted if these issues can't be overcome?&nbsp;</p><h4 id=""""><strong id="""">Review Iteration schedule</strong></h4><p id="""">The draft release plan has user stories, story points, and proposed sprints. This step takes those stories and arranges them into sprints. One method is to arrange the stories into sprints based on <a href=""https://www.agilealliance.org/glossary/velocity/"" id="""">velocity</a>. How much can you reasonably expect to get done in each sprint based on development resources the points assigned to each story?&nbsp;</p><h4 id=""""><strong id="""">Define ""Done""</strong></h4><p id="""">What does ""done"" mean? Do the stakeholders and the developers agree on what the completed release will look and act like? This is where the <a href=""https://www.agilealliance.org/glossary/definition-of-done/"" id="""">Definition of Done</a> needs to be codified by all stakeholders.&nbsp;</p><h4 id=""""><strong id="""">Step 4: Execute and Update</strong></h4><p id="""">Finally, it's time to execute the plan. Agile release plans are living documents, so putting them into action is part of the planning process.&nbsp;</p><p id=""""><br></p><p id="""">A review follows each iteration. What went wrong? What went better than expected? These inputs are critical for planning the next sprint. This step is crucial in keeping your development efforts on track.&nbsp;<br></p><h2 id=""""><strong id="""">Agile Release Plan Examples</strong></h2><p id="""">Let's finish up by looking at how to apply this process to the two most common product release scenarios.&nbsp;</p><h3 id=""""><strong id="""">New Release</strong></h3><p id="""">When you're planning a new release of an existing product, you already have a lot of information. There are few things more valuable than ""we've seen this before"" when planning and adjusting an Agile release plan!&nbsp;</p><p id=""""><br></p><p id="""">There's already a product out there in the world, which means someone had a vision for what it should do. It may not have been formally recorded or discussed, but there's something there. Now is your chance to get everyone together and make sure the current vision is in harmony with market conditions and business needs. Then, you can either update or create your roadmap and your design.&nbsp;</p><p id=""""><br></p><p id="""">The previous releases mean data on current efforts and timelines. You might be new to Agile, but you know how long the last release took and how long it takes to turn around a bug fix. That's all valuable input to the process.&nbsp;</p><p id=""""><br></p><p id="""">Finally, the success or failure of previous releases will be a valuable guide when getting the stakeholders together to define ""Done.""&nbsp;</p><h3 id=""""><strong id="""">New Product</strong></h3><p id="""">When planning the first release for a new product, you have to rely on the team's collective experience combined with data gathered in the testing and prototyping stages. (Of course, those stages should have gone through this process, too?) Maybe you're planning one of those stages and have nothing to go by.&nbsp;</p><p id=""""><br></p><p id="""">It's hard to say that there's a most critical step of the plan, but if it exists, it's the vision, especially when talking about a new product! If the interested parties can't agree on what the new product will be, how can you define the effort required to build it? How can you define ""Done?""&nbsp;</p><p id=""""><br></p><p id="""">Once you've agreed on the vision, you're going to do a lot of estimating and a lot of adjusting. That's okay! The estimating and adjustment are features, not bugs.&nbsp;</p><h2 id="""">Start Your Agile Release Plan</h2><p id="""">This post covered what an Agile release plan is and how to create one. We started with <a href=""https://www.agilealliance.org/agile101/12-principles-behind-the-agile-manifesto/"" id="""">first principles</a> (or at least a subset) and applied them to planning and executing a release. Then we wrapped up with an overview of two common release scenarios.&nbsp;</p><p id=""""><br></p><p id="""">Get started with your next Agile <a href=""https://releasehub.com/blog/awesome-release-docker-compose-examples-working-in-release-and-kubernetes"" id="""">Release</a> now!&nbsp;<br></p>",false,,,,https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/620c2656ed22924bac23629a_How%20to%20Make%20an%20Agile%20Release%20Plan%2C%20With%20Examples.jpg,How to make an agile release plan,eric-goebelbecker,,,,
Automating your day-to-day workflows with the Release CLI,automating-your-day-to-day-workflows-with-the-release-cli,62aa5a70cd5ba27d9d0d718a,642c6a463901bd58831bb29e,Tue Apr 04 2023 18:19:50 GMT+0000 (Coordinated Universal Time),Thu Jul 25 2024 15:13:55 GMT+0000 (Coordinated Universal Time),Thu Jul 25 2024 16:27:39 GMT+0000 (Coordinated Universal Time),Walk you through a hypothetical bug scenario and how Release can make your day-to-day workflows more enjoyable.,"<p id="""">A typical day for a developer is filled with coming up with great ideas for applications, writing flawless code and seeing everything work exactly as expected, right? Well, sometimes. Often, our days are filled with repetitive tasks, and if you’re anything like me, once you’ve done the same task a few times you get an urge to automate it. One of these common repetitive tasks is fixing pesky bugs. Today, I will walk you through a hypothetical bug scenario where we’ll be using Release for ephemeral environments and Release CLI to make your day-to-day workflows more enjoyable. This might give you some time back to focus on that perfect app!&nbsp;</p><p id="""">Before we begin, make sure to set up your version of Release. You can get a free trial <a href=""https://beta.release.com/register"" id="""">here</a>.&nbsp;</p><h3 id="""">The dreaded bug</h3><p id="""">When fixing bugs one of the first steps towards a fix is to replicate them. However, doing that can be very tricky. Our development environments are often pristine and perfect (right?) and bugs rarely happen! Somehow customers (or hopefully QA) find new ways to break your application and it’s our job to try to break it in the same way, so we can fix it for good.</p><p id="""">Sometimes you have good error handling and reporting which gives you all of the context needed for a fix. But if that’s not the case we have to dig deeper to uncover the application state when the error occurred.</p><p id="""">But let’s not get ahead of ourselves here. First things first: we must keep management happy by properly handling our ticket. Sadly this is one of the hardest things to automate because of the sheer diversity of ticketing systems and maturity of these products. Although most of them offer APIs, almost none offer good CLI tools which means you might have to resort to using the dreaded browser/mouse combo.</p><p id="""">Here at Release we use Linear which has an API but unfortunately has no CLI tool ready for us to use. Which means we have a choice: do we keep this step manual or do we automate it? I took the plunge and wrote myself a simple script, because at the end of the day, I’m going to have to deal with these tickets hundreds of times in the future. The few minutes spent automating this will pay off in dividends (and your wrist will thank you too).</p><p id="""">If you’re interested the script is <a href=""https://gist.github.com/Draiken/f23a98de94015b794219d9e64ef5c642"" id="""">available here</a>. Maybe you’re lucky and your ticketing system offers a CLI or you too will have to take the plunge and write something to automate this (you know you want to). Whatever the means, we can use that to tell the world we’re starting our ticket!</p><h3 id="""">Creating our environment</h3><p id="""">Now the first thing we need is an environment to reproduce our bug. Luckily Release offers a feature on top of the ephemeral environments that’s perfect for these situations: <a href=""https://docs.release.com/reference-documentation/instant-datasets-aws"" id="""">instant datasets</a>. We can spin up environments with production-like data within minutes. Depending on how fresh the snapshot we’re using is, the data that caused the bug we need to reproduce might be there already.</p><p id="""">For this we’re going to use the <a href=""https://docs.release.com/cli/getting-started"" id="""">Release CLI</a>. Once that’s setup we’ll login and we’ll be ready to create an environment. Here’s what I use:</p><div data-rt-embed-type='true'><pre>
<code class=""language-yalm line-numbers"" style=""white-space: pre-wrap; word-break: keep-all;"">
release environments create –app my_app –wait | notify-send “Environment ready!”
# or for our friends that run the fruit system
release environments create –app my_app –wait | osascript -e 'display notification ""Environment Ready!""
</code>
</pre></div><p id="""">Once the environment is up and running we get a notification and we can start our investigation.</p><p id="""">Figuring out the state around the bug more often than not requires you to dig through the data either directly on the database or through some sort of application console. Ideally we’d just have a perfect admin page showing what’s what, but that’s not always the case.</p><p id="""">Our Release CLI allows us to very easily dive right into the environment we just spun up and poke around. For this we’re going to use the <strong id="""">instances</strong> command, which lists the running instances and lets us open up a terminal inside those containers. All we need to do is type:</p><div data-rt-embed-type='true'><pre>
	<code class=""language-yalm line-numbers"" style=""white-space: pre-wrap; word-break: keep-all;"">
		release instances terminal
	</code>
</pre></div><p id="""">Select our app/instance and boom: we’re in! If we use something like Rails we can access our console here or alternatively we can connect to the database directly, given this environment is attached to our database and we have the environment variables needed to connect to it.</p><h3 id="""">Fix all the things</h3><p id="""">Now that we’ve investigated and replicated our bug, we can finally fix it! For this we can use another very handy tool from Release: <a href=""https://docs.release.com/cli/remote-dev"" id="""">development environments</a>. Rather than making changes, deploying them to the environment and then checking it, we can simplify all that by running our local code inside the release environment. Let’s start our development environment:</p><div data-rt-embed-type='true'><pre>
	<code class=""language-yalm line-numbers"" style=""white-space: pre-wrap; word-break: keep-all;"">
		release development start –app my_app –environment my_env
	</code>
</pre></div><p id="""">This will insert our local machine’s code into the environment and forward the configured ports right into our localhost. We get the best of both worlds: release is running our code (along all those services it depends on) and we get instant feedback from our changes. We can now fix the issue and close this ticket once and for all.</p><p id="""">With our changes done, we can commit those in and create our pull request. Tools like GitHub and GitLab provide nice CLIs which make it easy to automate this part. At Release we use GitHub and I personally use the <a href=""https://github.com/github/hub"" id="""">hub</a> CLI for this.</p><div data-rt-embed-type='true'><pre>
	<code class=""language-yalm line-numbers"" style=""white-space: pre-wrap; word-break: keep-all;"">
		hub pull-request -o -m “Fix all the things”
	</code>
</pre></div><p id="""">With our pull request open, we can also update our ticket to the appropriate state, if it’s not already done automatically. Now we can ask our teammates to review our code, give us the all important “LGTM” (Let Go The Mayo!) and merge that bad boy in.</p><h3 id="""">Automate it all</h3><p id="""">There’s much more in our daily workflow that we can automate, and hopefully this inspires you to tackle this piece with Release. Once you bite the bullet and make the effort to automate these small tasks you start to realize how much time is wasted outside of our beloved CLI.</p><p id="""">What about you? What else have you automated on your day-to-day?</p>",false,,,,https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/642c7af7e10a523f6b94c936_040423.png,,luiz-felipe,4,Tue Apr 04 2023 19:00:00 GMT+0000 (Coordinated Universal Time),product,
Awesome-release: Tons of Working Release Examples Running in Hosted Kubernetes,awesome-release-docker-compose-examples-working-in-release-and-kubernetes,62aa5a70cd5ba27d9d0d718a,62aa5a70cd5ba2985e0d72d3,Wed Feb 03 2021 05:02:01 GMT+0000 (Coordinated Universal Time),Fri Apr 21 2023 19:57:37 GMT+0000 (Coordinated Universal Time),Fri Apr 21 2023 20:04:43 GMT+0000 (Coordinated Universal Time),"I Need/Want/Love Examples I know that when I try out a new product, if it’s hard to see what it does quickly I usually","<h3 id="""">I Need/Want/Love Examples</h3><p id="""">I know that when I try out a new product, if it’s hard to see what it does quickly I usually move on. This is a shame because I’m sure there are products out there that I could have benefited from but it was just too difficult to get something up and running.</p><p id="""">Release is the easiest way to create environments from any code change, Pull Request or the click of a button. We deploy your environments in a Kubernetes cluster that’s completely managed for you. <em id="""">But in order to see how Release can change how you build software, you have to get your application running in Release.</em> Most of the time this is pretty straightforward, but there is definitely a setup curve to get over before you can see our product in full form.</p><p id="""">The best technical products I’ve used always have a TON of great examples. Stripe is definitely the king of this idea… we were just recently integrating their checkout feature into Release and <a href=""https://github.com/stripe-samples"" target=""_blank"" id="""">their examples repo</a> is a model for how this can be done. It made our integration with Stripe so much easier than digging through docs to figure this all out.</p><p id="""">I think examples are highly under-done, especially with highly technical products. I’m pretty sure I know why (especially after this project): it’s hard to spend the time and energy building out examples when you have features to build. But at Release, examples are a first class citizen and we believe will make getting up and running with Release easier.</p><h3 id="""">awesome-compose is awesome</h3><p id="""">As we started building and testing Release, we came across this fantastic repository, <a href=""https://github.com/docker/awesome-compose"" target=""_blank"" id="""">https://github.com/docker/awesome-compose</a>, that has community created applications known to work with docker-compose.</p><p id="""">Since Release can easily use a docker-compose file to create blueprints for environments, this was a perfect way for us to test out Release and make sure all the various ways docker-compose files are used are supported in the platform. Enter awesome-release.</p><h3 id="""">awesome-release is awesome++</h3><p id="""">We put together an <em id="""">awesome</em> organization in Github with a ton of repositories that just work in Release that are derived from awesome-compose.</p><p id=""""><a href=""https://github.com/awesome-release"" target=""_blank"" id="""">https://github.com/awesome-release</a> is chock full of amazing projects that just work. The great part about all of these projects is that they show the range of how making simple applications to complex applications can be run and environments created within Release.</p><p id="""">So if you’re getting started and need a project that will just work out of the box, I highly recommend forking or copying any of these repos and use it to create an app in Release. Every repository in <em id="""">awesome-release</em> just works.</p><p id="""">We’ve made modifications to some of the awesome-compose repos that needed slight tweaks. In each README we’ve described what we did to make the project work. Most of these things are minor adjustments that enable the project to run in a hosted Kubernetes environment vs being run locally via docker-compose.</p><p id=""""><a href=""https://releasehub.com/"" target=""_blank"" id="""">Click here to signup for Release and give it a shot!</a></p><p id="""">Please let us know if there is an example you’d like to see and we’ll put it together for you. Just send us an email at <a href=""mailto:support@release.com"" id="""">support@release.com</a> and we’ll add it to the list.</p><p id=""""><strong id="""">Have fun and happy Releasing!</strong></p>",false,,,,https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63e421b19cf814badc213817_111820%20(1).jpg,A man driving a boat,tommy-mcclung,2,Thu Nov 19 2020 00:00:00 GMT+0000 (Coordinated Universal Time),,
AWS re:Invent 2023 - What to Expect,aws-re-invent-2023-what-to-expect,62aa5a70cd5ba27d9d0d718a,655523a3adbaf4378db3ec0a,Wed Nov 15 2023 20:01:39 GMT+0000 (Coordinated Universal Time),Thu Apr 25 2024 20:27:35 GMT+0000 (Coordinated Universal Time),Thu May 02 2024 20:19:24 GMT+0000 (Coordinated Universal Time),Join Release at AWS re:Invent in Las Vegas ,"<p id="""">Release is heading to AWS re:Invent where we will showcase our latest product developments, share previews of upcoming releases, and network with fellow cloud enthusiasts. Come see us at Booth #503 of the Expo Hall for a custom demo and some Release swag. </p><p>As we gear up for <a href=""https://reinvent.awsevents.com/"" id="""">AWS re:Invent 2023</a> in Las Vegas (Nov 27 - Dec 1), here's a concise guide to help you navigate and make the most out of this expansive cloud computing event.</p><p>‍<strong id="""">Why Go:</strong></p><ul id=""""><li><strong id="""">Networking Galore:</strong> With over 60,000 attendees, it's a melting pot of cloud enthusiasts, including developers, system administrators, engineers, and IT execs.</li><li><strong id="""">Learning Opportunities: </strong>Expect over 2,000 sessions covering a range of topics from AWS innovations to practical DevOps strategies.</li><li><strong id="""">Hands-On Experience:</strong> Interactive sessions like Builders’ Sessions and Workshops are perfect for us to get our hands dirty with real-world AWS applications.</li><li>‍<strong id="""">Keynote Insights</strong>: Hear from AWS leaders like CEO Adam Selipsky and CTO Dr. Werner Vogels for the latest AWS developments and future directions. See the full list of keynotes <a href=""https://reinvent.awsevents.com/keynotes/"" id="""">here</a>.</li></ul>",true,"<p id="""">Take Release for a 30-day test drive <br>with code #CONF23</p>",https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=reaws,"<p id=""""><strong id="""">What to Expect:</strong></p><ul id=""""><li><strong id="""">Diverse Sessions: </strong>Tailored for various expertise levels, these sessions cover everything from AWS basics to advanced topics in various technologies. Use the <a href=""https://hub.reinvent.awsevents.com/attendee-portal/catalog/"" id="""">Sessions Catalog</a> to plan your agenda. The handy filter on the left side sorts by topic, complexity, and even your role. </li><li><strong id="""">Expo Hall: </strong>A great place to see live demos and discuss with AWS partners about their latest solutions. Find Release at Booth #503 of the <a href=""https://reinvent.awsevents.com/learn/expo"" id="""">Expo</a>. </li><li><strong id="""">After-Hours Fun:</strong> Don’t miss the <a href=""https://reinvent.awsevents.com/community/replay"" id="""">re:Play</a> party for some downtime. Las Vegas itself offers a plethora of entertainment options to unwind.</li></ul><p>‍<strong id="""">Practical Tips:</strong></p><ul id=""""><li><strong id="""">Early Arrival:</strong> Consider arriving a day early to settle in and maybe explore Vegas.</li><li><strong id="""">Hotel Choices: </strong>Take advantage of special rates for attendees. Options range from the luxury of Encore to the unique ambiance of The Venetian.</li><li><strong id="""">Getting Around:</strong> Free campus shuttles and monorail services are available for easy movement between venues.</li></ul><p>‍<strong id="""">Maximizing Your Experience:</strong></p><ul id=""""><li><strong id="""">Set Clear Goals:</strong> The event can be overwhelming, so deciding what you want to achieve up-front can help. Whether it's learning, networking, or exploring business opportunities, set a few goals and set some time aside for exploration.</li><li><strong id="""">Leverage Event Intelligence:</strong> Use tools (e.g. AWS events app) to get insights on attendees and sessions. Many things will be happening at the same time, so jotting down a custom agenda will help you navigate the event. </li><li><strong id="""">Post-Event Follow-Up:</strong> Reflect on your interactions and plan follow-ups to maintain the momentum. </li></ul><p>‍<strong id="""">Final Thoughts:</strong></p><p id="""">AWS re:Invent 2023 isn't just another tech conference; it's one of the largest outlets for anyone in the cloud computing and DevOps space to engage. It's an opportunity to stay ahead in the industry, learn from the best, and make connections that matter. </p><p id="""">See you in Vegas!</p><p></p>",https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/65552275d7be9a32f49f67be_AWS%20re_Invent.jpg,,ira-casteel,3,Wed Nov 15 2023 23:00:00 GMT+0000 (Coordinated Universal Time),events,
Beyond K8s: Introduction to Ephemeral Environments,beyond-k8s-introduction-to-ephemeral-environments,62aa5a70cd5ba27d9d0d718a,62aa5a70cd5ba2709d0d7304,Thu Feb 10 2022 19:55:34 GMT+0000 (Coordinated Universal Time),Wed Oct 30 2024 19:46:21 GMT+0000 (Coordinated Universal Time),Wed Oct 30 2024 19:46:21 GMT+0000 (Coordinated Universal Time),Everything you need to know about Ephemeral Environments,,true,<p>Experience true production-like testing with Release's ephemeral environments.</p>,https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=intro-ephemeral-envs,"<p id="""">Replicating the production environment to pre-production is the key to higher quality code and more frequent releases. Kubernetes is great, but with today’s complex application process, production environments are not just about the application itself. They are about cloud native services such as lambda, databases such as RDS, name servers, and more.<br>In this webinar, we discuss how to set up your environments that are as close to production as possible and will explain how to do that so that your environment is embedded with your development process, available on-the-fly for developers, all while removing the bottlenecks associated with a single staging environment.<br>In this webinar, you will learn:</p><p id="""">- When to use shared environments and when to use ephemeral environments<br>- How to set up the right data in the right environment<br>- About shifting left – bringing production environments to the developer branch</p><p id=""""><a href=""https://www.bigmarker.com/techwell-corporation/Beyond-K8s-Introduction-to-Ephemeral-Environments"" id="""">Watch the webinar on demand.</a></p><p>‍</p>",https://cdn.prod.website-files.com/603dd147c5b0a4221d1bd360/6205ad441d35015633adb7af_Beyond%20K8s.jpg,Beyond K8s: Introduction to Ephemeral Environments,tommy-mcclung,1,Thu Feb 10 2022 21:00:00 GMT+0000 (Coordinated Universal Time),,
Boost Developer Happiness with Ephemeral Environments,boost-developer-happiness-with-ephemeral-environments,62aa5a70cd5ba27d9d0d718a,6722b088f6eee65111716481,Wed Oct 30 2024 22:17:44 GMT+0000 (Coordinated Universal Time),Wed Oct 30 2024 22:38:04 GMT+0000 (Coordinated Universal Time),Thu Oct 31 2024 21:55:39 GMT+0000 (Coordinated Universal Time),"Ephemeral environments boost developer happiness by enabling quick, autonomous, and consistent setups delighting devs.",,true,"<p id="""">Explore how Release can boost developer happiness by setting up ephemeral environments for seamless development. </p><p id="""">‍</p>",https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=boost-dev-happiness,"<h2 id="""">Boosting Developer Happiness with Ephemeral Environments: A Game Changer for Developer Experience Teams</h2><p id="""">In today's fast-paced development landscape, keeping developers happy isn't just a nicety—it’s a necessity. Happy developers are more productive, write better code, and contribute to a positive team culture. Developer Experience (DevEx) teams are constantly seeking tools and practices that enhance satisfaction and streamline workflows. One such innovation that's making waves is the use of <strong>ephemeral environments</strong>. But what are they, and how can they align with your team's charter to amplify developer happiness? Let’s dive in.</p><h3 id="""">What Are Ephemeral Environments?</h3><p id="""">Ephemeral environments are temporary, on-demand computing spaces that developers can spin up and tear down effortlessly. Think of them as isolated sandboxes that replicate production environments without the overhead of long-term maintenance. They are typically used for testing, feature development, and experimentation, providing a unique blend of flexibility, consistency, and speed.</p><h3 id="""">Why Ephemeral Environments Matter for Developer Happiness</h3><h4 id="""">Instant Gratification with On-Demand Resources</h4><p id="""">Waiting is the enemy of productivity. Ephemeral environments allow developers to get immediate access to the resources they need. No more waiting for shared environments to be available or for lengthy setup processes. This instant access reduces frustration and keeps momentum going.</p><h4 id="""">Reduced ""It Works on My Machine"" Syndrome</h4><p id="""">By providing consistent, isolated environments, ephemeral setups minimize discrepancies between development and production. This consistency means fewer bugs slip through the cracks and less time is spent debugging environment-specific issues.</p><h4 id="""">Empowerment Through Autonomy</h4><p id="""">Developers can spin up their own environments without needing to coordinate with other teams. This autonomy not only speeds up the development process but also gives developers a sense of ownership over their work.</p><h4 id="""">Safe Space for Experimentation</h4><p id="""">Ephemeral environments are perfect for testing new ideas without the risk of affecting the main codebase or shared resources. This safety net encourages innovation and creative problem-solving.</p><h3 id="""">Real-World Success Story: DebtBook’s Transformation with Ephemeral Environments</h3><p id="""">DebtBook, a leading provider of debt and lease management software for public sector organizations, faced challenges with its development and testing environments. With complex systems and dependencies, DebtBook’s team often struggled with environment setup and maintenance, causing delays and impacting productivity.</p><p id="""">By adopting <strong id="""">Release's</strong> ephemeral environments, DebtBook streamlined its workflow and enabled developers to quickly spin up isolated environments tailored for specific features or testing needs. This shift allowed developers to deploy changes to isolated environments at any stage, ensuring consistent, reliable, and reproducible setups that closely mirrored production. The results were significant:</p><p id="""">- <strong id="""">Reduced Time to Environment Setup</strong>: Developers could now focus on coding rather than waiting for environment setup, accelerating productivity.</p><p id="""">‍<br>- <strong id="""">Improved Collaboration</strong>: With consistent environments available to all team members, communication and collaboration became seamless, reducing misalignment caused by environment inconsistencies.</p><p id="""">‍<br>- <strong id="""">Faster Releases</strong>: By simplifying testing and reducing setup times, DebtBook was able to reduce its overall release cycle and deliver value to clients faster.</p><p id="""">This real-world example illustrates how ephemeral environments can be transformative, providing measurable improvements in efficiency and satisfaction. <a href=""https://release.com/casestudy/debtbook"" id="""">Read the DebtBook case study.</a></p><h3 id="""">Getting Started with Ephemeral Environments</h3><p id="""">Getting started with ephemeral environments can be simplified with tools like Release, which provides comprehensive documentation and guidelines. Here’s a quick guide to help DevEx teams kickstart ephemeral environments based on <a href=""https://docs.release.com/"" id="""">Release’s documentation</a>:</p><h4 id="""">1. Set Up the Environment Template</h4><p id="""">Start by defining templates for your environments. These templates can include configurations, services, and dependencies. This setup ensures consistency across all environments and saves time when developers need to quickly spin up a new one.</p><h4 id="""">2. Integrate with Your CI/CD Pipeline</h4><p id="""">Integrate your ephemeral environments with your existing CI/CD pipelines. This integration enables automated provisioning and teardown of environments for each feature branch or pull request, so developers can test their code in isolated, production-like setups on demand.</p><h4 id="""">3. Automate Environment Teardowns</h4><p id="""">To prevent environment sprawl and control costs, set up automated teardown rules. Ephemeral environments are designed to be short-lived, so automating the teardown ensures that no environment outlasts its usefulness and resources are optimally utilized.</p><h4 id="""">4. Monitor and Gather Feedback</h4><p id="""">Use monitoring tools and gather feedback from developers to understand how these environments are impacting their workflow. Insights from monitoring and direct feedback can help refine your approach and make adjustments that further enhance developer experience.</p><h4 id="""">5. Educate Your Team</h4><p id="""">Provide training sessions or documentation to help your developers make the most of ephemeral environments. The more comfortable they are with the tools, the more benefits you'll reap.</p><h3 id="""">Conclusion</h3><p id="""">Ephemeral environments are more than just a trendy buzzword—they’re a powerful tool that can significantly enhance developer happiness and productivity. For Developer Experience teams committed to improving workflows and satisfaction, embracing ephemeral environments aligns perfectly with your charter. By reducing friction, empowering developers, and promoting best practices, you’re not just making your team’s life easier; you’re driving better business outcomes.</p><p id=""""><strong id="""">So, are you ready to make your developers happier than ever? Embrace ephemeral environments and watch your team's productivity soar! </strong><a href=""https://release.com/signup"" id=""""><strong id="""">Signup for Release for free.</strong></a></p><p id="""">‍</p>",https://cdn.prod.website-files.com/603dd147c5b0a4221d1bd360/6722b0336eaed513f4d41048_happy-developers.webp,Happy Developers using Ephemeral Environments,tommy-mcclung,5,Wed Oct 30 2024 22:15:00 GMT+0000 (Coordinated Universal Time),,how-debtbook-ships-6x-faster-with-release; ephemeral-environments-9-tips-for-seamless-deployment; 6-software-development-environment-best-practices
Build vs. Buy: Where to Focus Your Energy with IDPs,build-vs-buy-where-to-focus-your-energy-with-idps,62aa5a70cd5ba27d9d0d718a,6500a36f77bea25e02390ad4,Tue Sep 12 2023 17:44:15 GMT+0000 (Coordinated Universal Time),Thu Jul 25 2024 15:05:00 GMT+0000 (Coordinated Universal Time),Thu Jul 25 2024 16:27:39 GMT+0000 (Coordinated Universal Time),Let’s explore a significant decision in every software endeavor: do we build or do we buy?,"<p id="""">Previously, we wrote about the goals and outcomes of <a href=""https://release.com/blog/what-is-an-internal-developer-platform-and-why-should-i-have-one"" id="""">an Internal Developer Platform (IDP</a>), as well as the <a href=""https://release.com/blog/components-of-a-successful-idp-build-a-product-your-developers-actually-want-to-use"" id="""">components of a successful IDP</a>. At this point, you may be either excited by or overwhelmed with where your platform could go and the efficiencies that it could create.</p><p id="""">As the next step, let’s explore a significant decision in every software endeavor: do we build or do we buy?</p><p id="""">Considering this is rarely a clean “yes or no” question, we’re also going to look at the options that sit between the two. In actuality, the solution you choose will be somewhere on a large spectrum between a complete build from scratch and entirely buying software that you never have to maintain yourself. Where your org falls on that spectrum depends on your current needs and capabilities.</p><p id="""">First, let’s look at some of the options available.<em id=""""></em></p>",true,<p>Ready to try Release?<br>Use code #IDP to get 30 days free.</p>,https://release.com/signup,"<h4 id="""">Build or Buy, or Else?</h4><p id="""">When we make decisions around build vs. buy, we should first realize that when it comes to an IDP, it’s not all or nothing. This is a spectrum, where you can buy a managed IDP and all related components or you can pick and choose what you build and what you buy. As a reminder, an IDP isn’t one solution, but a platform with various components that provide solutions to different problems. Therefore, you can choose to build or buy the platform and the separate components.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1000px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1000px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/65008cf4824e9f9cb24dbc05_jkl6g0bDWEUvpq4FGnZYb7UdnE4yq5FQK6z5i-SeBeDbuMJD5uJjPA54m_uCEtIiuXHqHhv3fBtTwkDxXIgw6MZlJDP76sz2IBXVnNNJAPyT51VKyoTf2fLH-3yGMx9nLP0vZBRvY4mOagj0SgTViXk.png"" id="""" width=""auto"" height=""auto"" alt="""" loading=""auto""></div></figure><p id="""">‍<strong id="""">🛠️ Fully Build the IDP and Integrations</strong></p><p id="""">The most involved option would be to build everything yourself from scratch. This will be the most expensive option and will require a large team, but in theory, it will give you the most flexibility.</p><p id="""">Spotify did this when they built Backstage. Now, you may think that that’s the way to go. But do realize that Spotify had a large number of people working on this both internally and later externally when the platform was released as open source. Once they went open source, they had both individual contributors and partner companies that provided time and engineering efforts toward the build. Their<a href=""https://backstage.io/blog/2022/03/16/backstage-turns-two/"" id=""""> two-year anniversary called out 5,000 contributors</a> to the project.</p><p id="""">Most companies do not have hundreds, let alone thousands, of developers who can put in time and effort to building an IDP from scratch. In addition to the large number of resources, that level of contribution requires forgoing other projects within your organization that involve shipping features. As importantly, this platform is just that. A platform. Once you have the platform built, you will then need to configure the necessary components and add-ons that your organization needs. Each of these components will result in a build vs. buy decision, so focus on the most important features for your organization first.</p><h5 id="""">‍<strong id="""">🏗️ Build Atop Open-Source Frameworks and Commercial Components</strong></h5><p id="""">If you’re not at the stage where you can build everything (and most organizations aren’t), another option would be to take an open-source framework and build on top of that.</p><p id="""">In this scenario, you’re using something like backstage.io but building out the implementation. </p><p id="""">This is a better option over a complete build as the base framework. However, you’ll still need to build integrations and quickly form opinions on how this should work for your organization.</p><p id="""">Many prefer this to the build option, but it will still take a considerable team to make it happen. And it’s not the ideal choice if you’re just starting to learn about IDPs and how they can help your org.</p><p id="""">For example, companies could choose a managed observability component like Datadog or New Relic. These tools aren’t cheap but will get your organization started with solid monitoring and observability. However, as costs increase, you could find yourself building your observability tools over open-source libraries like Prometheus and Grafana.</p><p id="""">Coming at this from a similar angle, you could handle your environment management in-house using Ansible, Terraform, or Docker Compose. You might begin by building out your environment management using these tools as the solution seems simple. As the complexity of your environment management increases, and as the problems you need to solve become more sophisticated, choosing an ephemeral environment solution like Release can reduce the complexity for your teams.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1000px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1000px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/65008cf48ff0af7d98cbcdae_h5OYdEvOgxY3fAU70Bm96rICWGnTnLjl3pNt73hrRgRH2PSwplHw1EXMpbTMXfH0Iy_J5c4kLt_2zEUvXNLP4Y_5XmfjmdZWacSsktmZw_KIHjB1Z7OHDvltxaFChxNGdyhTfSurfw9gswOJ1IgUoH0.png"" id="""" width=""auto"" height=""auto"" alt="""" loading=""auto""></div></figure><h5 id=""""><strong id="""">🤝Hire Someone to Build It</strong></h5><p id="""">For some software projects, you could consider hiring out the build. For example, you could hire third-party contractors or consultants to build your IDP. Or you could directly hire new employees with the necessary experience and onboard them into your organization to build it out.</p><p id="""">Organizations will take this option if they don’t have the necessary knowledge in-house. Though this can work, it often has poor outcomes. The folks coming into the org don’t know the culture and processes of your teams and will need to learn them, or they’ll build something not based on your internal culture and processes. Frequently, for third-party development teams, they don’t have the level of ownership needed for projects of this size. And hiring new employees for this work will also increase the burden on hiring resources. And if we don’t have existing employees with this expertise, we will oftentimes make expensive hiring decisions.</p><p id="""">To mitigate some of the risks with these options, you could have a small number of experts build alongside your team so that you have more of your own long-term workforce working on the project than you have temporary parties. That would be the only way I’d consider this option.</p><h5 id="""">‍<strong id="""">💰Buy It: Vendor-Managed Options</strong></h5><p id="""">Buying enterprise software isn’t a plug-it-in-and-forget-it endeavor. There will still be work involved in configuration, management, and onboarding teams. This is even more true for IDPs, as they are simply platforms and will need the right components and integrations added on to make a usable product.</p><p id="""">Even though there’s not a complete buy-it option, buying the right components that provide value to your org will reduce the development burden on your teams. You’ll be able to focus on the integrations that solve your organization’s biggest development workflow pain points, which can free you up to decide how you want to incorporate the use of the IDP into your organizational culture.</p><p id="""">Additionally, for an IDP, you have managed solutions available for a number of components. For example, there are organizations that provide managed IDP solutions like <a href=""https://www.cortex.io/"" id="""">Cortex</a> or <a href=""https://www.opslevel.com/"" id="""">OpsLevel</a> for service catalogs and integrations to other tools.</p><p id="""">You can also consider whether it makes sense to buy components and capabilities that extend your IDP. Let’s consider the monitoring and observability components of your IDP. We already mentioned purchasing products like Datadog or New Relic. You can further this by adding paging options like <a href=""https://www.pagerduty.com/"" id="""">PagerDuty</a> or <a href=""https://www.atlassian.com/software/opsgenie"" id="""">Opsgenie</a>. These problems have robust solutions on the market that can fill your organization’s needs.</p><p id="""">When it comes to environment management, Release provides a managed platform as a service (PAAS) solution for managing both standard and ephemeral environments. This provides additional building blocks for your IDP, adding environment management capabilities to further increase your development team efficiency. And yes, you could build it yourself, scripting atop open-source solutions like <a href=""https://www.terraform.io/"" id="""">Terraform</a> or <a href=""https://docs.docker.com/compose/"" id="""">Docker Compose</a>. But again, these types of products are not something that will take a few days and still scale well.</p><p id="""">When considering what components to build or buy, consider how the value compares to the effort. What components can provide big benefits to your engineering teams that will take a lot of time and effort to build? And consider your most pressing needs. Do you need a central place to locate tools and services? How about robust monitoring tools to operate your systems? Or environment management to increase developer velocity? And which do you want to benefit from quickly?</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1000px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1000px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/65008cf4c6f0a6091603b5e8_zlfj-HbCvSHT4dpzP9VtND1UNfW0v1cvMQfD319E9gkLIGhubltz5Jv4PZql8X1oN85tZ1-uYXlxoBiFrvlC4xp_M7cUN3vRapNwm5sC_P2eTBu86Z5fkET9x1Mddu-giE-kPMVBfiDV25Kl6bD7xtM.png"" id="""" width=""auto"" height=""auto"" alt="""" loading=""auto""></div></figure><h4 id="""">Build vs. Buy Factors to Consider</h4><p id="""">We’ve gone over many alternatives in the build, build on open source, hire out, and buy spectrum. There isn’t a one-size-fits-all solution, and you’ll have to consider what’s best for your organization.</p><p id="""">But there are a number of things to consider when deciding if you will build or buy your IDP.</p><h5 id="""">1. Cost</h5><p id="""">Cost isn’t just the amount of money that you’ll spend on the software licenses or the developer headcount to build the IDP. If you buy software, you’ll still have to spend time on training, evangelizing, and configuring the IDP for your organization. If you build, you’ll need to include engineering time as well as the training, evangelizing, and configuring just like when you buy. Building also includes opportunity costs. What could your engineering staff be building to further differentiate your product in the market? Should you focus their energies on your product or on integrations that all organizations need?</p><h5 id="""">2. Expertise and Capabilities</h5><p id="""">Building software like an IDP requires a team with the right skills and expertise. And don’t think your team of five production engineers can take on this effort. Organizations like Netflix, Facebook, and Spotify can build tools like this in-house because they have hundreds, if not more, of employees working on internal systems and efficiencies. These folks are dedicated to improving the developer experience.</p><p id="""">If you have a small development team focused on application or product development, developing a <a href=""https://engineering.fb.com/category/production-engineering/"" id="""">production engineering</a> or platform enablement team will not work well. It won’t take advantage of their expertise in your domain and will take them away from building the product that is your market differentiator.</p><h5 id="""">3. Scalability</h5><p id="""">With the build option, you do control the ability to scale the needs of the system yourself to your specific loads and use cases. However, consider whether you could start with buying a managed IDP option and then move over to build after you’ve outgrown the capabilities and scale of that option.</p><h5 id="""">4. Custom Integrations</h5><p id="""">If your organization has its own custom-built tools for testing, security, governance, or more, integrations through an IDP may be more difficult. Or you’ll potentially have the same amount of effort between the build and the buy option.</p><h5 id="""">5. Security and Compliance</h5><p id="""">When you build the software, you can control the security and compliance features. But that also means you must be the experts in security and compliance. This loops back to #2. Do you have the expertise, or do you need to offload that need for expertise to another organization where it’s their product differentiator?</p><h5 id="""">6. Competitive Advantage</h5><p id="""">If you’re an org like Spotify, Netflix, Meta, or similar, then squeezing every bit of efficiency out of your development team by ensuring everything is custom-built for their needs makes sense. But if you’re not at that scale, consider starting smaller.</p><h5 id="""">7. Usability and Adoption</h5><p id="""">If you build a custom IDP but don’t consider usability, then you will have poor adoption. This means not only UI concerns of the IDP but also usability of configurations and interactions with the developer workflow.</p><h5 id="""">8. Support</h5><p id="""">Custom-built solutions won’t have an internet full of adopters that use the tool or similar tools for debugging, tutorials, and instructions. Buying software solutions through a vendor-managed option means you don’t have to create a large support network and can offload much of that to the wider developer ecosystem.</p><h5 id="""">9. Course Correction</h5><p id="""">One final factor involves transitioning from one decision to another. Consider this. Would it be easier and more economical to move from a “buy” solution that doesn’t meet your needs or from a “build” solution that fails? If you decide to build but find that this is not maintainable or salable, then the effort put into building will be wasted.</p><p id="""">If you find that a buy solution doesn’t work for your needs, yes, you’ll still have sunk costs, but they should be more manageable. Depending on the components involved and your expertise, your answer can vary.</p><p id="""">However, if you start with a buy solution or use a PaaS or managed solution, you can transition to build when you’re ready. You’ll build up the experience needed and the understanding of what you need from the product. For something like an IDP that consists of many integrated components, you can build piece by piece, taking on more of the build as your organization’s expertise and understanding of the component grows.</p><h4 id="""">Making a Decision</h4><p id="""">Making the right decision depends on many factors that vary across organizations. Consider what’s best for your situation today and focus on solving for the 80% before deciding to move to a build solution. This may consist of a combination of building, building on top of open source, buying, or hiring out.</p><p id="""">In the next installment of this series, we’ll talk about using product thinking to establish your IDP. Stay tuned!</p><p id="""">‍<em id="""">This post was written by Sylvia Fronczak. </em><a href=""https://sylviafronczak.com/"" id=""""><em id="""">Sylvia</em></a><em id=""""> is a software developer who has worked in various industries with various software methodologies. She’s currently focused on design practices that the whole team can own, understand, and evolve over time.</em></p>",https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/6500a31129b89ac202977d7c_Buid%20vs%20Buy.jpg,Credit: Google DeepMind,sylvia-fronczak,10,Tue Sep 12 2023 21:00:00 GMT+0000 (Coordinated Universal Time),platform-engineering,components-of-a-successful-idp-build-a-product-your-developers-actually-want-to-use
Cache Bundle Install with BuildKit,cache-bundle-install-with-buildkit,62aa5a70cd5ba27d9d0d718a,6450278dd986712ed591c882,Mon May 01 2023 20:56:45 GMT+0000 (Coordinated Universal Time),Mon May 22 2023 21:09:41 GMT+0000 (Coordinated Universal Time),Mon May 22 2023 21:15:30 GMT+0000 (Coordinated Universal Time),,"<p id="""">At Release, we’ve been using BuildKit to do our own builds for some time now and BuildKit does an awesome job of caching Docker image layers! But one thing that continued to slow down our builds was running &lt;code inline&gt;bundle install&lt;/code&gt; when upgrading gems in our Rails application. I decided to start researching if there was any way that we could <strong id="""">cache our &lt;code inline&gt;bundle install&lt;/code&gt; commands over many builds</strong> because a cold &lt;code inline&gt;bundle install&lt;/code&gt; is very slow, but<strong id=""""> incremental changes are quite fast</strong>. In my search, I came across a KubeCon video about BuildKit, <a href=""https://www.youtube.com/watch?v=wTENRhYt3mw"" target=""_blank"" id="""">“Running Cache-Efficient Builds at Scale on Kubernetes with BuildKit - Gautier Delorme, Apple Inc.”</a> which had a very interesting slide.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1170px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1170px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/6450272dca0c9c40411d087b_87104ee9.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><p id="""">This is exactly the type of solution I was looking for and it comes built in with BuildKit!</p><p id="""">Let’s take a look at an example from the <a href=""https://docs.docker.com/engine/reference/builder/#run---mounttypecache"" id="""">mount cache documentation</a> to start.</p><p id="""">Example: cache Go packages</p><div data-rt-embed-type='true'><pre>
<code class=""language-ruby line-numbers"">
# syntax=docker/dockerfile:1
FROM golang
RUN --mount=type=cache,target=/root/.cache/go-build \
  go build ...
</code>
</pre></div><p id="""">In this example, the &lt;code inline&gt;go build&lt;/code&gt; command uses the &lt;code inline&gt;/root/.cache/go-build&lt;/code&gt; directory to store the packages in between builds. Because the output of &lt;code inline&gt;go build&lt;/code&gt; is a binary and does not require anything to run besides that binary, this example makes correct use of the cache. If we think of the cache directory as a named volume from the host server into the container we can create a picture of how this example works. When &lt;code inline&gt;go build&lt;/code&gt; is run, the cache directory is populated on the host server and the resulting binary ends up in the container. The problem is that this mount cache functionality wasn’t built with the idea that the packages in the cache needed to be pulled into the resulting image. 	</p><p id="""">To solve this problem I continued my search and came across this issue on the BuildKit repository, “<a href=""https://github.com/moby/buildkit/issues/1173"" id="""">Am I misunderstanding RUN mount=type=cache?</a>”. The writer of the issue explains how this functionality isn’t working with &lt;code inline&gt;bundle install&lt;/code&gt; and is trying to figure out what to do. In one of the answers a link to blog post in Japanese is provided, “<a href=""https://qiita.com/k_kind/items/836bc7ba2e33dc2ed3e7"" id="""">Dockerfile for Rails6のベストプラクティスを解説</a>”. A Dockerfile is provided in the post, which has the solution we’ve been searching for.</p><div data-rt-embed-type='true'><pre>
<code class=""language-ruby line-numbers"">
WORKDIR /app
…
RUN bundle config set app_config .bundle
RUN bundle config set path .cache/bundle
# mount cacheを利用する
RUN --mount=type=cache,uid=1000,target=/app/.cache/bundle \
    bundle install && \
    mkdir -p vendor && \
    cp -ar .cache/bundle vendor/bundle
RUN bundle config set path vendor/bundle
</code>
</pre></div><p id="""">Now that we know what the solution is, let’s go through it line by line to make sure we fully understand what is happening.</p><div data-rt-embed-type='true'><pre>
<code class=""language-ruby line-numbers"">
WORKDIR /app
</code>
</pre></div><p id="""">First, we set the working directory for the Dockerfile to app</p><div data-rt-embed-type='true'><pre>
<code class=""language-ruby line-numbers"">
RUN bundle config set app_config .bundle
</code>
</pre></div><p id="""">Next, we set Bundler’s config to the &lt;code inline&gt;.bundle&lt;/code&gt; directory.</p><div data-rt-embed-type='true'><pre>
<code class=""language-ruby line-numbers"">
RUN bundle config set path .cache/bundle
</code>
</pre></div><p id="""">Then, we set Bundler’s path &lt;code inline&gt;.cache/bundle&lt;/code&gt; which is the directory the gems will be installed into.</p><div data-rt-embed-type='true'><pre>
<code class=""language-ruby line-numbers"">
RUN --mount=type=cache,uid=1000,target=/app/.cache/bundle \
    bundle install && \
    mkdir -p vendor && \
    cp -ar .cache/bundle vendor/bundle
</code>
</pre></div><p id="""">Now the important part! We use the &lt;code inline&gt;--mount=type=cache&lt;/code&gt; and set the cache to be the same location as Bundler’s path. But a key here is to include the &lt;code inline&gt;WORKDIR&lt;/code&gt; path so it becomes &lt;code inline&gt;target=/app/.cache/bundler&lt;/code&gt;. This means our directory of installed gems will be persisted from build to build. We run &lt;code inline&gt;bundle install&lt;/code&gt; to install the gems and then create a vendor directory. The last step here is to copy &lt;code inline&gt;.cache/bundle&lt;/code&gt; into &lt;code inline&gt;vendor/bundle&lt;/code&gt; because, if you recall from the Go example, the contents of the cache are not included in the layer.</p><div data-rt-embed-type='true'><pre>
<code class=""language-ruby line-numbers"">
RUN bundle config set path vendor/bundle
</code>
</pre></div><p id="""">Finally we set Bundler’s path to the directory we copied the files into and we’re good to go.</p><p id="""">And now we fully understand what is happening! To wrap up, there are a few final points to cover. The first is that the code above is not quite ideal. It works, but it is missing a few options to add some safety and reliability. There will be a full example shown below.</p><p id="""">The mount cache accepts an &lt;code inline&gt;id&lt;/code&gt; as a parameter. The <a href=""https://docs.docker.com/engine/reference/builder/#run---mounttypecache"" id="""">documentation</a> says:</p><blockquote id="""">Optional ID to identify separate/different caches. Defaults to the value of the target.</blockquote><p id="""">Setting an ID is valuable if there are potentially lots of Dockerfiles running on the same BuildKit server who might be attempting to use the same cache location; imagine if two different Rails projects started sharing the same directory!&nbsp;</p><p id="""">Which leads us to the second parameter of &lt;code inline&gt;sharing&lt;/code&gt;. The <a href=""https://docs.docker.com/engine/reference/builder/#run---mounttypecache"" id="""">documentation</a> says:&nbsp;</p><blockquote id="""">One of shared, private, or locked. Defaults to shared. A shared cache mount can be used concurrently by multiple writers. private creates a new mount if there are multiple writers. locked pauses the second writer until the first one releases the mount.</blockquote><p id="""">We want to opt for &lt;code inline&gt;sharing=locked&lt;/code&gt; meaning that if two builds of the same Dockerfile are running at the same time, only one can access the cache at a time. This ensures that the output of &lt;code inline&gt;bundle install&lt;/code&gt; won’t be mangled when the &lt;code inline&gt;cp&lt;/code&gt; command is issued.</p><p id="""">This is our suggestion for a full solution.</p><div data-rt-embed-type='true'><pre>
<code class=""language-ruby line-numbers"">
WORKDIR /app/
RUN gem install Bundler
RUN bundle config set app_config .bundle
RUN bundle config set path .cache/bundle
COPY Gemfile Gemfile.lock ./
RUN --mount=type=cache,id=-gem-cache,sharing=locked,target=/app/.cache/bundle \
bundle install && \
  mkdir -p vendor && \
  cp -ar .cache/bundle vendor/bundle
RUN bundle config set path vendor/bundle
</code>
</pre></div><p id="""">If you would like to read more about how the caching works, there is an issue on the BuildKit repository, “<a href=""https://github.com/moby/buildkit/issues/1673"" id="""">mount=type=cache more in-depth explanation?</a>” that has a great discussion on how this functionality actually works.</p><p id="""">And if you would like to apply this caching mechanism to your build process, sign-up for Release and take advantage of our BuildKit servers!</p><div data-rt-embed-type='true'><style>
	blockquote {
		font-size: 1.25rem;
	}

@media screen and (max-width: 479px) {
	blockquote {
		font-size: 1.0rem;
	}
}
</style></div>",false,,,,https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/64503dd4de7ae75387d8f75d_Cache%20Bundle%20Install%20with%20BuildKit%20(1).png,,jeremy-kreutzbender,4,Wed May 03 2023 20:56:00 GMT+0000 (Coordinated Universal Time),,
Code to Cloud simplified: how Release uses AI to make deployment easier  ,code-to-cloud-simplified-how-release-uses-ai-to-make-deployment-easier,62aa5a70cd5ba27d9d0d718a,64417f29ab01a10bf71a8b71,Thu Apr 20 2023 18:06:33 GMT+0000 (Coordinated Universal Time),Thu Jul 25 2024 15:10:56 GMT+0000 (Coordinated Universal Time),Thu Jul 25 2024 16:27:39 GMT+0000 (Coordinated Universal Time),"Getting your code into the cloud can be a challenge, especially when there is no Dockerfile to help with configuration.","<p id="""">Getting your code into the cloud can be a challenge, especially when there is no Dockerfile to help with configuration. With our latest AI project, Release is here to solve this problem once and for all!</p><p id="""">While the cloud brought some parity between big companies’ infrastructure and the rest of us, it also created unnecessary complexity. Release was founded to make it easier for companies to virtualize their environments and utilize the cloud to its full potential, all while being as easy as Heroku.</p><p id="""">At Release, we tackled the problem of organizing and configuring applications with a combination of software and people, making it easy to transition from code to cloud. Our newest AI project makes it even easier for our customers to get their modern apps, utilizing <a href=""https://docs.release.com/guides-and-examples/advanced-guides/kubernetes"" target=""_blank"" id="""">Kubernetes</a>, <a href=""https://docs.release.com/guides-and-examples/advanced-guides/infrastructure/terraform"" target=""_blank"" id="""">Terraform</a>, <a href=""https://docs.release.com/reference-documentation/helm"" target=""_blank"" id="""">Helm</a>, Pulumi, and more in the cloud. Without requiring expertise in every cloud and native service involved. <strong id="""">You code, and Release makes it run!</strong></p><p id="""">How does Release use AI to accomplish this feat? The answer is generative AI + iteration! With GPT-4’s help, Release automatically dockerizes your application and runs it in the cloud!</p><h3 id="""">How does it work?&nbsp;&nbsp;</h3><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1552px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1552px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/6442d335ef7a2f1a1fce3cd5_release%2BChat%20GPT%202.png"" loading=""lazy"" id="""" width=""auto"" height=""auto""></div><figcaption id=""""><em id="""">Figure 1. High level diagram showing how GPT-4 is utilized to dockerize and deploy your application with Release</em></figcaption></figure><p id="""">Here's how it works: Typically, we need some basic configuration from your repository in order to get your application running in AWS and/or GCP. We start with Dockerfiles and Docker Compose files which ultimately are translated into k8s manifests that can run in EKS and/or GKE. Now, with the help of AI, Release is able to generate and test the Docker files for your repository; point Release at your repository and it fills in the missing pieces.&nbsp;</p><p id="""">Release goes through an iterative process to generate and test the requisite files. It presents the output to the user so they can make any changes, and then tests the files again. Through this process you end up with requisite artifacts (Dockerfile, docker-compose.yml and .dockerignore) and Release gets your software up and running in the cloud!</p><p id="""">The steps are straightforward:&nbsp;</p><ol id=""""><li id="""">Associate your repository with Release</li><li id="""">Release will interrogate your repository and generate three files<br>a. Dockerfile, .dockerignore, and a docker-compose.yml</li></ol><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:720px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""720px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/6442d379732fab028cf5cdb6_AWheFnDRjwwYB4Lhcs1IrCErCOXzbFgHdP7xiY-2ouE52ab9Df3G4Fh_063RLaviLPraXpQ6OQMpM0ghm3whhc8JzSa0Z9HPfghLBOPXtjzrGkL_iM0mNEKf08tuWbP6Qb3nnJnlq70Bj82zOKu6ifI.gif"" id="""" width=""auto"" height=""auto"" loading=""auto""></div><figcaption id=""""><em id="""">Figure 2. Release auto-generating docker artifacts from repository interrogation + AI</em></figcaption></figure><ol start=""3"" id=""""><li id="""">Release runs a build and tests your generated files<br>a. If the build fails, you can see the logs and make changes and try again</li></ol><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:720px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""720px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/6442d3ab67af09f7e0dabafb_t10_C4Jx8NwpCcJ-NCYGV7FqJ4nEKQSZvQu87fJYn-chDWPV5YD3rcvej5l_um3_jXMO1oYx15iN30bvCbBliQ8WtDZ_tcWODh8_e-mVxtsVDXg5rFiaIyGcJlpjk1QUVlqqut57-LM2gLpVk7Rt5vQ.gif"" id="""" width=""auto"" height=""auto"" loading=""auto""></div><figcaption id=""""><em id="""">Figure 3. Shows a build error being automatically corrected by Release based on the errors in the docker build. The package list was missing “shared-mime-info”.&nbsp; Release noticed this and regenerated a Docker file, this time including “shared-mime-info”.</em></figcaption></figure><ol start=""4"" id=""""><li id="""">Once the build is working, Release deploys your application into your EKS/GKE cluster</li></ol><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1255px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1255px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/6442d3c9a1e517af277f4ab9_LkDtvmbwVgkcna1sgMseoEUlVkdVUIPWd_cqpRrxcthiFs2QP64OvQAXnLfnTNpvywSxHZWvvOjdDqWTHhdMKnZPVWeEwWYQWd88kCWrpq7JCP2h_xof6Y5DxFxCrTZRJ0Vm6E89UfclRICxHyEn7io.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div><figcaption id=""""><em id="""">Figure 4. Once the build passes a deployment kicks off automatically and we now have a fully working environment, in the cloud without having to create any docker files, k8s manifests, terraforms, etc.<br></em></figcaption></figure><ol start=""5"" id=""""><li id="""">Now you can instantly create ephemeral, production, and cloud development environments. And even deploy your software into your customer’s VPCs with <a href=""https://release.com/blog/release-delivery-helps-saas-companies-meet-the-needs-of-their-enterprise-customers"" id="""">Release Delivery</a>!&nbsp;</li></ol><h3 id="""">What’s next?</h3><p id="""">This is just the beginning of our code to cloud dream!&nbsp;We have plenty of things left to do:</p><ul id=""""><li id="""">We will continue to optimize our prompts and repository interrogation. We are using Rails applications as a proof of concept, but we will add support for all the major frameworks and improve the file generation through prompt optimization.&nbsp;</li><li id="""">We have an iterative loop for creating a successful build, but it currently requires manual approval to start the next iteration. We believe there is room to let our system and GPT-4 automatically build and apply fixes itself until the build succeeds.&nbsp;&nbsp;</li><li id="""">We are creating a virtual assistant to explain various aspects of our documentation and feature sets. Using AI we will be able to provide even more accessible documentation and examples that are context aware. We will open source this effort so other companies can add the same functionality to their applications.</li></ul><p id="""">If you would like to see what’s possible with Release + GPT come by our booth at <a href=""https://railsconf.org/"" target=""_blank"" id="""">2023 RailsConf</a> or take a look at this <a href=""https://vimeo.com/819902308"" target=""_blank"" id="""">video</a>. To try our development platform that is as easy as Heroku, but with the power of the largest cloud providers, use the promo code “<strong id="""">RailsConf</strong>” when you <a href=""https://release.com/get-started"" id="""">sign up</a> or contact sales for a <a href=""https://release.com/book-a-demo"" id="""">demo</a>!</p>",false,,,,https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/6442d3106b9c58a55c8d7069_release%2BChat%20GPT%204.jpg,Release + Chat GTP4,erik-landerholm,3,Mon Apr 24 2023 13:30:00 GMT+0000 (Coordinated Universal Time),product; ai,
Components of a Successful IDP: Build a Product Your Developers Actually Want to Use ,components-of-a-successful-idp-build-a-product-your-developers-actually-want-to-use,62aa5a70cd5ba27d9d0d718a,64dcf6f07cb1d8c5fe52d653,Wed Aug 16 2023 16:18:56 GMT+0000 (Coordinated Universal Time),Thu Jul 25 2024 15:05:54 GMT+0000 (Coordinated Universal Time),Thu Jul 25 2024 16:27:39 GMT+0000 (Coordinated Universal Time),"Learn about different qualities and components of successful IDPs, and explore some ways IDPs could fail.","<p id="""">After reading about the<a href=""https://release.com/blog/what-is-an-internal-developer-platform-and-why-should-i-have-one"" id=""""> Internal Developer Platforms (IDPs) and why you should build one</a>, you might be seeing certain benefits your organization could use. At this point, you might already be considering the first moves towards implementing an IDP. After all, an internal developer platform that provides automated self-service solutions for developers to simplify and standardize software practices, infrastructure, environments, and operations sounds like a dream come true. But what specifically should an IDP do? What functionality should it cover? And how can you ensure that teams adopt the IDP and that it provides real benefits?</p><p id="""">In this post, we’ll cover different qualities and components of a successful IDP, and share some ways in which IDPs can fail. Let’s dig in! </p><h3 id="""">Qualities of successful IDPs</h3><p id="""">Before you roll up your sleeves and start building your IDP, consider a set of qualities a successful IDP needs to solve for. These are the high level objectives an IDP addresses, without accomplishing these can easily become yet another tool gathering dust on the shelf. When building an IDP keep these three qualities at the top of your mind: </p><ol id=""""><li id="""">IDPs provide self-service solutions</li><li id="""">IDPs automate processes</li><li id="""">IDPs enforce guardrails and standards</li></ol><h4 id=""""><strong id="""">1. IDPs Provide Self-Service Solutions</strong></h4><p id="""">First, let’s consider self-service. Successful IDPs enable developers to access resources without tying up resources and people in other groups.</p><p id="""">The IDP should empower teams and developers to solve their own problems when it makes sense for the developer based on their timelines and needs.</p><p id="""">Of course, there’s a caveat with this as too much empowerment can lead to failure. For full self-service, this could mean that you give your development teams access to everything, allowing full self-service all the time. But that’s not going to go well.</p><p id="""">If you were to just provide access to everything, development teams would need to learn the ins and outs of infrastructure, tooling, governance, and multiple standards to self-service everything they need. Sure, they’d be able to self-service. But they wouldn’t have time to ship features. And they would end up re-learning similar tools and remaking similar mistakes that other teams or developers already made.</p><p id="""">For IDPs, we do need to provide self-service functionality. But <strong id="""">the surface area of the necessary knowledge for that self-service ability should be as small as possible.</strong> That way, developers can focus on creating a product with their development tools, and not tinkering with infrastructure.</p><p id="""">So how do we make that surface area small? Through automation and standards. In the next section, let’s consider automation.</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1000px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1000px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/64dcf6e30cf7b5aa33094a5d_MDWWIBEO_TA160J4tgVnLK4psmjjRyIi4RFz9sBGbU18ObOwiRFtgiVBW2Bp2b-6KP6DZnBckARKTT2Q2vV8fLzVNoFJwB4bwpQt42-xCy7APtxc5ijB7bfIIwAeswJhGwFH0Gy0GmSZFs_FWLuoD6E.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><h4 id=""""><strong id="""">2. IDPs Automate Processes</strong></h4><p id="""">When you read the word <em id="""">automation</em>, you may think you can skip this section. Let’s hold up a second, though.</p><p id="""">You may say, “Sure, I know what automation is. I create scripts that build and deploy apps, for example.”</p><p id="""">Yes, <a href=""https://prod.releasehub.com/blog/11-continuous-deployment-tools-and-how-to-choose-one"" id="""">CI/CD pipelines</a> are oftentimes the first thing that comes to mind when we think about automation and the development process, which is essential. But we should consider automation outside of the basics.</p><p id="""">The successful IDP is a product built for developers. It can provide solutions for all stages of a developer’s work, from the day they start at your company to the day they leave. And it involves automated self-service solutions that ideally do not require ticket queues, manual approvals, or reliance on other teams to complete the request.</p><p id="""">Let’s talk about approvals for a minute. In some cases, we do need manual approvals, and that’s OK for limited and special use cases. But the majority of the time, approvals don’t provide a lot of value. Consider access requests. If you need to manually approve every single request for access to your code repository, deployment tools, and monitoring solutions, then you will waste time. What developers on your team don’t need access to those tools—or can work without them? Probably all of them need some access. </p><p id="""">You may say these approvals are needed for auditing and validation. But are there better ways to get what you need? If every developer is given required access automatically as part of their first day’s onboarding, as part of the standard automated process, then can that fulfill your requirements? Alternatively, can you implement automations that verify that developers have access to the proper resources, that they’ve taken the required training, and that automate attestation of need to access those resources? When solving for compliance or auditing requirements, there are different ways to automate based on your specific needs. </p><p id="""">Frequently, we automate parts of a process while not always automating the most significant <a href=""https://release.com/blog/increase-developer-velocity-by-removing-environment-bottlenecks"" id="""">bottlenecks</a>. For example, if you’ve automated a request process to provision new databases but do not automate standard configuration, you’ve saved only a few minutes of someone’s time. Or if you pull reports frequently to verify that databases continue to conform to standards or don’t have unused capacity, then there is still room for improvement.</p><p id="""">When you consider automation, do not just consider automating the simple button clicks or simple scripting. Think about how your whole development process works and where the time and effort is spent. And then iterate. Once you’ve improved one part of the process, continuously re-evaluate how your automations can further improve the whole processes.</p><h4 id=""""><strong id="""">3. IDPs Enforce Guardrails and Standards</strong></h4><p id="""">OK, after the two previous sections, you may know where this is going.</p><p id="""">With a successful IDP, your self-service tools and automation conform to the standards and practices that your subject matter experts have determined work best.</p><p id="""">Now, in some cases, we do need to provide more freedom to developers to work outside of the normal guardrails or standards. And these standards should not limit your development teams to current use cases, as needs change. To provide flexibility for niche use cases, consider how you can enforce these guardrails while providing workarounds or approved exceptions. </p><p id="""">However, consider the ways that IDPs can standardize not just tools like CI/CD, but basic provisioning, configuration, monitoring, and more.</p><p id="""">Keeping the Qualities in mind, let’s look at the specific Components you should consider. &nbsp;</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1000px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1000px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/64dcf6e36c7b91548ea684d1_tmvC_Xj34SAqQsdlzGaR9dqocCsgY2_aEEd-z8H7FCm36Bhqpbm7BVxky1C9vFfwi3rDqVXJi3vaPwWzqC9JpnBEKbtPBgAdiu5GLGO2ip6NcgH5z9N8tfNm2wviTJ0z7dMq7s1CFFqc7mjd-g3kSu4.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><h3 id="""">Components of successful IDPs</h3><p id="""">The components that make up successful IDPs vary and don’t all need to be used at the same time. In fact, when getting started, you should never attempt to use this as a checklist, assuming your IDP will be successful when you’ve added all the components.</p><p id="""">Additionally, not all components need to be part of a central IDP deployment. They can involve other tools and bits of automation that work to improve the efficiency of your development teams, but function outside of the central IDP deployment.</p><p id="""">When you’re planning what to tackle first, consider which components of an IDP will provide your organization with the most value. </p><p id="""">Some components are necessary for all organizations, like code repositories, IDEs and CI/CD. Without them you’re missing the basics that all teams that write code need. </p><p id="""">Other components should be present, but not all orgs have automated them or added any integrations into the developer workflow to make them worthwhile just yet. This includes items like automated security scanning, typically integrated through your CI/CD pipeline. </p><p id="""">And then there are items specific to certain industries. For example, not every organization needs full integration with compliance and regulatory features.</p><p>Finally, not all of these components need to be integrated through an IDP from the start. You can build upon and expand as the value becomes clear. But once you have a number of these components that need to work together, incorporating them and integrating them into your IDP will simplify the development workflow. These integrations can remove unnecessary cognitive load of configuring and context-switching for your dev team and allow them to focus on more valuable tasks.</p><p id="""">IDP components can include but are not limited to:</p><ol id=""""><li id=""""><strong id="""">Code repositories:</strong> OK, so there are some of these components that are non-negotiable. And our repositories, like git, are # 1.</li><li id=""""><strong id="""">Infrastructure:</strong> This can include building, orchestrating, configuration management, <a href=""https://prod.releasehub.com/blog/a-simple-guide-to-software-environments"" id="""">environment management</a>, and monitoring of the infrastructure used by your development team.</li><li id=""""><strong id="""">Development tools:</strong> This includes IDEs, plugins, and extensions as well as stand-alone tools. A good IDP can integrate with other tools through plugins and extensions, all built out as part of the IDP.</li><li id=""""><strong id="""">CI/CD:</strong> As this is one of the basic tools IDPs use, consider ways to supercharge this functionality. In addition to the usual CI/CD functionality, can you automate changelogs, notify operations of deployments, and track failures and rollbacks?</li><li id=""""><strong id="""">Automated testing: </strong>In addition to automating unit tests, integration tests, and others, a successful IDP will drive advanced test features like flaky test detection, load testing, and even chaos engineering. </li><li id=""""><strong id="""">Security:</strong> The basics of security could involve static analysis tools in your CI/CD pipeline, <a href=""https://release.com/blog/kubernetes-secrets-management-a-practical-guide"" id="""">secrets management</a>, or live monitoring of security threats.</li><li id=""""><strong id="""">Monitoring/observability:</strong> Monitoring and observability provide your teams with the ability to ensure your code actually does what it is meant to do. It also can help teams not only identify problems (through monitoring alerts around availability or critical functionality) but also help find the root causes. We can also think about more than just a link from our IDP to our monitoring solutions, but integrations with our CI/CD tools to identify where problems occur and when the problems were introduced.</li><li id=""""><strong id="""">Project management and collaboration tools:</strong> When you’re looking at the IDP, do not just consider purely developer-focused tools. How can your IDP integrate your deployment processes with your project management and notifications?</li><li id=""""><strong id="""">Compliance and governance tools and workflows:</strong> We talked a bit about this when discussing access requests earlier. Consider what other manual processes and reporting could be automated and integrated into your IDP. And don’t think of this as just automating a report. Consider what can be done to remove the need for reports and manual intervention.</li><li id=""""><strong id="""">Workflow automation:&nbsp;</strong>Workflow automation is a bit of a catchall for tools that automate workflows. Once you’ve covered the basic components, you can begin to automate processes like creating new services, upgrading systems, or creating changelogs and API versions. Since the IDP centers around improving developer workflows, you’ll need to look outside of the basic components and find new ways to reduce toil.</li></ol><h3 id="""">Common Failures of IDPs</h3><p id="""">So, if an IDP covers all or a majority of the key components, why do many still fail to provide value? Why do they struggle with adoption?</p><p id="""">Well, several things can go wrong. We’ll cover just four of them.</p><p id="""">First, the IDP may be <strong id="""">focused on niche activities and processes and not the 80% of functionality that most developers need</strong>. For example, if you’re in a monolithic environment, automating project initiation for microservices obviously won’t add much value. On the other hand, if your <a href=""https://prod.releasehub.com/blog/improve-developer-velocity-with-ephemeral-environments"" id="""">team struggles with getting fast feedback, slow velocity, or environment bottlenecks for testing</a>, then spinning up <a href=""https://release.com/ephemeral-environments"" id="""">ephemeral environments</a> might be worth your time.</p><p id="""">Second, <strong id="""">to understand your developers, you must consider <em id="""">all</em> of your developers</strong>. Do not consider just the strongest, newest, or loudest developers. Interview a wide sample of your teams and consider their entire processes, so that you build a truly user-centric product.</p><p id="""">Third, sometimes <strong id="""">in an effort to remove security risks, companies will remove flexibility or functionality that the development team relies on</strong>. For example, in an attempt to automate environment provisioning, an IDP may lock down security on configuration that developers used to have access to. This may be the right move, but you have to have a way to adjust to specific use cases. There should be guardrails while still making it possible to work outside of those guardrails when necessary.</p><p id="""">And finally, perhaps you <strong id="""">haven’t considered or built a rollout strategy</strong>. You will need to consider how to onboard teams, get feedback, and quickly adjust and pivot based on that feedback and data. You can’t just build it and assume everyone will start using the tools and integrations available. It will take time and effort.</p><h3 id="""">Summary</h3><p id="""">When considering your future IDP or finding the next component your IDP should cover, consider the 80% of the functionality that your development teams need frequently. Add automated, standardized, self-serve functionality that addresses the majority of development workflows. And make sure you have enough of a feedback loop to ensure you’re building the right platform and improving the development cycle. In our next chapter we will look at the Build vs Buy question. Stay tuned! </p>",false,,,,https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/64dcf77651900c6b1ba283cc_IDP_2.jpg,Photo Credit: Dan Cristian Paduree,sylvia-fronczak,10,Wed Aug 16 2023 20:00:00 GMT+0000 (Coordinated Universal Time),platform-engineering,
Continuous Deployment Webinar Recap,continuous-deployment-webinar-recap,62aa5a70cd5ba27d9d0d718a,62d6da403f4da141c4d0bee1,Tue Jul 19 2022 16:22:24 GMT+0000 (Coordinated Universal Time),Tue Jul 19 2022 16:25:55 GMT+0000 (Coordinated Universal Time),,Tommy McClung joins the DevOps.com webinar to discuss the challenges of Continuous Deployment and what the future holds ,"<p id="""">Tommy McClung is featured on a DevOps.com<a href=""https://webinars.devops.com/continuous-deployment?utm_campaign=%242022.07.11%24_EdCal_Panel_Webinar_DO&utm_source=Release"" id=""""> webinar</a> to discuss the ins and outs of Continuous Deployment. &nbsp;He is joined by Field CTO of Harness and Senior Director of Engineering at Everbridge as they dive into the challenges of CD. </p><p id="""">Tommy, the co-founder and CEO of Release, has been building scalable infrastructure for at least over 20 years. He’s also a serial entrepreneur. He co-founded CarWoo!, which was acquired by TrueCar.</p><p id="""">It’s no secret that continuous deployment is challenging, because automating deployments across platforms has never been especially easy. Continuous Deployment also implies that you know where everything is being deployed into and you have environments that are stood up. &nbsp;Often platforms are customized to the point where it is difficult to automate application deployments—each platform is its own unique ""snowflake."" </p><p id="""">However, the growing complexity of application development projects coupled with increased reliance on software to drive digital business transformation initiatives and concerns over software supply chain integrity requires organizations to revisit their approach to application delivery. </p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-center"" data-rt-type=""image"" data-rt-align=""center""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/62d6d96204925403031a2ade_M1SpsRJJvkZ3Vm0LEg2tdYrIQbFhphUYAaRUiezUZgiInVzIQtmVyakJxtLcXOQxiFBi0C8RVNsbfPpcMsEGdv8CvLdb1nNj2ezxyRRQL2vEwLHMkw2aK6OaKWtlzei82VpGoYPs1kQTqSZzmJi4Tdc.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><p id="""">Tommy has been working on this problem for many years. As a developer at heart, he cares about getting great ideas to the world quickly, and environments tend to be a key ingredient in making that happen. This is why he created ReleaseHub and the concept of ephemeral environments, or “environments as a service”. With ReleaseHub, developers can create on demand environments with the click of a button or via a pull request, while maintaining robust velocity and faster deployment.</p><p id="""">If you’re looking to have some burning questions answered such as “How automated can CD get?” or “How do DevOps teams roll things back when something goes wrong?”, you’re in the right place. &nbsp;Tommy, along with his fellow speakers answer these questions along with many others. &nbsp;</p><p id="""">Check it out <a href=""https://webinars.devops.com/continuous-deployment?utm_campaign=%242022.07.11%24_EdCal_Panel_Webinar_DO&utm_source=Release"">here</a></p><p id="""">Want to try Release for yourself? <a href=""https://releasehub.com/"" id="""">Request a demo.</a></p>",false,,,,https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/62d6da06a3f5ad0be8fdc6a9_Screen%20Shot%202022-07-19%20at%2012.21.21%20PM.png,,anna-chandler,3,Fri Jul 22 2022 13:20:00 GMT+0000 (Coordinated Universal Time),,
Contributing To Open Source - Getting Started,contributing-to-open-source,62aa5a70cd5ba27d9d0d718a,62aa5a70cd5ba2d4870d72e2,Fri Jun 11 2021 03:05:52 GMT+0000 (Coordinated Universal Time),Fri Apr 21 2023 19:45:19 GMT+0000 (Coordinated Universal Time),Fri Apr 21 2023 20:04:43 GMT+0000 (Coordinated Universal Time),"Contributing to open source projects can be very fulfilling, but getting started may be intimidating and confusing. Most","<p id="""">Contributing to open source projects can be very fulfilling, but getting started may be intimidating and confusing. Most large open source projects have steep learning curves which can be off-putting for the first-time contributor. However, there are a lot of opportunities to get started and thrive, even if you’ve never contributed to an open source project before.&nbsp;<br></p><p id="""">Open source is incredibly valuable. Where would the Linux kernel be without the 13,500 developers and 1,300 companies that have contributed since 2005? And what of the thousands of projects built on its back?<br></p><p id="""">These days, every project you start is built using packages and building blocks from those who came before you. You can stand on the shoulders of giants. The wheel has already been invented, and it’s more reliable and battle-tested than anything you could build in-house. Open source software has been essential for developers, companies, and, ideally, consumers.<br></p><p id="""">However, there would be no giants, no blocks, no wheels without communities, contributors, and passionate people to drive these projects forward. These are ordinary people doing extraordinary things.&nbsp;<br></p><p id="""">Although the main objective of open source is to create accessible and valuable software for everyone to build off of, contributors can find a plethora of personal benefits from contributing. You will sharpen your technical skills, nurture your interpersonal communication skills, and practice giving and receiving feedback.&nbsp;<br></p><h3 id="""">The Culture<br></h3><p id="""">Okay, so you’ve decided you want to become a contributor but you’re not convinced that you’ve got what it takes. You’re feeling quite intimidated actually. That’s okay. You’ve never done this before. Every contributor has had a first contribution and open source projects tend to culture empathy, patience, and understanding.<br></p><p id="""">In my experience, successful open source projects have maintainers and communities that are the friendliest on the planet. They are excited and enthusiastic about someone wanting to contribute. Often they’ll be active on Twitter, Slack, Discord, and/or another platform that you can join and speak directly with the maintainers and other contributors. Many large open source communities even have their own community guidelines that boil down to “Don’t be a jerk”. These communities welcome new contributors with open arms.<br></p><h3 id="""">Finding the Right Fit<br></h3><p id="""">One of the best ways to find an open source project to work on is to look to open source software that you already use. Tools, packages, frameworks, or languages that you work with regularly and enjoy using could be great candidates. To find out if the project is open source, check its license and if it accepts contributions. It’s also important to check that the project is actively maintained. Are the pull-requests sitting dormant or are the maintainers providing feedback? Are the issues getting acknowledged or are they sitting there stale?<br></p><p id="""">If that approach doesn’t work for you, try using GitHub to explore. I recommend starting your contribution journey with languages and frameworks that you’re familiar with. It’ll be difficult enough to get up to speed on the codebase, but if you’re already familiar with a framework’s best practices and typical layout, you’ll have a more successful time hitting the ground running.<br></p><p id="""">GitHub has excellent search capabilities to find open source projects that are actively seeking out new contributors. By searching for the right tags and filtering by languages you know best, you can quickly find new issues that beg for assistance. Some great tags to search for are:<br></p><ul id=""""><li id=""""><a href=""https://github.com/topics/contributions-welcome"" target=""_blank"">contributions-welcome</a></li><li id=""""><a href=""https://github.com/topics/good-first-issue"" target=""_blank"" id="""">good-first-issue</a> </li><li id=""""><a href=""https://github.com/topics/hacktoberfest"" target=""_blank"" id="""">hacktoberfest</a> </li><li id=""""><a href=""https://github.com/topics/beginner-friendly"" id="""">beginner-friendly</a> </li><li id=""""><a href="""" id="""">good-first-bug</a></li><li id=""""><a href=""https://github.com/topics/easy"" target=""_blank"" id="""">easy</a></li><li id=""""><a href=""https://github.com/topics/low-hanging-fruit"" target=""_blank"" id="""">low-hanging-fruit</a></li><li id=""""><a href=""https://github.com/topics/first-timers-only"" target=""_blank"" id="""">first-timers-only</a><br></li></ul><p id="""">GitHub also suggests projects you may like based on the people and repositories that you have starred, follow, or watch.&nbsp;<br></p><h3 id="""">Your First Contribution<br></h3><p id="""">Once you find an issue that you feel confident in tackling in an active project that has a culture you feel comfortable in, it’s time to get your hands dirty! The first thing you will want to do is to “claim” the ticket. It’s a bad idea to run off and solve the problem without communicating your intentions to the maintainers. Instead, reply to the ticket in question by volunteering yourself for the task. This allows the maintainers and other contributors to know this ticket is being actively worked on and the maintainers may have requests, suggestions, or guidance to help solve the problem.<br></p><p id="""">Start small. Pick tickets that are easy and have the smallest contribution and code changes when you first start. Not only will this enable you to slowly get familiar with the codebase, but it will also build your confidence and credibility before you try taking on harder tasks.&nbsp;<br></p><p id="""">After you have picked up a ticket and think you have finished it, do your research before you open your first pull-request. Carefully read the documentation, code, and discussions related to this ticket to get the best understanding of how to handle the problem. If you’re stuck, reach out to the community and ask for guidance, clarification, or mentorship.&nbsp;<br></p><p id="""">Once you feel confident that you’ve solved the problem, it’s time to submit a PR. Look in the project for a <strong id="""">CONTRIBUTORS.md</strong> file--most open source projects on GitHub will likely have this within their project. This file will contain instructions on how this project would like PRs to be submitted by contributors. They may request strict branch naming conventions, PR titles, documentation or tests, comments, or other things in pull-requests. GitHub also has a <a href=""https://opensource.guide/how-to-contribute/#a-checklist-before-you-contribute"" target=""_blank"" id="""">great checklist</a> on what to check for before you open your pull-request.&nbsp;<br></p><p id="""">Create the PR and follow the PR template if the project has one. Be sure to link back to the original ticket. The maintainer may request changes or want to have a discussion about your changes. It’s normal to have a back-and-forth before a PR is accepted. Work with the maintainer until your PR is ready to be accepted.<br></p><p id="""">Once your PR passes the maintainer’s review, they will merge your code in.&nbsp;<br></p><p id=""""><strong id="""">Congratulations!</strong> You’re an open source contributor!!</p><h3 id="""">Are You An Open Source Project Maintainer? Release Wants To Support You!<br></h3><p id="""">Open source projects benefit dramatically from Ephemeral Environments on every pull-request. We love open source and are dedicated to giving back. <a href=""https://share.hsforms.com/1YM0zqIgsTc2aD2oPDQUQJA4shs5"" target=""_blank"" id="""">Contact us</a> to find out how we can help fuel your rocket through preview environments and help put confidence in contributor’s PRs.<br></p>",false,,,,https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63e4052bc7355650ca45e3b3_061121%20(1).jpg,Group of friends hugging each other conveying the idea of community,vicky-koblinski,7,Fri Jun 11 2021 14:05:00 GMT+0000 (Coordinated Universal Time),,
Creating Private AI Environments with Release AI: Keep Your Data and Intelligence Under Your Control,creating-private-ai-environments-with-release-ai-keep-your-data-and-intelligence-under-your-control,62aa5a70cd5ba27d9d0d718a,66ce285aa8e4b0477d2548af,Tue Aug 27 2024 19:26:18 GMT+0000 (Coordinated Universal Time),Tue Aug 27 2024 19:33:36 GMT+0000 (Coordinated Universal Time),Tue Aug 27 2024 19:33:36 GMT+0000 (Coordinated Universal Time),"Learn the simple steps to set up a secure, efficient AI workspace without sending sensitive data over the internet.","<p id="""">Let's talk about how to create AI environments in Release AI that keep your AI data and intelligence fully within your control. To understand why this matters, we'll start by looking at how traditional AI applications work when you're not fully in control.</p><h4 id="""">The Problem with Traditional AI Applications</h4><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-center"" data-rt-type=""image"" data-rt-align=""center""><div id=""""><img src=""https://cdn.prod.website-files.com/603dd147c5b0a4221d1bd360/66ce25bdb765c8c81a9e01ef_AD_4nXe-DazWpvij529eX-uXgo_jpnUgPmGCL3K8VU9AdnSonosdZuuWizITAlN1UzVNHzKL2L4QiSsYk1wvzlNpHTTAVpMCU3s_CIDpczsK-LmUiSyp4mngTbtxswO-3n3l77ncdEOSzZMwg1qmU930f4cSfdY.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div><figcaption id=""""><em id="""">Traditional AI application data flow diagram</em></figcaption></figure><p id="""">Here's what typically happens:</p><ol id=""""><li id="""">Your application runs in your data center.</li><li id="""">When you need AI capabilities - like observations, reasoning, or actions - you send data over the internet.</li><li id="""">This data might include context, embeddings, or other sensitive information (think RAG applications).</li><li id="""">Your data hits inference servers and GPUs in someone else's data center.</li><li id="""">Your data could end up in their AI models, potentially used for training.</li><li id="""">You have no control over anything outside your VPC or cloud.</li></ol><h4 id="""">Introducing Release AI's Private AI Environments</h4><p id="""">Now, let's look at how Release AI changes this:</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-center"" data-rt-type=""image"" data-rt-align=""center""><div id=""""><img src=""https://cdn.prod.website-files.com/603dd147c5b0a4221d1bd360/66ce25bd0838738a31fda3cc_AD_4nXcOHqFORF19NzuD-ZWsXWYqxohz6DyEWYDIlKNdfushqjBkb3RJGDBd8p-l_S4-HHytJ_Ztw7BCnBx9GQsZK24M_Qtoawmm2vIWOsQ0bfyEeBIF1yhW0F0bDFjUzkDjQgC5S1C25zPr46kVXUvEGXGj6xw.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div><figcaption id=""""><em id="""">Release AI private environment diagram</em></figcaption></figure><p id="""">With Release AI's private AI environments:</p><ul id=""""><li id="""">Everything stays within your control.</li><li id="""">Your application and data sources live in your VPC and cloud account.</li><li id="""">Your data never goes out to the internet.</li><li id="""">You host inference with models you've either gotten (like open-source models) or trained yourself.</li><li id="""">All your data and AI intelligence stays completely within your control.</li></ul><p id="""">The best part? Setting this up with Release AI is incredibly simple. Let me show you how.</p><h4 id="""">Creating a Private AI Environment with Release AI</h4><ol id=""""><li id="""">Connect to Your Cloud Account </li></ol><ul id=""""><li id="""">Click ""Create Cloud Integration""</li><li id="""">Give it a name</li><li id="""">Pick your cloud provider (AWS or GCP)</li></ul><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-center"" data-rt-type=""image"" data-rt-align=""center""><div id=""""><img src=""https://cdn.prod.website-files.com/603dd147c5b0a4221d1bd360/66ce25bda1d043b184452bbd_AD_4nXe4Z2VnPS3G6zayJdztuQUOW-Izj14vkEQipgexOG7vSjtN6REBzmb31rHtLBQmi8sqlrIAMQn25w2quzi-WlUupCwy-pet6xRVsXDv1Ob6MtcNSWTE2Ybsh3NxoMa60H806Bd1xNSk_s3gGPzRz_pQdi_P.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div><figcaption id=""""><em id="""">Cloud integration setup screen</em></figcaption></figure><p id="""">This lets Release AI interact with your cloud to create an environment that's completely under your control.</p><ol start=""2"" id=""""><li id="""">Create a Cluster </li></ol><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-center"" data-rt-type=""image"" data-rt-align=""center""><div id=""""><img src=""https://cdn.prod.website-files.com/603dd147c5b0a4221d1bd360/66ce25be9ddb66ca0b9ef967_AD_4nXeEnuHWXslKJhlVbCZRyuVNtW_QUEX_XOV4BKE_8f_5Apuf7wHheZE3k7ycCb9c9Uqn-ZLNTx-nv799kPaEC29rlr8KSb06zkZC8bHlP_1zD556VEBt99pmcr3d-FMTRI0dHBORHqt2MquQL_BkxRBHi9DK.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div><figcaption id="""">Cluster creation screen</figcaption></figure><p id="""">Next, you create a cluster that runs in your cloud account. In this example, I've already made one called ""release-ai-demo"".</p><ol start=""3"" id=""""><li id="""">Configure Your Cluster </li></ol><p id="""">Let's look at the details:</p><ul id=""""><li id="""">It's using the cloud integration we just set up</li><li id="""">It's a Kubernetes EKS cluster running version 1.29</li><li id="""">It's fully running within your AWS or GCP account</li><li id="""">All the nodes (in this case, I'm using G5 instances for AI workloads) are in your AWS account</li></ul><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-center"" data-rt-type=""image"" data-rt-align=""center""><div id=""""><img src=""https://cdn.prod.website-files.com/603dd147c5b0a4221d1bd360/66ce25bd6e73f1177f425691_AD_4nXdM5SxO9T7oXS-K-8HEMUVATA5Ka3LnDsqolBacITpzzewPjZInW2NAZ4TmvENSTUJqDGmjL0LI1Khsyq2YxTom5Q1p2jpfup3y4_5r1ZITOOzrr3ku-U3AfZDlDF82QoeMrcITVmSedV8HiAtccB1lBFDO.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div><figcaption id="""">Cluster creation screen - details</figcaption></figure><h4 id=""""><strong id="""">Why This Matters</strong></h4><p id="""">When you make queries to these instances, no data ever escapes your AI environment. Everything stays put:</p><ul id=""""><li id="""">All your data</li><li id="""">All your compute</li><li id="""">All your infrastructure</li></ul><p id="""">It's completely within your control. You don't have to worry about sending your data over the internet into somebody else's SaaS application where they could use it to train, fine-tune, or do who-knows-what with it.</p><h4 id=""""><strong id="""">Bottom Line</strong></h4><p id="""">By having a private AI environment running in your AWS account on your infrastructure, your applications, AI intelligence, and data stay fully within your control. It's that simple.<br>‍</p><p id="""">Watch the full demo <a href=""https://www.youtube.com/watch?v=0ysZ_bY0Gv8&t=2s"" id="""">here</a>. </p>",false,,,,https://cdn.prod.website-files.com/603dd147c5b0a4221d1bd360/66ce29efdbd7c29aab1f32ea_Private%20AI%20env.jpg,,tommy-mcclung,5,Tue Aug 27 2024 20:00:00 GMT+0000 (Coordinated Universal Time),ai; product,
Creating your first Application in Release with Docker Compose,creating-your-first-application-in-release-with-docker-compose,62aa5a70cd5ba27d9d0d718a,649c47e374782d81ccb2d9f7,Wed Jun 28 2023 14:46:59 GMT+0000 (Coordinated Universal Time),Thu Jul 25 2024 15:08:52 GMT+0000 (Coordinated Universal Time),Thu Jul 25 2024 16:27:39 GMT+0000 (Coordinated Universal Time),Take your Docker Compose file and Crete an Application to run on Kubernetes in six easy steps.,"<p id="""">If you’re using Docker compose for local development but have been interested in running your application on Kubernetes or creating ephemeral environments for your application, keep on reading, this post is for you! &nbsp; &nbsp;</p><p id="""">At Release we know that Applications consist of more than just your repository and code. There are other services that are required, such as databases or key value stores. An application usually cannot run without environment variables, backing data, infrastructure, or storage component(s). That’s why we think that a Docker compose file is one of the best ways to describe your application for local development. It is also a perfect way to get started on Release and get your near-production environments spun up with each pull request. In this blog, we will walk through the steps to create an Application, highlight how Release helps transform your compose file into an Application Template and ultimately deploy it on Kubernetes (which we will cover in part two of this series).</p><p id="""">We’ll be using <a href=""https://github.com/awesome-release/rails_postgres_redis"" id="""">https://github.com/awesome-release/rails_postgres_redis</a> as the example in this post. It is a small application that runs a Ruby on Rails server, has requirements of a Postgresql database and a Redis server, as well as runs Sidekiq, which is a background job processor. </p><p id="""">Let’s take a look at the compose file and then jump into creating our Application.</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yaml"">
version: '3'
services:
  api:
    build: .
    image: rails_postgres_redis:latest
    command: bash -c ""(rake db:exists && rake db:migrate || rake db:setup) && bundle exec rails s -b 0.0.0.0""
    environment:
      REDIS_URL: redis://redis:6379/0
      POSTGRES_PASSWORD: postgres
      POSTGRES_USER: postgres
      POSTGRES_HOST: db
    ports:
      - ""3000:3000""
    depends_on:
      - db
      - redis
  
  sidekiq:
    image: rails_postgres_redis:latest
    command: bundle exec sidekiq
    environment:
      REDIS_URL: redis://redis:6379/0
      POSTGRES_PASSWORD: postgres
      POSTGRES_USER: postgres
      POSTGRES_HOST: db
    depends_on:
      - api
      - redis
      - db    

  db:
    image: postgres:12-alpine
    ports:
      - ""5432""
    volumes:
      - postgres-data:/var/lib/postgresql/data
    environment:
      POSTGRES_PASSWORD: postgres
      POSTGRES_USER: postgres

  redis:
    image: redis
    ports:
      - ""6379""
    volumes:
      - redis:/data

volumes:
  postgres-data: {}
  redis: {}
</code>
</pre></div><p id="""">‍</p><p id="""">We won’t go through everything line by line but we wanted to make sure to note that in this compose file certain ports are exposed, as well as volumes are being defined to retain data between restarts. We also see that the containers have their &lt;code inline&gt;depends_on&lt;/code&gt; defined to tell us in which order the containers should be started in. We encourage you to take a look at our <a href=""https://docs.release.com/reference-documentation/docker-compose-conversion-support"" id="""">Docker Compose conversion support documentation</a> to see the full list of supported attributes.</p><h3 id="""">Translating a Compose file into an Application </h3><p id="""">Now that we have a Docker Compose file ready, let’s see how to translate it into an Application in Release.</p><p id=""""><strong id="""">Step 1: Create your application</strong></p><p id="""">The first step in creating your Application is to give it a name and select which repository we’ll be using. I’ve named mine “release-docker-compose-demo” and selected the awesome-release/rails_postgres_redis repository (linked above).</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" data-rt-type=""image"" data-rt-align=""fullwidth""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/649c43fe2e8ee3adbb03cb85__kH8wkgXnY02m-wOHgC5Hxd6oqy-nxZ8nKwc4xWMGiGC0RJS-i0WtHz_irWyPI1ccm2iNH1s4s9SNMzGHhG2jGQT5WwxsVZkvYJgUHCvYnyJ1AyKaS0l32LhV2gFmbnwu6AFO1aiGZZpoFN65EclEHk.png"" id="""" width=""auto"" height=""auto"" loading=""auto"" alt=""""></div></figure><p id=""""><strong id="""">Step 2: Analyze your repository </strong></p><p id="""">The next step is to analyze the repository. We’ve selected the main branch and are shown some options of files we can select in a dropdown. The files we select will be analyzed and converted into what Release calls Services. These Services can have many types, such as Containers, Static Javascript Builds, Helm, and Terraform; we’ll primarily focus on Containers in this post but look for future posts where we’ll cover the other options.</p><p id="""">We’ve selected our docker-compose.yml file and we will click the Start Analysis button.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1308px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1308px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/649c43fed7e5a9b68d1e587c_FZxsLKII73JlHM8rm4578YJgiNWa2E9rsImwofzHqhFtMOWkdcbdwX-XeeglwtesDDykSXEfB5wpYD_5IFodleI53vbs3qis7rXF8WiRZUQy0Y5nmNyEDsZXqIPS5HvlyN2ocrHk7mcFntry4JDeJg0.png"" id="""" width=""auto"" height=""auto"" loading=""auto"" alt=""""></div></figure><p id="""">We see that Release created four Services for us with the same names as what is in the compose file. The dependencies are also listed. In this case we know that we want all these Services, but if something had been displayed that we didn’t want to deploy on Release, we could uncheck the Service to remove it. Now we’ll click Next Step to move on.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1306px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1306px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/649c43fe1034e7fe33f259e3_SOn6d1lA7ltMXtXQGNtJU5lYj-ufk04JkG_VsN3Zdc3KhThdhw6oB_Kz2bnHdnCZPTYMTbwXw4unHoikxA440ZO7VSMF1Q7kjk5ce-07t52LUHC6KW-h2WrFYMwrvPOyZhE--D19BtKzRNAfHrzuxyE.png"" id="""" width=""auto"" height=""auto"" loading=""auto"" alt=""""></div></figure><p id="""">‍<strong id="""">Step 3: Generate a template </strong></p><p id="""">In this stage we get our first view of an Application Template. We won’t go through everything in the template here, however we encourage you to read through the documentation on <a href=""https://docs.release.com/reference-documentation/application-settings/application-template"">Application Templates</a> to understand all the possibilities. Instead we’ll highlight how Release has translated the Services from the last page into this yaml format. We see the familiar names of &nbsp;&lt;code inline&gt;api&lt;/code&gt;, &lt;code inline&gt;db&lt;/code&gt;, and &lt;code inline&gt;redis&lt;/code&gt; as well as the ports and volumes that were defined in the compose file.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1310px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1310px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/649c43feabe740f88f8f0119_KKZiowsvPWem1nU-r-NWW1tBWjJ7EbYR9Fj_kuXTtXq_G8CNCqPsk2JXcUVHVEfablgTybgCP_IP8irgOVJgaz6PuY7Z0fKCv3RyvFPLA5rwrSwsQlQ3KpYT5vMs2fneu5SOZPzIqDkGmRyp0GZo3GE.png"" id="""" width=""auto"" height=""auto"" loading=""auto"" alt=""""></div></figure><p id="""">The definitions of Services help to describe <em id="""">what</em> Release will deploy, but we also want to know <em id="""">how</em> Release will deploy these Services. That information is contained in the workflows stanza. There are three types of workflows defined: <strong id="""">setup</strong>, <strong id="""">patch</strong>, and <strong id="""">teardown</strong>. </p><p id="""">A <strong id="""">setup</strong> workflow defines a deployment where infrastructure can be deployed for the first time or if there are subsequent changes to the infrastructure; think, changing your Postgresql version, the number of replicas of the &nbsp;&lt;code inline&gt;api&lt;/code&gt; Service, or changing environment variables for your Application. We can also see that the order of the Services from the compose file &nbsp;&lt;code inline&gt;depends_on&lt;/code&gt; is translated into the setup workflow. &lt;code inline&gt;db&lt;/code&gt; and &lt;code inline&gt;redis&lt;/code&gt; will be deployed in parallel first. Once both of those Services are up and running, Release will move to the next step and deploy &nbsp;&lt;code inline&gt;api&lt;/code&gt;. Finally &nbsp;&lt;code inline&gt;sidekiq&lt;/code&gt; will be deployed.</p><p id="""">The <strong id="""">patch</strong> workflow is used when only code changes need to be deployed. In our case, both the &lt;code inline&gt;api&lt;/code&gt; and &lt;code inline&gt;sidekiq&lt;/code&gt; Services contain the code from the repository and would need to be deployed when we push new changes. The &nbsp;&lt;code inline&gt;db&lt;/code&gt; and &lt;code inline&gt;redis&lt;/code&gt; Services don’t require any changes so they don’t need to be referenced in a patch.</p><p id="""">The final workflow is the <strong id="""">teardown</strong> which uses a Release defined task called &lt;code inline&gt;remove_environment&lt;/code&gt;. This task will tear down all the infrastructure in Kubernetes and free up the resources that were being used. Additional steps can be added to a teardown workflow but the &lt;code inline&gt;remove_environment&lt;/code&gt; is a requirement.</p><p id="""">Now that we’ve had a quick runthrough of parts of our Application Template, we’ll click Next Step to move on.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1306px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1306px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/649c43fefe1f58472272aef0_9npWbpyLtdFE5znNVTpwbngC62BhjU6tNHteuox0x-VKqdYZ3CVzmB-AAqLoJ6Ru6Hc0iWgbY0UE2m_mb3yY0WrIHnT3-NZollLe5iqIEp0MeQOeUiyoSMCToDakKS0csyYwBPOKj1bO6YPPdXi-yC4.png"" id="""" width=""auto"" height=""auto"" loading=""auto"" alt=""""></div></figure><p id=""""><strong id="""">Step 4: Set Environment Variables </strong></p><p id="""">Here we are presented with the Environment Variables that Release was able to extract from the compose file. If we wanted to add additional variables here we could but for this Application we won’t need any more so we’ll click Next Step.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1308px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1308px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/649c43ff157dcc572cf4432d_qaUX3DZQVHECkbEN-4inwLXhU6i3RDgGGZeZoLFeu1LInL_Y_Aa8ymOM8C2sxvqLQuv9kiZcC2Lln__wMxahbqsjNHWrze9FqBN3qlOU3pr2pLOmQRFyj62esIpFq_H7fhXLi5aigeA5rm3fJbv_zPc.png"" id="""" width=""auto"" height=""auto"" loading=""auto"" alt=""""></div></figure><p id=""""><strong id="""">Step 5: Set Build Arguments </strong> <br>At this stage we are presented with the ability to add build arguments if we need to explicitly pass anything into our Docker build. For this Application, the <a href=""https://github.com/awesome-release/rails_postgres_redis/blob/main/Dockerfile"" id="""">Dockerfile</a> accepts a build argument for &lt;code inline&gt;RUBY_VERSION&lt;/code&gt; if we want to use a newer version than the &lt;code inline&gt;3.0.0&lt;/code&gt; default. We’ll add an argument for the &lt;code inline&gt;3.2.0&lt;/code&gt; version. We’ll be able to see this version used when we look at the build in part two of this series. After clicking the check mark to add our build argument, we’ll click Next Step to move on.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1311px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1311px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/649c43ff58648f53ef18cd4a_ld4XE9rojH8HGPgFAbFoO911idTlq3w_rkHlxvB-yQziPXKxmHpisoZFTwRWq43EfdTW-LeU9k1mfM9_IINprEPIqn3VMfMm53JQxqiYK4jBEQ-DT3_1tm-clJ38Xfa7nMjLVVCxD_P8-0bsZThIoSM.png"" id="""" width=""auto"" height=""auto"" loading=""auto"" alt=""""></div></figure><p id=""""><strong id="""">Step 6: save and deploy <br></strong>The final step in creating our Application is to create a deployment. By clicking the “Deploy your app!” button, Release will create an ephemeral environment and start a <strong id="""">setup</strong> workflow. </p><p id="""">That’s it! 🎉 In short six steps we took our Docker Compose file and created an Application Template that we can now use to spin up ephemeral environments on demand. </p><p id="""">In part two of this series, we’ll cover what happens during the deployment, as well as what Kubernetes objects were created. We will also use the ephemeral environment that was created to ensure that all of our Services are up and running. Stay tuned for part two and in the meantime<a href=""https://web.release.com/register""> take Release for a spin</a> and let us know if you have any questions. </p>",false,,,,https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/649c4468fe42982d3b57c290_Creating%20your%20first%20appliaction%20in%20Release%20with%20Docker%20Compose.jpg,,jeremy-kreutzbender,10,Wed Jun 28 2023 18:00:00 GMT+0000 (Coordinated Universal Time),docker; product,
CRV + Release - aligned from the beginning - now with $20M in new fuel,crv-release-aligned-with-20mil-new-fuel,62aa5a70cd5ba27d9d0d718a,62aa5a70cd5ba238480d72e6,Tue Oct 05 2021 01:38:20 GMT+0000 (Coordinated Universal Time),Wed Feb 08 2023 22:14:00 GMT+0000 (Coordinated Universal Time),Wed Feb 08 2023 22:48:51 GMT+0000 (Coordinated Universal Time),"Message from Co-Founder, Tommy McClung, regarding Release's $20M Series A funding round led by CRV.","<p id="""">Today <a href=""https://releasehub.webflow.io/blog/releasehub-20-million-series-a-led-by-crv"" id="""">we’re announcing a $20M Series A funding</a> round led by CRV with participation from Sequoia, Y Combinator, Bow Capital, Artisanal Ventures, SOMA, Amit Agarwal (CPO of DataDog), Chase Gilbert (Built) and Bill Clerico (WePay). This is on the heels of the Seed Round we announced back in April of 2021 that closed in April of 2020.</p><p id="""">While a funding round is just a step along the way, I wanted to take a quick minute and talk about why we, along with our investors, are so excited about what is being built at Release. I’ll do that by talking through the engagement we had with CRV and why we chose them as our Series A partners.</p><p id="""">Back in October of last year, like many entrepreneurs, I received a cold email from CRV saying they were interested in what we’re building and wanted to chat. I decided to ignore the email initially (like any good focused founder would do) but a line in the email stuck out in my head:</p><blockquote id="""">“I've recently been exploring the developer tools space, and Release stood out to me. I think your approach to helping developers create on-demand staging environments is compelling, and would eliminate a lot of unnecessary complexity for developers.”</blockquote><p id="""">It really was the last part of that sentence that kept ringing in my head: “eliminate a lot of unnecessary complexity for developers.” This is precisely the reason Erik, David and I started Release after our time at TrueCar. Building software in 2021 is still incredibly complex for developers. Devs today spend far too much time working on non-value added work. If environments worked for developers instead of against them, more ideas would make their way to the world. From the first interaction with CRV, we were aligned on the problem we’ve set out to solve with Environments as a Service here at Release.</p><p id="""">I decided to reply a couple weeks later and let CRV know we had just raised a Seed round earlier in the year, but we should chat in early 2021. They reached back out and the rest is history.</p><p id="""">From the very first conversations with Brittany Walker, James Green and Murat Bicer, it was obvious CRV had a thesis on our space and had really been doing their research. The questions were insightful and as we got to know each other, it was clear we had an aligned vision for what the future of developing on the cloud will look like. Environments are a manifestation of an application running in a complex cloud ecosystem, and if those environments are easy to reproduce then developers can move faster with higher quality output. Back to the original statement in the email they sent to me, “I think your approach to helping developers create on-demand staging environments is compelling, and would eliminate a lot of unnecessary complexity for developers.” Full alignment from the beginning.</p><p id="""">We’ve been growing like mad here at Release and this round of funding led by CRV will enable us to continue delivering Environments as a Service to companies who value velocity, quality and ensuring developers spend their time on value-added work. Too much time is wasted by organizations messing around with unnecessary complexity.</p><p id="""">If you’re an organization that’s tired of wasting time and you want to deliver quickly for your customers, hit us up at <a href=""mailto:hello@release.com"" id="""">hello@release.com</a> and we can show you how Environments as a Service can enable your organization to deliver like never before.</p><p id="""">Last, I wanted to say how proud I am to work with this team and what we’ve accomplished together so far. Thank you all for your hard work and dedication. More to come from this amazing team (and maybe you?).</p><p id="""">We’re building some amazing technology at Release and helping our customers achieve their missions. We’re heads down making AWS, GCP, Kubernetes, Terraform, Pulumi, RDS, and tons of other great technologies work seamlessly and effortlessly into the developer experience. We build Release with Release, so day to day we see how our solutions can make the lives of developers better. If you’re interested in being a part of the journey let us know! <a href=""https://releasehub.com/company"">We have a ton of open roles.</a><br></p>",false,,,,https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63e41e921fc65bd838a33393_100521%20(1).jpg,Founders pictures with Release Series A funding announcement,tommy-mcclung,3,Tue Oct 05 2021 14:00:00 GMT+0000 (Coordinated Universal Time),,
Cutting Build Time In Half with Docker’s Buildx Kubernetes Driver,cutting-build-time-in-half-docker-buildx-kubernetes,62aa5a70cd5ba27d9d0d718a,62aa5a70cd5ba2302a0d7269,Thu Feb 25 2021 01:40:58 GMT+0000 (Coordinated Universal Time),Fri Apr 21 2023 19:51:40 GMT+0000 (Coordinated Universal Time),Fri Apr 21 2023 20:04:43 GMT+0000 (Coordinated Universal Time),"At Release, environments are our main focus, but we can’t create environments without builds. Recently we undertook a pr","<p id="""">At Release, environments are our main focus, but we can’t create environments without builds. Recently we undertook a project to revisit our build infrastructure and determine if it needed to be upgraded. Build times had become a big factor in our service delivery and we needed a way to improve our customers’ experiences. One of the main areas that we wanted to improve upon was the parallelism of building multiple docker images for a single application.</p><p id="""">The title of the article already spoiled the solution, and the alternative ‘Release Did This One Thing To Cut Their Build Time In Half!’ didn’t quite fly with the rest of the company, but Docker’s new <a href=""https://github.com/docker/buildx"" target=""_blank"" id="""">buildx</a> project fit the bill. First, we’ll cover what our original infrastructure looked like and how long builds on an example project were taking. Then, we’ll describe the changes we made to use buildx and the speed increases we observed.</p><p id="""">Let’s start off with a diagram of what our original infrastructure looked like.<br></p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1270px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1270px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/603dd147c5b0a410671bd4be_release-builder-architecture.png"" alt=""release-builder-architecture"" width=""auto"" height=""auto"" loading=""auto"" id=""""></div></figure><p id="""">As you can see, the requests for builds would flow into our main Rails application and then divvied out to the different builder instances through Sidekiq. The builder container is Ruby code that would authenticate to Github, clone the repository, check out the correct SHA, and then execute the docker build. Due to the way we built the authentication to pull the code from Github, a single builder container could only clone one repository at a time. Which meant that the container could only do a single build request at a time. We added threading in the Ruby code to be able to execute multiple docker build commands at a time, but the number of builder containers we had spun up limited our concurrent builds. While it’s not hard to horizontally scale with Kubernetes, we saw this authentication setup as a major bottleneck.</p><p id="""">Another issue we encountered was that we had no mechanism for attempting to place builds on servers where they had been previously built, instead opting for grabbing the first free server. This meant there was very little chance to land on the same server and get the full benefit of Docker caching. While this isn’t a deal breaker for us, we still believed we could do better when creating the version of our build infrastructure. Enough of the theoretical, let’s actually build something!</p><p id="""">Release Applications can contain many docker images and one of our favorite example repositories to showcase this is our fork of <a href=""https://github.com/awesome-release/release-example-voting-app"" target=""_blank"" id="""">example-voting-app</a>. Looking at the <a href=""https://github.com/awesome-release/release-example-voting-app/blob/master/docker-compose.yml"" target=""_blank"" id="""">docker-compose</a> we see that there are 3 different Docker images that we have to build, result, vote, and worker. Now that we have an understanding of Release’s original infrastructure and the application we want to build, let’s start up a fresh build and see the results.</p><p id=""""><em id="""">NOTE</em> I forked the awesome-release repo to my own Github, jer-k for the following results.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:2796px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""2796px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/603dd147c5b0a4318e1bd4ba_uncached-release.png"" alt=""uncached-release"" width=""auto"" height=""auto"" loading=""auto"" id=""""></div></figure><p id="""">We can see that this brand new build with no cache hits took two minutes and 15 seconds to complete. Next, we want to make a few changes to ensure that each Docker image needs to be rebuilt. The changes are listed below.</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-ruby"">
git status
On branch release_builders
Changes to be committed:
  (use ""git restore --staged <file>..."" to unstage)
    modified:   result/views/index.html
    modified:   vote/app.py
    modified:   worker/src/main/java/worker/Worker.java
</code>
</pre></div><p id="""">For the purpose of this blog post, I ensured the following build ran on the same builder as the first and that we will have cache hits. As noted before, this wasn’t always the case in our production environment.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:2784px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""2784px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/603dd147c5b0a492ea1bd4bc_cached-release.png"" alt=""cached-release"" width=""auto"" height=""auto"" loading=""auto"" id=""""></div></figure><p id="""">The caching helps and cuts 45 seconds off the build! The uncached build took almost twice as long as the second build with caching, but our assumption was that we could do a lot better (cached and uncached) with some new technology.</p><h3 id="""">Enter Docker’s Buildx Kubernetes Driver</h3><p id="""">One of the first things we wanted to solve was the concurrency issue and we set out to ensure that Docker itself was able to handle a larger workload. We came across the issue <a href=""https://github.com/moby/moby/issues/9656"" target=""_blank"" id="""">Concurrent “docker build” takes longer than sequential builds</a> where people were describing what we feared; Docker slowed down when many builds were being run at the same time. Lucky for us, that issue was opened in 2014 and plenty of work had been done to resolve this issue. The final comment, by a member of the Docker team, was <a href=""https://github.com/moby/moby/issues/9656#issuecomment-610476810"" target=""_blank"" id="""">“Closing this. BuildKit is highly optimized for parallel workloads. If you see anything like this in buildkit or buildkit compared to legacy builder please report a new issue with a repro case.”</a> Thus we set out to learn more about <a href=""https://docs.docker.com/develop/develop-images/build_enhancements/"" target=""_blank"" id="""">BuildKit</a> (the Github repository is located <a href=""https://github.com/moby/buildkit"" target=""_blank"" id="""">here</a>). While researching, we came across <a href=""https://github.com/docker/buildx"" target=""_blank"" id="""">buildx</a>, which ended up having three key features we believed would resolve many of our issues. These three features were the <a href=""https://github.com/docker/buildx#buildx-bake-options-target"" target=""_blank"" id="""">bake</a> command, the <a href=""https://github.com/docker/buildx#--driver-driver"" target=""_blank"" id="""">buildx kubernetes driver</a>, and the ability for the Kubernetes driver to consistently send builds to the same server. Let’s cover each of these, first up the bake command.</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-none"">
buildx bake [OPTIONS] [TARGET...]
Bake is a high-level build command.

Each specified target will run in parallel as part of the build.
</code>
</pre></div><p id="""">bake intrigued us because it seemed to be a built-in command for us to avoid using Ruby threading for our parallelism. bake takes an input of a file, which can either be in the form of a docker-compose, .json, or .hcl. We initially tested bake with the docker-compose from example-voting-app and we were blown away at how smoothly it built directly out of the box and how quickly it was able to build the three images! However, we opted to create our own .json file generator in Ruby, parsing our <a href=""https://releasehub.com/blog/cutting-build-time-in-half-docker-buildx-kubernetes"" id="""">Application Template</a> into an output. Here is our generated file for example-voting-app.</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-json"">
{
  ""group"": {
    ""default"": {
      ""targets"": [
        ""vote"",
        ""result"",
        ""worker""
      ]
    }
  },
  ""target"": {
    ""vote"": {
      ""context"": ""./vote"",
      ""tags"": [
        ""<redacted>.dkr.ecr.us-west-2.amazonaws.com/jer-k/release-example-voting-app/vote:latest"",
        ""<redacted>.dkr.ecr.us-west-2.amazonaws.com/jer-k/release-example-voting-app/vote:buildx-builders""
      ]
    },
    ""result"": {
      ""context"": ""./result"",
      ""tags"": [
        ""<redacted>.dkr.ecr.us-west-2.amazonaws.com/jer-k/release-example-voting-app/result:latest"",
        ""<redacted>.dkr.ecr.us-west-2.amazonaws.com/jer-k/release-example-voting-app/result:buildx-builders""
      ]
    },
    ""worker"": {
      ""context"": ""./worker"",
      ""tags"": [
        ""<redacted>.dkr.ecr.us-west-2.amazonaws.com/jer-k/release-example-voting-app/worker:latest"",
        ""<redacted>.dkr.ecr.us-west-2.amazonaws.com/jer-k/release-example-voting-app/worker:buildx-builders""
      ]
    }
  }
}
</redacted></redacted></redacted></redacted></redacted></redacted></code>
</pre></div><p id="""">There are other inputs which can make their way into this file, such as build args, but since example-voting-app does not have any, they are omitted.</p><p id="""">Next, we wanted to find more information on the Kubernetes driver and we found this blog post <a href=""https://medium.com/nttlabs/buildx-kubernetes-ad0fe59b0c64"" target=""_blank"" id="""">Kubernetes driver for Docker BuildX</a> from the author of the <a href=""https://github.com/docker/buildx/pull/167"" target=""_blank"">Pull Request</a>. We encourage you to read the latter as it covers getting up and running with the Kubernetes driver as well how the caching works, which is exactly what we needed. With that information in hand, we were able to start work on adding the buildx servers to our cluster. We created a generic way to deploy the servers into different clusters and adjust the number of replicas with the final command being</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-none"">
docker buildx create --name #{name} --driver kubernetes --driver-opt replicas=#{num_replicas},namespace=#{builder_namespace} --use
</code></pre></div><p id="""">For us, we created a release-builder namespace with five replicas, in our development cluster. We can see the output by querying for the pods</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-none"">
kubectl get pods --namespace=release-builder
NAME                            READY   STATUS    RESTARTS   AGE
development0-86d99fcf46-26j9f   1/1     Running   0          6d10h
development0-86d99fcf46-5scpq   1/1     Running   0          6d13h
development0-86d99fcf46-jkk2b   1/1     Running   0          15d
development0-86d99fcf46-llkgq   1/1     Running   0          18d
development0-86d99fcf46-mr9jt   1/1     Running   0          20d
</code></pre></div><p id="""">Since we have five replicas, we wanted to ensure that when we build applications, they end up on the same server so that we get the greatest amount of caching possible (distributed caching is a topic for another day). Luckily for us, buildx, with the Kubernetes driver, has an option for where to send the builds called loadbalance.<br>The default sticky means that the builds should always end up on the same server due to the hashing (more detailed information on this is described in the aforementioned blog post). With all of that in place, we are ready to test out our new setup!</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-none"">
loadbalance=(sticky|random) - Load-balancing strategy. 
If set to ""sticky"", the pod is chosen using the hash of the context path. Defaults to ""sticky""
</code>
</pre></div><p id=""""></p><p id="""">Using the same example-voting-app repository as before, I created a new branch buildx_builders and pointed the code to the buildx servers.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:2782px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""2782px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/603dd147c5b0a43e171bd4bd_uncached-buildx.png"" alt=""uncached-buildx"" width=""auto"" height=""auto"" loading=""auto"" id=""""></div></figure><p id="""">What we see is that this uncached build was more than twice as fast as the other uncached build and even faster than the cached build on the old infrastructure! But uncached builds should be a thing of the past with the sticky load balancing, so let’s make the same changes as the previous branch and see the results.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:2774px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""2774px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/603dd147c5b0a415341bd4bf_cached-buildx.png"" alt=""cached-buildx"" width=""auto"" height=""auto"" loading=""auto"" id=""""></div></figure><p id="""">This build finished three times faster than the previous cached build! These types of speed increases are the reason we set out to redo our build infrastructure. The faster the builds complete, the faster we can create environments and help our customers deliver their products.</p><p id="""">We’re still experimenting with buildx and learning as we go, but the initial results were more than enough for us to migrate our own production builds to the new infrastructure. We’re going to continue to blog about this topic as we learn more and scale so check back in with the Release blog in the future!</p>",false,,,,https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63e3fcfea0280343b140cc02_021721%20(1).jpg,A red and an orange container,jeremy-kreutzbender,5,Thu Feb 18 2021 00:00:00 GMT+0000 (Coordinated Universal Time),,
Development Environments and how to Manage Them,development-environments-and-how-to-manage-them,62aa5a70cd5ba27d9d0d718a,63d77e13c043ed1dd88aa29c,Mon Jan 30 2023 08:21:39 GMT+0000 (Coordinated Universal Time),Mon Jan 30 2023 08:23:10 GMT+0000 (Coordinated Universal Time),,A development environment is a set of processes and tools to develop and maintain software. Read on to learn more.,"<p id="""">Different environments are used at different stages of a software project's development cycle. Each stage has a unique purpose and contributes to the software's stability and reliability.</p><p id="""">One of the stages in a software project's development cycle is the development phase, which uses the development environment. This environment contains various processes and programming tools for developing the project's source code and everything needed to maintain and scale the project.</p><p id="""">This post will look at the importance of development environments and how to manage them.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-center"" data-rt-type=""image"" data-rt-align=""center""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63d77db2fd62474dc2429ea6_4TAiQKuYi5aqmYlA84sOsskIddJAqHQhvkHIU-fgsW7QQb4IvzS1OgcvTDAcuKOuEnr7dzBff-BimOik8M_SibWHE3Szg7xOo108k1kASfoCCEs8LIialYCDAIdtqIrVh4PYOgJiBkRGchlnfa_7YHU2MQ-ga96jhp6rWVsNHskykqVNgV3UQ7SbvIEA.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><h2 id="""">What is a Development Environment?</h2><p id="""">A development environment is a set of processes and tools to develop and maintain software. It includes the entire environment that supports the development process, from writing and testing the source code to debugging, patching, and updating the project.</p><p id="""">Every project uses this environment, especially long-term, extensive software maintenance and configuration management.</p><h2 id="""">What is the use of Development Environments?</h2><p id="""">Working in a development environment allows developers to create, innovate, and test code without disrupting the user experience. The development environment also helps reduce costs and improve safety and privacy by enabling developers to work with simulated dependencies instead of real services, which may raise security or privacy concerns.&nbsp;</p><p id="""">Ultimately, the dev environment is used to build your application. This environment allows developers to create better code more efficiently and safely by automating or facilitating the routines involved in software development and maintenance.</p><p id="""">All of these ensure that nothing breaks in a live environment.</p><h2 id="""">Types of Development Environments</h2><p id="""">Developers can use several types of development environments to create and maintain software. These include:</p><ul id=""""><li id=""""><strong id="""">Local development environment</strong>: A development environment set up on a local computer or server. This environment allows developers to work on code without affecting the user experience or production environment. One advantage of this development environment is that it doesn't require an internet connection. However, it can be disadvantageous because of its limited resources and difficulty collaborating with team members.</li><li id=""""><strong id="""">Virtual development environment</strong>: A dev environment created using virtualization software. This type of environment allows developers to set up an isolated environment for development and testing purposes. Some advantages of this development environment include its ease of set up and configuration, and how it can mimic the production environment. On the other hand, a virtual development environment needs a host operating system and has the potential for slower performance compared to a local development environment.</li><li id=""""><strong id="""">Cloud-based development environment</strong>: A development environment hosted on a cloud computing platform. Developers can access a cloud-based development environment from anywhere with an internet connection. This type of development environment has some benefits, such as facilitating simple collaboration and not requiring local resources. It also has drawbacks, including shared cloud space and potential security issues.</li><li id=""""><strong id="""">Integrated development environment (IDE)</strong>: IDEs are programming tools with a code editor, debugger, and other features to help developers write and test code more efficiently. Some IDEs have a development environment, but others are used in conjunction with a separate development environment. The ability to streamline the dev process and the tooling and feature range this environment offers are some of its perks. However, it's only suitable for some projects and has a steep learning curve.</li></ul><h2 id="""">How to Manage a Development Environment</h2><p id="""">As a software developer, you know the importance of having a well-managed and optimized dev environment. You also understand how running and testing application code in a well-maintained environment is essential. To achieve this, you need to follow some best practices.</p><ul id=""""><li id="""">First, define the components and configuration of your development environment. This involves identifying and specifying all the hardware, software, and dependencies needed for your project. You should also document any customizations or specific settings relevant to your project.&nbsp;</li><li id="""">Automating the set-up process will save you a lot of time and effort, allowing you to quickly and easily set up a development environment with all the necessary components and configurations. Tools like Ansible can help you automate the configuring of software and dependencies. In contrast, containerization tools like Docker enable you to package your application and its dependencies into a self-contained unit that can be easily deployed and run in different environments.</li><li id="""">Host your development environment on a cloud platform like Amazon Web Services (AWS) or Google Cloud Platform (GCP) and use an on-demand environment-as-a-service platform like<a href=""https://releasehub.com/"" id=""""> Release</a>. These platforms help with scaling and improve team collaboration.</li><li id="""">Use version control systems like Git to track and manage changes to your code and environment. Version control is handy, especially when working with a team, as it drives collaboration by keeping track of who made what changes.</li><li id="""">Use IDEs because they simplify the development process by providing tools for writing and debugging code more effectively.</li><li id="""">Regularly test and debug your development environment so applications meet your users' needs. By doing this, you can ensure smooth-running and bug-free applications post-deployment. We have a <a href=""https://releasehub.com/blog/setup-test-environment"" id="""">comprehensive guide</a> on setting up easy-to-maintain test environments.</li><li id="""">Keep detailed documentation for your development environment. It provides an understanding of your environment configuration and simplifies troubleshooting.</li></ul><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-center"" data-rt-type=""image"" data-rt-align=""center""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63d77db2737d365ec6af2118_1HtEDNuCCI2jU3X6PLSXnz3Unv4n0Jz4Qpdj1ILZVWIW5fRLKJZXj4a2GLxvurLjXlT0q2L3i0S6BVI_OV_11mHas_WKyic_omk951npg8FHBnGIC_Ab6JjqBlasPiNySv2u6HM3geQ7fZjG5iKpb3QKp07UG3gSgkGWGIk0-abgiwthyijrIewMw6Ih.jpeg"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><h2 id="""">Development Environments vs. Testing and Staging Environments</h2><p id="""">Development environments are workspaces for software developers to create, run, and test their application code in a simulated environment. Here, codes are written and tested before being deployed to a production environment, where end users will access the application.</p><p id="""">Testing and staging environments, on the other hand, are different.</p><p id="""">Testing environments validate the functionality and performance of an application. They're typically used to catch bugs and defects not detected during development. While they can be separate from the dev environment, sometimes they're just a copy of the development environment with additional testing tools and resources.</p><p id="""">A separate testing environment from the dev environment allows developers to focus on writing and debugging code without worrying about the impact on testing efforts. In addition, it helps to ensure that the testing process is consistent and thorough.</p><p id="""">However, the critical point is that the testing environment has more resources or different configurations to simulate better real-world scenarios. Thus, issues are more apparent here.</p><p id="""">Staging environments test, validate changes, and update changes to the application before it's deployed to the production environment. For example, you might make changes to the code in your dev environment and then deploy those changes to the staging environment to test them.</p><p id="""">The staging environment should be a replica of the production environment, so you can be confident that the changes will work correctly in the live environment.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-center"" data-rt-type=""image"" data-rt-align=""center""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63d77db2a7491d92e48c2785_7RV4Os78NrnOdD6ku-vzEdkdoO8bZRqOPqRjgnf1MJ0ucg1RbEzAdUM-HrLDf_jsjXHfeeJzputTI2iymLzaVnyQ6g9OxQuC6TlHhJer5jwYATVywbRPTfa3ff17eCQot0ZlrvGZpHTOKI-vW8FknD5jFSBx7n8a2tuJfrxFzOyhtpa1O1JXduI-h6iS.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><h2 id="""">Integration of IDEs and Cloud-Based Development Environments</h2><p id="""">The convergence of IDEs and cloud-based development environments has revolutionized the software development process by providing developers with increased accessibility, flexibility, and efficiency.</p><p id="""">One such convergence can be seen in IDEs now integrated with cloud-based environments. These cloud-based IDEs, like Gitpod, allow developers to access and work on their projects from any location and device.</p><p id="""">Gitpod is an open-source, cloud-based developer platform by Google. This cloud-based IDE offers developers a pre-configured, ready-to-code dev environment directly from their Git repository, thus eliminating the need for local set up and configurations.</p><p id="""">This helps streamline development by providing a web-based, extensible IDE that supports various programming languages and technologies. It also offers other comprehensive features like automatic dependency management with a built-in terminal and command-line interface, which supports debugging and testing.</p><p id="""">Overall, integrating IDEs and cloud-based development environments has enabled organizations to support hybrid development teams of any size and allocate more computing power for high-demand workloads. These tools play a vital role in this convergence.</p><h2 id="""">In a Nutshell</h2><p id="""">Development environments are crucial for the creation and maintenance of software. Because there are various dev environments, it's essential to carefully consider which type of dev environment is best suited for a particular project.&nbsp;</p><p id="""">Proper development environment management is also essential for ensuring smooth and efficient software development. While you can do this by following best practices, the process can slow down, reduce your product velocity, and add bottlenecks in the development cycle.</p><p id="""">That's why<a href=""https://releasehub.com/whitepaper/easy-environments-management"" id=""""> on-demand environment-as-a-service platforms</a> like<a href=""https://prod.releasehub.com/#organizations"" id=""""> Release</a> can come in for easier environment management. With a low maintenance cost and reduced turnaround time for product features, your developers can spend more time optimizing code and less time worrying about downtime.</p><p id="""">Set up a demo with the team today; let's discuss how<a href=""https://releasehub.com/book-a-demo"" id=""""> Release</a> will help your business.</p><h2 id="""">Social Media Blurb</h2><p id="""">Did you know that development environments are crucial for successful software development? This post explores the different types of dev environments and how to manage them effectively. Click to learn more!</p>",false,,,,https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63d77e6832dbbf1dda301c6d_chess.jpeg,,,6,,,
Development vs Staging vs Production: What's the Difference?,development-vs-staging-vs-production-whats-the-difference,62aa5a70cd5ba27d9d0d718a,649af38c34bcc76b7df7ac6b,Tue Jun 27 2023 14:34:52 GMT+0000 (Coordinated Universal Time),Wed Oct 30 2024 19:53:58 GMT+0000 (Coordinated Universal Time),Wed Oct 30 2024 19:53:58 GMT+0000 (Coordinated Universal Time),"A closer look at the development, staging, and production environments of the software development cycle.",,true,"<p>Streamline development, staging, and production environments effortlessly with Release.</p>",https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=staging-production,"<p id="""">The lines between development, staging, and production environments are often blurred. The distinctions may vary depending on many factors, including: </p><ul id=""""><li id="""">the scale of the organization,</li><li id="""">the codebase, or </li><li id="""">whether you're viewing the environment from a product, unit testing, or security standpoint.</li></ul><p id="""">In this post, I use interviews with fellow developers to understand each environment's purpose and how it's distinct from the others. It's particularly challenging to differentiate between the development and staging environments, and some organizations forgo the staging environment altogether. Let’s find out why, and when it makes sense. </p><h3 id=""""><strong id="""">What Is a Development Environment?</strong></h3><p id="""">Generally, the development environment is the first environment developers use to check if all code changes work well with each other. It's known as a “sandbox” for developers to work in. Examples of commonly used integrated development environments (IDEs) are <a href=""https://code.visualstudio.com/"" id="""">Visual Studio Code</a>, <a href=""https://www.eclipse.org/ide/"" id="""">Eclipse,</a> <a href=""https://www.jetbrains.com/"" id="""">JetBrains tools</a>, and many others. Note that historically development environments were based on a developer's laptop—a local machine, but with the emergence of cloud, on-demand computing and <a href=""http://releasehub.com/ephemeral-environments"" id="""">ephemeral environments</a>, those environments are now being deployed in the cloud. </p><p id="""">IDE is where developers’ workflow with code takes place, reloading and debugging aids are enabled here. Also, this environment is where developers can make necessary code changes. In the IDE, approved code can merge with code from other developers working on the same project. </p><p id="""">Developers commonly use this space to experiment and receive feedback on what improvements they can make to their work. Consequently, this environment is the most unstable. It's also the most susceptible to bugs and potentially broken code. But, on the upside, in allowing mistakes to happen, this is the most conducive environment to learn collaboratively and create a standardized process. &nbsp;</p><p id="""">Besides the most commonly known local machine, there are virtual and cloud-based development environments. Your team might use the virtual and cloud-based environments mainly depending on whether multiple platforms and machines are needed to effectively test and run the code they are writing. </p><p id="""">Development environments historically only include a small subset of the entire application and often would lack elements like security, 3rd party APIs and cloud native services. Those would typically be introduced later in the development process and tested in staging. The result, however, turns into frequent rollbacks and bottlenecks in staging. To enable better code quality in development and more frequent release cycles, companies like <a href=""https://release.com/"" id="""">Release</a> came up with <a href=""http://release.com/ephemeral-environments"" id="""">ephemeral environments</a>, a production-like replica that allows developers to properly test their code (i.e shift-left) and isolate bugs to a single branch, while ensuring a smooth merge to staging and production. </p><h3 id="""">What Is a Staging Environment? </h3><p id=""""><a href=""https://release.com/staging-environments"" id="""">Staging environment</a> is the environment where your code is 'staged' prior to being run in front of users so you can ensure it works as designed. The staging environment should mirror production as much as possible. It reflects a production environment not yet exposed to clients, customers, and the general public. </p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1000px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1000px""><div id=""""><img src=""https://cdn.prod.website-files.com/603dd147c5b0a4221d1bd360/6210052390c0089989bce74a_An8E46TO8LdylR9ui9uJLhovcJQve9aUjCtxMnWu_XeTD7tfJK42GwFjYJ1k4Mp68nlZvE2CQSryq61a4HrzkApU6GjFuH-t-Q41gWXTjEqkCO9wZ82bb5ZERobGMSHUy3SCo9w3.png"" id="""" width=""auto"" height=""auto"" loading=""auto"" alt=""""></div></figure><p id="""">This environment is primarily used for system integration testing (SIT) and in-depth manual testing conducted before the client receives code changes. Developers also <a href=""https://www.unitrends.com/blog/development-test-environments"" id="""">perform</a> quality assurance (QA), security testing, chaos testing, alpha testing, beta testing, and end-to-end (E2E) testing in this environment. </p><p id="""">Additionally, <a href=""https://release.com/user-acceptance-testing-with-ephemeral-environments"" id="""">User acceptance testing (UAT)</a> often happens here. In UAT, users can test changes they requested before the new code goes to a production environment. </p><p id="""">How you carry out testing in the staging environment can depend on what programming language you're using. For example, Ruby on Rails doesn't have a mode for staging. Rails developers switch modes to a test environment that they use to run testing tools and debug failures. Technically, the <a href=""https://guides.rubyonrails.org/configuring.html"" id="""">Rails Guide</a> delves into how to customize configurations and initialization on applications. </p><h3 id=""""><strong id="""">Development vs. Staging Environments </strong></h3><p id="""">So, now that you know what development and staging environments are, you're probably wondering if you need both. Ultimately, the answer depends on the size of your organization, appetite for risk and speed of change, and your position on making a tradeoff between slowing down the process for quality and testing versus launching new features quickly. </p><p id="""">Sometimes smaller companies start out with fewer environments. One developer shared, “You just end up with multiple environments as the organization scales up.” &nbsp;</p><p id="""">In some cases organizations with fewer users don't have staging environments. As another developer elaborated: <em id="""">“Instead, we can deploy in a way that 1% of the traffic will go to each one branch and main branch. Then, we can check the monitoring to see if there are differences between the two. When we are certain that at the most we will affect 1% of traffic and everything is fine, we will then proceed with merging the two branches. I think it would be ideal if the continuous integration (CI) and continuous deployment (CD) process were to set up that 1%, then we could verify the results. This is the same as I have seen for verifying front-end changes in continuous integration.” </em></p><h3 id=""""><strong id="""">Is a Staging Environment Necessary? </strong></h3><p id="""">Deploying to staging is safe, because it will not affect users, but is not necessarily effective because you might not test all the features or combinations that end users will be using. The general solution to this problem is to deploy to production as quickly as you can but only enable or test subsets of new features with flags or canary testing. This way you are only risking challenges for a small subset of users, and are able to see the application perform with live traffic in the production environment. </p><p id="""">Developers say they like to see how real traffic works through the codebase and compare this technique to feature flagging. This may eliminate the need for a beta environment. This results in the concept of ""staging"" not being a distinct environment. </p><p id="""">However, developers agree that it's useful to have a separate beta domain to make significant changes. According to <a href=""https://www.atlassian.com/continuous-delivery/principles/feature-flags"" id="""">Atlassian CI/CD</a>, feature flagging allows developers to turn functionality on and off during runtime. That way, you don't need to deploy code at every update. </p><h3 id=""""><strong id="""">What Is a Production Environment? </strong></h3><p id="""">The production environment is the live site complete with performance optimizations. The codebase must be secure, performant, stable, and able to sustain heavy traffic as the client, customers, and public use it. &nbsp;</p><p id="""">There is a common misconception that production is more important than development or staging. Actually, the reverse could be true: development environments could be so critical to the business that they cannot tolerate any downtime at all but production can tolerate some downtime.</p><p id="""">As an example at Truecar and in most other companies I worked at, the website could be broken for some amount of time as long as it came back up relatively quickly. However, if development was down for more than an hour, you could be looking at losing an entire day of developer features for the whole company!</p><p id="""">Regardless of your setup, you should treat production with care, and restrict who updates the production code. Ideally, you won't be building new versions of the codebase for the production environment; it's better to deploy the same builds to the staging environment. </p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1000px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1000px""><div id=""""><img src=""https://cdn.prod.website-files.com/603dd147c5b0a4221d1bd360/62100523851be10a66a95efa_CBuQ6h9CyXcNOEHioeufwEQBEfXhZYtfIXFbYkIek3h9K_JrMYuCNopmZEVNpbWNRIz6NvFcv_YXJwn2sy5UdwyiXiuG7U8fDP0H5K1tNagjtZh13233W5tAdVI4GHx9E3uE3KRz.png"" id="""" width=""auto"" height=""auto"" loading=""auto"" alt=""""></div></figure><p id="""">At this point of the software development lifecycle, the code shouldn't have any bugs or require fixes. To avoid a poor user experience, you should consider it the final product. </p><p id="""">However, you can make urgent fixes in the production environment if needed. In doing so, you can consistently improve upon quality control for product releases, making it easier to keep tabs on new product updates. </p><h3 id=""""><strong id="""">Conclusion </strong></h3><p id="""">Although the development, staging, and production environments converge, they have their own significance in the larger software development lifecycle. The significance of each environment depends on the organization running the system. </p><p id="""">The way a company treats and leverages these environments today differs wildly depending on the organization and its DevOps practices and policies. Sometimes teams within the same organization use these environments in different ways and have different philosophies of what they mean, and how critical they are to the company’s mission. </p><p id="""">From my conversations with individuals who play different roles in the tech industry, I can say the overall development culture is shifting progressively toward promoting new code to all these environments as soon as possible. One developer expressed, ""The idea is that even the smallest code change gets released to production in a matter of minutes, not months."" </p><p id="""">With that in mind, the common goal is that the folks responsible for the software development life cycle want more efficient environments for producing the highest quality codebases. These people continuously strive to find new methods to make that process easier. </p><p id="""">For a better understanding of what environments are and to be inspired about optimizing them, read more about <a href=""https://release.com/staging-environments"" id="""">staging environments</a>, <a href=""https://release.com/ephemeral-environments"" id="""">ephemeral environments</a>, and <a href=""https://release.com/user-acceptance-testing-with-ephemeral-environments"" id="""">UAT</a> with Release ephemeral environments. </p><p id="""">‍</p>",https://cdn.prod.website-files.com/603dd147c5b0a4221d1bd360/649af331ed05a1e3eceea983_Dev%20vs%20Staging%20vs%20Prod.jpg,,regis-wilson,8,Wed Aug 09 2023 17:00:00 GMT+0000 (Coordinated Universal Time),product,
Development vs Staging vs Production: What's the Difference?,development-vs-staging-vs-production-whats-the-difference-na,62aa5a70cd5ba27d9d0d718a,62aa5a70cd5ba2136a0d7308,Fri Feb 18 2022 20:56:25 GMT+0000 (Coordinated Universal Time),Wed Aug 02 2023 21:04:23 GMT+0000 (Coordinated Universal Time),,"We'll take a look at the development, staging, and production environments of the software development lifecycle.","<p>The lines between development, staging, and production environments are often blurred. The distinctions may vary depending on many factors, including:&nbsp;</p><ul><li>the scale of the organization,</li><li>the codebase, or&nbsp;</li><li>whether you're viewing the environment from a product, unit testing, or security standpoint.</li></ul><p>In this post, I use interviews with fellow developers to understand each environment's purpose and how it's distinct from the others. It's particularly challenging to differentiate between the development and staging environments, and some organizations forgo the staging environment altogether.&nbsp;<br></p><p>Let’s get to it.&nbsp;</p><h2>What Is a Development Environment?</h2><p>Generally, the development environment is the first environment developers use to check if all code changes work well with each other. It's known as a “sandbox” for developers to work in. Examples of commonly used integrated development environments (IDEs) are Visual Studio Code and Atom. You should note that historically the development environment is usually on a developer's laptop—a local machine, but with the emergence of cloud, on-demand computing and <a href=""http://releasehub.com/ephemeral-environments"">ephemeral environments</a>, those environments are now being deployed in the cloud.&nbsp;</p><p>In my experience as a developer, the IDE is where developers’ workflow with code reloading and debugging aids are enabled. Also, this environment is where developers can make necessary code changes. In the IDE, approved code can merge with code from other developers working on the same project.&nbsp;<br></p><p>Developers commonly use this space to experiment and receive feedback on what improvements they can make to their work. Consequently, this environment is the most unstable. It's also the most susceptible to bugs and potentially broken code. But, on the upside, in allowing mistakes to happen, this is the most conducive environment to learn collaboratively and create a standardized process. &nbsp;<br></p><p>Furthermore, there are three environments within the development environment, <a href=""https://www.indeed.com/career-advice/career-development/development-environment"">according to the Indeed Editorial Team</a>. Besides the most commonly known local machine, there are virtual and cloud-based development environments. The reason for the virtual and cloud-based environments mainly depends on whether multiple platforms and machines are used.&nbsp;<br></p><p>Development environments historically would only include a small subset of the entire application and many times would lack elements like security, 3rd party APIs and cloud native services. Those would typically be introduced later in the development process and tested in staging. The results, however, would be frequent rollbacks and bottlenecks in staging. To enable better code quality in development and more frequent release cycles, companies like <a href=""https://releasehub.com"">Releasehub</a> came up with <a href=""http://releasehub.com/ephemeral-environments"">ephemeral environments</a>, a production-like replica that allows developers to properly test their code (i.e shift-left) and isolate bugs to a single branch, while ensuring a smooth merge to staging and production.&nbsp;</p><h2>What Is a Staging Environment? </h2><p>According to <a href=""https://releasehub.com/staging-environments"">Releasehub</a>, “A staging environment is the environment where your code is 'staged' prior to being run in front of users so you can ensure it works as designed.”&nbsp;<br></p><p>The staging environment should mirror production as much as possible. It reflects a production environment not yet exposed to clients, customers, and the general public.&nbsp;</p><figure class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:512px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""512px""><div><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/621005a0309890aba419e1f0_development%20staging%20production%20PQ01.png"" loading=""lazy"" width=""auto"" height=""auto"" alt=""development staging production PQ01""></div></figure><p>This environment is primarily used for system integration testing (SIT) and in-depth manual testing conducted before the client receives code changes. According to <a href=""https://www.unitrends.com/blog/development-test-environments"">Unitrends</a>, other tests developers perform in this environment are quality assurance (QA), security testing, chaos testing, alpha testing, beta testing, and end-to-end (E2E) testing.&nbsp;<br></p><p>Additionally, <a href=""https://releasehub.com/user-acceptance-testing-with-ephemeral-environments"">user acceptance testing (UAT)</a> often happens before production. In UAT, users can test changes they requested before the new code goes to a production environment.&nbsp;<br></p><p>How you carry out testing in the staging environment can depend on what programming language you're using. For example, Ruby on Rails doesn't have a mode for staging. Rails developers switch modes to a test environment that they use to run testing tools and debug failures. Technically, the <a href=""https://guides.rubyonrails.org/configuring.html"">Rails Guide</a> delves into how to customize configurations and initialization on applications.&nbsp;</p><h2>Development vs. Staging Environments&nbsp;</h2><p>So, now that you know what development and staging environments are, you're probably wondering if you should need both. Truth be told, the answer depends on the size of your organization.&nbsp;<br></p><p>Sometimes smaller companies start out with fewer environments. One developer I interviewed shared, “You just end up with multiple environments as the organization scales up.” &nbsp;<br></p><p>There are cases in which organizations with fewer users don't have staging environments. As another developer I interviewed elaborated at length,&nbsp;</p><p><br></p><blockquote><em>“Instead, we can deploy as such that 1% of the traffic will go to each one branch and main branch. Then, we can check the monitoring to see if there are differences between the two. When we are certain that at the most we will affect 1% of traffic and everything is fine, we will then proceed with merging the two branches. I think it would be ideal if the continuous integration (CI) and continuous deployment (CD) process were to set up that 1%, then we could verify the results. This is the same as I have seen for verifying front-end changes in continuous integration.”&nbsp;</em></blockquote><h3>Is a Staging Environment Necessary?&nbsp;</h3><p>It's better to merge and deploy code quickly rather than have a staging environment where untested code might linger. Using CI/CD in testing can have many benefits that may prevent problems. Tommy McClung, the former CTO of Truecar discussed his experience with staging environments <a href=""https://www.bigmarker.com/techwell-corporation/Beyond-K8s-Introduction-to-Ephemeral-Environments?utm_bmcr_source=TWComm"">here</a>, .&nbsp;<br></p><p>One benefit is that it balances out real input: you risk a bad result for only a small amount of your live traffic while you're able to get the most accurate test results.&nbsp;<br></p><p>Developers say they like to see how real traffic works through the codebase and compare this technique to feature flagging. This may eliminate the need for a beta environment. This results in the concept of ""staging"" not being a distinct environment.&nbsp;<br></p><p>However, developers agree that it's useful to have a separate beta domain to make significant changes. According to <a href=""https://www.atlassian.com/continuous-delivery/principles/feature-flags"">Atlassian CI/CD</a>, feature flagging allows developers to turn functionality on and off during runtime. That way, you don't need to deploy code at every update.&nbsp;</p><h2>What Is a Production Environment? </h2><p>The production environment is the live site complete with performance optimizations. The codebase must be secure, performant, stable, and able to sustain heavy traffic as the client, customers, and public use it. &nbsp;<br></p><p>Therefore, you must treat it with great care. You should restrict who updates the production code. Ideally, you won't be building new versions of the codebase for the production environment; it's better to deploy the same builds to the staging environment.&nbsp;</p><figure class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:512px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""512px""><div><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/62100663576f05031e66e5cb_development%20staging%20production%20PQ02.png"" loading=""lazy"" alt=""development staging production PQ02"" width=""auto"" height=""auto""></div></figure><p>At this point of the software development lifecycle, the code shouldn't have any bugs or require fixes. To avoid a poor user experience, you should consider it the final product.&nbsp;</p><p>However, you can make urgent fixes in the production environment if needed. In doing so, you can consistently improve upon quality control for product releases, making it easier to keep tabs on new product updates.&nbsp;</p><h2>Conclusion <br></h2><p>Although the development, staging, and production environments converge, they have their own significance in the larger software development lifecycle. The significance of each environment depends on the organization running the system.&nbsp;<br></p><p>The way a company treats and leverages these environments today differs wildly depending on the organization and its DevOps practices and policies. Sometimes teams within the same organization use these environments in different ways and have different philosophies of what they mean.&nbsp;<br></p><p>From my conversations with many individuals that play different roles in the tech industry, I can say the overall development culture is shifting progressively toward promoting new code to all these environments as soon as possible. One developer expressed, ""The idea is that even the smallest code change gets released to production in a matter of minutes, not months.""&nbsp;<br></p><p>With that in mind, the common goal is that the folks responsible for the software development life cycle want more efficient environments for producing the highest quality codebases. These people continuously strive to find new methods to make that process easier.&nbsp;<br></p><p>For a better understanding of what environments are and to be inspired about optimizing them, read more about <a href=""https://releasehub.com/staging-environments"">staging environments</a>, <a href=""https://releasehub.com/ephemeral-environments"">ephemeral environments</a>, and <a href=""https://releasehub.com/user-acceptance-testing-with-ephemeral-environments"">UAT</a> with release ephemeral environments.&nbsp;<br></p>",false,,,,https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/6210079e75f2c919d9a0e56c_development.jpg,,nuryani-asari,,,,
DevOps Metrics That Actually Matter,devops-metrics-that-actually-matter,62aa5a70cd5ba27d9d0d718a,62aa5a70cd5ba2e3ee0d7301,Wed Jan 26 2022 20:34:28 GMT+0000 (Coordinated Universal Time),Fri Apr 21 2023 19:27:36 GMT+0000 (Coordinated Universal Time),Fri Apr 21 2023 19:37:52 GMT+0000 (Coordinated Universal Time),"In this post, we'll take a look at DevOps metrics and cover some of the most important metrics.","<p id="""">DevOps is one of the greatest cultural shifts the IT industry has ever had. It's a set of practices that brings together the development and operations teams to deliver high-quality products and services in a more efficient, faster way.&nbsp;<br></p><p id="""">A large number of organizations have shifted to the DevOps culture because they realize the benefits of DevOpss. But getting the best out of DevOps isn't as easy as installing software and getting results. This is because DevOps is not just a tool but also a mindset.&nbsp;</p><h3 id="""">Measuring Change</h3><p id="""">Once organizations start practicing DevOps, the next step is to assess how this shift is performing. It's very difficult to know the amount of transformation a change has brought without measuring it. Therefore, it's important to measure the change, and that's where metrics come in.&nbsp;<br></p><p id="""">In this post, we'll focus on various DevOps metrics, how they help measure DevOps transformation, and how tools can help. But before getting to that, let's try to understand why it's difficult to measure the outcomes of a DevOps team.&nbsp;</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:512px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""512px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/61f1b2ebed16f04bbd534cc8_devops%20metrics%20pq01.png"" loading=""lazy"" alt=""DevOps Metrics Pull quote"" width=""auto"" height=""auto"" id=""""></div></figure><h3 id="""">Why Can It Be Hard to Measure Outcomes of DevOps?</h3><p id="""">To make this easy to understand, I'll first give an example of something that's easy to measure and then compare it with measuring a DevOps outcome.&nbsp;<br></p><p id="""">Let's say you want to measure your return on investment in the stock market. This is straightforward. You know the price you bought a stock for, and you know the price you sold it for. So, it's easy to calculate your ROI.&nbsp;<br></p><p id="""">Now, let's step it up a bit. How would you measure your success in the past year?&nbsp;<br></p><p id="""">Clearly, this is a bit more difficult than the previous example. First, there are multiple factors here. Success means different things to different people. It can involve salary, promotions, appreciation, revenue, and so on. Secondly, not all of these factors are quantitative.&nbsp;<br></p><p id="""">DevOps is even more complicated than the second example. There are a great number of factors and variables in the organization to measure the ROI or outcome. Also, most of these factors aren't quantitative. That's why it can be hard to measure the outcomes of a DevOps team.&nbsp;<br></p><p id="""">Nevertheless, you can still use some metrics to measure your DevOps success. And to make things easier, you can also use some <a href=""https://releasehub.com/"" id="""">tools</a>.&nbsp;</p><p id=""""><strong id="""">DevOps Metrics That Actually Matter</strong></p><p id="""">Depending on what changes you've made and what you want to measure, you can select various metrics for your use case. But for this post, we'll stick to the ones that are universal. We're going to refer to <a href=""https://services.google.com/fh/files/misc/state-of-devops-2021.pdf"" id="""">DORA's State of the DevOps Report</a> and the <a href=""https://queue.acm.org/detail.cfm?id=3454124"" id="""">SPACE framework</a> for these metrics.&nbsp;</p><h3 id="""">DORA's State of the DevOps Report</h3><p id="""">DORA's State of the DevOps Report is the result of years of research. It addresses capabilities and practices that drive software delivery, operational performance, and organizational performance. This report talks about four major metrics that you can use to measure DevOps outcomes.&nbsp;</p><h4 id="""">Metric 1: Deployment Frequency</h4><p id="""">One of the main pillars of DevOps is <a href=""https://continuousdelivery.com/"" id="""">continuous delivery</a>. And deployment frequency is a direct measure of continuous delivery.&nbsp;<br></p><p id="""">Deployment frequency means how often code deploys. Code deployment can be bug fixes, new features, updates for performance, configuration changes, and so on. Measuring deployment frequency helps you understand how smooth the workflow of your team is. Lower deployment frequency or frequent delays are indicators of problems in the workflow. By measuring this metric, you can improve the process to ensure a smoother workflow.&nbsp;<br></p><p id="""">As a side note, I found that on-demand readily available <a href=""http://releasehub.com/ephemeral-environments"" id="""">ephemeral environments</a> that replicate production as key to improving release frequency. The code quality the developer produces is higher and the likelihood that it will require rollback during the release process is lower&nbsp;</p><h4 id="""">Metric 2: Lead Time for Change</h4><p id="""">Lead time for change means how long the team takes to get the committed code to run in production. This metric is a measure of the performance, productivity and skills of the team. High lead times can occur when the change is too large, but it can also be an indication of performance.<br></p><p id="""">According to DORA's research, low performers reported lead time for change to be more than six months. In contrast, elite performers reported it to be less than an hour! So, this metric can help identify a lack of skills and experience. It makes sense to use this method to prioritize training and skill improvement.&nbsp;<br></p><p id="""">More on how to improve developers productivity <a href=""https://releasehub.com/blog/increase-developer-velocity-by-removing-environment-bottlenecks"" id="""">here</a>.</p><h4 id="""">Metric 3: Change Failure Rate</h4><p id="""">Change failure rate is the percentage of deployments to production that have resulted in degraded services. Degraded can mean bugs, outages, performance problems, and so on. Change failure rate is a metric that represents the efficiency of the deployment process. A high value of this metric indicates that there's a serious problem in the deployment process, and it needs immediate attention.&nbsp;</p><h4 id="""">Metric 4: Mean Time to Restore Service</h4><p id="""">Mean time to restore service is the average time a team takes to recover from a production failure. This metric helps you understand how efficient your team is in recovering from an incident.&nbsp;<br></p><p id="""">There are a lot of factors involved in recovering from an incident, from the point of detecting a failure to troubleshooting it and finding a fix or rolling back. Use this to determine the efficiency, skills, and stability of your DevOps team.&nbsp;<br></p><p id="""">So there you have it: the four main metrics used to measure the performance and efficiency of DevOps teams. Let's look at this process from another angle, though.&nbsp;<br></p><h3 id="""">SPACE Framework</h3><p id="""">The SPACE framework focuses on the <a href=""https://releasehub.com/blog/improving-developer-productivity-with-ephemeral-environments"" id="""">productivity</a> of teams. SPACE stands for satisfaction, performance, activity, communication and collaboration, and efficiency. Let's look at these factors one by one.&nbsp;</p><h4 id="""">SPACE Metric 1: Satisfaction</h4><p id="""">Satisfaction is the value teams get from their work. If a person doesn't like what they're doing, then it's obvious that their productivity will drop down. This metric helps you understand the alignment of an employee's interest with the plan of the organization or manager for that employee.&nbsp;</p><h4 id="""">SPACE Metric 2: Performance</h4><p id="""">This metric is straightforward and helps measure how well a team is functioning. The SPACE framework suggests measuring performance by outcomes rather than by output of tasks. The main reason for this is that a team's task is difficult to score as it's a combination of quality and quantity.&nbsp;</p><h4 id="""">SPACE Metric 3: Activity</h4><p id="""">Activity is the count of actions a team has performed or the number of outputs a team has produced. You can use this metric to measure the productivity and efficiency of a team to a certain extent. (Why only to a certain extent? Well, the tasks of a DevOps team are difficult to quantify.)&nbsp;</p><h4 id="""">SPACE Metric 4: Communication and Collaboration</h4><p id="""">For an organization to function smoothly and produce high-quality outputs, it's important for teams and team members to communicate and collaborate. This is especially vital for DevOps teams because different specialists work together. Teams with good understanding and rapport are more productive. And this metric helps you understand how easy (or difficult) it is for teams and team members to work with one another.&nbsp;</p><h4 id="""">SPACE Metric 5: Efficiency</h4><p id="""">Efficiency is the measure of completing a task with minimal delays. This metric is closely associated with productivity. Minimal interruptions and delays help maintain better flow. As a result, productivity also increases.&nbsp;<br></p><p id="""">In addition to the metrics I've discussed in this article, there are many others that help you measure various tasks and processes. But the ones I've listed are enough to give you a high-level general understanding of DevOps outcomes.&nbsp;</p><h3 id="""">The More Metrics You Use, the Bigger the Picture</h3><p id="""">You can measure and derive most DevOps metrics individually. But is it OK to also look at them independently and together and then draw conclusions?&nbsp;<br></p><p id="""">Each metric provides valuable information but not complete information. So to get the whole story, you need to make these metrics work together.&nbsp;<br></p><p id="""">For example, a high value of deployment frequency is a good thing for a team. But if the same team has the highest failure rate, then that changes the overall performance rating of the team. Therefore, you must use all relevant metrics to get the bigger picture.&nbsp;</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:512px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""512px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/61f1b321edf3870005ce30f0_devops%20metrics%20pq02.png"" loading=""lazy"" alt=""DevOps Metrics Pull quote"" width=""auto"" height=""auto"" id=""""></div></figure><h3 id="""">What Does It All Mean?</h3><p id="""">A shift to DevOps is something a lot of organizations have been very excited about. And DevOps metrics are a way to see and interpret results.&nbsp;<br></p><p id="""">Measuring these metrics and correlating them isn't easy. So you might want to consider using tools to collect these metrics, process them, and also visualize them for you.&nbsp;<br></p><p id="""">There are a lot of tools that can help you not only measure the metrics but also improve them. For example, <a href=""http://releasehub.com/company"" id="""">Release</a> allows you to set up production-like environments, including the right dataset, cloud native services, security policies, that are automatically&nbsp; available with each PR and automatically tear down when the PR is merged.. You can learn more about Release's <a href=""http://release.com/use-cases"">use cases</a> or check out the company's <a href=""http://release.com/blog"">blog</a>.&nbsp;<br></p><p id="""">We've discussed some of the most important metrics that almost every organization can use. And you can try out even more metrics to measure specific things as per your use case. Once you know where you're at with the help of these metrics, you can plan a better future.&nbsp;</p><h4 id="""">Additional Resources</h4><ul id=""""><li id=""""><a href=""https://release.com/blog/increase-developer-velocity-by-removing-environment-bottlenecks"">Increase Developer Velocity by Removing Environment Bottlenecks</a></li><li id=""""><a href=""https://release.com/ephemeral-environments"">What is an Ephemeral Environment?</a></li></ul>",false,,,,https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63e3fb859948b7a7925e8197_012722%20(1).jpg,Devops metrics that matter,omkar-hiremath,6,Thu Jan 27 2022 16:33:00 GMT+0000 (Coordinated Universal Time),,
Docker GenAI Stack Now Available as Release.ai Template,docker-genai-stack-now-available-as-release-ai-template,62aa5a70cd5ba27d9d0d718a,66b2f34d51916d8b30c095fc,Wed Aug 07 2024 04:08:45 GMT+0000 (Coordinated Universal Time),Wed Aug 07 2024 14:57:52 GMT+0000 (Coordinated Universal Time),Wed Aug 07 2024 15:51:32 GMT+0000 (Coordinated Universal Time),"Learn how the Docker-Release collaboration simplifies AI development, from local setup to cloud production. ","<p id="""">Release.ai is an orchestration and infrastructure management platform for custom AI applications. We handle the complex backend work, allowing developers and data scientists to focus on innovation. Our self-service platform provides powerful building blocks that simplify AI development and deployment, enabling teams to create advanced applications more efficiently.</p><p id="""">We are excited to announce the addition of Docker's GenAI Stack to our library of pre-configured templates, simplifying AI application development and deployment. The Docker GenAI Stack serves as a comprehensive template for building Retrieval-Augmented Generation (RAG) applications, allowing developers to quickly set up and experiment with AI-powered systems on their local machines. This collaboration combines Docker's containerization expertise with Release.ai's cloud deployment capabilities, enabling a seamless transition from local development to cloud-based production. With Release.ai, deploying your Docker GenAI Stack to the cloud becomes as straightforward as running it locally, offering developers a streamlined solution for building and scaling AI applications without the complexity typically associated with cloud infrastructure management.</p><p id="""">""By integrating Docker's GenAI Stack, we're helping developers spend more time on innovation and less on infrastructure setup."" explains Tommy McClung, Release founder. </p><p id="""">Key benefits of this collaboration include:</p><ul id=""""><li><strong id="""">Simplified Deployment: </strong>With just a few clicks, developers can create a fully functional GenAI environment, eliminating the need for manual configuration.</li><li><strong id="""">Time Savings:</strong> Early adopters report cutting setup time by up to 50%, allowing teams to focus on application development rather than infrastructure management.</li><li><strong id="""">Standardization: </strong>The template ensures consistency across different projects and teams, improving collaboration and reducing potential issues.</li><li><strong id="""">Scalability: </strong>Release.ai's managed Kubernetes infrastructure provides the ability to easily scale AI applications as needed.</li></ul><p id="""">There are several Docker GenAI Stack templates on Release.ai:</p><ul id=""""><li>Ollama, LangChain, Neo4j, with Open Source Models (default llama3:8b)</li><li>Ollama, LangChain, Chroma, with Open Source Models (default llama3:8b)</li><li>Ollama, LangChain, Chroma, with OpenAI</li></ul><p id="""">Docker and Release partnership represents a significant step forward in democratizing advanced AI technologies. Simplifying the deployment process opens up new possibilities for innovation across various industries. Looking ahead, we plan to expand the collaboration to integrate more AI-focused templates and explore ways to optimize AI workflows further. </p><p id="""">Ready to accelerate your AI development? </p><p id="""">1. Sign up for a free Release.ai account at Release.ai/sign-up</p><p id="""">2. Choose a Docker Gen AI template from our library when creating your application</p><p id="""">3. Launch your AI environment and start building</p><p id="""">In this <a href=""https://www.youtube.com/watch?v=-OdWRxMX1iA"" id="""">demo</a>, Docker GenAI Stack drives our documentation chatbot, showcasing the entire platform and demonstrating how to easily manage new data and changes using the RAG stack of your choice. </p><p id="""">Our sandbox comes with pre-set templates and free compute time to get you started. Don't miss this opportunity to transform your AI development process. Get started with Release.ai and Docker Gen AI today!</p>",false,,,,https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/651b75952eef99312adfdb1d_Release%2BDocker.jpg,,tommy-mcclung,5,Wed Aug 07 2024 20:07:00 GMT+0000 (Coordinated Universal Time),ai; docker; news,introducing-release-share-a-docker-desktop-extension
"DockerCon 2023 Takeaways: AI, Security and DevEx are Top of Mind",dockercon-2023-takeaways-ai-security-and-devex-are-top-of-mind,62aa5a70cd5ba27d9d0d718a,65260b7c9c3008ccee7b5ba3,Wed Oct 11 2023 02:42:04 GMT+0000 (Coordinated Universal Time),Thu Jul 25 2024 15:03:18 GMT+0000 (Coordinated Universal Time),Thu Jul 25 2024 16:27:39 GMT+0000 (Coordinated Universal Time),DockerCon showcased new tools in the Docker ecosystem and emphasized a move to local and cloud environments.,"<p id="""">This year DockerCon returned to an in-person format (along with the online option) and Release traveled to sunny LA to participate. We spent two days listening to talks, learning from peers and presenting our ideas and new products. As we digest the event, here are some standout moments and learnings we're keen to share.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1552px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1552px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/6526c7cbe89e3e6db68b7d88_nic7AnNjxEsgWOcGLusm0stwTZPVDF75Yo6U7czGomKr2fmGWGxHzfXU-Q9ITYOv-DkEcwd9Bopiz6Z7CQsNlgjgn073zv6SCIpGbSTppSYOlG3k4CfEZPJY9thkXSCNZDezszNCw9I5ZXs_4uHgmpI.jpeg"" id="""" width=""auto"" height=""auto"" alt="""" loading=""auto""></div></figure><h3 id=""""><strong id="""">Keynote Takeaways: </strong></h3><p id="""">Each day started with a keynote. Led by the CEO Scott Johnston and CTO Justin Cormack, the talks featured Docker experts, customers and partners, and focused on improvements, accomplishments and new initiatives Docker and the wider community are working towards. The announcements included: </p><p id=""""><a href=""https://www.docker.com/products/docker-scout/"" id=""""><strong id="""">Docker Scout</strong></a><strong id=""""> in GA:</strong> A new tool for analyzing Docker images, highlighting vulnerabilities, and offering fixes, that seamlessly integrates with platforms like Docker Hub and GitHub Actions.</p><p id=""""><strong id="""">Udemy &amp; Docker Collaboration:</strong> A dedicated Docker learning path on Udemy, that includes course discounts and exclusive content.</p><p id=""""><strong id="""">Next Generation Docker Build:</strong> Faster, cloud-assisted Docker image creation. Currently in public beta.</p><p id=""""><strong id="""">Docker Debug:</strong> A tool providing a local debugging feel for remote containerized apps. Also in public beta.</p><p id=""""><a href=""https://www.docker.com/ai-early-access-program/"" id=""""><strong id="""">Docker AI</strong></a>: Context-specific, automated guidance for editing a Dockerfile or Docker Compose file, debugging local ‘docker build,’ or running a test locally. In early access. </p><p id="""">All these new tools and initiatives underscore the idea of combining the best of local and cloud development environments, so that teams can collaboratively, quickly, and securely build, share, and run any app, anywhere. </p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1552px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1552px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/6526c7cb46dfb6905705cc2d_x2a0mn-Z9Q_bBCPIqAzTP3PAN8SgOyTZ4gSoqxVlH4azSvmD1uIeP5UGdx3PO5c28LM3E_IGAdFDHTWvq30Pi1IlZLMJegHR7sium_gemhsAu7ycACpFVGJK2MjUtW4o3HFskX4dmhxvJQWgFiwQDyo.jpeg"" id="""" width=""auto"" height=""auto"" alt="""" loading=""auto""></div></figure><h3 id=""""><strong id="""">Live Session Lineup:</strong></h3><p id="""">This year's sessions were full of insights and learning. From advanced container orchestration techniques to nuanced deployment strategies, the breadth and depth of topics discussed were on-par with what you’d expect from a vibrant Docker community. </p><p id="""">For the visually-inclined, <a href=""https://www.dockercon.com/2023/speaker/895940/aur%C3%A9lie-vache"" id="""">Aurélie Vache</a> shared the <a href=""https://www.dockercon.com/2023/session/1736615/understanding-docker-in-a-visual-way"" id="""">“Understanding Docker in a Visual Way”</a> session and later signed copies of her book. For the practical-minded, a Docker engineer shared <a href=""https://www.dockercon.com/2023/session/1736584/15-tips-tricks-to-improve-your-compose-experience"" id="""">“15 Tips &amp; Tricks to Improve Your Compose Experience”</a>. And for those building an internal platform or surviving a version already created in their organization, a principal software engineer at Docker shared his observations on <a href=""https://www.dockercon.com/2023/session/1736606/things-you-always-wanted-to-know-about-building-platforms"" id="""">“Things You Always Wanted to Know About Building Platforms”</a>. </p><p id="""">Release presented a lightning talk on shared environments and a recording can be found <a href=""https://vimeo.com/872652454?utm_source=blog&utm_medium=blog&utm_campaign=dockercon23"" id="""">here</a> (spoiler alert: isolated on-demand environments win over sharing). &nbsp;</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1552px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1552px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/6526c7cb0b984cdf63904b51_cO2OZuq1zUhQ6RtkYyO9CL_kmg7MXqmVwS9T-k5YGqic3tOhhfhNhmEPwyevUNlcyvuU06smo8oNmpWs3MNoHwHB1pNZBpptakXtLaKuJtXe0MMo7CBlwygbSzpJqhMxeyw2kLSQF3myMr-lMay0rPk.jpeg"" id="""" width=""auto"" height=""auto"" alt="""" loading=""auto""></div></figure><p id="""">If you missed any of the other sessions, stay tuned for the on-demand recordings available on the <a href=""https://www.dockercon.com/2023"" id="""">DockerCon portal</a> soon. </p><h3 id=""""><strong id="""">Sharing Release with the Docker community:</strong></h3><p id="""">Besides sessions, and hallway talks, our team got a chance to showcase Release products in our own booth. The conversations ranged from curious inquiries to in-depth discussions about integration possibilities and specific projects teams are tackling. It was great to share what we built and explore how customers could use it to improve their workflows and DevEx. And collected actionable feedback that we can apply to our product roadmap. </p><p id="""">On-demand environments are still a challenge for some teams. And many folks were impressed with the simplicity and power of the<strong> Release platform</strong>, and how effortlessly it provides isolated environments to every developer and every use case.</p>",true,<p>Take Release for a spin. <br>First 30 days are on us with code #LEARN</p>,https://release.com/signup,"<p id="""">In time for DockerCon, we launched <a href=""https://open.docker.com/extensions/marketplace?extensionId=releasecom/docker-extension"" id=""""><strong>Release Share</strong></a>, our Docker Desktop extension that lets you share your work in progress from your local laptop. Now, any running container can have a customizable URL that you can quickly share and use to test things with your team. The extension is free to use and you can try it <a href=""https://open.docker.com/extensions/marketplace?extensionId=releasecom/docker-extension"" id="""">here</a>. </p><p id="""">Lastly, we had lots of interesting discussions and useful feedback on our <strong>Release AI </strong>offering, including some working notes from the Docker team. Check out what we built so far and see how you can talk to your infrastructure in plain English today. Get your free access at <a href=""http://release.ai/"">release.ai</a> </p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1552px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1552px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/6526c7cb3f5769c2067b53a0_kS_NpQh0cn7OIihEZFUb5Lq3TBXc_Yg3KDO8YwW6279igS2ex2LTWi8XLt5MSZg2aQt8H_C9dzMkg2basQZReFZvYu1I2H7PnNjAn2BIBeQ2C7HJ89yJ_hYkEb2q7TgN84wypC7y2nqQfAmK1WjbZpo.jpeg"" id="""" width=""auto"" height=""auto"" alt="""" loading=""auto""></div></figure><p id="""">As with all conferences, there was a ton to learn, a ton to share and not enough time to do everything. So if you didn't get a chance to talk with us at DockerCon but want to know more about what we do, just send a message at regis_wilson@release.com. We're always ready to chat.</p>",https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/6526c80b4a632a7de9ea7703_DockerCon2023.jpg,DockerCoon 2023 Keynote October 5,ira-casteel,6,Wed Oct 11 2023 18:30:00 GMT+0000 (Coordinated Universal Time),docker; events,
Environment as a Service (EaaS): A Comprehensive Guide,environment-as-a-service-eaas-a-comprehensive-guide,62aa5a70cd5ba27d9d0d718a,63c671c203ef9cd3266c59c3,Tue Jan 17 2023 10:00:34 GMT+0000 (Coordinated Universal Time),Tue Jan 17 2023 10:00:34 GMT+0000 (Coordinated Universal Time),,"This is a complete guide to EaaS, including how it works, the benefits, and how to deploy it in your organization.","<figure id="""" class=""w-richtext-figure-type-image w-richtext-align-center"" data-rt-type=""image"" data-rt-align=""center""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63c67108c6c70f2cb21e9b9c_ftEy_t0gsQAKHNfix0ARAP6Qj1KW7QJJlLopWZAOzyfRisoLFqd7eX2GkMM1nvJtt8KKuf9jmoIhaTrOKoNkKGsaH56BrIiwoFbFMVzu4Q4P8BAWSRHcvV2ZXyXAOjuG6xIm9eSfSSrhE7-5CHVrItOPjk5ySR7OwLqxiVwQZ5p3ppNkBcoq6dWazEqu.jpeg"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><p id="""">Companies are creating more and more applications and services as they continue to embrace digital transformation and automation. In fact, the<a href=""https://www.k2io.com/average-enterprise-runs-464-custom-applications/#:~:text=Average%20Enterprise%20Runs%20464%20Custom%20Applications%20%7C%20K2%20Cyber%20Security"" id=""""> average enterprise</a> now runs 464 custom applications, according to a recent report. Even small organizations with fewer than 1,000 employees maintain around 22 custom applications on average.</p><p id="""">As a result, businesses are looking for ways to make software development faster, easier, more cost-effective, and more collaborative. To do this, many organizations are turning to on-demand environment as a service (EaaS) models. In fact, EaaS is increasingly helping engineering teams produce software at scale.&nbsp;</p><p id="""">It’s a good idea to have a working knowledge of EaaS heading into 2023. Keep reading for a complete guide to EaaS, including how it works, the benefits that it offers, and how to deploy it in your organization.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-center"" data-rt-type=""image"" data-rt-align=""center""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63c671084b209de868de2170_HoUkWImj6T2AvuEHKvRQAuGWWENjcDYhEezyVYH1meQy_DrqkAE5KA4Qw7HfN2fI_JrMHKOIq1ufwWy2BGdkFUCsAmEcPKpbLtTA5BstlGhuhJ-re_c2wwl8aYgUQNWJOFSHyHODoLLjfdrz986PbDLeM0QqnU_mBLNym5PhNoG0hVr9AXNe9Oyhpk_V.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><h2 id="""">What Is Environment as a Service?</h2><p id="""">EaaS is a cloud-based delivery model that streamlines the way developers create and deploy environments. The concept is an extension of infrastructure as a service (IaaS). But while IaaS only involves hardware and basic software, EaaS encompasses much more—like settings and code, as well as the underlying software and infrastructure that’s necessary for operating in an isolated environment.</p><p id="""">An environment in this case refers to the infrastructure, software, and platform services that engineers need to complete different tasks. Software engineers today use live production environments as well as nonproduction environments for things like development, staging, and testing.</p><p id="""">On one hand, environments are a necessary part of software production. However, they're expensive and time-consuming to provision, deploy, configure, and decommission. High demand often leads to bottlenecks and delays, preventing teams from shipping software quickly.&nbsp;</p><p id="""">By providing all necessary components in one central portal, EaaS eliminates the burden of having to manually create and manage software environments.</p><h2 id="""">What do Developers use EaaS for?</h2><p id="""">Engineering teams are finding more and more ways to use EaaS, with new use cases constantly arising. With that in mind, let’s examine three common examples of how you can use EaaS in your organization.</p><h3 id="""">Performance Testing</h3><p id="""">Software performance testing is more important than ever due to skyrocketing customer expectations and rising competition. In fact,<a href=""https://www.bls.gov/ooh/computer-and-information-technology/software-developers.htm#:~:text=in%20May%202021.-,Job%20Outlook,the%20average%20for%20all%20occupations."" id=""""> demand for QA engineers and software testers</a> is on pace to grow by 25% over the next decade.</p><p id="""">Businesses are now using EaaS to enable performance testing at scale while keeping costs low and ensuring accuracy.&nbsp;</p><h3 id="""">Sharing Sales Demos</h3><p id="""">Businesses need to ensure maximum performance and stability while sharing sales demos, especially when selling mission-critical software or products with speed and reliability guarantees.</p><p id="""">By using EaaS, companies are able to avoid sharing system resources with other users and programs, thus providing a more stable and predictable operating experience.&nbsp;</p><h3 id="""">Migrations&nbsp;</h3><p id="""">Businesses often need to transfer data between storage systems and cloud environments. However, this can be dangerous without the right tools and resources.</p><p id="""">EaaS enables stable, flexible, and risk-free storage during a migration project. EaaS environments are also fast and easy to manage and decommission, which can eliminate security violations and data leaks after a migration is complete.</p><h2 id="""">Top Benefits of EaaS</h2><p id="""">There are numerous<a href=""https://releasehub.com/blog/environments-as-a-service-eaas-top-3-benefits"" id=""""> benefits to adding EaaS</a> to your business’s software engineering toolkit.</p><h3 id="""">1. Improve Software Velocity</h3><p id="""">Software production teams are under rising pressure to improve velocity and deliver more updates and applications. To this end, velocity remains one of the top KPIs heading into 2023.</p><p id="""">EaaS increases software velocity by decreasing rework and removing bottlenecks during production. This is due to the fact that EaaS provides the resources to support staging and QA initiatives.</p><h3 id="""">2. Save Money&nbsp;</h3><p id="""">Companies are also under pressure to reduce production costs and maintain lean development environments. According to Gartner, cutting costs by optimizing IT is among the<a href=""https://www.cio.com/article/407109/top-10-strategic-technology-trends-for-2023-gartner.html#:~:text=Top%2010%20strategic%20technology%20trends%20for%202023%3A%20Gartner,to%20respond%20to%20organizational%20change%20...%20More%20items"" id=""""> top technology trends for 2023</a>.</p><p id="""">EaaS is very cost-efficient. It reduces maintenance costs and enables businesses to use resources more effectively. At the same time, EaaS lowers project cycle times, leading to further cost savings.&nbsp;</p><h3 id="""">3. Increase Flexibility for Developers</h3><p id="""">Most organizations are looking for ways to enable remote work in order to attract and retain talent. In light of this, another benefit to EaaS is that it increases flexibility for developers. Participants can access and modify environments from any location instead of having to be on-site.&nbsp;</p><h3 id="""">4. Free Teams to Focus on Other Work</h3><p id="""">At the end of the day, engineers need to be focused on high-value work, not spending time creating and managing environments. EaaS removes the burden of having to create environments, thus freeing team members to tackle the most important tasks.</p><h2 id="""">Are There Drawbacks to EaaS?</h2><p id="""">Unless your business has strict rules in place requiring on-site development tools and resources, then you shouldn't have any issues implementing EaaS. The technology is secure, flexible, and noninvasive, as it doesn’t require purchasing or managing any infrastructure. You can also use it alongside local developer resources.&nbsp;</p><p id="""">Truth be told, it’s riskier to disregard EaaS when considering the numerous benefits that it offers. EaaS positions companies to develop software at a faster clip without sacrificing quality or performance.</p><p id="""">Continuing to produce software without isolated, on-demand environments could present a major disadvantage as more and more competitors adopt flexible EaaS models that enable them to accelerate production.&nbsp;</p><h2 id="""">Implementing EaaS: Build or Buy?</h2><p id="""">Some companies choose to build their own EaaS platforms. However, this is extremely time- and cost-intensive, and most companies lack the bandwidth to ensure optimal results. For example, some organizations are now dealing with outdated internal EaaS systems and are struggling to retrofit their platforms for<a href=""https://releasehub.com/blog/kubernetes-pod-a-beginners-guide-to-an-essential-resource"" id=""""> Kubernetes</a>.&nbsp;</p><p id="""">Ultimately, EaaS systems are very complicated to implement, manage, and maintain. It’s much easier to work with a third-party EaaS provider that can ensure a smooth transition and future-proof your platform for long-term success.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-center"" data-rt-type=""image"" data-rt-align=""center""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63c67108d9f5ec62829ccc80_cKpQTMoc18zzq9eeGI5jJesijYSwNbl8mfdHBCrHhXF2G0tiGTMAhYL0893-qQT8itt-MfrhbHivWj_MRNgNILFG5Zwp2VIDfVjlbAEVoLCKPihTJjqm8FEs02R5CvsV-RfnOVh-D2Fw27erVEZsaEgRwxNfSKIblKcfWu-ZVPmuwJUdZp80lSRHH_BZ.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><h2 id="""">Best Practices for Implementing EaaS</h2><p id="""">Deciding to work with a third-party EaaS provider can save your business a significant amount of time and effort. However, there are a few things to keep in mind when implementing a solution.</p><h3 id="""">Consult With Your Stakeholders</h3><p id="""">Before you move forward with EaaS, it’s a good idea to bring your team up to speed about why you're making the change. During this strategy session, you can explain how EaaS works, the benefits that it offers, and how it will improve speed and productivity.&nbsp;</p><h3 id="""">Prioritize Security</h3><p id="""">When browsing for a provider, it’s important to select one that features enterprise-grade security and authentication features. You should also ask for a bill of materials and analyze the company’s supply chain to explore where any open-source components come from.</p><p id="""">This is critical if your team intends to use EaaS to store sensitive data. After all, not all vendors offer the same commitment to security.&nbsp;</p><h3 id="""">Consider the User Experience&nbsp;</h3><p id="""">Your engineers are going to spend a significant amount of time on the EaaS platform. As a result, it’s a good idea to factor in the user experience.</p><p id="""">You may also want to invite engineers to demo platforms and participate in the sourcing process. Otherwise, developers may turn to other tools for testing software, which could lead to shadow IT and create privacy and security issues.</p><h2 id="""">Streamline Development With ReleaseHub’s On-Demand Environments&nbsp;</h2><p id="""">As you can see, there are numerous advantages to using an on-demand EaaS model, like cost savings, faster time to market, and fewer production errors.&nbsp;</p><p id="""">ReleaseHub offers a purpose-built EaaS platform that lets you instantly spin up environments for testing, production, demoing, and more. It’s ideal for businesses of all sizes—from small startups to large enterprises and everything in between.&nbsp;</p><p id="""">To experience how ReleaseHub can change the way your team produces software,<a href=""https://releasehub.com/book-a-demo"" id=""""> try a demo today</a>.&nbsp;</p><h2 id="""">Social Blurb</h2><p id="""">Many companies are turning to environment as a service (EaaS) solutions to turbocharge software production. Learn all about EaaS in our latest <a href=""https://releasehub.com/blog"" id="""">blog</a>.</p>",false,,,,https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63c6716e2510c17b3d1261d2_compass.jpeg,,,6,,,
Environments as a Service (EaaS) - Top 3 benefits,environments-as-a-service-eaas-top-3-benefits,62aa5a70cd5ba27d9d0d718a,62aa5a70cd5ba20aeb0d72c3,Fri Mar 12 2021 22:29:07 GMT+0000 (Coordinated Universal Time),Fri Apr 21 2023 19:49:28 GMT+0000 (Coordinated Universal Time),Fri Apr 21 2023 20:04:43 GMT+0000 (Coordinated Universal Time),Everything is a service these days; Everything as a Service (XaaS) actually exists. So you can be forgiven for not,"<h3 id="""">Environments as a Service</h3><p id="""">Everything is a service these days; Everything as a Service (<a href=""https://simple.wikipedia.org/wiki/Everything_as_a_service"" target=""_blank"">XaaS</a>) actually exists. So you can be forgiven for not knowing exactly what Environments as a Service (EaaS) is and why it could be a game changer for your business.</p><p id="""">EaaS is the natural extension of infrastructure as a service (IaaS, e.g. AWS, GCP, etc.), but instead of just the hardware and base software, EaaS includes all your code and settings as well as the infrastructure and software to run your application in an isolated environment. You describe your application to the system and the EaaS platform does the rest. </p><p id="""">These environments can be used for performance testing, QA, Sales Demos, large software and/or data migrations—even production. Aside from production, these environments are ephemeral; they come and go based on your particular SDLC.</p><p id="""">There are a lot of reasons to implement an EaaS, but there are 3 specific benefits that can change the whole trajectory of your business. </p><h3 id="""">Cost Control</h3><p id="""">Using a cloud provider has huge advantages, but there are some downsides, and a big one is cost. It isn’t that using a cloud provider means your infrastructure costs are necessarily higher, but the risk of accidentally spending a lot more money than you meant to is real! An EaaS can make this a non-issue. Once you know what an environment costs to create, you can understand your expenditures in a way not possible when dealing with AWS or GCP directly.</p><blockquote id=""""><em id="""">Don’t forget to create all the correct billing alarms in AWS, they aren’t on by default!</em></blockquote><p id="""">Your EaaS provider gives you total control over your cloud bill by allowing you to limit the number of environments created and what they consist of… Your environments can be scaled to match your needs for each environment so that costs can be contained. For example, your demo environments need not be as big or fast as your load testing environment. Also, as noted above, your environments are ephemeral so they can be spun up and deleted automatically for only as long as you are actually going to use them.</p><p id="""">The costs of building your own internal EaaS is complicated to calculate, but you need to take into account a team of specialized devops engineers working on the project for 6-18+ months (depending on complexity), maintenance of the platform each year, cost of adopting new technologies, handling all your own AWS or other cloud costs, internal product management to make sure it stays competitive, and so on. It’s not cheap or easy to understand the costs of this kind of system. Even a team of only 3 Devops engineers working on an EaaS for about 6 months is over half a million dollars and that does not take into account opportunity cost and maintenance/upgrading costs going forward.</p><p id="""">Buying the right EaaS will be usable sooner than if you build it yourself and the costs will be simple to understand and more affordable. There are many things to spend your limited resources on to move your business forward, but building your own EaaS is not one of them.</p><h3 id="""">Massive Increases in Speed</h3><blockquote id=""""><em id="""">“Speed Kills.” - Al Davis</em></blockquote><p id="""">All things being equal you have an advantage in football and business if you are more agile than your opponents or competitors. Obviously, in software development you need to define speed with a component of quality. Deploying a bunch of changes quickly that result in lower conversion or (worse) down-time is a huge problem, and that’s not how we want to measure speed: it’s too naive. </p><p id="""">We need to measure our velocity by only counting product deliverables that meet or exceed your key metrics and don’t compromise the stability of the application. Having a fast and capable EaaS could increase your teams’ velocity more than implementing any other kind of platform. An EaaS can improve your velocity in at least two dimensions by removing bottlenecks and decreasing rework. </p><p id="""">With an EaaS your releases will never get stuck because of a lack of staging or QA environments. The ability to create and destroy production like environments with your EaaS means your releases aren’t being delayed because of environmental bottlenecks. Since your environments are now ephemeral you can create more when your teams need them and shut them down when they aren’t needed. Capacity planning can now be done in real time with the platform reacting to your teams’ changes in velocity.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1267px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1267px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/604be9a8d633692aadcd4637_Screen_Shot_2021-03-09_at_11.40.06_AM.png"" alt=""Ephemeral Environments"" width=""auto"" height=""auto"" loading=""auto"" id=""""></div></figure><blockquote id=""""><em id="""">So many beautiful Ephemeral Environments to work with!</em></blockquote><p id="""">Rework is the most costly kind of work you can do. You always want to tackle rework as early as possible. You can’t avoid it completely, but you can minimize its impact. Often features are complex—even the smallest ones—with all the ways people may interact with it. From mobile apps to APIs, features often need to be seen or experienced by many people and multiple teams before release to avoid rework. Having access to an isolated, ephemeral environment that looks like ‘production + the new feature’ gives each team the ability to test earlier on and give feedback while the developers and designers are creating it. A good EaaS will give you the confidence in your quality and remove bottlenecks, allowing all your teams to achieve more in less time than ever before. </p><h3 id="""">Implementing New Technology</h3><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:690px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""690px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/604be9a98e9b2a0ff26ad182_devops-tools-690x460.png"" alt=""Devops Tools and Technolgies"" width=""auto"" height=""auto"" loading=""auto"" id=""""></div></figure><blockquote id=""""><em id="""">Simplified Devops technolgies image, courtesy of OSOLABS.</em></blockquote><p id="""">Technologies are constantly changing and evolving. It wasn’t that long ago that open source databases and the cloud weren’t considered mature enough technologies for the enterprise. It was even less time ago when you didn’t use containers, but just virtual machines (VMs) on AWS using EC2. If you started building an internal EaaS 4-5 years ago you most likely did not build it using Kubernetes (k8s); it just wasn’t ready for production workloads at the time. The retro-fit of an internal EaaS from managing VMs or containers to managing k8s is a non-trivial process. This is one of the major reasons you want to use an external EaaS.</p><p id="""">Kubernetes is amazing when it’s running well and managed by an experienced group of people. Implementing it is not for the faint of heart and it changes rapidly compared to more mature software. It’s probably the ultimate “<a href=""https://en.wiktionary.org/wiki/footgun"" target=""_blank"" id="""">footgun</a>” in devops at the moment.</p><p id="""">K8s is not an EaaS by itself, but just a piece of the system—albeit an important one. In order to implement an internal EaaS on Kubernetes you need a deep understanding of it and a group of talented engineers to create an EaaS on top of it. And after you do all that, what happens if Kubernetes gets supplanted by something else? You will be stuck at that decision again, needing to invest all the time and resources to implement something new, or stay with what you have and hope it doesn’t hold you back compared to your competitors. </p><p id="""">Cloud providers are always supporting new technologies and many times the documentation, support, and stability of these technologies leaves something to be desired. An EaaS can help you avoid the time and distraction it takes to learn and implement all these changing technologies. You can think of Kubernetes as an engine and an EaaS as a car. Most of us buy a car, not all the parts of the car to make it yourself, unless we are in the business of building cars. You are in the business of creating amazing AI products, collaboration software, or just about anything other than an EaaS. </p><h3 id="""">Conclusion</h3><p id="""">An EaaS is crucial for your business to move as fast as possible while not sacrificing quality. The ability to control and predict costs while producing high quality work as quickly as possible is the holy grail of product development. The inability to quickly produce isolated environments of any specification will hold you back in innumerable ways. </p><p id="""">EaaS platforms are extremely complicated systems with rapidly changing technologies underpinning them. Just as most companies shouldn’t create their own alerting and monitoring solution, but instead use Datadog or an analog, they shouldn’t be creating their own EaaS. </p><p id=""""><em id="""">At Release we work tirelessly to bring your application to life in an orchestrated, human interface. We write software to deal with all the complexity, difficulty, and strain so that no one else has to (unless they want to!) We create the engine that drives the Kubernetes vehicle, and we deliver solutions that our customers can use to get on with their business of doing business. Checkout </em><a href=""https://releasehub.com"" target=""_blank"" id=""""><em id="""">Release</em></a><em id=""""> and let us help your business streamline feature development with EaaS!</em></p><p id="""">Photo by <a href=""https://unsplash.com/@alschim?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText"" target=""_blank"" id="""">Alexander Schimmeck</a> on <a href=""https://unsplash.com/"" target=""_blank"" id="""">Unsplash</a></p>",false,,,,https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63e3fe483487046b70a85e83_030821%20(1).jpg,Dozens of colorful sunflowers representing the diversity of EaaS,erik-landerholm,5,Tue Mar 09 2021 00:00:00 GMT+0000 (Coordinated Universal Time),,
Ephemeral Environments: 9 Tips for Seamless Deployment,ephemeral-environments-9-tips-for-seamless-deployment,62aa5a70cd5ba27d9d0d718a,65d67222a4b0301e24527f98,Wed Feb 21 2024 21:58:58 GMT+0000 (Coordinated Universal Time),Wed Oct 30 2024 19:46:39 GMT+0000 (Coordinated Universal Time),Wed Oct 30 2024 19:46:39 GMT+0000 (Coordinated Universal Time),"Unlock the power of ephemeral environments for seamless deployment. Streamline workflows, enhance collaboration & more.",,true,<p>Experience ephemeral environments for yourself—sign up for a free trial on Release!</p>,https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=9-tips-seamless,"<p id="""">Ephemeral environments became a game-changer in modern software development. They are temporary, short-lived, and created as needed. These environments are perfect for specific tasks like testing new features or fixing bugs. Their main purpose is to give developers a safe space to try out and validate changes without affecting the main codebase or ongoing operations.</p><p id=""""><strong id="""">Key benefits of ephemeral environments are:</strong></p><ul id=""""><li id=""""><strong id="""">Risk Reduction:</strong> Isolating changes in temporary environments minimizes the potential for disruptions in the production environment.</li><li id=""""><strong id="""">Resource Efficiency:</strong> These on-demand environments require resources only when active, freeing up computational power and reducing costs when not in use.</li><li id=""""><strong id="""">Speed and Flexibility:</strong> On-demand creation allows for rapid testing cycles and quick pivots based on real-time results.</li></ul><p id="""">These advantages are just the beginning. As we explore further, we'll see how ephemeral environments not only improve development workflows but also align with broader goals like continuous integration and deployment, ultimately fostering a culture of innovation and efficiency. We will go over 9 areas you need to understand to successfully implement ephemeral environments in your organizatuon. Let’s get started.</p><h2 id=""""><strong id="""">1. Understand the Key Characteristics of Ephemeral Environments</strong></h2><p id="""">Ephemeral environments are catalysts in software development, closely mirroring production environments to provide a realistic testing ground for new features and updates. These dynamic setups are designed to be short-lived, with several key characteristics that make them a valuable asset for today’s development teams:</p><h3 id=""""><strong id="""">Resemblance to Production</strong></h3><p id="""">By closely emulating the production environment, ephemeral environments allow developers and testers to interact with applications under conditions that are nearly identical to the live production setup. This similarity ensures that any functionality, behaviors, or issues observed during testing will likely hold true after deployment.</p><h3 id=""""><strong id="""">Automated Creation and Fast Provisioning</strong></h3><p id="""">Speed is of the essence in modern development workflows. Ephemeral environments thrive on automation for their creation and provisioning, which allows them to be spun up quickly as needed. This rapid availability is essential for maintaining their temporary nature while supporting continuous integration and delivery practices.</p><h3 id=""""><strong id="""">Replicated Data Consistency</strong></h3><p id="""">Data plays a crucial role in testing and validating application behavior. Ephemeral environments often include mechanisms for replicating data from production or using synthetic data sets that maintain consistency across test cases. This replication ensures that tests are not only relevant but also reliable.</p><h3 id=""""><strong id="""">Accessibility via Unique URLs</strong></h3><p id="""">Stakeholders from developers to product managers require easy access to these environments. Unique URLs enable this accessibility, allowing for seamless sharing and review processes. Whether it's for internal reviews or external stakeholder demonstrations, these URLs provide direct entry points into the temporary world where the latest features reside.</p><p id="""">For teams looking to leverage on-demand ephemeral staging environments, exploring services like<a href=""https://release.com/usecase/on-demand-ephemeral-staging-environments"" id=""""> Release</a> can offer insight into how these environments streamline development and deployment processes.</p><p id="""">By understanding these foundational elements of ephemeral environments, organizations equip themselves with the tools necessary for efficient and effective software development cycles. Moving forward, embracing these characteristics can significantly transform how teams approach development challenges.</p><h2 id=""""><strong id="""">2. Embrace the Benefits of Using Ephemeral Environments in Your Development Workflow</strong></h2><p id="""">Ephemeral environments offer numerous benefits that can transform your development workflow. By embracing these advantages, you can streamline your development process, improve code quality, and foster a more collaborative working environment.</p><h3 id=""""><strong id="""">Reducing Rework and Decrease Cycle Time</strong></h3><p id="""">One such advantage includes reducing rework, a key strategy to enhance productivity and minimize errors. Another advantage is getting results quickly up front during development before reaching production or staging. These environments provide an identical replica of your production environment, enabling developers to identify and fix issues prior to deployment. This process saves time, resources, and reduces the likelihood of recurring problems.</p><h3 id=""""><strong id="""">Self-Service Capabilities</strong></h3><p id="""">Developers often require access to different environments at various stages of their workflow. Ephemeral environments empower them with self-service capabilities on internal platforms, facilitating faster iterations. With automated creation and provisioning, developers can spin up as many environments as needed without waiting for manual provisioning or risking conflicts in shared spaces.</p><h3 id=""""><strong id="""">Running Production Workloads with Aligned Data</strong></h3><p id="""">Another significant benefit is the capacity to run production workloads with aligned data. This feature allows you to validate system behavior under realistic conditions, mitigating risks associated with deploying untested code into production. With data consistency ensured through mechanisms like replicated and scrubbed data, you can confidently assess how new features or changes will perform when actually deployed.</p><h3 id=""""><strong id="""">Improving Collaboration</strong></h3><p id="""">Lastly, ephemeral environments play a vital role in improving collaboration and gathering early feedback from stakeholders. Through the use of automated preview environments that facilitate<a href=""https://release.com/blog/improve-developer-velocity-with-ephemeral-environments"" id=""""> measuring and improving developer velocity</a>, stakeholders can easily access and review changes via unique URLs. This real-time collaboration fosters transparency, accelerates decision-making, and keeps everyone informed about development progress.</p><h2 id=""""><strong id="""">3. Leveraging Ephemeral Environments for Different Use Cases</strong></h2><p id="""">Ephemeral environments have many practical uses in different situations, each with its own advantages. Here are two common examples:</p><h3 id=""""><strong id="""">Development and Testing of New Features</strong></h3><p id="""">Think of ephemeral environments as sandboxes that provide a controlled yet realistic setup. Developers can build features with confidence, knowing they are working in an environment that closely mirrors production conditions. This practice not only enhances code reliability but also minimizes surprises during the deployment phase.</p><p id="""">A perfect example of this is creating a new feature for an e-commerce site, like a personalized recommendation engine. An ephemeral environment allows developers to assess the impact of this feature in isolation from the rest of the application, ensuring it performs as expected when integrated into the larger system.</p><h3 id=""""><strong id="""">Running Performance-Intensive or Distributed Applications</strong></h3><p id="""">This use case applies to applications that require significant computing resources or need to handle high volumes of data. Ephemeral environments excel in situations where you need to:</p><ul id=""""><li id="""">Test how well your application scales under heavy load.</li><li id="""">Evaluate the performance of individual components or services.</li><li id="""">Validate the behavior of distributed systems.</li></ul><p id="""">For instance, consider a microservices-based application that needs to scale up rapidly during peak traffic hours. In an ephemeral environment, you can simulate this scenario and assess how well your application scales under load, well before deploying it into production. Once the tests are completed, the whole environment can be torn down automatically to free up valuable resources, which could be quite expensive to build, maintain, or configure otherwise.</p><p id="""">As you can see, ephemeral environments offer flexibility and control while providing a realistic preview of production conditions. They are undoubtedly a powerful tool in any developer's toolbox.</p><p id="""">To delve deeper into ephemeral environments, check out Release's insightful article on<a href=""https://release.com/blog/beyond-k8s-introduction-to-ephemeral-environments"" id=""""> Beyond K8s: Introduction to Ephemeral Environments</a>.</p><h2 id=""""><strong id="""">4. Integration Possibilities with Collaboration Tools like GitHub and Jira</strong></h2><p id="""">In the realm of software development, <strong id="""">GitHub</strong> and <strong id="""">Jira</strong> stand as titans of collaboration, offering robust platforms for code management and issue tracking, respectively. Ephemeral environments gain an added layer of efficiency when integrated with these tools, streamlining workflows and enhancing productivity.</p><h3 id=""""><strong id="""">Seamless Integration with GitHub</strong></h3><ul id=""""><li id=""""><strong id="""">Automated Environment Spin-up</strong>: Upon a new pull request in GitHub, an ephemeral environment can be automatically created. This provides immediate feedback on how code changes will perform in a live setting.</li><li id=""""><strong id="""">Status Checks</strong>: Integrating ephemeral environments into GitHub's status checks allows developers to see if their environment is ready for review directly from the pull request, ensuring that only fully provisioned environments are tested.</li><li id=""""><strong id="""">Bot Notifications</strong>: Custom bots can comment on pull requests with ephemeral environment URLs and deployment statuses, making it effortless for reviewers to access the latest version of the application.</li></ul><h3 id=""""><strong id="""">Streamlining Workflows with Jira</strong></h3><ul id=""""><li id=""""><strong id="""">Linking Environments to Issues</strong>: Attach ephemeral environment URLs to relevant Jira tickets. This encourages a clear association between task progress and the actual environment where the feature is implemented.</li><li id=""""><strong id="""">Transition Automation</strong>: Trigger the creation or teardown of ephemeral environments based on issue status transitions within Jira. For example, an environment can be spun up when an issue moves to ""In Progress"" and torn down once it reaches ""Done.""</li></ul><p id="""">By weaving ephemeral environments into the fabric of GitHub and Jira workflows, teams harness easy sharing capabilities that complement Agile practices. The result is a streamlined process where code merges and feature developments are transparently connected to dynamic testing environments, fostering an ecosystem where sharing becomes second nature to development processes.</p><h2 id=""""><strong id="""">5. Ensuring Quality in Ephemeral Environments through Effective Testing Strategies</strong></h2><p id=""""><strong id="""">Unit tests</strong> are the backbone of software testing, but they often fall short in evaluating <strong id="""">system behavior outside unit tests</strong>. The complexity of modern applications necessitates comprehensive testing strategies that cover more ground. Enter <strong id="""">smoke and integration tests</strong>—essential tools that probe the interactions between various components and ensure seamless deployments.</p><p id="""">When applied to <strong id="""">live ephemeral environments</strong>, these tests do more than just verify code correctness; they simulate real-world usage to expose issues that would otherwise remain hidden until production. This is crucial because while unit tests validate individual pieces, smoke and integration tests examine the assembled puzzle, catching errors that occur when all pieces work together.</p><h3 id=""""><strong id="""">Key Strategies for Effective Testing in Ephemeral Environments:</strong></h3><ul id=""""><li id=""""><strong id="""">Parallel Testing:</strong> Managing multiple ephemeral environments allows teams to run concurrent tests for different features or branches, significantly reducing the time to release.</li><li id=""""><strong id="""">Automated Test Suites:</strong> By automating smoke and integration tests within ephemeral environments, developers can quickly identify defects early in the development cycle.</li><li id=""""><strong id="""">Dynamic Resource Allocation:</strong> Allocating resources on-the-fly to handle a large number of parallel environments ensures that testing is not bottlenecked by infrastructure limitations.</li><li id=""""><strong id="""">Continuous Monitoring:</strong> Integrating monitoring tools to track the health and performance of ephemeral environments during testing can provide immediate feedback on system stability.</li></ul><p id="""">Incorporating these strategies into your development workflow can transform the quality assurance process. Teams become equipped to deliver robust software at a faster pace by leveraging the unique benefits of ephemeral environments for comprehensive testing. For insights into how this approach can increase developer velocity, consider exploring Release's whitepaper on<a href=""https://release.com/blog/increase-developer-velocity-by-removing-environment-bottlenecks"" id=""""> increasing developer velocity by removing environment bottlenecks</a> using Environments as a Service.</p><p id="""">By ensuring thorough testing in environments that mimic production closely, software teams can confidently push new features, knowing they've been vetted in conditions that match what users will encounter.</p><h2 id=""""><strong id="""">6. Realizing the Agile Potential of Ephemeral Environments in Software Development</strong></h2><p id="""">Ephemeral environments play a significant role in fostering <em id="""">Agile/Scrum practices</em> within software development teams. With their dynamic and transient nature, they align perfectly with the iterative and adaptive nature of Agile methodologies.</p><h3 id=""""><strong id="""">Supporting Continuous Delivery with Ephemeral Environments</strong></h3><p id="""">One of the key principles of Agile is <em id="""">continuous delivery</em>, and ephemeral environments are instrumental in supporting this. They allow constant production-like testing and validation, enabling software updates to be developed, tested, and released rapidly and frequently. As such, developers can:</p><ul id=""""><li id="""">Test code changes immediately in a production-like environment.</li><li id="""">Detect and resolve issues early before they reach production.</li><li id="""">Accelerate the feedback loop with stakeholders for quicker iterations.</li></ul><p id="""">In essence, ephemeral environments serve as an enabler for continuous delivery – one of the cornerstones of Agile.</p><h3 id=""""><strong id="""">Facilitating Iterative Software Development with Ephemeral Environments</strong></h3><p id="""">Another attribute of Agile is its emphasis on <em id="""">iterative software development</em>. Here, ephemeral environments shine by facilitating rapid iterations and feedback loops. For instance, developers can share unique URLs of these temporary environments with stakeholders to gather early feedback. The possibility to quickly set up, test, and tear down these environments aligns perfectly with the iterative cycles of Agile development.</p><p id="""">Incorporating ephemeral environments into an Agile workflow thus enhances efficiency while maintaining high quality standards – a win-win for any modern software development team.</p><h2 id=""""><strong id="""">7. The DevOps Connection: Ephemeral Environments as a Catalyst for Collaboration and Efficiency</strong></h2><p id="""">Ephemeral environments are a perfect fit for DevOps and Platform Engineering, where teams prioritize automation and collaboration. These dynamic setups are specifically designed to work within a DevOps or PE framework,<a href=""https://release.com/blog/extend-your-idp-with-environments-for-every-developer-and-every-change"" id=""""> bridging the gap between software development and IT operations</a>.</p><h3 id=""""><strong id="""">How Ephemeral Environments Benefit DevOps and Platform Engineering</strong></h3><p id="""">Here's how ephemeral environments contribute to the success of DevOps and PE:</p><h4 id=""""><strong id="""">Automation Aligned with DevOps</strong></h4><ul id=""""><li id="""">Ephemeral environments automate the process of creating and tearing down environments, aligning with the DevOps principle of streamlining the software development pipeline.</li><li id="""">This automation reduces the manual effort required for environment setup, allowing teams to focus on more important tasks.</li></ul><h4 id=""""><strong id="""">Collaboration Across Teams for Platform Engineering</strong></h4><ul id=""""><li id="""">Ephemeral environments can be spun up at any stage of the development process for various purposes, such as development or testing.</li><li id="""">This shared access promotes collaboration between different teams involved in the software lifecycle, breaking down silos and fostering a culture of teamwork. This platform allows a common place for all self-service environments to be tested, shared, and reviewed.</li></ul><h3 id=""""><strong id="""">The Role of Ephemeral Environments in CI/CD Pipelines</strong></h3><p id="""">Integrating ephemeral environment provisioning into continuous integration (CI) and continuous delivery (CD) pipelines can revolutionize the deployment process. Here's how it works:</p><ul id=""""><li id="""">A new ephemeral environment is automatically created by the CI/CD tool/platform whenever there's a code commit or pull request.</li><li id="""">Developers receive immediate feedback on their changes in an environment that closely resembles the production setup.</li><li id="""">The team can perform tests and quality assurance processes in real-time, ensuring that only thoroughly tested code moves forward in the pipeline.</li></ul><p id="""">This approach allows organizations to make the most out of their DevOps investment by speeding up deployment cycles while maintaining high standards of quality and collaboration.</p><h2 id=""""><strong id="""">8. Configurability for Rapid Application Development and Testing in Ephemeral Environments</strong></h2><p id="""">Rapid application development and <a href=""https://release.com/blog/test-environment-a-definition-and-how-to-guide"" id="""">testing</a> thrive on the ability to quickly adapt to different requirements and scenarios. Ephemeral environments extend this flexibility with their inherently dynamic nature. The key to harnessing this potential lies in the configurability of these temporary spaces, which can be tailored to match a myriad of production setups.</p><h3 id=""""><strong id="""">How Configurability Enhances Ephemeral Environments</strong></h3><p id="""">Here are some ways configurability enhances ephemeral environments for rapid application development and testing:</p><ul id=""""><li id=""""><strong id="""">Customization of Infrastructure Components</strong>: Teams can customize OS, servers, memory, and storage parameters to simulate various target environments. This customization ensures that applications are tested under conditions that closely replicate those they will encounter in real-world deployments.</li><li id=""""><strong id="""">Utilization of Deployable Artifacts</strong>: An essential aspect is the use of deployable artifacts, which are pre-built versions of software ready to be launched into the environment. These artifacts are essential for replicating the software deployment process and can range from binary executables to Docker containers, depending on the technology stack utilized.</li><li id=""""><strong id="""">Automated Deployment Processes</strong>: Automation is at the core of ephemeral environments, with pipelines designed to provision infrastructure, deploy applications, and tear down resources without manual intervention. Automated processes not only ensure efficiency but also contribute significantly to consistency across testing scenarios.</li></ul><p id="""">The streamlined deployment process not only saves time but also reduces errors by minimizing manual setup steps. By integrating these capabilities into ephemeral environments, teams can focus on developing and testing rather than managing infrastructure details.</p><h3 id=""""><strong id="""">Benefits of Configurability in Ephemeral Environments</strong></h3><p id="""">By optimizing these elements within ephemeral environments, organizations can achieve a significant competitive edge—accelerating time-to-market while ensuring high-quality standards are met before any release.</p><h2 id=""""><strong id="""">9. Advantages of Ephemeral Environments over Traditional Staging Approaches</strong></h2><h3 id=""""><strong id="""">Asynchronous Collaboration Across Time Zones</strong></h3><p id="""">Ephemeral environments facilitate asynchronous collaboration across distributed teams by providing on-demand access to consistent testing and development environments. This feature is a game-changer for global teams working across different time zones, enabling them to work together seamlessly.</p><h3 id=""""><strong id="""">Cost-Effective Infrastructure</strong></h3><p id="""">Compared to traditional staging setups that require dedicated infrastructure and maintenance, ephemeral environments offer a more cost-effective solution. Since these environments are only activated when needed and decommissioned after use, they significantly reduce the overhead costs associated with maintaining permanent staging servers.</p><h3 id=""""><strong id="""">Agile and Scalable</strong></h3><p id="""">Ephemeral environments provide unmatched agility and scalability. Teams can quickly set up, modify, or tear down environments as required, thus facilitating flexible scaling and testing processes. This capability enables companies to adapt rapidly to changing requirements without incurring additional costs or delays.</p><p id="""">One key benefit of decreasing cycle time and per-use costs is that productivity and utilization will actually increase. As an example, a single shared environment might support one team for 24 hours of usage costs, but 24 teams or individuals can use one-hour ephemeral environments for the same overall cost. If appropriate auto-scaling is used, resource utilization costs could go to nearly zero when not used after hours or on the weekend, for example. However, utilization and productivity during normal work hours could skyrocket!</p><h3 id=""""><strong id="""">Increased Security and Reliability</strong></h3><p id="""">Another advantage of ephemeral environments over traditional staging approaches is their enhanced security and reliability. Since each environment is isolated and short-lived, the risk of lingering vulnerabilities or data breaches is minimized. Moreover, these dynamic environments can be replicated exactly as per production standards, ensuring reliable testing outcomes. Not only that, but security tests, penetration tests, and destructive testing can happen without affecting the live production site, enabling the security posture to be verified and tested before reaching production. This is a massive boost in confidence on security practices that most production environments miss out on.</p><p id="""">For a deeper dive into the benefits of ephemeral environments as part of<a href=""https://release.com/blog/environments-as-a-service-eaas-top-3-benefits"" id=""""> Environments as a Service (EaaS) offerings</a>, you might find this article helpful.</p><p id="""">With these advantages in mind, it's clear why ephemeral environments are becoming an integral part of modern software development workflows.</p><h2 id=""""><strong id="""">Why should you care?&nbsp;</strong></h2><p id="""">Ephemeral environments are an innovative approach to software development that can greatly benefit your team. By creating temporary environments that closely resemble your production settings, you can streamline your development workflow and improve collaboration among team members, and make sure you stay competitive in your industry.&nbsp;</p><p id="""">Here are some key takeaways from this article:</p><ul id=""""><li id=""""><strong id="""">Streamline your development workflow:</strong> Ephemeral environments allow for faster iteration cycles, as you can quickly spin up new environments for testing and debugging.</li><li id=""""><strong id="""">Enhance collaboration:</strong> With on-demand setups, developers, QA teams, and stakeholders can easily access and work in the same environment, reducing communication barriers.</li><li id=""""><strong id="""">Improve testing strategies:</strong> Ephemeral environments provide an isolated space for thorough validation of system behavior before deploying to production.</li></ul><p id="""">Ready to give ephemeral environments a try? Check out<a href=""https://www.example.com/"" id=""""> Release</a> - a platform specifically designed for managing ephemeral environments.</p><p id=""""><strong id="""">Sign up for a free trial</strong> today and see how Release can help your team achieve greater agility and flexibility in your software development process.</p><p id="""">‍</p><p>‍</p>",https://cdn.prod.website-files.com/603dd147c5b0a4221d1bd360/65d671f72e0c193577309c13_Screenshot%202024-02-21%20at%201.58.05%E2%80%AFPM.png,ephemeral environments release,regis-wilson,5,Wed Feb 21 2024 22:00:00 GMT+0000 (Coordinated Universal Time),product,
Extend your IDP with Environments for Every Developer and Every Change,extend-your-idp-with-environments-for-every-developer-and-every-change,62aa5a70cd5ba27d9d0d718a,65317d4034fe8d0d2913256d,Thu Oct 19 2023 19:02:24 GMT+0000 (Coordinated Universal Time),Thu Jul 25 2024 15:02:50 GMT+0000 (Coordinated Universal Time),Thu Jul 25 2024 16:27:39 GMT+0000 (Coordinated Universal Time),Explore environments as a way to enhance your IDP and elevate the development process.,"<p id="""">In our <a href=""https://release.com/blog/components-of-a-successful-idp-build-a-product-your-developers-actually-want-to-use"" id="""">previous posts</a>, we talked about how the internal developer platform (IDP) is comprised of many components and tools working together to optimize your development team’s workflow. Some of these components, like code repositories, CI/CD, test environments, and development tools, are must-haves.</p><p id="""">For these must-have components, you have two choices. You can cover the basics and call it good, or you can look at your organization’s needs and enhance the components to provide your teams with extra leverage.</p><p id="""">In this post, we’ll talk about environments and how you can extend and enhance your IDP through flexible and automated environment management that mimics your production environment. This flexibility will enable your developers to isolate changes, reduce configuration-drift-related bugs, remove testing bottlenecks, and improve time-to-market in complex environments.</p>",true,"<p id="""">Ready to test-drive Release? </p><p id="""">Try it free for 30 days with code #IDP</p>",https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=idp5,"<p id="""">With Release, you can provide on-demand <a href=""https://release.com/get-started"" id="""">environments</a> for every change and every pull request (PR), providing multiple efficiencies for your development team.</p><h3 id="""">Why Do We Need More Flexible Environment Management?</h3><p id="""">To begin, let’s talk about common problems that developers experience with static environments and local environments.</p><h4 id=""""><strong id="""">#1 It Works on My Machine</strong></h4><p id="""">Environment discrepancies have been a software development headache since we stopped coding live in production. You may not remember that, but on mainframes, it wasn’t unusual to hop into production and change code live. To be fair, even non-mainframe programmers occasionally would hop onto production servers to make small HTML or JavaScript tweaks. Yes, I know, scary. But in small orgs with few controls, it did happen. Thankfully, we’ve evolved our development practices, and we know we don’t want to go back to those days.</p><p id="""">However, environment differences continue to plague us with bugs and replication difficulties. Whether it’s differences in the operating system, container configuration, or dependencies like the database, caches, and queues, all of these can cause churning due to unexpected behaviors in each environment, leading to the dreaded problem “but it works on my machine”. </p><h4 id=""""><strong id="""">#2 The Overhead of Microservices</strong></h4><p id="""">If you’ve ever been on a team that works in a microservices environment, you already know the pain of trying to run systems locally.</p><p id="""">Years ago, I frequently had to start up three, five, eight, or more microservices on my laptop to validate a change, test functionality, or debug a production problem. The fan noise coming off the laptop could be startling, and the heat emanating from the machine could warm a small room in the winter. It was slow, painful, and finicky to get everything running consistently.</p><p id="""">In addition to the local resource burden, testing was often done in static test or staging environments and required multiple validations and deploys to ensure the right versions of APIs and implementation were where you thought they should be.</p><h4 id=""""><strong id="""">#3 Load Testing Challenges</strong></h4><p id="""">For operations like load testing or performance benchmarking, we turn to static load testing environments. In previous organizations, I’d see folks scheduling time in environments to conduct load tests, often having to wait weeks for the environment and tools to free up to complete testing. </p><p id="""">Not being able to quickly access environments that allow proper load testing delays critical tests and makes automation of these load tests difficult.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1000px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1000px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/6538305adc621c1063b43d15_XgEvdUnVDo_3LV0_LqtQpMSvDp7YlNcFKGJ7ARns8Vquc3RRxKYWR8mGi21khZlK0rJTFS9lbV1Bm6VY4XKINPy4jEyEtsBM1DO1Gvxv8uKQTXXsfr4bXViF08i-nPwIYOW1qc03mx636nhGNxZ_nXU.png"" id="""" width=""auto"" height=""auto"" alt="""" loading=""auto""></div></figure><h4 id=""""><strong id="""">#4 Slow Pivots to Changing Priorities</strong></h4><p id="""">Picture this. A developer has been coding a new feature for several days. Though they’re committing and pushing frequently, they still have their local development set up specifically for this work. They’ve set up the data they’re testing with and ensured the environment is right for this particular change. Maybe they’ve even changed their local environment configuration to optimize it for their current task. But suddenly, a hot bug comes in, and they need to switch and focus on shipping a fix.</p><p id="""">Without fast ephemeral development environments, they not only have to check out a new branch and switch focus, but potentially, they’ll have to undo data schema changes they’re in the middle of and then recreate the data for the production scenario on their machine.</p><p id="""">It’s bad enough that your engineering teams have to context switch at times. But switching over local dev environments or test environments to hot priorities shouldn’t be a big ordeal.</p><h4 id=""""><strong id="""">#5 Data Degradation</strong></h4><p id="""">With static environments, after a while the data or the whole environment becomes degraded. Sometimes, this is due to testing in that environment with different versions of the code and not as much backward compatibility as we need. Other times, it’s because we’ve had to contrive data into a particular state to test or investigate issues.</p><p id="""">Regardless of the reason, static environments experience data degradation and require periodic cleanup. For example, I worked in a group that would go through routine quarterly weeklong data refreshes to get non-production environments to a healthy and working state in order to support further rounds of testing, for the whole next quarter.</p><p id="""">If we treat environments as static, long-living beasts, we aren’t primed to wipe them out and start fresh frequently.</p><h4 id=""""><strong id="""">#6 Configuration Drift</strong></h4><p id="""">Configuration drift occurs when our configuration between environments falls out of sync. This can include application configuration, infrastructure configurations or versions, and network differences between environments.</p><p id="""">This is especially true when working in environments where most configurations are managed manually.</p><p id="""">Configuration drift can cause pain when testing, reproducing bugs, or making assumptions about what an environment’s configuration looks like.</p><h4 id=""""><strong id="""">#7 Slow Feedback Loops</strong></h4><p id="""">Shared environments can make testing and feedback loops slow and inefficient because of waiting for environments or debugging issues unrelated to your changes.</p><h4 id=""""><strong id="""">#8 Inconsistent Security Measures</strong></h4><p id="""">Similar to configuration drift, security constraints in some environments are not replicated in local or test environments, causing unexpected bugs or security flaws in production.</p><p id="""">Though we can’t replicate everything between all environments, key differences can mean more bugs and incidents once the code is deployed to production.</p><h3 id="""">How Can Development Environments Help?</h3><p id="""">Before we dive into the benefits of ephemeral development environments, let’s review typical environment types:</p><ol id=""""><li id="""">The <strong id="""">developer’s local environment is </strong>where we write code and tests. The audience is limited to the developer(s) on this machine.</li><li id=""""><strong id="""">Static non-production environment</strong>s like test, QA, UAT (user acceptance testing), or SIT (system integration testing), where additional testing is completed. These environments are used by development teams, QA/testing, and both internal and external partner teams for testing and validation. These can also be used for demos both within and outside the organization.</li><li id=""""><strong id="""">Production</strong>, where your product is live for your customers.</li></ol><p id="""">When bottlenecks emerge with testing in static non-prod environments, organizations might expand the number of environments, hoping it relieves some of the pressure. This does temporarily provide folks with more test environments, but it also creates more of the problems we saw in the previous section.</p><p id="""">Instead of adding yet another static environment, consider investing in automated, fine-grained, flexible, and ephemeral environments. With these, spinning up environments as part of your development workflow and IDP takes little to no effort after the initial investment. Your team has purposeful environments for quick bug testing or feature deployment tied to a PR or branch of code.</p><p id="""">So how do these environments solve the problems we see with static environments? Let’s look at some of the answers.</p><h4 id=""""><strong id="""">Isolation and Consistency</strong></h4><p id="""">When testing changes, your development team won’t struggle with isolating one change’s impact from others in static test environments. They’ll work in a production-like setup, and then the CI/CD pipeline will create an environment for further validation/testing after the pull request is made.</p><h4 id=""""><strong id="""">Reduced Configuration Drift</strong></h4><p id="""">In long-lived environments, either on a developer’s machine or in a static testing environment, we can’t eliminate drift. However, if we’re frequently rebuilding environments for each change, we keep configurations in sync with production and provide a consistent experience across environments.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1000px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1000px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/6538305a2c138883e7929f12_CwdbSkg7PAYtC3WqeMNl7DVvBVdp6uzC3XtYUUV6c7qICtTScRzjVaZWq-Qrz2fYN1oAmSzHxTHHNpCS2ssHlG3D10p7laYzXI2qZk6QxXkJJWookPUYGHcHfS3XgmPUwsg7qq5cMCLu_5vVfJfsYvg.png"" id="""" width=""auto"" height=""auto"" alt="""" loading=""auto""></div></figure><h4 id=""""><strong id="""">Dependency Management</strong></h4><p id="""">With ephemeral and on-demand environments, you can provide easy dependency management. Developers won’t need to manage their own dependencies (the databases, caches, and queues mentioned above) while developing and testing their app. It can all be included in the release environment setup.</p><p id="""">Furthermore, if developers are specifically working on changes to dependencies, they’ll have a full environment to validate their changes.</p><h4 id=""""><strong id="""">Fast Feedback</strong></h4><p id="""">First, automated tests are necessary, and you can’t go without them. But they’re not bulletproof. You can’t test what you don’t expect. With environments set up for each story or issue, you can get fast feedback by starting up an environment, testing your change, and exploring side effects. Then you can receive that feedback immediately instead of waiting to get the change into a test environment.</p><h4 id=""""><strong id="""">Production-Like Data</strong></h4><p id="""">With on-demand environments, your development team will have the data and scenarios they need without excessive data setup. Features like <a href=""https://release.com/product/instant-datasets"" id="""">Instant Datasets</a> preload a pool of production-like datasets (sanitized and truncated, if needed) and have even terabytes of data available in minutes. </p><h4 id=""""><strong id="""">Consistent Development Workflows</strong></h4><p id="""">Finally, with ready-to-use and fully automated environments, development teams will have a consistent experience rolling out their changes to each environment in your CI/CD pipeline.</p><h3 id="""">How Do Ephemeral Environments Tie In with Your IDP?</h3><p id="""">Your IDP consists of multiple components and integrations. Once you have the essential components like CI/CD, where you’re automating basic workflows, then you’re ready to level up. </p><p id="""">If your organization is small and you don’t yet have environmental bottlenecks, adding ephemeral environments might not be your top priority on enhancing your IDP. </p><p id="""">On the other hand, if your teams experience pain or bottlenecks involved in using static development or non-production environments, then it’s time to experiment with <a href=""https://release.com/signup"" id="""">Release</a>. Incorporating automated on-demand ephemeral environments provides benefits like consistency, dependency management, and fast feedback. This helps your teams get new features into production faster and with fewer defects. You’ll manage, provision, and eventually destroy environments quickly and automatically, so that you get the most use out of them when your development team needs them.</p><p id="""">In short, IDPs integrate developer environments into the development workflow. They improve efficiency, collaboration, and quality.</p><p id="""">‍<em id="""">This post was written by Sylvia Fronczak. </em><a href=""https://sylviafronczak.com/"" id=""""><em id="""">Sylvia</em></a><em id=""""> is a software developer who has worked in various industries with various software methodologies. She’s currently focused on design practices that the whole team can own, understand, and evolve over time.</em></p>",https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/65385270172590ed49168651_PE%20%235%20-%20Env-2.jpg,,sylvia-fronczak,10,Wed Oct 25 2023 18:00:00 GMT+0000 (Coordinated Universal Time),platform-engineering,
Extending Release Environments with Terraform,extending-releasehub-environments-with-terraform,62aa5a70cd5ba27d9d0d718a,62f2f4c29aa17f959207efa9,Tue Aug 09 2022 23:58:58 GMT+0000 (Coordinated Universal Time),Mon Apr 03 2023 22:31:50 GMT+0000 (Coordinated Universal Time),,This blog shows how to utilize Terraform alongside Release to provision additional resources in your cloud account.,"<p id=""""><a href=""https://app.diagrams.net/?page-id=4EaHbZaufD5BJVQ07xAE&scale=auto#G1ZjCA67a8oFC8ztyAz2rx3ehoodHwKE2D"" id=""""><br></a></p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-center"" data-rt-type=""image"" data-rt-align=""center""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/62f2f318530a761d6102f974_mnVl_LfyacC5ubrGRgKEXJx0kqqvM_OIsOdr1sosZ46Bv2itteskY68L6an8QSa_RuYmBnw-g9SnWoG3Vxfgw1WO196CUqrDFypaHFQWIBDmZu5-cEO__1t48CCRUMCTloNHpsdjwD9SHGo03QcHgYU.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><p id=""""><a href=""https://release.com/"">Release</a> offers the ability to easily reproduce your production environment <em id="""">N</em> times in your cloud account. The construction of environments can be accomplished using our website, CLI or API and are useful for a variety of purposes - from quality assurance testing and user acceptance to sales demonstrations and penetration testing.</p><p id="""">Environments today typically extend beyond a set of Docker images to include a variety of resources from your chosen cloud provider. In order to consistently get those resources alongside your Docker images when building environments, the instructions to get those resources instantiated must be codified somewhere in your application with some flavor of Infrastructure-as-Code (IaC).</p><p id="""">A recently launched abstraction within the Release Application Template removes the previous requirements of packaging your Terraform into a Dockerfile, creating the scripts to apply/update/teardown your resources, and managing the variables required to namespace appropriately. Using this new abstraction, the process becomes much more streamlined. Write your Terraform, tell us where to find it, and then tell us when to execute it.</p><h3 id="""">What is Terraform?</h3><p id=""""><a href=""https://www.terraform.io/"">Terraform</a> is declarative Infrastructure as Code. Said another way, it is a way of programmatically specifying what resources you need created in your cloud provider. </p><p id="""">There are two approaches to IaC - declarative (what) versus imperative (how). Declarative focuses on the desired end state; you tell the system what resources you want provisioned and it figures out how to get you there. Imperative, on the other hand, focuses on the instruction set to get to your end state; you are responsible for maintaining the order of operations required to get to your desired end state. </p><p id="""">Another important concept behind IaC is state and state management. Terraform is no exception here, by default it uses JSON in a local file to persist the state of real world resources created by it. Configurations can be added to allow for remote storage, such as in S3.</p><h3 id="""">Why should I use Terraform?</h3><p id="""">Terraform, or any IaC, is an important piece of the puzzle to modern, cloud infrastructure stacks. Codifying the instructions to your production environment allows you to increase visibility and auditability of changes - alterations to your stack are now managed within your source control, allowing you to use change requests for granular inspection of changes before they are applied. </p><p id="""">Your infrastructure becomes repeatable, predictable and consistent. Running the <em id="""">terraform apply </em>command will produce the same results every time. This stands in contrast to the manual process of clicking through a web console based on a document containing instructions, or from memory. </p><p id="""">In the event of a full or partial failure of your application, you can quickly and easily replace <em id="""">everything</em>.</p><h3 id=""""><em>How </em>do I use Terraform with Release?</h3><p id="""">‍<em id="""">The following example assumes a project layout as outlined in the image below</em></p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-center"" data-rt-type=""image"" data-rt-align=""center""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/62f2f31812068e6e8a48a345_-gB_y1CgCzfUWKFHhUMQpB3BKdN2EyZVPrPlUfpEcdI7eB5zHGGU3yQ27yeNCqRxmbVgjjzZIlSud90538QyQOnZZuKVxoMvubLMWTIPHlnX6xvQsIbvIYXfdBjfRNyCOz6A7-LJfcxSQtd3VifbYk8.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><p id="""">When writing your Terraform, it’s important to keep in mind that you want it to be repeatable without resource naming collisions, regardless of how many times you execute the same combination of resources. This can be achieved by using dynamic values to namespace resources. Release makes available to you all of the Release generated environment variables for use within your <em id="""">.tfvars</em> file, allowing you to use the environments’ context as the namespacing material. For example, you might have the following inside of your <em id="""">.tfvars</em> file</p><p id=""""># .release/lambda.tfvars<br><br>namespace = ""${RELEASE_ACCOUNT_ID}/${RELEASE_ENV_ID}/""</p><p>‍</p><p id="""">When specifying the name of a resource, we’ll use a Lambda as an example, you would reference that namespace in the name, like below</p><p id=""""># terraform/lambda.tf<br><br>resource ""aws_lambda_function"" ""lambda"" {<br> &nbsp;function_name = ""${vars.namespace}/lambda""<br> &nbsp;...</p><p id="""">‍</p><p id="""">The inclusion of the environment_id ensures that each time this Terraform is executed by Release, it will be unique.</p><p id="""">WIthin the Application Template for a given Release Application, there is an abstraction over IaC providers, such as Terraform, that we can leverage. For the purposes of this demonstration, we will be using Terraform in conjunction with AWS resources.</p><p id="""">A new section has been added to the template at the top level - infrastructure. This section takes an array with a few required and optional fields. Required fields are name (this is how you will reference the aforementioned Terraform in the workflows section) and type (for now, the only supported type is Terraform but over time we will add abstractions for Pulumi, CDK, etc.). Optionally, you can also specify a directory (where to find the Terraform) and values (where to find environment variables). By default, if no directory is specified then Release will assume your Terraform is at the root of the repository and if no values file is given, we assume this file does not exist.</p><p id="""">We need to add a few things to our Application Template to support this. We need to first tell Release what to build, then we specify when we execute it.</p><p id="""">To tell Release what we need it to build, we add a new array item to the infrastructure key in our Application Template.</p><p id="""">‍</p><p id=""""># Application Template<br><br>infrastructure:<br>- name: lambda<br> &nbsp;<strong id="""">type</strong>: <strong id="""">terraform</strong><br> &nbsp;directory: ""./terraform""<br> &nbsp;values: "".release/lambda.tfvar""</p><p id="""">‍</p><p id="""">This section tells Release where within the repository to find the code it needs for execution in the workflows. In the future, Release will support remote repositories. Now to execute this, it must be added to your workflows section</p><p id=""""># Application Template<br><br>workflows:<br>- name: setup<br> &nbsp;order_from:<br> &nbsp;- infrastructure.lambda</p><p id="""">‍</p><p id="""">This adds the creation of the Lambda function synchronously to the workflow. Workflows may be parallelized if order of execution is not important during a given step, for more information on how to parallelize your workflows, visit our <a href=""https://docs.releasehub.com/reference-documentation/application-settings/application-template/schema-definition#workflow-parallelization"">documentation</a>.</p><p id="""">Resources created during the execution of the workflow will have their state written to the local file store. In an already planned update, Release will extend this functionality to include the additional support of remote backends, like <a href=""https://aws.amazon.com/s3/"">Amazon S3</a>.</p><h3 id="""">Summary</h3><p id="""">This blog covered at a high level the concepts around Infrastructure-as-Code, some specifics related to Terraform and how to use Terraform in combination with Release Environments to extend their functionality. A repository containing the code written in this sample can be found <a href=""https://github.com/releasehub-samples/terraform-runner-example"">here</a>. If you have any questions, find me on <a href=""https://www.linkedin.com/in/joshdirkx/"">LinkedIn</a> or reach me at josh@release.com.</p>",false,,,,https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/62f2f4ad7568b4f32d847103_joshua-sortino-LqKhnDzSF-8-unsplash.jpg,,josh-dirkx,5,Wed Aug 10 2022 05:58:00 GMT+0000 (Coordinated Universal Time),,
Feature Flags and Ephemeral Environments,feature-flags-and-ephemeral-environments,62aa5a70cd5ba27d9d0d718a,62aa5a70cd5ba27dfb0d72bf,Tue Jan 26 2021 22:22:08 GMT+0000 (Coordinated Universal Time),Wed Oct 30 2024 19:53:06 GMT+0000 (Coordinated Universal Time),Wed Oct 30 2024 19:53:06 GMT+0000 (Coordinated Universal Time),"Learn about feature flags in devops, feature flags and ephemeral environments, and more.","<h3 id=""""></h3>",true,<p>Streamline deployment with feature flags in ephemeral environments with Release.</p>,https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=feature-flags-ephem-envs,"<h3 id="""">Overview</h3><ul id=""""><li id=""""><a href=""#devops"" id="""">Feature Flags in DevOps and Software Development</a></li><li id=""""><a href=""#environments"" id="""">Feature Flag Environments</a></li><li id=""""><a href=""#ephemeral"" id="""">Working with Ephemeral Environments</a></li><li id="""">Step 1: <a href=""#create"" id="""">Create an Application in Release</a></li><li id="""">Step 2: <a href=""#modify"" id="""">Modify the Application Template</a></li><li id="""">Step 3: <a href=""#add"" id="""">Add Environment Variables</a></li><li id="""">Step 4: <a href=""#deploy"" id="""">Deploy</a></li></ul><p id="""">‍</p><div data-rt-embed-type='true'><a id=""devops""></a></div><h3 id="""">Feature Flags in DevOps</h3><p id="""">Feature Flags are a necessary and ubiquitous part of modern software development. As your company and the complexity of your application grows it becomes imperative to be able to control what features are available to your internal development teams, stakeholders and customers. In the long-ago, before-times, we would just have a variable that you would toggle between true and false to control behavior of your application. However, as application development transitioned to the Web we needed the same kind of control, except that hard-coded feature flags just weren’t going to cut it. Enter Dynamic Feature Flags!</p><p id="""">Dynamic feature flags were a big improvement over static feature flags, but also added complexity and presented challenges different from static feature flags. Gone were hard-coded flags, but they were replaced with if statements and more importantly, retrieving the appropriate flags for your application. Most people started by rolling their own, but as developing with feature flags gained popularity many different companies popped into existence looking to solve the problems of:</p><ul id=""""><li id="""">One interface to manage your flags</li><li id="""">Easy maintenance of your flags</li><li id="""">Very fast and reliable retrieval of your flags</li><li id="""">Splitting traffic to one feature or another</li></ul><p id="""">While companies like LaunchDarkly, Optimizely, Rollout, Split.io and others made it fairly easy to create and manage these flags this doesn’t solve all of your issues. Many software orgs, especially as they grow, need lots of environments for testing. This poses a challenge to your Feature Flag setup specifically if your environments are ephemeral.</p><p id="""">Ephemeral environments are like any environment except they will be removed in a relatively short amount of time unlike your staging or production environments. Good examples are:</p><ul id=""""><li id="""">Feature branches</li><li id="""">Sales Demos</li><li id="""">Load Testing</li><li id="""">Refactors</li></ul><p id="""">These environments may not last a long time, but they are exceedingly important and can be just as complex as production. While a sales demo environment may be able to function with seed data, a load testing environment will need production or production-like data and many replicas of each service to give a valid result. These can be super complex to create and manage and their ephemeral nature can play havoc with your feature flag setup.</p><div data-rt-embed-type='true'><a id=""environments""></a></div><h3 id="""">Feature Flag Environments to the Rescue…Sort of</h3><p id="""">LaunchDarkly (and others) recognized this issue and created the concept of environments in their own applications. You can read about their implementation here. They have apis that allow you to create and manipulate these sets of feature flags on an environment by environment basis. This works great if you have a finite set of environments and the set of them doesn’t change often, but with ephemeral environments the ability to spin them up and down is a feature not a bug.</p><p id="""">In order to simplify this issue most people create two kinds of environments in their favorite Feature Flag provider: one for development (or test) and one for production. In larger organizations development teams may have a few, such as development, test, uat, staging, and production. This works fine as long as you don’t want to add another one or you never take the plunge toward truly ephemeral application environments.</p><p id="""">Once you move to ephemeral environments most people take the shortcut of assigning every ephemeral environment to a single Feature Flag environment, which is simple enough, but creates a large problem with people stepping on each other’s toes.</p><p id="""">Imagine you have 10 environments all pointing to a single database with writes happening from all those environments: it’s the same issue here. The great thing about feature flags is the ability to toggle them and see different behavior, but if every environment is pointing to the same one you now have another resource contention problem. If you toggle Feature A ‘on’ what’s to stop your co-worker from toggling it ‘off’? Any issues you have with permanent staging environments are magnified with ephemeral environments.</p><p id="""">The best solution would be upon the creation of an ephemeral environment you would create an environment in LaunchDarkly based on something unique about your ephemeral environment and when it comes up, you would make sure it was using the unique SDK api for that particular Feature Flag Environment. Let’s implement the workflow and see how that would work with Release!<br></p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:491px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""491px""><div id=""""><img src=""https://cdn.prod.website-files.com/603dd147c5b0a4221d1bd360/67228ea1a2ff23c502edc350_603dd147c5b0a404f91bd44f_Ephemeral-environment-creation-workflow.png"" loading=""lazy"" width=""auto"" height=""auto"" id="""" alt=""""></div><figcaption id="""">Ephemeral Environment Creation Workflow</figcaption></figure><div data-rt-embed-type='true'><a id=""ephemeral""></a></div><h3 id="""">Working with Ephemeral Environments</h3><p id="""">In order to try this out with Release we need a repository with a Docker File that has implemented Feature Flags with LaunchDarkly. I’m going to use <a href=""https://github.com/elanderholm/rails_postgres_redis"" target=""_blank"" id="""">this</a> repository on Github and you can do the same by first forking the repository so you can use it to create an application with Release.</p><p id="""">Once you have forked the repository you can navigate to <a href=""https://github.com/elanderholm/rails_postgres_redis"" target=""_blank"" id="""">release.com </a>and sign-in using github in order to follow along with this example.</p><p id="""">The steps to get our ephemeral environments created in Release with support for environments in LaunchDarkly are:</p><ol id=""""><li id="""">Create our application in Release</li><li id="""">Create a job with Release to create the environment in LaunchDarkly</li><li id="""">Add some environment variables so the application can contact LaunchDarkly and pull in the SDK Api key from our newly created LaunchDarkly Environment</li><li id="""">Deploy our Ephemeral Environment</li></ol><blockquote id=""""><em id="""">If you don’t have a launch darkly account, you can create one for free for 30 days to use for this example. You will also need to create at least one feature flag. If you already have a launch darkly account with a lot of feature flags you can just skip this step.</em></blockquote><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1999px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1999px""><div id=""""><img src=""https://cdn.prod.website-files.com/603dd147c5b0a4221d1bd360/67228ea1a2ff23c502edc347_603dd147c5b0a48bbe1bd4a6_Test-flag.png"" loading=""lazy"" width=""auto"" height=""auto"" id="""" alt=""""></div></figure><div data-rt-embed-type='true'><a id=""create""></a></div><p id="""">‍</p><h3 id=""""><em id="""">Create </em>the Application In Release</h3><p id="""">Once we are logged into Release we want to click <strong id="""">Create New Application</strong> in the left-hand sidebar. After doing that we will be presented with Create New Application Workflow.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1201px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1201px""><div id=""""><img src=""https://cdn.prod.website-files.com/603dd147c5b0a4221d1bd360/67228ea1a2ff23c502edc368_603dd147c5b0a41adb1bd4a1_Create-your-application.png"" loading=""lazy"" width=""auto"" height=""auto"" id="""" alt=""""></div></figure><p id="""">First, we will click the “refresh” button to find our newly forked repository. Then, we will select that repository and “Docker” for the “api” service. Finally, name your application. Once you have finished click the purple “Generate App Template” button.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:918px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""918px""><div id=""""><img src=""https://cdn.prod.website-files.com/603dd147c5b0a4221d1bd360/67228ea1a2ff23c502edc365_603dd147c5b0a457cf1bd49f_Pick-your-repository.png"" loading=""lazy"" width=""auto"" height=""auto"" id="""" alt=""""></div></figure><p id="""">Lastly name your application and generate the template for your configuration.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:949px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""949px""><div id=""""><img src=""https://cdn.prod.website-files.com/603dd147c5b0a4221d1bd360/67228ea1a2ff23c502edc353_603dd147c5b0a420a21bd497_Name-your-application.png"" loading=""lazy"" width=""auto"" height=""auto"" id="""" alt=""""></div></figure><div data-rt-embed-type='true'><a id=""modify""></a></div><p id="""">‍</p><h3 id="""">Modify the Application Template</h3><p id="""">Before we can deploy our environment(s) we need to make a modification to our application template and add a few environment variables. We also need to create a job that will create our LaunchDarkly environment upon initial environment deployment. Jobs in Release are described in detail <a href=""https://docs.releasehub.com/reference-guide/application-settings/application-template#jobs"" target=""_blank"" id="""">here</a>. The TL;DR is that with a small amount of configuration you can run any arbitrary script or task in a container. For example, these jobs are very useful for running migrations before a deployment of your backend service. In this case we will run a rake task to set up our LaunchDarkly Environment.</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yaml"">
jobs:
- name: create-launch-darkly-env
  from_services: api
  args:
  - bundle
  - exec
  - rake
  - launch_darkly:create_environment
</code>
</pre></div><blockquote id=""""><em id="""">The above yaml represents a job in Release</em></blockquote><p id="""">We will place the above lines right before the “services” stanza in our application template.</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-none"">
memory:
   limits: 1Gi
   requests: 100Mi
 replicas: 1
jobs:
  - name: create-launch-darkly-env
    from_services: api
    args:
    - bundle
    - exec
    - rake
    - launch_darkly:create_environment
services:
  - name: api
    image: erik-opsnuts-test-001/rails_postgres_redis/api
    has_repo: true
    static: false
</code>
</pre></div><blockquote id=""""><em id="""">Place the jobs snippet into the Application Template</em></blockquote><p id="""">In order for Release to utilize this job as part of the workflow to deploy an environment we will need to add one line near the bottom of the file in the ‘workflows` section. Under ‘setup’:’order_from’ we will add <strong id="""">jobs.create-launch-darkly-env</strong>. Then, click “Save and Continue.”</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-none"">
workflows:
- name: setup
  order_from:
  - jobs.create-launch-darkly-env
  - services.all
- name: patch
  order_from:
  - services.api
  - services.sidekiq
  - services.db
  - services.redis
</code>
</pre></div><blockquote id=""""><em id="""">Place jobs.create-launch-darkly-env before services.all under the workflows stanza</em></blockquote><p id="""">‍<strong id="""">That’s all the configuration needed, now we just need to add two environment variables before we deploy!</strong></p><div data-rt-embed-type='true'><a id=""add""></a></div><p id="""">‍</p><h3 id="""">Adding Environment Variables</h3><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1664px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1664px""><div id=""""><img src=""https://cdn.prod.website-files.com/603dd147c5b0a4221d1bd360/67228ea1a2ff23c502edc385_603dd147c5b0a4c1e21bd47e_Adding%2520Environment%2520Variables.png"" loading=""lazy"" width=""auto"" height=""auto"" id="""" alt=""""></div></figure><p id="""">Click ‘EDIT’ for ‘Default Environment Variables’ to bring up the editor. We will add two environment variables that contain information about LaunchDarkly. They are:</p><p id=""""><strong id="""">LAUNCH_DARKLY_API_KEY</strong>: Your LaunchDarkly Api Key which is found here. If you don’t have an api token create the “+ TOKEN” button to make one. You will want to give it admin privileges. If you can’t do that contact your administrator. <strong id=""""><em id="""">Once you create it, make sure you copy it and paste it somewhere you can retrieve it.</em></strong> LaunchDarkly will obfuscate your token and if you don’t save it somewhere you will need to generate a new one.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:405px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""405px""><div id=""""><img src=""https://cdn.prod.website-files.com/603dd147c5b0a4221d1bd360/67228ea1a2ff23c502edc34d_603dd147c5b0a435c11bd3f9_Create%2520an%2520access%2520token.png"" loading=""lazy"" width=""auto"" height=""auto"" id="""" alt=""""></div></figure><p id=""""><strong id="""">LAUNCH_DARKLY_PROJECT_NAME</strong>: We will just use ‘default’ for this example, but if there is another project you would like to test with feel free.</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yaml"">
defaults:
- key: POSTGRES_USER
  value: postgres
- key: POSTGRES_PASSWORD
  value: postgres
- key: LAUNCH_DARKLY_PROJECT_NAME
  value: default
- key: LAUNCH_DARKLY_API_KEY
  value: your-api-key
  secret: true
</code>
</pre></div><p id="""">Click ‘Save’ to save your environment variables as part of your application configuration. Then, click ‘Build and Deploy’. You will be redirected to the activity dashboard for that application and a Docker build was kicked off in the background. This will be followed by the deployment of the environment for your application. You can view the build and the deployment under the ‘builds’ and ‘deploys’ sections respectively.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:496px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""496px""><div id=""""><img src=""https://cdn.prod.website-files.com/603dd147c5b0a4221d1bd360/67228ea1a2ff23c502edc344_603dd147c5b0a4de251bd418_Build%2520and%2520Deploy.png"" loading=""lazy"" width=""auto"" height=""auto"" id="""" alt=""""></div></figure><div data-rt-embed-type='true'><a id=""deploy""></a></div><p id="""">‍</p><h3 id="""">Your Environment</h3><p id="""">This process of doing the docker build will take a few minutes the first time. Once the build and deployment have finished you can find the url for your new environment by clicking ‘Environments’ on the left and then by clicking into your new environment.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1999px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1999px""><div id=""""><img src=""https://cdn.prod.website-files.com/603dd147c5b0a4221d1bd360/67228ea1a2ff23c502edc36b_603dd147c5b0a424311bd47f_Environment%2520Details.png"" loading=""lazy"" width=""auto"" height=""auto"" id="""" alt=""""></div></figure><p id="""">Once you click on the url for your newly created ephemeral environment another window in your browser will open to the example rails site with postgres and redis. It should look something like this:</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1424px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1424px""><div id=""""><img src=""https://cdn.prod.website-files.com/603dd147c5b0a4221d1bd360/67228ea1a2ff23c502edc34a_603dd147c5b0a446db1bd483_Welcome%2520to%2520Rails%2520on%2520Release.png"" loading=""lazy"" width=""auto"" height=""auto"" id="""" alt=""""></div></figure><p id="""">If you have a flag named ‘test-flag’ in your launch darkly account you can go ahead and toggle it from ‘false’ to ‘true’ and vice versa and reload your environment to see the changes. If you would like to use a different flag, you only need to make one change in: <strong id="""">app/views/welcome/index.html.erb</strong></p><p id="""">&lt;% test_flag=""Rails.configuration.ld_client.variation("" test-flag"",""="""" {key:="""" ""user@test.com""},="""" false)="""" %=""""&gt;&lt;/%&gt;</p><p id="""">Once you have changed ‘test-flag’ to the flag name of your choosing, you only need to commit and push the change to github. Once that happens, Release will automatically do a build and then deploy your changes. When the process finishes, you will be able to see the welcome page change based on your new flag.</p><p id="""">In your LaunchDarkly interface you will see a newly created environment with a name of the form ‘tedfe34`. This name is the same as your <strong id="""">RELEASE_ENV_ID</strong> environment variable that Release creates automatically for your new environment. You will see this value in a few places in the Release UI besides the environment variable editor.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1999px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1999px""><div id=""""><img src=""https://cdn.prod.website-files.com/603dd147c5b0a4221d1bd360/67228ea1a2ff23c502edc362_603dd147c5b0a483951bd487_Feature%2520flags.png"" loading=""lazy"" width=""auto"" height=""auto"" id="""" alt=""""></div></figure><h3 id="""">Conclusion - What’s next?</h3><p id="""">Now that you can get pristine feature flag environments dedicated to your Release environments what’s next? In this example the clean-up would need to be done manually, not a huge deal, but we can do better. Release will be implementing a deeper integration with LaunchDarkly in the near future to make this seamless and handle deleting the environments in LaunchDarkly when your Release environment is removed.</p><p id="""">Stay tuned for integrations with other feature flag providers in the future. If you would like to have environments for your applications that are fast, simple to define and incredibly powerful send us a note at <a href=""mailto:support@releasehub.com"" id="""">support@releasehub.com</a> and we will help you and your team become more efficient utilizing ephemeral environments with Release.</p><p id="""">Hero Image by <a href=""https://unsplash.com/@dnevozhai?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText"" target=""_blank"" id="""">Denys Nevozhai</a> on <a href=""https://unsplash.com/s/photos/traffic?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText"" target=""_blank"" id="""">Unsplash</a></p><h3 id="""">Additional Resources</h3><ul id=""""><li id=""""><a href=""https://releasehub.com/ephemeral-environments"" id="""">What is an Ephemeral Environment?</a></li><li id=""""><a href=""https://releasehub.com/user-acceptance-testing-with-ephemeral-environments"" id="""">User Acceptance Testing (UAT) with Release Ephemeral Environments</a></li><li id=""""><a href=""https://releasehub.com/blog/improve-developer-velocity-with-ephemeral-environments"" id="""">5 Ways To Improve Developer Velocity Using Ephemeral Environments</a></li><li id=""""><a href=""https://releasehub.com/blog/you-need-agile-devops-methodology"" id="""">Agile DevOps Methodology</a></li></ul><p>‍</p>",https://cdn.prod.website-files.com/603dd147c5b0a4221d1bd360/63e422e9f62bdd8403b8cb6d_120220%20(1).jpg,Top view of city traffic,erik-landerholm,7,Thu Dec 03 2020 00:00:00 GMT+0000 (Coordinated Universal Time),,
How To: Get a Free Minecraft Server Running on Release,free-minecraft-server-running-on-releaseapp-io,62aa5a70cd5ba27d9d0d718a,62aa5a70cd5ba26e670d72c8,Tue Jan 26 2021 22:22:08 GMT+0000 (Coordinated Universal Time),Fri Apr 21 2023 20:00:56 GMT+0000 (Coordinated Universal Time),Fri Apr 21 2023 20:04:43 GMT+0000 (Coordinated Universal Time),Setup your own free Minecraft Server running on release.com One of the coolest things about working at Release,"<h3 id="""">Setup your own free Minecraft Server running on release.com</h3><p id="""">One of the coolest things about working at Release has been figuring out all of the fun stuff that we can do with the platform. While our main use case is helping people build environments for their applications, anything that runs in Docker will run easily on Release.</p><p id="""">Early on, I found a handful of repos that helped us build out our platform. One that has been the most fun all along is the <a href=""https://github.com/itzg/docker-minecraft-server"" target=""_blank"" id="""">docker-minecraft-server from itzg</a>. I used it in the early days because it had a little complexity and a fully working docker-compose ecosystem to play around with. It’s got the great side effect of when it runs, I let my kids test it out!</p><p id="""">So while you’re sipping on egg nog and enjoying a 2020 Holiday season on COVID lock-down, here’s a walkthrough of how to get your very own free Minecraft server up and running on Release. </p><p id="""">I highly recommend following along on the video tutorial. I’ve included step by step instructions for anyone that learns better through reading or if you’re confused about a step.</p><p id="""">If you want to see a live version of this setup, we fired up our own Minecraft server that we built using these steps. So if you’re bored over the Holidays, pop in and say hello! Here’s our Server name if you want to say hi.</p><blockquote id=""""><em id="""">Play Minecraft With Us on the Release Team Minecraft Server team-release-minecraft.release.com</em></blockquote><h3 id="""">Full video walkthrough of this tutorial</h3><figure id="""" class=""w-richtext-figure-type-video w-richtext-align-fullwidth"" style=""padding-bottom:75%"" data-rt-type=""video"" data-rt-align=""fullwidth"" data-rt-max-width="""" data-rt-max-height=""75%"" data-rt-dimensions=""1920:1440"" data-page-url=""https://www.loom.com/share/4702d2c9f02e4379af0fa14bbcaa6a22""><div id=""""><iframe allowfullscreen=""true"" frameborder=""0"" scrolling=""no"" src=""//cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.loom.com%2Fembed%2F4702d2c9f02e4379af0fa14bbcaa6a22&display_name=Loom&url=https%3A%2F%2Fwww.loom.com%2Fshare%2F4702d2c9f02e4379af0fa14bbcaa6a22&image=https%3A%2F%2Fcdn.loom.com%2Fsessions%2Fthumbnails%2F4702d2c9f02e4379af0fa14bbcaa6a22-full-1608178029873.jpg&key=96f1f04c5f4143bcb0f2e68c87d65feb&type=text%2Fhtml&schema=loom""></iframe></div></figure><h3 id="""">Detailed instructions to get your Minecraft Server up and running</h3><h4 id="""">Background<br></h4><p id="""">At the time of this writing, we have a <a href=""https://releasehub.com/pricing"" target=""_blank"" id="""">“Starter” plan</a> that’s free so you can give this a shot and have some Holiday fun on Release. Since we’re hosting all of the environments on Release on the starter plan we have a limitation of 2Gb/container. That’s sufficient for a Minecraft server for your kids and their friends.</p><p id="""">To get started, take a look at <a href=""https://github.com/awesome-release/docker-minecraft-server"" target=""_blank"" id="""">https://github.com/awesome-release/docker-minecraft-server</a>, which we cloned from <a href=""https://github.com/itzg/docker-minecraft-server"" target=""_blank"" id="""">itzg</a>. Fork or clone this repo into your GitHub account so you’ve got your own version of it to play around with.</p><p id="""">Once you’ve got your own repo to work with, I recommend taking a quick read through the <a href=""https://github.com/awesome-release/docker-minecraft-server/blob/master/README.md"" target=""_blank"" id="""">README</a>, there are a lot of configuration options and the documentation is extremely well done. </p><p id="""">We’re also going to use the <a href=""https://github.com/rcon-web-admin/rcon-web-admin"" target=""_blank"" id="""">Rcon Web Administrative portal. Take a look at the documentation</a>, <em id="""">specifically the environment variables that can be configured.</em> itzg made a version of this for Docker called <a href=""https://github.com/itzg/docker-rcon-web-admin"" target=""_blank"" id="""">docker-rcon-web-admin</a> that we are using when when we load the rcon and rcon-ws services in this tutorial.</p><p id="""">For this walkthrough, we’re going to bring up a vanilla Minecraft server with an Rcon administrative portal running in a standalone container. This will let you and your kids have full control over the Minecraft server and ban friends who can’t fight off Zombie Pig Men. Here’s an overview of what the system architecture looks like.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/603dd147c5b0a44b9d1bd4a9_minecraft_rcon_overview.png"" loading=""lazy"" width=""auto"" height=""auto"" id=""""></div></figure><p id="""">The master branch of this repo is already setup to work with this docker-compose file in Release. Take a look at the .release.yaml file in the root of the repo’s directory.</p><div data-rt-embed-type='true'><pre>
<pre><code class=""language-markdown"">
compose: examples/docker-compose-with-rcon.yml
</code>
</pre></pre></div><p id=""""><br>This sets the compose directive to `examples/docker-compose-with-rcon.yml’ which tells Release that’s the docker-compose file you want to use. If you want to play around with a Forge server or other examples, just point the .release.yaml file at the corresponding docker-compose.</p><h4 id="""">1. Create a new application in Release</h4><p id="""">Ok, let’s setup the server.</p><ul id=""""><li id="""">Fork, clone or copy this repo: <a href=""https://github.com/awesome-release/docker-minecraft-server"" target=""_blank"" id="""">https://github.com/awesome-release/docker-minecraft-server</a> <a href=""https://docs.github.com/en/free-pro-team@latest/github/creating-cloning-and-archiving-repositories/duplicating-a-repository"" target=""_blank"" id="""">Here are some simple instructions on how to copy this repo over to your account.</a></li><li id="""">Login or create an account on Release here: <a href=""https://release.com"" target=""_blank"" id="""">https://release.com</a></li><li id="""">Follow the steps to create your account. Once your account is created, click the “Create an application” button.</li><li id="""">Select your docker-minecraft-server repo. If you don’t see it in the list, click Configure the Release app on Github link to assign permissions to your repo.</li></ul><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1692px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1692px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/603dd147c5b0a43a661bd49d_select_repo.png"" loading=""lazy"" width=""auto"" height=""auto"" id=""""></div></figure><ul id=""""><li id="""">Add a name for your application. Note this name is used in your server hostname.</li><li id="""">Click Generate App Template. </li></ul><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1668px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1668px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/603dd147c5b0a4484c1bd498_generate_app_template.png"" loading=""lazy"" width=""auto"" height=""auto"" id=""""></div></figure><h4 id="""">2. Edit the generated application template</h4><p id="""">Release automatically detects and creates an application template from the docker-compose file but there are a few edits we need to make based on how this repo works and to make sure we can fit the server into the Starter plan. If you want to dive in, <a href=""https://docs.releasehub.com/reference-guide/application-settings/application-template"" target=""_blank"" id="""">read the documentation about Release Application Templates.</a></p><p id="""">For a little background, take a look at this diagram.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/603dd147c5b0a476481bd4a2_docker_minecraft_server_network.png"" loading=""lazy"" width=""auto"" height=""auto"" id=""""></div></figure><p id="""">We need to make our application reflect this networking setup.</p><p id="""">In Release we have two different kinds of loadbalancers based on Amazon’s <a href=""https://aws.amazon.com/elasticloadbalancing/?elb-whats-new.sort-by=item.additionalFields.postDateTime&elb-whats-new.sort-order=desc"" target=""_blank"" id="""">ELB’s</a> and <a href=""https://docs.aws.amazon.com/elasticloadbalancing/latest/application/introduction.html"" target=""_blank"" id="""">ALB’s</a>.</p><p id="""">We also need to make sure we’re using the correct type of port for the use case. There are two types of ports container_port and node_port. In short, a node_port is exposed to the Internet and a conatiner_port is not. Because the rcon service is only internally facing, we want to set its port to a container_port. For more info on setting the correct type of port, <a href=""https://docs.releasehub.com/reference-guide/application-settings/application-template#ports"" target=""_blank"" id="""">read about ports in Release</a>.</p><p id="""">So let’s make the changes necessary to setup the Application Template correctly.</p><h5 id="""">Update memory to 2Gb</h5><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/603dd147c5b0a47ef71bd49a_increase_2gi.png"" loading=""lazy"" width=""auto"" height=""auto"" id=""""></div></figure><ul id=""""><li id="""">The Minecraft server is setup to use 1Gb of max memory so we need to set the default memory limit in Release to 2Gb to leave enough room with some overhead. Edit the app template to allow the services to use up to 2Gb of memory.</li></ul><h5 id="""">Update hostnames and ports</h5><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1280px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1280px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/6041726d16b6c27de9b48f7e_ports-minecraft.gif"" loading=""lazy"" width=""auto"" height=""auto"" id=""""></div></figure><ul id=""""><li id="""">Change the port type for 25575 to container_port and remove the target_port line.</li><li id="""">Add a loadbalancer: true for port 25565.</li><li id="""">Add a hostname field at the same level as ports in the file and set to hostname: my-server-${env_id}-${domain}. You can set my-server to anything you’d like. ${env_id} and ${domain} are variables that Release will automatically fill in to customize your domain.<br>‍</li></ul><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1280px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1280px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/6041757aebc151662000c462_remove-minecraft-alb.gif"" loading=""lazy"" width=""auto"" height=""auto"" id=""""></div></figure><ul id=""""><li id="""">Remove the ALB hostname for the minecraft service. (We only need the minecraft service exposed on port 25565 via an ELB not an ALB which is for http/https).</li><li id="""">Click “Save and Continue”.</li></ul><h4 id="""">3. Setup Environment Variables</h4><p id="""">We need to set a few passwords via environment variables and an <a href=""https://docs.releasehub.com/reference-guide/reference-examples/environment-variable-mappings"" target=""_blank"" id="""">environment variable mapping</a> for the rcon websocket hostname. For more information about these environment variables, see the documentation/README files here:</p><ul id=""""><li id=""""><a href=""https://github.com/awesome-release/docker-minecraft-server/blob/master/README.md#rcon"" target=""_blank"" id="""">Rcon environment variables for the minecraft service.</a></li><li id=""""><a href=""https://github.com/rcon-web-admin/rcon-web-admin#environment-variables"" target=""_blank"" id="""">Environment variables for rcon-web-admin</a></li></ul><p id="""">In this diagram we show the passwords that need to be set via environment variables.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/603dd147c5b0a476481bd4a2_docker_minecraft_server_network.png"" loading=""lazy"" width=""auto"" height=""auto"" id=""""></div></figure><h5 id="""">Setup passwords via environment variables</h5><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:2224px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""2224px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/6041792e2c96751130aa2f11_env-mappings.gif"" loading=""lazy"" width=""auto"" height=""auto"" id=""""></div></figure><p id="""">On the minecraft service we need to set a password for its local rcon service on port 25575 so other containers can connect to it. RCON_PASSWORD is the environment variable that needs to be set for this and on the rcon and rcon-ws service we need to set RWA_RCON_PASSWORD to the same value so those services can control the minecraft server.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:2224px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""2224px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/604178f19efda74b03dabeac_rwa-password.gif"" loading=""lazy"" width=""auto"" height=""auto"" id=""""></div></figure><ul id=""""><li id="""">Click on “Edit” for “Default Environment Variables”.</li><li id="""">Set RCON_PASSWORD in the minecraft service and add secret: true. To encrypt this value in the database.</li><li id="""">Set RWA_RCON_PASSWORD to the same value as you set in step 2 on both the rcon and rcon-ws services.</li><li id="""">Set RWA_PASSWORD which will be the default password used for the RCON Web Administration tool in both the rcon and rcon-ws services. Make sure to add secret: true to encrypt this value.</li></ul><h5 id="""">Setup mapping of environment variable RWA_WEBSOCKET_URL_SSL</h5><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/603dd147c5b0a4b5561bd49e_rwa_websocket_hostname_mapping.png"" loading=""lazy"" width=""auto"" height=""auto"" id=""""></div></figure><p id="""">The last environment variable we need to add is a mapping that tells Release to map RWA_WEBSOCKET_URL_SSL to a dynamically created environment variable for hostnames created in Release RCON_WS_INGRESS_HOST. RWA_WEBSOCKET_URL_SSL tells the Rcon Web Admin tool which container host url is running the websocket for this service which is on our rcon-ws service on port 4327.</p><p id="""">RCON_WS_INGRESS_HOST is automatically created everytime a new environment is created by Release and always conatins the correct hostname for rcon-ws. This value can change when new environments are created, thus we can’t just hard set RWA_WEBSOCKET_URL_SSL. This is where an environment variable mapping comes into play. The diagram above represents the change we need to add in our Default Environment Variables.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:2224px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""2224px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/6041792e2c96751130aa2f11_env-mappings.gif"" loading=""lazy"" width=""auto"" height=""auto"" id=""""></div></figure><ul id=""""><li id="""">Add a mapping: directive and map RWA_WEBSOCKET_URL_SSL to the top of the file.</li></ul><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yaml"">
mapping:
  RWA_WEBSOCKET_URL_SSL: wss://${RCON_WS_INGRESS_HOST}
  </code>
  </pre></div><p id="""">When these chages and your env passwords have been made, your file should look like this:</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yaml"">
---
mapping:
  RWA_WEBSOCKET_URL_SSL: wss://${RCON_WS_INGRESS_HOST}
defaults:
- key: RWA_RCON_HOST
  value: minecraft
services:
  minecraft:
  - key: EULA
    value: 'TRUE'
  - key: MAX_MEMORY
    value: 1G
  - key: ENABLE_RCON
    value: true
  - key: RCON_PASSWORD
    value: ""rcon_password""
    secret: true
  - key: VIEW_DISTANCE
    value: 15
  - key: MAX_BUILD_HEIGHT
    value: 256
  rcon:
  - key: RWA_RCON_HOST
    value: minecraft
  - key: RWA_RCON_PASSWORD
    value: ""rcon_password""
    secret: TRUE
  - key: RWA_PASSWORD
    value: ""rwa_password""
    secret: true
  rcon-ws:
  - key: RWA_RCON_HOST
    value: minecraft
  - key: RWA_RCON_PASSWORD
    value: ""rcon_password""
    secret: TRUE
  - key: RWA_PASSWORD
    value: ""rwa_password""
    secret: true
    </code>
    </pre></div><ul id=""""><li id="""">Click ‘Save &amp; Deploy’</li></ul><p id="""">Your environment is now deploying, you can click on the deploy and watch its progress. When it’s done, navigate to the environment screen and inspect your created hostnames.</p><h4 id="""">4. Setup the Minecraft Client to Connect to your new server and login to the RCON web admin tool.</h4><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:2358px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""2358px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/604179c93242ff72a008208e_setup-minecraft-server.gif"" loading=""lazy"" width=""auto"" height=""auto"" id=""""></div></figure><ul id=""""><li id="""">Using the minecraft hostname that was created by Release, create a new server within the Minecraft Client.</li></ul><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:2258px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""2258px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/604179fe638a9cb9a12a4a9b_rcon-web-admin.gif"" loading=""lazy"" width=""auto"" height=""auto"" id=""""></div></figure><ul id=""""><li id="""">Click on the rcon hostname that was created by Release to access the RCON Web Admin user interface.</li><li id="""">Login using the same password you set for the RWA_PASSWORD environment variable.</li><li id="""">Add the minecraft server.</li><li id="""">Add the console widget.</li><li id="""">Run admin commands on your server!</li></ul><h3 id="""">What if it doesn’t work???</h3><p id="""">If for any reason you made a mistake and something doesn’t work. You can navigate to your App Settings and edit your Application Template and your Default Environment Variables. Double check you’ve made the proper settings. Once you’ve made these edits, navigate to your environments screen, delete your environment and create a new one. The beauty of Release is environments can be torn down and up whenever you want. Here are the links to the docs on how to edit your App Template and Default Environment Variables.</p><ul id=""""><li id=""""><a href=""https://docs.releasehub.com/reference-guide/application-settings"" target=""_blank"" id="""">Modify Application Settings</a></li></ul><h5 id="""">Delete and Create a new Environment</h5><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:2200px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""2200px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/60417a3d7248c16ffc81bd6b_delete-and-create.gif"" loading=""lazy"" width=""auto"" height=""auto"" id=""""></div></figure><h3 id="""">Conclusion</h3><p id="""">You now have your very own Minecraft Server running on the Release Starter Plan. This server was created in an Ephemeral Environment in Release and will destroy itself in 7 days. If you’d like your server to remain indefinitely, you’ll need to delete the environment and re-create it as a permanent environment.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:2200px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""2200px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/60417a3d7248c16ffc81bd6b_delete-and-create.gif"" loading=""lazy"" width=""auto"" height=""auto"" id=""""></div></figure><p id="""">Make sure you choose permanent when creating the environment.</p><p id="""">With the RCON Web Admin tool you can control and make the server your own special place. If you have any questions, please contact the Release team at <a href=""mailto:hello@release.com"" id="""">hello@release.com</a>. Jump in and say hello on our Release Team Minecraft Server here:</p><blockquote id=""""><em id="""">team-release-minecraft.releasehub.com</em></blockquote><p id=""""><a href=""https://app.releasehub.com/auth/login-page"" target=""_blank"" id="""">Don’t forget to signup for your free Release account on our starter plan!</a></p><p id="""">We also have a Release Community Discord you can join if you need help or just want to say hello. Join us here: <a href=""https://discord.gg/8FKKZvBKwc"" target=""_blank"" id="""">Release Community Discord</a></p><p id=""""><strong id="""">Happy Holidays from the Release Team!!!</strong></p>",false,,,,https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63e4238842aa6384d73d59df_121420%20(1).jpg,Minecraft game miniature,tommy-mcclung,6,Tue Dec 15 2020 00:00:00 GMT+0000 (Coordinated Universal Time),,
Full Fidelity Data for Ephemeral Environments,full-fidelity-data-for-ephemeral-environments,62aa5a70cd5ba27d9d0d718a,6407ab016a26e77e18141ecd,Tue Mar 07 2023 21:22:09 GMT+0000 (Coordinated Universal Time),Wed Oct 30 2024 19:46:56 GMT+0000 (Coordinated Universal Time),Wed Oct 30 2024 19:46:56 GMT+0000 (Coordinated Universal Time),Why you should use production-like data in ephemeral environments,,true,<p>Ready to bring production-like data to your ephemeral environments?</p>,https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=full-fidelity-data,"<p id="""">This blog post is a followup to the wildly successful webinar on the same topic. Refer to our webinar <a href=""https://www.youtube.com/watch?v=TAhz_UxPWG4"" target=""_blank"" id="""">“Full Fidelity Data for Ephemeral Environments”</a> for more information. In the webinar and this post, we will explore why you should use production-like data in ephemeral environments, how you can (and should) do so, and how to reduce the cost, complexity, and avoid difficulties associated with using production-like data at scale.</p><h3 id="""">Overview</h3><p id=""""><a href=""https://release.com/ephemeral-environments"" id="""">Ephemeral environments</a> are an indispensable tool in almost any software development workflow. By creating environments for testing and pre-production phases of the <a href=""https://docs.release.com/reference-documentation/workflows-in-release"" target=""_blank"" id="""">workflow</a>, advanced software development teams can shift testing and verification “left” (that is, earlier in the life cycle rather than later when features or bugs reach production).</p><p id="""">The reason that environments should be ephemeral is that they can be set up on-demand with specific features and/or branches deployed in them and then torn down when the task is completed. Each feature or branch of the development code base can be tested in an isolated environment as opposed to the legacy shared and fixed stages of QA, <a href=""https://release.com/staging-environments"" id="""">Staging</a>, <a href=""https://release.com/user-acceptance-testing-with-ephemeral-environments"" id="""">User Acceptance Testing (UAT)</a>, and so forth.</p><p id="""">Ephemeral environments are always used for a specific purpose so it is clear which feature and code base is deployed. The data is ideally based on a very close approximation to production so that the feature under development or test can be deployed into production with as much confidence as possible.</p><h3 id="""">Why Use Production-Like Data in Ephemeral Environments?</h3><p id="""">The best way to test new features or fix bugs that exist in production is to use the same code and data from the production environment. Version Control Systems (VCS) like git solved this problem&nbsp; decades ago, at least for&nbsp; code. But the data portion has long lagged behind due to complexity in access and cost, which&nbsp; we will address toward the end of this post.</p><p id="""">Testing code against stale or seed data is fine for automated testing, but when developing new features or diagnosing problems in production, the data should mimic production as closely as possible. Otherwise, you are risking chasing the same fugs over and over again, or launching a faulty feature.&nbsp;&nbsp;</p><p id="""">It is rare that data in production is static and unchanging; if it were, you could make the database read-only! While some amount of read-only or write-rarely data exists, it almost always needs to be updated and changed at some point, the only difference is a matter of frequency of updates.</p><p id="""">Because the data is a living and breathing thing that changes and evolves constantly&nbsp; and possibly is updated based on customer live inputs or actions, the legacy strategy of fixed QA, staging, and test databases get out of date extremely quickly. In my experience, a fixed environment will stray from production data in as short a time as a few days or weeks. Many times the QA or staging databases are several years out of date from production, unless you specifically “backport” data from production.</p><p id="""">Lastly, production databases and datasets are often quite large (and grow larger every day) compared to fixed QA and staging databases. Thus, testing on limited data or fake seed data when developing new features or changing the code base can introduce unexpected regressions in performance or bugs when large results are pulled out of a table.</p><h3 id="""">Should I Really Use Production-like Data in Ephemeral Environments?</h3><p id="""">Short answer, yes. However, you need to be mindful of the possible moral, ethical, and legal implications in using actual production data subject to HIPPA or other regulatory controls. This is why it is crucial to generate a so-called Golden Copy of the data that is scrubbed of any private or confidential data, while still maintaining the same qualities as production in terms of size, statistical parameters, and so forth. This Golden Copy is the source of truth that is&nbsp; updated frequently from your actual production data. We recommend daily updates, but depending on your particular use case it can be more or less often.</p><p id="""">With the&nbsp; Golden Copy that is sufficiently production-like (each case varies in how much fidelity to production is required), it is possible to accurately portray behavior features as they would occur in production. Transactions, events, customer (faked) accounts, and so forth are all available and up-to-date with real (-ish) data. For example, if there is a bug that manifests in production, an engineer could easily reproduce the bug with a high degree of confidence, and either validate or develop fixes for the problem.</p><p id="""">Testing features with fake or fixed data is suitable for automated testing but for many use cases, especially when testing with non-technical users, real production-like data is valuable to ensure the feature not only works properly but also looks correct when it reaches production.</p><h3 id="""">Isn’t that Terribly Difficult, Expensive, and Prohibitive? Or, “My Database is too Unique.”</h3><p id="""">The most common objections to using a production-like dataset are around the difficulties in creating, managing and accessing the dataset, and around the overall cost. I can try to address these in two points: the first one (creation/maintenance/access) can be&nbsp; pretty difficult depending on your use case, but it can be solved; the second one (cost) can readily be handled in this modern era of cloud computing and on-demand managed services offered by cloud providers.</p><p id="""">The first problem is that access to production data is usually kept in the hands of a small group of people, and those people are usually not the software engineers who are developing code and testing new features. Separation of concerns and responsibilities is vital for keeping production data clean and secure, but this causes problems when engineers need to test new features, verify bugs in production, or experiment with changes that need to be made.</p><p id="""">The best practice is to generate a Golden Copy of the data that you need as mentioned above. The Golden Copy should be a cleaned, stable, up-to-date version of the data that exist in production, but without any data that could compromise confidentiality or proprietary information if it were to accidentally be exposed, (or even internally exposed).</p><p id="""">What I tell most people who are involved with production datasets is to create a culture of welcoming access to the Golden Copy and distributing the data internally on a self-service model so that anyone who would like the latest snapshot can access it relatively easily and without a lot of hoops to jump through. Making the data available will ensure that your cleaning and scrubbing process is actually working properly and that your test data is going to have a high degree of similarity to the data in production. This will only make your production data and operations cleaner and more efficient, I promise you.</p><p id="""">The second problem is that allowing software engineers, QA people, testers, and even product people access to these datasets comes at a cost. Every database will typically cost a certain amount of money to run and store the data, but there are definitely some optimisations you can implement to keep costs down while still enjoying access to high quality datasets.</p><p id="""">The best way to keep costs down is to make sure that the requested access to the production-like dataset is limited in time. For example, a software engineer might only need to run the tests for a day or a week. At the end of that time, the data should automatically be expired and the instance removed because it is no longer needed. If the data is still needed after the initial period of time has expired, you can implement a way to extend the deadline as appropriate.</p><p id="""">Another way to reduce costs is to use Copy on Write (COW) technologies if they are available for your database engine and cloud provider. The way this works is that the Golden Copy holds most of the data in storage for use, while the clones that are handed out to engineers are sharing most of the data with the original. It is only when a change or update is made to a table or row that the data is&nbsp; “copied” over for the clone to use. This is what a Copy on Write will do: it means that the only additional costs for storage on the clone are the incremental changes or writes that are executed during testing.</p><p id="""">Another good way to reduce costs is to pause or stop a database when it is not in use. Depending on your cloud provider, you may be able to execute a pause or stop on the database instance so that you can save money during the evenings or weekends when the database is not likely to be in use. This can save 30-60% off your costs versus running the database 24/7.</p><p id="""">The good news is that <a href=""https://docs.release.com/"" id="""">Release</a> offers all of the features, including cloning snapshots from the Golden Copy, pausing and restarting databases on a schedule and expiring them as well, and using COW to save time, money, and storage. We support all of the above (using <a href=""https://docs.release.com/frequently-asked-questions/aws-support-faqs"" id="""">AWS</a>, GCP and soon Azure) and we can easily build a dataset pipeline that is checked out to each <a href=""https://www.merriam-webster.com/dictionary/ephemeral"" id="""">ephemeral</a> environment where your code is deployed.</p><p id="""">You can refresh the dataset to get new changes from the Golden Copy (which is a cleaned version of production, or directly from production as you wish), and you can also update the data in your ephemeral environment to start over from scratch with a new database. You can also set an expiration for each ephemeral environment that will last as long as the branch or feature pull request is open, and engineers can extend the environment duration as needed. Lastly, you can set a schedule for pausing the databases in the dataset so that you save additional costs when the environments are unlikely to be used.</p><p id="""">Ready to learn more? Watch the on-demand webinar <a href=""https://www.youtube.com/watch?v=TAhz_UxPWG4"" id="""">“Full Fidelity Data for Ephemeral Environments.”</a></p><p>‍</p>",https://cdn.prod.website-files.com/603dd147c5b0a4221d1bd360/640b469aa86b83a04600fe16_030723.jpg,Full Fidelity Data for Ephemeral Environments,regis-wilson,8,Tue Mar 07 2023 21:20:00 GMT+0000 (Coordinated Universal Time),,
GitLab self-managed now available on Release  ,gitlab-self-managed-now-available-on-release,62aa5a70cd5ba27d9d0d718a,641b6dfe834f15fa23425edc,Wed Mar 22 2023 21:07:10 GMT+0000 (Coordinated Universal Time),Thu Apr 25 2024 20:31:05 GMT+0000 (Coordinated Universal Time),Thu May 02 2024 20:19:24 GMT+0000 (Coordinated Universal Time),Release now supports self-managed versions of GitLab Enterprise and Community Editions.,"<p id="""">I’m happy to announce that we now support customers using self-managed versions of <a href=""https://about.gitlab.com/"" target=""_blank"">GitLab</a> Enterprise and Community Editions. Like with the <a href=""https://docs.gitlab.com/ee/subscriptions/index.html#choose-between-gitlab-saas-or-gitlab-self-managed"" target=""_blank"">SaaS</a> offering, you can now integrate Release and deploy environments against repositories that live in your self hosted GitLab service.</p><p id="""">Release is a platform that enables you to deploy all your environments and applications. As part of your development cycle, you can use Release to test branches before they hit production using our ephemeral environments. With <a href=""https://docs.gitlab.com/ee/subscriptions/index.html#choose-between-gitlab-saas-or-gitlab-self-managed"" target=""_blank"">instant Datasets</a>, it helps your team ship with confidence by testing changes with production-like data.</p><p id="""">We will go over the GitLab setup steps today. We will:</p><ol id=""""><li id="""">create an application on GitLab,</li><li id="""">configure your Release account to talk to GitLab,&nbsp;</li><li id="""">connect your Release account to GitLab</li><li id="""">use ephemeral environment to test your code with Release</li></ol><p id="""">&nbsp;If you have any questions about Release or any new features we are working on, feel free to <a href=""mailto:hello@release.com"" id="""">drop us a line</a>.&nbsp;&nbsp;</p><h3 id="""">Create an application on GitLab</h3><p id="""">For Release to authenticate with your hosted solution, you need to configure an application on GitLab’s side to whitelist Release.</p><p id="""">In the user’s settings, there’s a section called “Application”. If you access that section, you will be presented with a screen to create a new application. This application needs to have the proper settings defined so Release can access your GitLab repository. In essence, you’ll need to make sure two things are set up properly: <strong id="""">the callback URL</strong> and <strong id="""">the scopes</strong>.</p><p id="""">The scopes is a set of permissions that will be granted to Release on your behalf. For the integration to work properly, you need to set the scope to “API”.</p><p id="""">The callback URL is the URL that GitLab is authorized to send requests back to. If you’ve ever worked with OAuth, this will be familiar to you.</p><p id="""">Here’s how the application should look like in GitLab:</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:629px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""629px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/641b790f9965f457df886619_Screenshot%202023-03-22%20at%202.54.05%20PM.png"" loading=""lazy"" id="""" width=""auto"" height=""auto""></div></figure><p id="""">Once your application is saved, you will see additional information that will be needed on the Release side. Keep that tab open until you have successfully connected Release to GitLab.</p><p id="""">Here’s how it should look like in your GitLab account: </p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:597px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""597px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/641b798646c1142aa1ead079_Screenshot%202023-03-22%20at%202.55.59%20PM.png"" loading=""lazy"" id="""" width=""auto"" height=""auto""></div></figure><h3 id="""">Configure your Release account to talk to GitLab</h3><p id="""">You can now configure Release to talk to GitLab. Go to your <a href=""https://app.release.com/admin/user-profile"" target=""_blank"">user profile</a> to see all the current integrations we support. We want to use GitLab self-hosted.&nbsp;</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:627px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""627px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/641b79a01782869e89511bdc_Screenshot%202023-03-22%20at%202.56.33%20PM.png"" loading=""lazy"" id="""" width=""auto"" height=""auto""></div></figure><p id="""">Click on Configure to open up a dialog that allows you to enter the credentials you created in GitLab earlier.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:556px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""556px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/641b79b61dcc9a07064c5a6a_Screenshot%202023-03-22%20at%202.56.54%20PM.png"" loading=""lazy"" id="""" width=""auto"" height=""auto""></div></figure><p id="""">Fill in your GitLab URL, ClientID and Client secret and Save. Note that pressing Save will only store the information and won’t validate what you have entered. Validation will occur once you attempt to connect to GitLab. The “connect” button will appear once you save your information.&nbsp;</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:630px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""630px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/641b79cc9edc53aec6694b62_Screenshot%202023-03-22%20at%202.57.18%20PM.png"" loading=""lazy"" id="""" width=""auto"" height=""auto""></div></figure><h3 id="""">Connect your Release account to GitLab</h3><p id="""">Clicking on that Connect button will open a new tab and direct you to GitLab. If your information checks out, you will be redirected to your GitLab self-hosted service.</p><p id="""">The first time Release will attempt to connect to GitLab, you will see a dialog that will ask if you want to authorize Release to connect to your application. Once you authorize the connection, GitLab will redirect you back to Release. At this point, GitLab and Release are connected.</p><h3 id="""">Use ephemeral environment to test your code with Release</h3><p id="""">Now that you are connected and ready to go, you can use Release to deploy ephemeral environments connected to any working branches in GitLab.</p><p id="""">For every app you configure, the Pull Requests your team does will automatically deploy to Release as an Ephemeral environment. These unique Ephemeral environments are then available for QA and testing by everyone on your team (see <a href=""https://docs.release.com/getting-started/prepare-to-use-release"" id="""">docs</a> for more details).&nbsp;</p><p id="""">This is very helpful to teams that want to validate their changes before hitting production as you can deploy an application, test features and make sure there is no regression. It’s the kind of safety net you didn’t think you needed until you started using it.</p>",false,,,,https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/641b6cba855a3866007b7c93_release%2BGitLab.jpg,Release + GitLab,pier-olivier-thibault,3,Wed Mar 22 2023 21:00:00 GMT+0000 (Coordinated Universal Time),customer-stories,
GitOps vs DevOps: Understanding the Difference,gitops-vs-devops-understanding-the-difference,62aa5a70cd5ba27d9d0d718a,6318c157a03cf06370954de5,Wed Sep 07 2022 16:05:43 GMT+0000 (Coordinated Universal Time),Fri Apr 21 2023 17:55:13 GMT+0000 (Coordinated Universal Time),Fri Apr 21 2023 18:22:03 GMT+0000 (Coordinated Universal Time),Let's look at a comparison between the classical GitOps vs DevOps. What are the key similarities and differences?,"<p id="""">When it comes to cooperative coding, DevOps is fundamentally a culture of continuous integration and deployment that aims to keep things running smoothly. GitOps is a newer, similar practice that builds on many of the same principles, only with a specifically version-tracking-based approach. It can be quite difficult to understand the differences, as both practices overlap greatly. In this article, we'll take a look at that overlap and outline some of the key differences.&nbsp;</p><h3 id="""">What Is DevOps?</h3><p id="""">DevOps is, put simply, a strategy for increasing the efficiency of coding projects by connecting development and operations, enabling them to work together in a steady feedback loop of <a href=""https://en.wikipedia.org/wiki/CI/CD"" target=""_blank"">CI/CD</a>. Instead of keeping development and operations separate, and needing to coordinate between them as two separate branches of a project, new developments are continually integrated. The product is then tested, deployed, and monitored by operations and fed back for further development.&nbsp;</p><h4 id="""">GitOps: Git for Automation</h4><p id="""">In many ways, GitOps can seem like a branch of classic DevOps. GitOps represents an approach to DevOps functionality and goals that specifically relies on Git, <a href=""https://www.cyberfella.co.uk/2020/03/23/what-is-devops/"" target=""_blank"">the most commonly used version tracking software today</a>. In fact, because Git is so commonly used, many DevOps pipelines also use Git! However, GitOps refers to a practice that builds on the specifics of Git. It prioritizes automation specifically tied to Git's version control systems, and it leans into those functionalities. A few examples of this are:&nbsp;</p><ul id=""""><li id="""">Rollouts and rollbacks: With version control tracking, it's easy to deploy a new feature, environment, or other aspect, then quickly revert back to an earlier, more stable version if it doesn't behave as expected.</li><li id="""">Infrastructure as code: With Git, environments and infrastructure can be version tracked as well.</li><li id="""">Push versus pull: Git allows operations to pull and review changes, and then the requested changes are automatically deployed.</li></ul><h4 id="""">GitOps vs DevOps</h4><p id="""">At their most basic level, DevOps and GitOps have the same root: a continuous pipeline that connects all the steps of a stable project into one flow. However, GitOps is more specific and tied to a singular tool. This leads to a few subtle, but important, differences.&nbsp;</p><h3 id="""">Better CD in CI/CD Pipelines</h3><p id="""">A classic DevOps pipeline often starts with a repository for the developers' contributions. This is often in GitHub already, but it doesn't necessarily have to be for DevOps. Developers would then commit changes, which would be handled by a CI/CD tool (such as Azure). The CI/CD tool runs automated tests and integrates changes if they're acceptable. This then moves immediately into deployment via an automated deployment tool.&nbsp;</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1000px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1000px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/6318c04b24a46fed4e0a9b85_btB0UzLsU-Ms8Tyst7uqbdz1i_n_d97332txBo16ApLxWILXUzaTZbBSJRrVrhBRjowN5ZdopUSUnvZRG63LlZA8OJBlrVreMmpxeR3VP16l0vM6l9GBvTyKek8Or8qxPwblTBbBZVsiEEHaeVWxwuRXAKrVAijfLgvND7pL2TFl3jnBl4ReTemaVA.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><p id="""">GitOps expands on the continuous deployment (CD) stage of the pipeline, creating a more secure way to deploy with fewer failure points. The continuous integration (CI) step remains largely the same, but GitOps gives CD its own focus. GitOps tools continuously compare the current state of the operating environment with the desired state captured in the repository. If changes are made, they're pulled to the deployment server. Deployment automation tools can now pull those changes in turn—and revert to the latest stable version if there's an issue.&nbsp;</p><p id="""">It's important to note that this kind of CD implementation isn't exclusive to GitOps. Though GitOps is conducive to good deployment strategies, in part due to automated version control tracking making them easier, any DevOps pipeline can employ them. They also aren't Git exclusive! However, they're often tied to GitOps as the shift to GitOps has placed more attention and focus on the CD step using Git functionalities to refine and improve the process of deployment.&nbsp;</p><h3 id="""">Infrastructure as Code</h3><p id="""">One thing that comes up frequently when talking about GitOps is infrastructure as code (IaC). <a href=""https://microtica.com/blog/infrastructure-as-code-from-the-beginning-to-gitops/"" target=""_blank"">Infrastructure as code</a> is a method of managing infrastructure to ensure that projects run in a consistent environment. This is done by including infrastructure inside of your code, and in GitOps, this means Git will apply the same version control to infrastructure as it does to any other part of the source code. In fact, Git is such a useful tool for this that many DevOps pipelines today also use Git for it.&nbsp;</p><p id="""">Though many people regard GitOps as a direct response to the rise of the IaC concept, the relationship between GitOps and IaC has some subtle nuances. You can read about <a href=""https://microtica.com/blog/infrastructure-as-code-from-the-beginning-to-gitops/"" target=""_blank"">some of these considerations in more detail here</a>. Ultimately, while DevOps often deals with IaC as well, Git simplifies and enables this practice to the point that any DevOps techniques along these lines will either overlap significantly with GitOps or simply use Git. GitOps also places specific priority on IaC, while DevOps encompasses a greater, more generalized culture.&nbsp;</p><p id="""">A good example of this is Release's own <a href=""https://docs.releasehub.com/reference-documentation/gitops"" target=""_blank"">environment configuration tracking</a>. In this case, we track the environment as part of the infrastructure. Release's UI builds on Git's existing version tracking, expanding on the common aspects of GitOps, such as automation, templating, and version tracking.&nbsp;</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-center"" data-rt-type=""image"" data-rt-align=""center""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/6318c04bf9bc57cb6fb05ab6_8Ppl4gvGodXtU3oEpR_Sv3ppbnJ-VmtqTCiyek3BGlMP6x39_jTqUdKrkBYECeYmdXjkT8PQMH1r0J5QNxFlCFZ5zOab-RZOO4RJZMXgbxAuHpsvJmuEYt_W6LJi2z7lM3V1_OAo6dFXLSZnKXXQD5rUeW_9j0rnRbp-O76XwW5fLsbrbQAY4qrlCA.jpeg"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><h3 id="""">Challenges and Best Practices of GitOps</h3><p id="""">Ultimately, GitOps is an approach within the DevOps mindset that helps get the most out of Git and its toolkit. A majority of its advantages over traditional DevOps come from the strategic use of its foundations to further streamline a CI/CD pipeline. As such, to get the most out of GitOps, you need to know the foremost challenges that can arise and how to best navigate them. Here are a few of the major points to keep in mind:&nbsp;</p><ul id=""""><li id="""">Security: One issue that often arises with Git is security. Sensitive data (such as security keys, passwords, or tokens) cannot be stored in areas that are committed to shared repositories. <a href=""https://release.com/blog/how-to-manage-gitops-secrets-a-detailed-guide"">Because these cannot be stored in plain text</a>, you need additional tools to either encrypt them or store them elsewhere.</li><li id="""">Multi-environment configurations: Git only allows one environment per branch, and it requires that you define the environment within the repository. Any changes to environments, therefore, must be committed and pushed. Even if one environment per branch is enough, it can easily snowball. Release and environments as service (EaaS) provide solutions to manage this.</li><li id="""">Scalability: This is one of the challenges that is hardest to manage. Git stores the history of all commits and changes. Over a number of versions and repositories, this can become quite a lot of data to store. While there isn't an easy fix for this, it's something that can be kept in mind. Limit the number of branches, cut down on duplicate code if possible, and keep commit permissions and protocol organized. With adequate planning and care, it's possible to keep GitOps relatively scalable.</li></ul><h3 id="""">Is GitOps an Offshoot of DevOps?</h3><p id="""">In many ways, a CI/CD pipeline developed with GitOps does much of the same things that one developed with classic DevOps does, and a good classic DevOps setup has elements in place to cover much of what GitOps does. Therefore, GitOps can be regarded both as a tool in the DevOps toolkit or as its own practice, often depending on which elements of it you prioritize.&nbsp;</p><p id="""">However, in either case, GitOps is a continuation of the culture that DevOps encompasses. It fills in blanks in areas that aren't prioritized in classical DevOps with strategies that remain in line with the core philosophy of DevOps. By knowing the most important advantages and challenges of GitOps, you can capitalize on the most important aspects of DevOps while taking it one step further.&nbsp;</p><h3 id="""">Conclusion on GitOps vs DevOps</h3><p id="""">Good CI/CD and DevOps practices can have a huge impact on how efficiently a company operates. In this article, we went over what DevOps is, how GitOps fits into DevOps, and their differences. We also mentioned some best practices for incorporating GitOps effectively into your team's DevOps mentality. You can now take this information and make the best decisions possible for your development processes.&nbsp;</p>",false,,,,https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63e41e1c9a7f5718a6199f0e_092822%20(1).jpg,a person working on a laptop,vicky-koblinski,3,Wed Sep 28 2022 18:00:00 GMT+0000 (Coordinated Universal Time),,
Great SaaS Sales Demos - 3 Game Changers,great-saas-sales-demos,62aa5a70cd5ba27d9d0d718a,62aa5a70cd5ba2d41f0d72d2,Thu Mar 18 2021 20:48:40 GMT+0000 (Coordinated Universal Time),Fri Apr 21 2023 19:49:02 GMT+0000 (Coordinated Universal Time),Fri Apr 21 2023 20:04:43 GMT+0000 (Coordinated Universal Time),Great SaaS sales demos using these three game changers enabled by Release environments. ,"<h3 id="""">Great SaaS&nbsp;Sales Demos</h3><p id="""">Sales demos for Software as a Service (SaaS) products are vital to introducing your product to customers and gaining their interest or desire to use your product. The typical way that a SaaS product is demonstrated, especially in this day and age of remote work, is by presenting the application remotely, showing off features and highlighting use cases for the potential customer. A good demo can show the customer that your product is a good fit for them, but it can also do the opposite: a demonstration might reveal problems, shortcomings, or gaps that your SaaS product has that your customer might pick up on.<br></p><p id="""">As a DevOps engineer, I’ve received and viewed many demonstrations for a wide variety of products over many years. I’ve never had to present a sales demonstration myself (at least, not directly), but I have had to do setup and perform internal demonstrations, and I’ve also had to participate in designing or configuring SaaS product demonstrations. There are two SaaS demonstrations that I’ve participated in that were so memorable, they always come to mind even years later and after hundreds of demonstrations I’ve witnessed. The first demonstration was an unmitigated disaster and the second was one of the best that I ever witnessed thus far. Both of these examples lead to the 3 ways that you can make your SaaS sales demo the best it can possibly be.</p><h3 id="""">The Bad Example</h3><p id="""">One of the worst examples of a SaaS sales demo disaster happened quite a long time ago when a SaaS company was in the middle of releasing a new version of their software. They were also going public and the combination of these two events was a source of great fanfare in the industry and at the SaaS company. The company I worked for had tested the previous version of the product—let’s say the “1.0” product—and we were prospecting to get a very good deal on a one year implementation. The new “2.0” product was being released and our company was one of the customers that would&nbsp; be a “logo” used to promote how well the product worked and that was trusted by our brand.<br></p><p id="""">The problem was that internally, many of us engineers and even some of the leadership, agreed that the product wasn’t very good. We were excited to try out the new version but were a bit skeptical the product would be vastly different than the 1.0 product. In our view, the product was large, bloated, unstable, and crashed often. In preparation for the big 2.0 sales demonstration, I sat across from an executive at the SaaS company and listed a long laundry list of issues that I considered important to be fixed in the new product. The executive (who turned out to be the SaaS provider’s CTO!!!) agreed with me completely and reassured me that “all of the issues” had been resolved in the new version.<br></p><p id="""">The large conference room lights dimmed and in a standing-room-only hush, the product demonstration started and was displayed on the projection screen. It was obvious the product was slow: noticeably slower than even the original. It also crashed almost immediately. The sales person started the demonstration again with a smooth cover-up. I remember the mood was still forgiving because it had crashed so suddenly and so quickly, nobody had really invested too much time yet. The sales demo started again and the product was slightly better. However, I could tell from the tics and movements on the screen that the sales person was purposely avoiding certain features and buttons that I personally knew were problematic in the old version. I knew he was avoiding them because they were likely to be buggy or cause issues.<br></p><p id="""">Despite such careful choreography, the product crashed yet again, in fact, it crashed numerous times to the point where the demonstration simply couldn’t continue. The executives from the SaaS company who were there to personally oversee the demonstration were flabbergasted. I personally heard one of the sales engineers speaking in hushed tones into his cell phone telling his operations folks to try to reboot the demo server, again.<br></p><p id="""">These are a sample of the types of excuses we were offered:</p><ul id=""""><li id="""">“Oh, the new version hasn’t been deployed yet, this is a pre-release demo.”</li><li id="""">“You know, the dataset is wrong, we need to load more data.”</li><li id="""">“The internet connection is really laggy and we have a production network environment for the real product.”</li><li id="""">“The demo server isn’t ready for production yet, we’ve been upgrading our systems as fast as we can.”</li><li id="""">“We haven’t fully tested this new feature, and it wasn’t supposed to appear in this demonstration. Sorry.”<br></li></ul><p id="""">But eventually the truth was that the sales demo itself had failed. This had almost nothing to do with the product in actual practice, but that didn’t matter. The demonstration was unable to satisfactorily show what the product could do.</p><h3 id="""">The Good Example</h3><p id="""">On the opposite side of the spectrum, one of the best sales SaaS demos I witnessed was nearly perfect and stood out how great the process was. The SaaS product was a monitoring tool that provided metrics and log events for a running application, and would be used to drill down and into metrics, events, logs, and so forth. This demonstration occurred many years ago and does not reflect on any present companies or products you may think of. I just want to clarify this so that you do not form an impression of any existing company or product today.<br></p><p id="""">The SaaS salesperson ran the demonstration by logging into the actual live product with a special demonstration credential, and showed real, live data that were flowing from a fake application that was written specifically for the demonstration. It ran on a ten-minute cycle of generating pre-created events, metrics, logs, and so forth. The salesperson was able to look at “real,” “live,” and updated data as events happened in the application. Because the data were pregenerated and ran continuously, it seemed like the product worked perfectly and would do exactly what the salesperson seemed to make so easy.<br></p><p id="""">There was one flaw that I spotted and confirmed but it was relatively minor. This flaw does demonstrate one of the key points I will discuss later on for a successful SaaS demonstration. The salesperson was unable to change or update any of the data or the layout of the screen. They were unable to demonstrate the ability to create and edit reports for the product. They were using what was essentially a “read-only” demonstration of the product and could not change anything inside the account. The reason for this is to perfectly preserve the demonstration process so that the next time the product is reviewed, it would be exactly the way it was before without any possible changes.<br></p><p id="""">Based on these examples, I will now detail the 3 most important ways to pull off a perfect SaaS sales demonstration.</p><h3 id="""">Game Changer 1: Provide a “Real” Experience</h3><p id="""">Your SaaS demonstration must be as “real” as possible. I use the word “real” in scare quotes because often you cannot provide any actual “real” customer data since the prospects aren’t customers yet. Even if you do have some customer data or can use actual real data of some sort, the customers are not interacting in any real way with the product yet, so let’s just use the scare quotes. Never-the-less, the data that you use in the sales demonstration needs to be realistic enough to provide a valid reflection of what your product can do and what the customer needs, in a way that the product actually works.<br></p><p id="""">There is no other way to perform this than to use your actual live product in its actual live state. There are a lot of excuses and corners you will want to cut around this issue, but believe me when I tell you that this is critical. You must use your actual product in its actual state to perform the SaaS demonstration. When a customer test drives a car at the dealership, they are driving a real car that can be sold, not a fake or reproduction or toy model. You must do the same with your SaaS product.<br></p><p id="""">You will need a stable, performant, fully functional, and fully capable product that functions exactly the way it will function when the customer logs in and starts using the product. It must function perfectly the first time, the second time, and the <em id="""">nth</em> time you demonstrate your product. It must not conflict with someone else when two salespeople perform a demonstration at the same time, and it must not have any leftover or incorrect settings from a previous demonstration. It also must be stable and not be updated or rebooted or crash because of normal operations of the live site.<br></p><p id="""">This is where a <a href=""https://releasehub.com"" id="""">Release</a> sales demonstration environment comes into play. A salesperson or organization that uses Release would have an environment setup and ready to deploy at a moment’s notice with pre-populated data, user accounts, and features fixed to a particular time or branch set. A few minutes before the demonstration, the salesperson would simply start a new, completely isolated and perfectly preserved application stack and dataset, ready to fully demonstrate the full, live, exact duplicate SaaS product to the customer. There is no chance for shared environments to conflict; the demonstrations can be tailored to each customer industry or feature set (if applicable); and there is no possibility of having incorrect settings or data present that can disrupt the demonstration.<br></p><p id="""">Because the scale of the application can be set for one person or only one small subset of a customer, the application can be tuned to perform perfectly and for very little cost compared to the production environment. As soon as the demonstration is finished, the environment can be destroyed so that any hosting costs stop immediately.</p><h3 id="""">Game Changer 2: Provide a Live Experience</h3><p id="""">Notice I did not use scare quotes in this section for the word “live.” The demonstration experience can be faked to some extent in terms of pre-populated data or accounts, but it cannot be faked in terms of actual functionality. You cannot just show a static video of your product demonstration and expect customers to buy into it. The potential customers must be able to intervene, interject, ask “what if,” or (even better) take over the controls and “see what happens.”<br></p><p id="""">I already can hear some objections from salespeople who say that when an application is truly live then it is subject to unknown influences and problems. This is true, except that if you have a product experience that is well-tested and fixed in place at a particular time and version, then the demonstration can be well-rehearsed and practiced. This is directly at odds with the usual organization's push to release new features and update the product regularly. How is it possible to be able to have completely stable, well-populated, isolated environments at the same time that the development cycle needs to be fast, regularly spaced, and turn quickly?<br></p><p id="""">This is where <a href=""https://releasehub.com"" id="""">Release</a> sales demonstration environments come into play. An application	template can be set to a stable version or branch that is well-tested and cleaned for use to show to customers. It will be prepopulated with well-known, well-tested data. When a salesperson or sales organization is ready to show off new features or product enhancements, those features can be versioned and configured for use either as a separate application or as an existing branch of the Release application so that it can be spun up on demand for a customer. Once the branch and application template are tested, that version can be used by anyone in the sales organization to demonstrate the new features or product experiences and data.<br></p><p id="""">In this scenario, it doesn’t matter if the production application is running on version 0.9 or 1.1 or even 2.0, the sales demonstration environment can be configured to run exactly the same version 1.0 every single time it is deployed and demonstrated to potential customers. Also, it could be devastating if the product version were to change in the middle of the demonstration or during the customer demonstration time period. Conversely, if the production environment is running on a delayed version and a customer wants to see the new features, the new sales environments can be configured and tested for potential preview customers to give early feedback, advice, or to gain other valuable information.<br></p><p id="""">In fact, all of these scenarios could be in play at any one time: some sales environments could be set behind on an extremely stable, well-tested product experience while some sales environments can be spun up on demand at exactly the same version of the production experience, while yet other environments can be ahead of production to showcase new features and product experiences. The sales person or organization could pick and choose which scenario they would like to demonstrate and create the new environment tailored to their exact requirements and customer profile.<br></p><p id="""">Each environment would be isolated and stable without any contention or changes interfering.</p><h3 id="""">Game Changer 3: Provide a Malleable, Persistent Experience—Which Can be Recreated</h3><p id="""">Even in the best SaaS sales demonstrations I have experienced, there was always a critical and nagging worry I had that the environment was not going to persist and that anything that I was working on or being demonstrated would not last for long after the demo. Strangely, the reverse is true as well: oftentimes the customer might want to revert all their changes back to a pristine state after “messing around” with things. How can you resolve this tension between two extremes?<br></p><p id="""">This is where <a href=""https://releasehub.com"" id="""">Release</a> sales demonstration environments come into play. The salesperson or organization can spin up entire environments for one or more customers and allow the customer(s) to “play” with the product without any fear of damaging or impacting anything else. If the customer wants to start over, a new environment could be created, or the exact same environment could be duplicated to start over. In fact, the salesperson could construct several accounts for the customer to use if that is appropriate, or if multiple employees or departments could be interacting with each other or the demonstration. All of this can happen in such a way that the isolated environment can be snapshot or ported to production once everything is set up the way it should be. Alternatively, the snapshot could be ported to a new sales demonstration environment for a “reset.”<br></p><p id="""">Even better, in some cases, the new sales demonstration environment could become a staging or testing environment for the customer to use indefinitely or on-demand when they need to test possible scenarios or possibilities. All of this can occur without any impact or conflicts with the live production product. In a few rare cases, the SaaS product could become so customized to an industry vertical or customer that it may become a standalone environment live in production for use by that customer or industry.<br></p><p id="""">This brings up a great point about environments at Release: they are all identical in design and spirit, and can be used for any and for all purposes. A software engineer’s local development idea can become a testing ground for product development, which can easily be deployed for a sales demonstration, which could easily become a live production environment running live paying customers. With Release, your environments are not limited by even the sky above or the ground below!<br></p><p id="""">Photo by <a href=""https://unsplash.com/@dimage_carlos?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText"" id="""">Carlos Esteves</a> on <a href=""/?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText"" id="""">Unsplash</a></p>",false,,,,https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63e3fe8d90f3c97fe8c26b1d_031821%20(1).jpg,A man playing chess representing game changers in SaaS Sales Demos,regis-wilson,7,Fri Mar 19 2021 00:00:00 GMT+0000 (Coordinated Universal Time),,
Gromit: an Open Source AI Assistant for your Documentation,gromit-an-open-source-ai-assistant-for-your-documentation,62aa5a70cd5ba27d9d0d718a,645c0ed46adde36c2988a35a,Wed May 10 2023 21:38:28 GMT+0000 (Coordinated Universal Time),Mon May 22 2023 22:17:09 GMT+0000 (Coordinated Universal Time),Tue May 23 2023 14:56:09 GMT+0000 (Coordinated Universal Time),,"<h2 id="""">Gromit: an Open Source AI Assistant for your Documentation</h2><p id="""">Here at Release we have been working on tooling to integrate with OpenAI and ChatGPT using many open source tools. We have learned a lot from integrating these tools into our product and wanted to give back to the Open Source community with our AI Assistant Gromit project.&nbsp;</p><p id="""">Imagine that you built an awesome app, wrote up the documentation and laid out all the relevant tidbits of information a new or tenured user would need to know. You keep on adding to your knowledge base to explain features, functionality and process in more detail, and somehow it raises more questions. Now users are pouring over even more information you generated to find answers to their questions. You could structure your documentation differently, add more tags, do more backlinks, etc., or you could have AI help you surface the right pieces, for the right user, depending on their question. This is how the idea for Gromit came about.&nbsp;</p><p id="""">Using <a href=""https://release.com/blog/training-chatgpt-with-custom-libraries-using-extensions"" id="""">embeddings and vector search databases</a> we can search all of your documentation based on a question given to the AI Assistant. The relevant documentation from the search results are used to generate a prompt for OpenAI. We have an internal project called Gromit that provides a simple way to index your documentation using OpenAI’s embeddings and stores these indices in a Redis vector search database.&nbsp;</p><p id="""">We have broken out the underlying Ruby code into the <a href=""https://rubygems.org/gems/gromit"" id="""">Gromit Ruby Gem</a> which allows developers to create their own API on top of the basic Gromit functionality. Gromit also provides a Rails engine which can be mounted in any Rails application which will give you the basic APIs out of the box within minutes. We are pleased to give back to the community and support open source projects by contributing this Gem to the open source community.&nbsp;</p><p id="""">Here is a quick example of how you can start using it:</p><p id="""">To mount the Gromit Rails engine in your Rails project. First edit your &lt;code inline&gt;Gemfile&lt;/code&gt; and add</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-ruby"">
gem “gromit”
</code>
</pre></div><p id="""">After executing the &lt;code inline&gt;bundle&lt;/code&gt; command, edit your &lt;code inline&gt;config/routes.rb&lt;/code&gt; and add the following:</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-ruby"">
Rails.application.routes.draw do
  mount Gromit::Engine => ""/""
end
</code>
</pre></div><p id="""">You will need to add an environment variable with your OpenAI API access token. Edit your &lt;code inline&gt;.env&lt;/code&gt; or &lt;code inline&gt;.env.local&lt;/code&gt; and add the following:</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-ruby"">
OPENAPI_ACCESS_TOKEN=your-openai-token
</code>
</pre></div><p id="""">To install the redis-stack-server do the following. Note, that if you are already running Redis you will need to stop/disable the old version of redis. The redis-stack-server provides additional search indexing functionality not provided in the default Redis server.</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-bash"">
brew tap redis-stack/redis-stack
brew install redis-stack
</code>
</pre></div><p id="""">At this point you can index all of your documentation that’s in markdown format like so:</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-bash"">
bundle exec gromit-reindexer -s /path/to/your/docs
</code>
</pre></div><p id="""">We have also created a <a href=""https://github.com/releasehub-com/gromit-example"" id="""">Gromit Example Rails Application</a> which also includes a <a href=""https://github.com/supabase-community/nextjs-openai-doc-search"" id="""">Nextjs OpenAI Documentation Search</a> user interface which is hooked into the Gromit Rails Application.&nbsp;</p><p id="""">Here is an example of Gromit in action using the Next.js frontend <a href=""https://supabase.com/"" id="""">created by the folks at supabase</a>:&nbsp;</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/645c0e4f5d71bc7a181e3163_71b470ba.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><p id="""">At Release we are strong believers in the power of open source. In our drive to create a development platform that better serves modern development teams, our founding team has been using and contributing to the open source community for years. Founded in 2019 Release was and still is built entirely with open source software!</p><p id="""">Test our Gromit for yourself, tell us what you think and share it with others. We look forward to sharing more fun projects with the community in the future.&nbsp;</p>",false,,,,https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/645cefdb473a8b75dd24f778_Gromit%20OSS%20project%20v3.jpg,A dog wearing a glass and the prashe: Gromit OSS project,david-giffin,4,Wed May 10 2023 21:35:00 GMT+0000 (Coordinated Universal Time),,training-chatgpt-with-custom-libraries-using-extensions
Learn how to host your own Mastodon instance on AWS,hosting-mastodon-on-release-and-aws,62aa5a70cd5ba27d9d0d718a,639c61d6c216ec98c8281e73,Fri Dec 16 2022 12:17:26 GMT+0000 (Coordinated Universal Time),Thu Apr 20 2023 21:47:57 GMT+0000 (Coordinated Universal Time),Thu Apr 20 2023 21:48:03 GMT+0000 (Coordinated Universal Time),"A guide on how to self-host Mastodon on Release. You'll set up a web server, storage and everything else you need.","<h3 id="""">Mastodon development environments on Release and AWS</h3><p id="""">Twitter users are flying the coop in droves and many are turning to Mastodon, an open-source federated social network.</p><p id="""">Unlike Twitter’s centralized architecture, a federated social network consists of thousands of individual social networks, called <em id="""">instances</em>, each with its own users.</p><p id="""">Federation is sometimes described as being similar to email in the sense that email users can send each other messages, even if they have different email hosts and use different domains.</p><p id="""">Mastodon instances fetch content from other instances, which allows a user on one server to follow a user on a different server, thereby creating one big network.</p><p id="""">To the DevOps-minded among us, the most compelling thing about the fastest-growing Twitter alternative is figuring out how to host and scale this exciting new platform.</p><p id="""">At Release, we enable teams to create development, staging, and production environments for their apps. If you intend to add new features or otherwise modify your Mastodon instances, Release can provide the environment you need to move your project forward. Whether you want to just take Mastodon for a spin to see what it’s all about, or run a production Mastodon instance that can scale up as you need, <a href=""https://release.com/book-a-demo"" id="""">using Release</a> means you don’t have to trade off simplicity for power. You can easily collaborate with your colleagues, preview the latest features, and upgrade to the latest builds. </p><p id="""">In this post, we’ll put Mastodon’s welcoming community and complex federation protocols aside so that we can focus on the nuts and bolts of hosting a Mastodon instance.</p><h3 id="""">What makes a Mastodon instance</h3><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:2056px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""2056px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/639c69bdd3661875e15616e6_mastodon_architecture.png"" loading=""lazy"" id="""" width=""auto"" height=""auto""></div><figcaption id="""">Mastodon Services Diagram</figcaption></figure><p id="""">A Mastodon instance consists of seven services:</p><ol id=""""><li id="""">A <strong id="""">web server</strong>, which runs on Ruby on Rails.</li><li id="""">A <strong id="""">streaming server</strong> that enables the instance’s local users to send and receive real-time updates to the server.</li><li id=""""><strong id="""">Sidekiq</strong>, a job scheduler that runs background tasks, polls other instances’ web servers, and sends data to other instances’ web servers.</li><li id="""">A <strong id="""">Redis server</strong> that stores jobs for Sidekiq and caches data such as feeds for local users.</li><li id="""">A <strong id="""">PostgreSQL database</strong> that stores posts, user profiles, and instance settings.</li><li id=""""><strong id="""">File storage</strong> to store and serve media such as images and videos.</li></ol><p id="""">An <strong id="""">SMTP email server</strong> to send messages to local users’ email accounts.</p><h3 id="""">How we’ll run a Mastodon instance on Release and AWS</h3><p id="""">Release creates Kubernetes clusters in your AWS account.</p><p id="""">Although all seven of Mastodon’s services could be hosted on Kubernetes, we’ll use a few AWS-provided services to host supporting systems.</p><p id="""">We’ll create containers to run Mastodon’s web server, streaming server, and Sidekiq in Kubernetes.</p><p id="""">From AWS, we’ll use:</p><ul id=""""><li id="""">Amazon ElastiCache for Redis</li><li id="""">Amazon RDS to host a PostgreSQL server</li><li id="""">Amazon S3 as a file store</li><li id="""">Amazon SES to send emails to local users</li></ul><h3 id=""""><strong id="""">Prepare your Release account</strong></h3><p id="""">Before we get started, you’ll need to <a href=""https://docs.releasehub.com/getting-started/create-an-account"" id="""">create a Release account</a> and upgrade to a paid account.</p><p id="""">Next, <a href=""https://docs.releasehub.com/guides-and-examples/domains-and-dns/external-dns"" target=""_blank"" id="""">verify your domain</a> in Release, and <a href=""https://docs.releasehub.com/guides-and-examples/advanced-guides/create-a-cluster"" target=""_blank"" id="""">create a cluster</a> using the AWS integration.</p><h4 id="""">Configuration details to save</h4><p id="""">In this guide, configuration details to note down will be pointed out with the heading: <strong id="""">Configuration details to save</strong>.</p><p id="""">While in Release, navigate to your cluster, and note down the following:</p><ol id=""""><li id="""">The <strong id="""">cluster context</strong>.</li><li id="""">Your <strong id="""">cluster region</strong>.</li></ol><p id="""">Your <strong id="""">node group</strong>.</p><h3 id="""">Prepare your local machine</h3><p id="""">On your local system, you’ll need to install Git and Docker.</p><p id="""">Keep a text editor handy to take notes. We’ll gather configuration details as we go along, and saving these in a central file will make setting up your environment much easier later.</p><h3 id="""">Fork the Mastodon repository on GitHub</h3><p id="""">Fork the Mastodon repository from GitHub to your GitHub, Bitbucket, or GitLab account. We’ll use GitHub in this guide.</p><ol id=""""><li id="""">Log into your GitHub account.</li><li id="""">Fork the <a href=""https://github.com/mastodon/mastodon/fork"" target=""_blank"" id="""">Mastodon</a> repository.</li></ol><p id="""">GitHub creates a public fork by default. Since Mastodon is AGPL-licensed, keeping your fork public is a good way to make sure you adhere to licensing requirements from the start.</p><h4 id="""">Configuration details to save</h4><p id="""">Make a note of your <strong id="""">repository name</strong> from GitHub.</p><h3 id="""">Clone your Mastodon fork to your local machine</h3><ol id=""""><li id="""">On the main page of your repository on GitHub.com, click <strong id="""">&lt;&gt;Code</strong>.</li><li id="""">Copy the URL for the repository.</li><li id="""">Open your terminal and navigate to where you want to clone your repository.</li><li id="""">Clone your repository using the URL you copied</li></ol><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-docker"">
git clone git@github.com:YOUR-USERNAME/mastodon.git
</code>
</pre></div><ol start=""5"" id=""""><li id="""">Change your current working folder to the new repository and keep your terminal open to use again later.</li></ol><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-docker"">
cd mastodon
</code>
</pre></div><h3 id=""""><strong id="""">Generate Mastodon secret keys and VAPID keys</strong></h3><p id="""">Mastodon needs three secret keys and one public key to run. We’ll generate these using the tootsuite/mastodon Docker image on our local machine.</p><ol id=""""><li id="""">Set a local environment variable SECRET_KEY_BASE. In your terminal, run:</li></ol><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-docker"">
SECRET_KEY_BASE=$(docker run --rm -it tootsuite/mastodon:v4.0.2 bin/rake secret)
</code>
</pre></div><ol start=""2"" id=""""><li id="""">Set a local environment variable OTP_SECRET, by running the following in your terminal:</li></ol><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-docker"">
OTP_SECRET=$(docker run --rm -it tootsuite/mastodon:v4.0.2 bin/rake secret)
</code>
</pre></div><ol start=""3"" id=""""><li id="""">Generate VAPID keys, using the SECRET_KEY_BASE and OTP_SECRET as inputs, and print our new keys to the terminal.</li></ol><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-docker"">
docker run --rm \
   -e OTP_SECRET=$OTP_SECRET \
   -e SECRET_KEY_BASE=$SECRET_KEY_BASE \
   -it tootsuite/mastodon:v4.0.2 \
   bin/rake mastodon:webpush:generate_vapid_key && \
   echo ""SECRET_KEY_BASE=$SECRET_KEY_BASE"" && \
   echo ""OTP_SECRET=$OTP_SECRET""</code>
</pre></div><h3 id=""""><strong id="""">Configuration details to save</strong></h3><p id="""">Copy and save VAPID_PRIVATE_KEY, VAPID_PUBLIC_KEY, SECRET_KEY_BASE, and OTP_SECRET values from the output, which should look something like this:</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-docker"">
VAPID_PRIVATE_KEY=cDpok1oPz1u6jpP2fE_Vf2TWBy-VVHh0n3KqdCEz81A=
VAPID_PUBLIC_KEY=BAb1gkLWzalGfAZq_7IeGX19T1Rp4I5aIerN_sDfon5eenIEn9DAWU1LLpFSu6VjtnhJJilbZXLBBdUSa6DL74Y=
SECRET_KEY_BASE=bb0231c8e07870b70934a9487cd6e796bbd6f4fe086dfa9039be3743a96e18726ea168cde3cf3ea823c5214e09b8afb7696324eb024f4317cb2626e0545aac12
OTP_SECRET=19a4d2e15d4ffa63c5ad08d716f33b2296419a6529a49908bd4f879891710d1f9efc1007efc9c18e82aeb52cba763cbd89b030ed2069be8464d9f26d167ea102</code>
</pre></div><h3 id=""""><strong id="""">AWS Step 1: Create a PostgreSQL database using Amazon RDS</strong></h3><p id="""">Log in to your AWS account and navigate to RDS.</p><p id="""">Make sure your AWS region is set to the same region as your Release cluster.</p><p id="""">Create a new PostgreSQL 14 database:</p><ol id=""""><li id="""">Click on <strong id="""">create database</strong>.</li></ol><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div id=""""><img alt=""RDS create database"" src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/639c695b8dc1c6e8e820fb62_jWqmDuvKIWstF5ELPGO8iBs7rL8qQuCCN72srXCjjr7KAIoTCupw3tzXTsxm_pdwDZrywXTVkNvHQnDHiTJQS4VZpRinrmdk8JFR6PNqUf7mT0LRH0xp4iZX5WOB3gmJhQDdCROYSytympzeFpQG_ME1FeUhMfZENNjIMpTxAkVW-GPtNyI3G7ClPe4owZqPWhoXJ_-zQg.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div><figcaption id=""""><em id="""">RDS create database</em></figcaption></figure><ol start=""2"" id=""""><li id="""">Select <strong id="""">standard create</strong>.</li><li id="""">Select PostgreSQL as the engine type.</li><li id="""">Pick PostgreSQL 14.5-R1 as the engine version.</li></ol><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div id=""""><img alt=""RDS create database steps 2 to 4"" src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/639c76798adaf0885f06b4ac_IDS0sNwM4kAiNNjRsKtOC5ThuxX6C6DlH6m6gNUaLA81gLWT37DbuCDG9BdlIdbopV5tj3KBJo446ftIi3OBg0w1uZuEL9U_JrqPyI3pedVfElcNYG_Y0kLNPA6n8AeugYaAtyWTYEHowE72iP5F9wxgAe3deez_KqTXOrJsTiz1dpd65rm6SJasdawJfbjrjjwyFACCkw.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div><figcaption id=""""><em id="""">RDS create database steps 2 to 4</em></figcaption></figure><ol start=""5"" id=""""><li id="""">Select <strong id="""">Free tier</strong> if you’re only setting up a small/test instance.</li><li id="""">Set the <em id="""">DB instance identifier</em> to mastodon-1. We’ll need to refer to this again later.</li></ol><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div id=""""><img alt=""RDS create database steps 5 to 6"" src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/639c7768c9a6a2768620e4ba_dbpnXWfqOuZlXqoALdYNleWiqISuWGSXwWnEQEsgDL21lAroDEcmJ4j4kY7cEJ_kz6YdjLNclHzPthWN1QurT7QpSZzO7rQTcqbx6Xj686u2Npj5hohZrxXoYYhNn0zufU0e6yVN1HAfoNHAVXLWoFtRV0GJylqLjIBjXKCaLZlByTcqMmoSDv_1O2KOB3NaZyGOKq6aRg.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div><figcaption id=""""><em id="""">RDS create database steps 5 to 6</em></figcaption></figure><ol start=""7"" id=""""><li id="""">To make sure you don’t use more space than needed, change the <em id="""">allocated storage</em> to 20 GiB.</li><li id="""">Disable autoscaling.</li><li id="""">Under the <strong id="""">connectivity</strong> section, select the virtual private cloud (VPC) that Release created for your cluster. This will enable services running in the cluster to reach your new database.</li></ol><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div id=""""><img alt=""RDS create database steps 7 to 8"" src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/639c7794bde454280e3693fa_RhOpdgnWHJrqITO8n9yCAfjzIXfUgLRZCNwuvW2XkjmIY17Q0VtB344ykLOb9eJ8f4sqISt96bF0vmb0w8bAJAW504YiJtgT_hxgZEABCRM0XIGXEMD1upJ-XtETIcpKsxbyCo9zpMo-JPXfi95KSGUn89nFPg7P9654yR4OSS0WUP2F9y5Ke2QVVIhKeWGIiAsZj7GjPA.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div><figcaption id=""""><em id="""">RDS create database steps 7 to 8</em></figcaption></figure><ol start=""10"" id=""""><li id="""">Choose to allow an existing security group.</li><li id="""">Under <strong id="""">Existing VPC security groups</strong>, pick the security group that starts with eks-cluster-sg- followed by your cluster context, then close the dropdown.</li></ol><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div id=""""><img alt=""RDS create database steps 8 to 10"" src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/639c77eab398f44cc55bc788_bD0V12ERfmRRSoTsGK7MsZ7e3dQtdpztKDfNnL44vC6LYKFzlp_kg2khc_Z_8semrAtx5Dz91HUs2hOtqZc3XLH2G8YT3eKa3ZICTSFrMUYB83MrgURA7-PZvMA6uHY9TJ6YerYBorpE-Ji1vq_c5X4vj6UImQ480z-tlhL1QADtgkexlqG-yK5pGCvtyklz9GAX52GdeQ.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div><figcaption id=""""><em id="""">RDS create database steps 9 to 11</em></figcaption></figure><ol start=""12"" id=""""><li id="""">Click <strong id="""">Create database</strong>.</li></ol><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div id=""""><img alt=""RDS create database steps 8 to 10"" src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/639c7805ae8e9f1bd348593f_8snOu6crS2l_U8ETpz8QZT6JZhPJI3WFuN8hZB2CH1AU2Bqx-xouajQVHfMQzpe_F11VFQZJWv854WIN2bS8WPrqAD43UFY3gSCAgIc7a7IlTAFI9LbjtYMHBRY_sQuSvjWnX1gNyCn7Dj1fDTmQxPOW5WJ6fegeUZYTF9h9cU9s_v-gkGyye9ymbYc463iL6mKqe8cTWQ.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div><figcaption id=""""><em id="""">RDS create database step 12</em></figcaption></figure><p id="""">Your database will take a few minutes to launch.</p><h4 id="""">Configuration details to save</h4><p id="""">Once the database is ready, click on <strong id="""">view connection details</strong> and note down the <strong id="""">database endpoint</strong> and <strong id="""">database password</strong>.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div id=""""><img alt=""Screenshot of RDS PostgreSQL configuration details"" src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/639c7831c31ac66a799565a4_TqF0uFvzWUTORZ2epJ19QOMBaMDlVX0FQ1ffKlsFChzOueJ1nKnJWH_g5Ttehdu29_ULJRu8nWcAEB9BNC9vLq3S5sTyJ6IMmP2zb65N8vIdjR5K4_20j9HKMajfggxk2SKvzxH6rZHU154MqdQH8z7W8SFOxJu5Dg1-fglEYoTGUGTve2cT_II98eGob7RHzWVO7uoBuw.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div><figcaption id=""""><em id="""">RDS PostgreSQL configuration details</em></figcaption></figure><h3 id="""">AWS Step 2: Create a Redis cache using Amazon ElastiCache</h3><p id="""">Before we create a Redis cache, we’ll need to get the VPC ID for our Release cluster (Amazon ElastiCache does not show the full VPC name when creating a Redis cluster).</p><p id="""">In AWS, navigate to <strong id="""">VPC</strong>, then note down the VPC ID for your Release cluster.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div id=""""><img alt=""Screenshot of Amazon VPC"" src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/639c7832b7572c03c770db3d_qUV6TbZblzZy0OgsMocVR64h0hy6xmZSi7m7TgNI8NIK9s9Yr3GXgJ-okG9egsfWQDlyzW_R-xc9IlRAez4XXp8m4JAVY5NBFVz40OET4Iz8J7PrC8Wg_DUoN-L0EtIOc5sOXkDCkkNIoxbd31wodN4psnsgJ-zK0VyFFXS-KXu0Ug9kGmPMcrIDzI0c8KTVrjRYNRFOiw.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div><figcaption id=""""><em id="""">Amazon VPC</em></figcaption></figure><p id="""">With the VPC ID at hand, we can create a Redis cluster. In your AWS account, navigate to ElastiCache.</p><ol id=""""><li id="""">Click on <strong id="""">create cluster</strong>.</li><li id="""">Select <strong id="""">create Redis cluster</strong>.</li></ol><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div id=""""><img alt=""Screenshot of ElastiCache"" src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/639c7835708249ee715ad4c8_kDft8J-AuScnkdcs7ST_lNkRLbeChp6inoBitDssCBAVRu5GfeJE040oxwysFyeBvROmCx3mEjO-dnEZGZ-i6xzZviQz4EX37XPLgW4NC-9BpY1NR6vnX-NqWK-Oa8YSGi_FgjK_64wkZy8odx5OVvA5NbOo7JC5G4qlALWvOCV0bODlzVctTyPUa0-YCNNAS2fqCevQ9A.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div><figcaption id=""""><em id="""">ElastiCache</em></figcaption></figure><p id="""">This opens up the <strong id="""">cluster settings</strong> page, where we’ll configure our Redis cluster.</p><p id="""">Under <strong id="""">cluster info</strong>, set a name for this Redis cluster, for example, mastodon-redis.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div id=""""><img alt=""Screenshot of Redis configuration section"" src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/639c7835bf582d7f360db6ce_diohMVdW8tN4KvwvXuMdHKmha5YTLxDEpTfNZe0DqnUsPRO8gtgGM_ATBUrO2Bfc1E9QAaTxzulhDdW86aBPw7XrVchbuSbhi4fgOqhoZWbzhc4oQdk2lqDrgDcGB5ATqGx3Rytt-fkX5uYEbjlzSGNuiKIz1yv0N2wb_ri9j2a0dmLhqaqwyh7xBkqak9YNu7tgahy-Eg.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div><figcaption id=""""><em id="""">Redis configuration section</em></figcaption></figure><p id="""">In the <strong id="""">connectivity</strong> section:</p><ol id=""""><li id="""">Set a name for the Redis cluster’s subnet, for example, mastodon-redis-subnet.</li><li id="""">Select your Release cluster’s <strong id="""">VPC ID</strong> from the dropdown.</li></ol><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div id=""""><img alt=""Screenshot of Redis connectivity section"" src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/639c7835c75e2e83bffde4ca_qdwaWQtOKC_yP3UWSjwmNordmNfJM_X5REg3S5Xdl0JsmVgbmynFFqrwognxzcHcft5oTgeaGA6O4_JOBOldTpjoXTUveycYD3C-yRt9EIlEKDsiRkwp-Kxdgux81eIMuALUulEAe_KA95UOO-OasPpN7E0qJUFC-xd9GHjxYFIOZzQ9Zf9GwdpwxUc8mRrZUwflu34f6A.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div><figcaption id=""""><em id="""">Redis connectivity section</em></figcaption></figure><p id="""">Click <strong id="""">Next</strong>.</p><p id="""">Under <strong id="""">selected security groups</strong>, click <strong id="""">Manage</strong>.</p><ol id=""""><li id="""">Filter the security groups to find one that starts with eks-cluster-sg-.</li><li id="""">Select the security group that starts with eks-cluster-sg- followed by your Release cluster’s context.</li><li id="""">Click <strong id="""">choose</strong>.</li></ol><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div id=""""><img alt=""Screenshot of Redis advanced settings page"" src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/639c7835ae8e9f6a56485e49_4fb1yawolUE4DYiJup6YiQRWaEHUKvG0eX1RRAYpOGmvHXs4Rj4KGycXZ4SZBvjPT0QCRMKiSJXullmtGTsL9nnTvPCDE3SzVvgKJ8v6OcHscIkjECEkn5fEU7d6aGK_o4Cze4BGZvHQ0meHacNPreg-F1JHjG_zu-p9TFzpb_WgiCJsnruD0dfYzeBAOsNuBfm2zy4o7w.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><p id=""""><em id="""">Screenshot of Redis advanced settings page</em></p><p id="""">Scroll down to the bottom of the page and click <strong id="""">next</strong>.</p><p id="""">Scroll down the page to review your Redis settings, then click on <strong id="""">create</strong>.</p><p id="""">AWS will take a moment to create your Redis cluster.</p><h4 id="""">Configuration details to save</h4><p id="""">Open the Redis cluster’s details and copy the Redis cluster’s endpoint (you can omit the port number).</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div id=""""><img alt=""Screenshot of Redis Cluster details"" src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/639c7833872f31c6a3d2e075_whvaLYqqdHXoI3Pd-K9itZUdoPwBh2OgAITAZ-rT69EV_z1UbcTX6dzG4v_VUuNlN_3moG2GT-rPPdn_rxAUozZUD38S7gTUfhvZCHehIP1kqKa9Wt63d5RNAK_9njx30_BwCgEHVk1fWjsl5_NdUnpdo_O_vL8A0OQhQPOAcrJ6W3sgoGIM9OE3Xq1yXZje4M5PXYdFSg.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div><figcaption id=""""><em id="""">Redis Cluster details</em></figcaption></figure><h3 id="""">AWS Step 3: Create an Amazon S3 bucket for user media</h3><p id="""">In your AWS account, navigate to S3.</p><p id="""">Click on <strong id="""">create bucket</strong>.</p><p id="""">Enter a unique bucket name, for example, mastodon-media-example.</p><p id="""">Uncheck <strong id="""">Block all public access</strong>.</p><p id="""">Check “I acknowledge that the current settings might result in this bucket and the objects within becoming public”.</p><p id="""">Click <strong id="""">create bucket</strong>.</p><p id="""">AWS will take a moment to create your new S3 bucket.</p><h3 id="""">AWS Step 4: Create an IAM user to write to S3</h3><p id="""">In your AWS account, navigate to <strong id="""">Identity and Access Management (IAM)</strong>.</p><p id="""">Click on <strong id="""">Users</strong>, then on <strong id="""">Add users</strong>.</p><ol id=""""><li id="""">Enter a username, for example, mastodon-s3-writer.</li><li id="""">Under <strong id="""">Select AWS credential type</strong>, select only <strong id="""">Access key - Programmatic access</strong>.</li><li id="""">Click <strong id="""">Next: Permissions</strong>.</li></ol><p id="""">On the <strong id="""">Set permissions</strong> page:</p><ol id=""""><li id="""">Select <strong id="""">Attach existing policies directly</strong>.</li><li id="""">Filter for s3 policies.</li><li id="""">Check <strong id="""">AmazonS3FullAccess</strong>.</li><li id="""">Click <strong id="""">Next: Tags</strong>.</li></ol><p id="""">Click <strong id="""">Next: Review</strong>.</p><p id="""">Finally, click <strong id="""">Create user</strong>.</p><h4 id=""""><strong id="""">Configuration details to save</strong></h4><p id="""">Note down the access key ID and secret access key. Keep in mind that you won’t be able to see this secret again, so you’ll need to save it now and keep it secure.</p><h3 id="""">AWS Step 5: Create an Amazon SES identity</h3><p id="""">In AWS, navigate to <strong id="""">Amazon Simple Email Service (SES)</strong>.</p><p id="""">Click <strong id="""">Create identity</strong>.</p><ol id=""""><li id="""">Select <strong id="""">Email address</strong> as the identity type.</li><li id="""">Enter an email address where you can receive emails.</li><li id="""">Click <strong id="""">Create identity</strong>.</li></ol><p id="""">You’ll need to log in to the inbox for the email address you entered and verify the SES sending identity by clicking a link. Look for an email with the subject “Amazon Web Services – Email Address Verification Request”.</p><p id="""">Back in AWS, you should now see that the identity status has changed to <strong id="""">verified</strong>. If not, reload the page.</p><p id="""">Send a test email to see whether the sending identity works.</p><p id="""">Click on <strong id="""">SMTP settings</strong> in the left sidebar, then on <strong id="""">Create SMTP credentials</strong>.</p><p id="""">Click <strong id="""">Create</strong>.</p><h4 id="""">Configuration details to save</h4><p id="""">Toggle <strong id="""">Show User SMTP Security Credentials</strong>, then copy and save the SMTP username and password. You won’t have access to the password again after this step, so keep it secure.</p><p id="""">After saving the credentials, click <strong id="""">Close</strong>.</p><h3 id="""">Create an application in Release</h3><p id="""">Log in to your Release account, then click on <strong id="""">create new app</strong>.</p><ol id=""""><li id="""">Enter a name for your Mastodon app, for example, mastodon.</li><li id="""">Pick your forked repository.</li><li id="""">Click <strong id="""">Next step</strong>.</li></ol><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div id=""""><img alt=""Pick your repository in Release"" src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/639c7833ae8e9f0ed6485e30_R9vJBatFkU4S0utF4eUunuI87JvVR2r4Q2rFRqpYDfGNQKy4tQUcqnTxwTqLkmeFwipLPbfnullKPvV3dMXUhSxiDwixAbVZUJTnujFGIKe1daEwfsWYVxXSypOu_TSVTrKMYgJXJPt9dJxYIDpDx4Dt8xFlvKg41cEtZVTiB0pJFjs5GpRFrWGZD9voCCPEW__oZVopKg.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div><figcaption id=""""><em id="""">Pick your repository in Release</em></figcaption></figure><h3 id="""">Pick your services in Release</h3><ol id=""""><li id="""">Select <strong id="""">Analyze the repository</strong></li><li id="""">Select the branch you would like to run.</li><li id="""">Select the root docker-compose.yml file.</li><li id="""">Click <strong id="""">Start Analysis</strong>.</li></ol><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div id=""""><img alt=""Analyze your repository in Release"" src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/639c78344f93befe314ef6b2_WOHI6bs1V4HpNQnnkA3fjSe5hywkWk1WHgoZ0d5C92KM8223V9SWsFY7V2F9RirXHt2cBoPie2V1eTCS--PxRRojpLr0LpkMWCujWgx2m0fsDKCSZnT9KI-qBgicTvbMP9XTLX3M_fnTVa1vEEP_KVF0vjGBFP3DxOG-Olg-3pZ2U4I9HXdQSpPKFx2tHPMZUcENkkKOzQ.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div><figcaption id=""""><em id="""">Analyze your repository in Release</em></figcaption></figure><p id="""">Release will now find services in your docker-compose.yml file.</p><p id="""">Pick only the following services:</p><ul id=""""><li id="""">sidekiq</li><li id="""">streaming</li><li id="""">web</li></ul><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div id=""""><img alt=""Pick services in Release"" src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/639c783547e46bc8922ef4f0_EbKauheLIszKarknc4w9ifOHcuvDpt-tJNRljX97U8HHHaPhw6S8octsO3mkLDoqcPnIBmTUGmeVS9RQLYTceXVOJVQbf2uSdeTtoOYEVQ0yl7ZUAtgCfSOVfWPiXK1jyjSDspVZ1dNkFtpG0qt_z9xPRSk0t-5zLiSv6B4ZWHbpwkQ_UOuMmkM2RJCGcmgfmmF2_TI6KQ.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div><figcaption id=""""><em id="""">Pick services in Release</em></figcaption></figure><p id="""">Click <strong id="""">Next Step</strong>.</p><h3 id="""">Edit the generated template</h3><p id="""">Release generates a template from the services we selected in the previous step.</p><p id="""">This template is a YAML file that Release uses to generate new environments. It specifies the services, ingress rules, and workflows required to set up your app.</p><h4 id="""">Double-check the template context and domain</h4><p id="""">Make sure the context in your application template matches your Release cluster’s context and that the domain matches the verified domain you’d like to use for this application.</p><h4 id="""">Replace hostnames with rules</h4><p id="""">Release automatically generates hostname templates for services that have node ports and adds them to a hostnames section in your Application Template.</p><p id="""">On a Mastodon server, the public services web and streaming share a hostname but are served at different paths.</p><p id="""">Release configures a Kubernetes ingress controller to route traffic to your applications, based on the hostnames or rules settings, but only one of these settings can be used per environment. We’ll use only rules.</p><p id="""">Update the template to <strong id="""">replace</strong> hostnames with the following rules:</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-docker"">
rules:
- service: web
  hostnames:
  - mastodon-${env_id}.${domain}
  path: ""/""
- service: streaming
  hostnames:
  - mastodon-${env_id}.${domain}
  path: ""/api/v1/streaming""
</code>
</pre></div><p id="""">The changes should look like this:</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-docker"">
- hostnames:
-   streaming: streaming-mastodon-${env_id}.${domain}
-   web: web-mastodon-${env_id}.${domain}
+ rules:
+ - service: web
+   hostnames:
+   - mastodon-${env_id}.${domain}
+   path: ""/""
+ - service: streaming
+   hostnames:
+   - mastodon-${env_id}.${domain}
+   path: ""/api/v1/streaming""
</code>
</pre></div><p id="""">This instructs Release to create two Nginx location blocks on the Kubernetes ingress controller for your application, to direct requests to either web or streaming, depending on the path in the request.</p><h3 id="""">Update the readiness_probe for Sidekiq</h3><p id="""">In the sidekiq service, replace the readiness_probe with the following:</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-docker"">
  readiness_probe:
    exec:
      command:
      - bash
      - ""-c""
      - ps aux | grep '[s]idekiq\ 6' || false
    period_seconds: 30
    timeout_seconds: 30
    failure_threshold: 3
    initial_delay_seconds: 10
</code>
</pre></div><h3 id="""">Update the readiness_probe for the webserver</h3><p id="""">In the web service, replace the readiness_probe with the following:</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-docker"">
  readiness_probe:
    http_get:
      path: ""/health""
      port: 3000
    initial_delay_seconds: 20
    period_seconds: 30
    timeout_seconds: 30
    failure_threshold: 3
</code>
</pre></div><h3 id="""">Update the readiness_probe for the streaming server</h3><p id="""">In the streaming server, replace the readiness_probe with the following:</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-docker"">
  readiness_probe:
    http_get:
      path: ""/api/v1/streaming/health""
      port: 4000
    initial_delay_seconds: 20
    period_seconds: 30
    timeout_seconds: 30
    failure_threshold: 3
</code>
</pre></div><h3 id="""">Add database migrations as jobs</h3><p id="""">Release can run jobs in your environments to handle once-off or repeat tasks. These can be triggered by steps in an environment’s workflows.</p><p id="""">For example, when Release first creates an environment for Mastodon, a db:setup job needs to be run to create Mastodon’s database schema. Release calls this stage in the workflow setup.</p><p id="""">Mastodon also needs to run database migrations during version upgrades, so that the database schema is updated along with the code. We can use Release’s patch stage for this job.</p><p id="""">During version upgrades, Mastodon splits pre-deployment and post-deployment database migrations. We’ll add two migration jobs, dbmigratepre and dbmigrate.</p><p id="""">Add the following jobs to your template:</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-docker"">
jobs:
- name: dbmigratepre
  command:
  - ""/usr/bin/tini""
  - ""--""
  args:
  - bash
  - ""-c""
  - rails db:version && export SKIP_POST_DEPLOYMENT_MIGRATIONS=true && rails db:migrate || rails db:setup
  from_services: web
- name: dbmigrate
  command:
  - ""/usr/bin/tini""
  - ""--""
  args:
  - bash
  - ""-c""
  - rails db:version && rails db:migrate || rails db:setup
  from_services: web
</code>
</pre></div><h3 id="""">Add an admin user as a job</h3><p id="""">After our services are live and the database is ready, we’ll use the Mastodon CLI to create an admin user.</p><p id="""">Add the following job to the jobs you created earlier:</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-docker"">
- name: mastodonuser
  command:
  - ""/usr/bin/tini""
  - ""--""
  args:
  - bash
  - ""-c""
  - tootctl accounts create $MASTODON_OWNER_USERNAME --email $MASTODON_OWNER_EMAIL --role Owner
  from_services: web
</code>
</pre></div><h3 id="""">Add jobs to workflows</h3><p id="""">Let’s update the workflows section to run these jobs during setup and patch stages.</p><p id="""">Because the order in which our new jobs are run is important, we’ll update the parallelize sections in these stages to create new steps.</p><p id="""">Replace the workflows section of the template with the following:</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-docker"">
workflows:
- name: setup
  parallelize:
  - step: dbmigratepre
    tasks:
    - jobs.dbmigratepre
  - step: services
    tasks:
    - services.sidekiq
    - services.streaming
    - services.web
  - step: dbmigrate
    tasks:
    - jobs.dbmigrate
  - step: mastodonuser
    tasks:
    - jobs.mastodonuser
- name: patch
  parallelize:
  - step: dbmigratepre
    tasks:
    - jobs.dbmigratepre
  - step: services
    tasks:
    - services.sidekiq
    - services.streaming
    - services.web
  - step: dbmigrate
    tasks:
    - jobs.dbmigrate
- name: teardown
  parallelize:
  - step: remove_environment
    tasks:
    - release.remove_environment
</code>
</pre></div><h3 id="""">Save your changes to the template</h3><p id="""">Click <strong id="""">Next Step</strong> to save your template, and move on to your environment variables.</p><h3 id="""">Edit the default environment variables</h3><p id="""">For this step, you’ll need to retrieve the following information that we saved previously:</p><h4 id="""">Generated on your local machine using Docker</h4><ul id=""""><li id="""">SECRET_KEY_BASE</li><li id="""">OTP_SECRET</li><li id="""">VAPID_PRIVATE_KEY</li><li id="""">VAPID_PUBLIC_KEY</li></ul><h4 id="""">PostgreSQL endpoint and password from Amazon RDS</h4><ul id=""""><li id="""">DB_HOST</li><li id="""">DB_PASS</li></ul><h4 id="""">Redis cluster endpoint from Amazon ElastiCache</h4><ul id=""""><li id="""">REDIS_HOST</li></ul><h4 id="""">SMTP credentials from Amazon Simple Email Service</h4><ul id=""""><li id="""">SMTP_LOGIN</li><li id="""">SMTP_PASSWORD</li><li id="""">SMTP_SERVER: Get the SES endpoint for your region from <a href=""https://docs.aws.amazon.com/general/latest/gr/ses.html"" target=""_blank"" id="""">AWS docs</a></li><li id="""">SMTP_FROM_ADDRESS: This is the verified email address Mastodon will send emails from</li></ul><h3 id="""">S3 bucket details and IAM access keys</h3><ul id=""""><li id="""">S3_BUCKET</li><li id="""">AWS_ACCESS_KEY_ID</li><li id="""">AWS_SECRET_ACCESS_KEY</li><li id="""">S3_REGION</li></ul><h3 id="""">Mastodon user details</h3><ul id=""""><li id="""">MASTODON_OWNER_USERNAME: Some names, such as admin, owner, and user, are reserved. Use something unique.</li><li id="""">MASTODON_OWNER_EMAIL: Use an email address you can receive your sign-up email at. Keep in mind that Amazon SES can only send messages to verified email addresses while in sandbox mode.</li></ul><p id="""">In Release, paste the following environment variables file in the default environment variables YAML editor, then update any values that are empty with the values we saved previously.</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yaml"">
---
defaults:
  - key: SINGLE_USER_MODE
    value: true
  - key: SECRET_KEY_BASE
    value: 
    secret: true
  - key: OTP_SECRET
    value: 
    secret: true
  - key: VAPID_PRIVATE_KEY
    value: 
    secret: true
  - key: VAPID_PUBLIC_KEY
    value: 
  - key: DB_HOST
    value: 
  - key: DB_PORT
    value: 5432
  - key: DB_USER
    value: postgres
  - key: DB_PASS
    value: 
    secret: true
  - key: REDIS_HOST
    value: 
  - key: REDIS_PORT
    value: 6379
  - key: SMTP_SERVER
    value: 
  - key: SMTP_PORT
    value: 587
  - key: SMTP_LOGIN
    value: 
  - key: SMTP_PASSWORD
    value: 
    secret: true
  - key: SMTP_AUTH_METHOD
    value: plain
  - key: SMTP_OPENSSL_VERIFY_MODE
    value: none
  - key: SMTP_FROM_ADDRESS
    value: 
  - key: S3_ENABLED
    value: true
  - key: S3_BUCKET
    value: mastodon-media-example
  - key: AWS_ACCESS_KEY_ID
    value: 
  - key: AWS_SECRET_ACCESS_KEY
    value: 
    secret: true
  - key: S3_REGION
    value: 
  - key: S3_PROTOCOL
    value: https
  # MASTODON_OWNER_USERNAME can't be admin, owner, user
  - key: MASTODON_OWNER_USERNAME
    value: 
  - key: MASTODON_OWNER_EMAIL
    value: 
services:
  sidekiq: []
  streaming: []
  web: []
mapping:
  LOCAL_DOMAIN: WEB_INGRESS_HOST
  S3_HOSTNAME: s3-${S3_REGION}.amazonaws.com
  DB_NAME: mastodon_prod_${RELEASE_RANDOMNESS}
</code>
</pre></div><p id="""">Click <strong id="""">Next Step</strong> to save your default environment variables.</p><h3 id="""">Deploy the app</h3><p id="""">You can skip over the <strong id="""">build arguments</strong> step because we’re not adding any build arguments to this application.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div id=""""><img alt=""Screenshot of app deploy button"" src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/639c7835708249b6615ad4c7_VFdy98n-rLDzf11zQeWpVcaEbwKgLMn0QgZlthGDXc6n_2gRiCcI4pRF-OPKWaNv7ljiwgupsBea0-l_sFVjsS-o1xA_oGoqrIs-sN8mo94p11hNb4qbEtRbr5oxD60I5dXL9al-DkUXEcpxfQ5lman2vGv5CnCwsgVrjOMytakXCzmdjHqALt6wdEg1ylzCwpx61iJKEg.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div><figcaption id="""">Deploy your app</figcaption></figure><p id="""">Finally, click <strong id="""">Deploy your app!</strong></p><h3 id="""">Follow the deployment</h3><p id="""">Release will now build Docker images for our services and deploy an environment.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div id=""""><img alt=""Screenshot of deployment"" src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/639c7835dc48796e567e5407_dmZ_lOtFTH-0wjUyzuvtPjTXMXsdeckiLHVNYzo4cWJTH5rM3UGotkaWyJhSbcIOyseOFbyT0ThvkGzsaiWlekmzS-OSxSTo9PPLKZL4TV8soO55iyN_TKJq1PauDYI1Rtl4oq_9e_yNs__V97sZGOrdXIxoTmcQrR_MkyC6c002nwghCG6BPBmCZm60H6qtyCokybbTmg.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div><figcaption id="""">Deployment progress</figcaption></figure><p id="""">The deployment information page shows all of the steps involved, from building the Docker images, through database migrations, and finally adding your admin user.</p><p id="""">Click the mastodonuser job step to reveal the output, and copy the password Mastodon generated for your user.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div id=""""><img alt=""Screenshot of step showing user password"" src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/639c7836872f31026dd2e0ab_z_jsn0TfTu9MVnj5EbGGBHJ9qRu_zeEMLwGsC5MkvPKFyhN6CipkpoYMLFiUuAalwMrl8aGqoF7LV2bhwOAmHIaTOeZNIpOtv_UxHJQsr42ksbCA2hwMAIHBFevs-YVmBMSskq6tNZQrvuOCGGQruTI7aHs-FrJX6F9T4aBz5GV9z6kZcHDsNCHGd1iQcI8_a-HMHF-AXg.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div><figcaption id="""">View user password</figcaption></figure><p id="""">If you miss this password, you can use the Mastodon web interface to request a password reset.</p><h3 id="""">Log in and start posting</h3><p id="""">With your Mastodon instance up and running, log in and start posting.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div id=""""><img alt=""Screenshot of environment info page, showing the web URL"" src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/639c78364d03cc55f6d1aa85_JATgB0cVh_yh5xWGx3vgRytYwdLkHJBnNLk3fnHY0JqM1th0QePjPvgaTJPt50k_KX4IrXW9_CFqD3SUHbbkEh5ai2GlN34daPyBqikjchZm8LzvfarscWpJ4vvkNv7mK72lj8QRRrba9_da4flW1LhpKTQ9_v6g7CafsVOVXZo18o8cFlvJPY61lZwPgVt_tpbeyIh2TA.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div><figcaption id=""""><em id="""">environment info page, showing the web URL</em></figcaption></figure><p id="""">You can find the public URL for your instance by navigating to the Environment Info page in Release. Click the web URL in the right sidebar to visit your instance.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div id=""""><img alt=""Screenshot of Mastodon"" src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/639c78366c72a7890d4f57e0_VvfuGML1dt3lxa0pttMuo1KFoCTH9BLK9676VeBblci110Vb5x91ftqhRVFSjj4HBiNVDYUa2Tqr-24VQ33nt_YHe5db8AkGdqnfd5NGZibGMJf7Qr80LDvDtiSMVg3N9aHKzm7hT6LuwiXXLelLu9fNZ45DJ1oT63j6pMnD9iigHedj7mCYH8q8Gm8KVsbc4mHw1wq_KQ.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div><figcaption id="""">Mastodon in action</figcaption></figure><h3 id="""">What to do next: create more environments</h3><p id="""">Release really shines when you create environments for different branches in your codebase.</p><p id="""">Test this by creating a feature branch with a small change, and follow Release’s guide on <a href=""https://docs.releasehub.com/getting-started/create-an-environment"" target=""_blank"" id="""">creating a new environment</a> for your new branch.</p><h3 id="""">Further reading</h3><p id="""">Even if Mastodon doesn’t replace Twitter, it will likely only grow in popularity. If you plan to host a public instance for your organization, you may want to make sure you focus on two important aspects: moderation and scaling.</p><p id="""">Running any community on the internet is notoriously hard to get right. Keeping users, the public, and your servers happy might seem impossible, but many have gone before us and shared their experiences.</p><p id="""">Here’s a quick overview of recent writing on these topics:</p><ul id=""""><li id=""""><a href=""https://docs.joinmastodon.org/admin/scaling/"" target=""_blank"" id="""">Scaling up your server</a>: The official Mastodon documentation on scaling your server, which might not be as suited to a Kubernetes-hosted instance.</li><li id=""""><a href=""https://docs.joinmastodon.org/admin/moderation/"" target=""_blank"" id="""">Moderation actions</a>: The official Mastodon documentation on available moderation tools. These are invaluable if you plan to allow public content on your instance.</li><li id=""""><a href=""https://joinmastodon.org/covenant"" target=""_blank"" id="""">Mastodon Server Covenant</a>: A covenant which can also inform your moderation and hosting decisions.</li></ul><h3 id="""">Learn more about Release</h3><p id="""">Of course, you aren't limited to hosting Mastodon. If you want to learn more about how Release can help you host your own software or other third party packages, <a href=""https://release.com/book-a-demo"" id="""">book a demo.</a></p>",false,,,,https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63e424029a7f57d2d51a127c_121622%20(1).jpg,an elephant with a tusk,,20,Fri Dec 16 2022 12:17:00 GMT+0000 (Coordinated Universal Time),,
How DebtBook Ships 6x Faster with Release  ,how-debtbook-ships-6x-faster-with-release,62aa5a70cd5ba27d9d0d718a,65e6035faf111d2974c452ee,Mon Mar 04 2024 17:22:39 GMT+0000 (Coordinated Universal Time),Wed Oct 23 2024 19:39:40 GMT+0000 (Coordinated Universal Time),Wed Oct 23 2024 19:39:57 GMT+0000 (Coordinated Universal Time),Every customer story is different. See how DebtBook implemented Release and never looked back.,,true,"<p id="""">Release helped Debtbook increase developer velocity by 6x. Sign up now and speed up your velocity.</p>",https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=casestudy&utm_content=debtbook,"<p id="""">We sat down with Michael Gorsuch, Director of Infrastructure at DebtBook, a rapidly growing financial software company to talk about ways they are improving developer experience and accelerating product delivery in a highly regulated field. Throughout the conversation we learned about the unexpected cultural shifts implementing Release brought to their team, discussed process improvements, and explored future collaboration plans. </p><p id="""">‍<strong id="""">Q: Michael, tell us about DebtBook. &nbsp;</strong></p><p id=""""><strong id="""">A: </strong>DebtBook builds accounting software for public sector finance teams. Regulatory landscape around public spending is rapidly changing and becoming more complex with the introduction of the Financial Data Transparency Act, GASB 87, GASB 96, and more. All these regulations are great for the taxpayers but add layers of complexity for the resource-strapped finance teams, making it that much more difficult to keep up. DebtBook founders spent years in the public sector regulatory space and saw how no one was addressing these challenges well, leaving the finance teams buried in disparate spreadsheets with outdated figures. With DebtBook, teams at your local hospitals, universities and city governments have a unified view of their debt, lease, subscription management, and compliance reporting, making sure tax-payers funds are spent wisely and transparently.</p><p id=""""><strong id="""">Q: Sounds like DebtBook has an important mission to fulfill. Tell us more about your particular team and the role you have in the company. </strong></p><p id=""""><strong id="""">A:</strong> I lead the Infrastructure team inside the Product Engineering org at DebtBook, and our primary role is to <strong id="""">help our engineering team win</strong>. Aside form the day-to-day duties, my team has two main objectives: <br>1. Ensure that the infrastructure is well aligned to how our engineering team wants to function, so that it is &nbsp;easy for them to stay on task. <br>2. Make sure everything [infra] is running smoothly and securely, so our engineering team is not bogged down with unnecessary toil. <br>I believe that our work is hard enough, and the challenges we are solving for our customers are extremely complex, so we do not need infrastructure to slow us down. We need to work on the hard, unsolved problems, and make everything that’s already been solved by someone else easy. </p><p id=""""><strong id="""">Q: What development challenges were top of mind for you? </strong></p><p id=""""><strong id="""">A: </strong>We relied on a number of shared environments to work through our deployment process. As the team grew and our product became more complex, the shared environments became more complex as well and our commit-to-customer cycle expanded to go well over a month. As you might expect, such long release cycles caused friction. Developers would spend a significant amount of energy perfecting their code, only to learn a few weeks later that they built an awesome feature that was not actually addressing the customer's needs. It was frustrating to context switch to old issues all because our shared environments setup was too slow and cumbersome to accommodate timely feedback. We knew we could ease these frustrations, shorten the feedback loop and speed up our deployment process, we just needed the right tools. </p><p id=""""><strong id="""">Q: Why did you think an ephemeral environments solution like Release could be the answer? </strong></p><p id=""""><strong id="""">A:</strong> We knew we needed a preview or on-demand environments option, and that our shared environments process was not going to cut it as the company grew and our product became more complex. In the past I worked with Heroku and came to rely on the preview functionality it offered. It made feedback faster and more actionable, and made collaboration easier overall. For various reasons Heroku was not a good fit at Debtbook, so we needed to find a reliable, secure, scalable and functional solution that our engineers could incorporate into their workflow quickly. </p><p id=""""><strong id="""">Q: Did you consider building a preview or an ephemeral environment solution in-house? </strong></p><p id=""""><strong id="""">A: </strong>Building an ephemeral environment platform ourselves was never a serious consideration at DebtBook. We knew it would take at least two senior engineers to build something usable, and it would take time. Time we did not have and time we did not want to spend on a project outside of our core competencies. As an engineer you always want to tackle the problem yourself, but as a leader you need to allocate your resources to the most impactful work, and building a platform someone already perfected made no sense for our growth. </p><p id=""""><strong id="""">Q: Release is not the only environments-as-a-service offering (although we’d argue it’s the best), how did you evaluate all the options and make your decision? </strong></p><p id=""""><strong id="""">A: </strong>Surprisingly there are not that many options for true environments-as-service platforms out there. Many claim they offer ephemeral environments, but are not a fit for complex, large applications that require high levels of reliability, security and scalability. That said, we did find a couple that seemed to fit the bill. We timeboxed the evaluation to a week, and set out to build a working prototype for engineers to play with. Release was the only one that actually worked as described. I was able to stand something up in one afternoon, shared it with the engineers and they got excited. It started a feedback loop to bring more things into Release and refine it, and it worked, it simply worked. I got good use of the free trial and once we were happy with the results, we engaged Release to start rolling it out to a wider team. The team at Release was on-point from day one, and continues to be a trusted partner. </p><p id=""""><strong id="""">Q: As you onboarded with Release, what changes did you start seeing in your organization?</strong></p><p id=""""><strong id="""">A: </strong>Release team was tremendously helpful during the onboarding process. It took us about a month to get everything up and running at the level we needed it, and roll it out to the wider team. From the start, we got amazing adoption among engineers. They all wanted a simpler, more seamless way to preview, test and deploy their changes and Release did just that from day one. Now within 10 minutes on average of pushing up a commit they have a full-stack production-like environment they can share with product or other engineers and show their work. They can do whatever they want in it without any risk of messing up production, or stepping on each other’s toes. </p><p id="""">With ephemeral environments we expected to see the commit-to-customer cycle to go down, and it did. We went from 40 days to less than a week to ship changes. What we did not expect was the massive culture change that followed. Giving engineers self-service, on-demand, isolated environments that spin up in minutes, with production-like data in tow, allowed them to take more chances, satisfy their curiosity and start digging into the issues that they simply could not have thought about before. Release allowed them to try out completely different patterns and effectively deploy something and show it to their peers or even to the customer right then and there, without breaking anything. We could not do these kinds of experiments with the static environments we had before. This has a tangible impact for our customers too. The other day we had a ticket go from creation to completely reviewed, tested and merged, and it took us less than 2 hours. A year ago before several changes (including using Release as the latest one) that would have taken days. Things like this make our engineers happy to come to work each day.</p><p id=""""><strong id="""">Q: What are the key benefits of implementing Release at DebtBook?</strong></p><p id=""""><strong id="""">A:</strong> There are three main benefits: <br>1. <strong id="""">Development velocity</strong> - we 6x our development velocity. Going from over 40 days to ship new features, to shipping multiple times a week. And that’s still slow. We are working on moving even faster as we speak. </p><p id="""">2. <strong id="""">Developer experience </strong>- &nbsp;our feedback loops shrank significantly. Our engineers are less frustrated, they collaborate and share ideas more freely, and get features in the hands of our customers faster. All this gives our team excitement and energy to tackle the next challenge. </p><p id="""">3. <strong id="""">Lean operations</strong> - as a small infrastructure team, we are able to deliver functionality well above our size. We’re small but mighty, leveraging our resources in a smart way. </p><p id=""""><strong id="""">Q: How would your team react if you stopped using Release today? </strong><br><strong id="""">A: </strong>Funny you ask that. At our recent offsite I had multiple engineers come up and comment on how happy they were with Release and how it made their life easier, and if I made it go away they would not stand for it. At this point Release is an essential part of our development workflow. It allows engineers to experiment and get relevant feedback in minutes, not days, which makes them that much more excited to work on our product. </p><p id=""""><strong id="""">Q: It’s great to hear that everyone is happy with Release. But I’m sure you have suggestions for improvement. Are there features or functionality Release is missing? </strong></p><p id=""""><strong id="""">A: </strong>We’ve been vocal about our feature requests from early days, and so far Release either implemented the requests, found workarounds, or helped us rethink processes on our side to make things flow smoother. It’s been a great partnership. Every time we have questions, Release team responds within minutes with genuine curiosity and willingness to help. I’d say Release meets our requirements at the moment and I’m confident they will continue to grow and improve alongside us. </p><p id=""""><strong id="""">Q: Who would you recommend Release to? </strong></p><p id=""""><strong id="""">A:</strong> Who wouldn't I recommend Release to? If you are building software you should be using Release. Unless you’re building an environments-as-a-service or infrastructure-as-a-service platform, and that’s your core business, or you're Google, Facebook or Netflix with hundreds of infrastructure engineers, you should not be building this functionality yourself. Inevitably it will consume too much of your time, effort and money, and you’ll still get something that’s only a fraction as good as Release. So who should use Release? Any company with complex cloud-first, containerized applications (internal or external) who wants to move faster and have a competitive advantage in their industry. <a href=""https://release.com/signup"" id="""">Give it a try</a>, and see for yourself. </p>",https://cdn.prod.website-files.com/603dd147c5b0a4221d1bd360/65e63ce75a5cdd6eb9c29cf8_release%2BDebtBook.jpg,Release DebtBook,ira-casteel,5,Tue Mar 05 2024 21:00:00 GMT+0000 (Coordinated Universal Time),customer-stories; product,components-of-a-successful-idp-build-a-product-your-developers-actually-want-to-use; lessons-learned-from-maintaining-the-soc-2-type-2-certification-over-the-years
How to Prepare For User Acceptance Testing?,how-do-you-prepare-for-user-acceptance-testing,62aa5a70cd5ba27d9d0d718a,62aa5a70cd5ba2c03e0d72f3,Tue Jan 04 2022 23:31:35 GMT+0000 (Coordinated Universal Time),Fri Apr 21 2023 19:41:18 GMT+0000 (Coordinated Universal Time),Fri Apr 21 2023 20:04:43 GMT+0000 (Coordinated Universal Time),In this post you'll learn about various stages of user acceptance testing and tips while preparing for UAT testing.,"<p id="""">This is a 4-part series on User Acceptance Testing (UAT)</p><ul id=""""><li id="""">Part 1: <a href=""https://release.com/blog/user-acceptance-testing-best-practices"" id="""">What is User Acceptance Testing and its Best Practices</a></li><li id=""""><strong id="""">Part 2: How to Prepare for User Acceptance Testing?</strong></li><li id="""">Part 3: <a href=""https://release.com//blog/what-are-the-challenges-faced-during-uat-testing"" id="""">User Acceptance Testing Challenges &amp; UAT Environment Examples</a></li><li id="""">Part 4: <a href=""https://release.com/blog/user-acceptance-testing-checklist"" id="""">UAT Checklist</a></li></ul><p id="""">Ensuring that the appropriate solution is delivered to the users is the ultimate goal of every test engineer. In this post, we are going to discuss various stages of user acceptance testing and some tips you can use while preparing for UAT.</p><h3 id="""">Tips to Prepare for User Acceptance Testing</h3><ul id=""""><li id=""""><strong id="""">Prepare and set up the right environment: </strong>To to carry out accurate performance testing, the first and foremost requirement is to have a production-like test environment.</li><li id=""""><strong id="""">Plan your test:</strong> Once done, it is crucial to design a clear test acceptance plan during the design and the requirement analysis phase as it helps to reduce pressure to meet deadlines.</li><li id=""""><strong id="""">Train the UAT staff adequately:</strong> Another important way to prepare for UAT is to train the testers adequately on the developed business requirements as it can help increase the success of UAT significantly.</li><li id=""""><strong id="""">Set up the right communication channel:</strong> UAT is a process that involves seamless collaboration between various teams, including the development team, the QA team, and the UAT team. It is, therefore, important to have a proper communication channel between these teams to ensure the success of UAT, especially when all these teams are working remotely.<strong id="""">‍</strong></li><li id=""""><strong id="""">Do not involve the functional testing team:</strong> Functional testers are not equipped enough to conduct UAT, and they may not test all real-world scenarios. This can less to end-users finding several issues when the software is in final production.</li></ul><h3 id="""">What are the Stages of Acceptance Testing?</h3><p id="""">Among the key stages of user acceptance testing are-</p><h4 id="""">Planning phase<br></h4><p id="""">The stage involves assigning a dedicated UAT test manager to oversee the end-to-end process of UAT. The main objective of this stage is to outline proper planning and execution strategy along with identification of important resources and preparation of a powerful resource plan.</p><h4 id="""">Preparation of UAT test data and test environment<br></h4><p id="""">This stage ensures UAT readiness as the UAT test environment is set up along with preparation of test management with test data, interfaces, authorization, and scenario readiness.</p><h4 id="""">UAT test scheduling and management<br></h4><p id="""">The phase marks preparing proper action plans along with UAT priorities. During this stage, a triage process is also kept in place to prioritize the assessments of defects blocking, if any. Apart from this, an effective mechanism to effortlessly track test scenarios/ test scripts based on the requirements defined is also taken up at this stage.</p><h4 id="""">Testing execution and defect management<strong id=""""><br></strong></h4><p id="""">An important phase of UAT, here the key goal is to take up proper identification of priority defects with more focus being placed on performing root cause analysis assessments. Apart from this, the stage also marks a trial run of UAT processes to validate execution and defects assignment.</p><h4 id="""">UAT sign-off and reporting<br></h4><p id="""">This is the last stage of UAT that involves testing of accurate defect, status reports as well as defect report generation from the test management system. Once done, a sign-off when all bugs have been fixed indicates the acceptance of the software. The idea here is to validate that the application being developed meets the user requirements and is ready for production.</p><p id="""">Now that you are familiar with <a href=""https://releasehub.com/blog/user-acceptance-testing-best-practices"" id="""">what is user acceptance testing, UAT best practices</a>, and how to prepare for user acceptance testing let's explore some of the typical challenges faced during user acceptance testing.<br></p>",false,,,,https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63e424ddfca74e23b048693a_122221%20(1)%20(1).jpg,a person holding a phone,tommy-mcclung,4,Thu Dec 23 2021 01:37:00 GMT+0000 (Coordinated Universal Time),,
How Does Terraform Enable Infrastructure as Code?,how-does-terraform-enable-infrastructure-as-code,62aa5a70cd5ba27d9d0d718a,63c670ee46d7cff95644e9ce,Tue Jan 17 2023 09:57:02 GMT+0000 (Coordinated Universal Time),Tue Jan 17 2023 09:57:02 GMT+0000 (Coordinated Universal Time),," In this post, learn about infrastructure as code (IaC) and how Terraform enables infrastructure as code.","<p id="""">A good deployment pipeline requires a well-balanced use of tools and resources. But finding that balance can be challenging. With infrastructure as code (IaC), you don't need to worry about the configuration-management side of the equation because you're creating your infrastructure based on how it works and what it should look like. In addition, as your infrastructure scales, it does so without human intervention. This eliminates the need for manual operations such as redeployment or rollback when changes go awry.&nbsp;</p><p id="""">This post will shed more light on what infrastructure as code is. Then, we'll explain how Terraform enables it.&nbsp;</p><h2 id=""""><strong id="""">What is Infrastructure as Code?</strong></h2><p id="""">Infrastructure as code is the art of using software tools to manage and provision physical, virtual, and cloud-based resources.&nbsp;</p><p id="""">IaC allows infrastructure engineers and application developers to design and build systems in an automated fashion with high levels of reliability. We can achieve this by defining the infrastructure in text-based files that a specification language interpreter then processes.&nbsp;</p><p id="""">The concept of IaC has its roots in configuration management. Configuration management refers to managing and maintaining the configuration of systems and devices in a consistent and controlled way.&nbsp;</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-center"" data-rt-type=""image"" data-rt-align=""center""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63bdad109a701b37c6be4411_Jg8_B0vUsNq3VTjXAl7FAw38--63NqLvx4FDPZca9duruhDTyEDMAt2uJnZb1LxXW2mlBpp3JM0lM-flMpsif5Z-39ccMP_gv25UK0Iaw7PBrlaTiycdIHiEgrb2XuJdNWENfZ466sjK_xuA9hlMyzqttl5yrmg-Dwc73tGmNIujXVbzsiejLEuqXjMb5w.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><p id="""">IaC builds on this concept by extending it to include infrastructure resources such as servers, storage devices, and networking equipment. This enables organizations to manage their infrastructure using configuration files rather than manual processes.&nbsp;</p><p id="""">Developing tools like <a href=""https://www.terraform.io/"" id="""">Terraform</a> and CloudFormation have enabled IaC by providing a way to define and manage infrastructure using configuration files. These tools have made it easier for organizations to automate the provisioning and management of infrastructure and have helped drive the adoption of IaC.&nbsp;</p><h2 id=""""><strong id="""">What Are the Benefits of IaC?</strong></h2><p id="""">These are some of the main benefits of IaC.&nbsp;</p><ul id=""""><li id="""">Simpler and more consistent provisioning: IaC helps you collaborate on infrastructure management and share infrastructure configurations with other teams and organizations.</li><li id="""">Easy and efficient infrastructure replication: By using IaC tools like Terraform, Ansible, and Puppet, we can define how we want our infrastructure to look using code. We can also deploy the configurations to any environment.</li><li id="""">Repeatable and auditable changes: You can track changes using version control, which helps keep the infrastructure up to date and reduces the risk of security threats.</li></ul><h2 id=""""><strong id="""">How do you Write IaC?</strong></h2><p id="""">There are several ways to write IaC depending on your project's specific needs and requirements. Some common approaches include:&nbsp;</p><ul id=""""><li id="""">Using a declarative language such as YAML or JSON to define the desired state of your infrastructure. This approach allows you to specify your infrastructure and the IaC tool will automatically provision and configure the resources to meet those specifications.</li><li id="""">Using a procedural language such as Python or Ruby to write scripts that provision and configure your infrastructure. This approach allows you to specify the steps that need to be taken to provision and configure your infrastructure. The IaC tool will execute those steps to create the desired infrastructure.</li><li id="""">Using a configuration management tool such as Ansible or Puppet to manage your infrastructure. These tools provide a set of predefined modules to provision and configure your infrastructure in a repeatable and predictable way.</li></ul><h2 id=""""><strong id="""">What is Terraform?</strong></h2><p id="""">Terraform is a tool for building, changing, and versioning infrastructure safely and efficiently. It can manage existing popular service providers and custom in-house solutions.&nbsp;</p><p id="""">Terraform manages the complete state of your infrastructure with declarative configuration files, which are treated like code. The system keeps knowledge of rebuilding or changing any resource or infrastructure component. This makes it safer than a traditional on-site human expert who needs to guide every action.&nbsp;</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-center"" data-rt-type=""image"" data-rt-align=""center""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63bdad1049cc7a3ce1232adc_zCa7rv82CnYu6dJZU3_56ooZQs_DSRmIwyzwrNwz9uBT_i5ql7fWcUiYTpFLdOqrhOXiv4-q54dgK6QsNYunyB_9H20--z8OKxFYAXmFTPZVG7v1_h9kGLHzrQhH0xxeADZMprDtSvBcfo1U2uelS9vCjl7C31p90LTsK1SJRVXOBr_A9VEOKtC6UNbcRQ.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><p id="""">When applied with IaC practices such as continuous integration (CI), <a href=""https://releasehub.com/blog/terraform-kubernetes-deployment-a-detailed-walkthrough"" id="""">Terraform</a> can become an increasingly powerful tool for rapidly provisioning new environments and then scaling them out safely while minimizing disruption to existing customers.&nbsp;</p><p id="""">Terraform's IaC feature keeps your infrastructure up to date with the latest versions of Terraform modules through updates and releases performed by other users.&nbsp;</p><h2 id=""""><strong id="""">What Code is Terraform Written in?</strong></h2><p id="""">Terraform is primarily written in the <a href=""https://go.dev/"" id="""">Go</a> programming language. HashiCorp initially developed Go and it's now an open-source project with contributions from a community of users and developers.&nbsp;</p><p id="""">Besides Go, Terraform uses the <a href=""https://developer.hashicorp.com/terraform/language"" id="""">HashiCorp Configuration Language</a> (HCL) as its primary configuration language.&nbsp;</p><p id="""">HCL is a declarative language that is human-readable and easy to write and understand. It's used to define the desired state of infrastructure, which Terraform uses to provision and configure the resources.&nbsp;</p><h2 id=""""><strong id="""">What Are the Benefits of Using Terraform?</strong></h2><ul id=""""><li id="""">Easy to use: You can codify existing system architecture into declarative configuration files. No more brittle shell scripts and manual operations.</li><li id="""">Fast: With Terraform, you can plan, create, and destroy infrastructure for hundreds of services in a matter of seconds. It doesn't require a central database, so it's also easy to install, no matter how big your environment is.</li><li id="""">Secure: Terraform provides locking mechanisms to prevent multiple teams from accidentally creating overlapping resources, which could easily happen with manual setups. Terraform also uses standard modules, so it's difficult to expose sensitive data like API keys or passwords accidentally.</li></ul><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-center"" data-rt-type=""image"" data-rt-align=""center""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63bdad105b7f2427295b9fec_On_-yD2hwBNtC2rLOCoeT-WOGy8dJB0rZEk7Ruh1kdoA-CEN4HG7vCDeGi0y1zF3qwaKt0eVPK6fd_JrVRMPgcmC6dsLZT-ZOPa_rLFJYzjIikkdwtDw3J6GSkvQXFd3ZEdTHz-F8ReAvq9UnU_4pMbt92YV90tTUv9roXjyAdX9zhmZimxT6rhZpHNmAQ.jpeg"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><h2 id=""""><strong id="""">How Does Terraform Enable IaC?</strong></h2><p id="""">Terraform enables IaC by providing a declarative language and tools that allow organizations to define and manage their infrastructure using code.&nbsp;</p><p id="""">Here's an example of how Terraform can enable IaC:&nbsp;</p><div data-rt-embed-type='true'># Define a virtual machine using the HashiCorp Configuration Language (HCL)
resource ""aws_instance"" ""web_server"" {
  ami           = ""ami-123456""
  instance_type = ""t2.micro""
  vpc_security_group_ids = [aws_security_group.web_server.id]

  tags = {
    Name = ""Web Server""
  }
}

# Define a security group for the virtual machine
resource ""aws_security_group"" ""web_server"" {
  name        = ""web_server""
  description = ""Security group for the web server""

  ingress {
    from_port   = 80
    to_port     = 80
    protocol    = ""tcp""
    cidr_blocks = [""0.0.0.0/0""]
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = ""-1""
    cidr_blocks = [""0.0.0.0/0""]
  }
}</div><p id="""">In this example, you define the <strong id="""">aws_instance</strong> and <strong id="""">aws_security_group</strong> resources using HCL. This code specifies the desired state of the infrastructure: A virtual machine with a specific AMI and instance type and a security group that allows incoming traffic on port <strong id="""">80</strong>.&nbsp;</p><p id="""">When you execute this code using the Terraform command-line interface (CLI), Terraform will provision and configure the resources in AWS to meet these specifications. The virtual machine and security group will be created and configured automatically without requiring manual intervention.&nbsp;</p><h3 id=""""><strong id="""">Key Features of Terraform That Support IaC</strong></h3><p id="""">Below are some features of Terraform that support and enable IaC:&nbsp;</p><ul id=""""><li id="""">You can use the HCL declarative language to define the desired state of your infrastructure. This allows you to specify how your infrastructure should look.</li><li id="""">Support for version control enables organizations to track changes to their infrastructure over time and collaborate with other developers on infrastructure management. This makes it easier to review and test changes to infrastructure and to roll back changes if necessary.</li><li id="""">The resource graph allows you to visualize and understand your entire infrastructure.</li><li id="""">Execution plans created in Terraform show you the actions that will be required to reach your desired state.</li><li id="""">Terraform can also orchestrate multiple services and providers, allowing you to manage your resources across different cloud platforms.</li></ul><h2 id=""""><strong id="""">Is Terraform an IaC?</strong></h2><p id="""">No, but it's an IaC <em id="""">tool</em>. IaC is a practice. Terraform is an IaC tool that enables organizations to manage their infrastructure using code.&nbsp;</p><h2 id=""""><strong id="""">What Other Tools can you use to Implement IaC?</strong></h2><p id="""">Other tools that can implement IaC include:&nbsp;</p><p id="""">1. Ansible&nbsp;</p><p id="""">2. Puppet&nbsp;</p><p id="""">3. Progress Chef&nbsp;</p><p id="""">4. SaltStack&nbsp;</p><p id="""">5. CloudFormation&nbsp;</p><p id="""">6. Vagrant&nbsp;</p><h2 id=""""><strong id="""">Conclusion</strong></h2><p id="""">With Terraform, your team can easily provision and manage their IaC. They'll scale their resources and deploy applications across different environments. Terraform also allows you to take advantage of different environments, like ephemeral environments. Maybe you'd like to spin up an environment—for example, to show a certain feature or product—without worrying that someone else will mess it up. Follow <a href=""https://releasehub.com/ephemeral-environments"" id="""">this guide</a> to get started with ephemeral environments.&nbsp;</p><p id=""""><em id="""">Discover the benefits of using infrastructure as code (IaC) with Terraform! IaC enables organizations to manage their infrastructure in a more predictable and repeatable way, which can improve the reliability and maintainability of their infrastructure. Learn more with Release. #IaC #Terraform #Ephemeral Environment</em>&nbsp;</p>",false,,,,https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63c6708a62d9338bfa5af8dd_skyscraper.jpeg,,,6,,,
How Release Uses Action Cable and Redux Toolkit,how-release-uses-action-cable-redux-toolkit,62aa5a70cd5ba27d9d0d718a,62aa5a70cd5ba2cfba0d72e3,Wed Jul 14 2021 14:45:06 GMT+0000 (Coordinated Universal Time),Fri Apr 21 2023 19:44:56 GMT+0000 (Coordinated Universal Time),Fri Apr 21 2023 20:04:43 GMT+0000 (Coordinated Universal Time),Walk through setting up Action Cable messages that are received by a React Component hooked up to Redux Toolkit.,"<p id="""">Over the past few weeks at <a href=""https://releasehub.com"" id="""">Release</a> the Frontend Engineering team has started working on adding Redux to Release. We had been making use of <a href=""https://reactjs.org/docs/context.html"" id="""">React Context</a> but felt that we were starting to stretch its capabilities. In some places we were having to add multiple providers to implement new features. After some research on the current state of Redux, we decided to go with <a href=""https://redux-toolkit.js.org/"" id="""">Redux Toolkit</a> and <a href=""https://redux-saga.js.org/"" id="""">Redux Saga</a>. Moving all our data into the Redux store and out of local state meant that we were going to have to change our approach with <a href=""https://guides.rubyonrails.org/action_cable_overview.html"" id="""">Action Cable</a> and how we were going to receive the messages, store them, and display changes for the user.</p><h3 id="""">Action Cable, Redux, and Release</h3><p id="""">Release uses Action Cable in a single direction, which is from the backend to the frontend. The frontend is a separate React application running as a <a href=""https://docs.releasehub.com/reference-guide/static-service-deployment"" id="""">Static Service Application</a>, not a part of Rails. The backend will send messages to the frontend when the state of objects change or to stream logs of deployments and builds. Today we're going to go through the thought process, including code snippets, of how we set up our Redux implementation for Action Cable when Release builds a Docker image. If you’re curious about how Release builds Docker images, read about we <a href=""https://releasehub.com/blog/cutting-build-time-in-half-docker-buildx-kubernetes"" id="""">Cut Build Time In Half with Docker’s Buildx Kubernetes Driver</a>.</p><h3 id="""">Action Cable Setup</h3><p id="""">Let’s start off with how we set up the backend to send updates as a <strong id=""""><em id="""">Build</em></strong> object progresses. We have two <strong id=""""><em id="""">ActiveRecord</em></strong> models to consider in this scenario, <strong id=""""><em id="""">Build</em></strong>, and <strong id=""""><em id="""">Log</em></strong>. The <strong id=""""><em id="""">Build</em></strong> class includes the <a href=""https://github.com/aasm/aasm"" id="""">aasm</a> gem functionality to progress it through the lifecycle of actually creating a Docker build. The following is an extremely pared down version of our <strong id=""""><em id="""">Build</em></strong> class, but has enough information to explain how we’re sending the Action Cable messages.</p><div data-rt-embed-type='true'><pre>
<code class=""language-ruby line-numbers"">
class Build &lt; ApplicationRecord
  include AASM
  include Logging

  has_many :logs
 
  aasm use_transactions: false do
    state :ready, initial: true
    state :running, after_enter: Proc.new { update_started_at; log_start }
    state :done, after_enter: Proc.new { set_duration; log_done }
    state :errored, after_enter: Proc.new { set_duration; log_error }

    event :start do
      transitions from: [:ready], to: :running
    end

    event :finish do
      transitions from: [:running], to: :done
    end

    event :error do
      transitions from: [:running], to: :errored
    end

  def log_start
    message = ""Build starting for #{repository.name}!""
    log_it(:info, message, metadata: log_metadata)
  end

  def log_done
    message = ""Build finished for #{repository.name}!""
    log_it(:info, message, metadata: log_metadata)
  end

  def log_error
    message = ""Build errored for #{repository.name}!""
    log_it(:error, message, metadata: log_metadata)
  end

  def log_metadata
    {
      build_id: self.id, 
      aasm_state: self.aasm_state,
      started_at: self.started_at,
      duration: self.total_duration
    }
  end

  def logs_channel
    ""build_channel_#{self.id}""
  end
end
</code>
</pre></div><p id="""">Whenever the <strong id=""""><em id="""">Build</em></strong> transitions its state, we create a <strong id=""""><em id="""">Log</em></strong> record through the <em id="""">log_it</em> method. A log level is supplied, along with the message, and metadata about the <strong id=""""><em id="""">Build</em></strong> itself. That metadata is used by the frontend to make changes for the user as you’ll see when we go through the Redux code. <em id="""">log_it</em> also sends the message to the <em id="""">logs_channel</em> through Action Cable. Since that wasn’t defined above, let’s look at that now.</p><div data-rt-embed-type='true'><pre class=""language-ruby line-numbers"">
<code class=""language-ruby"">
module Logging
  module Log
    def log_it(level, message, metadata: {})
      log_hash = {
        level: level,
        message: message.dup.force_encoding('UTF-8')
      }

      self.logs << ::Log.new(log_hash)

      payload = log_hash.merge(metadata)
      ActionCable.server.broadcast(logs_channel, payload)
    end
  end
end
</code>
</pre></div><p id="""">There is not too much to it. We create the <strong id=""""><em id="""">Log</em></strong> record and ensure the message is properly encoded. Then we combine the level, message, and supplied metadata to Action Cable and broadcast it. We use the <em id="""">log_it</em> method with more classes than just <strong id=""""><em id="""">Build</em></strong> and have found it makes for an easy and reliable way to store and send messages.</p><p id="""">That takes care of our state transitions. The last piece needed to wrap up our backend setup is to create the <strong id=""""><em id="""">BuildChannel</em></strong>.</p><div data-rt-embed-type='true'><pre class=""language-ruby line-numbers"">
<code class=""language-ruby"">
class BuildChannel < ApplicationCable::Channel
  def subscribed
    Rails.logger.info ""Subscribing to: build_channel_#{params['room']}""
    stream_from ""build_channel_#{params['room']}""
  end
end
</code>
</pre></div><p id="""">The method receives a room parameter to ensure we are sending messages about a specific <strong id=""""><em id="""">Build</em></strong> and does not go to everyone. I like to have the logging message in there so that it is easy to tell in the Rails logs if the frontend has successfully connected to the channel. With all that covered, we’re ready to dive into the setup on the frontend to receive those messages!</p><h3 id="""">Redux Setup</h3><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1496px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1496px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/60ef22b789c8568347b5569a_Release-Build-Screen.png"" loading=""lazy"" width=""auto"" height=""auto"" id=""""></div><figcaption id="""">Release Build Screen with Logs</figcaption></figure><p id="""">As you’ll recall we’re using Redux Toolkit and we’re not going to cover our entire setup with Toolkit, instead focusing only on the portions relevant to updating the <strong id=""""><em id="""">Build</em></strong> when we receive an Action Cable message. From there we’ll go over a small wrapper component we made to handle receiving the Action Cable messages and tie it all together with a small demo component.</p><p id="""">We’ll start off with the <strong id=""""><em id="""">BuildsSlice</em></strong>.</p><div data-rt-embed-type='true'><pre class=""language-javascript line-numbers"">
<code class=""language-javascript"">
import { createSlice } from ""@reduxjs/toolkit"";

import {
  handleBuildMessageReceived,
} from ""./helpers/actionCable/builds"";

const initialState = {
  activeBuild: undefined, // object
};

export const buildsSlice = createSlice({
  updateBuildFromMessage(state, action) {
    const message = action.payload;

    const build = state.activeBuild;
    const newBuild = handleBuildMessageReceived(build, message);

    return {
      ...state,
      activeBuild: newBuild,
    };
  },
})

export const {
  updateBuildFromMessage,
} = buildsSlice.actions;

export default buildsSlice.reducer;
</code>
</pre></div><p id="""">You’ll notice that we import <em id="""">handleBuildMessageReceived</em> from a file under <em id="""">helpers/actionCable</em>. We wanted to separate out the code for the logic of updating the build from the slice itself so that our slice file does not grow too enormous. Other than that, the slice itself follows the suggested setup of a slice from the <a href=""https://redux-toolkit.js.org/api/createslice"" id="""">createSlice</a> documentation.</p><p id="""">Now we need to look at our <em id="""">handleBuildMessageReceived</em> function.</p><div data-rt-embed-type='true'><pre class=""language-javascript line-numbers"">
<code class=""language-javascript"">
const handleBuildMessageReceived = (build, message) => {
  const buildId = message[""build_id""];
  const aasmState = message[""aasm_state""];
  const duration = message[""duration""];
  const startedAt = message[""started_at""];
  const level = message[""level""];
  const messageLog = message[""message""];
  
  const logs = build.logs;

  if (build.id !== buildId) {
    return build;
  } else {
    const newLogLine = { level: level, message: messageLog };
    const newBuild = {
      ...build,
      logs: [...logs, newLogLine],
      aasm_state: aasmState || build.aasm_state,
      total_duration: duration || build.total_duration,
      started_at: startedAt || build.started_at,
    };
    return newBuild;
  }
};

export { handleBuildMessageReceived };
</code>
</pre></div><p id="""">First a sanity check is done to ensure we didn’t somehow receive a message for a <strong id=""""><em id="""">Build</em></strong> that we aren’t viewing. This shouldn’t happen because we open and close our Action Cable subscriptions when we enter and leave a page, but an extra check never hurts. Then we construct a new <strong id=""""><em id="""">Build</em></strong> object by appending the new log line and adding the metadata. If the metadata fields are <em id="""">undefined</em>, we’ll retain what the <em id="""">build</em> variable already had.</p><p id="""">We’re ready to receive messages so we need a component that will handle that for us. The <strong id=""""><em id="""">ActionCableWrapper</em></strong> component is just that.</p><div data-rt-embed-type='true'><pre class=""language-javascript line-numbers"">
<code class=""language-javascript"">
import React, { useEffect, useState } from ""react"";
import actionCable from ""actioncable"";

export default function ActionCableWrapper({ channel, room, onReceived }) {
  const [actionCableConsumer, setActionCableConsumer] = useState(undefined);

  useEffect(() => {
    if (!actionCableConsumer) {
      setActionCableConsumer(actionCable.createConsumer(""ws://localhost:3000/cable""));
    } else {
      actionCableConsumer.subscriptions.create(
        { channel, room },
        {
          received: onReceived,
        }
      );
    }

    return () => {
      if (actionCableConsumer) {
        actionCableConsumer.disconnect();
      }
    };
  }, [actionCableConsumer]);

  return &lt;&gt;&lt;/&gt;
}
</code>
</pre></div><p id="""">This component will mount and check to see if <em id="""">actionCableConsumer</em> is not <em id="""">undefined</em>. However, if it is <em id="""">undefined</em>, which it will be on the first pass through the <em id="""">useEffect</em>, we will create a consumer through <em id="""">actionCable.createConsumer</em> connecting to a <em id="""">/cable</em> endpoint. <em id="""">""ws://localhost:3000/cable""</em> is hard coded but the URL should come from an environment variable so the component works locally or in production. That consumer is set into the local state <em id="""">actionCableConsumer</em> and the <em id="""">useEffect</em> will trigger a second time.</p><p id="""">In the second pass through, the <em id="""">else</em> block is entered and a subscription is created with the passed in <em id="""">channel</em>, <em id="""">room</em>, and <em id="""">onReceived</em> properties. The <em id="""">return</em> function is set to call <em id="""">disconnect()</em> if we have an <em id="""">actionCableConsumer</em> set and will ensure that no web socket connections are left open if a user navigates away from the page. With that, we have a reusable component that will take care of our Action Cable needs throughout the application.</p><p id="""">Pulling it all together, we can create a demo component that will display the state and logs and update whenever it receives a message.</p><div data-rt-embed-type='true'><pre class=""language-javascript line-numbers"">
<code class=""language-javascript"">
import React from ""react"";
import { useDispatch, useSelector } from ""react-redux"";

import { Grid } from ""@material-ui/core"";

import ActionCableWrapper from ""../ActionCableWrapper""; 

import { updateBuildFromMessage } from ""redux/slices/builds"";

export default function BuildDetailsCard(props) {
  const dispatch = useDispatch();
  const build = useSelector(state => state.builds.activeBuild);

  const handleMessageReceived = message => dispatch(updateBuildFromMessage(message));

  return (
    &lt;&gt;
       &lt;ActionCableWrapper channel=""BuildChannel"" room={build.id} onReceived={handleMessageReceived} /&gt;>
       &lt;Grid container&gt;
         &lt;Grid item xs={3}&gt;
           &lt;div&gt;
             &lt;b&gt;Repository Name: &lt;/b&gt; {build.repository.name}
           &lt;/div&gt;
           &lt;div&gt;
             &lt;b&gt;Commit Message: &lt;/b&gt; {build.commit_message}
           &lt;/div&gt;
           &lt;div&gt;
             &lt;b&gt;Commit SHA: &lt;/b&gt; {build.commit_short}
           &lt;/div&gt;
           &lt;div&gt;
             &lt;b&gt;State: &lt;/b&gt; {build.aasm_state}
           &lt;/div&gt;
         &lt;/Grid&gt;
         &lt;Grid
          item
          xs={9}
          style={{
            border: ""2px"",
            backgroundColor: ""#343a40"",
            fontSize: ""0.9rem"",
            fontFamily: ""Monaco"",
            color: ""white"",
            padding: 10,
          }}
        &gt;
          {build.logs.map(log => (
             &lt;div&gt;{log.message} &lt;/div&gt;
          ))}
         &lt;/Grid&gt;
       &lt;/Grid&gt;
    &lt;/&gt;
  );
}
</code>
</pre></div><p id="""">For demo purposes I probably went a little overboard with the styling, but I wanted to create something that resembles our actual application which you saw at the start of this post. The two things needed to power the page are the <em id="""">build</em>, which is retrieved with <em id="""">useSelector</em> and the <em id="""">handleMessageReceived</em> function, which dispatches <em id="""">updateBuildFromMessage</em> every time we receive a message through Action Cable. We supply the <em id="""">”BuildChannel”</em> and <em id="""">build.id</em> as the channel and room to <em id="""">ActionCableWrapper</em> along with <em id="""">handleMessageReceived</em> as the <em id="""">onReceived</em> function.</p><p id="""">In the video below I’ll move the build through its different states and we’ll be able to see the frontend receive the messages, update the state, and add the logs to the screen.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:800px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""800px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/60ef2c81b39a8fc12e675820_Release-Demo-Build-Component.gif"" loading=""lazy"" width=""auto"" height=""auto"" id=""""></div></figure><h3 id="""">Conclusion</h3><p id="""">That's a wrap on my adventure into how we set up our Action Cable integration with Redux Toolkit. There are tons of places in the application we’re going to be adding live updates too so that our users will always be up to date on the state of their application. I hope you enjoyed taking a peek inside some development work at Release. If you're interested in having an ephemeral environment created whenever we receive a Pull Request webhook from your Repository, head on over to the <a href=""https://release.com"">homepage</a> and sign up! If you’d like to join our awesome team, check out our <a href=""https://releasehub.com/company"" id="""">job listings</a>.</p>",false,,,,https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63e4195bb4efc07c08a62677_071521%20(1).jpg,"A toolkit that includes several tools such as hammer, saw and file",jeremy-kreutzbender,8,Thu Jul 15 2021 14:00:00 GMT+0000 (Coordinated Universal Time),,
How to Create and Configure Your Kubernetes Service Account,how-to-create-and-configure-your-kubernetes-service-account,62aa5a70cd5ba27d9d0d718a,63177b58675bafd004e5abf6,Tue Sep 06 2022 16:54:48 GMT+0000 (Coordinated Universal Time),Fri Apr 21 2023 18:00:47 GMT+0000 (Coordinated Universal Time),Fri Apr 21 2023 18:22:03 GMT+0000 (Coordinated Universal Time),How to access Kubernetes API from inside the pod? Learn what Service Accounts are and how to use them. ,"<p id="""">Accessing Kubernetes clusters has always been straightforward. You only need to download a kubeconfig file and place it in a specific place for your kubectl tool to read. This works well for human access, but there are use cases when you'd like some tools to access your Kubernetes API server. For example, your CI/CD pipeline somehow needs to authenticate to your cluster in order to deploy your applications there. For non-human access, Kubernetes offers what it calls service accounts. In this post, you'll learn what they are and how to use them.&nbsp;</p><h3 id="""">What Are Kubernetes Service Accounts?</h3><p id="""">Let's start with the basics. In order to understand what a Kubernetes service account is, you first need to know how the authentication mechanism works.&nbsp;</p><p id="""">When you access your Kubernetes cluster, you authenticate to the Kubernetes API as a human user via a user account. This is just an ordinary user account like in any other system. It distinguishes one user from another (however, by default, Kubernetes uses the same user account for all users).&nbsp;</p><p id="""">Normally, you should connect your Kubernetes cluster to an external user management solution like <a href=""https://en.wikipedia.org/wiki/Active_Directory"" target=""_blank"">Active Directory</a> or <a href=""https://en.wikipedia.org/wiki/Lightweight_Directory_Access_Protocol"" target=""_blank"">LDAP</a>. When you do that, users will authenticate to Kubernetes using their company email address. So, for each request to its API server, Kubernetes will be able to see who made the request. In most organizations, this will follow the typical firstname.lastname@company.com format.&nbsp;</p><p id="""">This model works perfectly fine for human users. But what about non-human users? They can't authenticate using user accounts because they're not human. They won't have a firstname.lastname@company.com email address. Which brings us to the point of this post. For these use cases, instead of user accounts, Kubernetes offers service accounts. And again, as the name suggests, these are special accounts that are meant to be used by non-humans or services.&nbsp;</p><h3 id="""">How To Create a Service Account</h3><p id="""">Now that you know the theory, let's get into the nuts and bolts. As with any other resource on Kubernetes, you can create a service account by using the <strong id="""">kubectl create</strong> command. In the case of service accounts, it's as simple as specifying <strong id="""">serviceaccount</strong> as the resource to be created, followed by its name.&nbsp;</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yaml"">
$ kubectl create serviceaccount my-service-account
serviceaccount/my-service-account created
</code>
</pre></div><p id="""">That's it. You just created a new service account. But don't get too excited yet. This service account won't be very useful because, by default, it won't have any permissions associated with it. In other words, it won't be able to do anything. In order to change that, you can use the same Kubernetes RBAC mechanism as with user accounts. Therefore, you need to create a role binding for your new service account to an existing Kubernetes role or create a new custom role. Here's an example.&nbsp;</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yaml"">
$ kubectl create rolebinding my-service-account-rolebinding \
   --clusterrole=view \
   --serviceaccount=default:my-service-account \
   --namespace=default
rolebinding.rbac.authorization.k8s.io/my-service-account-rolebinding created
</code>
</pre></div><p id="""">In the code above, I created a Kubernetes role binding that associates build in the ""view"" role with my new service account. By doing so, my service principal will now be able to contact the Kubernetes API and perform read-only operations. So, how do you actually use a service principal?&nbsp;</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1000px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1000px""><div id=""""><img alt=""Graphical user interface, text, applicationDescription automatically generated"" src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63177833e4caca1e059594ca_oV7DJmVT2hbRQeQxNMlaLQ5v1p-W7xPSjcixlYTyNBdILoJrHieW_VAaZ6JrtGwsFlMeM-MKOtFejyW5cZ-OPskFsb-XmEXYRm-3oT3o2PcfMO8X3DOAAKa-qKhwGsIefgSJQNfmVtQgqnm8a3mDrEg.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><h3 id="""">How to Use a Service Account</h3><p id="""">Using Kubernetes as a human user in most cases means downloading <a href=""https://kubernetes.io/docs/concepts/configuration/organize-cluster-access-kubeconfig/"" target=""_blank"">kubeconfig</a> and interacting with the cluster using the kubectl command. And as we already established, service accounts are used by non-humans. You already know how to create a service account, so now it's time to discuss how non-humans actually use them.&nbsp;</p><p id="""">First of all, what is non-human? In most cases, it just means pods on your cluster, be it your CI/CD agent that needs to be able to deploy other pods on the same cluster, a monitoring solution that needs to be able to get metrics from Kubernetes, or a security scanning tool that needs to get details about all pods on the cluster.&nbsp;</p><h3 id="""">Assigning Service Accounts to Pods</h3><p id="""">These are just a few examples. The point is that anytime an application running in a pod on your cluster will need to get some information about other pods or the cluster itself, it will need a service account. You already know how to create a service account, but your pods won't magically start using it. Especially since you may have a few different service accounts with different permissions assigned to them.&nbsp;</p><p id="""">Therefore, you need to somehow tell a pod which service account to use. The good news is that it's pretty simple. All it takes is one extra line in the <strong id="""">spec</strong> section of your deployment YAML definition.&nbsp;</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yaml"">
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: example-deployment
spec:
  replicas: 1
  selector:
    matchLabels:
      app: example-deployment-app
  template:
    metadata:
      labels:
        app: example-deployment-app
    spec:
      serviceAccountName: my-service-account
      containers:
        - name: busybox
          image: busybox
          command:
            - sleep
            - ""3600""
</code>
</pre></div><p id="""">By specifying <strong id="""">serviceAccountName</strong> in your deployment (or any other object that creates pods), you'll tell Kubernetes which service account to assign to the underlying pods. It's worth remembering that service accounts are assigned to pods themselves, not higher-level resources like deployments.&nbsp;</p><p id="""">Why did we specify <strong id="""">serviceAccountName </strong>in the deployment definition then? Simple. Because you normally don't create pods directly. You usually use these higher-level resources that create pods for you. And Kubernetes is smart enough and won't complain. It will just apply specified service accounts on the pods directly.&nbsp;</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1000px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1000px""><div id=""""><img alt=""Graphical user interface, text, applicationDescription automatically generated"" src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63177834f13ee47a783a650d_7czqMIoUHpFMWiG_VXBz58t98CiFFPeaXtRE7HQ0Edlfv99iVnax_dOEkVoN20lfThOmJBQrPm9iqzyyriZEj9110S4pKEico8QWCIjJGAfoCPd1FWT9lqEIeqhoqjwgi5whSIt7hIpknI1k2-4p7D0.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><h3 id="""">How to Validate If It Works</h3><p id="""">Now you know how to create and apply a service account to your pods. But how can you be sure that everything works and that your pod is, in fact, using a specified service account?&nbsp;</p><p id="""">It's quite straightforward. You can get the details of the pod with <strong id="""">kubectl get pod</strong> and pass the <strong id="""">-o yaml</strong> parameter. One of the lines in the <strong id="""">spec</strong> section of the output will tell you which service account the pod is using.&nbsp;</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yaml"">
$ kubectl get pod nginx-deployment-c486548df-4spkw -oyaml

apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: ""2022-08-27T15:36:29Z""
  generateName: nginx-deployment-c486548df-
(...)
spec:
  containers:
  - image: nginx:1.14.2
    imagePullPolicy: IfNotPresent
    name: nginx
    (...)
  serviceAccount: my-service-account
  serviceAccountName: my-service-account
(...)
</code>
</pre></div><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1430px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1430px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63177834064a3725ddca607b_sSs0tYmRggDaWUXhSgQ7lQWZWjBkD7RbnVkXbm2UPEpXc9lkkoo2sYO1hPHUQWeBah1tQ75cf8jHRejf3dYeYpDdVVJnwAa-bRFtEHCpziWnAEQo9HAg9q_lXUWaSpqS5Q-7AhNWDZaDpbq-JyBYumA.jpeg"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><h3 id="""">Adjusting Permissions</h3><p id="""">OK, now you have a running pod with a custom service account attached to it that allows the application running in the pod to view resources on the cluster. What if you want to add read-write permissions You have two options. You can delete the existing role binding for your service account and create a new one, or you can start from scratch and create a separate service account altogether.&nbsp;</p><p id="""">Let's look at the first option. For that, you first need to execute the <strong id="""">kubectl delete</strong> <strong id="""">rolebinding my-service-account-rolebinding</strong> command to delete the existing role binding. You need to do that because Kubernetes doesn't allow you to change role bindings.&nbsp;</p><p id="""">Now you can create a new role binding, this time binding your service account to the <strong id="""">edit </strong>role instead of <strong id="""">view.</strong> Previously you did it with an inline kubectl command. This time I'll show you how to do it using the YAML file. The definition for role bindings looks like this:&nbsp;</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yaml"">
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: my-service-account-rolebinding
  namespace: default
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: edit
subjects:
- kind: ServiceAccount
  name: my-service-account
  namespace: default
</code>
</pre></div><p id="""">Save the above snippet in a YAML file and apply it to the cluster just like with any other YAML definition using <strong id="""">kubectl apply</strong>.&nbsp;</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yaml"">
$ kubectl apply -f rolebinding.yaml 
rolebinding.rbac.authorization.k8s.io/my-service-account-rolebinding created
</code>
</pre></div><p id="""">And just like with any other Kubernetes resource, you can always list existing role bindings using the <strong id="""">kubectl get</strong> command.&nbsp;</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yaml"">
$ kubectl get rolebindings
NAME                             ROLE               AGE
my-service-account-rolebinding   ClusterRole/edit   34s
</code>
</pre></div><p id="""">Now, after restarting your pod, it will have read-write permissions.&nbsp;</p><h3 id="""">Summary</h3><p id="""">As you can see, creating and configuring a service account is not that difficult. It is, however, a useful thing to know since most Kubernetes-based tools these days use service accounts. On top of that, it's a good security practice to have the least privileged service accounts for your pods. Misconfigured service accounts with too many permissions and no control over which pod gets which service principal could easily lead to an attacker taking control over your cluster.&nbsp;</p><p id="""">If you want to learn more about Kubernetes, take a look at our other posts on <a href=""https://release.com/blog"">our blog</a>.&nbsp;</p>",false,,,,https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63e41d8d2ded594b89cabf54_091422%20(1).jpg,a laptop with a plant on the screen,kevin-luu,4,Wed Sep 14 2022 18:00:00 GMT+0000 (Coordinated Universal Time),,
How to Delete Hundreds or Thousands of Route53 DNS Entries,how-to-delete-hundreds-or-thousands-of-route53-dns-entries,62aa5a70cd5ba27d9d0d718a,62aa5a70cd5ba2f6d40d72d9,Wed Feb 03 2021 05:21:12 GMT+0000 (Coordinated Universal Time),Fri Apr 21 2023 19:57:26 GMT+0000 (Coordinated Universal Time),Fri Apr 21 2023 20:04:43 GMT+0000 (Coordinated Universal Time),"How do Delete Hundreds, or Possibly Thousands of Route53 DNS Entries Quickly and Easily","<h3 id="""">How to Delete Hundreds (or Thousands) of Route53 Entries Quickly on the Command Line</h3><h3 id="""">Overview</h3><p id="""">At Release, we make Staging environments easy by quickly creating and updating environments to run, test, and share your application code in full fledged, isolated environments. In previous versions of our product, we were able to quickly roll out new environments and features by creating tons of AWS Route53 DNS entries for each new application and environment. Unfortunately, that meant that we were quickly creating over 5,000 Route53 entries.</p><p id="""">The maximum number of Route53 entries you can have by default in one hosted zone is 10,000, so we needed to fix this before we ran out. Luckily, we added new features to create smart wildcard entries and a routing system to drastically reduce the number of entries we needed to create for ourselves and our customers.</p><p id="""">But then we were stuck with a legacy of over 5,000 entries that needed to be deleted (carefully!) in a reasonable timeframe and preferably automatically, rather than by hand. This article will show you how we accomplished the task and how this relatively obscure and niche problem (we hope!) can be solved relatively quickly and painlessly.</p><h3 id="""">Investigation</h3><p id="""">The initial approach is to simply come up with a command-line query to list Route53 entries and then parse them one by one to delete them. Unfortunately, the documentation quickly shows this to be the wrong method, since Route53 entry “upserts” (additions or changes) or “deletes” (as you would expect) need to be batched and uploaded in a transaction. There is no simple “delete one Route53 entry” command on the CLI as of the time of this writing. In point of fact, this naive approach is actually not a good way to do this type of bulk update anyway. Route53 will correctly handle each batch of operations as a transaction; so that if one entry fails to update or delete for some reason, the whole batch will be rolled back to preserve the integrity of your records.</p><p id="""">I therefore started with one of my favourite Stack Overflow answers that I turn to way more often than I should: <a href=""https://stackoverflow.com/a/48498598"" target=""_blank"" id="""">How to Export Route53 Zone File</a>. This was one of those copy-paste answers I would blindly use when approaching a Route53 use-case and it happily contained enough of a starting solution to building out the entire point of this blog post.</p><h3 id="""">A Slight Tangent on JQ</h3><p id=""""><a href=""https://stedolan.github.io/jq/"" target=""_blank"" id="""">JQ</a> is a <a href=""https://stackoverflow.com/a/56114895"" target=""_blank"" id="""">JSON query language</a> and is billed as “<a href=""https://en.wikipedia.org/wiki/Sed"" target=""_blank"" id="""">sed</a> for JSON”. For me, jq has always been a bit opaque and I usually just copy-paste whatever a Stack Overflow answer has provided. In the case of the problem presented in this blog post, I needed to really dive in and learn about the power jq offers to help me solve this problem. It is, indeed, part of the core solution the above Stack Overflow answer is based on.</p><p id="""">The first thing to note is that jq can be used to extract, transform, output, rollup, and filter JSON objects or text in a programmatic fashion. In this way, I have started changing my pitch to be that jq is “<a href=""https://en.wikipedia.org/wiki/AWK"" target=""_blank"" id="""">awk</a> for JSON”. I have found jq syntax and structure to be a bit difficult to test or grasp, so I was delighted to find <a href=""https://jqplay.org/"" target=""_blank"" id="""">jq Play</a>, an online resource for testing and visualising jq syntax and test inputs. I will use the screenshots from jq play to display my steps as we go along.</p><h4 id="""">Step 1, Gather Test Data</h4><p id="""">The first step, as exactly described above in Stack Overflow, is to run the AWS CLI to output a JSON list of all of your Route53 entries (in our case, over 5,000+!!!). Take a few lines of the first part of the output to play with. You can grab a few entries by limiting the --max-items option in the list-resource-record-sets command. Take those and paste them into the jq play screen on the left, then select “.” as the operator to output everything. You can follow along with this <a href=""https://jqplay.org/s/0EWB_zhjvG"" target=""_blank"" id="""">snippet</a>. Here is what it looks like initially:</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1314px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1314px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/603dd147c5b0a4b7781bd4ab_download.png"" loading=""lazy"" width=""auto"" height=""auto"" id=""""></div></figure><p id="""">This is a good starting point but we need to first start with unwrapping the outer layer of the “ResourceRecords” key to find the list of entries as <a href=""https://jqplay.org/s/PM3uM757DE"" target=""_blank"" id="""">follows</a> (turn on the “Compact View” to make it easier to understand and see more of what’s happening:</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1311px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1311px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/603dd147c5b0a418091bd4b4_figure2.png"" loading=""lazy"" width=""auto"" height=""auto"" id=""""></div></figure><h4 id="""">Step 2, Filter Records</h4><p id="""">Now we need to filter out records so that only the “AliasTarget -&gt; DNSName” keys matching a particular endpoint get deleted. That is relatively easy to do by filtering results with the pipe (“|”) character and using the “Select” operator as <a href=""https://jqplay.org/s/cKEL8vTJHr"" target=""_blank"" id="""">follows</a>:</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1329px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1329px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/603dd147c5b0a42e111bd4b9_figure3.png"" loading=""lazy"" width=""auto"" height=""auto"" id=""""></div></figure><p id="""">Keep in mind that in practice, we will be filtering way more than 2 records from 4 records, but this is just a test before we run the full solution. Also, keep in mind that you could use any number of filters and selector operators (for example on the “Type” field) to choose which entries to act upon. The world is your oyster!</p><p id="""">Which is a silly saying, of course. If the world is your oyster, then that is a salty, squishy, goey, messy, muscly world. And where is the pearl in your world? Some hard round misshapen thing rolling around in your bedroom so you can’t sleep comfortably? I suppose it’s better than sand everywhere, but really. “The world is your oyster”?</p><h4 id="""">Step 3, Manipulate Rows for Delete</h4><p id="""">The next step is to manipulate the rows we’re filtering/selecting to create the individual records that will become the batch delete operation. To do this, we will need to construct the output record form for each individual delete action using the schema that Route53 is going to expect. In this case, we extract several fields from each record and wrap them inside an “Action: DELETE” key as <a href=""https://jqplay.org/s/hsZ65XuciY"" target=""_blank"" id="""">follows</a>:</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1312px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1312px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/603dd147c5b0a43e5a1bd4ae_figure4.png"" loading=""lazy"" width=""auto"" height=""auto"" id=""""></div></figure><p id="""">Notice how the JSON in the right hand pane is looking like the output that we will be able to pipe back into an AWS CLI call to delete entries. We’re coming along nicely!</p><p id="""">Also keep in mind that you could extend this example to manipulate entries in any way you like, for example, changing record types, or bulk-changing TTLs or some other field.</p><p id="""">Notice how the outputs are newline-separated? This initially confused me, but you can easily create a list or map by wrapping the whole query inside either square brackets (for a list) or curly brackets for a map:</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1320px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1320px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/603dd147c5b0a4ca571bd4b3_figure5.png"" loading=""lazy"" width=""auto"" height=""auto"" id=""""></div></figure><h4 id="""">Step 4, Mind the Max Batch Size</h4><p id="""">We’re almost done, but since we’re deleting multiple hundreds (thousands, actually) of records, we want to set a batch size that is reasonable and that Route53 will accept. According to the documentation, the maximum batch size is 1000. I arbitrarily chose a batch size of 100 that is more reasonable and manageable for the CLI, so layer on the _nwise() operator as <a href=""https://jqplay.org/s/uu649cb-BM"" target=""_blank"">follows</a>:</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1317px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1317px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/603dd147c5b0a479e11bd4ad_figure6.png"" loading=""lazy"" width=""auto"" height=""auto"" id=""""></div></figure><p id="""">Keep in mind that in this example, I’m playing with 4 records, filtered to 2, and then batched into sizes of 1. In reality, we’re going to apply this to 5,000+ records, filtered to ~4,000 records, and batched at 100. The question you will want to ask yourself is, “How many records do I want to hassle with (possibly manually) if something explodes in the middle? Or if some intervention is required in a batch to add/remove/massage form one or more batches?” I settled on about 100.</p><p id="""">Hopefully you’re not as crazy as I am and in a similar predicament. You should be smart enough to avoid this situation in the first place. But if you are as crazy as I am, welcome to the club; we’re very sympathetic to your problems around here. I also really appreciate you reading all the way through to this spot, you crazy, wonderful, patient soul.</p><h4 id="""">Step 5, Wrap It Up</h4><p id="""">Each batch is ready to be delivered on one line as shown above, however, before we’re done we need to add a “Changes” key at the top level for Route53 to accept. This is easy to accomplish by just piping the results into a map with one key and using the period (“.”) to select “everything” as <a href=""https://jqplay.org/s/lYveGGoUwC"" target=""_blank"" id="""">follows</a>:</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1319px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1319px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/603dd147c5b0a4a83c1bd4b0_figure7.png"" loading=""lazy"" width=""auto"" height=""auto"" id=""""></div></figure><h4 id="""">Step 6, Apply</h4><p id="""">Now we are ready to actually apply the records and see how much damage we can do! Take the entire output of your records with this kind of query:</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yaml"">
aws route53 list-resource-record-sets \
  --hosted-zone-id ${hostedzoneid} \
  --max-items 10000 \
  --output json
</code>
</pre></div><p id="""">And pipe it into the handy command line options provided at the bottom of your screen in the jq player application:</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yaml"">
jq --compact-output '[.ResourceRecordSets[] |
  select(.AliasTarget.DNSName == ""something.us-west-2.elb.amazonaws.com."") |
  {Action: ""DELETE"", ResourceRecordSet: {Name: .Name, Type: .Type, AliasTarget: .AliasTarget}}] |
  _nwise(1) |
  {Changes: .}'
  </code> 
  </pre></div><p id="""">And use split to create a bunch of individual files:</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yaml"">
split -l 1
</code>
</pre></div><p id="""">Then loop over all your files to apply them in Route53:</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yaml"">
for file in x*; do
  aws route53 change-resource-record-sets \
  --hosted-zone-id=${hostedzoneid} \
  --cange-batch=file://${file}
done
</code>
</pre></div><h4 id="""">Step 7, Profit</h4><p id="""">I hope you enjoyed this exploration and how quickly you can manipulate JSON data with jq to produce a fast, efficient, and automated method of clearing out a bunch of old Route53 entries in your zones!</p><p id="""">hero image: <a href=""https://unsplash.com/photos/1wz7cN1XTmk"" target=""_blank"" id="""">Sharon McCutcheon via Unsplash.com</a>‍</p>",false,,,,https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63e41fada8bd241a98c471b6_101320%20(1).jpg,Hundreds of bright candy representing Route53 DNS Entries,regis-wilson,6,Wed Oct 14 2020 00:00:00 GMT+0000 (Coordinated Universal Time),,
How to edit a file in a Docker container,how-to-edit-a-file-in-a-docker-container,62aa5a70cd5ba27d9d0d718a,62d65f3d61ad71f6a4706949,Tue Jul 19 2022 07:37:33 GMT+0000 (Coordinated Universal Time),Wed Oct 30 2024 19:56:20 GMT+0000 (Coordinated Universal Time),Wed Oct 30 2024 19:56:20 GMT+0000 (Coordinated Universal Time),See how to edit files in Docker containers with command line editors or through connecting VS Code.,,true,<p>Automate Docker environments and boost efficiency with Release.</p>,https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=debtbook,"<p id="""">You want to edit a file in your Docker container, but you’ve run into an error that leaves you with none of the tools you need to make your changes. Now what?</p><p id="""">Docker intentionally keeps containers as lean as possible with no unnecessary packages installed to maximize performance and stability. Unfortunately, this also means Docker containers don’t have a file editor like Vim or Nano preinstalled. </p><p id="""">In this guide, we’ll show you how to install an editor, make the changes you need to, and return the container to its original state, both from the command line and using the Docker extension inside VS Code.</p><p id="""">First, though, some housekeeping. It’s considered bad practice to edit Docker files currently running in a production environment, and, once you’ve made your change, you should remove any packages you installed to do so (the editor, for example).</p><p id="""">Here’s our step-by-step guide to editing a file in Docker.</p><h3 id="""">Option 1: Edit from the command line</h3><h4 id=""""> #1 Log in to your container</h4><p id="""">If your container is not already running, run the container with the following:</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yaml"">
docker run --name <yourcontainername> -d -t
</code>
</pre></div><p id="""">To check all your running containers, you can use the command:</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yaml"">
docker ps
</code>
</pre></div><p id="""">You should be met with something like this:</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1234px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1234px""><div id=""""><img src=""https://cdn.prod.website-files.com/603dd147c5b0a4221d1bd360/62d65d3694842936728aa779_SaUETzedK1CnRTj6W7Cmhige6ZSr461UPfD87HrTaX9tnRljMs9ed0WZYY1cuhGT4M2tVWHMxXKBFToSG2ILxn-oXGcG5qMCAjpL9OdHs54HWONhqGAZ05EfA1KXDzx-7fNidFNnOQSPRXl4d9DblVs.png"" id="""" width=""auto"" height=""auto"" loading=""auto"" alt=""The console showing the output of docker ps listing the container ID and other information.""></div></figure><p id="""">This list indicates your target container is up and running. Note that every container has a discrete ID, which we’ll need to gain root access to the container.</p><p id="""">To gain root access to the container, run:</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yaml"">
docker exec -it <container-id>
</code>
</pre></div><p id="""">You should see something like this:</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1342px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1342px""><div id=""""><img src=""https://cdn.prod.website-files.com/603dd147c5b0a4221d1bd360/62d65d36948429df818aa77a_L76otMZwJANc57Pp3gcUeQaYd2CukDtsleYcwhObv1DMJZB69jriZ82PDzE6ChqBSfFbDHTs8alddTXvXdUXxTEdK074nWXWA-8iWEnkTTLAjf8-7GN0xxKlcyEl2xMBEByAZTVHlev3-0q70F5J1bA.png"" id="""" width=""auto"" height=""auto"" loading=""auto"" alt=""A prompt showing that root login has succeeded.""></div></figure><p id="""">As you can see, <strong id="""">root@&lt;container-id&gt;:/#</strong> indicates we now have root access to the container.</p><h4 id="""">#2 Install the editor</h4><p id="""">It’s a good idea to update your package manager before you install the editor. This ensures that you install the latest stable release of the editor. On Ubuntu, that command is:</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yaml"">
apt-get update
</code>
</pre></div><p id="""">To install your preferred editor, such as Vim, Nano or GNU Emacs:</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yaml"">
apt-get install <your package manager>
</code>
</pre></div><p id="""">For example, to install Vim:</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yaml"">
apt-get install vim
</code>
</pre></div><h4 id="""">#3 Edit the File</h4><p id="""">To edit the file, ensure you are in the appropriate directory and use the command:</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yaml"">
vim yourfilename.yaml
</code>
</pre></div><p id="""">Once you’ve made the edit to the file, you can remove the editor (in our case, Vim) like this:</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yaml"">
apt-get remove vim
</code>
</pre></div><p id="""">Or like this:</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yaml"">
apt-get purge vim
</code>
</pre></div><p id="""">The command “remove” will remove only Vim, and no other config files or dependencies involved in the initial install. The command “purge” will remove all config files associated with Vim. In the interest of leaving no trace, the purge command is probably appropriate in this case. </p><p id="""">Your package manager may change depending on your OS. These commands are associated with Ubuntu and Vim.</p><h3 id="""">Persisting an editor for regular changes</h3><p id="""">The above steps are useful for one-off changes, but if you need to make changes often – in a development environment, for example – it’s best to add your editor to your Dockerfile. This will ensure your chosen editor is always available whenever you spin up another instance of your container.</p><p id="""">Add your editor to the Dockerfile like this:</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yaml"">
RUN[""apt-get"", ""update""]
RUN[""apt-get"", ""install"", ""vim""]
</code>
</pre></div><p id="""">Every image created with that Dockerfile will have Vim pre-installed and ready to go.</p><p id="""">You can replace “Vim” with your editor of choice, such as Nano or GNU Emacs. Keep in mind that the commands in the square brackets are specific to Ubuntu Linux. You may need to adapt these to the operating system you are running in your Docker container.</p><h3 id="""">Option 2: Edit from VS Code</h3><p id="""">If you prefer to use a GUI editor (for example, if you’d like to use your mouse to navigate through large files, or cut and paste text), you can use VS Code.</p><p id="""">This option requires both the Visual Studio Code IDE and the Docker extension from Microsoft. To install the extension, navigate to the extensions tab in VS Code and type in “Docker”.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""600px""><div id=""""><img src=""https://cdn.prod.website-files.com/603dd147c5b0a4221d1bd360/62d65d376aba1158b03935b7_4tTOk_vnatQ5QG4ayy3KkwQ7aY6jwCy2hbf3NQSicPRDwUJT8rBpUMdqymctHZcMAAs86hjbeEBrrr8KBI7H66pNphS_KqAKyQHTV0ny6w1WE5euIxUXuPxFd9NB4X3lfOUSUfA_kTVvKQ7Ksp8rw2Y.png"" id="""" width=""auto"" height=""auto"" loading=""auto"" alt=""Search results in VS Code extensions for docker.""></div></figure><p id="""">Be sure to select the Docker extension from Microsoft. This extension allows you to easily manage any containers on your system directly from its UI. </p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div id=""""><img src=""https://cdn.prod.website-files.com/603dd147c5b0a4221d1bd360/62d65d37e934bf2a9e677479_QcGjdbrIzcSoSGQ0_vcW5JOj3jFV9KtLGi-RU9PdW4avj7AmlbxFne9uBkKp7RRQt4x9OJP_IB5TzIOBHpniHzWV-xz1KVKMUF5BwgE3vccEKebvFAqW2BKSKXwVMV2DbrOToJq6pZorWdmnI5aik8o.png"" id="""" width=""auto"" height=""auto"" loading=""auto"" alt=""VS Code displaying a file inside the docker container.""></div></figure><p id="""">From here, treating a container like any file directory, you can navigate to and open files in that container, and make your changes right in VS Code.</p><h3 id="""">Closing remarks</h3><p id="""">Now that you know how to edit files in a Docker file, it’s important to take note of the best practice for it.</p><p id="""">Editing files in a running Docker container is only recommended when working in a development environment during conceptualisation and when building proof-of-concepts.</p><p id="""">Once you’ve made changes to your project in Docker containers, save a new image with those changes in place. This leaves flexibility for testing two containers comparatively while ensuring stability and consistency across containers.</p><p id="""">‍</p><p id="""">‍</p>",https://cdn.prod.website-files.com/603dd147c5b0a4221d1bd360/63e41a723aa063959a913f67_080122%20(1).jpg,Some files on a table,regis-wilson,3,Mon Aug 01 2022 16:15:00 GMT+0000 (Coordinated Universal Time),docker,
How to Evolve Your Development and Deployment Workflow,how-to-evolve-your-development-and-deployment-workflow,62aa5a70cd5ba27d9d0d718a,649222d72f0e57ddbbde9324,Tue Jun 20 2023 22:06:15 GMT+0000 (Coordinated Universal Time),Thu Jul 25 2024 15:09:42 GMT+0000 (Coordinated Universal Time),Thu Jul 25 2024 16:27:39 GMT+0000 (Coordinated Universal Time),See how on-demand environments can provide an effective development experience for your team.,"<p id="""">In my recent PlatformCon talk <a href=""https://www.youtube.com/watch?v=9BDfebNQTGM&list=PLR74Ng-6aEfBOEdyuk3OMZu2cLFma4r6b&index=13&t=1s"" id="""">“From skateboard to car: How to evolve your developer experience”</a> I talked about different ways teams can evolve their development and deployment workflows. As your team grows and your application becomes more complex, it's important to upgrade your development and deployment workflow to keep up with the needs of the team. In this article I will recap the main points of the talk, and show how on-demand environments can provide an effective development experience for your team.</p><h3 id="""">What are Environments?</h3><p id="""">An environment is any machine or group of machines that serves an application, API, or service. From your laptop to an entire data center, everything in between can be an environment.</p><p id="""">Environments can be categorized into different types, depending on the purpose and level of access. For instance, a development environment is where developers write and test their code. A testing environment is where the code is tested in a close-to-production environment. A staging environment is a replica of the production environment, where the code is tested before deployment.</p><h3 id="""">The Problem</h3><p id="""">As your team and application grow, you may experience bottlenecks and coordination overhead in your shared ""sandbox"" environment. Waiting for test environments to be created or updated can slow down development and cause frustration.</p><p id="""">The traditional model of testing in a single environment can lead to problems. For example, if multiple developers are working on the same code and trying to test it in the same environment, it can lead to conflicts. Also, if developers are waiting for an environment to be created, they are not able to work on other things during that time. This can lead to a lot of downtime and lower productivity.</p><h3 id="""">The Solution</h3><p id="""">One way to solve this problem is to use ephemeral environments. These are temporary environments that are created on demand and then destroyed when they are no longer needed.</p><p id="""">Ephemeral environments can reduce developer burnout by removing bottlenecks and reducing the time it takes to ship features and bug fixes. When developers have to wait hours, days, or sometimes even longer for test environments to be created or updated, it can slow down the development process and create frustration.</p><p id="""">Ephemeral environments remove this bottleneck by allowing developers to create their own environments on demand, without having to wait for someone else to do it for them. This means that developers can test their code in an environment that is specific to their needs without having to worry about conflicts with other developers.</p><p id="""">Also, ephemeral environments are cost-effective and can lead to a more efficient workflow. They allow developers to work more independently and focus on their work instead of coordinating with others to get access to an environment.</p><h3 id="""">Implementing On-Demand Environments</h3><p id="""">There are many different tools available to implement on-demand environments, with varying levels of complexity and functionality. We'll start with an example ""skateboard"" approach, and discuss more advanced tooling until we get to the ""car"" level. </p><h4 id="""">🛹 Skateboard</h4><p id="""">The simplest option is to just build your environments by hand. This requires a lot of manual effort, but it does work. Typically done on an on-premise server or a cloud VPS, this is usually the first stop for most teams. Make a ""QA"" or ""staging"" environment, maybe both if you're feeling fancy, and call it good. </p><p id="""">Data issues crop up often at this level. Seed files are notoriously difficult to wrangle into actually representing anything close to what production data would look like. This often leads to bugs slipping through the cracks, because the data just isn't there to expose the bug in the first place. &nbsp;</p><p id="""">This approach is suitable for small teams, but it can quickly become unmanageable as the team grows, and it is not on-demand. </p><p id=""""><strong id="""">🎯 When to use:</strong> Small team; Very early stage</p><p id=""""><strong id="""">🧩 How to build it:</strong> On-prem server or Cloud server; Lots of manual work</p><p id=""""><strong id="""">👍 Pros:</strong> Simplest to set up initially; Easiest to understand and debug</p><p id=""""><strong id="""">👎 Cons:</strong> Creates bottlenecks; Coordination and maintenance overhead</p><h4 id="""">🚲 Bicycle</h4><p id="""">Tools like GitLab and Rancher can automate environment creation on a per-branch basis or through a button push. This approach is more automated and can save time and effort by reducing the toil of coordinating who/what gets to use which environment when, while still giving you a lot of control over the environment.<br>GitHub Codespaces integrates tightly with your existing workflow, but you'll still need to know enough about how your infrastructure is designed to be able to configure everything correctly.</p><p id="""">This approach reduces coordination overhead but still requires significant maintenance, and the data issues often still persist at this level. </p><p id=""""><strong id="""">🎯 When to use:</strong> Growing team; Encountering environment bottlenecks</p><p id=""""><strong id="""">🧩 How to build it:</strong> <a href=""https://about.gitlab.com/"" id="""">GitLab</a> + <a href=""https://www.rancher.com/"" id="""">Rancher</a> or <a href=""https://github.com/features/codespaces"" id="""">GitHub Codespaces</a></p><p id=""""><strong id="""">👍 Pros</strong>: Low costs; Low maintenance overhead</p><p id=""""><strong id="""">👎 Cons:</strong> Limited flexibility; Still decent bit of initial set up</p><h4 id="""">🛵 Motorcycle</h4><p id="""">Cloud-based platforms like Heroku automate environment deployments, but can lead to high vendor lock-in and cost. This stage is generally most useful for teams without much or any dedicated infrastructure / DevOps / SRE resources, and while requiring less maintenance than a home baked deployment system, it can lead to some pretty high bills.</p><p id="""">🎯<strong id=""""> When to use:</strong> Medium to large dev team(s); Limited ops talent</p><p id=""""><strong id="""">🧩 How to build it:</strong> <a href=""https://www.heroku.com/home"" id="""">Heroku</a>; <a href=""https://fly.io/"" id="""">Fly.io</a>; <a href=""https://render.com/"" id="""">Render</a>; <a href=""https://vercel.com/"" id="""">Vercel</a></p><p id=""""><strong id="""">👍 Pros:</strong> Easier setup than manual options; Offload operational overhead</p><p id="""">👎<strong id=""""> Cons:</strong> Cost can be higher; More reliant on 3rd party services</p><h4 id="""">🏎️ Car</h4><p id="""">Enterprise-ready Environment-as-a-Service companies like Okteto, Ergomake, and Release provide the most fully-featured environment solution, but can be costly.<br>Each of these options has its own advantages and disadvantages. The best option for your team will depend on your specific needs and budget.</p><p id="""">This stage may seem like overkill for smaller teams, but I would argue the benefits outweigh the drawbacks. Having as close to production environments as possible will increase the velocity of teams of any size, and give you confidence when deploying new code that it's already been tested in a production-like environment. Depending on the provider, you can use helm charts with your own kubernetes cluster to automatically spin up and tear down environments that mirror production at the individual component level. </p><p id="""">Some allow you to pause environments when not in use, reducing the costs. And some have features like Instant Datasets that mirror production data (scrubbed of PII) and avoid data integrity issues. Many services also offer Remote Development options, where developers can edit code on their local machine, and those changes are synced directly and immediately to the cloud environment. \It really is the best of both worlds as far as speed of iteration and environment correctness.</p><p id="""">It really is the best of both worlds as far as speed of iteration and environment correctness.</p><p id=""""><strong id="""">🎯 When to use:</strong> Need enterprise features; On-prem requirement; Advanced use cases</p><p id=""""><strong id="""">🧩 How to build it:</strong> <a href=""https://release.com/"" id="""">Release.com</a>; <a href=""https://www.okteto.com/"" id="""">Okteto</a>; <a href=""https://ergomake.dev/"" id="""">Ergomake</a>; <a href=""https://www.bunnyshell.com/"" id="""">BunnyShell</a>; <a href=""https://blog.massdriver.cloud/changelog/2023-01-19-preview-environments/"" id="""">Massdriver</a>; <a href=""https://shipyard.build/"" id="""">Shipyard</a>; <a href=""https://docs.acorn.io/#what-is-acorn"" id="""">Acorn</a>; <a href=""https://www.tugboatqa.com/"" id="""">Tugboat</a></p><p id=""""><strong id="""">👍 Pros:</strong> Professional level support; Easy onboarding; Power-user features</p><p id=""""><strong id="""">👎 Cons:</strong> Switching costs can be high; Many features leads to more footguns</p><h3 id="""">Conclusion</h3><p id="""">Whether you choose to go with a skateboard, a car, or something in between, the most important thing is to find the approach that works best for your team and your product. Continually reevaluating your needs as your team and application grow is crucial. By providing fast, efficient, and fulfilling environments, you can increase productivity, avoid burnout, and provide better outcomes for your product and users.</p><p id="""">The benefits of using on-demand environments are clear. They can help you reduce coordination overhead, increase productivity, and improve the quality of your code. By implementing the right tools and processes, you can create an environment that is tailored to the needs of your team and your product. And by adopting new technologies and best practices, you can ensure that your team is always working at their best.</p><p id="""">‍</p>",false,,,,https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/6492228c8c879d44a7927b38_Evolve%20Developer%20experince.jpg,,nick-busey,8,Wed Jun 21 2023 16:00:00 GMT+0000 (Coordinated Universal Time),product,
How to Get Started With Infrastructure As Code on AWS,how-to-get-started-with-infrastructure-as-code-on-aws,62aa5a70cd5ba27d9d0d718a,6321fe285343a069bfd44357,Wed Sep 14 2022 16:15:36 GMT+0000 (Coordinated Universal Time),Fri Apr 21 2023 17:35:52 GMT+0000 (Coordinated Universal Time),Fri Apr 21 2023 18:22:03 GMT+0000 (Coordinated Universal Time),A quick overview of getting started with Infrastructure as Code (IaC) on AWS (Cloudform and Terraform).,"<p id="""">Infrastructure as code (IaC) is a crucial component of the DevOps playbook. It approaches infrastructure by including it in the code, as code, ensuring consistent configurations and environments across many stages of a project.</p><p id="""">AWS is an excellent tool to use to facilitate this, as it offers many ways to create and operate infrastructure as code.</p><p id="""">In this post, we'll take a look at how to get started with this approach and what it should look like. We'll also look at two tools AWS uses for building infrastructure—CloudFormation and Terraform—as well as best practices for infrastructure as code on AWS.</p><h3 id="""">What Is Infrastructure as Code?</h3><p id="""">The key goal of infrastructure as code is to automate infrastructure. Rather than having a system administrator (or several, as is usually the case) devote valuable work hours to manually setting up configurations on every single server, configurations can be stored as code.</p><p id="""">This configuration then provisions and maintains servers with well-documented and version-tracked code. This comes with several advantages, including:</p><ul id=""""><li id=""""><strong id="""">Speed</strong>—By having a standard code to run for provisioning, deployment, and production, the entire pipeline becomes faster and simpler. It cuts down on decision-making and implementation time needed to run environments significantly.</li><li id=""""><strong id="""">Less manual effort </strong>— System administrators don't need to handle every facet of infrastructure, meaning less work and lower cost.</li><li id=""""><strong id="""">Prevents environment drift</strong>—When system administrators handle infrastructure on a server-by-server basis, each configuration is ultimately handled in different ways. This causes the configurations to ""drift"" further and further apart until the configuration active on one server is impossible to repeat on another. IaC provides a consistent configuration for each environment.</li></ul><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1386px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1386px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/6321fd0dcabd98eae9a66e6d_UUY2OtuFiHI7JgOLdPMb3hiAmsx-q9bqSKetRrEqENeeOionZfN7d77xN6AvwTSt8vl7UXWXLPk74ULlUmAY3RxuXKQWvRNv_0dS36KS5GZqMn3FZfEPsC8ZEIkkVAnrNUKlo8w8s4_xkntReDbNeHA8LUOO-uSFseE_vBYJQgWQudX8iXoEvwa3Zw.jpeg"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><h4 id="""">Infrastructure As Code on AWS</h4><p id="""">AWS is the lead cloud-computing platform in today's market. &nbsp;As such, many infrastructure as code tools are built for or to integrate smoothly with AWS.</p><p id="""">However, AWS isn't the go-to infrastructure as a service (IaaS) platform only because it's the most common.</p><p id="""">It also comes with a number of advantages. The first and foremost benefit of AWS—and the one we'll focus on here—is that it only charges for computational usage. This goes hand-in-hand with infrastructure as code, as it streamlines provisioning even further.</p><p id="""">You allocate servers through AWS, initialize them immediately with an already-coded configuration (aided by AWS' tools for exactly that), and you have a consistent environment ready to go.</p><p id="""">But enough about how easy infrastructure as code is. Let's take a look at how to actually set it up and what it should look like!</p><p id="""">In this post, we'll look at two tools: CloudFormation and Terraform. If you're ready to get started with your infrastructure setup, you've likely already chosen a tool to work with. However, as a quick recap, <a href=""https://docs.aws.amazon.com/cloudformation/index.html"" target=""_blank"">CloudFormation</a> is an AWS service for setting up AWS resources, and <a href=""https://www.terraform.io/intro"" target=""_blank"">Terraform</a> is an open-source infrastructure as code tool by HashiCorp.</p><h4 id="""">CloudFormation</h4><h5 id="""">Templates</h5><p id="""">Both CloudFormation and Terraform are declarative tools. This means that rather than defining what steps to take while provisioning infrastructure, you'll be defining a desired <em id="""">end</em> state. CloudFormation then decides the path to take to achieve that state.</p><p id="""">You'll use a template to tell CloudFormation what configurations you want created in the stack. The template is a text file written in JSON or YAML formats. It doesn't really matter which format you use. It can be slightly more advantageous to use YAML, as parsers can read JSON and not vice versa. It's also the default format for other tools, such as Kubernetes and <a href=""https://docs.release.com/reference-documentation/release.yaml"" target=""_blank"">Release's build files,</a> so for consistency's sake, if you're using Release for your project, it might be easier to keep everything in YAML.</p><p id="""">The important part is that you have all your desired configuration settings and resources matched to string keys that explain what they do to both the parser and anyone who might be reading the code—for example, ""WebServer"" followed by specifications for desired web server properties.</p><p id="""">Readability is important for template files, as anyone working with this configuration should be able to check the specifics of it.</p><h5 id="""">Stacks</h5><p id="""">Once you have a template file, this is used to create a stack. CloudFormation checks if the resources listed in the template are available and creates the stack based on the uploaded template. Use the Events tab to monitor the creation process.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:899px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""899px""><div id=""""><img alt=""events tab showing progress creating stacks"" src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/6321fd0b698fe347b933faec_4ZXYfMINJ4uPgoNUFDhV3RX3AMgcj1uUs7vaq_aL6LsaeBimXMwxqwGUKtda60KOFnVpotB6RXeLc7qErjYSS1q06Ch1qhREcx10vwPn-UzbqSlaLg-dE6U5OX588n_PDxvrNkuQaxK-e1BoO6IDu2JWhxTcajB42giXNDYOt0cgFssrf1jGj2UM5g.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><p id=""""><em id="""">Source: aws.amazon.com</em></p><p id="""">Once the Events tab shows ""CREATE_COMPLETE"" for a new stack, that stack is ready to go! You can start using those resources right away.</p><p id=""""><a href=""https://docs.aws.amazon.com/cloudformation/index.html"" target=""_blank"">For more details, read through AWS' CloudFormation documentation.</a></p><h4 id="""">Terraform</h4><p id="""">Terraform is an independent program, meaning you'll have to link it to AWS first. Do this by creating an access key for your AWS account and setting the access key environment variables in Terraform to your access key.</p><p id="""">Once you've done this, you can get to work on defining an AWS-based infrastructure in Terraform. Terraform has its own native configuration language, and while Terraform also includes functions for encoding and decoding both JSON and YAML, the YAML functions are experimental, so it's best to keep your configuration files in Terraform's native language.</p><p id="""">Similar to CloudFormation's template files, the configuration files declare the desired configuration and settings. For a detailed overview of these steps and how to write these configuration files, <a href=""https://learn.hashicorp.com/collections/terraform/aws-get-started"" target=""_blank"">make sure to read Terraform's AWS-specific documentation.</a></p><p id="""">We then have to implement the configuration files. Terraform does this via the CLI. The configuration is initialized, formatted, validated, and applied using the following Terraform commands:</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yaml"">
terraform init
terraform fmt
terraform validate
terraform apply
</code>
</pre></div><p id="""">Once we reach the ""apply"" stage, we see one of the notable differences between CloudFormation and Terraform. While both are declarative and create a plan to reach the defined end state, Terraform prints this execution plan into the CLI for user approval, detailing the changes made to the infrastructure to achieve the desired configuration. This gives you a bit more control over what exactly happens to your infrastructure.</p><p id="""">After approving this execution plan, Terraform applies the steps. This stage is, again, quite similar to the CloudFormation process, though it uses a CLI to monitor instead of a GUI tab.</p><p id="""">The end result can be reviewed with the command</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yaml"">
terraform show
</code>
</pre></div><h4 id="""">Best Practices for IaC on AWS</h4><p id="""">AWS' focus lies in providing infrastructure and infrastructure as code. As such, they have extensive support and documentation, and it's worth reading their own <a href=""https://docs.aws.amazon.com/cdk/v2/guide/best-practices.html"" target=""_blank"">best practices for infrastructure</a> section. However, let's look at a few of the most important considerations.</p><ul id=""""><li id=""""><strong id="""">Streamline</strong>—Because this infrastructure as code <em id="""">is</em> code, it behaves like code. It's version-tracked, duplicated, and used often. By eliminating duplicates and discrepancies, you can avoid overusing resources to store and integrate the code. Try to avoid unnecessary duplicates of code or code segments, and avoid unnecessary code segments overall.</li><li id=""""><strong id="""">Test</strong>—Infrastructure as code is also a form of automation and, as such, unit testing and other fail-safes are crucial to ensure that your code is actually doing what it's supposed to be doing.</li><li id=""""><strong id="""">Clarify</strong>—Infrastructure as code is meant to be used often and will likely be used by several members of your team. Make sure your code is readable, well-documented, and able to be quickly interpret by others. YAML and Terraform's native language are conducive to easy-to-read code, so with a little bit of attention, this is easy enough to achieve.</li></ul><p id="""">There's also a lot to be said for using <a href=""https://release.com/blog/environments-as-a-service-eaas-top-3-benefits"">environments as a service (EaaS)</a> to make sure you're getting the most out of AWS with streamlined and tested environments. Release also <a href=""https://docs.aws.amazon.com/cdk/v2/guide/best-practices.html"" target=""_blank"">has existing AWS support.</a></p><h3 id="""">Conclusion</h3><p id="""">AWS is the leading Infrastructure-as-a-Service provider, with countless resources for infrastructure as code.</p><p id="""">AWS documentation has a wealth of tutorials, best practices, and FAQs for infrastructure as code. Additionally, because AWS is so common and robust, the most commonly used IaC tools—such as CloudFormation and Terraform—work with it seamlessly. This means comprehensive support and resources.</p><p id="""">With some attention, you can set up a stable foundation for your infrastructure, making your workflow simpler and more efficient.</p>",false,,,,https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63e420dac599227a8a72b00d_110222%20(1).jpg,a close up of a keyboard,kelsey-degeorge,3,Wed Nov 02 2022 16:00:00 GMT+0000 (Coordinated Universal Time),,
How Do You Make Kubernetes Config Files Not Suck?,how-to-make-kubernetes-config-files-not-suck,62aa5a70cd5ba27d9d0d718a,62aa5a70cd5ba240f30d72c1,Wed Feb 03 2021 05:22:59 GMT+0000 (Coordinated Universal Time),Thu Apr 25 2024 20:52:43 GMT+0000 (Coordinated Universal Time),Thu May 02 2024 20:19:24 GMT+0000 (Coordinated Universal Time),I know that when I try out a new product if it’s hard to see what it does quickly I usually move on. ,"<h3 id="""">How Do You Make Kubernetes Config Files Not Suck?</h3><p id="""">Nothing makes me break out in a panic and cold sweat faster than someone saying, “Edit the YAML config files and push it to production.” I have so many welts and scars on my backside from years of YAML file mishaps in production. I have also personally witnessed and had to try to fix many more such production outages due to YAML files being edited and pushed to production. In some cases, just figuring out what was wrong with the YAML file, much less how to fix it took seemingly endless minutes of frantic searching and scrambling to save a production website that was down and losing money.</p><p id="""">Please note, this is not a JSON vs. YAML or Yet Another Data Language vs. YAML(because YAML Ain’t Markup Language) religious war! You can actually use JSON for Kubernetes configuration files <em id="""">if you want to</em>. The real issue is that <em id="""">there are so many</em> of them and <em id="""">they repeat so often</em> and <em id="""">I don’t know what to put where</em> or even <em id="""">where to find out where to start</em>.</p><p id="""">There is a very good page of <a href=""https://kubernetes.io/docs/concepts/configuration/overview/"" target=""_blank"" id="""">best practices</a> and the documentation for Kubernetes does tend to be surprisingly useful. There are tons of useful videos on Youtube that are helpful, so I am not even complaining about that.</p><p id="""">The problem begins with just trying to connect to a cluster the very first time! The mysteries of the ~/.kube/ directory arise swiftly from the depths and bottom out the boat on your Kubernetes journey before you’ve even begun. Fortunately, there are a lot of ways you can avoid editing or creating the configuration files with a few steps that were enlightening to me; hopefully they will be useful for you.</p><p id="""">I like to keep my configuration files separated and specify them explicitly. This prevents me from, say, deploying or sending commands to a production environment by accident. I also tend to have a few pre-production or even developer environments laying about and I want to choose which one I interact with each time. I also don’t want to overwrite any important credentials I may have stored in a default location so I like to keep all my files separated away from the default file names if possible.</p><h3 id="""">Prerequisites</h3><p id="""">We use Amazon Web Services (AWS) managed Kubernetes service called EKS and so my configuration setup is pretty AWS-centric, but by no means unusual. I also run a local Ubuntu 20.20 instance on Windows 10, so even though I have Windows, I’m not a Powershell or Command Prompt user. This will be a Linux/AWS configuration example but it should be usable on a Macintosh, or with proper translation, a native Windows environment. Similarly, you can use the same approach for other cloud providers or on-premise clusters.</p><p id="""">You will need an AWS account, AWS credentials (preferably an Admin, but if your cluster is already created, then just a user), the AWS CLI, EKSCTL, Kubectl commands installed.</p><h4 id="""">Your AWS credentials</h4><p id="""">The first step is to set up your AWS credentials. You can setup default credentials just by typing</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yaml"">
$ aws configure
AWS Access Key ID [None]: accesskey
AWS Secret Access Key [None]: secretkey
Default region name [None]: us-west-2
Default output format [None]:

$ aws ec2 describe-instances
</code>
</pre></div><p id=""""></p><p id="""">This works well for defaults or if you’ve never setup AWS credentials on your computer before. However, I will always move these credentials into a profile that I can access only when needed. Edit your ~/.aws/credentials file with an editor and move your credentials from [default] to some other named profile, for example if you have a production and development account, your file might look like the following.</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yaml"">
[default]

[production]
aws_access_key_id = SOMETHING
aws_secret_access_key = SOMETHINGELSE

[development]
aws_access_key_id = SOMETHING
aws_secret_access_key = SOMETHINGELSE
</code>
</pre></div><p id="""">The great thing about this setup is you can choose which environment you want to deploy into, and you won’t accidentally deploy to production if you switch terminal windows or pick up where you left off after a break. On the downside, you will have to remember to always specify your profile in one of several ways, for example:</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yaml"">
$ AWS_PROFILE=production aws ec2 describe-instances
$ aws ec2 describe-instances --profile=production
</code>
</pre></div><p id="""">You may find that less than convenient, but I enjoy it. I even go so far as not specifying a default region, so that I have to specify both profile and region in my commands (but it prevents me from making a lot of mistakes I would otherwise make):</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yaml"">
$ AWS_DEFAULT_REGION=us-west-2 AWS_PROFILE=production aws ec2 describe-instances
$ aws ec2 describe-instances --profile=production --region=us-west-2
</code>
</pre></div><p id="""">And finally, in <a href=""https://registry.terraform.io/providers/hashicorp/aws/latest/docs#shared-credentials-file"" target=""_blank"" id="""">Terraform</a>, you can easily switch environments by using this existing setup and specifying an input variable for the provider profile.</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yaml"">
provider {
    region = var.aws_region
    profile = var.credentials_profile
}
</code>
</pre></div><h3 id="""">The EKS Cluster Configuration</h3><p id="""">Now, you need to connect to an EKS cluster by generating a file which is known as a kubeconfig. By default, the kubeconfig files will be merged or written into your ~/.kube/config file, or if you have a $KUBECONFIG variable set, into the first file in that list (more on the $KUBECONFIG variable later).</p><p id="""">Again, I break out in hives around anything to do with YAML files and merging multiple configurations into one default file sounds like an easy recipe for disaster or rolling out the wrong changes to production late at night. Ideally, I’d like to keep all my configurations separate and specify them when I need them. I also want to avoid editing files or updating labels in dense, hard to read YAML.</p><p id="""">If you have more than one EKS cluster, and perhaps in separate AWS accounts, I want to make sure I keep them straight. The first step is to create a cluster configuration file and save it to a specific file:</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yaml"">
$ AWS_PROFILE=production AWS_DEFAULT_REGION=us-west-2 \
aws eks update-kubeconfig --name=prodEKS --alias=production \
--kubeconfig=~/.kube/config-prod-us-west-2
</code>
</pre></div><p id="""">One of the great tools you should check out is EKSCTL which has a similar use case:</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yaml"">
$ eksctl utils write-kubeconfig --cluster=prodEKS \
--kubeconfig=~/.kube/config-prod-us-west-2 \
--set-kubeconfig-context --profile=production \
--region=us-west-2
</code>
</pre></div><p id="""">I also like to use the --auto-kubeconfig option instead of --kubeconfig because it will save the file in ~/.kube/clusters/&lt;clustername&gt; by default.&lt;/clustername&gt;</p><p id="""">Now, you can access your cluster by name, for example:</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yaml"">
$ AWS_PROFILE=prod kubectl get pods -A -o wide \
--kubeconfig=~/.kube/config-prod-us-west-2
</code>
</pre></div><p id="""">So it gets a bit hairy to keep listing the file to specify which cluster you want to connect with. There must be a better way to do this, and there is luckily a way to specify a <a href=""https://kubernetes.io/docs/concepts/configuration/organize-cluster-access-kubeconfig/#the-kubeconfig-environment-variable"" target=""_blank"" id="""">context and merge files</a> to get this to work.</p><p id="""">So let’s say that all your cluster configurations are stored in separate files (which I like) and they all have a convention of starting with ~/.kube/config-* or exist in a subdirectory like ~/.kube/clusters/*. Now you can create a KUBECONFIG colon-separated list of the files like so:</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-"">
FILES=(~/.kube/config-*); IFS=: eval 'export KUBECONFIG=""${FILES[*]}""'
</code>
</pre></div><p id="""">Add the above snippet to your ~/.bash_aliases (or whatever bashrc script you prefer) and then start a new shell and you’ll be able to select a cluster by context:</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-"">
$ exec bash -l # This just loads my exports if I have updated anything
$ AWS_PROFILE=production kubectl get pods -A -o wide --context=production
</code>
</pre></div><p id="""">So you will need to specify your AWS profile (to gain access credentials to your assumed role for EKS) and also specify the context in order to choose a cluster to connect to. But I find it fairly usable and keeps all my configuration files in separate locations while still being relatively easy to maintain and manage. I also remove the accidental possibility of accessing the incorrect cluster or environment and wreaking havoc.</p><h3 id="""">Conclusion</h3><p id="""">It is possible to keep YAML file configuration and management of EKS kubernetes clusters separate and yet accessible with some flags that switch into the correct environment. Adding and removing clusters and credentials is easily managed in the filesystem and without editing files directly. Also, defaults are removed so that a command does not get executed on the wrong cluster inadvertently.</p>",false,,,,https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63e41b06fc8cc07d3b595c0b_081620%20(1).jpg,Pile of trash representing bad Kubernetes Config Files,regis-wilson,4,Mon Aug 17 2020 00:00:00 GMT+0000 (Coordinated Universal Time),kubernetes,
How to Manage GitOps Secrets: A Detailed Guide,how-to-manage-gitops-secrets-a-detailed-guide,62aa5a70cd5ba27d9d0d718a,63053b865759c60b77567d23,Tue Aug 23 2022 20:41:42 GMT+0000 (Coordinated Universal Time),Fri Apr 21 2023 18:06:16 GMT+0000 (Coordinated Universal Time),Fri Apr 21 2023 18:22:03 GMT+0000 (Coordinated Universal Time),GitOps practices promote storing all your configs in git repositories. Learn how to store your secrets in plain text.,"<p id="""">GitOps is becoming increasingly popular. More and more companies have started using Git as the source of truth for their infrastructure and application configuration. However, with its advantages comes also challenges. For example, if all your configuration is stored in Git, how do you manage secrets? You can't simply commit your passwords and tokens in clear text to the Git repository even if that repository is private and only a few people have access to it. In this post, you'll learn how to manage GitOps secrets securely. Stay tuned.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1000px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1000px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/62fd34a69197bca92b6a0ba9_8ViF9cXWG_AkaLtbiQLh1nZ5VsG_SUHEI5XTcpAMoB6wJONqzBwVsDBaAAbo-wzXmVcHrd_EOYUo-xA-LYBAVJKQUPzyDxNwWkxkgIKNNREWJAcVsE0M36e202ZEotxJhD5jGifhxb_qnCVAjkVROTw.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><h3 id="""">GitOps vs Secrets</h3><p id="""">If you've never used GitOps before, here's a short introduction for you. GitOps is a way of managing infrastructure and application configuration purely via Git repositories in a declarative manner. Here's how it works: You store all the configuration in Git, and then you have a GitOps tool installed somewhere that constantly monitors changes to that Git repository and applies infrastructure and application changes once it detects that something changed in the repository. The whole point of GitOps is that you have one, centralized, single point of truth for all your infrastructure and application configuration. GitOps is most commonly used with Kubernetes.</p><p id="""">But as mentioned in the beginning of this post, there are some challenges when using GitOps. And the biggest one is secret management. There will be many secrets that your infrastructure will require. Your application configuration is probably full of secrets too. And it ought to go without saying that storing secrets in the Git repository in plain text is a security vulnerability. That's true even if that repository is private. You need a different solution for that, but ideally something that still works in a GitOps manner. This means it would be great not to have a separate process to define secrets. I'll show you how it can be done.</p><h3 id="""">Secrets the GitOps Way</h3><p id="""">There are two popular ways of solving this problem. They work quite differently, but both achieve the same outcome: the ability to store secrets or their references in a Git repository. Which one you choose will depend on your company's needs. Let's discuss both of them.</p><h4 id="""">SealedSecrets</h4><p id="""">We already established that you can't store secrets in plain text in a Git repository. But how about storing them in a non-plain-text version? That's precisely what the<a href=""https://github.com/bitnami-labs/sealed-secrets"" target=""_blank""> SealedSecrets</a> tool does. It allows you to encrypt your secrets and only store their encrypted version in your Git repository. Simple as that.</p><p id="""">How does SealedSecrets work, you ask? You install a SealedSecrets controller on your Kubernetes cluster and the <strong id="""">kubeseal</strong> binary on your local machine. SealedSecrets will generate private and public keys for encrypting the secrets. Before committing a secret to a Git repository, you'll use <strong id="""">kubeseal</strong> binary to encrypt it. Then, in an encrypted form, it's totally safe to store it in a repository, and only the SealedSecrets controller running in your Kubernetes cluster will be able to decrypt it. Quite smart, if you ask me.</p><h4 id="""">How to Use SealedSecrets</h4><p id="""">First, follow the installation instructions for SealedSecrets<a href=""https://github.com/bitnami-labs/sealed-secrets#installation"" target=""_blank""> here</a>. Once you have it up and running, you can try to seal your first secret with kubeseal. Let's create a simple Kubernetes secret definition YAML file and use kubeseal to seal it.</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yaml"">
apiVersion: v1
kind: Secret
metadata:  
  name: example-secret
type: Opaque
data:
  username: my-username
  password: super-secret-password
</code>
</pre></div><p id="""">Once you have the file, you can pipe its content to the kubeseal binary.</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yaml"">
cat secret.yaml| kubeseal --controller-name=sealed-secrets-controller --format yaml > sealed-secret.yaml
</code>
</pre></div><p id="""">And if you now take a look at the created sealed-secret.yaml file, you'll see that the actual username and password values are encrypted.</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yaml"">
$ cat sealed-secret.yaml
apiVersion: bitnami.com/v1alpha1
kind: SealedSecret
metadata:
  creationTimestamp: null
  name: example-secret
  namespace: default
spec:
  encryptedData:
    password: AgC7jlVk(...)eb+XOk5/99fKHk=
    username: AgAHbCU7(...)hIgv5D6LDYopF4n
  template:
    data: null
    metadata:
      creationTimestamp: null
      name: example-secret
      namespace: default
    type: Opaque
</code>
</pre></div><p id="""">This file is now safe to be stored in a Git repository since only a SealedSecrets controller that was used to encrypt this file will be able to decrypt it.</p><p id="""">But how do you consume that secret in your cluster? It's very straightforward. You can directly apply that sealed file to your cluster, and the SealedSecrets controller running on it will automatically unseal it and create a standard Kubernetes secret resource from it. Let's take a look.</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yaml"">
$ kubectl apply -f sealed-secret.yaml
sealedsecret.bitnami.com/example-secret created

$ kubectl get secret
NAME                                 TYPE                 DATA   AGE
example-secret                       Opaque               2      7s
</code>
</pre></div><p id="""">From now on, you can use <strong id="""">example-secret</strong> just like any standard Kubernetes secret.</p><h4 id="""">ExternalSecrets</h4><p id="""">Another way to store secrets for your GitOps needs is using<a href=""https://external-secrets.io/"" target=""_blank""> ExternalSecrets</a>. It works differently than SealedSecrets but also solves the problem of storing plain text secrets in a Git repository. ExternalSecrets does this by removing the need to store the actual secret in your repository. Instead, your secret can be safely stored in a secret vault, and you only need to store a reference to a secret in your repository.</p><p id="""">So instead of having, for example, the actual username and password in a file in your Git, you'll instead have a file that says something like ""this username is password is stored in that secret vault, under this key."" And then it's the external secret operator's job to go and fetch the actual value for you when you need it. Let's try that.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1000px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1000px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/62fd34a6cba21f639a0e105a_8kPN_1hOmgzKxt_7UXY7WfSjsjIGZoHJujyXJBQm1hfeatiewdP2M3UeN2WMomzubazLt1a8xqPUXCysfGY3yrkXVcrUc6JSxeY5D8-5SsHqZpMnCMU7aCjvnrMDB3cxG8EzEAGuGsZbi4Ho8CgroyU.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><h4 id="""">Using ESO</h4><p id="""">The external secrets operator can be installed just like any other tool using Helm. You can follow the installation and initial configuration steps<a href=""https://external-secrets.io/v0.5.8/guides-getting-started/"" target=""_blank""> here</a>. Once you have ExternalSecrets up and running, using it is quite simple. You first need to add your secrets to the secret vault that you want to use and then create an ExternalSecrets reference file. This file will be a replacement for your typical Kubernetes secret definition file.</p><p id="""">As explained before, working with ESO means referencing the actual secrets from an external secret vault. So, you create an external secret resource, and the external secret operator will fetch the actual secret from an external vault in the background and create an actual Kubernetes secret for you. Here's an example:</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yaml"">
apiVersion: external-secrets.io/v1beta1
kind: ExternalSecret
metadata:
  name: database-externalsecret
spec:
  refreshInterval: 3h
  secretStoreRef:
    name: azure-keyvault
    kind: SecretStore
  target:
    name: database-secret
    creationPolicy: Owner
  data:
  - secretKey: database-secret-dev
    remoteRef:
      key: database-secret-dev
</code>
</pre></div><p id="""">That is an ExternalSecrets definition file that tells ESO to fetch the value of the <strong id="""">database-secret-dev</strong> key from the Azure Key Vault and create a Kubernetes secret called <strong id="""">database-secret</strong> from it. As you can see, we don't have actual secret values in this file, so storing it in a Git repository is perfectly fine.</p><p id="""">It's the same when it comes to consuming secrets. You simply apply that ExternalSecrets definition file to your cluster, and the ESO operator will automatically fetch the secret from the defined secret vault and create an actual Kubernetes secret from it.</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yaml"">
$ kubectl apply -f external-secret.yaml
externalsecret.external-secrets.io/database-externalsecret created

$ kubectl get secret
NAME                                 TYPE                 DATA   AGE
example-secret                       Opaque               2      12m
database-secret                      Opaque               2      4s
</code>
</pre></div><h3 id="""">Summary</h3><p id="""">As you can see, the GitOps secrets problem can be solved. It's not even that difficult. However, it does include some extra steps and tools. But once the initial setup is done, it doesn't take much more daily effort to manage secrets securely in your GitOps practices.</p><p id="""">If you want to learn more about secrets or GitOps, you can find more content on<a href=""https://release.com/blog""> our blog</a>.</p>",false,,,,https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63e41c1a34aa769447f96ca0_082322%20(1).jpg,a small black toy,ashley-penney,6,Tue Aug 23 2022 20:40:00 GMT+0000 (Coordinated Universal Time),,
How to Self-Host PostHog Using Release Part One,how-to-self-host-posthog-using-release-part-one,62aa5a70cd5ba27d9d0d718a,63ec1e4ffe80080c26cff7f3,Tue Feb 14 2023 23:50:39 GMT+0000 (Coordinated Universal Time),Thu Apr 20 2023 20:56:31 GMT+0000 (Coordinated Universal Time),Thu Apr 20 2023 21:44:19 GMT+0000 (Coordinated Universal Time),Learn how to run a hobby version of PostHog on your own cloud infrastructure using Release,"<p id="""">This first part of a series explains how to run a hobby version of <a href=""https://github.com/PostHog/posthog"" target=""_blank"" id="""">PostHog</a> on your own cloud infrastructure using Release. Check back later to read how to perform the self-hosted enterprise version. You can read the <a href=""https://posthog.com/faq"" target=""_blank"" id="""">PostHog FAQ</a> for more details on the software and self-hosting options for hobby and enterprise options.</p><h3 id="""">Introduction</h3><p id="""">PostHog is an open source tool for collecting and analyzing behavior metrics and events from your application without sending data to a third party hosting provider. As the GitHub repository says, “<em id="""">…third-party analytics tools do not work in a world of cookie deprecation, GDPR, HIPAA, CCPA, and many other four-letter acronyms. PostHog is the alternative to sending all of your customers' personal information and usage data to third-parties.</em>”</p><p id="""">This goes together with the Release philosophy of deploying high-quality, full-stack environments in your own cloud accounts. Deploying PostHog next to your development environments for complete testing and development, or as a shared staging location for integration testing is a valuable component for developing and testing user interactions and metrics collection that would be much harder to support without Release.</p><p id="""">This article will walk you through the simple steps for configuring a PostHog application template that you can deploy to your own cloud infrastructure using Release in about 30 minutes or so. The hobby version is a full functionality installation of PostHog but will not be configured with redundant services and long-term storage and backup solutions like the enterprise version would. The most common use-case for wanting to install the hobby version is to support developer environments for testing in isolation or for QA environments for complete end-to-end testing of product analytics.</p><p id="""">We will cover the enterprise version for permanent installations (like staging or production) in a followup post. The most common use-case to install the enterprise version is to self-host and scale your own analytics engine and data from your product customers <em id="""">without sending the data outside your organization or to a third-party Software-as-a-Service (SAAS)</em>. Release allows you to self-host applications in your own cloud environments keeping your and your customers’ data safe and secure.</p><h3 id="""">Prerequisites</h3><p id="""">To get started, open a <a href=""https://app.release.com/"">Release account</a> which allows you to have unlimited applications and pay only for the environments you are running. You can test out Release by creating a trial account in a shared environment; or if you already have a paid plan, you can use your Release account to host these environments in your own cloud infrastructure next to your existing applications. See our <a href=""https://docs.release.com/getting-started/quickstart"" target=""_blank"">quickstart documentation</a> for more details.</p><p id="""">Next, fork the <a href=""https://github.com/PostHog/posthog"" target=""_blank"">PostHog GitHub repo</a> using either <a href=""https://github.com/releasehub-samples/posthog"" target=""_blank"">our fork with the configuration options already built in</a>, or the original upstream repository. You can also use one of our integrations with GitLab or BitBucket but you will need to clone and push the repository separately before you get started.</p><h3 id="""">Configure the PostHog Application</h3><p id="""">The first step is to import the application into Release by analyzing the repository and reading in the docker-compose and package.json files to get a running version of the repository in your account. If you are using the fork from our repository, then you will have a head start by using our configuration YAML already integrated into the repository. These instructions will work for the plain upstream version and they show how the process works.</p><p id="""">First, click the “Create New App” button and select your forked repository (note: if the repository does not show up, go to your profile page and make sure you have configured the correct permissions and scope for the version control system integration you are using). Give the application a name and go to the next step.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1265px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1265px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63ec1a2388c4ab7490423c2d_lNotnqjBbxbSwqbPiTKTz1Bgpk3jlsXO5xCjwmpR6m-S6V2U6E0ro8briW96L8thxCMAZd64lG4wkabVyczdPeYMn7Jgkb6fUsnvIah0x6OTXDPZ6qBtn_EiuOqX5Q92oP_QYEYRGZXBsttyv073os4.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><p id="""">Next, analyze the repository using a branch and the common hint files we search for to configure this application. Select at least one docker-compose file and then select the services to your preference as shown in the following image. The hobby version will not include any helm charts or cloud native service integrations, but this is a good example and low-cost way to test out the functionality of the application.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63ec1a24472f8f5870e71583_WP4X5N7etjaFbDcHWRmiqGleu9UOK6swhTNvzW52gegL_SVpmagzbV7q3n84ybQfOyBidi2wWvWnOEzs40Hr9fvpqJOThUYxyAzd8N1Uq6-ssWNWoVuOifaZhRKRSwsRcVWqGimDboE_R0FGS9UsqAQ.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><p id="""">When you’ve selected the services to import and analyze, you will fine tune the application template so that it has the specific customizations and workflows you would like to use for testing purposes. You can start by editing the hostnames for each service (you will only need a service hostname for <strong id="""">web</strong>, <strong id="""">maildev</strong>, and <strong id="""">clickhouse</strong> for example). You can also customize the domain you will deploy the test application URLs to.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1056px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1056px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63ec1a23bde671f2c3f87fe9_KK7clB2H75Y7Ux0K7Y4GrLnqhKRbFxUcDacympThC3lapczr1Vx6YDUgSoRZYs6-VZT-EopU5UCDOLNxGjgsbMySK3tjViqqd683n0gyCRdbDzEff-nLI8Z_vvbU0N7Z8iqIKtSE0iSRl4H4d8dyg6E.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><p id="""">You should pay close attention to the workflow steps as the order of the services is important. Reading up on the architecture diagram and understanding the <strong id="""">depends_on</strong> fields in the docker compose will help you understand the correct workflow as well. This is all organized for you in the <a href=""https://github.com/releasehub-samples/posthog"" target=""_blank"" id="""">forked version we host and maintain</a>.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:474px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""474px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63ec1a237f0a0d5b0a422e64_ERlaImWGbgdrbqKw5jO2X25iFfmt55Upu-ZVeZb2e7aW4r2N2dZQavKu1SiDIRjESQj-p4XWPyDo7u3KmaMA6UZ-XlODZ_SQGcg1qjkjbwvYg8RY9DtZB7wQJst5Dp5leJ5zIGzEeM8KmlyUxjgMLOA.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><p id="""">Once you are happy with the application template and service definitions, you can proceed with fine-tuning the environment variables for the application. Here, the <a href=""https://posthog.com/docs/self-host/configure/environment-variables"" target=""_blank"" id="""">PostHog documentation</a> is excellent in helping you compare the defaults and necessary customization you will need side-by-side.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1404px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1404px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63ec1a247fa9a916ffd7c57b_VIembDSOPWSnSCz47vmplGvzXUTVrBjU0I80vny-zU688o5J25BWgMevII0_TUkpbULc_KDyjuHhf_O0hRRkDKUPuena73dYgxxdfGp22DnkBDxY1UaWW0kyLryTV0ZRJioLxoJcaTQ2v6I9PFqnyoI.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><p id="""">You can see that I’ve set some environment variable mappings which allow interpolation of Release-injected variables so that the application URLs and internal configuration items work correctly for each environment that will be created. Since we are using a hobby (or ephemeral) version of this application, we are not going to hardcode values or secrets as we might do in a permanent or production environment.</p><p id="""">The next steps involve creating build arguments. We always recommend using the <strong id="""">production</strong> value for <strong id="""">NODE_ENV </strong>since Release ephemeral environments run in production mode, rather than development mode (except when using our <a href=""https://docs.release.com/cli/remote-dev"" id="""">Remote Development</a> feature).</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1229px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1229px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63ec1a237fa9a9cf7ad7c57a_MtlcUd51sKy_BwLo6XWHYXpg3ulBz3jk0-d0oWvC3X4q96WUxGxl3yiFiSr3P1816twItVY2StGTT7CJNyEti0xAhNPXYuzCBC8dWeonJ9bm_rY_YY1r2zC4dE4UFg_fgf0bgg-kPJfP4PtDfIDFSSM.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><p id="""">When those steps are done, you can go back to review all your work and confirm it looks good or you can immediately deploy your environment and see the results of your hard work! The application template and environment variables will be deployed into your cloud environment in a completely isolated environment that you can test and play with. When you are done with testing the environment, it can be expired, removed, or you can redeploy it with any changes and tweaks you need to make until you are satisfied with the results.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63ec1a249fa56f42692fdaef_9hvS-JZ9ce0X-YK7BGZjATxHypwQOH5tcezh5o6L4sKoFomPB4BNuyXB86-mGeP6wliHU22_su5I-qa30jrRpTUeqe68cTTSu8HxU-P8luGiDK15VmCo26fiZl84W4IEC5I7lDV6olGcBfGGVrIjyBI.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><h3 id="""">Deploy the Application as an Environment</h3><p id="""">You should see your deploy starting up and an ephemeral environment is immediately spun up to begin playing with!</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1229px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1229px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63f68d9e54bf4e7e4eea6a5d_Screenshot%202023-02-22%20134759.jpg"" loading=""lazy"" id="""" width=""auto"" height=""auto""></div></figure><p id="""">‍<br>Once&nbsp; complete, you can visit the environment status page which shows you a list of clickable URL links for your application (we recommend editing those down to services that you actually need, for example, <strong id="""">redis</strong> and <strong id="""">postgres</strong> are not going to be reachable on the internet and you should remove those from your configuration).</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1209px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1209px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63f68dd3c2d24d31d3e61b54_Screenshot%202023-02-22%20134858.jpg"" loading=""lazy"" id="""" width=""auto"" height=""auto""></div></figure><h3 id="""">Debug and Test the Environment</h3><p id="""">You can inspect the services and statuses of each underlying instance that is running in your kubernetes namespace. This gives you the up-to-date picture of what is started and what is running properly, what needs to be investigated for errors, logs, and so-forth. Alternatively, you can jump into a terminal session on a service for immediate debugging and testing.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1023px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1023px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63f68e000731b4539666592a_Screenshot%202023-02-22%20134936.jpg"" loading=""lazy"" id="""" width=""auto"" height=""auto""></div></figure><p id="""">Clicking on each service link, you can see Clickhouse works</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:105px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""105px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63ec1a2459cee68e6ee9f343_cEuLql7LRmwl-j1lmWK87GRT2W8hubgH8PNzddUVAZC89xDzUJsMIzlyM35L6hKkt5wfKjdOR-504Uy1qH2uvOhZOidlv78P6uPg1Cu3hFmLucUShlZojfaSyBQCWksnsCOrC_UnVmBVuGSu3srNbws.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><p id="""">And the same with Maildev:</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1136px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1136px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63ec1a2488c4ab3673423c2f_x63Cri-9L7D_LRsZ5psALAdvmyKkMxfd7nPVA9ieHtYiQrv_SHQv0ZnZbQUysOn4hd-WQ67nTNVQ4EjtF1ZzmVk08jZOGiIXAEU3r14-6tTwVPSnB6_8C0CCvP5tIFFDXDff_xrggfJr2W4laNRmObI.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><h3 id="""">Login and Configure the Application</h3><p id="""">But the most important thing to look at is the PostHog application itself. We will click on “Just experimenting” for now.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:636px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""636px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63ec1a242f502cd58748d36e_TAE4TSv4Z5cWXKt6dO_X1ROMOMKyVCYGma3j6I9f7EkydNpf-4Sudu-CcEHcf5gWHcFnGV7hinNKJxTwd10T9632Gmb8E_vW6CqZGzST5q7mbqjx4Mq2vgPJdLN6QwFs8Zs0K_fIyf97dvc3ufdyJxY.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><p id="""">We can verify that all the services and prerequisites are running:</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:535px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""535px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63ec1a246f148f931b1e4f92_sI6YM1MNr5tst8BGYxROCX8W_vKsRcaLyOb-zZaCn-HS8mELl40cP_GIjtjudIUi3MFEcaP3EOG9HEZgG0O4Ti5C7MeBeGQw7fZ6ig81KkSwkICHpT9Q1M8xXxqsBu2NoxzRCOpT7xdmQKJYpvDBpZU.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><p id="""">Then get started by creating an account:</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:964px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""964px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63ec1a24472f8f2be3e71588_agSPPcs5-5eal6YxUb8znTY6FcffHIPVoqWmKb4Bhb0HUl7O3llmaV-K2GA0RVHasM7-zZN_4buxM84s3KJ-06nesGt_71elUzxdGNd3ZVV1aKlZigMfXGPfRl9cQRjtuD3kSfUpdgGlUnzvzrzEEE4.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><p id="""">And finally you can start configuring your application and settings to use in your new isolated hobby experience:</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1184px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1184px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63ec1a2459cee63793e9f344_DUsRl2AF3WFg918Y7yKa6X6WL2kTJSC96JZtlheOdOnxqVFxWn3fxWxBfktjWCaRlm7OCL1uUrsQAEHOsouu5XKghbpf9plxZLnEP_eXzAfvTmGpy36zyiXCHQMVp0oD3ntZ-yWB-ZfeRENVC_O1jFI.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><h3 id="""">Conclusion</h3><p id="""">Using Release to deploy a hobby version of PostHog allows you to run an isolated version of analytics and metrics collection during the development and testing of your own application code. PostHog can be deployed alongside every application deployment you use for each developer or for each branch/feature/pull request that needs to integrate with PostHog, depending on your workflow and requirements.</p><p id="""">With Release, it is easy to spin up environments for almost any conceivable use case and this empowers developer teams to deploy applications without relying on a devops or an infrastructure team and minimizing costs associated with provisioning and maintaining the application. You can deploy dependencies or third-party tools that need to be tested or verified when performing application changes or when rolling out new features. Verifying this functionality with third party tools and services is a valuable way to ensure that features reaching staging or production will work even during the earliest phases of development.</p><p id="""">Ready to get started? Open a<a href=""https://app.release.com/"" target=""_blank"" id=""""> Release account</a> and start your free trial today or check out our<a href=""https://docs.release.com/getting-started/quickstart"" target=""_blank"" id=""""> quickstart documentation</a> for additional information.</p>",false,,,,https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/64405b20fafd193e648ed8d7_122422%20(1).jpg,Image of two women sitting in front of a computer,regis-wilson,7,Wed Feb 15 2023 17:42:00 GMT+0000 (Coordinated Universal Time),,
How to set Docker Compose Environment Variables,how-to-set-docker-compose-environment-variables,62aa5a70cd5ba27d9d0d718a,62f50e4e87ee5b732e34be5d,Thu Aug 11 2022 14:12:30 GMT+0000 (Coordinated Universal Time),Wed Oct 30 2024 19:55:42 GMT+0000 (Coordinated Universal Time),Wed Oct 30 2024 19:55:42 GMT+0000 (Coordinated Universal Time),"How to define environment variables directly in a Docker Compose file, or copy tnem from the host's environment.",,true,"<p id="""">Streamline Docker Compose environments and manage secrets effortlessly with Release.</p>",https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=docker-compose-envs,"<p id="""">Docker Compose lets you run multi-container Docker applications. If you want to use Docker Compose with environment variables, you have several options. The <a href=""https://docs.docker.com/compose/environment-variables/"" target=""_blank"" id="""">official documentation</a> is a good reference, but it's hard to understand and very abstract!</p><p id="""">Let's take a practical look at how to pass environment variables to your Docker containers using Docker Compose.</p><p id="""">We'll use a simplified version of the <a href=""https://docs.docker.com/compose/gettingstarted/"" target=""_blank"" id="""">official getting started</a> Docker Compose app, which is a basic web application built with Python and Flask, but you'll be able to use the same method to access environment variables using any other programming language.</p><h3 id="""">Setting up the example project</h3><p id="""">If you want to follow along, clone our <a href=""https://github.com/ritza-co/docker-compose-environment-variables-demo"" target=""_blank"" id="""">Example Docker Compose Project</a>. You'll also need Docker and Docker Compose installed on your machine.</p><p id="""">We'll assume that you want to pass an environment variable called <em id="""">SECRET_ENV</em> to your application.</p><p id="""">The project consists of:</p><ul id=""""><li id="""">An <em id="""">app.py</em> file that returns a string containing the <em id="""">SECRET_ENV</em> value.</li><li id="""">A Dockerfile that starts up the Flask app.</li><li id="""">A Docker Compose that defines the variable and starts the Dockerfile.</li></ul><p id="""">A <em id="""">requirements.txt</em> file that installs the Flask framework.</p><h3 id="""">Defining the environment variable in the Docker Compose file</h3><p id="""">The easiest way to send an environment variable to your Docker app is by defining it using an <em id="""">environment:</em> section in your <em id="""">docker-compose.yml</em> file as in the example below:</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yml"">
version: ""3.9""
services:
  web:
    environment:
      - SECRET_ENV=my_secret_defined_in_docker_compose_yml
    build: .
    ports:
      - ""8000:5000""
  redis:
    image: ""redis:alpine""
</code>
</pre></div><p id="""">If you do this, run <em id="""">docker compose build &amp;&amp; docker compose up</em>, and visit <a href=""http://localhost:8000/"" target=""_blank"" id="""">http://localhost:8000</a> in your web browser. You'll see the variable displayed.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1482px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1482px""><div id=""""><img src=""https://cdn.prod.website-files.com/603dd147c5b0a4221d1bd360/67195462582e7259ab067aab_62f50e25577b6e912b67d0d7_environment-variable-docker-compose-yml.png"" loading=""lazy"" id="""" width=""auto"" height=""auto"" alt=""""></div></figure><p id="""">This is easy, but not very useful. If you don't want to have the variable in your application code, you probably don't want it in your <em id="""">docker-compose.yml</em> file either.</p><h3 id="""">Passing an environment variable from the host</h3><p id="""">Press Ctrl + C twice to stop the Docker server and edit the <em id="""">docker-compose.yml</em> file to look as follows:</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yml"">
version: ""3.9""
services:
  web:
    environment:
      - SECRET_ENV=${HOST_SECRET}
    build: .
    ports:
      - ""8000:5000""
  redis:
    image: ""redis:alpine""
</code>
</pre></div><p id="""">Save the file and run <em id="""">export HOST_SECRET=my_secret_from_host</em> in your shell. This sets the environment variable on your host machine, which will be read by the <em id="""">docker-compose.yml</em> file after you rebuild with Compose.</p><p id="""">Now run <em id="""">docker compose build &amp;&amp; docker compose up</em> again.</p><p id="""">Visit <a href=""http://localhost:8000/"" id="""">http://localhost:8000</a> in your browser and you'll see that the app now displays the new secret variable.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1476px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1476px""><div id=""""><img src=""https://cdn.prod.website-files.com/603dd147c5b0a4221d1bd360/67195462582e7259ab067aa4_62f50e1bb9b83105993abe1c_environment-variable-host.png"" loading=""lazy"" id="""" width=""auto"" height=""auto"" alt=""""></div></figure><h3 id="""">Passing an environment variable from a <em id="""">.env</em> file</h3><p id="""">If you have a lot of environment variables or you want to be sure you don't lose them, you can define them in a file called <em id="""">.env</em> in the same directory as your <em id="""">docker-compose.yml</em> file.</p><p id="""">Quit your shell to clear the <em id="""">HOST_SECRET</em> variable that you set before and open a new shell. Create a file called <em id="""">.env</em> containing the following:</p><p id="""">HOST_SECRET=my_secret_from_dot_env_file</p><p id="""">Save the file and run <em id="""">docker compose build &amp;&amp; docker compose up</em> again.</p><p id="""">Now you'll see the variable as you defined it in your <em id="""">.env</em> file. Usually, you would add this file to <em id="""">.gitignore</em> and recreate it on each machine you run the code on, which means that your (sensitive) environment variables are not stored with your codebase.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1470px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1470px""><div id=""""><img src=""https://cdn.prod.website-files.com/603dd147c5b0a4221d1bd360/67195462582e7259ab067aa7_62f50e40910e076083512d7f_environment-variable-dot-env.png"" loading=""lazy"" id="""" width=""auto"" height=""auto"" alt=""""></div></figure><h3 id="""">Understanding the priority of environment variables</h3><p id="""">If you define the same variable in a <em id="""">.env</em> file and directly in your environment (for example, using <em id="""">export HOST_SECRET=my_secret_from_host</em>), Docker Compose will give priority to your environment. The variable in your <em id="""">.env</em> file will be ignored.</p><p id="""">If you want to check exactly what is being used by your <em id="""">docker-compose.yml</em> file without building the whole project, you can run:</p><p id=""""><em id="""">docker compose convert</em></p><p id="""">This will output the model it uses, based on your <em id="""">docker-compose.yml</em> file but with any environment variables replaced with their real values. If you run it now, you'll see it still picks up the value from the <em id="""">.env</em> file.</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yml"">
...
    environment:
      SECRET_ENV: my_secret_from_dot_env_file 
...
</code>
</pre></div><p id="""">If you run <em id="""">export HOST_SECRET=my_secret_from_host</em> and <em id="""">docker compose convert</em> again, you'll see it prefers the variable you set explicitly over the one in the <em id="""">.env</em> file.</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yml"">
...
    environment:
      SECRET_ENV: my_secret_from_host
...

</code>
</pre></div><h3 id="""">The best way to handle environment variables in your Docker Compose projects</h3><p id="""">To recap, you can:</p><ul id=""""><li id="""">Hard code your variables directly in your <em id="""">docker-compose.yml</em>.</li><li id="""">Pull the variables from the host environment.</li><li id="""">Define the variables in a <em id="""">.env</em> file.</li></ul><p id="""">For non-sensitive variables that don't change very often, it's easiest to simply put the values directly in the <em id="""">docker-compose.yml</em> file. This means you don't have to spend extra time and effort tracking down the values and messing around with multiple files.</p><p id="""">For sensitive values, such as database passwords and API tokens, you should ideally only define these directly in secure environments (for example, your production server). However, it's inconvenient to lose all of these values every time you need to restart your server or change to a new machine.</p><p id="""">Therefore a good tradeoff between security and convenience is to use a <em id="""">.env</em> file containing your sensitive environment variables. It's important to keep any copies of this file in a secure place, such as a secrets manager or vault, and to not check in this file as part of your code base.</p><h3 id="""">Managing environment variables and secrets with ReleaseHub</h3><p id="""">If you're looking for a simple and powerful platform to manage all of your environments for you, take a look at <a href=""https://release.com/"" id="""">Release</a>. Using our custom application template file, you can easily set environment variables and map them to specific environments, ensuring that your secret management is both secure and convenient.</p><p>‍</p>",https://cdn.prod.website-files.com/603dd147c5b0a4221d1bd360/63e41b6e42aa63cab73cc58e_081822-1%20(1).jpg,Cog wheels,regis-wilson,4,Thu Aug 18 2022 14:20:00 GMT+0000 (Coordinated Universal Time),docker,
How to Solve AWS EFS “Operation Not Permitted” Errors in EKS,how-to-solve-aws-efs-operation-not-permitted-errors-in-eks,62aa5a70cd5ba27d9d0d718a,62d326cad51c79c83f29270b,Sat Jul 16 2022 20:59:54 GMT+0000 (Coordinated Universal Time),Fri Apr 21 2023 19:08:01 GMT+0000 (Coordinated Universal Time),Fri Apr 21 2023 19:37:52 GMT+0000 (Coordinated Universal Time),Learn how we have helped our customers maintain strong deployment without overloading the workload and application stack,"<p id="""">At ReleaseHub, we operate dozens of Amazon Elastic Kubernetes Service (EKS) clusters on behalf of our customers. The various workloads and application stacks we have to support are practically as diverse as the number of engineers who use our product. One very common use case is a permanent storage space for the workloads that are deployed in each environment.</p><p id="""">The most common general solution for storage in AWS for compute workloads is the Elastic Block Service (EBS), which has the advantage of being relatively performant and easy to set up. However, it has the drawback that EBS volumes are tied to a specific Availability Zone (AZ). Therefore, using Kubernetes workloads running in multiple Availability Zones (AZs), it turns out that ensuring pod workloads correctly map to the correct AZ is actually difficult to do properly and has caused numerous issues for our customers who use EBS storage in their clusters. We also discovered that EBS storage costs can add up quickly and over-provisioning volume sizes (which is a necessary evil) can add to this problem.</p><p id="""">Without going too far down the pros and cons of each storage system, we found that most customers were well satisfied with using Elastic FileSystem (EFS) mount points to provide persistent storage volumes backing the application workloads deployed to their clusters. EFS provides a good balance of performance, reliability, price (pay-for-what-you-store), and AZ diversification. As such, we made an early decision to move almost all customer workloads off EBS to EFS and only allowed the EBS option for customer workloads who specifically opt-in to it. This solution worked well for us since EKS version 1.14 all the way up until recently when we started moving customers to 1.21 and beyond.</p><h3 id="""">The Problem</h3><p id="""">In our original implementation of EFS workloads in EKS, we started out using the (now retired) <a href=""https://github.com/kubernetes-retired/external-storage/tree/master/aws/efs"" target=""_blank"">EFS provisioner</a>. This solution allowed our customers to specify a volume for persistent storage and the provisioner would create a filesystem mount from an existing EFS infrastructure point (which we create automatically upon cluster creation). The customer pods would then mount this filesystem and have unlimited storage that would persist until the workload expired or was deleted, at which point the volume space would be removed. We literally experienced zero issues with this configuration from the first time we tested it.</p><p id="""">In recent months, we have been tirelessly upgrading to the latest version(s) of EKS to keep customers up to date with the latest features and deprecations in the never ending Kubernetes versions. Upon reviewing the various addons and plugins, we realised that the EFS provisioner was replaced by the modern <a href=""https://github.com/kubernetes-sigs/aws-efs-csi-driver"" target=""_blank"">EFS CSI driver</a>. You can read more about the two projects in <a href=""https://github.com/kubernetes-sigs/aws-efs-csi-driver"" target=""_blank"">this stack overflow article</a>.</p><p id="""">The upgrade process was not terribly difficult for us since we could easily run both provisioners side by side and then switch over workloads using the <a href=""https://kubernetes.io/docs/concepts/storage/storage-classes/"" target=""_blank"">Kubernetes Storage Class</a> objects. As one example, Customer A would be using the legacy <em id="""">provisioner: releasehub.com/aws-efs</em> storage class and then we could upgrade any subsequent workloads to <em id="""">provisioner: efs.csi.aws.com</em> and then test until we were satisfied with the results. Rolling back was easy to revert the workloads back to the original storage class.</p><p id="""">Eventually, after demonstrating that the process worked seamlessly and nearly flawlessly with the new driver and the same infrastructure in a variety of scenarios, we were able to confidently roll out the changes to more and more customers in a planned migration.</p><p id="""">That was when we ran into two major stumbling blocks with customer workloads that use persistent volumes: postgres and rabbitmq containers. Here are the horrible details we discovered for each:</p><p id=""""><em id="""">initdb: could not change permissions of directory ""/var/lib/postgresql/data/pgdata"": Operation not permitted</em></p><p id="""">‍<em id="""">chown: /var/lib/rabbitmq: Operation not permitted</em></p><p id="""">It is important to note that this could happen to any workloads that use the chown command, but these were the most common complaints we got from customers.</p><h3 id="""">Diagnosis</h3><p id="""">At first, we did what every engineer does: we searched Google and confirmed the problems were widespread, finding stack overflow and server fault questions <a href=""https://stackoverflow.com/questions/51801220/postgres-on-kubernetes-volume-permission-error"">here</a> and <a href=""https://serverfault.com/questions/993907/rabbitmq-kubernetes-with-nfs-mount"" id="""">here</a> respectively. Unfortunately, and most frustratingly, there were no good solutions to the problem(s) and even worse, many of the solutions posited by people were highly complex, tightly tied to a particular implementation, or technically brittle. There seemed to be no particularly elegant, easy solution especially for our wide diversity of customer user cases.</p><p id="""">We tried using the latest versions of the drivers to no avail. We tried even older versions of the CSI driver to see if this might have been a regression (to no avail). Digging in even deeper to EKS and EFS specifically, we discovered that dynamic provisioning (which is what we rely on to provide a seamless, fast, efficient service for workloads) was <a href=""https://aws.amazon.com/blogs/containers/introducing-efs-csi-dynamic-provisioning/"">recently added to the new CSI driver</a>. This <a href=""https://github.com/kubernetes-sigs/aws-efs-csi-driver/issues/300"">GitHub issue</a> (unsolved to this day) indicates that the problem has actually been in place from the beginning of the driver’s use cases.</p><p id="""">Reading through the various use cases affected was like reading a long-lost diary of all our horrible secrets and failures laid bare: including some horrific harbingers of doom we had nearly inflicted on the rest of our customers who were yet to be migrated. We quickly reviewed our test cases and made the stunning discovery that we had been testing all kinds of workloads that read and write to NFS volumes, but hadn’t tested the ones that use chown. That was the only use case we hadn’t considered, and it was the one use case that failed.</p><p id="""">The root cause of the issue is that an EFS mount point that is dynamically created for a pod workload is given a set of mapped numerical User IDs (UIDs), but the UID that is stored inside the pod workload typically will not match the UID assigned to the EFS mount point. In most use cases, the operating system will not necessarily care what UID is in use on the mounted filesystem; it will typically just blindly read and/or write to the filesystem and assume that if the operation is a success that the permissions are correct. There are a number of good reasons not to be that trusting however. For example, in a database scenario, the permissions related to reading and writing data for the storage of important information is not left to chance and the application will attempt to ensure the UID (and maybe even Group IDs [GIDs]) match.</p><p id="""">This did not answer the question of why the legacy deprecated provisioner seems to work flawlessly, but we will dig into that on another blog post.</p><p id="""">To date, there does not seem to be any way to match the UIDs so that the operating system inside the container can set or even pretend to set the UID of a directory the application needs for reading and writing so that it matches the physical infrastructure underlying Kubernetes. This is not just an academic legacy issue, it is a real concern for security and privacy reasons that affect modern applications running in modern Cloud Native environments.</p><h3 id="""">A Few Solutions</h3><p id="""">Finally we present a few solutions, in chronological order of ones that we tried. We gradually settled on the last option as you will see the rationale behind this decision unfold.</p><p id="""">Option 1: Find every occurrence of Waldo and fix it for each customer and application workload. This option sounds as bad as you imagine it would be. Worse, it could make an easy and simple solution (pull a standard container and run it) unusable under normal circumstances. Even worse, our work would never be done: any new customers we onboard would have a new set of changes or fixes or workarounds to find and implement.</p><p id="""">For example, we could easily identify the lines affecting us in the <a href=""https://github.com/docker-library/postgres/issues/361#issuecomment-468391845"" id="""">postgresql image entrypoint</a> and create our own version. Which you would then need to create a separate dockerfile and modify it to your tastes…for each customer and each version of postgres and operating system that is in use times the number of applications each customer uses. Or, we could try to force the UID and GID numbers to match the CSI provisioner’s UID and GID to match (again, with a <a href=""https://github.com/docker-library/postgres/issues/361#issuecomment-508303459"" id="""">splinter version of the dockerfile</a>). Now that we have quote-unquote, allegedly, supposedly, air quotes “solved” the problem, do the exact same thing for the next application (like rabbitmq, or Jenkins, or <em id="""">whatever</em>) and all the application and operating system versions. Not just now, but also moving forward into the future forever.</p><p id="""">Option 2: Try to boil the ocean to find every single species of fish and identify them. Taking a step back, it is clear that we cannot hope to ever solve every use case of chown that is out there in the wild today, not to mention new ones that are being born every year. We were able to identify that most docker images use a specific UID and GID combination and the numbers of these are fairly limited. Examining two use cases in question, we found that postgresql images tended to use 999:999 and several others used 99 or 100, perhaps 1000 and 1001. This seemed like a promising lead to a solution because you can specify the <a href=""https://github.com/kubernetes-sigs/aws-efs-csi-driver#storage-class-parameters-for-dynamic-provisioning"" id="""">UID in the CSI provisioner</a>.</p><p id="""">This elegant solution would result in creating several StorageClasses in Kubernetes, like say, “postgresql-999”, “rabbitmq-1001”, and so forth. Or maybe just “efs-uid-999” to be more generic. Then we would teach each customer who enjoyed a failed build or deploy stack trace to change their settings to use the appropriate StorageClass. Even better, there are only about 2^16 possible unique UIDs in Linux, so we could programmatically create all of them in advance and apply them to our cluster to be stored in etcd, ready for retrieval whenever a customer wanted a UID-specific storage class. Or to limit choices in an opinionated but friendly way, we could require all containers to use a fixed UID, like 42, in order to use the storage volumes on our platform. If a customer wanted to use a different UID, like 43, we could charge $1 for every UID above and beyond the original one.</p><p id="""">If you did not detect any sarcasm in the preceding paragraph, you may want to call a crisis hotline to discuss obtaining a sense of humour. Amazon does not sell any upon last check; although you might find a used version on Etsy or eBay. I once ordered a sense of humour and it was stolen by a porch pirate before I could bring it in. Once I had obtained a suitable one, I would occasionally rent mine out on the joke version of Uber or Lyft, and sometimes you can even spend the night in my sense of humour on AirBNB, but due to abuse and lack of adequate tipping I have had to scale my activities down lately.</p><p id="""">Option 3: When in doubt, rollback to when it worked. We ultimately had to decide that we would be unable to support the new CSI driver until an adequate solution for dynamic deployments of EFS volumes was found for EKS. In the world of open source, there is always someone who comes up with a clever solution to a common problem and that becomes the de facto implementation recommendation. Currently, we were satisfied with the original functionality of the deprecated provisioner.</p><p id="""">But this raises another issue, how do we square using a deprecated and potentially unsupported solution on a platform our customers depend and rely upon? The answer is that we can make small adjustments and updates to the yaml and source code since the original solution code is still available and can be updated by Releasehub to support our customers.</p><h3 id="""">Conclusion</h3><p id="""">Sometimes we must accept that we live in an imperfect world and accept the fact that we are as imperfect as the imperfect world we live in which means that we should accept the imperfection as the correct way that things should be and thus, the imperfection we see in the world merely reflects the imperfections in ourselves, which makes us perfect in every way.</p>",false,,,,https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63e419afd2ee114ed068ffeb_071922%20(1).jpg,A block of code in a computer screen,regis-wilson,7,Tue Jul 19 2022 14:52:00 GMT+0000 (Coordinated Universal Time),,
How to Use GitHub Actions With Environment Variables,how-to-use-github-actions-with-environment-variables,62aa5a70cd5ba27d9d0d718a,62fe71eaaeaf9c8e9fad580d,Thu Aug 18 2022 17:07:54 GMT+0000 (Coordinated Universal Time),Fri Apr 21 2023 18:10:31 GMT+0000 (Coordinated Universal Time),Fri Apr 21 2023 18:22:03 GMT+0000 (Coordinated Universal Time),We'll cover GitHub Actions & show you how to use it to automate your deployment workflow & save secrets,"<p id="""">In this post we'll cover GitHub Actions. We'll show you how to use it to automate your deployment workflow and how to use environment variables to save secrets. </p><p id="""">We will deploy a simple Nest app using GitHub Actions. We'll start with a simple NestJS app. You don't need to know NestJS to get started, and you can use your own app if you wish. The starting point of the project can be taken from <a href=""https://drive.google.com/file/d/1CJFwKAXWL8lZ0ujf3YPpG5dpdOLQsHpF/view?usp=sharing"" target=""_blank"" id="""">here</a>. </p><h3 id="""">GitHub Actions Environment Variables</h3><p id="""">We're going to store Docker secrets later in the post using secret variables from GitHub Actions. They can be easily found in the Settings tab and will help you during the automation process. </p><h3 id="""">The Tasks</h3><p id="""">Like in any production app, we'll create a feature branch from the main branch. After the changes are made, we'll create a pull request to the main branch. </p><p id="""">Before merging, you need to ensure that this feature branch will not cause any breaking changes. For each pull request, you want to ensure that the build is successful and that all tests passed. </p><p id="""">If you don’t use GitHub Actions, this will require a manual process. Suppose you’re happy with your pull request and want to merge it into the main branch. The next set of tasks is to build Docker image and push it into Docker hub. You can also automate these manual tasks with GitHub Actions. </p><h3 id="""">The Setup</h3><p id="""">You need to extract the <strong id="""">Nest-App</strong> from the link provided earlier in your local system and run <strong id="""">npm install</strong> in it. </p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1024px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1024px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/62fe718c8479b73da7223a7f_aZjcIsfId-O8w4PFkpXyFK7VhD9gp07xFwN-wueBbOR_BJ4iAhjZrYYyb8mYBPoaZwFhfDUSzt-ONkDsLbQyBcP8i5UBzkMgO005LR4g4ZgACC5KtEQW2YvuLqDkUG7odwsFf2Yffm8hky2gkGdiA8o.jpeg"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><p id="""">After that, create a new repository in GitHub and push your project. </p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1024px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1024px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/62fe718cdb6ee077e73b2e44_iTzX3zxDEgAfdtjgcPskHX8kXDIOtfIFJrDjSKVCiW2lbFLwJSSgtHzSaQJQrWXSmar_F3qW27mozpUtGyTPT1QCF5zB1fIkw_1zu9_-D4mlN3KGrjGU2mnGv9zXb4fSi0PI8m0DcxQsMJf7pcL12dY.jpeg"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><p id="""">Click on the Actions tab. There you will find a lot of snippets for different projects. </p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1024px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1024px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/62fe718c6d2e0b8f9d3ef4bb_VbFP7LkjZv0YNnZztFAiUatXsNS9H6JAUKOnuAnz1YTotRJ3NurV0tX7ywRXuT0ACyqYY-jYJpIDphJ09W_pnGjycjmq4236WfSTabwH5U7Vjma8egNBP43n-T8mRbyvugleTnM-4vR8LEZT1wvhGFk.jpeg"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><p id="""">Search for <strong id="""">Node</strong> and then click on the Configure button of a Node.js project. </p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1024px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1024px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/62fe718c81ffbc30c1d242b5_20oPBDZgmZAwf6b98hak2MeLwl_CczmaJjXznvfgf3BLvKlsRt_BsaXF390_d6u-jtHEqZ4_A-Eqo-eMpE-J6hrIxHj6jME1FZ5KZ0WGGSA6PTZVazioC4Orxp6QKcySfsYQlYzoUbfY12lMp8Qjyhg.jpeg"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><p id="""">Copy all the code from this page, but don't save it. You are going to create it from VS Code in the next step. </p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1024px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1024px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/62fe718caeaf9c124dad53fe_wXDOGa2HclOzuJj8yNwc0-7cb4gqV2wiRDfwWGD0L2xo5TpWh37MwjS4_JEuhxlOSOkqnX1QoBViWnoBY4mtaUpM-KF2t6IxkvDI5EdHPUePgUwGzdK0voFQv1jj6stxLWIs7QBaHOGPMeXjowlftPw.jpeg"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><p id="""">Back in your app, you need to create a <strong id="""">.github</strong> folder and a <strong id="""">workflows</strong> folder inside it. So inside it, create an <strong id="""">integrate.yml</strong> file and paste the code from earlier. The .yml file basically says that on push or pull to the main branch, you need to run jobs. The first job name is <strong id="""">build</strong>, and it runs-on an Ubuntu machine provided by GitHub. It also specifies the node version it will run. </p><p id="""">In the steps below, you are using GitHub's Checkout Actions to check out into the build agent's directory. After that, you are setting up Node in that directory. </p><p id="""">Lastly, the main command of <strong id="""">npm i</strong> and <strong id="""">npm run build</strong> are run. </p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1024px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1024px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/62fe718c29b11f31896c1cb5_41jVMc6sjQZdrztkgd7KY_yZkcs4pxHpYsisOlyeC1UwaALSgmHfmPrgx0nelpC0zzyRBy7TV5_VFiux43vzJAX7kimcKPHgCIQwGJWuxknXEkixZtcy6OJ2E4AiK9PfjoZ_3l3ex7Vp9XoXSvBkc18.jpeg"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><p id="""">You will also create the workflow for your tests. Below, you are creating another job called <strong id="""">unit-tests</strong> and doing the same setup again. </p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1024px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1024px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/62fe718caeaf9c87d3ad53ff_2bA9GKhYcmNViEblA57JOG8zY6RGN5pfHuetszjjZSuJIvZMnZBjABcVCI2k0NkTl4TCbRH0rNIYjN7is5F6to-uS1O98Ov8C79H1qizfE-uU0iRUuCowIe9dWyodkjgyRJdZc6EsbWG7N9Pq8s67e4.jpeg"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><h3 id="""">The Failed Run</h3><p id="""">Now create and push a remote branch that you'll use for your changes. </p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1024px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1024px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/62fe718d22d31a17d8e12705_S1S1CgKxB0ZpuDq2107lCCIS9u3U2pofwiQCQ8X3sFy5_ikS2Xp3OlMdyPFl_d56M_acnVdwJC2V6WZ3uGsEog0P_-iUc4KFL6aRAfqPRF503ThRVR0-386_xDphUsr6GGTWNYYioF6YHZ43AR6vKUY.jpeg"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><p id="""">Back in GitHub, you'll see a compare and pull request button that you need to click. </p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1024px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1024px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/62fe718dba806237fad37a60_yXu_1xrC5qUd9FIzBU0IVqpXXdHg4FoSQGmkOFF0UK4RgZZhXZp7DfRbi9XxZfVbldnrO8e1jc27Tlm0PYwKgQWcjGKRdvxZDS9uM06fvfDsOX0J7XdBnKk_lbf3JIcOyKk9UEPDmG4XKhm93roPtV4.jpeg"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><p id="""">In the next screen, you just need to write a description and click the <strong id="""">Create pull request</strong> button. </p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1024px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1024px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/62fe718db4e1dcc9e17eea9e_P6CGt0IHrm-DVJuhoDI9zlF_fbQVQR84o3a6FSCyIrAvsGsL2ZG0n4wjvry1wlfJiQxrvidfkoZLpwh8VE2lDWEJaI1Y5SzzmNm8Pdd5YHyKleyXkVbe7xlFGri7f_MeaMpeT1h-xZClCLGGg0EajQM.jpeg"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><p id="""">The next screen says that a new branch got created called <strong id="""">new-actions</strong>. Click on the <strong id="""">Actions</strong> tab. </p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1024px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1024px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/62fe718d97168836355a7678_VKgm0i6yqV1RwOs0NbkomGAz4-LXjZRpgl3NzL6Vzt78qrHPndzr6FqV1eYn5YHRM2MgTNbh4r7jx4eeQoh7qVqbb-5JEmFyDFK6TiLZfe_nl_CpvMna-LuKE2w1rXUQbyqBj3zyW3Mdk0y6onWCOaY.jpeg"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><p id="""">On the Actions page, you can see that your action failed. You need to click on the same to know the exact issue.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1024px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1024px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/62fe718d6ba3c97586d6fa98_IoVsnnts_Dh-yFLXv8AU1UhMFHNKIIpjCx55CMpM5kUa_Ev2sy0jKC8aOteBZXJZovTeZOeMHxakcNeAV7pEu__gplknb1040urQ3scuIqI9-ZA2l6WLJUHrf26DPiUN3pmUHznl7FRYXtVoDOgvsoY.jpeg"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><p id="""">The next page shows that there is an issue in <strong id="""">unit-tests</strong> in the <strong id="""">integrate.yml</strong> file. </p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1024px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1024px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/62fe718d552a8790db659a07_1CFIXF9E8y39S66nsm-yEONdUZ6WznkI15sH8-yOQVohoGDCqg2tGIDHD0eyo4Z5SLngDNpiQ5k4xYjn-OXM9Nyo_bgZ_LEgIPhRw_27HZS-jZ4eNG7Aw4UxkarSo8pPp-WN8D0zPK95TDJ_UxFPeRY.jpeg"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><h3 id="""">The Successful Run</h3><p id="""">Upon inspection, it's clear that there is no space in the unit tests. So, we're adding the space and again pushing the changes to new-actions. </p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1024px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1024px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/62fe718d6ba3c925d7d6fa99_6IIyJj0bYWsRGwzPaX45vW605H_SOhrfTC1sp2QhtxL1kOltCAkeu_cv0UPhyP-XMsdDIksGeqpWtEG9EL612qT4Ih_0MGylBdENe1Fy8quw5ayU7dlL17XSpgv1QkQFAKJ_-ITHVZDe7TFAfnWQjcg.jpeg"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><p id="""">Back in GitHub, you can see that a new action has been running. </p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1024px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1024px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/62fe718e25a6fd1f063cdb38_OvHpFX0LyGvIEidH5Keq5RA7wVUA7GFWyHW3RqkodXBTsS_wXUWxCQ7mbB5IQG_wj-6P-qSYavyNbrzJI2n-xaumHRw50ky0i0W1F_uB8DQq3jTEkk96nSuJEcpmVqZpVgDvC2B75Ae13BJu7nP9A90.jpeg"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><p id="""">This time the build completed successfully. </p><p id="""">Also notice that the build and unit-tests jobs were run on three versions of Node.js. </p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1024px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1024px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/62fe718d4cb4d620a0f4ccf1_r-JOHlairmsoOcAqfb9Po-oYdEwBJGsCimM2jr8KJ-mgDHjK2mUjgreQMFL_Fka-zEEgQnfwwi7bdl0QDF3DLfgGYBgc2HhwADG4pwTBBDUxYfuRo-lOGYKkAxTT401UMK3biWQaed4_SXhNTswrp5Q.jpeg"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><p id="""">Now you can Merge this pull request and will get a confirmation. </p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1024px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1024px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/62fe718e3c1df10d37333c7f_GAUEDrVd2dgjjQMRuyyGL-le7C2_8iX6ZMniybHmssBQZgcAkc0BA2QEtv-z4Zr17eHz1VYW5_QOhBvcBqkWBnfP09ODFDdnLs_tQ4TJc6iOIAlqZF28kmsBXw1n2_CuJvezSSQ8_Aj_ghB_JhIATSQ.jpeg"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><h3 id="""">The Docker Setup</h3><p id="""">You'll need to go to <a href=""https://hub.docker.com/"" target=""_blank"" id="""">https://hub.docker.com/</a> and register if you don't have an account. </p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1024px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1024px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/62fe718e127c0579c27f0096_nNoNI3zEyoLKCR0FoDBXqACe5SOlEw3E_Cco3LnOGV1s6niAwT-_AQWM6xTelqPjrKTyGfewS_GwAZty5Za7BxbvYy_yrtKQ0LfVVQQcA4czYG-TnYfktIpoXvmjWHW2SGLRsCTqL7P9kN65xdSy3u4.jpeg"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><p id="""">Once you're logged in, click on the <strong id="""">Create Repository</strong> button. </p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1024px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1024px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/62fe718e08e80d14002f0630_epQzQPuVBC94XADtbTUgYAM1lelwcV7ZbQqeBF_0WZW-UUdTeVy04AXMhcrimX-f4YyKWcHA3s4WyDRMyJEetoAkufvj8OOJSfPxz3DLigJIbQ3dOidTD9ykHWm7g1Hq6YoyqKIx6wuDJXHnKXbC_jU.jpeg"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><p id="""">On the next page, you need to give the app a name and a description. Then click the <strong id="""">Create</strong> button. </p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1024px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1024px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/62fe718e4d979e49f96cd46b_rNVLCDr-g5d_RGT7rjmYJmukxWEc_TIW3rfW7-bbmZcaFuS6mqfvYuN8A6YkcwCCrL0JNcgARWPNqStN4ARP-7vHxybCuPJWU_cF6ahYUR9P8Cf748qnONLvxP5j1IB-9UVYRzh1WRsUfUv83z3sn68.jpeg"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><p id="""">On the next page, you should see the success message. Then click on the user profile in the top-right corner and then click the <strong id="""">Account Settings</strong> link. </p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1024px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1024px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/62fe718efc5fc3d117afbdca_6ixfuQmqkWy_R1hzXrd0Sux1-y01w3T6FweSpDzmvm_7L1T-naDqG-488bkKWulR4pTGls5BC9l_HX0xOwlqBgLDtw636MDKsxwljw7tOcNmkX9hbn8UL28x7VBNQEzJ8GkkLQbUW60P12RHAXPa6RI.jpeg"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><p id="""">On the next screen, click on the <strong id="""">Security</strong> tab and then the <strong id="""">New Access Token</strong> button. </p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1024px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1024px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/62fe718ee77b846eebd5c8a9_hWdXWQhYo5ytPjFarGD5alDVdjbj__UjEiI-72paO6guAQONXugv4xq7KVt-G4hYMCoN48zkh1cpdpQ0Sv_IIModj7e3ErSJv5qD_2dqfT4RA-91jhc_lzJOqK2djA7Z2-a02UDikFTEKxguxU9r0lk.jpeg"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><p id="""">A pop-up will open, and you need to give a description and permissions. After that, click the <strong id="""">Generate</strong> button.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1024px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1024px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/62fe718e127c0520027f0097_U-czQHgwb2B_EqUSGsNqS_7X2p6k003FKN0Axmkf97c7MKq4FreLOteOHnEUSmfMRfIMjEy_maA1SCfxwS2lWQPPxf43zueQXMkQkeCcvOJYhrQQmJp3H_IDWa3LPJMiKfXnD7sXcuJ4AZG-20aBmLI.jpeg"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><p id="""">On the next screen, you will get your username and the token. You'll need to copy those. </p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1024px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1024px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/62fe718e2fabc45777c2437c__74WYY4kplgkuRNVpNboKw2mUZVYy6GWe8ddUdWradaKKITuOQuNUDqN_Lsn2ljEepESyIQoriqAAwpDaQm-6-pCil5oBh1aeJFW015Nrj0lip621ALGgG3r1VLY1FxNJrsDTkF__N5k1wDZPw5vlDY.jpeg"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><h3 id="""">Environment Variables in GitHub</h3><p id="""">You cannot expose the Docker username and password, so you'll save them in GitHub environment variables. </p><p id="""">In the GitHub repository, click <strong id="""">Settings</strong>, <strong id="""">Secrets</strong>, then <strong id="""">Actions</strong>. You should see a button for <strong id="""">New repository secret</strong>, and you'll need to click on it. </p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1024px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1024px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/62fe718fdb6ee010e83b2e6c_25ZYC_oOwZOAzDuGf4a-K0XMnItSrdfelfd2XC7fey3Z9yQMuuvt8_2cp_CXIngFnkXZqCsu9K2iK91QLeTX_DdsIlSiIzW3rCC0P2ZEhrsDI0ibvoFrznBFjdBk2TxuLZeB-NJVa3krRnt3NZuwHfA.jpeg"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><p id="""">On the next page, you will give the new secret a name of <strong id="""">DOCKER_PASSWORD</strong>. Add the access token from the earlier section. After that, click on the <strong id="""">Add secret</strong> button. </p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1024px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1024px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/62fe718ff8ef6a17d6749395_TF_9QfeN6ahluYtEyusXIppkX0oZLKpQk_Q_6nJVrn58VWWQyjw7TIfnXyevkGDFQ9ZCsbo9sK1uQlis55KFpdXkTepnrwBBHINjd7hQqFhTOEr8un5CxH83B1e_zHYjfzFaJEgIVY6UQzRMzklTuOM.jpeg"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><p id="""">You'll also create another variable of DOCKER_USERNAME and store the username here.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1024px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1024px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/62fe718fdd8b199adffa6860_gVjaW6KREaRs5gHO62GQJdy6nujpRce6hKLkBVR7EmCmGYXhd3KJj8ZEyxiX2fFNY9yxfeEpd7NeM8dz3J_UYJAhrXoBn4K3Wo6KWloDlV4Dvs5uGRy_-918Kab2jXBOzGRCFGCk0SajqmDB4GfF0Fw.jpeg"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><p id="""">Now you should see two Repository secrets in the Actions tab. </p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1024px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1024px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/62fe718f8479b7fd9f223aad_z8Jdfnhapve4v9hNnGkpiRFpVk-69uHQh3RzwHkswfubK7fW2q_PjEFiT7ws3Knw6wmqouTlQa0Z5R7S2Re_evfwc5mhGWLwyeTN94z8O0cPxWzWdjmRSnFVqA0MmW8J3r5LGRsN5cCjjwqJRKVIHvs.jpeg"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><h3 id="""">The Release Flow</h3><p id="""">Next, you'll create the Docker release flow. Go to the main branch. Then get the updates and create a new branch of <strong id="""">release-flow</strong>. </p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1024px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1024px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/62fe718ff3d4a43690dbf240_1Z-QBJsPQtIkx0MfJoN5cnf0cnAMvJ8IIW4D3dFop0xUyD_sgL-wejjS4W4jIN7BVuERaFtvZ1TKS8LHDF7z0SPFVEQQJ1xn_Q_D2zdSX1980iBosRWhNua04mkMwoiOF2bKSvIkPMHs8vWtK5d2F0s.jpeg"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><p id="""">Next, create a file <strong id="""">release.yml</strong> in the <strong id="""">workflows</strong> folder. Place the code below in it. Here, you're running the job <strong id="""">deploy</strong> only when merging to the main branch. You just need to check out to the branch. After that, you will be running the Docker build command. </p><p id="""">Next, you'll use the username and password to log in to Docker using the secrets stored in GitHub in the previous section. </p><p id="""">Lastly, you're running the push command to push to Docker. </p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1024px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1024px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/62fe718f97168802985a7681_W0FENrptBJuUwkVXoati7js9L32xYx-IE9q4DBHsaB0c9qURJ9a6joST92HRJ7ACNvrYZXSbvu_tTnuM8JqLySu9JJSzxjjJcglg44XWybPB8jdKuVmDPW0Ne__dKzhEtH_f_AjuABmcywGkqBTaS4M.jpeg"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><h3 id="""">The Docker Run</h3><p id="""">First, you'll add these changes and then commit them to your new <strong id="""">release-flow</strong> branch. After that, you'll push it to GitHub. </p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1024px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1024px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/62fe718f25a6fd1f8f3cdb39_L33zhEBdpM5RuqHsbi_-vOaVvVuBMIqrnGp-iLViJGd7ACiDeoDRqGJUbeKVnXinvdeGmAsxRqyAhCoHKwH54i1Wk8fgnZX9TnHOA5wVXgsvku5YsVOQD_GTFnM_pug1M2DWkTHzNv82r57nETODhNQ.jpeg"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><p id="""">Back in GitHub, you should see a new <strong id="""">Compare &amp; pull request</strong> on the home page of your app. </p><p id="""">Click on it.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1024px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1024px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/62fe718f896a1564935b3728_bNLVIpjthmU_dsrD--0yDPFIjUHimv0eKycAGdlF7nH67-3t5Goe6CWKanvTUze907je8Zy_LtA2oEJXBFmNHhvQ6e7d8m7ZmsvrGSPSEQ0rGj43TIJj0dRlVkYVm19iFTXeQ1wyhCEpYAP6l5BDwV0.jpeg"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><p id="""">On the next page, you need to provide some descriptions and click the <strong id="""">Create pull request</strong> button. </p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1024px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1024px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/62fe718f4cb4d68089f4ccfe_4L_aUxxafCxWp9uqMPB6evHjOpgzt2N7IUp3UnLh3kTcVopXe_rHVA-_ZqoBoOClMXNG9MS3oQwrwRCkcgt-nzldhP1B3WFBCWIsCXV2hDwNgnDPeN5f3C31pvaLr7RypS0igjhTLULFgoUTjr83ukg.jpeg"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><p id="""">Since you've created a new branch, the integrate flow, the build, and the unit test jobs should run successfully. </p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1024px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1024px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/62fe718f56fbe72072a35b3e_QbybgnvZO1plVw_CsnQ2emnsjQxzn4mGW-woI-RgECm_G4gEv8aeucbB7Z1CCAqotl-rZL4zbyUl_L9LmWirzCx2NtrsYNMPE_6wwnr54Wtq3NHnfNyXTHbpFdJnlgKyOVgRndkj4-2OmAlCnOLJTwY.jpeg"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><p id="""">Now you can merge your pull request in the main branch by clicking the <strong id="""">Merge pull request</strong> button. </p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1024px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1024px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/62fe7190e77b84afcfd5c8ae_YmP9SJxsSokLf6qXxRg3XTTAYMYI5-oHz9-0bEW1QwDV0sHXHUYQLYwcpU3PWsFCy5L-Qx1rJbROarwt77X3gc-AkzRlNtVDCvAEGnzsJFJFjQ4EbxxaTKO-hojyBDLnY6BILJisB5FP3VG6fQqRHG4.jpeg"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><p id="""">You will be asked to confirm this merge on the next page. Just click the <strong id="""">Confirm merge</strong> button. </p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1024px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1024px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/62fe719020c3707347963b2a_7VOQk7jIk04FoN-mCYNKaaxmgIk4lHZFt2H95nAigFQVaQ4DhX7DwoVgJDiJ3yNS009zhWWbszC6ThuX42_xbzKerhfgVJcvihjOJcpf6ilJlaOoGG0VELm0GrvUMtzIdhjSjnEKsVfgr7Riici85Us.jpeg"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><p id="""">Your new actions of release-flow should run now, and you can see it in the Actions tab.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1024px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1024px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/62fe7190b9de596e3eb11c09_iy88tfEno3KzLmrwQhw6sWh1JqfIr09b3M3Y99ZIgBh6ExzOGmNdx064RSo7kQVgodWOszpsJxtdoeZgBVE_Az1LTPTZ6r721n2VTpw8zCCmf3H2nC1c0pLwSmWHB4crlz6JOXemb9DElqGHHM_fMDY.jpeg"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><p id="""">After clicking any of them, you should find that your Docker commands also ran. </p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1024px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1024px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/62fe71902e137b258b8fcabe_jAan0GV5R-jF0jgYf3njvjGLEVnqG6BdGDYl7FtH7JSm_NeTByzx0BfclrEZ1hmwcoZNSdkek3nWe6pEyZjB8LNWXrXJ7VGxHjfPG42GwAo5CodvQh8dyjflxxgewSmS3jwS0zrBzhqU0YswiWTsE8w.jpeg"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><p id="""">After two or three minutes, your workflows should complete successfully. </p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1024px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1024px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/62fe71908479b70e39223aae_Nr74LJyr_qTTVI-7mN16K53aMNpVAipp_GiSWeWA2kWD7phgZ6KvalVOwQpzYxNPjVEYg1Jqwx2qo-uJovKTCcWV34CoqSOitdBy8cyCX3Bu_yd8yZRSxqq5iEdVVzchwWtNok7rcRVYIuWBVn7lfN8.jpeg"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><p id="""">You can also confirm that the new Docker image was added in Docker hub. </p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1024px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1024px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/62fe71906d2e0b31353ef4c5_awH0ImNbKMl_R6RXxMr_ENetcMPR01MYpPvUxuw8BG_ti4xb_behOLwnIYYJk1CuwUjs5PAUNnYGoJv0M8R_stbioJPgD70PCAGVFuF_8qxODuF3d_4H-Kjc1WWX5VEvD3bkhqIxUfELfEEcyYOcI-k.jpeg"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><h3 id="""">Conclusion</h3><p id="""">In this post, you pushed a simple Nest app to GitHub. Then you created two automated workflows on GitHub through GitHub Actions. </p><p id="""">First, you created an integrate workflow that ran when a pull request was made to main branch from a feature branch. It ran the build and test jobs. Next, you created integrate workflow for your Docker workflow. It ran after a push was made to the main branch. The job created a Docker build and a Docker image in Docker hub. </p><p id="""">Did you know you can easily spin up an environment on release directly from your docker-compose file? Give it a <a href=""https://release.com/"">shot</a>.</p>",false,,,,https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63e41be98552d14972615f5a_081822%20(1).jpg,,ashley-penney,8,Thu Aug 18 2022 17:45:00 GMT+0000 (Coordinated Universal Time),,
How to Use the Kubernetes RBAC API for More Secure Apps,how-to-use-the-kubernetes-rbac-api-for-more-secure-apps,62aa5a70cd5ba27d9d0d718a,63373b984b0eb22e442227b3,Fri Sep 30 2022 18:55:19 GMT+0000 (Coordinated Universal Time),Fri Apr 21 2023 17:04:55 GMT+0000 (Coordinated Universal Time),Fri Apr 21 2023 17:05:06 GMT+0000 (Coordinated Universal Time),RBAC API gives permissions to an object in a Kubernetes cluster to improve governance & strengthen secuity.,"<p id="""">If you manage a Kubernetes cluster, sooner or later, you'll need to assign roles and permissions to users so that everyone has visibility and access only to the resources they need. For example, some users must have read-only access, while others must only be able to write to certain resources or APIs. In addition, there must be users with unlimited access to all available resources.&nbsp;</p><p id="""">Suppose you have a team of four users:&nbsp;</p><ul id=""""><li id="""">Patricia: leader of the dev team</li><li id="""">James: part of the dev team</li><li id="""">Oliver: leader of the QA team</li><li id="""">Amelia: part of the QA team</li></ul><p id="""">James works on one of the many projects within the company's cluster and therefore needs access to your namespace. Patricia, being the dev leader, needs full access to all projects. Oliver, likewise, needs full read access to all objects in the cluster. However, Amelia is working on the same project as James and can only have read access in this namespace.&nbsp;</p><p id="""">Role-based access control (RBAC) is an authorization mechanism built to handle such cases. In this article, we will discuss what RBAC API is and how you can use Kubernetes RBAC API to develop secure applications.&nbsp;</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1067px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1067px""><div id=""""><img alt=""Close-up of people shaking handsDescription automatically generated"" src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/633738ce1e3861f002eb0852__Lx93tUKeGRrcRl9prHtDucNQ6LVwTN2S92UY94YG9aw5zStCAZW4p_qsMraU2JpGLyto9XdJrv4Q1F_QpwLAqP0OclKj_r0uVqQS5Cf-U__FYtZ5ddW3VLAGS58xwMfqUgBdSKyI6iUlIThV7yc4AjrFTKsfhASQnfVsLl6lZpUdalnSTDoLA4QFg.jpeg"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><h3 id="""">What is RBAC?</h3><p id="""">The concept of role-based access control is not something new. RBAC is based on the concepts of roles, permissions, and user groups, and it's one of the most widespread access control models being used in organizations today. &nbsp;</p><p id="""">In organizational use, RBAC allows you to create secure access models based on the real functions that people have within the organization rather than on the actions they must be able to perform.&nbsp;</p><h3 id="""">What is the RBAC API, and how is it used in Kubernetes?</h3><p id="""">Kubernetes (as of version 1.6) introduced the concept of role-based access control as a system for distributing access rights to various objects in a Kubernetes cluster.&nbsp;</p><p id="""">Objects in a Kubernetes cluster are YAML manifests, and permissions determine which user can only view the manifests and who can create, modify, or even delete them.&nbsp;</p><p id="""">Before we go into how RBAC works in Kubernetes, it's important to understand what a user is in Kubernetes. Everyone who sends requests to the API server is a user in a Kubernetes cluster. This means that not only administrators and developers are users, but also various CI/CD scripts and control plane, <strong id="""">kubelet</strong>, and <strong id="""">kube-proxy</strong> components on nodes are considered users. &nbsp;</p><p id="""">The RBAC model includes five entities: &nbsp;</p><ul id=""""><li id="""">Role</li><li id="""">RoleBinding</li><li id="""">ClusterRole</li><li id="""">ClusterRoleBinding</li><li id="""">ServiceAccount</li></ul><p id="""">Let's explore each entity in more detail.&nbsp;<strong id="""">‍</strong></p><h3 id="""">Role</h3><p id="""">The <strong id="""">role</strong> is a YAML manifest that describes a set of rights on Kubernetes cluster objects.&nbsp;</p><p id="""">Here it's important to understand that cluster objects are YAML manifests stored in <strong id="""">etcd</strong>. The API server checks all rights as they relate to the requests that the API server receives. &nbsp;</p><p id="""">If you restrict the user to execute <strong id="""">kubectl exec</strong>, but that user has access to the worker node, then they will not be able to block it from entering the worker node and doing docker exec in the RBAC container.&nbsp;</p><p id="""">We can go to the cluster, and in <strong id="""">ns ingress-nginx</strong>, look at the <strong id="""">role: ingress-nginx</strong>:&nbsp;</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yaml"">
kubectl get role -n ingress-nginx ingress-nginx -o yaml
</code>
</pre></div><p id="""">Here we are interested in the rules section. This is a list of rules that describe access rights.&nbsp;</p><p id="""">In each rule, we have three parameters. Let's look at an example:&nbsp;</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yaml"">
apiGroups:
  - extensions
  - networking.k8s.io
 resources:
  - ingresses
 verbs:
  - get
  - list
  - watch
</code>
</pre></div><p id="""">Here <strong id="""">apiGroups:</strong> describes the manifest API group. If only the version is specified in apiVersion—without a group, for example, as in the Pod manifest—then this manifest is considered to have the so-called root group (core-group). In the role, the root group is specified as an empty string.&nbsp;</p><p id="""">The <strong id="""">resources:</strong> parameter refers to a list of resources to which we describe access. You can view the list of resources in your cluster with the command <strong id="""">kubectl api-resources</strong>. Some sub-resources describe specific actions. For example, the <strong id="""">pods/log</strong> sub-resource allows you to view container logs in a pod. &nbsp;</p><p id="""">The<strong id=""""> verbs:</strong> parameter is a list of actions that you can perform on the resources described above: get, view the list, monitor changes, edit, delete, etc.&nbsp;</p><h3 id="""">RoleBinding</h3><p id="""">Let's now look at the <strong id="""">RoleBinding</strong> manifest:&nbsp;</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yaml"">
kubectl get rolebinding ingress-nginx -n ingress-nginx -o yaml
</code>
</pre></div><p id="""">It has two types of fields: <strong id="""">roleRef</strong> and <strong id="""">subjects:</strong>&nbsp;</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yaml"">
roleRef: 
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: ingress-nginx
subjects:
- kind: ServiceAccount
  name: ingress-nginx
  namespace: ingress-nginx
- kind: User
  name: jane              # ""name"" is case sensitive
  apiGroup: rbac.authorization.k8s.io
- kind: Group
  name: developer      
  apiGroup: rbac.authorization.k8s.io
</code>
</pre></div><p id="""">Here's what we have:&nbsp;</p><ul id=""""><li id=""""><strong id="""">roleRef</strong> specifies the role.</li><li id=""""><strong id="""">subjects</strong> specifies who will be assigned this role.</li><li id=""""><strong id="""">kind</strong> specifies permissions for requests not authenticated through a token from the service account.</li></ul><h3 id="""">ClusterRole</h3><p id="""">The <strong id="""">role </strong>entity is namespace dependent and we can create roles with the same name in different namespaces. While <strong id="""">ClusterRole</strong> is a cluster object, this entity describes the rights to objects in the entire cluster.&nbsp;</p><p id="""">Kubernetes has many preconfigured cluster roles. These include the <strong id="""">admin</strong>, <strong id="""">edit</strong>, and <strong id="""">view</strong> roles, which describe the rights that allow administrating, editing, or only viewing entities. If you have administrator rights, you can view the role in your cluster with the following command:&nbsp;</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yaml"">
kubectl get clusterrole edit -o yaml
</code>
</pre></div><h3 id="""">ClusterRoleBinding</h3><p id="""">RoleBinding only gives access to entities in the same namespace as the RoleBinding manifest. <strong id="""">ClusterRoleBinding</strong> allows you to grant access to entities in all cluster namespaces simultaneously.&nbsp;</p><h3 id="""">Service Account</h3><p id="""">Kubernetes knows nothing about users in the form we are used to seeing them in when it comes to other access restriction systems, where users have a login or a password. Still, it has mechanisms for calling external password verification services, such as<strong id=""""> oidc</strong>, a user certificate verification option, or even the usual HTTP basic auth with the classic Apache file <strong id="""">htpasswd</strong>.&nbsp;</p><p id="""">‍<strong id="""">ServiceAccount</strong> was created primarily to limit the rights of applications that run in a cluster. All communication between cluster components goes through requests to the API server, and a special JWT token just authorizes each such request. This token is automatically generated when an object of the <strong id="""">ServiceAccount</strong> type is created and placed in <a href=""https://release.com/blog/kubernetes-secrets-management-a-practical-guide"">secret.</a> &nbsp;</p><h3 id="""">How to use RBAC API in Kubernetes</h3><p id=""""><a href=""https://kubernetes.io/docs/reference/access-authn-authz/rbac/"" target=""_blank"">RBAC authorization</a> is one way to assign roles to users in a Kubernetes cluster. Here are the steps:&nbsp;</p><p id="""">1. Connect the <a href=""https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/"" target=""_blank"">service account token</a>. Without this token, you will need to re-download <strong id="""">kubeconfig </strong>after any change in roles.&nbsp;</p><p id="""">2. Assign roles. Here's an example manifest that creates two namespaces and two users, each of which will only be able to <a href=""https://release.com/blog/kubernetes-pod-a-beginners-guide-to-an-essential-resource"">manage pods</a> in their own namespace:&nbsp;</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yaml"">
apiVersion: v1
kind: Namespace
metadata:
  name: test-one
---
apiVersion: v1
kind: Namespace
metadata:
  name: test-two
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: read-pods-one
  namespace: test-one
subjects:
- kind: ServiceAccount
  name: test-sa-one
  apiGroup: """"
roleRef:
  kind: Role
  name: pod-reader-one
  apiGroup: rbac.authorization.k8s.io
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: read-pods-two
  namespace: test-two
subjects:
- kind: ServiceAccount
  name: test-sa-two
  apiGroup: """"
roleRef:
  kind: Role
  name: pod-reader-two
  apiGroup: rbac.authorization.k8s.io
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  namespace: test-one
  name: pod-reader-one
rules:
- apiGroups: [""""] # """" indicates the core API group
  resources: [""pods""]
  verbs: [""get"", ""watch"", ""list""]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  namespace: test-two
  name: pod-reader-two
rules:
- apiGroups: [""""] # """" indicates the core API group
  resources: [""pods""]
  verbs: [""get"", ""watch"", ""list""]
---
apiVersion: v1
kind: ServiceAccount
metadata:
  namespace: test-one
  name: test-sa-one
---
apiVersion: v1
kind: ServiceAccount
metadata:
  namespace: test-two
  name: test-sa-twoRun the manifest.
</code>
</pre></div><p id="""">3. Run the manifest. </p><p id="""">4. Create tokens:&nbsp;</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yaml"">
kubectl get secret $(kubectl get serviceaccount test-sa-one -o jsonpath='{.secrets[0].name}' --namespace test-one) -o jsonpath='{.data.token}' --namespace test-one | base64 -d
<long and secure token for test-sa-one>

kubectl get secret $(kubectl get serviceaccount test-sa-two -o jsonpath='{.secrets[0].name}' --namespace test-two) -o jsonpath='{.data.token}' --namespace test-two | base64 -d
<long and secure token for test-sa-two>
</code>
</pre></div><p id="""">5. Manually add tokens to users in the <strong id="""">kubeconfig.yaml </strong>file for authorization without a password:&nbsp;</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yaml"">
users:
...
- name: test-sa-one
user:
  token: <long and secure token test-sa-one>
- name: test-sa-two
user:
  token: <long and secure token test-sa-two>
...
</code>
</pre></div><p id="""">6. Check the distribution of roles:&nbsp;</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yaml"">
kubectl config set-context --current --user=test-sa-two
Context ""admin@kubernetes"" modified.

kubectl get pods --namespace test-two
No resources found in test-two namespace.

kubectl get pods --namespace test-one
Error from server (Forbidden): pods is forbidden: User ""system:serviceaccount:test-two:test-sa-two"" cannot list resource ""pods"" in API group """" in the namespace ""test-one""
________

kubectl config set-context --current --user=test-sa-one
Context ""admin@kubernetes"" modified.

kubectl get pods --namespace test-two
Error from server (Forbidden): pods is forbidden: User ""system:serviceaccount:test-one:test-sa-one"" cannot list resource ""pods"" in API group """" in the namespace ""test-two""

kubectl get pods --namespace test-one
No resources found in test-one namespace.
</code>
</pre></div><p id="""">The test-sa-two user now has access to pods in the <strong id="""">test-two</strong> namespace and no access to the <strong id="""">test-one</strong> namespace. Similarly, the ""test-sa-one"" user has access to pods in the <strong id="""">test-one</strong> namespace but not those in the <strong id="""">test-two</strong> namespace.</p><h3 id="""">Kubernetes RBAC as a Security Strategy</h3><p id="""">The RBAC model has proved particularly effective for managing roles and is now considered a Kubernetes security best practice. &nbsp;</p><p id="""">RBAC reduces the risk of unwanted access to critical resources and allows you to implement the principle of least privilege by giving access to only needed resources, which makes your cluster more secure. &nbsp;</p><p id="""">Moreover, if you are using RBAC, you don't have to check the individual permissions assigned to each user. You can monitor who has access to resources simply by consulting roles, which makes auditing easier.&nbsp;</p><h3 id="""">Conclusion</h3><p id="""">In summary, RBAC API is a role-based approach to giving permissions to an object in a Kubernetes cluster. By making it part of the Kubernetes pipeline, you can improve governance and significantly strengthen security.&nbsp;</p>",false,,,,https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63e4221942aa6304a33d3c05_112322%20(1).jpg,Close-up of people shaking hands,tommy-mcclung,4,Wed Nov 23 2022 19:00:00 GMT+0000 (Coordinated Universal Time),,
How To Write Route53 Stubbed Responses For Rspec Tests,how-to-write-route53-stubbed-responses-for-rspec-tests,62aa5a70cd5ba27d9d0d718a,62aa5a70cd5ba27dce0d72e5,Thu Sep 09 2021 02:28:14 GMT+0000 (Coordinated Universal Time),Fri Apr 21 2023 19:44:00 GMT+0000 (Coordinated Universal Time),Fri Apr 21 2023 20:04:43 GMT+0000 (Coordinated Universal Time),Short Example for writing AWS Ruby SDK unit tests for Route53 API calls with stubbed responses,"<h3 id="""">How to Write Route53 Stubbed Responses For Rspec Tests</h3><p id="""">In this blog post, I will go over a recent exercise to fix some bugs, refactor, and write tests for some of our code related to Route53. Route53 is an AWS service that creates, updates, and provides Domain Name Service (DNS) for the internet. The reason that code unit tests are so important is because it helps reveal bugs, creates supportable and high quality code, and allows restructuring and refactoring with confidence. The downside to writing unit tests is that it can be time consuming, difficult at times, and bloating to the normal code base. It is not uncommon for unit tests’ ""lines of code"" (LOC) count to far exceed the LOC for the actual codebase. You would not be crazy to have nearly an order of magnitude difference in LOC for actual codebase versus LOC&nbsp;for unit test cases.<br></p><p id="""">In this case, interacting with the AWS Route53 API was daunting to test and stubbing responses seemed incredibly difficult until I found some examples written by another one of our engineers that showed how the rspec and API SDKs could be made to work in a fairly straightforward and (dare I say) downright fun method for unit testing Ruby code.</p><h3 id="""">The Code Under Examination</h3><p id="""">This straightforward code snippet was my first target for unit testing. It is very simple and only does one thing. It is ripe for refactoring for readability and reusability for other sections of the code. This should be the best way to begin the project and get familiar with the rspec templates I’d be using later. Before I start refactoring and fixing bugs, I wanted to write tests. Other than the fairly “inliney” and hard to follow syntax and “magical” code, can you spot any bugs?</p><div data-rt-embed-type='true'><pre>
<code class=""language-ruby line-numbers"">
def route53_hosted_zone_id(subdomain)
  route53.list_hosted_zones_by_name.map do |response|
    response.hosted_zones.detect{|zone| zone.name == ""#{subdomain}."" }&.id&.gsub(/.*\//, '')
  end.flatten.compact.first
end
</code>
</pre></div><h3 id="""">Write Helpers Before the Refactor</h3><p id="""">I am already itching to remove the magical subdomain rewriting and gsub deleting into separate methods that can be reused and are easier to read:</p><div data-rt-embed-type='true'><pre>
<code class=""language-ruby line-numbers"">
def cannonicalise(hostname)
  hostname = domain_parts(hostname).join('.')

  ""#{hostname}.""
end

def parse_hosted_zone_id(hosted_zone_id)
  return nil if hosted_zone_id.blank?

  hosted_zone_id.gsub(%r{.*/+}, '')
end
</code>
</pre></div><h3 id="""">Stub and Test the New Methods</h3><p id="""">First things first, we need to do a little bit of boilerplate to get the API calls mocked and stubbed, then add a few very simple tests to get started.</p><div data-rt-embed-type='true'><pre>
<code class=""language-ruby line-numbers"">
# frozen_string_literal: true

require 'rails_helper'

RSpec.describe Cloud::Aws::Route53 do
  let(:route53) { Aws::Route53::Client.new(stub_responses: true) }

  subject { FactoryBot.create(:v2_cloud_integration) }

  before do
    allow(subject).to receive(:route53).and_return(route53)
  end

  describe '#parse_hosted_zone_id' do
    context 'with a valid hostedzone identifier' do
      it 'returns just the zoneid' do
        expect(subject.parse_hosted_zone_id('/hostedzone/Z1234ABC')).to eq('Z1234ABC')
      end
    end
  end
  describe '#cannonicalise' do
    context 'without a dot' do
      it 'returns the zone with a dot' do
        expect(subject.cannonicalise('some.host')).to eq('some.host.')
      end
    end
    context 'with a dot' do
      it 'returns the zone with a dot' do
        expect(subject.cannonicalise('some.host.')).to eq('some.host.')
      end
    end
  end
end
</code>
</pre></div><h3 id="""">Write A Fixture</h3><p id="""">Perfect, now we can test our new <strong id="""">cannonicalise</strong> and <strong id="""">parse_hosted_zone_id</strong> methods and we have a stubbed response coming from the Route53 API calls. Let’s write a simple new test to uncover some bugs by testing the api responses we get. The first step is to write some fixtures we can test with. Here we generate two faked stubbed responses for a very <a href=""https://example.com"" id="""">common domain</a>.</p><div data-rt-embed-type='true'><pre>
<code class=""language-ruby line-numbers"">
  context 'an AWS cloud integration' do
    before do
      route53.stub_responses(:list_hosted_zones_by_name, {
                               is_truncated: false,
                               max_items: 100,
                               hosted_zones: [
                                 {
                                   id: '/hostedzone/Z321EXAMPLE',
                                   name: 'example.com.',
                                   config: {
                                     comment: 'Some comment 1',
                                     private_zone: true
                                   },
                                   caller_reference: SecureRandom.hex
                                 },
                                 {
                                   id: '/hostedzone/Z123EXAMPLE',
                                   name: 'example.com.',
                                   config: {
                                     comment: 'Some comment 2',
                                     private_zone: false
                                   },
                                   caller_reference: SecureRandom.hex
                                 }
                               ]
                             })
    end
end
</code>
</pre></div><p id="""">If you’re wondering how to make these fixtures, you can easily read the <a href=""https://docs.aws.amazon.com/sdk-for-ruby/v3/api/Aws/Route53/Client.html#list_hosted_zones_by_name-instance_method"" id="""">AWS Ruby SDK V3 documentation</a> for sample inputs and outputs, or you can make API calls via the AWS CLI and inspect the responses, or you can even just put in some values and see what happens when you run rspec. For example, if I remove, say, the `caller_reference` parameter, I’ll get an error that helpfully identifies the problem.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1231px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1231px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/61396dc7ba2cf9785f834a62_gi0P_23ZwcyygmAxR4v_7ry9Fxx4FbkZaC-J2fk-nQWpLS3U4LL5frpOlDvxvDzmIYuABovL8jGHN4pop8Kx_5YveIQW1a1J_BbRasvLjx922NCWe3fjNFD5tqu-YkxcvSdSgxID%3Ds0.png"" width=""auto"" height=""auto"" loading=""auto"" id=""""></div><figcaption id="""">Removing required parameters gives a helpful error message to correct the problem.</figcaption></figure><p id="""">You really can’t go wrong with the SDK validation and stubbed responses taken from the examples or from live requests you make with the CLI! This is already a tremendous benefit and we’re not even testing our own code yet.</p><h3 id="""">Write a Test Case with the Stubbed Responses</h3><p id="""">Now we can write some unit test cases and loop through several responses that we expect to find the hosted zone. Voilá we’ve uncovered some bugs just by being a little creative with our inputs! Do you see why?</p><div data-rt-embed-type='true'><pre>
<code class=""language-ruby line-numbers"">
describe '#route53_hosted_zone_id' do
  %w[
    example.com
    example.com.
    www.example.com
    www.example.com.
    test.www.example.com
    test.www.example.com.
    deep.test.www.example.com
  ].each do |hostname|
    context 'for hosts that exist in the parent zone' do
      it ""returns the hosted_zone_id for #{hostname}"" do
        expect(route53).to receive(:list_hosted_zones_by_name).with(no_args).and_call_original
        hosted_zone_id = subject.route53_hosted_zone_id(hostname)
        expect(hosted_zone_id).to eq('Z123EXAMPLE')
      end
    end
  end
end
</code>
</pre></div><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1466px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1466px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/61396dc79b1b794109eb360d_DUieXIeY28jN5yrI7Fi3cBv1gS5k1T14NEzcaiqNICQ5f3n-jYC3us0jPQ5mf3_E6kx-qYXRiPBFqOj9mOYsDyfxkXfL80ARkdaageomfIKvf97tE84oD0tULF8ykQMFGn-kiqFQ%3Ds0.png"" width=""auto"" height=""auto"" loading=""auto"" id=""""></div><figcaption id="""">With some creativity in test inputs and stubbed responses from the API, we can uncover some edge cases and bugs to fix!</figcaption></figure><p id="""">What these failed test cases are telling us is that the code worked under perfect conditions but in strange scenarios that may not be uncommon (for example, having an internal private zone and public zone with the same name, or selecting a two-level-deep name in a zone) could cause unpredictable behaviours.</p><h3 id="""">The Solution is an Exercise for the Reader</h3><p id="""">Now we merely need to write or refactor the code from our original snippet to pass all of our new test cases. One of the issues that our test cases revealed was that two-level-deep names (say, test.www.example.com in the zone example.com) would be missed. We also needed a way to ensure that zones are not private, perhaps with an optional parameter to specify private zones. Here is an example that passes all the existing tests and welcome feedback on any other bugs or optimisations you find.</p><div data-rt-embed-type='true'><pre>
<code class=""language-ruby line-numbers"">
def route53_hosted_zone_ids_by_name(is_private_zone: false)
  # TODO: danger, does not handle duplicate zone names!!!
  hosted_zone_ids_by_name = {}
  route53.list_hosted_zones_by_name.each do |response|
    response.hosted_zones.each do |zone|
      if !!zone.config.private_zone == is_private_zone
        hosted_zone_ids_by_name[zone.name] = parse_hosted_zone_id(zone.id)
      end
    end
  end
  hosted_zone_ids_by_name
end

def route53_hosted_zone_id(hostname)
  # Recursively look for the zone id of the nearest parent (host, subdomain, or apex)
  hosted_zone_ids_by_name = route53_hosted_zone_ids_by_name

  loop do
    hostname = cannonicalise(hostname)
    break if hosted_zone_ids_by_name[hostname].present?

    # Strip off one level and try again
    hostname = domain_parts(hostname).drop(1).join('.')
    break if hostname.blank?
  end
  hosted_zone_ids_by_name[hostname]
end
</code>
</pre></div><h3 id="""">Congratulations</h3><p id="""">All test cases now pass! Keep writing tests until you get nearly 100% coverage!</p><blockquote id="""">Hero Image by <a href=""https://unsplash.com/@jeswinthomas?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText"" id="""">Jeswin&nbsp;Thomas</a> on <a href=""https://unsplash.com/s/photos/testing?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText"" id="""">Unsplash</a><br></blockquote>",false,,,,https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63e41d4c77c34a4d95cebeb2_091421%20(1).jpg,A person hands holding wires and making tests on a circuit board,regis-wilson,6,Tue Sep 14 2021 15:00:00 GMT+0000 (Coordinated Universal Time),,
IDP Trends: Where Do We Go From Here,idp-trends-where-do-we-go-from-here,62aa5a70cd5ba27d9d0d718a,65416881633d6cfac7dae8d8,Tue Oct 31 2023 20:50:09 GMT+0000 (Coordinated Universal Time),Thu Jul 25 2024 15:02:27 GMT+0000 (Coordinated Universal Time),Thu Jul 25 2024 16:27:39 GMT+0000 (Coordinated Universal Time),Explore how IDPs will shape the future of software development by looking at current and anticipating future trends.,"<p id="""">We’ve shared a lot about <a href=""https://release.com/blog/what-is-an-internal-developer-platform-and-why-should-i-have-one"" id="""">Internal Developer Platforms (IDPs)</a>, <a href=""https://release.com/blog/components-of-a-successful-idp-build-a-product-your-developers-actually-want-to-use"" id="""">what they can provide</a>, and <a href=""https://release.com/blog/use-product-thinking-to-establish-your-idp"" id="""">how we should build them</a>. As they exist today, simple IDPs focus on improving dev workflows to make our development teams more efficient.</p><p id="""">But what should we expect in the future? Will further developments and patterns in IDPs provide even more efficiencies? Will new opportunities open up to integrate additional tools that simplify product development?</p><p id="""">In short, yes!</p><p id="""">The more organizations start building and using IDPs to enhance workflows, the more development teams will expect IDPs to automate the toil, and the more benefits we’ll gain.</p>",true,"<p id="""">Ready to test-drive Release? </p><p id="""">Try it free for 30 days with code #IDP</p>",https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=idp6,"<p id="""">In order for you to stay on top of trends and continue to provide your developers with a competitive advantage, creating effective IDPs will become more critical than ever.</p><p id="""">In this post, we’ll explore how IDPs will shape the future of software development by looking at existing trends and envisioning what else IDPs can bring to our teams.</p><p id="""">To kick it off, let’s look at some of the biggest trends in software development today.</p><h3 id="""">What Existing Software Trends Will Continue?</h3><p id="""">To determine what will happen with IDPs, we must first consider the environment our developers work in. By following development trends, we can shape how IDPs need to expand to meet future needs.</p><h4 id="""">⚙️ Increased Complexity in Distributed Systems</h4><p id="""">First, we can expect software development to continue to grow in complexity. Distributed systems will continue to grow and become even more distributed.</p><p id="""">Why is that?</p><p id="""">To start, organizations will continue to utilize microservices and serverless systems as they attempt to compartmentalize their products’ complexities within well-defined boundaries. Even monolithic tech stacks don’t provide much value within a vacuum and require other systems and services to function.</p><p id="""">With more services to manage and integrate, dev teams face more and more complexity in developing, testing, debugging, and understanding their systems. To simplify that complexity through automation, IDPs will continue to streamline their day-to-day activities.</p><h4 id="""">🤖Reliance on AI</h4><p id="""">Next, let’s talk about our growing dependence and reliance on AI. We should consider this reliance to be a positive trend, and not one that takes the purity of software development away from our developers. Organizations increasingly leverage AI to enhance their software products, from automating repetitive tasks to optimizing performance. IDPs can harness the power of AI to augment the developer experience significantly.</p><p id="""">Using AI as part of software development is the next step in taking tedious or basic tasks and simplifying software development. Similar to when developers all moved to IDEs, organizations are embracing tools to keep their development orgs efficient and productive.</p><p id="""">Tools like <a href=""https://github.com/features/copilot"" id="""">Copilot</a>, <a href=""https://www.tabnine.com/"" id="""">Tabnine</a>, and <a href=""https://aws.amazon.com/codewhisperer/"" id="""">CodeWhisperer</a> will grow in functionality, and new competitors will join the market. New tools like <a href=""https://release.ai/"" id="""">release.ai</a> will take those capabilities a step further, allowing development teams to interact with not just their code but also their infrastructure in a smooth and natural fashion. Teams already use tools like ChatGPT to automate the writing of documentation. Forward-thinking orgs will integrate these functionalities throughout the developer workflows through the IDP. Handwritten wikis and instructions will be replaced by AI-driven tools that provide the latest information with considerably less searching.</p><h4 id="""">🚧 Establishment of Platform Teams</h4><p id="""">Finally, more organizations recognize the importance of dedicated platform teams responsible for delivering IDPs and integrating the various components.</p><p id="""">These internal product teams build, test, market, and sell your IDP to your development organization. They play a vital role in providing developers with the tools they need to excel in their work.</p><p id="""">The trend of building these platform teams will continue as their work leads to more efficient and productive development workflows.</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1000px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1000px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/6541669603a6566fee792bb9_TWZ9AsCSOLt20AmyTj2-e-nJV0QM5ah5Ps8_OKFSl2hE8smyrZlnk1tSCWQA1MskVUm50VQ2g_cHcCOsHERz3NPzj5FqXEdU52sS6mAAMmjilfbrhwUwwtf-W_lpCqVzxekfcE3uKo1yMQtaWQf3hNV0kGFxpVpfVX5ZaXCxGiKm_4i4I9TczfYeWZ-nJw.png"" id="""" width=""auto"" height=""auto"" alt="""" loading=""auto""></div></figure><h3 id="""">What IDP Trends Will We See?</h3><p id="""">Keeping in mind the development trends mentioned above, where should we expect IDPs to grow next?</p><h4 id="""">🦾 More AI Integrations</h4><p id="""">As we already hinted, incorporating AI within IDPs will become more pervasive, improving the developer experience in several ways. Let’s take a look at a few of those: </p><h5 id=""""><strong id="""">:: AI-Written Documentation</strong></h5><p id="""">AI will generate comprehensive documentation for internal services, development practices, and standards. This documentation will always be up to date, ensuring developers can access the latest information without hours of searching wikis, docs, and internal organization sites.</p><p id="""">These docs, integrated throughout the developer workflow, will give our development teams the context they need when they need it.</p><p id="""">Eventually, this AI-written documentation will extend to externally facing docs. At first, individuals will still review these docs, ensuring that the information is appropriate for external audiences. But eventually, with the right prompts and guardrails, you won’t need to spend human time and resources on documentation at all.</p><h5 id=""""><strong>:: Automated Code Reviews</strong></h5><p id="""">We already have some automated code checking through our IDEs and code linters. But is that it? Can we expect more?</p><p id="""">Of course we can! AI-driven code review tools will further our capabilities, using context and `information about the entire system to point out potential flaws or to improve readability.</p><p id="""">This will also help identify bugs faster and suggest potential fixes.</p><h4 id="""">🌎 Improved Environments</h4><p id="""">Next, let’s talk about environments. The future of IDPs will drive improvements in developer and production environments, making development more efficient and responsive to real-world scenarios.</p><h5 id=""""><strong>:: Emulating External Dependencies</strong></h5><p id="""">Due to our growing complexities, our systems depend more on other tools, data stores, and APIs.</p><p id="""">Right now, developers who require complex external dependencies to validate changes experience significant setup and toil to mock those dependencies. Whether mocking interactions between microservices or mocking APIs to infrastructure tools like storage and queues, it’s time consuming and difficult to create realistic test and developer environments that have all the working interactions that you’d expect of production. </p><p id="""">In the future, IDPs will improve our environments to make them efficient and reliable. We’ll evolve beyond basic emulation of our dependencies and instead seamlessly mock external dependencies with minimal configuration.</p><p id="""">There won’t be a question of whether something will work in production, because when it works on my machine, you’ll know it’ll also work in production.</p><h5 id=""""><strong>:: Scalability and Provisioning</strong></h5><p id="""">Though we don’t like to admit it, many organizations still manually scale environments to accommodate changes in load—either the upcoming expected load around a new feature launch or surprise loads coming from unexpected use or nefarious attacks.</p><p id="""">Future IDPs will fully automate this process, integrating cloud tools, load forecasts, and real-time monitoring.</p><p id="""">For example, in the future, we can expect IDPs to use predictive algorithms to anticipate increased (or decreased) load and scale environments accordingly. As another example, ephemeral environments can start up with the right amount of resourcing needed for their use. Environments used for simple testing will have minimal resources, while load test environments will have enough oomph to replicate what we’d see on a node in production.</p><p id="""">Do you want to tie this into expected cost calculators and better reporting? Sure, why not! Teams will be able to track their infrastructure spend and receive alerts when trends indicate that they’ll exceed their budgeted amounts. Did your team forget to scale down environments after some load testing? No problem. They’ll soon get an email or a Slack message to their team channel so they can make changes as needed.</p><p id="""">Ultimately, anything we’re still thinking through as engineers can be automated with the right context and expectations.</p><h5 id=""""><strong>:: Production Bug Replication</strong></h5><p id="""">Production bugs can be difficult to fix. Sometimes it’s hard to understand what’s really going on in our distributed systems. Also, replicating the bug locally can be close to impossible if we don’t fully know why it’s occurring.</p><p id="""">With the right IDP integrations, we won’t just have tools like <a href=""https://sentry.io/welcome/"" id="""">Sentry</a> or <a href=""https://www.bugsnag.com/"" id="""">BugSnag</a> notifying teams of production exceptions. We’ll integrate AI components to provide context and potential fixes. </p><p id="""">And if that doesn’t do it, the IDP can stand up an environment emulating that particular defect. In fact, if you’re using Release in production and can use that data to seed other environments, you could do this today.</p><p id="""">To make it even smoother for the development team, they’ll have that environment hooked up to their IDE, so they can quickly debug and test potential fixes, all with little overhead.</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1000px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1000px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/65416696447e033677d90b7f_2xB6daLJaKwhLxxvLmWCWnfA6UPpP9PWDjkWfD-Hv4ASUIuL6d2rewiIs8xZQ-R4sQjidOq4a0WRxTyq-RM_JOG7JyK8ev-Oz2d_4yZCGW-fw_qdf3IHSt0QYpEeaoQeCuBUWLuBkD6jgKpkahVEJmd8a11z8SOaH32UfNZEu7D_hDH35wGVhyOQLt9CJA.png"" id="""" width=""auto"" height=""auto"" alt="""" loading=""auto""></div></figure><h5 id=""""><strong>:: Performance Optimization</strong></h5><p id="""">Next, let’s talk about performance.</p><p id="""">We’re still in a distributed environment where fully understanding performance bottlenecks and constraints can be difficult.</p><p id="""">In the future, IDPs can integrate components that provide performance analysis throughout the dev workflow. We’ll be able to empower developers with tools to optimize performance during development and before we get to production.</p><p id="""">Using ephemeral environments, you’ll be able to quickly generate production-like traffic and use cases, enabling rapid testing and validation of functionality.</p><p id="""">Imagine a scenario where an IDP not only measures code performance but also simulates realistic user interactions and data loads. Developers can proactively identify and address performance bottlenecks early in the development process.</p><h4 id="""">🤝 Increased Collaboration Between Disciplines</h4><p id="""">One difficulty in almost any organization involves creating highly coupled collaboration between disciplines. A lot of this comes from different workflows, ways of working, and tools.</p><p id="""">In the future, IDPs will bridge the gap between disciplines, fostering seamless collaboration by integrating more than just developer tools. IDPs will enable communication across the organization of the right data at the right time, similar to our body’s complex nervous system.</p><h5 id=""""><strong>:: Integration with UX</strong></h5><p id="""">To improve collaboration with UX, IDPs will soon integrate with UX tools like Figma and Miro, providing developers with easy access to validate workflows and UX for their work without hunting for flows and images.</p><p id="""">Once deployed, UX can receive automated notifications and links to ephemeral environments to validate the customer experience.</p><h5 id=""""><strong>::&nbsp;Integration With Product Management</strong></h5><p id="""">IDPs will integrate more closely with product management tools as well, providing real-time status updates that encompass not only pull requests but also projections on work completion. This integration will enable better estimations based on historical data.</p><p id="""">Product managers can make more informed decisions based on real-time data.</p><h5 id=""""><strong>:: Data Science and Data Warehouse Integration</strong></h5><p id="""">Right now, whenever production schemas are updated, there’s often a separate task for either developers or data engineers to sync up the schemas between production and the data warehouse.</p><p id="""">If this isn’t automated in any way within the workflow, we end up with missing data in our data warehouse, and our models fall out of sync.</p><p id="""">Future IDPs will automate data engineering and data warehouse tasks by efficiently funneling changes from developers to the data warehouse. They’ll completely automate some tasks while also signaling for help when automation isn’t enough.</p><p id="""">This automation will reduce the manual effort required to update schemas in response to code changes.</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1000px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1000px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/65416696e96d1f58997e5a5d_tJTTuM64Ap9GDxwsg6OoEhdUbRzUTMQhUsSOy_mNfaG8vGEfkYXb6jLsy4U2zRZtgCjso45Uy9rT75sQjfu9u9ckbZbtOq4yHcEghHvQlRwyWcYwIH97z-IHdRDtzffioRgMd5PReGhOIRiHY-_kJPdEJmWMNlmrBD4hMMmo5xrxQka3s6JiB8dnMBQBqg.png"" id="""" width=""auto"" height=""auto"" alt="""" loading=""auto""></div></figure><h3 id="""">Where Do We Go From Here?</h3><p id="""">For developers, complexity will continue to increase in our environments. At the same time, developers will need to expand the knowledge they need to work with that complexity.</p><p id="""">IDPs that follow the current trends will continue to simplify development workflows either through AI capabilities or through adding spokes on the IDP wheel of components. These IDPs will develop into an all-encompassing and ever-changing platform based on development needs.</p><p id="""">What does that mean for us?</p><p id="""">If your organization already has a robust internal developer platform in place, you can begin expanding into future trends by experimenting with the ideas above. </p><p id="""">On the other hand, if your organization is just beginning its IDP journey, it’s not too late. In fact, it’s a great time to get started. In order to capitalize on the changes headed our way, we need to begin building a solid base of automation today.</p><p id="""">Either way, our priority should direct our efforts toward improving our IDPs to solve our most pressing problems today while leaving room for future integrations.</p><p id="""">‍<em id="""">This post was written by Sylvia Fronczak. </em><a href=""https://sylviafronczak.com/"" id=""""><em id="""">Sylvia</em></a><em id=""""> is a software developer who has worked in various industries with various software methodologies. She’s currently focused on design practices that the whole team can own, understand, and evolve over time.</em></p>",https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/654168c2511c1c513a1f7526_PE%20%236%20trends.jpg,DALL-E genrated image,sylvia-fronczak,10,Wed Nov 01 2023 18:00:00 GMT+0000 (Coordinated Universal Time),platform-engineering,
5 Ways To Improve Developer Velocity Using Ephemeral Environments,improve-developer-velocity-with-ephemeral-environments,62aa5a70cd5ba27d9d0d718a,62aa5a70cd5ba2b50c0d72d7,Thu Feb 25 2021 01:44:25 GMT+0000 (Coordinated Universal Time),Wed Oct 30 2024 19:51:37 GMT+0000 (Coordinated Universal Time),Wed Oct 30 2024 19:51:37 GMT+0000 (Coordinated Universal Time),Learn how to measure/improve developer velocity,"<h3 id=""""><br></h3>",true,<p>Boost development velocity—try Release's ephemeral environments.</p>,https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=5-ways-ephemeral-envs,"<h3 id="""">What is Developer/Development Velocity?</h3><p id="""">Velocity is a measurement of how many story points a software development team can finish within a sprint (usually one or two weeks). These points are set by the software development team when they review a ticket and estimate how complex the ticket is. When a team measures this output over a period of time, generally they have a consistent amount of story points they can deliver in a sprint and their velocity is known.</p><p id="""">Improving developer velocity is directly correlated with performance. <a href=""https://www.mckinsey.com/industries/technology-media-and-telecommunications/our-insights/developer-velocity-how-software-excellence-fuels-business-performance"" target=""_blank"" id="""">McKinsey published an article in April 2020</a>, where they cite that companies in the top 25% on their Developer Velocity Index grow up to twice as fast as companies in their same industries. Intuitively this makes sense since delivering more allows the development team to learn through iterating and improving.</p><p id="""">One might argue that velocity alone doesn’t make for great software, but assuming a development team is aware that quality is important, one can see how velocity usually helps. The ability to deliver quickly also allows a development team to address quality issues quickly. It’s easy to argue that development teams with high velocity have the ability to deliver better quality software because they can address issues quickly.</p><p id="""">In the same study, McKinsey highlighted several factors that allow a software development team to move quickly. Specifically they highlight that Technology Tools are an incredibly important dimension to velocity and business outcomes. And the most important tools are: Planning, Collaboration, Development and DevOps tools.</p><p id="""">In this post I’m going to discuss the <strong id="""">top 5 ways Ephemeral Environments can improve developer velocity</strong> by touching on how they are a <em id="""">Collaboration</em>, <em id="""">Development</em> and <em id="""">DevOps</em> tool. As we’ve spoken about in our article <a href=""https://releasehub.com/ephemeral-environments"" target=""_blank"" id="""">“What is an Ephemeral Environment?”</a>, ephemeral environments are spun up on demand and contain the code and data that approximates production closely. These environments are used by development teams in the software development process to test, debug and ensure features are built correctly before code is pushed to production.</p><h3 id="""">Here are the top 5 ways ephemeral environments can be used to improve developer velocity</h3><h4 id="""">1. Ephemeral environments are a DevOps tool designed to remove the staging or QA environment bottleneck</h4><p id="""">Traditional pre-production ecosystems usually have a limited amount of environments for developers. The staging or QA environment is generally used as a step before production where all code is merged and tested. Most organizations have one or very few of these environments, so as the organization grows these environments become a bottleneck in the process as all code must be tested here before production.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1822px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1822px""><div id=""""><img src=""https://cdn.prod.website-files.com/603dd147c5b0a4221d1bd360/603dd147c5b0a437111bd4b6_Screen_Shot_2021-02-22_at_2.43.18_PM.png"" alt=""Example of ephemeral environments for each branch"" width=""auto"" height=""auto"" loading=""auto"" id=""""></div></figure><p id="""">With ephemeral environments, the traditional idea of “staging” is gone. Every feature branch is contained in its own isolated environment and becomes its own integration environment. There is no longer a need to have a single testing and integration environment where all code must merge before going to production. With ephemeral environments you have a limitless supply of environments for any purpose.</p><h4 id="""">2. Ephemeral environments are a collaboration tool designed to allow for “early and often” feedback</h4><p id="""">Feedback is the lifeblood of great products. If you’ve ever read <a href=""https://www.amazon.com/High-Output-Management-Andrew-Grove/dp/0679762884/"" target=""_blank"" id="""">Andy Grove’s book on high output management</a>, you know he does an amazing job of discussing how rework is so costly. If you haven’t read this book, I highly recommend it, even if all you read are the first few chapters where he discusses trying to cook a high quality egg repeatedly, in under three minutes. In summary, Andy suggested through this analogy that finding issues/defects early in the egg cooking process is the most important part of consistently cooking a high quality egg in under three minutes.</p><p id="""">Likewise in software development, getting feedback and finding quality issues early in the development cycle reduces costly rework and improves velocity. If a product is delivered to a customer that doesn’t work or has bugs, it has to be reworked and go through the entire process again. Or if a product manager or designer doesn’t have a way to see changes until an engineer is finished with development, there is a high likelihood they will spot something wrong and rework the solution. These are all examples of rotten eggs in the process that hamper developer velocity.</p><p id="""">With ephemeral environments, rework can be minimized because stakeholders become a part of the development process. When an ephemeral environment is created, URLs to the environment are created so stakeholders can see progress while code is being developed.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1726px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1726px""><div id=""""><img src=""https://cdn.prod.website-files.com/603dd147c5b0a4221d1bd360/603dd147c5b0a4d2711bd4b8_Screen_Shot_2021-02-22_at_2.40.44_PM.png"" alt=""Links in the PR"" width=""auto"" height=""auto"" loading=""auto"" id=""""></div></figure><p id="""">At <a href=""https://releasehub.com/"" target=""_blank"" id="""">Release</a>, we highly recommend to our customers that they create a PR as soon as a developer starts working on a feature so a Release Ephemeral Environment is automatically created. When the developer pushes code to their source control system, the environment is updated making it a live reflection of the feature during development. Product managers, designers and QA are automatically notified when changes are live and they can preview those changes and give feedback immediately.</p><p id="""">At Release, we will also share our own ephemeral environments with our customers as we’re building a feature so we can get feedback directly from the people we’re making the software for before we release it to production.</p><h4 id="""">3. Ephemeral environments can limit rework and thus increase developer velocity.</h4><p id="""">Ephemeral environments are a developer tool that allows for full integration and smoke testing on isolated features</p><p id="""">Traditional continuous integration (CI) is the idea that your developer process should constantly be testing as a developer pushes code. What this leaves out many times is that most CI systems only perform unit tests continuously. Unit tests are meant to test small units of code and not the entire system as a whole. Integration and Smoke tests are where full paths of user experience can be tested. Usually Integration and Smoke tests are left to be tested only when the code makes its way via a merge to the mainline code branch and a traditional staging environment.</p><p id="""">Again, if we refer back to Andy Grove’s three minute egg analogy, this step of running Integration and Smoke tests only when the code branch is merged to the mainline is extremely late in the process. If issues are found during Integration and/or Smoke tests, the developer has to start the development cycle again from the beginning after finding this issue too late in the process.</p><p id="""">To add to the issue, if a team only has a single staging environment, the bottleneck around this staging environment is exacerbated with developers waiting for Integration and Smoke tests to be run on this single environment. On top of this, many code changes/features/branches may have been a part of the mainline merge making finding the cause of failed Integration/Smoke tests difficult and time consuming.</p><p id="""">With ephemeral environments, Integration and Smoke tests can be run when the ephemeral environment is created for a feature branch. This ensures that Integration and Smoke tests are run as frequently as unit tests so developers can find issues early in the process. Additionally, Integration and Smoke tests run against a single feature change/branch will isolate changes against the mainline and make finding the root cause much easier.</p><h4 id="""">4. Ephemeral environments are a DevOps tool that allow for experimentation with infrastructure</h4><p id="""">Making changes to infrastructure is hard and when a developer introduces the need for an infrastructure change it’s costly in time across the board. In a traditional environment setup (without ephemeral environments) this will result in an overall slow down in developer velocity as the shared staging environments must be updated by the DevOps team so the developer has some place to test their changes and new infrastructure.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1456px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1456px""><div id=""""><img src=""https://cdn.prod.website-files.com/603dd147c5b0a4221d1bd360/603dd147c5b0a493e31bd4c2_Screen_Shot_2021-02-22_at_2.44.56_PM.png"" alt=""Experiment with environment configuration"" width=""auto"" height=""auto"" loading=""auto"" id=""""></div></figure><p id="""">With ephemeral environments, this testing can be done in isolation and does not impact any other developer. For instance, with Release Ephemeral Environments, a developer can add services, environment variables, new infrastructure dependencies, new datasets/databases on their own through use of environment templates (environments as code) to experiment and develop without interfering with any other developers work or environments. This results in higher developer velocity again through minimization of rework and bottlenecks on shared resources.</p><h4 id="""">5. Ephemeral environments are a collaboration tool designed to be an agile/scrum catalyst</h4><p id="""">Many organizations have made the move to Agile/Scrum but their infrastructure and technology haven’t adapted to support a more iterative approach to building software. The entire premise of Agile/Scrum is for teams to be empowered and driven by early and often feedback. If your organization is on Agile/Scrum and you’re still using a single or few staging environments, you’re technologically hampering your process improvements. Ephemeral environments are the homes and office buildings where agile teams live, work, build, and play.</p><p id="""">Ephemeral environments are a catalyst to the Agile/Scrum methodology. When a developer does a pull request the ephemeral environment is created and collaboration on the feature can begin. The team is free to iterate, share, nand solicit feedback all while keeping the rest of the organization freely moving with their own ephemeral environments. Stakeholders are a part of the development process and true customer driven development, which is the heart of the Agile/Scrum methodology, can occur.</p><h3 id="""">Conclusion</h3><p id="""">Ephemeral environments turbo charge development velocity by eliminating bottlenecks in the process (DevOps Tool), including stakeholders in the process (Collaboration Tool) and improving product quality (Developer Tool). All of these factors were highlighted in the McKinsey report on developer velocity as critical and ephemeral environments are <em id="""">an investment that will put your organization in the top 25%</em>.</p><p id="""">Photo by <a href=""https://unsplash.com/@maicoamorim?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText"" target=""_blank"" id="""">Maico Amorim</a> on <a href=""https://unsplash.com/@maicoamorim?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText"" target=""_blank"" id="""">Unsplash</a>.</p><h3 id="""">Additional Resources</h3><ul id=""""><li id=""""><a href=""https://releasehub.com/blog/increase-developer-velocity-by-removing-environment-bottlenecks"" id="""">Increase Developer Velocity by Removing Environment Bottlenecks</a></li><li id=""""><a href=""https://releasehub.com/ephemeral-environments"" id="""">What is an Ephemeral Environment?</a></li><li id=""""><a href=""https://releasehub.com/staging-environments"" id="""">What is a staging Environment?</a></li></ul><p id="""">‍</p>",https://cdn.prod.website-files.com/603dd147c5b0a4221d1bd360/63e3fd360fbe35293df90b82_022221%20(1).jpg,Two athletes cycling in high speed representing the developer velocity,tommy-mcclung,6,Tue Feb 23 2021 00:00:00 GMT+0000 (Coordinated Universal Time),,
Improving Developer Productivity with Ephemeral Environments,improving-developer-productivity-with-ephemeral-environments,62aa5a70cd5ba27d9d0d718a,62aa5a70cd5ba22b290d72e7,Thu Nov 11 2021 00:52:43 GMT+0000 (Coordinated Universal Time),Wed Oct 30 2024 19:48:18 GMT+0000 (Coordinated Universal Time),Wed Oct 30 2024 19:48:18 GMT+0000 (Coordinated Universal Time),Release Environments can help improve Developer productivity.,,true,<p>Streamline development with on-demand ephemeral environments—try Release today.</p>,https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=ephemeral-env-productivity,"<h3 id="""">Introduction</h3><p id="""">There seems to be an insatiable demand for software; indeed there is an app for everything these days. In many ways, modern software development is more akin to a continuous manufacturing process, where products are frequently produced and shipped. Software companies are constantly iterating on new product features. User expectations demand constant improvements ranging from bug fixes to expanded product offerings.</p><p id="""">The Developer process for <em id="""">HOW</em> software is shipped is vital for maintaining a competitive edge. For example, having access to rapid-prototyping test environments is a critical process that can greatly enhance the overall Developer Experience. <strong id=""""><em id="""">At Release, our mission is to enable companies to get their best ideas to the world quickly by helping them produce consistent, reliable, and plentiful Environments on demand.</em></strong></p><p id="""">Additionally, having an overall improvement strategy for streamlining the Developer Experience is crucial for eliminating any internal friction that may arise. There are many different approaches for conducting process improvement, ranging from formal methodologies to informal ad hoc projects. All approaches are valid and can add value depending on your company culture. In recent years, the Lean Manufacturing philosophy has become a popular approach for analyzing and improving internal work processes.</p><h3 id="""">Brief Lean Manufacturing History</h3><p id="""">Prior to pivoting my career toward software development, I lived another life as an industrial engineer facilitating Lean Manufacturing projects and workshops where the goal was to leverage employee ingenuity to identify and creatively eliminate any waste in a process.</p><p id="""">The Lean Manufacturing philosophy has a rich history that can be traced back to Walter Shewart’s statistical quality control methods at Bell Laboratories in the early 20th century. W. Edward Deming learned and enhanced these techniques from Shewart. After WWII, Deming was called upon to help rebuild Japanese industry and he championed this thought-process of continuous improvement through statistical analysis. Companies like Toyota embraced this approach and continued to enhance it where it gradually morphed into the Lean philosophy that many of us are familiar with today.</p><p id="""">Lean Manufacturing can now be found in other non-manufacturing sectors such as healthcare, banking, government, software development, etc. Essentially, all work consists of a “process” that can be continually improved upon, regardless of the product or work environment. Continuous improvement enables the employee to be more successful by standardizing or automating routines, eliminating blockers, and harvesting worker ideas for improving their own work. Many software companies do this naturally and they may or may not call it “kaizen”, which is the Japanese word for continuous improvement. There are agile scrum retrospectives where teams debrief on how their last Sprint went to see whether they can improve the overall developer experience. However a team approaches it, continuous improvement cannot be avoided.</p><h3 id="""">How Release Can Help</h3><p id="""">At many tech organizations, there is still the common bottleneck problem of generating test environments. Software companies typically employ a DevOps team to produce such environments which can be costly to maintain and frequently break down. Any shared resource can be a pain point for both large and small companies that are trying to rapidly ship new features. Developer teams must wait around for testing environments to become available or argue over priorities and who can utilize the environment.</p><p id=""""><strong id=""""><em id="""">Our entire mission at Release is to enable companies to get their best ideas to the world quickly by helping them produce consistent, reliable, and plentiful Environments on demand.</em></strong> This is a huge kaizen improvement opportunity that can greatly improve the overall Developer Experience by eliminating frustrating downtime and improving quality, throughput, and morale. If you wish to get your organization set up with automated Environments, let us know how we can help you accomplish your mission faster and easier.</p><p id="""">Photo by <a href=""https://unsplash.com/@lennykuhne?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText"" id="""">Lenny Kuhne</a> on <a href=""https://unsplash.com/s/photos/manufacturing?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText"" id="""">Unsplash</a></p><p>‍</p>",https://cdn.prod.website-files.com/603dd147c5b0a4221d1bd360/63e42176fc8cc0ea8b59f24b_111121%20(2).jpg,A car production line gaining productivity creating a ephemeral environment,sam-allen,2,Thu Nov 11 2021 16:45:00 GMT+0000 (Coordinated Universal Time),,
Increase Developer Velocity by Removing Environment Bottlenecks,increase-developer-velocity-by-removing-environment-bottlenecks,62aa5a70cd5ba27d9d0d718a,62aa5a70cd5ba218470d72fc,Wed May 05 2021 23:48:15 GMT+0000 (Coordinated Universal Time),Wed Oct 11 2023 20:32:20 GMT+0000 (Coordinated Universal Time),Wed Oct 11 2023 21:48:44 GMT+0000 (Coordinated Universal Time),"In our latest whitepaper, we show how to increase developer velocity by 35% using Environments as a Service.","<h3 id="""">Remove Environment Bottlenecks</h3><p id="""">We’ve all heard the phrase “time is money” and we intuitively know this statement to be true, but understanding just <em id="""">how</em> much money is spent on labor can be a tricky thing to estimate. This is especially true with complex operations like software development.&nbsp; Before I learned how to write code in React/Node JS, I was an industrial engineer for many years and spent time studying this topic at university.<br></p><p id="""">Industrial engineering is a systems-thinking discipline that is obsessed with figuring out how to optimize resources and improve processes to get the most out of a system. It borrows from other fields like economics, project management, mechanical engineering, and statistics, to name a few, and lies at the intersection between business operations and engineering.&nbsp; Quality, Cost, Schedule, and Safety can all be measured and quantified with incremental improvements made across each category.<br></p><p id="""">These topics can be easy to grapple with when dealing with a consistent, repeatable process like a manufacturing assembly line, hospital queue, or restaurant. However, wrestling with non-standard operations like software development can be nebulous, abstract, and difficult to shove into a one-type-fits-all solution. But that doesn’t mean that we shouldn’t attempt to understand it.&nbsp; Any attempt at understanding and gathering data is still incrementally better than remaining ignorant and relying on gut-intuition alone.</p><h3 id="""">The Problem</h3><p id="""">As I started to learn how to write software applications a couple years ago, I had high hopes of perhaps crossing my industrial engineering and project management skills into the realm of software.&nbsp; Gradually, as I began to understand the Agile/Scrum approach, I realized it is challenging to estimate computer programming labor resources and it’s not a very good planning approach, especially in a start-up culture.<br></p><p id="""">You don’t have a blueprint, there are no bills of materials, there is no work breakdown structure or sequence of operations.&nbsp; Instead, it’s better to deal with chunks of hazy ranges, like “well, it could take a day or two, but less than a week” and then iterate toward a solution, biting off smaller chunks at a time.&nbsp; Precedence is still knowable in many cases and you can break the problem into smaller pieces, but estimating <em id="""">how</em> long it will take is not really worth figuring out because it doesn’t help you gain any ground toward solving the problem. Time estimation is purely an administrative task that will need to be repeated <em id="""">ad infinitum</em> because no two tickets are ever the same.<br></p><p id="""">Software development can have many unknowns which further complicates any attempts at labor estimation.&nbsp; The ‘Johari Window’ is a method for identifying known or unknown knowledge that a person and their surrounding organization may possess. Some things fall into the ‘known-unknown’ category which means you need to research something that you don’t know yet. But even worse, the ‘unknown-unknown’ realm often crops up, which is to say that you have no idea what is going on until you dive in and start to uncover hidden things.<br></p><p id="""">In software development, especially when trying something new and novel—like in a startup—there are many unknowns.&nbsp; Pioneering into uncharted areas takes an extra amount of time and effort when building a greenfield product, fixing bugs, doing user research, discovering go-to-market fit, and so on.&nbsp; As a company matures, some software development and ticket refinement might approach a stable steady-state, but in many cases, if the company continues to innovate, there will always be many unknowns.</p><h3 id="""">The Solution</h3><p id="""">So what are we supposed to do?&nbsp; We know that software development labor is costly, and in fact, is often the top operating cost for a tech company. It’s important to investigate and attempt to understand <em id="""">how</em> labor is allocated so we can begin to feel more confident about what we are willing to build or not.&nbsp; There are important decisions that many managers face: should we build something in-house using our own labor resources?&nbsp; Or can we get something off-the-shelf that can be customized to fit our needs?&nbsp; It would also be good to know whether a manager’s most precious resource is blocked with bottlenecks and being under- or over-utilized.&nbsp; Having a rough understanding of your labor resources can help make this type of decision much easier.<br></p><p id="""">One of the best ways to understand a complex system is to model it.&nbsp; We see this all the time when we watch the weather report on the news when the reporter stands in front of a weather map and gives a rough forecast using a computer simulation.&nbsp; Statistical modeling is now used in a variety of complex industries to make planning forecasts with many different input variables.<br></p><p id="""">It’s important to know that a model is just that: a simulation, a mock-up, an imaginary scenario.&nbsp; It’s not <em id="""">real</em>, just ask Morpheus in the Matrix.&nbsp; Every statistical model relies heavily upon baseline assumptions and measured, knowable, controlled inputs that can be adjusted for a range of possible outcomes.&nbsp; The more data you have, the more reliable the model becomes, but it has to start somewhere with a simplified version of reality broken up into discrete events built upon statistical averages.&nbsp;&nbsp;<br></p><p id="""">So what are some of the safe assumptions we can make about a typical tech startup?&nbsp; First, we would want to add boundaries to our system.&nbsp; We can fix the number of employees and their typical working hours.&nbsp; Tickets tend to vary widely, but it is possible to make different ticket types broken down into difficulty levels.&nbsp; Let’s say the easy ones are half a day, while the tougher ones might take a week.&nbsp; We also know the number of environments we have available for testing our code.&nbsp; These might be custom built, maybe there are 2 or 3.<br></p><p id="""">Some other knowable assumptions might be how long it takes to deploy our code to production and whether a certain amount of tickets will need rework after QA testing, let’s say 25%.&nbsp; It’s rather arbitrary, but in the absence of solid data, we can plug in some intuitive anecdotal numbers to start with.&nbsp; If you hold all things consistent, but only adjust one variable at a time, then you can begin to compare the results to uncover any major bottlenecks in the system.&nbsp; Models are a simplified version of reality, so we start really simply.&nbsp; To use a crude example, we could simulate 5 farmers working 8 hrs/day in a 500 acre field using 3 tractors, record what happens, then run it again with only 2 tractors instead for comparison.</p><h3 id="""">Simulation Results</h3><p id="""">In this blog post, we will share the end results of our analysis. For a full detail of the simulation setup and results, you can download our free <a href=""https://release.com/whitepaper"">whitepaper</a>.</p><blockquote id=""""><em id="""">Key takeaway: If we increase the number of available environments to 5 while keeping all other variables consistent, we saw the simulated throughput go from 12 to 39 tickets, more than a threefold increase.</em></blockquote><p id="""">After running the baseline setup with only a single staging environment, we can see the team can only complete 12 out of 42 tickets, but more importantly, we can identify a major bottleneck as 23 tickets are piled up waiting for an environment resource and waiting to pass thru the testing process. If we increase the number of available environments to 5 while keeping all other variables consistent, we saw the simulated throughput went up to 39 tickets, more than a threefold increase. The bottleneck has also been eliminated and there are even a few surplus environments available.</p><p id="""">Again, to read the full results and analysis, be sure to download the free <a href=""https://release.com/whitepaper"">whitepaper</a>.</p>",false,,,,https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63e4009c5f7d55c75adeb148_050621%20(1).jpg,Workflow of development environment with Increase on Developer Velocity by Removing Environment Bottlenecks,sam-allen,9,Thu May 06 2021 15:00:00 GMT+0000 (Coordinated Universal Time),,
Introducing Release AI: Talk to your Infrastructure ,introducing-release-ai-talk-to-your-infrastructure,62aa5a70cd5ba27d9d0d718a,64adbadc636a75cc1ef13bcb,Tue Jul 11 2023 20:26:04 GMT+0000 (Coordinated Universal Time),Tue Jul 11 2023 20:26:04 GMT+0000 (Coordinated Universal Time),,,,false,,,,,,,,,,
"Introducing Release Share, a Docker Desktop Extension",introducing-release-share-a-docker-desktop-extension,62aa5a70cd5ba27d9d0d718a,651b286a6a8005ab9c90ea86,Mon Oct 02 2023 20:30:34 GMT+0000 (Coordinated Universal Time),Wed Aug 07 2024 14:37:35 GMT+0000 (Coordinated Universal Time),Wed Aug 07 2024 14:55:11 GMT+0000 (Coordinated Universal Time),Easily share your containers and apps with collaborators and reviewers using Release Share Extension in Docker Desktop,"<p id="""">At Release, we like to move fast and break fewer things. So we use ephemeral environments a lot. They're like playgrounds where we build and test things before moving them to the real world. This way, we can quickly see if something works (or doesn't) and fix it early. And since sharing what we make is a big part of this process, we thought: why not make sharing easier?</p><p id="""">So, we made<a href=""https://hub.docker.com/extensions/releasecom/docker-extension"" id=""""> Release Share</a> for Docker Desktop. It's a simple tool to help you share your containers. Just one click and you get a public URL, which you can even customize if you like.</p><h4 id=""""><strong id="""">What Release Share does:</strong></h4><p id="""">🔗 <strong id="""">Make Your Own URL</strong>: Change the link name to whatever you like.</p><p id="""">▶️ <strong id="""">List Live Containers</strong>: If it's running and has exposed ports, you can share it.</p><p id="""">✅ <strong id="""">Simple Sharing</strong>: Send the link to anyone, and they can see your app in their browser</p><h4 id=""""><strong id="""">How to Use It:</strong></h4><p id=""""><strong id="""">1.</strong> Start Docker Desktop (or download the latest version <a href=""https://www.docker.com/products/docker-desktop/"" id="""">here</a>).</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/651b74ddea2f99f601d958be_usxygGHqt8lzeU0lTohoFD7KfsfmZxxbVzu6vnhi97O-UZt9cA1o3XqwVgZHE9KMo5BHiR8i3u_1gIFUeXLc63SII7KmWicOOd4EAnAuCHqLa4lgs9witLZ-8b5q3nLK3j_rBFYJ_OQpb0rcEeAAhnI.png"" id="""" width=""auto"" height=""auto"" alt="""" loading=""auto""></div></figure><p id=""""><strong id="""">2.</strong> Go to Extensions Marketplace, find <a href=""https://open.docker.com/extensions/marketplace?extensionId=releasecom/docker-extension"" id="""">Release Share</a>, and install it.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/651b74dd616124d1aaa74d20_I1hJGhZhSyFhOFU7eqAyVpKAFiWIrBYJG22U3U0QhsHDre8n9iY_FwZOE0CX5rkl3GQELH8paWO8osShrmtTW_eljVG3RinY8eZUIx_Lj6QO5kV8gz0YzcC4lxbNwsWun-hAw01h7r2OEpLWXKNZiPY.png"" id="""" width=""auto"" height=""auto"" alt="""" loading=""auto""></div></figure><p id=""""><strong id="""">3.</strong> Click on “Get Started” on the welcome screen (tip: you can always get back to this screen by clicking “About” in the top right corner).</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/651b74dd37d53bdb0414fb88__QdyHk_F1hmDHMYB2QZpzP64ijSTgDTac2fUem4Wa_5gvfGG4pXwW8Q-Wk0XcJB22iXfEnZ8Whi2SckMawpD-w0ZxIsc7yKL1d-XZsE2AsqzcVYGaMBvMZrjT_LTBsJ8flSzyoQV1gQo2dfSdUTT5q4.png"" id="""" width=""auto"" height=""auto"" alt="""" loading=""auto""></div></figure><p id=""""><strong id="""">4. </strong>Release Share looks for your active containers and lists all that have exposed ports and can be shared externally. Choose one, click “Connect” and Release Share will create a private tunnel for your container with a shareable link. </p><p id="""">Note that you can also see recent inactive containers by toggling the “Show containers without exposed ports” switch. </p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/651b74dda3255cf8cae21fa3_O_DBEnL7XKgCSy6LgHI5wda2KPAz9bDtEU_tMTZww7w4m9q4DnFAbRNEx59RNJPWrgPC6AUuUypjIvAE-j9775UCQUHSlRIZ0CxkBYmXMqVsqX3vIDXpYch354vT2KdBt-98uP-bfRc9hlva1xoPme8.png"" id="""" width=""auto"" height=""auto"" alt="""" loading=""auto""></div></figure><p id=""""><strong id="""">5.</strong> The standard URL looks like <a href=""https://random-name.rshare.io/"" target=""_blank"" id="""">https://random-name.rshare.io/</a>. Copy and share the URL and the recipient can preview your container or app in any browser.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/651b74dd99ce542198526939_ScIqoicnrdlK1x16BY3Gpl1cqClR8gd9NNoNmYj2rArcmNuUa_VRL2K7alaod9iwP0aFcgTxNYmN3E0tZMAnfLj1GNo6NeatDV-17K3ZnK-VTc-1Ayv2ctFVoAs18F6WoyWEW8Shsi-SrB_5nlcv8Ps.png"" id="""" width=""auto"" height=""auto"" alt="""" loading=""auto""></div></figure><p id="""">Like this:</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1330px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1330px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/651b74dd72b771d42c568e8e_L1GfVf1gTACTeEA_EsJu9g1n2byWGPSMeRHex_XdOnQBtYYDlEHnqmszh_xFd8-6p5TkobVqKECDRxgPe6Raj0my2kRX3CgwUEH3lLSdohso_sK_jr_SbnUlwO2kqFzaWgNkpLW-PoYZEi3RpD0WyXo.png"" id="""" width=""auto"" height=""auto"" alt="""" loading=""auto""></div></figure><p id=""""><strong id="""">6.</strong> Want a different name? You can change it by clicking “Edit”. </p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/651b74dd2214daa45d933bfb_nd016aMVNZSP7UJ5V-dfhJ4Lpq9qz7NhKKKS__Z5XvEXOysW1Nx-mYeVTcN4VUv3SZCae_wpsH4KU_7kzYHmV8m_7gXoy6pKiZxDHDIUZfUZ8MPfD7g7ZwH9vJ2ByaWWUI-j8O521GXwsWnaN3gXrTA.png"" id="""" width=""auto"" height=""auto"" alt="""" loading=""auto""></div></figure><p id="""">The system will check if the name is available and assign the new name to your public URL. At this point the old URL becomes disabled and whoever had it will not be able to see the contents of your container or app any more. This comes handy for version control or tagging specific changes or audiences.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/651b74dd5b7d5a16866e8a77_0ukiXVFqFcdxprD4KS8exQNCZmvXu6YFvZuy10HqQA5jIP8pDuHYEwdktmQCxePMM8zMHsqMPB4S3HtTC8Cp8AGuHj-VcKVkBm1cLLPD3Sg8aJ52hWZSbxxOy7r-NesjGeO-Kl9Dzv1eAGCmzem_L-Y.png"" id="""" width=""auto"" height=""auto"" alt="""" loading=""auto""></div></figure><p id="""">Your new URL will go live as soon as you hit save. </p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1318px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1318px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/651b74dd1e3e28d046b7d1e4_BT1neQlfuPu9GlriqooFVBLk17gceeufeB9gMQx213Ph9jRUl9oNCWxn9xI_zXq_YAQqp9kFK14mAGCoN5MzM42JxehJZdO09h_Feg5WGTwj7Jme1Z_Y1AkKclyQe4uVQqxf17XQ-PTo2TBgQiFMg9I.png"" id="""" width=""auto"" height=""auto"" alt="""" loading=""auto""></div></figure><p id=""""><strong id="""">7.</strong> When you're done sharing, click “Disconnect” to take your container offline. Note, the custom URL is saved for future use, so if you want to share the same container later, it will spin up with the last name used. </p><p id="""">This concludes our quick rundown of Release Share. Give it a try. If you've got suggestions or feedback, we're all ears. Find us at <a href=""https://www.dockercon.com"" id="""">DockerCon2023</a> or drop us a line. </p>",false,,,,https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/651b75952eef99312adfdb1d_Release%2BDocker.jpg,,ira-casteel,5,Tue Oct 03 2023 16:00:00 GMT+0000 (Coordinated Universal Time),docker; product,release-is-going-to-dockercon-2023
Introducing Standalone Instant Datasets: Build and Test with Realistic Production-like Data with Ease,introducing-standalone-instant-datasets-build-and-test-with-realistic-production-like-data-with-ease,62aa5a70cd5ba27d9d0d718a,64c038dfe0413edc89d77734,Tue Jul 25 2023 21:04:31 GMT+0000 (Coordinated Universal Time),Thu Jul 25 2024 15:06:32 GMT+0000 (Coordinated Universal Time),Thu Jul 25 2024 16:27:39 GMT+0000 (Coordinated Universal Time),Release Instant Datasets in now a standalone product allowing everyone to build and test with production-like data.,"<p id="""">At Release, we are strong advocates of using production-like data for all feature development. We build and test with production-like data daily and think you should too, that’s why we built the dataset replication capability directly into our platform. </p><p id="""">Understanding how your database's data integrity impacts your application's behavior and performance, as well as challenging your UI assumptions against production data, can significantly enhance every aspect of software development. This approach not only streamlines the development process but also brings potential issues and defects to light as early as possible, fostering high performance within your team and company.</p><p id="""">Today, we are introducing <a href=""https://release.com/product/instant-datasets"">Instant Datasets </a>as a standalone product. Now everyone can build and test with production-like data instantaneously, no exceptions! Whether you already use the Release platform for ephemeral environments, share environments, manage complex applications with multiple data stores, or run a simpler setup, Instant Datasets is now accessible to you. Anyone in need of realistic, up-to-date data for building and testing applications can take advantage of Instant Datasets within their own cloud accounts.</p><p id="""">Now, you may wonder why you would need Instant Datasets when you can manually create a copy of your snapshot and add it to your application. While that is a viable solution for obtaining production-like data, Instant Datasets offers a more comprehensive and efficient approach by managing the entire process for you. It generates a pool of instances that can be easily checked out and utilized by individual developers, product managers, test environments, staging environments, and any other user or use case you can imagine. With Instant Datasets, you can effortlessly create and manage multiple datasets based on various cloud databases.</p><p id="""">But what about the potential cost of maintaining all those available instances on your cloud account? Fear not! Instant Datasets has implemented several measures to protect you from the dreaded ""surprise cloud bill"" we've all experienced:</p><ul id=""""><li id="""">Each database comes with a user-defined default Time-to-Live (TTL) when created.</li><li id="""">Databases can be paused manually or on a schedule, helping you control costs effectively.</li><li id="""">The ""check-in/check-out"" process ensures that Instant Datasets cleans up after itself when you no longer need the data, further managing costs.</li></ul><p id="""">Although the ability to instantly use production-like data comes with some associated costs, the benefits in terms of application performance and experience make it well worth it. </p><p id="""">Now, you might wonder if you can share the databases with the rest of your team and whether they will see the changes you've made to your working copy. Rest assured, every checked-out database is exclusively yours to use and modify for as long as you need it. The moment you check one out, a replacement copy is instantly generated; and once you check your database back in, it is deleted to help you manage costs and maintain integrity of the data. Note that currently Instant Datasets works with AWS RDS and Aurora and we are adding new services soon. </p><p id="""">But what if you don't have a budget for another subscription? We've got you covered! For our initial user cohort, we are making standalone Instant Datasets absolutely free. We understand the pain of dealing with inaccurate seed data, and our aim is to make access to production-like data as easy as possible for everyone.</p><p id="""">Now, let's address the crucial matter of security. Your data is YOURS, and it remains securely stored within your cloud account. Release orchestrates and manages access to instant replicas without interacting with the actual data itself, ensuring a safe choice even for the most stringent security environments. Datasets are password-protected and you can further <a href=""https://docs.release.com/release-instant-datasets/security/aws-instant-dataset-security"" id="""">limit access</a> based on your specific needs. Additionally, we integrate with services like <a href=""https://www.tonic.ai/"" id="""">Tonic</a> to obfuscate data and create ""fake"" datasets that closely mimic production data without risking exposure to sensitive values. </p><p id="""">Finally, we built Instant Datasets for ourselves, using it as a scratchpad of sorts to gain early insights into real applications. By doing so, we've saved countless hours of rework and bug hunting, making our entire team happier and more productive. Now, we invite you to join us on our mission to embrace production-like data and give Instant Datasets a try for yourself. Check out the <a href=""https://docs.release.com/release-instant-datasets/quickstart"" id="""">Quickstart Guide</a> in our documentation and snag a<a href=""https://beta.release.com/instantdatasets/register"" id=""""> free account</a> while they last, and let's revolutionize the way we build and test applications with realistic data!</p>",false,,,,https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/64c036f0207505b0d29cfa93_Standalone%20Instant%20Datssets.jpg,photo credit: vox.athena,erik-landerholm,5,Wed Jul 26 2023 16:00:00 GMT+0000 (Coordinated Universal Time),news; product,syncing-databases-how-to-do-it-and-best-practices
"Build AI, Automate the Infrastructure. Join the Release.ai Technical Preview",join-the-release-ai-technical-preview,62aa5a70cd5ba27d9d0d718a,65f8a85b830242eacf1771c8,Mon Mar 18 2024 20:47:23 GMT+0000 (Coordinated Universal Time),Thu Jul 25 2024 14:57:55 GMT+0000 (Coordinated Universal Time),Thu Jul 25 2024 16:27:39 GMT+0000 (Coordinated Universal Time),"Learn about Release.ai, a cloud-agnostic, framework-agnostic, and compute-agnostic platform for AI infrastructure.","<p id="""">Today we are excited to launch the technical preview for <a href=""http://release.ai"" id="""">Release.ai</a> – a cloud-agnostic, framework-agnostic, and compute-agnostic platform for AI infrastructure. As the team here at Release was working on our own AI DevOps assistant, and comparing notes with teams who build, fine-tune, and integrate AI models into products, we saw a consistent challenge: the infrastructure is burdensome.&nbsp;</p><p id="""">All across the industry we see amazing innovations pop up daily, and underneath the surface to make it all happen are infrastructure teams toiling away at accumulating GPUs, managing finicky clusters, investigating the latest timeouts, grappling with latest framework peculiarities, and making their finance folks bug-eyed at the bills being generated. It does not have to be this hard.</p><p id=""""><a href=""http://release.com"">Release</a> has been abstracting away infrastructure complexity for conventional SDLC processes for years. Helping engineering teams focus on shipping high-quality products, faster. Now we bring our expertise to support AI engineering teams. To begin, we partnered with NVIDIA to simplify infrastructure setup and management of the <a href=""https://www.nvidia.com/en-us/ai-data-science/generative-ai/nemo-framework/"" id="""">NVIDIA NeMo framework</a>. Now you can use NeMo and Release to build and deploy models in a portable and cloud-agnostic way, on top of the Kubernetes clusters that we manage for you. This automates resource management, speeds up time-to-insight and gives AI development teams greater control over their infrastructure.&nbsp;</p><p id="""">While we are building Release.ai to simplify AI infrastructure, you don’t have to choose between simplicity and control - everything on Release.ai is orchestrated in your cloud account, on the hardware that you own, simplifying toil while maintaining control. Currently we support training, fine-tuning, and inference workflows with more workflows and frameworks to come.&nbsp;</p><p id="""">This is your opportunity to get in on the ground floor. <a href=""https://release.ai/"" id="""">Join our technical preview today</a> to experience our solution firsthand, provide invaluable feedback, and become a key design partner helping to shape the future of AI infrastructure tooling. Together, we can untether AI teams from infrastructure complexities so they can pour their energy into innovation.</p>",false,,,,https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/65fc7b19094502738685bcda_Frame%201171274982.png,Join the ReleaseAI early access program,michael-poon,3,Mon Mar 18 2024 20:45:00 GMT+0000 (Coordinated Universal Time),ai; product,
Why I joined Release,kelsey-degeorge-why-i-joined-release,62aa5a70cd5ba27d9d0d718a,62aa5a70cd5ba25c070d7309,Thu Feb 24 2022 03:14:43 GMT+0000 (Coordinated Universal Time),Wed Feb 08 2023 19:52:45 GMT+0000 (Coordinated Universal Time),Wed Feb 08 2023 22:48:51 GMT+0000 (Coordinated Universal Time),Exciting update on why CRO Kelsey DeGeorge decided to join Release,"<p>During my 5 year tenure at AWS, leading teams who supported B2B software companies, I had the opportunity to observe an extremely broad cohort of customers, challenges they faced, and opportunities for technology disruption. Companies were making substantial investments in the cloud - all at various phases. Some were in their infancy and needed guidance formulating a migration strategy, others were all-in seeking support to modernize their stack, and every customer in between. One commonality across these customers in various phases of the cloud journey is that they were all looking to achieve a similar outcome: offload undifferentiated heavy lifting of maintaining infrastructure to focus on what’s core to their business, their “special sauce.”</p><p>While the cloud offers building block services to ultimately achieve this ideal state, there is still a substantial burden on the teams responsible for managing cloud services. The burden wasn’t eliminated - it simply transferred. Teams previously in place to manage hardware became teams in place to manage cloud infrastructure. In many instances, companies invested <strong>even more</strong> in internal cloud teams (SRE, DevOps, etc.) than they did in the data center days. This is by no means a slight to the cloud. It is the only way companies can innovate at the pace necessary in today’s demanding market. However, I instantly recognized Release’s ability to address these challenges and maximize the value of the cloud’s agility, ease of experimentation, and cost efficiency.</p><p>With Release, instead of allocating massive amounts of resources to DevOps, engineers can focus on the core of the business. How, though? Release offers environments-as-a-service, containing the manifestation and execution of all of the code, data, infrastructure, settings and services needed to run any application. Environments can be ephemeral, short-lived for testing and branch-based development or permanent for environment needs such as staging and production.</p><p>Environments are critical to organizations because every member of product development is dependent on them in every step of building and delivering products. Oh and my fellow sales and GTM folks, I’m talking to you too. You might not know it, but you rely on environments every time you demonstrate your product offering to a customer (arguably the most critical stage of the sales cycle). We have on-demand environments for you too, so you never have to worry about the tech failing mid-demo (defeating Murphy’s law!). With Release, customers get environments for virtually every use case, best practice DevOps out of the box, instant data sets, development isolation, uniformity, and more.</p><p>Not only do I firmly believe in the company’s mission and their novel technology. I strongly believe in the founding team and have had quite a poetic journey with the three of them. It started back in 2017 when I supported Tommy, David, and Erik on a journey to migrate from several data centers to AWS. This was a multi-year effort requiring expertise from dozens of AWS resources, substantial SOW dollars, and months of training and enablement. I wish I could say this multi-year effort gave them everything we promised (agility, scale, innovation), but the migration was only the beginning. They spent years building a homegrown solution in an effort to become less reliant on DevOps resources and remain product-oriented. They recognized the need to give developers the independence to build, test, iterate, and re-iterate in their own isolated environments and to share those changes with others. Their mission to bring ideas to the world faster ultimately led to the inception of Release.</p><p>It has come an extraordinary way since then, but we’re still early. If you’re excited about joining a mission-driven company backed by the most credible investors in the world, you know where to find us. Spoiler alert: We’re hiring.</p>",false,,,,https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63e3fd85452a4475cff3ea83_022422%20(1).jpg,,kelsey-degeorge,3,Thu Feb 24 2022 16:50:00 GMT+0000 (Coordinated Universal Time),,
Kubernetes CRDs: What They Are and Why They Are Useful,kubernetes-crds,62aa5a70cd5ba27d9d0d718a,62aa5a70cd5ba25adf0d7306,Tue Feb 15 2022 22:12:00 GMT+0000 (Coordinated Universal Time),Fri Apr 21 2023 19:11:46 GMT+0000 (Coordinated Universal Time),Fri Apr 21 2023 19:37:52 GMT+0000 (Coordinated Universal Time),What are Kubernetes Custom Resource Definitions? What are they useful for and how to create your own? Read all about it ,"<p id="""">One of the main reasons that Kubernetes became so popular is the fact that it's so flexible. When we say Kubernetes, we typically think about deploying and managing containers. And while this is, in fact, Kubernetes's main job, it can actually do much more than that. This is possible thanks to something called Custom Resource Definitions, or CRDs for short. In this post, you'll learn what CRDs are and what you can use them for. We'll also take a look at how to create them.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1000px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1000px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/620c24f3863c2a628b190ffa_Kubernetes%20CRDspq01.png"" loading=""lazy"" width=""auto"" height=""auto"" id=""""></div></figure><h3 id="""">Kubernetes API</h3><p id="""">Before we dive into custom resource definitions, let's first talk about Kubernetes in general. If I asked you, ""What is Kubernetes?"" then you'd probably answer, ""Kubernetes is a container orchestrator."" This would, of course, be one correct answer.</p><p id="""">But by looking under the Kubernetes hood, you could see that the main component of Kubernetes is an API server and etcd data store. And there are other, more important components like kube-scheduler, kube-controller-manager, and cloud-controller-manager, but pretty much any operation on your cluster needs to go through an API server.</p><p id="""">That API has a few built-in objects that it understands. Things you may be familiar with like Pods, Namespaces, ConfigMaps, Services, or Nodes are all API objects. That's why whenever you execute <strong id="""">kubectl get pods</strong> or <strong id="""">kubectl get nodes</strong>, you get a list of pods or nodes. But if you try to get a list of objects that don't exist in the Kubernetes API—like, for example, <strong id="""">kubectl get biscuits</strong>—you'd get a response similar to this:</p><p id="""">error: the server doesn't have a resource type ""biscuits""</p><p id="""">And this is because there is no such thing as ""biscuits"" defined in the Kubernetes API. Quite logical, right? Well, what if I told you that you could add a biscuits definition to your Kubernetes cluster? In fact, you can extend your Kubernetes API with any custom object you like. That's exactly what custom resource definitions are for.</p><h3 id="""">Why CRDs?</h3><p id="""">So what's the point of adding a biscuits definition to your Kubernetes cluster? Remember when I mentioned earlier that the success of Kubernetes comes from its flexibility? The ability to extend the Kubernetes API with custom resource definitions is a really great feature that lets you do something magical. It allows you to instruct Kubernetes to manage more than just containers.</p><p id="""">Why is that such a great thing? Because CRDs together with Kubernetes operators give you almost unlimited possibilities. You can adapt Kubernetes in a way that it will take care of older parts of your infrastructure. If you do it right, you'll be able to avoid bottlenecks and easily modernise things that normally would require long and costly redesigns.</p><h3 id="""">CRDs on Your Cluster</h3><p id="""">Before we dive into creating our own CRD, you need to know two things.</p><p id="""">Firstly, creating a custom resource definition is an advanced topic. Many companies don't even need to create any CRDs. The Kubernetes community finds interesting solutions for common problems all the time, and it’s likely that any use case you encounter probably already has a CRD you can use! And if you're still new to Kubernetes, you definitely shouldn't jump into CRDs before you understand the basics well.</p><p id="""">Secondly, as already mentioned, you don't need to create any CRDs yourself if you don't feel the need to. However, many Kubernetes tools will install their own CRDs, so even if you don't create any yourself, you'll probably still end up having some on your cluster.</p><p id="""">One example is <a href=""https://cert-manager.io"" target=""_blank"">cert-manager</a>, a very popular Kubernetes tool for managing certificates. It installs a few CRDs on your cluster in order to do its job. If you execute <strong id="""">kubectl get clusterissuers</strong> before installation of cert-manager, your cluster won't know what ClusterIssuers are:</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
	<code class=""language-yml"">
error: the server doesn't have a resource type ""clusterissuers""
 </code>
</pre></div><p id="""">But if you execute the same command after cert-manager installation, you'll get the list of ClusterIssuers on your cluster.</p><p id="""">In fact, you can list all custom resource definitions installed on your cluster by executing <strong id="""">kubectl get crd</strong>:</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
	<code class=""language-yml"">
$ kubectl get crd
NAME                                    CREATED AT
addons.k3s.cattle.io                    2022-01-23T12:48:31Z
helmcharts.helm.cattle.io               2022-01-23T12:48:31Z
helmchartconfigs.helm.cattle.io         2022-01-23T12:48:31Z
serverstransports.traefik.containo.us   2022-01-23T12:49:48Z
tlsoptions.traefik.containo.us          2022-01-23T12:49:48Z
ingressroutetcps.traefik.containo.us    2022-01-23T12:49:48Z
ingressroutes.traefik.containo.us       2022-01-23T12:49:48Z
tlsstores.traefik.containo.us           2022-01-23T12:49:48Z
middlewares.traefik.containo.us         2022-01-23T12:49:48Z
traefikservices.traefik.containo.us     2022-01-23T12:49:48Z
middlewaretcps.traefik.containo.us      2022-01-23T12:49:48Z
ingressrouteudps.traefik.containo.us    2022-01-23T12:49:48Z
 </code>
</pre></div><p id="""">The above output comes in the form of <strong id="""">&lt;object&gt;.&lt;group&gt;</strong> and tells me that I can execute commands like <strong id="""">kubectl get addons</strong>, <strong id="""">kubectl get helmcharts</strong>, or <strong id="""">kubectl get middlewares</strong>. None of them are defined in vanilla Kubernetes and are only there because they were defined as custom resource definitions.</p><h3 id="""">How to Create CRDs</h3><p id="""">OK, forget about biscuits. Let's take a look at some more realistic examples. Imagine that you want Kubernetes to somehow manage your custom routers in your datacenter. For that, you could create a custom resource definition similar to this one:</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
	<code class=""language-yml"">
apiVersion: apiextensions.k8s.io/v1
kind: CustomResourceDefinition
metadata:
  # Name of your CRD. Must match the spec block below, and be in the form: .
 name: routers.example.com
spec:
# Group name to use for REST API: /apis//
 group: example.com
 names:
# Plural name to be used in the URL: /apis///
   plural: routers
   # Singular name to be used as an alias on the CLI and for display
   singular: router
   # Kind is normally the CamelCased singular type. Your resource manifests use this.
   kind: Router
   # ShortNames allow shorter string to match your resource on the CLI
   shortNames:
   - rt
 # Scope can be either Namespaced or Cluster-wide
 scope: Cluster
 versions:
   - name: v1
     # Each version can be enabled/disabled by Served flag.
     served: true
     # One and only one version must be marked as the storage version.
     storage: true
     schema:
       openAPIV3Schema:
         type: object
         properties:
           spec:
             type: object
             properties:
               dataCenter:
                 type: string
               rack:
                 type: integer
               type:
                 type: string
                 enum:
                 - f5
                 - virtual
             required: [""dataCenter"", ""rack"", ""type""]
         required: [""spec""]
 </code>
</pre></div><p id="""">You can apply the above CRD to the cluster by executing <strong id="""">kubectl apply -f router-CRD.yaml</strong>. Once you do that, your Kubernetes cluster will already know what ""router"" is. Therefore, you'll be able to execute <strong id="""">kubectl get routers</strong>. Of course, we just applied the resource definition, not the resource itself. So <strong id="""">kubectl get routers</strong> will return the following:</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
	<code class=""language-yml"">
No resources found.
 </code>
</pre></div><p id="""">But as you can see, it doesn't return this:</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
	<code class=""language-yml"">
error: the server doesn't have a resource type ""routers""
 </code>
</pre></div><p id="""">Which means we successfully added a new object to the Kubernetes API. To add an actual router resource, you can construct a YAML definition file like with any other object:</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
	<code class=""language-yml"">
apiVersion: example.com/v1
kind: Router
metadata:
 name: example-router
spec:
 dataCenter: eu-1
 rack: 3
 type: virtual
  </code>
</pre></div><p id="""">Now, you can create a new router on your cluster by executing <strong id="""">kubectl apply - f example-router.yaml</strong>, and if you try to get the list of routers again with <strong id="""">kubectl get routers</strong>, you should see one now:</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
	<code class=""language-yml"">
$ kubectl get routers
NAME             AGE
example-router   4s
  </code>
</pre></div><p id="""">Congratulations! You just extended the Kubernetes API.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1000px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1000px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/620c25109878e0a22342e1f5_Kubernetes%20CRDspq02.png"" loading=""lazy"" width=""auto"" height=""auto"" id=""""></div></figure><h3 id=""""><strong id="""">What to Do With CRDs</strong></h3><p id="""">You may be thinking, ""OK, great, but that router doesn't do anything!"" And yes, that's right. In its current form, our CRD doesn't do anything besides being processed and stored by the Kubernetes API. And while there are use cases where this is enough, usually CRDs are combined with custom controllers.</p><p id="""">Custom controllers are another concept in Kubernetes that lets you actually do something with your custom resources. In our case, we would like to actually create or configure the routers in our datacenter. Therefore, we'd have to write a custom controller and instruct it to listen to the Kubernetes API and wait for any changes to our custom <strong id="""">router</strong> objects.</p><p id="""">Custom controllers under the hood are just applications or scripts written in your programming language of choice. They're deployed on the cluster as pods, and their job is to listen to the Kubernetes API and perform some actions based on defined logic.</p><h4 id="""">CRD vs. ConfigMap</h4><p id="""">Last but not least, by looking at CRDs, you may see some similarities with a Kubernetes built-in object, ConfigMap. And if you use CRDs without a custom controller, they may, in fact, serve a similar purpose. They both can be used to store custom configurations. However, there are noticeable differences between them.</p><p id="""">First of all, ConfigMaps by design are meant to provide configuration for your pods. They can be mounted as files or environment variables into the pod. They work well if you have well-defined config files like, for example, Apache or MySQL config.</p><p id="""">CRDs can also be consumed by pods but only by contacting the Kubernetes API. They simply have a different purpose than ConfigMaps. They're not meant to be used to provide configuration to your pods but to extend the Kubernetes API in order to build custom automation.</p><h3 id="""">Summary</h3><p id="""">Kubernetes's flexibility is what made it so successful (among other things, of course). Now, you can make use of that flexibility by creating your own Kubernetes objects. The possibilities are almost limitless, and it's only up to you how you'll make use of CRDs.</p><p id="""">Come back to us for more Kubernetes articles.<a href=""https://release.com/blog/kubernetes-daemonset-tutorial""> Here's</a> our article explaining another Kubernetes object, DaemonSets. Also, feel free to take a look at our offerings. We simplify the development process by providing<a href=""https://release.com/"" id=""""> Environments as a Service.</a><br></p>",false,,,,https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63e40204910e72e343debd7a_053122%20(1).jpg,a close-up of a keyboard,dawid-ziolkowski,7,Tue May 31 2022 22:10:00 GMT+0000 (Coordinated Universal Time),,
Kubernetes DaemonSets: A Detailed Introductory Tutorial,kubernetes-daemonset-tutorial,62aa5a70cd5ba27d9d0d718a,62aa5a70cd5ba2c5160d72f8,Fri Jan 07 2022 17:33:41 GMT+0000 (Coordinated Universal Time),Fri Apr 21 2023 19:29:52 GMT+0000 (Coordinated Universal Time),Fri Apr 21 2023 19:37:52 GMT+0000 (Coordinated Universal Time),"Kubernetes deployment strategy: DaemonSets. What are they, what advantages they bring, and when to use them.","<p id="""">Kubernetes is one of the most popular container orchestrator systems. One of the reasons for its popularity is the fact that it offloads you from a lot of maintenance tasks when it comes to containers. It does a lot of stuff for you. For example, Kubernetes saves you a lot of time from planning where to deploy your microservices and spending even more time making sure that all the pods are distributed equally across all available nodes.</p><p id="""">But as with everything, there is no one solution that suits all use cases. That's why Kubernetes has a few different types of deployment strategies. In this post, you'll learn what DaemonSets are, what advantages they bring, and when to use them.</p><h3 id="""">What Is Kubernetes Deployment?</h3><p id="""">Before we dive into DaemonSets, let's make sure we understand the general concept of Kubernetes workflows. <a href=""https://en.wikipedia.org/wiki/Kubernetes"" id="""">Kubernetes</a> is quite a complex system with a lot of components and options. There are many choices for networking, storage, scaling, etc. But the core function of Kubernetes is to run containers. So, if you want to instruct Kubernetes to run a container (as a <a href=""https://kubernetes.io/docs/concepts/workloads/pods/"" id="""">pod</a>), you need to create a workflow.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1000px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1000px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/61d878619b849815e0c92cd9_Kubernetes%20is%20quite%20a%20complex%20system.png"" loading=""lazy"" id="""" width=""auto"" height=""auto""></div></figure><p id="""">As with anything else on Kubernetes, there are a few configuration options for workflows. The most common type of workflow is Deployment. Creating a Deployment means telling Kubernetes, ""Please run a container from this Docker image."" This is, of course, a hugely simplified explanation, but you get the idea. To create a workflow of a Deployment type, you need to include just that in your typical Kubernetes YAML definition:</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yaml"">
apiVersion: apps/v1
kind: Deployment
metadata:
	(...)
spec:
	(...)
</code>
</pre></div><h3 id="""">Deployment in Action</h3><p id="""">So what happens when you create a Deployment? Kubernetes will first find appropriate nodes to run your pod. One of the main criteria for being ""appropriate"" is the load on the node. Kubernetes, by default, will try to distribute the load across all nodes. So, for example, say you have five nodes, and on four of them you have 10 pods running, whereas the last one is running only eight. There's a high chance that Kubernetes will schedule any new Deployment on that last node. Also, when one of the nodes becomes unavailable for whatever reason, Kubernetes will try to reschedule all the pods that were running on that node to the remaining nodes, and again, it will try to distribute these pods to all nodes.</p><p id="""">All of this decision-making on where to schedule containers is happening under the hood, and you don't need to worry about where your pods will be scheduled. This is one of the main features of Kubernetes. You just add new nodes whenever your cluster becomes saturated, and Kubernetes does all the management for you.</p><h3 id="""">Different Types of Workflows</h3><p id="""">All of the above is just Kubernetes' default behavior. Of course, sometimes you actually may want to have more control over the scheduling process. You may want to schedule some microservices on specific nodes, something that's often used with multiple node pools. For example, you may want to add a few nodes with high-performance graphics cards and schedule some big data for AI processing microservices specifically on these nodes. This is just one example. There are more use cases where you may want a different behavior from Kubernetes than the default ""schedule my pods anywhere."" One such use case is the need for scheduling a copy of a pod on every single node. Let me now introduce you to DaemonSets.</p><h3 id="""">Enter DaemonSet</h3><p id="""">So why would you want to schedule the same containers on every single node? There are many possible reasons. The most common one is the need for scheduling a ""daemon""-type application that needs to perform some action on every node. Common examples are logs or metrics-gathering daemons. It's also possible to schedule a copy of a pod not on all nodes but on a subset of them. This can be useful for scheduling a daemon-type pod, for example, only on a specific node pool.</p><p id="""">For instance, if you want to get metrics (like CPU or RAM usage) from each node, the best option is to schedule a container on every node that will gather these metrics from each individual node. Why not simply schedule one container instead that will gather metrics from all nodes? Well, you would run the risk that the node on which the metrics are running dies for whatever reason, and you'd lose metrics from the whole cluster. Of course, Kubernetes would redeploy that service on another node. But depending on how busy your cluster is, that could take a while, and therefore, you would miss some of the data. In the case of metrics, maybe it wouldn't be such a big deal, but imagine losing logs from all containers for a moment.</p><p id="""">But besides these common use cases, you may simply want to have a copy of the same container on every node for any application-specific use case—things like node-local application caches, for example.</p><h3 id="""">DaemonSets in Detail</h3><p id="""">Now that we understand the need behind DaemonSets, let's talk about them in more detail. We know already that the main point of a DaemonSet is to ensure that all nodes are running a copy of a pod. Therefore, unlike with a typical Kubernetes Deployment, you don't specify how many pods you want to run. Kubernetes will automatically run as many pods as you have nodes. Another difference from normal deployment is the fact that in case of a node being removed from the cluster, Kubernetes won't move the pod that belongs to the DaemonSet to a different node but instead will simply destroy it.</p><p id="""">So how do you create a workflow with DaemonSet? Very similarly to a normal Deployment. In fact, as with any other Kubernetes definition, you need to prepare a YAML definition with <strong id="""">apiVersion</strong>, <strong id="""">kind</strong>, and <strong id="""">metadata</strong> fields. However, instead of <strong id="""">Deployment</strong>, the <strong id="""">kind</strong> value, in this case, will be <strong id="""">DaemonSet</strong>. So an example DaemonSet YAML definition could look like this:</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yaml"">
apiVersion: apps/v1
kind: DaemonSet
metadata:
 name: fluentd-daemon
spec:
 selector:
   matchLabels:
     name: fluentd-daemon
 template:
   metadata:
     labels:
       name: fluentd-daemon
   spec:
     containers:
       - image: fluent/fluentd
         name: fluentd-daemon
</code>
</pre></div><p id="""">Following the idea of a DaemonSet, the above definition will deploy a <strong id="""">fluentd</strong> pod on every node in the cluster. Kubernetes will make sure that there's only one pod on every node. For example, if you have five nodes, you'll have five <strong id="""">fluentd</strong> pods running. If one of the nodes becomes unavailable, you'll have four <strong id="""">fluentd</strong> pods running.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1000px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1000px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/61d8781019207ae99ba7bd9f_In%20the%20case%20of%20Kubernetes%20DaemonSets.png"" loading=""lazy"" width=""auto"" height=""auto"" id=""""></div></figure><h3 id="""">Summary</h3><p id="""">Kubernetes DaemonSets can be a bit tricky to understand at first. They may seem like something against the whole point of Kubernetes. But just like with anything else, there are use cases where something that seems odd is actually useful. In the case of Kubernetes DaemonSets, they're quite commonly used for things like logs or monitoring. Also, don't forget that the main advantages of Kubernetes are flexibility and the ability to adjust it to different companies and infrastructures.</p><p id="""">Of course, no one will force you to use DaemonSets. It's totally fine to not use them if you feel like you don't need them. But on the other hand, when you do actually need a daemon-like functionality, it's way better and easier to use DaemonSets than trying to achieve the same with normal Kubernetes Deployment. If you want to learn more about Kubernetes, check out <a href=""https://releasehub.com/blog/kubernetes-pods-advanced-concepts-explained"" id="""">this post about advanced concepts for Kubernetes pods</a>.</p>",false,,,,https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63e3fcc1736b95ac4780e808_020822%20(1).jpg,keyboard kubernetes daemonsets,dawid-ziolkowski,5,Tue Feb 08 2022 15:50:00 GMT+0000 (Coordinated Universal Time),,
How to Make the Most of Kubernetes Environment Variables,kubernetes-environment-variables,62aa5a70cd5ba27d9d0d718a,62aa5a70cd5ba2f28e0d7302,Mon Feb 07 2022 09:44:46 GMT+0000 (Coordinated Universal Time),Fri Apr 21 2023 19:22:27 GMT+0000 (Coordinated Universal Time),Fri Apr 21 2023 19:37:52 GMT+0000 (Coordinated Universal Time),"There are a few ways to add Kubernetes environment variables. In this post, we are going to learn what they are and when","<p id="""">In traditional systems, environment variables play an important role but not always a crucial one. Some applications make more use of environment variables than others. Some prefer configuration files over environment variables. However, when it comes to Kubernetes, environment variables are more important than you may think. It's partially due to how containers work in general and partially due to the specifics of Kubernetes. In this post, you'll learn all about environment variables in Kubernetes.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:512px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""512px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/6200e9e2e569ed1cfd14f279_environment%20variables%20are%20dynamic%20key%20value%20variables.png"" loading=""lazy"" alt=""environment variables are dynamic key value variables"" id="""" width=""auto"" height=""auto""></div></figure><h3 id="""">The Basics</h3><p id="""">Let's start with the basics. What are environment variables, and why do they exist? Traditionally, environment variables are dynamic key value variables that are accessible to any process running on the system. The operating system itself will set many environment variables that help running processes understand the specifics of the system. Thanks to this, software developers can include logic in their software that makes the programs adjustable to a specific operating system. Environment variables also hold a lot of important information about the user. Things like username, preferred language, user home directory path, and many other useful bits of information.</p><h3 id="""">User-Defined Environment Variables</h3><p id="""">As a user, you can easily create and access your own environment variables. On Unix-based systems, you can do that by executing the <strong id="""">export </strong>command followed by the name of your variable and its value. So, for example, to create an environment variable called <strong id="""">myvar</strong> with a value of <strong id="""">10</strong>, you need to execute the following:</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yaml"">
export myvar=10
</code>
</pre></div><p>You can then access the value of your variable using a dollar sign followed by your variable name. In our case to print (using Linux command<a href=""https://en.wikipedia.org/wiki/Echo_(command)"" target=""_blank"" id=""""> echo</a>) the value of our variable, we can execute the following:</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yaml"">
echo $myvar
</code>
</pre></div><p>And if you want to print all the environment variables, you can execute either <strong id="""">printenv</strong> or <strong id="""">env</strong> commands. All of this applies to applications running in your pods too.</p><h3 id="""">Environment Variables in Kubernetes</h3><p id="""">The basic principle of environment variables in Kubernetes is the same. However, Kubernetes uses environment variables quite extensively and for a few different things. Therefore, it's good to understand what role environment variables play in Kubernetes. That's especially true if you want to migrate an existing application that doesn't use environment variables that much. You can still create your pods without any environment variables. But if you ignore environment variables in Kubernetes completely, you may lose some of the value of the more powerful Kubernetes features.</p><p id="""">Before we move any further, you also need to know that it's generally good practice when developing microservices to provide configuration to your Docker containers as environment variables whenever possible. This way you can make your Docker image more generic and possibly reuse the same image for different purposes. With that being said, let's see how you can inject some environment variables into your Kubernetes pods.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:512px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""512px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/6200e97969cb7389d5525d7a_The%20main%20use%20case%20for%20environment%20variables%20in%20Kubernetes.png"" loading=""lazy"" alt=""The main use case for environment variables in Kubernetes"" width=""auto"" height=""auto"" id=""""></div></figure><h3 id="""">Configuration for Your Pods</h3><p id="""">The main use case for environment variables in Kubernetes is similar to the one from traditional software development. That is to provide information about the environment to your software. This information is usually used to alter or adapt the way software works to the specifics of the environment. The definition may seem vague, so let me give you an example. Instead of having separate Docker images for your development and production environments, you can use the same image but run your application in a development or production mode based on an environment variable.</p><p id="""">In Kubernetes, environment variables are scoped to a container, and there are three main ways of adding them. Let's break them down.</p><h4 id="""">Direct</h4><p id="""">The first option is the most straightforward. You can simply specify environment variables directly in your deployment definition with an <strong id="""">env</strong> keyword:</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yaml"">
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: example-app
spec:
  replicas: 1
  selector:
    matchLabels:
      app: example-app
  template:
    metadata:
      labels:
        app: example-app
    spec:
      containers:
        - name: example-app-dev
          image: [yourimage]
          env:
            - name: ENVIRONMENT
              value: ""development""
</code>
</pre></div><p>As soon as your application is instructed to read the value of an environment variable called ""ENVIRONMENT"", you can use it directly to run your application in the desired mode.</p><p id="""">To run the same application in a production mode, you can simply reuse the same deployment definition. You'll only need to change the environment variable value (and optionally a name of the pod):</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yaml"">
---
apiVersion: apps/v1
kind: Deployment
(...)
      containers:
        - name: example-app-prod
          image: [yourimage]
          env:
            - name: ENVIRONMENT
              value: ""production""
</code>
</pre></div><p>Here's another example: Imagine that you have a web application that needs to download a product catalogue. This catalogue will then be served to the users. This catalogue may differ in a few ways (by, for example, a country, month, or supplier). This is a perfect use case for an environment variable. Instead of creating many different versions of your application to accommodate different download options, your application can remain generic. Which catalogue it has to download will be determined by the value of some specific environment variable.</p><h4 id="""">Secrets</h4><p id="""">Another way of providing environment variables to your application is by passing them from Kubernetes<a href=""https://kubernetes.io/docs/concepts/configuration/secret/"" target=""_blank"" id=""""> secrets</a>. You may guess that this is a good option when you need to pass some sensitive information like passwords or tokens. This way you don't specify the value of the environment variable directly in the deployment as we did before. Instead, you instruct Kubernetes to take the value of a specified secret object and use it as a value of an environment variable for your pod.</p><p id="""">For example, if you have a Kubernetes secret like this:</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yaml"">
apiVersion: v1
kind: Secret
metadata:
  name: secret_data
type: Opaque
stringData:
    username: ""example""
    password: ""supersecretpassword""
</code>
</pre></div><p>and you want to pass the password as an environment variable to your pod, you can reference it in the deployment definition as follows:</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yaml"">
---
apiVersion: apps/v1
kind: Deployment
(...)
      containers:
        - name: example-app-prod
          image: [yourimage]
          env:
            # Inject variables from a Kuberentes secret
            - name: secret_variables
              valueFrom:
                secretKeyRef:
                  name: secret_data
                  key: password
</code>
</pre></div><p>In your pod, you will then be able to access the actual password (supersecretpassword) by accessing an environment variable called <strong id="""">secret_variable</strong>. For example, in Python you could do it like this:<br></p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yaml"">
import osPASSWORD = os
.environ.get['secret_variable']
</code>
</pre></div><p>As you can see, in our example we have <strong id="""">username</strong> and <strong id="""">password </strong>defined in Kubernetes secret, but we are only passing the <strong id="""">password</strong> value to the pod. If you want to pass all the secrets from a Kubernetes secret without specifying each key, you can use <strong id="""">secretRef</strong> instead of <strong id="""">secretKeyRef</strong>. This way, you only need to specify the Kubernetes secret object name, and all the values from it will be automatically loaded as environment variables:</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yaml"">
---
apiVersion: apps/v1
kind: Deployment
(...)
      containers:
        - name: example-app-prod
          image: [yourimage]
          env:
            # Inject variables from a Kuberentes secret
            - name: secret_variables
              valueFrom:
                secretRef:
                  name: secret_data
</code>
</pre></div><h4 id="""">ConfigMaps</h4><p id="""">Another way of injecting environment variables into your pods is by using values from<a href=""https://kubernetes.io/docs/concepts/configuration/configmap/"" target=""_blank"" id=""""> ConfigMaps</a>. For example, if you have ConfigMap like this:</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yaml"">
apiVersion: v1
kind: ConfigMap
metadata:
  name: config-data
data:
  environment: ""dev""
  timezone: ""UTC""
</code>
</pre></div><p>and you want to load both <strong id="""">environment </strong>and <strong id="""">timezone</strong> as environment variables into your pod, you can add the following <strong id="""">valueFrom </strong>definition to your deployment:</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yaml"">
---
apiVersion: apps/v1
kind: Deployment
(...)
      containers:
        - name: example-app-prod
          image: [yourimage]
          env:
            # Inject variables from a Kuberentes ConfigMap
            - name: config_variables
              valueFrom:
                configMapRef:
                  name: config-data
</code>
</pre></div><p>In your pod, you'll then be able to see both environment variables as defined in your ConfigMap:</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yaml"">
# env
HOSTNAME=5ad4e9e78e57
environment=dev
timezone=UTC
</code>
</pre></div><p>As with secrets, if you don't want to load all values from a ConfigMap, you can define specific keys instead by changing <strong id="""">configMapRef </strong>to <strong id="""">configMapKeyRef.</strong></p><p id="""">The main difference between passing environment variables from ConfigMaps and specifying them directly as in the first example is the fact that here the environment variable lifecycle is separated from the pod lifecycle. This means you can update the value of your variable independently from the running pod. Or, to put it differently, you'll need to restart the pod yourself in order to load the new value of the environment variables into the pod. On the other hand, when you specify environment variables directly in the deployment, every change to the variables will automatically trigger pod restart.</p><h3 id="""">Summary</h3><p id="""">Environment variables play an important role in Kubernetes. You can use them not only to provide basic information about the operating system to your application. You can also use them as the main configuration mechanism for your pods or for passing sensitive information. It's not uncommon in Kubernetes to extract as much configuration as possible info ConfigMaps and environment variables to keep your Docker images as generic as possible. As you can see, even something simple like environment variables have a few options in Kubernetes. If you want to learn more,<a href=""https://release.com/blog/why-kubernetes-is-so-hard"" id=""""> Regis Wilson wrote about why Kubernetes is hard and what you can do about it.</a></p>",false,,,,https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63e3f3b023b39c72f117d4d2_0224_222%20(1).jpg,kubernetes environment variable secret,dawid-ziolkowski,5,Thu Feb 24 2022 15:06:00 GMT+0000 (Coordinated Universal Time),,
Kubernetes Health Checks - 2 Ways to Improve Stability in Your Production Applications,kubernetes-health-checks-2-ways-to-improve-stability,62aa5a70cd5ba27d9d0d718a,62aa5a70cd5ba2285a0d72c2,Wed Mar 24 2021 17:36:54 GMT+0000 (Coordinated Universal Time),Fri Apr 21 2023 19:48:49 GMT+0000 (Coordinated Universal Time),Fri Apr 21 2023 20:04:43 GMT+0000 (Coordinated Universal Time),These two methods for using Kubernetes health checks will improve your applications running in production.,"<h3 id="""">Kubernetes Health Checks - 2 Ways to Improve Stability in Your Production Applications</h3><p id="""">Health checks are often a last-minute addition to your application stack, if they are even included at all. Advanced Site Reliability Engineering (SRE) practices try to push best practices (like health checks) forward so they are included early before applications are deployed. Many engineers know intuitively that health checks are important, but getting them implemented correctly—and keeping them up to date—is very hard. This article tries to document best practices for health checks, application development including SRE tenets, and how to improve the stability and even performance of your application when it runs in production.<br></p><p id="""">At <a href=""https://release.com"" id="""">Release</a>, we have <a href=""https://release.com/blog/kubernetes-pods-advanced-concepts-explained"" id="""">previously written</a> about how we monitor and configure applications so you can also read that blog post now or at a later time.</p><h3 id="""">Costs of Downtime and Instability in Production</h3><p id="""">Do you know how much it costs for your application to go offline? Don’t worry if you don’t—or can’t—know the exact figure: the important thing is to go through the mental process of estimating how much an outage or degradation to your application would “cost.” Costs are not only measured in currency, you need to also consider impacts to your brand, your Net Promoter Score (NPS), chatter online and on social media among customers and potential customers, and even negative reactions in the public media.<br></p><p id="""">I have worked in Site Reliability and DevOps my whole career and I have worked at many different companies whose responses for downtime ranged from the casual “our site will be back up eventually and we’ll be fine” to “we have lost $XXX per minute in revenue and we need to investigate methods for replacing that revenue”. No matter the response, I still did the best job my team and I could muster to keep the application and infrastructure services alive and well. There will always be bugs and issues with the code that is deployed and how it runs, however, if a problem occurs at a lower level in the application stack or in the infrastructure itself, then the application simply has no hope of servicing the needs of the consumers who visit your site.<br></p><p id="""">The metaphor that I used often was one of cars driving on the motorway: if the roads are wet and slippery, then the cars will be unsafe and dangerous. When and if a crash occurs, then the roads will also be blocked and traffic will stop while the crash is cleaned up. It's true that the cars may run out of petrol, the drivers may get lost and go to the wrong destination, or the cars may not have good horsepower to drive quickly, but all of those factors are a higher order concern in the traffic stack. In this way, I saw my team’s and my job as keeping the roads as clear and safe and uncongested as possible so that the cars could operate at the best possible level.</p><h3 id="""">The Symptoms Are The Disease</h3><p id="""">Very early in the internet industry, the best practices for application stability were primitive and reactionary. Site reliability involved a manual post-mortem approach: finding out what happened and then applying monitoring and alerting on that behaviour to alert an operator that something was wrong. The best practices at the time involved a team of on-call engineers and operators who would literally watch an application 24 hours a day, 365 days a year (one extra day for leap years) and respond within a certain timeframe (usually less than fifteen minutes) to manually investigate and fix any issues that came up. In some cases the “team” was actually one poor person tasked with the impossible job of being on-call indefinitely.<br></p><p id="""">There are several drawbacks to this approach, not the least of which is the human toll such manual response takes and the unsustainable pace. The cost of the team, the cost of staff turnover and training, the losses due to turnaround time and missed calls, and the impact to end users were all huge reasons to implement a better solution.</p><h3 id="""">Health Checks to the Rescue</h3><p id="""">One key initiative that came about in the early aughts was the concept of a health check in the load balancer. I was part of a team that worked with several major load balancer manufacturers to implement a way to not only route traffic to services in our application, but to add monitors and tests (even then we called them “health checks”) to the endpoints which would allow us to add or remove services that were not responding or were unhealthy. The concept was that a web application would respond on a well-known port and respond with a well-known response that proved the application was ready to serve traffic.<br></p><p id="""">For example, we might query the backend service at <em id="""">http://192.168.0.10/health-check</em> and we expected the service to respond with a string like <em id="""">200 OK</em>. This trivial example doesn’t sound like much until you realised that our end-goal was to actually perform some internal checks in the application which would allow us to do more than respond with a static string. For example, the application might check that the database is responding to a sample query that a table exists, and then the application could check that the CPU was at some nominal value. Therefore, the health check could be expanded to something like:<br></p><div data-rt-embed-type='true'><pre class=""language-yaml line-numbers"">
<code class=""language-yaml"">
HTTP/1.0 200 OK

Checking DB… ok
Checking CPU… ok
Checking User cache… ok
</code></pre></div><p id="""">Conversely, if something went wrong, the application could respond something like this:</p><div data-rt-embed-type='true'><pre class=""language-yaml line-numbers"">
<code class=""language-yaml"">
HTTP/1.0 500 CRITICAL

Checking DB... ok
Checking CPU… ok
Checking User cache… CORRUPTED
</code></pre></div><p id="""">Using the response code of 200 and looking for the string “OK” (for example), the load balancer manufacturers were able to remove a service from the backend pool, allowing other servers to accept requests and avoid servers that would otherwise have an error. Also, we could set a timeout so that the load balancer would consider no response to be an error. In this way, we can remove traffic from servers that were not responsive. The beauty of the system we were designing was that we were going to be able to monitor errors proactively and directly at the origin. The servers would be removed before they became a problem.</p><p id="""">We would also use the same health check in our monitoring and alerting systems that we had perfected over the previous decades by manually watching them and using them for diagnosis. The difference is that we had more information about what was going wrong, and simultaneously we had more time to respond and properly diagnose the problems without affecting customers at all. Imagine the relief at not having to respond to every alert at 2AM within 15 minutes, but being able to automatically open a ticket to have a technician during the graveyard shift respond within the hour and restart the server and add it back to the pool.</p><p id="""">Even better, we were able to convince the load balancer manufacturers to implement an inline-retry policy based on the same idea. For example, if a live service request to a backend server failed with a 500 error code, the load balancer could not only remove the server from the pool, but it could retry the request one more time on a healthy server. With this technology, the loadbalancer could try to resolve the situation before the customer even noticed anything was wrong, and no human could be quicker.</p><h3 id="""">It’s Not Just Human Labour</h3><p id="""">We did better than save human labour and effort in monitoring the systems and responding to problems. After implementing health checks on the load balancers and inside the application, we were able to reduce errors and outages to the point where we actually raised our traffic levels by a double-digit percentage, and also increased actual revenue by a measurable amount. Users who might have encountered an error and navigated away after a Google search were staying around to browse and (more importantly) make purchases. By further tweaking the load balancing algorithms to favour healthier (or faster) servers, we further increased this beneficial business result even further. Steady growth over time occurred as well, because Google saw improved signals from users and fewer errors and therefore moved the site up in rankings. This was a stunning and unexpected outcome that was attributed to removing errors and downtime from our application running with this infrastructure and by utilising SRE practices (long before the term was coined).</p><h3 id="""">The Modern Solutions</h3><p id="""">With the advent of Kubernetes, the lessons learnt the hard way over the past few decades have been carried forward in architecting a resilient and reliable design for complex service interactions. Kubernetes uses the concepts of a probe to test the application for liveness and readiness (there is a third probe that tests for startup delay, but we’re skipping that for the purposes of this article). With these two probes, we can implement a solution that makes applications far more stable and reduces downtime and manual intervention.</p><h3 id="""">Liveness Probes</h3><p id="""">The first solution is the liveness probe which has the job of figuring out if a service is responding properly and within a certain time frame so that it can be considered running properly. If the probes fail, then the pod is considered “dead” and the pod will be terminated and restarted somewhere else in the cluster. For example, a web server may have a memory leak and stop responding after a certain amount of time or number of requests have occurred. Another example might be a database that fills up a temporary disk space area and is unable to process further transactions until the space is cleared out.</p><p id="""">You may be saying to yourself that these seem like errors that should be corrected and dealt with properly rather than simply killing the pod and waiting for it to be rescheduled somewhere else. You would be absolutely correct, but let me counter with a rhetorical question asking, “Given this error condition, what do you want me to do at 2AM when no one is available?” The liveness probes can be excellent at monitoring non-responsive servers without state, but may not be so great at monitoring and restarting services with state, like the database example I gave above. So we recommend using the liveness probe only if you feel it would help more than it would hurt. We also spend extra care and effort to ensure that the liveness probes are very forgiving so they do not trigger on false-positive alarms.</p><p id="""">Another counter-argument to the “fix it” stance has to do with direct or indirect engineering costs and interacting with third party or open-source code. Trying to allocate resources to fully diagnose an intermittent problem, much less attempt to fix the problem can be difficult. In the case of a third party software or an open-source project where getting upstream fixes submitted, prioritised, approved, tested, and pulled back downstream can be enormously expensive and time consuming. Sometimes the answer really is “just restart it”.</p><h3 id="""">Readiness Probes</h3><p id="""">The second solution is a readiness probe which is much like the solution I described earlier in this article with the load balancers. Indeed, the readiness probe does exactly what I’ve described: Kubernetes will periodically run a command to test the service running inside the container to gauge proper and timely responses. The ingress (just a fancy name for the load balancer) will not send traffic to this pod unless and until the readiness probe states that the service is ready for correct operation.</p><p id="""">This helps in some scenarios where a web server may hit a threshold in connections or traffic levels where it may slow down or stop responding to new requests. It may be the case that the application simply cannot handle more than a certain number of transactions and so Kubernetes can use this signal to route traffic to another pod that is less busy. If this slow down or refusal to respond can be correlated with other metrics (like traffic volume, CPU utilisation, etc.) then the <a href=""https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"" id="""">horizontal pod autoscaler</a> could trigger more resources to be added to the service.</p><p id="""">In fact, we believe that readiness probes are so important to correct functioning of applications that we strongly recommend <em id="""">all services</em> have a health check of some kind enabled and tested. We feel so strongly about this that we have considered making it a warning condition when no health check is configured on a running service in any of your environments at Release. Specifically, we could make readiness probes an opt out requirement rather than an opt in nicety.</p><h3 id="""">Some examples</h3><p id="""">Here are some actual examples of health checks that we have implemented for our customers. These examples are generic enough to be applied almost anywhere.</p><p id="""">In this example we do a simple Nginx check on port 80 to ensure that the application is responding before we send traffic to the proxy.<br></p><div data-rt-embed-type='true'><pre class=""language-yaml line-numbers""><code class=""language-yaml"">  readiness_probe:
    exec:
      command:
      - curl
      - ""-Lf""
      - http://localhost
    failure_threshold: 5
    period_seconds: 30
    timeout_seconds: 3
</code></pre></div><p id="""">In this example we perform a health check against an Elastic search node to ensure that the cluster is healthy before accepting traffic (which presumably cannot be processed yet). This is a straight port from the Docker Compose examples in the open source repositories.</p><div data-rt-embed-type='true'><pre class=""language-yaml line-numbers""><code class=""language-yaml"">- name: elasticsearch
 image: docker.elastic.co/elasticsearch/elasticsearch:7.9.2
 ports:
 - type: node_port
   target_port: '9200'
   port: '9200'
 readiness_probe:
   exec:
     command:
     - curl
     - ""--fail""
     - localhost:9200/_cluster/health
   timeout_seconds: 2
   failure_threshold: 3
   period_seconds: 30</code></pre></div><p id="""">This example is good to show how a non-HTTP check for a postgres database can be used to ensure the database is up and responding to requests. Note that if this database is not clustered, then application database requests can fail when the health check fails. Your application will need to respond accordingly (either fail in turn to cascade a failover at a higher level, or perform some sort of mitigation so that a graceful failure happens). Recall that if this were a liveness probe, the postgres container would be killed and restarted, which may not be what you want at all.</p><div data-rt-embed-type='true'><pre class=""language-yaml line-numbers""><code class=""language-yaml""> readiness_probe:
   exec:
     command:
     - psql
     - ""-h""
     - localhost
     - ""-c""
     - SELECT 1
   period_seconds: 2
   timeout_seconds: 2
   failure_threshold: 30</code></pre></div><h3 id="""">Step N, Profit</h3><p id="""">By implementing either (or both!) of these health checks, you can not only reduce the amount of time humans have to spend monitoring and interfering with applications, but you can even dramatically improve your traffic response levels, response times, and performance. In some cases, you might even be able to measure the impact to your customers’ NPS and/or your company’s top and bottom line.<br></p><p id="""">Photo by <a href=""https://unsplash.com/@hush52?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText"" id="""">Hush Naidoo</a> on <a href=""https://unsplash.com/s/photos/health?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText"" id="""">Unsplash</a></p>",false,,,,https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63e3ff8b9b93d7c71a29a1db_032421%20(2).jpg,A doctor checking the health of a patient representing stability of applications,regis-wilson,5,Thu Mar 25 2021 00:00:00 GMT+0000 (Coordinated Universal Time),,
Kubernetes - How to Debug CrashLoopBackOff in a Container,kubernetes-how-to-debug-crashloopbackoff-in-a-container,62aa5a70cd5ba27d9d0d718a,62aa5a70cd5ba25d790d72cb,Thu Feb 04 2021 19:23:56 GMT+0000 (Coordinated Universal Time),Fri Apr 21 2023 19:53:58 GMT+0000 (Coordinated Universal Time),Fri Apr 21 2023 20:04:43 GMT+0000 (Coordinated Universal Time),"If you’ve used Kubernetes (k8s), you’ve probably bumped into the dreaded CrashLoopBackOff. A CrashLoopBackOff is possibl","<p id="""">If you’ve used Kubernetes (k8s), you’ve probably bumped into the dreaded CrashLoopBackOff. A CrashLoopBackOff is possible for several types of k8s misconfigurations (not able to connect to persistent volumes, init-container misconfiguration, etc). We aren’t going to cover how to configure k8s properly in this article, but instead will focus on the harder problem of debugging your code or, even worse, someone else’s code 😱</p><p id="""">Here is the output from kubectl describe pod for a CrashLoopBackOff:</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-ruby"">
Name:           frontend-5c49b595fc-sjzkg
Namespace:      tedbf02-ac-david-nginx-golang-tmcclung-nginx-golang
Priority:       0
Start Time:     Wed, 23 Dec 2020 14:55:49 -0500
Labels:         app=frontend
                pod-template-hash=5c49b595fc
                tier=frontend
Status:         Running
IP:             10.1.31.0
IPs:            <none>
Controlled By:  ReplicaSet/frontend-5c49b595fc
Containers:
  frontend:
    Container ID:   docker://a4ed7efcaaa87fe36342cf7532ff1de5cd51b62d3d681dfb9857999300f6c587
    Image:          .amazonaws.com/tommyrelease/awesome-compose/frontend@sha256:dfd762c
    Image ID:       docker-pullable://.amazonaws.com/tommyrelease/awesome-compose/frontend@sha256:dfd762c
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Waiting
      Reason:       CrashLoopBackOff
    Last State:     Terminated
      Reason:       Error
      Exit Code:    1
      Started:      Sun, 24 Jan 2021 20:25:26 -0500
      Finished:     Sun, 24 Jan 2021 20:25:26 -0500
    Ready:          False
    Restart Count:  9043
</code>
</pre></div><p id="""">Two common problems when starting a container are OCI runtime create failed (which means you are referencing a binary or script that doesn’t exist on the container) and container “Completed” or “Error” which both mean that the code executing on the container failed to run a service and stay running.</p><p id="""">Here’s an example of an OCI runtime error, trying to execute: “hello crashloop”:</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-ruby"">
Port:          80/TCP
    Host Port:     0/TCP
    Command:
      hello
      crashloop
    State:          Waiting
      Reason:       CrashLoopBackOff
    Last State:     Terminated
      Reason:       ContainerCannotRun
      Message:      OCI runtime create failed: container_linux.go:370: starting container process caused: exec: ""hello"": executable file not found in $PATH: unknown
      Exit Code:    127
      Started:      Mon, 25 Jan 2021 22:20:04 -0500
      Finished:     Mon, 25 Jan 2021 22:20:04 -0500
</code>
</pre></div><p id="""">K8s gives you the exit status of the process in the container when you look at a pod using kubectl or <a href=""https://github.com/derailed/k9s"" target=""_blank"" id="""">k9s</a>. Common exit statuses from unix processes include 1-125. Each unix command usually has a man page, which provides more details around the various exit codes. Exit code (128 + SIGKILL 9) 137 means that k8s hit the memory limit for your pod and killed your container for you.</p><p id="""">Here is the output from kubectl describe pod, showing the container exit code:</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-ruby"">
Last State:     Terminated
      Reason:       Error
      Exit Code:    1
      Started:      Sun, 24 Jan 2021 20:25:26 -0500
      Finished:     Sun, 24 Jan 2021 20:25:26 -0500
    Ready:          False
    Restart Count:  9043
</code>
</pre></div><h3 id="""">All containers are not created equally.</h3><p id="""">Docker allows you to define an Entrypoint and Cmd which you can mix and match in a Dockerfile. Entrypoint is the executable, and Cmd are the arguments passed to the Entrypoint. The Dockerfile schema is quite lenient and allows users to set Cmd without Entrypoint, which means that the first argument in Cmd will be the executable to run.</p><p id="""">Note: k8s uses a different naming convention for Docker Entrypoint and Cmd. In Kubernetes command is Docker Entrypoint and Kubernetes args is Docker Cmd.</p><div data-rt-embed-type='true'><table>
  <tr>
    <th>Description</th>
    <th>The command run by the container</th>
    <th>Arguments passed to the command</th>
  </tr>
  <tr>
    <td>Docker field name</td>
    <td>Entrypoint</td>
    <td>Cmd</td>
  </tr>
    <tr>
    <td>Kubernetes field name</td>
    <td>Cmd</td>
    <td>args</td>
  </tr>
</table></div><p id="""">There are a few tricks to understanding how the container you’re working with starts up. In order to get the startup command when you’re dealing with someone else’s container, we need to know the intended Docker Entrypoint and Cmd of the Docker image. If you have the Dockerfile that created the Docker image, then you likely already know the Entrypoint and Cmd, unless you aren’t defining them and inheriting from a base image that has them set.</p><p id="""">When dealing with either off the shelf containers, using someone else’s container and you don’t have the Dockerfile, or you’re inheriting from a base image that you don’t have the Dockerfile for, you can use the following steps to get the values you need. First, we pull the container locally using docker pull, then we inspect the container image to get the Entrypoint and Cmd:</p><ul id=""""><li id="""">docker pull &lt;image id=""""&gt;&lt;/image&gt;</li><li id="""">docker inspect &lt;image id=""""&gt;&lt;/image&gt;</li></ul><p id="""">Here we use jq to filter the JSON response from docker inspect:</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-ruby"">
david@sega:~: docker pull docker.elastic.co/elasticsearch/elasticsearch:7.10.2
7.10.2: Pulling from elasticsearch/elasticsearch
ddf49b9115d7: Pull complete
e736878e27ad: Pull complete
7487c9dcefbe: Pull complete
9ccb7e6e1f0c: Pull complete
dcec6dec98db: Pull complete
8a10b4854661: Pull complete
1e595aee1b7d: Pull complete
06cc198dbf22: Pull complete
55b9b1b50ed8: Pull complete
Digest: sha256:d528cec81720266974fdfe7a0f12fee928dc02e5a2c754b45b9a84c84695bfd9
Status: Downloaded newer image for docker.elastic.co/elasticsearch/elasticsearch:7.10.2
docker.elastic.co/elasticsearch/elasticsearch:7.10.2
david@sega:~: docker inspect docker.elastic.co/elasticsearch/elasticsearch:7.10.2 | jq '.[0] .ContainerConfig .Entrypoint'
[
  ""/tini"",
  ""--"",
  ""/usr/local/bin/docker-entrypoint.sh""
]
david@sega:~: docker inspect docker.elastic.co/elasticsearch/elasticsearch:7.10.2 | jq '.[0] .ContainerConfig .Cmd'
[
  ""/bin/sh"",
  ""-c"",
  ""#(nop) "",
  ""CMD [\""eswrapper\""]""
]
</code>
</pre></div><h3 id="""">The Dreaded CrashLoopBackOff</h3><p id="""">Now that you have all that background, let’s get to debugging the CrashLoopBackOff.</p><p id="""">In order to understand what’s happening, it’s important to be able to inspect the container inside of k8s so the application has all the environment variables and dependent services. Updating the deployment and setting the container Entrypoint or k8s command temporarily to tail -f /dev/null or sleep infinity will give you an opportunity to debug why the service doesn’t stay running.</p><p id="""">Here’s how to configure k8s to override the container Entrypoint:</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-ruby"">
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: elasticsearch
  namespace: elasticsearch
spec:
  progressDeadlineSeconds: 600
  replicas: 1
  revisionHistoryLimit: 3
  selector:
    matchLabels:
      app: backend
      tier: backend
  strategy:
    rollingUpdate:
      maxSurge: 25%
      maxUnavailable: 25%
    type: RollingUpdate
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: backend
        tier: backend
    spec:
      containers:
      - command:
        - tail
        - ""-f""
        - /dev/null
 </code>
 </pre></div><p id="""">Here’s the configuration in Release:</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-ruby"">
services:
- name: elasticsearch
  image: docker.elastic.co/elasticsearch/elasticsearch:7.10.2
  command:
  - tail
  - ""-f""
  - /dev/null
</code>
</pre></div><p id="""">You can now use kubectl or k9s to exec into the container and take a look around. Using the Entrypoint and Cmd you discovered earlier, you can execute the intended startup command and see how the application is failing.</p><p id="""">Depending on the container you’re running, it may be missing many of the tools necessary to debug your problem like: curl, lsof, vim; and if it’s someone else’s code, you probably don’t know which version of linux was used to create the image. We typically try all of the common package managers until we find the right one. Most containers these days use Alpine Linux (apk package manager) or a Debian, Ubuntu (apt-get package manager) based image. In some cases we’ve seen Centos and Fedora, which both use the yum package manager.</p><p id="""">One of the following commands should work depending on the operating system:</p><ul id=""""><li id="""">apk</li><li id="""">apt-get</li><li id="""">yum</li></ul><p id="""">Dockerfile maintainers often remove the cache from the package manager to shrink the size of the image, so you may also need to run one of the following:</p><ul id=""""><li id="""">apk update</li><li id="""">apt-get update</li><li id="""">yum makecache</li></ul><p id="""">Now you need to add the necessary tools to help with debugging. Depending on the package manager you found, use one of the following commands to add useful debugging tools:</p><ul id=""""><li id="""">apt-get install -y curl vim procps inetutils-tools net-tools lsof</li><li id="""">apk add curl vim procps net-tools lsof</li><li id="""">yum install curl vim procps lsof</li></ul><p id="""">At this point, it’s up to you to figure out the problem. You can edit files using vim to tweak the container until you understand what’s going on. If you forget all of the files you’ve touched on the container, you can alway kill the pod and the container will restart without your changes. Always remember to write down the steps taken to get the container working. You’ll want to use your notes to alter the Dockerfile or add commands to the container startup scripts.</p><h3 id="""">Debugging Your Containers</h3><p id="""">We have created a simple script to get all of the debuging tools, as long as you are working with a container that has curl pre-installed:</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-ruby"">
# install debugging tools on a container with curl pre-installed
/bin/sh -c ""$(curl -fsSL https://raw.githubusercontent.com/releaseapp-io/container-debug/main/install.sh)""
</code>
</pre></div><h3 id="""">Conclusion</h3><p id="""">In this article, we’ve learnt how to spot and investigate the CrashLoopBackOff errors in containers. We walked you through how to inspect and investigate the container image itself. We’ve listed and shown some tools that we use to spot problems and investigate issues. We got several useful and basic tools installed on the image, hopefully regardless of base image. With these steps in mind and all the tools ready at your disposal, go forth and fix all the things!</p>",false,,,,https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63e3fa830f0a3053de028262_012521%20(1).jpg,Storage containers stacked representing a CrashLoopBackOff in a container,david-giffin,3,Tue Jan 26 2021 00:00:00 GMT+0000 (Coordinated Universal Time),,
"Kubernetes Jobs: How to Create, Schedule, Run, and More",kubernetes-jobs-how-to-create-schedule-run-and-more,62aa5a70cd5ba27d9d0d718a,63179195f4c4ea86c66c78eb,Tue Sep 06 2022 18:29:41 GMT+0000 (Coordinated Universal Time),Mon Apr 24 2023 22:07:05 GMT+0000 (Coordinated Universal Time),Mon Apr 24 2023 22:07:12 GMT+0000 (Coordinated Universal Time),"Let's walk through a tutorial on how to create, schedule, configure, and run Kubernetes Jobs.","<p id="""">Kubernetes jobs have many real-life applications. For example, you can use jobs to execute a process for data backup. Once the backup is successful, the job stops running the pods.&nbsp;</p><p id="""">In this post, you'll learn how to use Kubernetes jobs. We'll walk you through a tutorial on how to create, schedule, configure, and run Kubernetes Jobs.&nbsp;</p><h3 id="""">What Are Kubernetes Jobs?</h3><p id="""">Kubernetes jobs are controllers that create one or more <a href=""https://release.com/blog/kubernetes-environment-variables#:~:text=Configuration%20for%20Your%20Pods"" id="""">pods</a> that will run until execution successfully terminates for a specific number of pods. Once the task assigned to a job completes without any error, the task(job) stops running. In case of a failure, the job attempts to retry until all pods run successfully. You can limit how many times a job retries execution using configurations like <strong id="""">activeDeadline</strong> and <strong id="""">backoffLimit</strong>.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1405px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1405px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/631791b447d7f7940785446e_3IZPMV64FmWyDWzgDArvO8z7qfyhNQDinD__NnsP46kSkCkJcliXjry66FLpPjedRg9vu4Op5iW3I4yCEGl9ZKXPLTA67-9jH1KRuwUwN9le9_w2qJs4OuIMXtLuh2qjVpzamCxsrXw4RT1tcdcNxoU.jpeg"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><h3 id="""">Use Cases of Kubernetes Jobs (When to Use It)</h3><p id="""">Now that we know some examples of tasks we can execute using jobs, let's walk through a brief tutorial on how to work with Kubernetes jobs.&nbsp;</p><h4 id="""">1. Backups</h4><p id="""">You can use Kubernetes jobs to perform a task like the periodic backup of data on your server or application. For example, if you want a backup, you set up a job and run it one time. The job will continue running its pods until the backup completes. If the job fails, it will retry. Hence, you can just start the job and not worry about it stopping until the backup completes. Also, you don't have to worry about the task executing again after the backup succeeds.&nbsp;</p><h4 id="""">2. Installing and Uninstalling Services</h4><p id="""">Another good example of a task you can perform with Kubernetes jobs is installing new services for your application. Likewise, you can use jobs to remove existing services that you no longer need. Similar to our backup example, these jobs will run and stop executing their pods as soon as they successfully add or remove the target services. If they don’t succeed, the jobs retry the tasks.&nbsp;</p><h3 id="""">How to Use Kubernetes Jobs</h3><p id="""">Now that we know some examples of tasks we can execute using jobs, let's walk through a brief tutorial on how to work with Kubernetes jobs.&nbsp;</p><h3 id="""">Prerequisites</h3><p id="""">In order to follow along better, you'll need to have the following tools and experience:&nbsp;</p><ul id=""""><li id="""">Kubernetes installation on your target machine</li><li id="""">Basic knowledge of <a href=""https://releasehub.com/blog/6-docker-compose-best-practices-for-dev-and-prod"" id="""">Docker</a> and Kubernetes</li><li id="""">Knowledge of Terminal and CLI</li></ul><p id="""">With that out of the way, let's walk through the actual steps for using jobs.&nbsp;</p><h4 id="""">Step 1: Creating a Job</h4><p id="""">Just like most operations in Kubernetes, you create jobs using a <a href=""https://en.wikipedia.org/wiki/YAML"" target=""_blank"" id=""""><strong id="""">YAML file</strong></a>. The YAML file will contain all the details about your job, like the name of the job, what should happen when a pod fails, and so on. In later steps, we'll take a closer look at the various job configuration options.&nbsp;</p><p id="""">So, on your machine, create a new file with the name <strong id="""">hello_world_job.yaml</strong> and paste the configuration for your new job into it. For this tutorial, we'll use the following code:&nbsp;</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yaml"">
apiVersion: batch/v1
kind: Job
metadata:
  name: hello-world-job
spec:
  template:
    metadata:
      name: hello-world-job
    spec:
      containers:
      - name: hello-world
        image: centos:7
        command:
         - ""bin/bash""
         - ""-c""
         - ""echo hello world""
      restartPolicy: Never
</code>
</pre></div><p id="""">The above configuration is for a job that simply prints ""hello world."" Next, finish up creating the job by running the following command:&nbsp;</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yaml"">
kubectl apply -f hello_world_job.yaml
</code>
</pre></div><p id="""">You should get the following message on your terminal if the command runs successfully:&nbsp;</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yaml"">
job.batch/hello-world-job created
</code>
</pre></div><p id="""">Also, you can verify that your job was created by running this command:&nbsp;</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yaml"">
kubectl get jobs
</code>
</pre></div><p id="""">The output of this command is a list of all your jobs, similar to the following:&nbsp;</p><figure class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:2000px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""2000px""><div><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/6446fd61736ad06235c40a46_Frame%201171274501.svg"" loading=""lazy""></div></figure><p id="""">From the above photo, we can see that we created our <strong id="""">hello-world-job</strong> successfully.&nbsp;<strong id="""">‍</strong></p><h4 id=""""><strong id="""">Step 2: Configuring a Job</strong></h4><p id="""">From the previous step, we already have a few configurations for our job. However, let's walk through a few more complex configurations. In order to do that, let's create a new job. Create a new <strong id="""">hello_world_4x.yaml</strong> file and add the following code to it:&nbsp;</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yaml"">
apiVersion: batch/v1
kind: Job
metadata:
  name: hello-world-4x-job
spec:
  completions: 4
  template:
    metadata:
      name: hello-world-4x-job
    spec:
      containers:
      - name: hello-world-4x
        image: centos:7
        command:
         - ""bin/bash""
         - ""-c""
         - ""echo hello world""
      restartPolicy: Never
</code>
</pre></div><p id=""""><strong id="""">Completion</strong>: In this bit of code, we introduce a new configuration (i.e., completions). In step 1, Kubernetes created a single pod that runs our task once. However, using completions, we can perform the same task multiple times. Completions run multiple pods one after the other.&nbsp;</p><p id="""">Let's take a look at another configuration option. Again, create a new <strong id="""">hello_world_4x_parallel.yaml</strong> file and add the following code to it:&nbsp;</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yaml"">
apiVersion: batch/v1
kind: Job
metadata:
  name: hello-world-parallel-job
spec:
  completions: 4
  parallelism: 2
  template:
    metadata:
      name: hello-world-parallel-job
    spec:
      containers:
      - name: hello-world-parallel
        image: centos:7
        command:
         - ""bin/bash""
         - ""-c""
         - ""echo hello world""
      restartPolicy: Never
</code>
</pre></div><p id="""">‍<strong id="""">Parallelism</strong>: Notice the new configuration item, parallelism. The previous job executed pods one after another. However, we can configure a job to run pods in parallel using this new configuration.&nbsp;</p><h4 id="""">Step 3: Schedule a Job</h4><p id="""">If you need to start jobs at a specific time in the future, or you want to run them in a repetitive pattern at specific intervals, you should consider using a <strong id="""">CronJob</strong>. A CronJob creates jobs that repeat using a schedule. You can schedule the job using the cron format and can set the schedule in the <strong id="""">schedule</strong> object.&nbsp;</p><p id="""">The following example YAML file shows a CronJob:&nbsp;</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yaml"">
apiVersion: batch/v1
kind: CronJob
metadata:
  name: hello-world-cron
spec:
  schedule: ""*/5 * * * *""
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: hello-world
            image: centos:7
            imagePullPolicy: IfNotPresent
            command:
            - /bin/sh
            - -c
            - ""echo Hello World""
          restartPolicy: OnFailure
</code>
</pre></div><p id="""">In this code, the cron schedule format is the string ""*/5 * * * *."" It contains 5 sections (separated with white spaces), representing a minute, hour, day of the month, and day of the week in that order. ""*/5"" means the task will run every 5 minutes. To explain the schedule format further, if you change the schedule to ""0 */5 * * *"", the job will execute every 5 hours. Also, setting all 5 fields to ""*"" means a job will run every minute.&nbsp;</p><p id="""">To create the job on your machine, run the following command:&nbsp;</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yaml"">
kubectl create -f your-cronjob-yaml-file
</code>
</pre></div><p id="""">To see the cronjob you just created, simply run the <strong id="""">kubectl create -f cronjob.yaml</strong> command.&nbsp;</p><h4 id="""">Step 4: Running a Job</h4><p id="""">To run a job after creating the YAML file for it, simply run the <strong id="""">kubectl apply -f [yaml-file]</strong> command. Replace [yaml-file] with the actual file name for your job configuration.&nbsp;</p><p id="""">You can verify the status of your job by running the <strong id="""">kubectl get jobs</strong> command. For an even more detailed report, you can run <strong id="""">kubectl describe job [job-name]</strong>.&nbsp;</p><h4 id="""">Step 5: Deleting a Job</h4><p id="""">For logging and tracking purposes, jobs and the pods they create do not get deleted even after they stop running. However, when you no longer need them, you can clean old jobs and their pods up. To do this you can use the <strong id="""">kubectl delete jobs/[job-name]</strong> command. &nbsp;&nbsp;</p><h3 id="""">Summing Everything up</h3><p id="""">In this post, we've covered what Kubernetes Jobs are—resources that create pods that keep running until successful completion.&nbsp;</p><p id="""">You also learned how to create, configure and run Kubernetes jobs. For jobs that need to run at a specific time or repetitively, you can use the CronJob Kubernetes resource.&nbsp;</p><p id="""">Finally, you learned how to delete Kubernetes jobs after they complete. Since a record of jobs and their pods stay even after completion, if you no longer need that record, you can delete it by deleting the job.</p>",false,,,,https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63e41dc234aa76e68ef98872_092122%20(1).jpg,,tommy-mcclung,4,Wed Sep 21 2022 18:00:00 GMT+0000 (Coordinated Universal Time),,
"Kubernetes Jobs: How to Create, Schedule, Run, and More",kubernetes-jobs-how-to-create-schedule-run-and-more-2,62aa5a70cd5ba27d9d0d718a,63fe3d1d4b1c5247fa38aa39,Tue Feb 28 2023 17:42:53 GMT+0000 (Coordinated Universal Time),Wed Oct 30 2024 19:54:14 GMT+0000 (Coordinated Universal Time),Wed Oct 30 2024 19:54:14 GMT+0000 (Coordinated Universal Time),"How to use Kubernetes Jobs and a tutorial on how to create, schedule, configure, and run K8 Jobs.
",,true,<p>Effortlessly manage Kubernetes jobs and automate your workflows with Release.</p>,https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=kubernetes-jobs,"<p id="""">Kubernetes jobs have many real-life applications. For example, you can use jobs to execute a process for data backup. Once the backup is successful, the job stops running the pods.&nbsp;</p><p id="""">In this post, you'll learn how to use Kubernetes jobs. We'll walk you through a tutorial on how to create, schedule, configure, and run Kubernetes Jobs.&nbsp;</p><h3 id="""">What Is Kubernetes Jobs?</h3><p id="""">Kubernetes jobs are controllers that create one or more <a href=""https://release.com/blog/kubernetes-environment-variables#:~:text=Configuration%20for%20Your%20Pods"" id="""">pods</a> that will run until execution successfully terminates for a specific number of pods. Once the task assigned to a job completes without any error, the task(job) stops running. In case of a failure, the job attempts to retry until all pods run successfully. You can limit how many times a job retries execution using configurations like <strong id="""">activeDeadline</strong> and <strong id="""">backoffLimit</strong>.&nbsp;</p><h3 id="""">Use Cases of Kubernetes Jobs (When to Use It)</h3><p id="""">Now that we know some examples of tasks we can execute using jobs, let's walk through a brief tutorial on how to work with Kubernetes jobs.&nbsp;</p><h4 id="""">1. Backups</h4><p id="""">You can use Kubernetes jobs to perform a task like the periodic backup of data on your server or application. For example, if you want a backup, you set up a job and run it one time. The job will continue running its pods until the backup completes. If the job fails, it will retry. Hence, you can just start the job and not worry about it stopping until the backup completes. Also, you don't have to worry about the task executing again after the backup succeeds.&nbsp;</p><h4 id="""">2. Installing and Uninstalling Services</h4><p id="""">Another good example of a task you can perform with Kubernetes jobs is installing new services for your application. Likewise, you can use jobs to remove existing services that you no longer need. Similar to our backup example, these jobs will run and stop executing their pods as soon as they successfully add or remove the target services. If they don’t succeed, the jobs retry the tasks.&nbsp;</p><h3 id="""">How to Use Kubernetes Jobs</h3><p id="""">Now that we know some examples of tasks we can execute using jobs, let's walk through a brief tutorial on how to work with Kubernetes jobs.&nbsp;</p><h4 id="""">Prerequisites</h4><p id="""">In order to follow along better, you'll need to have the following tools and experience:&nbsp;</p><ul id=""""><li id="""">Kubernetes installation on your target machine</li><li id="""">Basic knowledge of <a href=""https://release.com/blog/6-docker-compose-best-practices-for-dev-and-prod"" id="""">Docker</a> and Kubernetes</li><li id="""">Knowledge of Terminal and CLI</li></ul><p id="""">With that out of the way, let's walk through the actual steps for using jobs.&nbsp;</p><h4 id="""">Step 1: Creating a Job</h4><p id="""">Just like most operations in Kubernetes, you create jobs using a <a href=""https://en.wikipedia.org/wiki/YAML"" target=""_blank"" id=""""><strong id="""">YAML file</strong></a>. The YAML file will contain all the details about your job, like the name of the job, what should happen when a pod fails, and so on. In later steps, we'll take a closer look at the various job configuration options.&nbsp;</p><p id="""">On your machine, create a new file with the name <strong id="""">hello_world_job.yaml</strong> and paste the configuration for your new job into it. For this tutorial, we'll use the following code:&nbsp;</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
	<code class=""language-yml"">
apiVersion: batch/v1
kind: Job
metadata:
  name: hello-world-job
spec:
  template:
    metadata:
      name: hello-world-job
    spec:
      containers:
      - name: hello-world
        image: centos:7
        command:
         - ""bin/bash""
         - ""-c""
         - ""echo hello world""
      restartPolicy: Never
 </code>
</pre></div><p id="""">The above configuration is for a job that simply prints ""hello world."" Next, finish up creating the job by running the following command:&nbsp;</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
	<code class=""language-yml"">
kubectl apply -f hello_world_job.yaml
 </code>
</pre></div><p id="""">You should get the following message on your terminal if the command runs successfully:&nbsp;</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
	<code class=""language-yml"">
job.batch/hello-world-job created
 </code>
</pre></div><p id="""">Also, you can verify that your job was created by running this command:&nbsp;</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
	<code class=""language-yml"">
kubectl get jobs
 </code>
</pre></div><p id="""">The output of this command is a list of all your jobs, similar to the following:&nbsp;</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:2000px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""2000px""><div id=""""><img src=""https://cdn.prod.website-files.com/603dd147c5b0a4221d1bd360/6446fd61736ad06235c40a46_Frame%201171274501.svg"" loading=""lazy"" width=""auto"" height=""auto"" alt="""" id=""""></div></figure><p id="""">From the above photo, we can see that we created our <strong id="""">hello-world-job</strong> successfully.&nbsp;</p><h4 id="""">Step 2: Configuring a Job</h4><p id="""">From the previous step, we already have a few configurations for our job. However, let's walk through a few more complex configurations. In order to do that, let's create a new job. Create a new <strong id="""">hello_world_4x.yaml</strong> file and add the following code to it:&nbsp;</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
	<code class=""language-yml"">
apiVersion: batch/v1
kind: Job
metadata:
  name: hello-world-4x-job
spec:
  completions: 4
  template:
    metadata:
      name: hello-world-4x-job
    spec:
      containers:
      - name: hello-world-4x
        image: centos:7
        command:
         - ""bin/bash""
         - ""-c""
         - ""echo hello world""
      restartPolicy: Never
 </code>
</pre></div><p id=""""><strong id="""">Completion</strong>: In this bit of code, we introduce a new configuration (i.e., completions). In step 1, Kubernetes created a single pod that runs our task once. However, using completions, we can perform the same task multiple times. Completions run multiple pods one after the other.&nbsp;</p><p id="""">Let's take a look at another configuration option. Again, create a new <strong id="""">hello_world_4x_parallel.yaml</strong> file and add the following code to it:&nbsp;</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
	<code class=""language-yml"">
apiVersion: batch/v1
kind: Job
metadata:
  name: hello-world-parallel-job
spec:
  completions: 4
  parallelism: 2
  template:
    metadata:
      name: hello-world-parallel-job
    spec:
      containers:
      - name: hello-world-parallel
        image: centos:7
        command:
         - ""bin/bash""
         - ""-c""
         - ""echo hello world""
      restartPolicy: Never
 </code>
</pre></div><p id=""""><strong id="""">Parallelism</strong>: Notice the new configuration item, parallelism. The previous job executed pods one after another. However, we can configure a job to run pods in parallel using this new configuration.&nbsp;</p><h4 id="""">Step 3: Schedule a Job</h4><p id="""">If you need to start jobs at a specific time in the future, or you want to run them in a repetitive pattern at specific intervals, you should consider using a <strong id="""">CronJob</strong>. A CronJob creates jobs that repeat using a schedule. You can schedule the job using the cron format and can set the schedule in the <strong id="""">schedule</strong> object.&nbsp;</p><p id="""">The following example YAML file shows a CronJob:&nbsp;</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
	<code class=""language-yml"">
apiVersion: batch/v1
kind: CronJob
metadata:
  name: hello-world-cron
spec:
  schedule: ""*/5 * * * *""
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: hello-world
            image: centos:7
            imagePullPolicy: IfNotPresent
            command:
            - /bin/sh
            - -c
            - ""echo Hello World""
          restartPolicy: OnFailure
 </code>
</pre></div><p id="""">In this code, the cron schedule format is the string ""*/5 * * * *."" It contains 5 sections (separated with white spaces), representing a minute, hour, day of the month, and day of the week in that order. ""*/5"" means the task will run every 5 minutes. To explain the schedule format further, if you change the schedule to ""0 */5 * * *"", the job will execute every 5 hours. Also, setting all 5 fields to ""*"" means a job will run every minute.&nbsp;</p><p id="""">To create the job on your machine, run the following command:&nbsp;</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
	<code class=""language-yml"">
kubectl create -f your-cronjob-yaml-file
 </code>
</pre></div><p id="""">To see the cronjob you just created, run the <strong id="""">kubectl create -f cronjob.yaml</strong> command.&nbsp;</p><h4 id="""">Step 4: Running a Job</h4><p id="""">To run a job after creating the YAML file for it, run the <strong id="""">kubectl apply -f [yaml-file]</strong> command. Replace [yaml-file] with the actual file name for your job configuration.&nbsp;</p><p id="""">You can verify the status of your job by running the <strong id="""">kubectl get jobs</strong> command. For an even more detailed report, you can run <strong id="""">kubectl describe job [job-name]</strong>.&nbsp;</p><h4 id="""">Step 5: Deleting a Job</h4><p id="""">For logging and tracking purposes, jobs and the pods they create do not get deleted even after they stop running. However, when you no longer need them, you can clean old jobs and their pods up. To do this you can use the <strong id="""">kubectl delete jobs/[job-name]</strong> command. &nbsp;&nbsp;</p><h3 id="""">Summing Everything up</h3><p id="""">In this post, we've covered what Kubernetes Jobs are—resources that create pods that keep running until successful completion.&nbsp;</p><p id="""">You also learned how to create, configure and run Kubernetes jobs. For jobs that need to run at a specific time or repetitively, you can use the CronJob Kubernetes resource.&nbsp;</p><p id="""">Finally, you learned how to delete Kubernetes jobs after they complete. Since a record of jobs and their pods remain even after completion, if you no longer need a specific record, you can delete it by deleting the job. </p><p id="""">‍</p>",https://cdn.prod.website-files.com/603dd147c5b0a4221d1bd360/64405a11869c71f52b23aca6_022823%20(1).jpg,Image with a lot of containers,ira-casteel,7,Tue Feb 28 2023 22:30:00 GMT+0000 (Coordinated Universal Time),kubernetes,
Kubernetes Namespaces: The Ultimate Guide,kubernetes-namespaces-the-ultimate-guide,62aa5a70cd5ba27d9d0d718a,62ddd7edee175f325fa9893f,Sun Jul 24 2022 23:38:21 GMT+0000 (Coordinated Universal Time),Fri Apr 21 2023 18:32:33 GMT+0000 (Coordinated Universal Time),Fri Apr 21 2023 18:46:03 GMT+0000 (Coordinated Universal Time),"In this post we talk about kubernetes namespaces. We discuss what are they, how they work and what they're useful for","<p id="""">The Kubernetes learning curve can be quite steep, but it's relatively easy to get started, especially today since many cloud providers offer easy-to-deploy, managed Kubernetes solutions. Therefore, you can run your application on a Kubernetes cluster in a few minutes. One of the things you'll probably discover after taking your first steps are Kubernetes namespaces. They're fundamental to Kubernetes, and you'll be using them extensively. But what are they actually, how do they work, and how are they useful? This post will give you the answers.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1000px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1000px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/62ddd77648e9484213d117b4_eIKjNczdOdjMDxneMsTXePw3exAJhwdtnuY_Evyi1TywtxxpfpjgUrc7U7u0svKw31rJvk_kK5ynA9CujgVzR3zsiCBdKJwL4k8uCnJ9-YCquJZbopP-3fgOwDVDTQQwVmUMGrtZQNepC2glbVMYNw.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><h3 id="""">What Are Kubernetes Namespaces?</h3><p id="""">The official Kubernetes documentation says that namespaces ""provide a mechanism for isolating groups of resources within a single cluster."" The word ""grouping"" is key here. Namespaces are simple Kubernetes resources (just like deployments and pods) that can be used to create groups of resources. You can think of them as mini virtual clusters within your cluster. If you just started with Kubernetes and have been deploying things on it without specifying a namespace (we'll talk about how to do that later), you were still using a namespace. Every Kubernetes cluster comes with a few namespaces, one of which is called the ""default"". And that's where your deployment will end up if you don't specify otherwise.</p><h3 id="""">How Are Kubernetes Namespaces Useful?</h3><p id="""">Now that you know what Kubernetes namespaces are, let's talk about what they can be used for. The main purpose is isolation in multi-tenant environments. Imagine you have two separate teams using the same Kubernetes cluster. Both teams want to deploy, for example, an nginx server. Let's say both teams try to create a deployment as follows:</p><p id="""">Only one team will succeed. The other will get an error message saying that an nginx deployment already exists. A simple solution for that could be to make the names of the deployments unique by, for example, adding a number or the name of the team to the deployment name.</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yaml"">
kubectl run nginx-teamA --image=nginx
kubectl run nginx-teamB --image=nginx
</code>
</pre></div><p id="""">This would work, but it would be prone to errors, and it requires extra overhead. You won't be able to simply follow a tutorial on the internet about Kubernetes and copy and paste the commands from there because you'd have to adjust the names of resources every single time. Not great. That's exactly why namespaces exist. They isolate resources. This means you can have two nginx deployments running with the same name but in two separate namespaces. From each team's perspective, it would look like they have their own clusters. They won't have to worry about the other team's deployments.</p><h4 id="""">Keeping Your Cluster Tidy</h4><p id="""">Another good thing about using namespaces is that it's easier to keep your Kubernetes cluster clean. For example, if you're doing a lot of tests on your cluster or creating a lot of proof-of-concept deployments, it may be complicated to clean up afterward. If, for example, you have a hundred deployments running on your cluster, making sure that you delete the old ones used only for some tests can be not only time-consuming but even dangerous. Imagine deleting the production application by mistake when you just wanted to delete its test version. Namespaces can solve that problem too. Simply create a new namespace for each test or POC you want to run, and once you're done, you only need to delete that namespace. All resources within the namespace will be deleted automatically with it.</p><h4 id="""">Environment Separation</h4><p id="""">Some people even use namespaces for separating environments. For example, you could have your application's development and production instances running on the same Kubernetes cluster separated by namespaces. It's cheaper and easier than running two separate clusters. However, it comes with its own disadvantages, so make sure you understand all the implications of doing so.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1000px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1000px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/62ddd7765b94183e29a83744_d9qpxSuLgT1HA4sZyPa9Ruauf_J2WMiVW5sn7OS8JCHn_YsF5HUiGXLjQnGzSTedLeE1adC8wuXspUDdufcp65m2l0HwoG_wRG5D6_d9JTqky5rHnccfCMJ20jhSxXq8S1qZeGZJbEB5KIEHIOgBqA.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><h4 id="""">Logical, Not Physical, Separation</h4><p id="""">There are many use cases for namespaces, and you can use them however you like as long as it makes sense for your organization. However, you should remember that namespaces provide only a logical and not a physical separation of resources. This means that isolation provided by namespaces is not the same as separation provided by separate clusters. In fact, you should think of namespaces more in terms of grouping capabilities than isolation.</p><p id="""">This is especially important when using namespaces, for example, to separate environments for your application. You should know that your development instance can break your entire cluster, for example, due to too aggressive performance testing or even simple misconfiguration. So, if you run development and production instances in separate namespaces but on the same cluster, breaking the development environment could also bring your production instance down.</p><p id="""">You also should be aware that there is no network isolation between namespaces by default. This means that pods in one namespace can freely talk to pods in any other namespace. This can be an advantage or disadvantage, depending on your needs. The good news is that it's relatively easy to implement namespace-based network isolation for your cluster. You just need to be aware that it doesn't happen automatically.</p><h3 id="""">How Do Namespaces Work in Kubernetes?</h3><p id="""">Enough of theory. Let's see namespaces in action. The basic usage of namespaces is the same as with any other objects in Kubernetes. It means you can execute commands like <strong id="""">kubectl create</strong> or <strong id="""">kubectl get</strong> for namespaces. Let's try that.</p><h4 id="""">kubectl get namespaces</h4><p id="""">We mentioned before that Kubernetes comes with a namespace called ""default"". In fact, there may be more namespaces on your brand-new cluster, depending on how it was deployed and which version of Kubernetes you're using. Let's validate that. To see the list of namespaces on your cluster, you can execute <strong id="""">kubectl get namespaces:</strong></p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yaml"">
$ kubectl get namespaces
NAME              STATUS   AGE
default           Active   69s
kube-system       Active   69s
kube-public       Active   69s
kube-node-lease   Active   69s
</code>
</pre></div><p id="""">As you can see, my cluster comes with four namespaces. Those named with prefix <strong id="""">kube- </strong>are Kubernetes's own namespaces. <strong id="""">kube-system </strong>holds Kubernetes components, so you definitely don't want to delete anything there. In fact, if you're a beginner, you should not touch any <strong id="""">kube-</strong> prefixed namespaces. For you, there is the <strong id="""">default</strong> namespace created. As mentioned before, this is the namespace where all your resources will be deployed if you don't specify otherwise. For example, if I execute <strong id="""">kubectl get pods</strong> on an empty cluster, I'll see the following message:</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yaml"">
$ kubectl get pods
No resources found in default namespace.
</code>
</pre></div><p id="""">Following the same logic, if I create a pod without specifying a namespace, it will be created in a default namespace:</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yaml"">
$ kubectl run nginx --image=nginx --restart=Never
pod/nginx created

$ kubectl describe pod nginx
Name:         nginx
Namespace:    default
(...)
</code>
</pre></div><p id="""">As with many other Kubernetes objects, you can also use a shortcut, in this case <strong id="""">ns</strong>, instead of typing <strong id="""">namespace</strong> every time, so executing <strong id="""">kubectl get ns </strong>will have the same effect as <strong id="""">kubectl get namespaces:</strong></p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yaml"">
$ kubectl get ns
NAME              STATUS        AGE
default           Active        25m
kube-system       Active        25m
kube-public       Active        25m
kube-node-lease   Active        25m
</code>
</pre></div><h4 id="""">kubectl create namespace</h4><p id="""">Now that you know how to see which namespaces you have in your cluster, let's add some more. For this, you can execute <strong id="""">kubectl create namespace</strong> followed by the name of the desired namespace:</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yaml"">
$ kubectl create namespace frontend
namespace/frontend created
</code>
</pre></div><p id="""">If you execute <strong id="""">kubectl get namespaces</strong> again, you should see your new namespace in the list now:</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yaml"">
$ kubectl get namespaces
NAME              STATUS   AGE
default           Active   14m
kube-system       Active   14m
kube-public       Active   14m
kube-node-lease   Active   14m
frontend          Active   8s
</code>
</pre></div><p id="""">Now that you have a new namespace, you can append <strong id="""">--namespace=frontend</strong> to any other <strong id="""">kubectl</strong> action to specify that you want to use that specific namespace. So, for example, to deploy your nginx deployment in that namespace, you can run the following:</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yaml"">
$ kubectl run nginx --image=nginx --restart=Never --namespace=frontend
pod/nginx created
</code>
</pre></div><p id="""">If you check the pods on your cluster now, you'll see no running pods.</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yaml"">
$ kubectl get pods
No resources found in default namespace.
</code>
</pre></div><p id="""">Wait, what? Well, read the message again. It says that there are no resources in the <strong id="""">default </strong>namespace. This is expected since we just deployed nginx in the <strong id="""">frontend</strong> namespace. Therefore, you need to append <strong id="""">--namespace=frontend</strong> to the <strong id="""">kubectl get pods</strong> command:</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yaml"">
$ kubectl get pods --namespace=frontend
NAME    READY   STATUS    RESTARTS   AGE
nginx   1/1     Running   0          3m7s
</code>
</pre></div><p id="""">If you get tired of adding the long <strong id="""">--namespace=frontend</strong> parameter every time, you can also force your <strong id="""">kubectl</strong> to use a specific namespace by default instead of the default ""default"" namespace. You can do that by executing the following:</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yaml"">
kubectl config set-context --current --namespace=frontend
</code>
</pre></div><p id="""">You just need to remember that all your commands will now be executed against the <strong id="""">frontend</strong> namespace and not the <strong id="""">default </strong>namespace.</p><h4 id="""">kubectl delete namespace</h4><p id="""">Deleting a namespace is as straightforward as creating one. You can do it by executing the <strong id="""">kubectl delete namespace</strong> followed by the name of the namespace you want to delete. Here's an example:</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yaml"">
$ kubectl get namespaces
NAME STATUS AGE
default Active 22m
kube-system Active 22m
kube-public Active 22m
kube-node-lease Active 22m
frontend Active 8m16s

$ kubectl delete namespace frontend
namespace ""frontend"" deleted

$ kubectl get namespaces
NAME STATUS AGE
default Active 22m
kube-system Active 22m
kube-public Active 22m
kube-node-lease Active 22m
</code>
</pre></div><p id="""">Keep in mind that this will delete all resources within the namespace too. Your pods won't automatically move to another namespace. Also keep in mind that for the same reason, deleting the namespace can sometimes take a long time. If you have a lot of resources running in the namespace, Kubernetes will first try to delete all of them before deleting the namespace itself. For example, deleting pods can take some time, depending on their configuration.</p><h4 id="""">kubectl describe namespace</h4><p id="""">Again, as with any other Kubernetes object, you can execute <strong id="""">kubectl describe</strong> for namespaces, which should give you more detailed information about the namespace. Unlike most resources, however, namespaces are simple objects, therefore there isn't usually much information:</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yaml"">
$ kubectl describe namespace default
Name:         default
Labels:       kubernetes.io/metadata.name=default
Annotations:  <none>
Status:       Active
No resource quota.
No resource limits.

</code>
</pre></div><h3 id="""">Namespaces in YAML</h3><p id="""">It's great to know how to use namespaces with <strong id="""">kubectl,</strong> but in the real world, you'll probably manage all your Kubernetes resources with YAML files. You'll then need to specify which namespace to use in that YAML file. How do you do that? It's straightforward. Just add <strong id="""">namespace: [namespace_name]</strong> in the <strong id="""">metadata</strong> section of your Kubernetes definition file.</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yaml"">
apiVersion: v1
kind: Pod
metadata:
  name: nginx
  namespace: frontend
  labels:
    name: nginx
spec:
  containers:
  - name: nginx
    image: nginx
</code>
</pre></div><p id="""">You can also create a namespace from a YAML file. It follows the same structure as your other Kubernetes YAML files, so you need to specify <strong id="""">apiVersion</strong>, which in the case of namespaces is <strong id="""">v1</strong>, then <strong id="""">kind</strong>, which—you guessed it—is <strong id="""">namespace</strong>. Then just define its name in the metadata section.</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yaml"">
apiVersion: v1
kind: Namespace
metadata:
  name: backend
</code>
</pre></div><p id="""">It's the simplest Kubernetes file you've ever seen, isn't it? You can apply it like any other Kubernetes file using <strong id="""">kubectl apply -f namespace.yaml</strong>.</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yaml"">
$ kubectl apply -f namespace.yaml 
namespace/backend created
</code>
</pre></div><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1000px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1000px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/62ddd776e38bd0bb7890555e_-rNBoYXLDFD1-V7Q-xMG3CX_meeDJZc2CwnU_4gCqg_Ws6cYeapms9WhznD8hGrq75YUwK9fRch2EJADyr6mZMg9Eet20bX8wvwDSqfLQV2JufjhlTgfYRVS5Oe-2XM5TcJiFi99k3zZlwajo9svdg.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><h3 id="""">Summary</h3><p id="""">Kubernetes namespaces are extremely useful. In this post, you learned what they are and how to use them. Now it's up to you and your company how to use them. Just remember that because namespaces can't be nested, it's also possible to overuse them. Namespaces should bring you grouping capabilities, making it easier for you to manage your resources. But if you create too many unnecessary namespaces, you won't make anything easier. But these are extreme cases, and namespaces are usually easy to get right.</p><p id="""">If you want to learn more about Kubernetes, check out other articles on<a href=""https://release.com/blog""> our blog</a>.</p>",false,,,,https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63e41a381b58a53b8322b1d8_072922%20(1).jpg,a laptop on a desk,ashley-penney,7,Fri Jul 29 2022 23:37:00 GMT+0000 (Coordinated Universal Time),,
Kubernetes Pod: A Beginner's Guide to an Essential Resource,kubernetes-pod-a-beginners-guide-to-an-essential-resource,62aa5a70cd5ba27d9d0d718a,630f86b3463c99fbd8b881cd,Wed Aug 31 2022 16:05:07 GMT+0000 (Coordinated Universal Time),Fri Apr 21 2023 18:02:37 GMT+0000 (Coordinated Universal Time),Fri Apr 21 2023 18:22:03 GMT+0000 (Coordinated Universal Time),Pods are essential to any Kubernetes cluster. Learn everything about one of the most important Kubernetes resources.,"<p id="""">Kubernetes is a complex tool, but taking your first steps is relatively easy. This is especially true today when all major cloud providers offer easy one-click creation of Kubernetes clusters; you can have a fully working Kubernetes cluster in a matter of minutes. So, what do you do then? You'll probably deploy some pods. Pods are arguably the most important Kubernetes resources. You may have heard about them already, since deploying pods is usually one of the first things in any Kubernetes tutorial. You may have even heard ""they're kind of like containers."" In this post, you'll learn everything you need to know about pods.&nbsp;</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1429px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1429px""><div id=""""><img alt=""A picture containing vegetable, green, pea, edible-pod peaDescription automatically generated"" src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/630f8582ef2fdb3f6a198bb4_BUlK5A1MnwQ2qk5RD1GvrM1mSUdl8jVZqpcSTifuFM_8rLLSEYr4hkrB9--8elvsIQOtQU6rK0U_czcdnAHh6rj6DSlcTzF5_EZW6k4W-GG3vhIy57V7grHe4iWq4o31gKooG9ZJVnRfAQfAwgzugRU.jpeg"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><h3 id="""">Kubernetes Pods 101</h3><p id="""">Before Kubernetes, everyone was talking about containers. When you wanted to deploy only one small microservice, you'd say that you needed to deploy ""one container."" On Kubernetes, everyone talks about pods instead. So, when you only want to deploy one microservice, you'll say that you need to deploy one pod.&nbsp;</p><p id="""">Are pods the same as containers, then? Well, not really. A pod is the smallest deployable unit in a Kubernetes world. This means that you can't directly deploy a single container in Kubernetes. If you want one container running, you need to package it into a pod and deploy one pod. A pod can also contain more than one container. It's basically like a box for containers.&nbsp;</p><p id="""">Long story short: if you mainly deploy single containers, there isn't much difference between a pod and a container. Technically, a pod encapsulates your container, but in general you can treat it similarly to a container. But pods' ability to contain more than one container is what opens doors of possibilities. We'll dive into that later in this post. But before that, let's talk about pod lifecycles.&nbsp;</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1000px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1000px""><div id=""""><img alt=""Graphical user interface, text, applicationDescription automatically generated"" src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/630f858259b9a104d5f637c9_Obvp1sgyHqOzACcIPxTw9oE1DdA7g23uXXcawB4tjtyUOhN8brTBIpoW_GRstIgBnDVCn5h46sMHZulEA3-sKmA9xQsDcTsyZTfDHBQRBReN1LLFJMl1jym69iPYFY4w9vl9ielVE0FRJ7dinH8VTZk.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><h3 id="""">Pod Lifecycles</h3><p id="""">Just like many other resources Kubernetes pods can be in a pending, running, or succeeded/failed state. You can check the status of your pod by executing <strong id="""">kubectl describe pod [your_pod_name]:</strong>&nbsp;</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yaml"">
$ kubectl describe pod nginx-deployment-6595874d85-hnjzw
Name:           nginx-deployment-6595874d85-hnjzw
Namespace:      default
Priority:       0
Node:           k3s-worker3/10.133.106.222
Start Time:     Sun, 21 Aug 2022 12:24:58 +0200
Labels:         app=nginx
                pod-template-hash=6595874d85
Annotations:    <none>
Status:         Pending
(...)
</code>
</pre></div><p id="""">As you can see from the snippet above, my pod is in a <strong id="""">Pending</strong> state. So, what do these states mean?&nbsp;</p><h3 id="""">Pending</h3><p id="""">Pending, as the name suggests, means that the pod is waiting for something. Usually, it means that Kubernetes is trying to determine where to deploy that pod. So, in normal circumstances, you'll see your pod in the pending state for the first few seconds after creation. But it may also stay in a pending state longer if, for example, all your nodes are full and Kubernetes can't find a suitable node for your pending pod. In such a case, your pod will stay in a pending state until some other pods finish and free up resources or until you add another node to your cluster.&nbsp;</p><h3 id="""">Running</h3><p id="""">Running is pretty straightforward: It's when everything is working correctly and your pod is active. There is a small caveat to this, though. If your pod consists of multiple containers, then your pod will be in the status ""running"" if at least one of its primary containers starts successfully. This means there's a chance that your pod will be in a running state even though not all containers are actually running. So, in the case of multiple containers, it's always best to double-check individual container states to be sure.&nbsp;</p><h3 id="""">Succeeded/Failed</h3><p id="""">Succeeded or failed is what comes after running. As you can imagine, you'll see ""succeeded"" when your pod did its job and finished as expected, and you'll see ""failed"" when your pod terminated due to some error. And again, in the case of multiple containers in one pod, you need to be aware that your pod will end up in a failed state if at least one of the containers ends up having issues.&nbsp;</p><h3 id="""">Unknown</h3><p id="""">The other phase a pod can be in is called ""unknown,"" and you probably won't see it often. A pod will be in a state unknown when Kubernetes literally doesn't know what's happening with the pod. This is usually due to networking issues between the Kubernetes control plane and the node on which the pod suppose to run. &nbsp;&nbsp;</p><h3 id="""">What Are Pods Used for?</h3><p id="""">Now, the big question: What are pods actually used for? The simple answer would be ""to run your application."" At the end of the day, the point of running Kubernetes is to run containerized applications on it. And pods are the actual resources that make it possible. They encapsulate your containerized application and allow you to run it on your Kubernetes cluster.&nbsp;</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1000px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1000px""><div id=""""><img alt=""ApplicationDescription automatically generated with medium confidence"" src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/630f858257da7c4d4f95642a_IrKS4TFjAyFqCp0AxyI7xwHnz62rZWpHshl726OOX4hG6ga1d8JZ8qti0408eNmnlY4jqegCc8nWw3e7TcYgIYsIEn2E-KZ2fRgjTUWZyeTLGmysFEb03j_BMsXtROfINCGy_VlxWqO8lDPr91i0rKQ.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><p id="""">However, it's worth mentioning that usually you won't actually be deploying pods themselves. You'll be using other, higher-level Kubernetes resources like <a href=""https://kubernetes.io/docs/concepts/workloads/controllers/deployment/"" target=""_blank"" id="""">Deployments</a> or <a href=""https://release.com/blog/kubernetes-daemonset-tutorial"" id="""">DaemonSets</a> that will create pods for you.&nbsp;</p><h3 id="""">Pods vs. Other Resources</h3><p id="""">Pods are only one of many Kubernetes resource types. Most other types are directly or indirectly related to pods, because as we already said, pods are resources that will actually be running your application on the cluster. Therefore, pretty much anything that your application may need—be it a secret or storage or a load balancer—will all need to somehow relate or connect to a pod.&nbsp;</p><p id="""">Kubernetes secrets can be consumed by pods. Kubernetes service resources used to expose a containerized application on your cluster to the network or internet need to reference a pod. Volumes in Kubernetes are mounted to pods. Kubernetes ConfigMaps used to store configuration files are loaded to pods. These are just a few examples, but in general, pods are usually at the center of everything that's happening on Kubernetes.&nbsp;</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1431px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1431px""><div id=""""><img alt=""A picture containing pea, vegetable, edible-pod peaDescription automatically generated"" src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/630f8583a7aae48da226e7f4_AwpjoRAauYZPSUSlx0gB39HAC-gpeJmVxGzSbG3f9rT5Ks5KvlLeLbKoB_O4cDEF1j-fjmM826FoIiZrCcsGf4pc3Z6hmMOtrnY5NXK33yNGRv4mO6HakpqyZ-Ma3jJYzO_O6jXMMQr9ehoBVfy09ZI.jpeg"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><h3 id="""">How to Create A Pod</h3><p id="""">I'll show you how to create a pod, but be aware that normally you wouldn't create pods directly. You should use higher-level resources like <a href=""https://kubernetes.io/docs/concepts/workloads/controllers/deployment/"" target=""_blank"">Deployments</a> that will take care of creating pods for you. But if you ever need it for testing or learning purposes, you can create a pod with the following YAML definition:&nbsp;</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yaml"">
apiVersion: v1
kind: Pod
metadata:
  name: nginx-pod-example
spec:
  containers:
    - name: nginx
      image: nginx
</code>
</pre></div><p id="""">You can apply it just like any other Kubernetes YAML definition, using <strong id="""">kubectl apply -f:</strong>&nbsp;</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yaml"">
$ kubectl apply -f pod.yaml 
pod/nginx-pod-example created

$ kubectl get pod nginx-pod-example
NAME                READY   STATUS    RESTARTS   AGE
nginx-pod-example   1/1     Running   0          6s
</code>
</pre></div><h3 id="""">Pods With Multiple Containers</h3><p id="""">We mentioned pods with multiple containers already, so let's dive into that a bit more. The first thing for you to know is that pods' ability to run multiple containers is not something you should overuse. For example, it's not meant to be used to combine front-end and back-end microservices into one pod. Quite the opposite; you actually shouldn't combine multiple functional microservices into one pod.&nbsp;</p><p id="""">Why does Kubernetes give you that option then? Well, it's for a different purpose. Putting more than one container into a single pod is useful for adding containers that are like assistants or helpers to your main container. A common example is log gathering containers. Their only job is to read logs from your main container and forward it (usually to some centralized log management solution). Another example is secret management containers. Their job is to securely load secrets from some secret vault and securely pass it to your main container.&nbsp;</p><p id="""">As you can see, multiple containers in a pod are typically used in the main container + secondary containers configuration. We call these secondary containers ""sidecar containers.""&nbsp;</p><p id="""">Of course, even though it's not usually recommended, there's nothing stopping you from combining two containers into one pod. If you have a very specific use case and you think it would make sense in your case, you can add more containers to your pod. You just need to be aware of the consequences of such an approach. The main one is that, in the case of the failure of the pod, both containers will die.&nbsp;</p><h3 id="""">Summary</h3><p id="""">As you can see, pods are pretty straightforward resources. In most cases, you can treat them the same as containers, but they do offer extra sidecar functionality when necessary.&nbsp;</p><p id="""">Learned all you need to for pod basics? Read on to our advanced pod concepts article <a href=""https://release.com/blog/kubernetes-pods-advanced-concepts-explained"">here</a>!</p>",false,,,,https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63e41a381b58a53b8322b1d8_072922%20(1).jpg,a laptop on a desk,regis-wilson,4,Fri Sep 09 2022 18:00:00 GMT+0000 (Coordinated Universal Time),,
Kubernetes Pods Advanced Concepts Explained,kubernetes-pods-advanced-concepts-explained,62aa5a70cd5ba27d9d0d718a,62aa5a70cd5ba2578a0d72c5,Thu Feb 25 2021 01:31:08 GMT+0000 (Coordinated Universal Time),Thu Apr 25 2024 20:52:32 GMT+0000 (Coordinated Universal Time),Thu May 02 2024 20:19:24 GMT+0000 (Coordinated Universal Time),"In this blog post we’ll investigate certain advanced concepts related to Kubernetes init containers, sidecars","<p id="""">In this blog post we’ll investigate certain advanced concepts related to Kubernetes init containers, sidecars, config maps, and probes. We’ll show you how to implement these concepts in your own cluster, but more importantly how to apply these to your projects in <a href=""https://releasehub.com"" target=""_blank"" id="""">Release</a> for both fun and profit.</p><p id="""">We’ll start with a brief introduction to pods and containers in Kubernetes, and then show specific examples of each item listed above. Below you will find a drawing of these examples to keep yourself oriented during our bumpy ride ahead.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1024px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1024px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/603dd147c5b0a4e2931bd4af_Understanding_Advanced_Kubernetes_Concepts.jpg.jpg"" loading=""lazy"" width=""auto"" height=""auto"" id=""""></div></figure><h3 id="""">Key Kubernetes Pod Concepts</h3><p id="""">Before we begin, let’s get a brief overview of some key concepts.</p><h4 id="""">Container</h4><p id="""">In Docker, a container is an image that bundles layered filesystems which can be deployed as a runnable bundle. This container is usually built with a Dockerfile and has a startup binary or executable command.</p><h5 id="""">Sidecar Container</h5><p id="""">A <a href=""https://kubernetes.io/docs/concepts/workloads/pods/#how-pods-manage-multiple-containers"" target=""_blank"" id="""">sidecar container</a> is simply a container that runs alongside other containers in the pod. There’s no official definition of a sidecar concept. The only thing that distinguishes a container as a sidecar container is that you consider it ancillary or secondary to the primary container. Running multiple sidecar containers does not scale well, but does have additional advantages of being able to reuse configuration files and container images. The reason sidecars do not scale well is that they may be overprisioned or wasteful based on the performance of the main application container. However, the tradeoffs can make sense in legacy applications or during migrations toward truly cloud-native designs.</p><h5 id="""">Init Container</h5><p id="""">An <a href=""https://kubernetes.io/docs/concepts/workloads/pods/init-containers/"" target=""_blank"" id="""">init container</a> is simply a container that runs before any other containers in the pod. You can have several init containers that run sequentially. As each container finishes and exits properly (with a zero!), the next container will start. If an init container exits with an error or if it does not finish completely, the pod could go into a <a href=""https://releasehub.com/blog/kubernetes-how-to-debug-crashloopbackoff-in-a-container"" target=""_blank"" id="""">dreaded CrashLoopBackoff</a>. All of the containers share a filesystem, so the benefit here is that you can use or reuse container images to process, compile, or generate files or documents that can be picked up later by other containers.</p><h5 id="""">Probes</h5><p id="""">Although the word “probes” may stir up visions of Alien tools used for discovery and investigation of humans, fear not. These probes will only make your services run better! Kubernetes has <a href=""https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/"" target=""_blank"" id="""">several probes</a> for defining the health of containers inside a pod. A startup probe allows the scheduler to tolerate delays in a slow-startup container. A liveness probe allows Kubernetes to restart a faulty or stalled container. A readiness probe allows a container to receive traffic only when it is ready to do so.</p><h4 id="""">Pod</h4><p id="""">You may harbour some fear in the back of your mind of “pod people” or vegetable clones grown to replace humanity with mindless zombies who hunt and destroy mankind. However, in Kubernetes, the smallest managed unit is the pod. But a pod could be composed of several containers that run in a single process space and filesystem. A pod is usually composed of one container that runs a single process as a service. However, there are several advanced usage examples we will go into that run multiple containers for expanded options and use cases.</p><h4 id="""">Node</h4><p id="""">A Kubernetes node is ultimately a physical machine (which can have several layers of virtualisation) that runs the pod or pods, providing the critical CPU, memory, disk, and network resources. Multiple pods can be spread across multiple nodes, but a single pod is contained on a single node.</p><h4 id="""">Volumes</h4><p id="""">Volumes are simply abstractions of filesystems that can be mounted inside containers. You cannot overlap or nest volume mounts. However, there are several mount types that might be very useful to your use case.</p><h5 id="""">configMap</h5><p id="""">A <a href=""https://kubernetes.io/docs/concepts/storage/volumes/#configmap"" target=""_blank"" id="""">configMap</a> is a so-called “blob” of information that can be mounted as a file inside your container. Remember, that this is not an evil, destructive blob out to devour our planet! It is a batch of text that is treated amorphously, like a… well… blob. The usual use case here is for a configuration file or secrets mount.</p><h5 id="""">emptyDir</h5><p id="""">An <a href=""https://kubernetes.io/docs/concepts/storage/volumes/#emptydir"" target=""_blank"" id="""">emptyDir</a> is an empty filesystem that can be written into and used by containers inside a pod. The usual use case here is for temporary storage or initialization files that can be shared.</p><h5 id="""">hostPath</h5><p id="""">A <a href=""https://kubernetes.io/docs/concepts/storage/volumes/#hostpath"" target=""_blank"" id="""">hostPath</a> is a filesystem that exists on the Kubernetes node directly and can be shared between containers in the pod. The usual use case here is to store cached files that could be primed from previous deployments if they are available.</p><h5 id="""">Persistent Volume Claim (PVC)</h5><p id="""">A <a href=""https://kubernetes.io/docs/concepts/storage/persistent-volumes/"" target=""_blank"" id="""">persistent volume claim</a> is a filesystem that lasts across nodes and pods inside a namespace. Data in a PVC are not erased or destroyed when a pod is removed, only when the namespace is removed. PVCs come in many underlying flavors of storage, depending on your cloud provider and infrastructure architecture.</p><h4 id="""">Namespace</h4><p id="""">A Kubernetes namespace is a collection of resources that are grouped together and generally have access to one another. Multiple pods, deployments, and volume claims (to list a few) will run together, potentially across multiple nodes.</p><h3 id="""">Sidecars and Init Containers</h3><p id="""">The first use case we will cover involves running several containers inside a single pod. Once again, a pod here refers to one or more containers grouped together in Kubernetes, not vegetable human clones grown for evil reasons. In the following scenario, we will examine how multiple containers can share a single process space, filesystem, and network stack.</p><p id="""">Keep in mind that most docker and Kubernetes purists will tell you that running more than one process in a container, or having more than one container in a pod is not a good design and will inevitably lead to scalability and architectural issues down the road. These concerns are generally well founded. However, careful application of the following supported and recommended patterns will allow you to thrive either during your transition from a legacy stack to Kubernetes or once you are successfully running your application in a cluster.</p><p id="""">One particular use case we encounter with customers is that their application has a backend container that requires a reverse proxy like Nginx to perform routing, static file serving, and so forth. The best method to achieve this objective would be to create a separate pod with Nginx (for example) and run the two service pods in a single namespace. This gives us the flexibility to scale the backend pods and Nginx pods separately as needed. However, typically the backend service or application needs to also serve static files that are located inside the container filesystem and would not be available across the pod boundary. We agree this is not a preferred pattern to use, but it is common enough with legacy applications that we see it happen.</p><p id="""">In this scenario, we often recommend a sidecar container running Nginx which can be pulled directly from Docker Hub or a custom image can be created. We also recommend that customers reuse their backend application container as an init container that starts with a custom command for creating any initialization or other startup tasks that need to be completed before the application itself starts.</p><p id="""">One feature of this multi-container setup is that the Nginx container can use the “localhost” loopback to communicate with the backend service. Of course the sidecar container might be a logging or monitoring agent, but the principle is the same: the containers can speak with each other over a private network that is potentially not available outside of the pod, unless you make it available. In our Nginx example, the backend could be isolated so that all communication traffic inbound to the service container must be routed to the Nginx proxy.</p><p id="""">The other nice feature of this configuration is that the containers all share a common file system so that the Nginx container can access static files generated by (or stored on) the backend service container.</p><p id="""">Here is a link to our documentation that shows an example of running <a href=""https://docs.releasehub.com/reference-guide/application-settings/application-template#sidecar-containers"" target=""_blank"" id="""">sidecar</a> and <a href=""https://docs.releasehub.com/reference-guide/application-settings/application-template#init-containers"" target=""_blank"" id="""">init</a> containers on Release.</p><h3 id="""">Probes</h3><p id="""">As we have noted, probes are not just for Aliens! Kubernetes uses them to test your application stack and report on its health. Kubernetes will also take action based on these probes, just like an Alien might. There are several probes that are supported natively by Kubernetes. The main use cases we support for our customers are the liveness probe and readiness probe.</p><p id="""">The liveness probe is a way to test whether a container is “alive” or not, and if it fails the probe, then Kubernetes will restart the container. We usually recommend that your application not freeze up or have memory leaks and so forth so that a liveness probe should not be necessary. This “reboot your app to fix the problems” philosophy is not generally considered good practice. However, perfect code is impossible and when services are running in a production container environment, we know that almost anything can (and will) happen.</p><p id="""">The readiness probe is a way to test whether a container is capable of serving traffic or not, and if it fails the probe, then the service port will be removed from the ingress controller. Contrary to our stance on the liveness probe, we strongly encourage and recommend that customers implement a readiness probe on any service that receives inbound traffic. In some sense, we consider a readiness probe mandatory for your production services.</p><p id="""">Here is a link to our documentation that shows an example of using a <a href=""https://docs.releasehub.com/reference-guide/application-settings/application-template#readiness-and-liveness-probes"" target=""_blank"" id="""">liveness and readiness probe</a> for services running in Release.</p><h3 id="""">Volumes</h3><p id="""">This section gets a bit technical and tricky. Of course, no actual customer stacks would use every single type of volume, container, and probe listed in this article. But we do hope this overview shows all the features that are possible. You should carefully consider the use cases presented below and choose the one that best fits your use case.</p><p id="""">Here is a link to our documentation that shows options for our <a href=""https://docs.releasehub.com/reference-guide/application-settings/application-template#resources"" target=""_blank"" id="""">storage volume types</a>.</p><h4 id="""">configMap (Just in Time File Mounts)</h4><p id="""">A configMap (purposely spelled in <a href=""https://en.wikipedia.org/wiki/Camel_case#Programming_and_coding"" target=""_blank"" id="""">camelCase</a>) is not itself a volume in Kubernetes. Strictly speaking, a configMap is just a blob of text that can be stored in the <a href=""https://kubernetes.io/docs/concepts/overview/components/#etcd"" target=""_blank"" id="""">etcd key-value datastore</a>. However, one convenient use case Release supports is creating a container storage volume that is mounted inside a container as a file whose contents are the text blob stored in etcd. At Release, we call this customer helper function a <a href=""https://docs.releasehub.com/reference-guide/application-settings/file-mounts"" target=""_blank"" id="""">Just in Time File Mount</a>. The common use case for a configMap at Release is being able to upload a file with configuration details. For example, in our previous example involving an Nginx sidecar, the <a href=""https://www.nginx.com/resources/wiki/start/topics/examples/full/"" target=""_blank"" id="""">nginx.conf</a> file could be uploaded as a Just in Time File Mount. <em id="""">“What do we want? File Mounts! When do we want them? Just in Time!”</em></p><h4 id="""">emptyDir (Scratch Volume)</h4><p id="""">An emptyDir volume is a native Kubernetes construct Release supports for containers in a pod to share empty space that can be mounted locally. This volume is erased as soon as the pod ends its life-cycle, and it is blank to begin with. Thus, the most common use case is for a scratch or temporary location to store files that only need to be stored during the lifetime of the pod.</p><h4 id="""">hostPath (Intra-pod Cache or Shared Volume)</h4><p id="""">The next example is a native Kubernetes construct that Release supports for containers in a pod to share a filesystem path that stays on a node. The most common use case for a hostPath volume is to store cache or build data that can be generated and re-generated as needed inside a pod. Unlike an emptyDir volume that only lasts as long as the pod does, the hostPath can last as long as the application that deploys the pods. Thus, a container could generate (or compute) files, assets, or data that could be reused or incrementally updated with the next pod deployment on the same node. Release automatically sets the correct permissions and ensures that each namespace has unique files so that data are not leaked between customers.</p><h4 id="""">PVC (Long Term Persistent Storage)</h4><p id="""">The final example of a volume mount that Release offers is the ability to store data on persistent storage that is available across nodes and pods in a namespace. This long term storage is persistent and does not disappear during pod or node life cycles. Release uses Amazon Web Services (AWS) Elastic File System (EFS), which is their cloud offering of Network File System (NFSv4) storage. This allows customers to store long term data that will persist between deployments, availability zones (AZs), and node failures, and can be shared between multiple pods. The most common use cases for persistent storage of this type are for pre-production databases that need long term storage between deployments.</p><h3 id="""">Conclusion</h3><p id="""">In this article, we’ve given you an overview of key advanced concepts for Kubernetes pods that you will not find anywhere else. If you are confident and practiced in using these examples in your Kubernetes deployments, then you can consider yourself one of the members of an elite club of practitioners. This benefit does not just come with a distinguished title or piece of paper stating your qualifications: it also confers substantial success and accomplishment in your DevOps career journey.</p><p id="""">Photo by <a href=""https://unsplash.com/@wynand_uys?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText"" id="""">Wynand Uys</a> on <a href=""https://unsplash.com/s/photos/pod?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText"" id="""">Unsplash</a></p>",false,,,,https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63e3fc7932673157aa31f108_020821%20(1).jpg,Six dolphins swimming in the sea,regis-wilson,8,Tue Feb 09 2021 00:00:00 GMT+0000 (Coordinated Universal Time),kubernetes,
Kubernetes Probes: SRE Anywhere Lightning Talk,kubernetes-probes-sre-anywhere-lightning-talk,62aa5a70cd5ba27d9d0d718a,62aa5a70cd5ba2c0a30d72e1,Thu Jun 10 2021 22:11:25 GMT+0000 (Coordinated Universal Time),Fri Apr 21 2023 19:45:42 GMT+0000 (Coordinated Universal Time),Fri Apr 21 2023 20:04:43 GMT+0000 (Coordinated Universal Time),Regis Wilson presents a 5 minute video submission for a lightning talk presentation at the SRE From Anywhere event.,"<p>It was my pleasure to present a five minute lighting talk at the <a href=""https://www.catchpoint.com/sre/from-anywhere-2021"">Catchpoint SRE&nbsp;from Anywhere</a> virtual conference on June 10, 2021. You can see the video presentation which was based on my earlier blog post on <a href=""https://releasehub.com/blog/kubernetes-health-checks-2-ways-to-improve-stability"">Kubernetes probes</a>.</p><div data-rt-embed-type='true'><div style=""position: relative; padding-bottom: 56.25%; height: 0;""><iframe src=""https://www.loom.com/embed/c83889416fe547e297eeed9f155cb077"" frameborder=""0"" webkitallowfullscreen="""" mozallowfullscreen="""" allowfullscreen style=""position: absolute; top: 0; left: 0; width: 100%; height: 100%;""></iframe></div></div>",false,,,,https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63e404de251ec62705114585_61022%20(1).jpg,Catchpoint SRE from Anywhere virtual conference announcement banner,regis-wilson,5,Thu Jun 10 2021 16:25:00 GMT+0000 (Coordinated Universal Time),,
Kubernetes Secrets Management: A Practical Guide,kubernetes-secrets-management-a-practical-guide,62aa5a70cd5ba27d9d0d718a,630f810848eb73d690532f7a,Wed Aug 31 2022 15:40:56 GMT+0000 (Coordinated Universal Time),Fri Apr 21 2023 18:04:53 GMT+0000 (Coordinated Universal Time),Fri Apr 21 2023 18:22:03 GMT+0000 (Coordinated Universal Time),What are Kubernetes secrets? Learn how to do Kubernetes secrets management in this post.,"<p id="""">If you've worked with Kubernetes, you've probably heard of or used Kubernetes secrets. They are one of many Kubernetes resources. As the name suggests, they're meant to be used with secrets in your cluster. Imagine that your application running in a pod on a Kubernetes cluster needs some credentials.&nbsp;</p><p id="""">Using a Kubernetes secret is the most straightforward way to provide these credentials to your application. But are they actually secure? What's the best way to use them? Should you use some other secret management solutions for your Kubernetes cluster? Read on to learn everything about Kubernetes secrets management.&nbsp;</p><h3 id="""">Why Is Secret Management Important?</h3><p id="""">Before we dive into the do's and don'ts of Kubernetes secret management, let's take a moment to discuss why it's important in the first place. You see, Kubernetes secrets are a nice built-in semi-secret management solution, but they are not entirely secret (we'll get to that later), and they don't create a complete secret management solution. The typical problem that quickly arises when you use Kubernetes secrets is how to create and store them securely before they end up in a Kubernetes cluster. Kubernetes doesn't come with any integration to secret vaults out of the box. Therefore, they need a bit more engineering effort beyond simple creation to be secure.&nbsp;</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1000px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1000px""><div id=""""><img alt=""TextDescription automatically generated"" src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/630e5c8b5f424e71b51cf019_qXqm13SVUXQo4sXh0cZcAQ5st5VbmrqdFXsQ1KN91WX9Nr3T2lb5epNh-KDvz9O2PIZjPzP3iQqD2Ggj0grlR8g5OMXMGmxEEsV8PBHGhJi-v9G1T-qI8HBoossw9yrz9bgo8ybiieIbMdM80BsY6GvYNEDz9C4SYKBWkr-UvdXGc_HEmte2YEFSkg.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><h3 id="""">Are Kubernetes Secrets Actually Secure?</h3><p id="""">As we mentioned, a critical aspect of Kubernetes secret management is the fact that Kubernetes secrets are not actually that secret. You may be surprised to hear that, but Kubernetes secrets are not encrypted and can be easily read by anyone with access to the cluster. Kubernetes secrets are only encoded using basic <a href=""https://en.wikipedia.org/wiki/Base64"" target=""_blank"" id="""">base64</a> format. Let me show you. I'll apply the following YAML definition file of my Kubernetes secret to the cluster using the <strong id="""">kubectl apply</strong> command:&nbsp;</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yaml"">
$ cat  | kubectl apply -f -
apiVersion: v1
kind: Secret
metadata:
  name: example-secret
type: Opaque
stringData:
  username: admin
  password: super_secret_password
EOF
secret/example-secret created
</code>
</pre></div><p id="""">Now that we've created a secret, you'd expect it to be difficult to get the plain text values again from the cluster. If I execute <strong id="""">kubectl describe</strong> on our secret value, Kubernetes won't show you the values by default:&nbsp;</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yaml"">
kubectl describe secret example-secret
Name:         example-secret
Namespace:    default
Labels:       <none>
Annotations:  
Type:         Opaque

Data
====
password:  21 bytes
username:  5 bytes
</code>
</pre></div><p id="""">However, you can force it to show the values as follows:&nbsp;</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yaml"">
$ kubectl get secret example-secret -o jsonpath='{.data}'
{password:c3VwZXJfc2VjcmV0X3Bhc3N3b3Jk username:YWRtaW4=}
</code>
</pre></div><p id="""">Now, we can see the values, but as you would expect, they're not in plain text. However, as we mentioned before, the values are in base64, which is very easy to decode using base64 binary that comes installed on all modern operating systems. You only need to pipe the above output to a <strong id="""">base64 --decode</strong> command:&nbsp;</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yaml"">
$ kubectl get secret example-secret -o jsonpath='{.data.username}' | base64 --decode
admin
$ kubectl get secret example-secret -o jsonpath='{.data.password}' | base64 --decode
super_secret_password
</code>
</pre></div><p id="""">As you can see, I didn't need to specify any encryption key or certificate. Anyone who has access to my cluster could do the same.&nbsp;</p><h3 id="""">Is This a Problem?</h3><p id="""">Is this a Kubernetes bug or vulnerability? No, not really. Kubernetes is simply not a secret management tool. It allows you to use Kubernetes secrets out of the box to get you started, but if you really want to stay secure, you'd use an external secret management solution. Another aspect of this is that it's possible to make Kubernetes secrets a bit more secure by applying RBAC rules to your cluster.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1430px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1430px""><div id=""""><img alt=""A picture containing insect, colorful, brightDescription automatically generated"" src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/630e5c8bbd97c729bc17d229_xiYSpTh6mED-BoLDi7A9TKqc_zXtt550asYE8kbLIlr5ag69eTt9qs1vfbs1ltU2XSDNwg6RGkCF-QCrkdF18KNvIplwv6ow4nbgrGjg45FW621R1egA9Vyj_V2NtAk_5lSXNZE-kNFYqO8QwLdV3-S2yeM1DGFYmgyfPlUt7MVGp2dBD3CD6cOtrQ.jpeg"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><p id="""">Also, in non-multi-tenant clusters, it's not that big of an issue since access to the cluster is limited to one team anyway. Everyone who has access to the cluster can probably access the secrets too. So, the fact that Kubernetes secrets are not that secret isn't automatically bad. It simply depends on the use case.&nbsp;<br></p><p id="""">For customers who use Release in AWS, we automatically assign a KMS key at cluster creation so that their secrets are actually encrypted at rest. If you are managing your own EKS cluster, you can find out how to do that by following <a href=""https://docs.aws.amazon.com/eks/latest/userguide/enable-kms.html"" target=""_blank"" id="""">these instructions</a>.</p><p id="""">Let's get into how to actually manage secrets in Kubernetes.&nbsp;</p><h3 id="""">Secrets vs. GitOps</h3><p id="""">One of the most common issues regarding secrets in Kubernetes is that you can't simply commit secret YAML definition files to your Git repository. This is because your secret would be there in plain text (or base64-encoded values if you use <strong id="""">data </strong>instead of <strong id="""">stringData</strong>—but as we just showed, base64 is easy to decode). And since manually applying secrets would be slow and not scalable, you need to find a way to store your YAML secrets definition securely. There are two popular approaches to doing so. Let's discuss both.&nbsp;</p><h3 id="""">External Secrets</h3><p id="""">The first option is to use the <a href=""https://external-secrets.io/"" target=""_blank"" id="""">External Secrets</a> tool. The idea behind it is quite clever. You store your secret values in a safe secret vault and only commit to Git repository YAML definition files that, instead of having the actual values, hold the reference to them. Then, you install External Secrets Operator on your cluster. And once you apply this reference YAML file, the ESO will go to your secrets vault, grab the true secret value, and create your ordinary Kubernetes secret on your cluster for you. Here's an example external secret resource definition:&nbsp;</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yaml"">
apiVersion: external-secrets.io/v1beta1
kind: ExternalSecret
metadata:
  name: example
spec:
  refreshInterval: 1h
  secretStoreRef:
    name: secretstore-sample
    kind: SecretStore
  target:
    name: secret-to-be-created
    creationPolicy: Owner
  data:
  - secretKey: secret-key-to-be-managed
    remoteRef:
      key: provider-key
      version: provider-key-version
      property: provider-key-property
  dataFrom:
  - extract:
      key: remote-key-in-the-provider
</code>
</pre></div><p id="""">As you can see, there are no actual secret values here, just pointers to where that value is. So, if that file were exposed and read by someone that shouldn't read it, they still wouldn't know your actual secrets without getting access to your secrets vault.&nbsp;</p><h3 id="""">SealedSecrets</h3><p id="""">Another alternative is to use <a href=""https://github.com/bitnami-labs/sealed-secrets"" target=""_blank"" id="""">SealedSecrets</a> project. It works differently but achieves the same result. SealedSecrets lets you encrypt the content of your Kubernetes secret YAML definition file. After encrypting, the file can be safely committed to the Git repository. It could even be exposed to the internet because only the SealedSecrets controller running in your cluster will be able to decrypt it. Here's what it looks like:&nbsp;</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yaml"">
apiVersion: bitnami.com/v1alpha1
kind: SealedSecret
metadata:
  name: mysecret
  namespace: mynamespace
spec:
  encryptedData:
    foo: AgBy3i4OJSWK+PiTySYZZA9rO43cGDEq.....
</code>
</pre></div><p id="""">After the controller decrypts it, SealedSecrets will create an ordinary Kubernetes secret for you. Therefore, you won't need to adjust your application code.&nbsp;</p><h3 id="""">Bypassing Kubernetes Secrets</h3><p id="""">The tools mentioned above have one thing in common: at the end of the day, they still create ordinary Kubernetes secrets. In highly regulated environments with strict security rules, it may be necessary to avoid Kubernetes secrets completely because they're only base64 encoded. In such cases, you need to find another solution for passing secrets to your pods.&nbsp;</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1000px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1000px""><div id=""""><img alt=""TextDescription automatically generated with medium confidence"" src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/630e5c8bea873f1d6757641e_bes6Cw4zD2Wu1P0d_gA7ZhtvOLDJ_1YFWUO--ZSf5jfO7AP3kKiruiYSW2YQ_OB3huUowxQR1sdDTgEepThyijcj3GkcC0CNwccnqVtV85RgZJ_hZh-nPf2XoFeL9-ucvf51zetRaTU5AjmRQAuvQtguNXbHrUS-iJ4L_mm3EMwOCHfOPy74fVCDpw.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><h3 id="""">Kubernetes Secrets Store CSI Driver</h3><p id="""">One option is to use Kubernetes's own new <a href=""https://secrets-store-csi-driver.sigs.k8s.io/"" target=""_blank"" id="""">Secrets Store CSI Driver</a>. With this option, you store your secrets in the external secrets store. Then Secrets Store CSI Driver will load them from there and mount them directly to your pods as volumes. Therefore, you'll bypass Kubernetes secrets resources completely.&nbsp;</p><p id="""">However, there are two downsides to this approach. First is the fact that you'll need to adjust your application to load secrets from files instead of from <a href=""https://release.com/blog/kubernetes-environment-variables"" id="""">environment variables</a> like with normal Kubernetes secrets. Second, Secrets Store CSI Driver currently has alpha functionality, so it may not be fully stable.&nbsp;</p><h3 id="""">Hashicorp Vault Injector</h3><p id="""">Another option is to use Hashicorp Vault together with their <a href=""https://www.vaultproject.io/docs/platform/k8s/injector"" id="""">Secret Injection</a> option. The concept is similar to the Kubernetes Secrets Store CSI Driver. You store your secrets in Hashicorp Vault. Then, Hashicorp Vault Agent Injector will get the secrets for you and load them directly to the pod, bypassing Kubernetes secrets. And similarly to CSI Driver, Hashicorp Injector will load your secrets as volumes. However, in the case of Hashicorp, it will be shared memory volume instead of standard inline volume.&nbsp;</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1431px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1431px""><div id=""""><img alt=""A picture containing wall, indoor, white, oldDescription automatically generated"" src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/630e5c8b2a23637a0ee6796e_o_ksV_nnjvF8i5eQNrlaLJhh-ZrgL0NIrBBmdGbGAJjX2PRKehFXtJnBZ-T3kyGnpMX1ZMvyKHNJxVnyyZ3AESoscMq9ZjiEzK79OS2s-McZVK8h_9zErJYRt2PHascS2FKnkFQ0lSgLEhlgK5y8Jc0lwM7l4wBeE1fNNIrzC7z_AKJH6_BdOSyhhQ.jpeg"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><h3 id="""">Keep Your Secrets Safe</h3><p id="""">As you can see, Kubernetes secret management isn't as straightforward as one may think. It's not as simple as creating Kubernetes secret resources for your pods. Besides the fact that these secrets are not so secret, you must also consider the whole secret life cycle. Even if base64 encoding is enough in your case, you still need to figure out how to store your Kubernetes secret YAML definitions without exposing them.&nbsp;</p><p id="""">In this post, you learned a few ways to do that—and how to bypass Kubernetes secrets completely and pass your credentials directly to your pods. Your choice of option will depend on your use case and company specifics.&nbsp;</p><p id="""">If you want to learn more about Kubernetes or Security, look at <a href=""https://release.com/blog"">our blog</a> for more articles.</p>",false,,,,https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63e41ca01b58a5d88b22e012_090722%20(1).jpg,a laptop on a table,ashley-penney,5,Wed Sep 07 2022 18:00:00 GMT+0000 (Coordinated Universal Time),,
Kubernetes StatefulSet: When and How to Use It,kubernetes-statefulset-when-and-how-to-use-it,62aa5a70cd5ba27d9d0d718a,6318c4a638507d099e1597ca,Wed Sep 07 2022 16:19:50 GMT+0000 (Coordinated Universal Time),Fri Apr 21 2023 17:49:27 GMT+0000 (Coordinated Universal Time),Fri Apr 21 2023 18:22:03 GMT+0000 (Coordinated Universal Time),"At first glance, StatefulSets are very similar to standard Deployments, but there are some important differences.","<p id="""">‍<a href=""https://en.wikipedia.org/wiki/Kubernetes"" target=""_blank"" id="""">Kubernetes</a> was designed with stateless microservices in mind. But these days, it also comes with support for stateful applications, which is especially handy if you want to migrate your applications gradually. At first glance, StatefulSets are very similar to standard Kubernetes Deployments, but there are some important differences. In this post, you'll learn what StatefulSets actually are and when and how to use them.&nbsp;</p><h3 id="""">What Are Stateful Applications?</h3><p id="""">Before we start explaining Kubernetes StatefulSets, you need to understand what stateful means and the difference between stateless and stateful applications. When you think about cloud-native applications, you most likely have a picture of an application that can run in multiple copies and where any copy can be restarted at any time while traffic is being redirected effortlessly to other instances.&nbsp;</p><p id="""">In order for this model to work, the application needs to get some data from somewhere, execute some functions, and return the data. It can't store the data itself, and it shouldn't be dependent on other pods. If it were, you wouldn't be able to easily kill that instance without risking data loss. But in general, if an application doesn't store data itself in persistent storage and doesn't need to be started together with other microservices in a specific order, then it's stateless.&nbsp;</p><h4 id="""">Stateful vs Stateless</h4><p id="""">And as you can probably guess, stateful applications are the opposite. They do need to keep some data in order to work. The most common example of a stateful application is a database. The whole point of, for example, MongoDB or MySQL applications is to store data. Therefore, both MongoDB and MySQL are stateful. You can't simply kill the instance of MongoDB and restart it somewhere else and expect it to work.&nbsp;</p><p id="""">First of all, by killing it unexpectedly, the data may get corrupted. And second, you can't simply restart MongoDB somewhere else because you need to first somehow reference the same data for it, which usually means either copying data or attaching the same persistent storage to it.&nbsp;</p><p id="""">Using persistent data is not the only thing that can make an application stateful. If your microservice doesn't store any data but needs to be started in a specific order with other microservices, then it's also stateful. Or if you can't simply roll out a new version of the application because you also need to follow specific update procedures, then your application is most likely stateful.&nbsp;</p><p id="""">Now that you have that clear, let's talk about Kubernetes StatefulSets.&nbsp;</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1000px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1000px""><div id=""""><img alt=""Graphical user interface, text, applicationDescription automatically generated"" src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/6318c2597911654db083617a_wlu2ur7k7w1sObr6stXJ4VhzNHs15H73vToTTrhPg9XyK0IJ4uZgFyBbeTsvm9BvNImmnH1u4IuaydTAt2vKpJ84JbABozg69MUmMAxeYG04ti6lRVPdyZ5ACnFSVXmYshE5U6WK9gEnG4LYDxlZtRp0bnq83eLWcZGD4ULFyF6cmICH-exn-fdU.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><h4 id="""">Kubernetes StatefulSet</h4><p id="""">Traditionally, a normal Kubernetes Deployment assumes that your application is stateless. Therefore, Kubernetes may, at any point, just kill one of your instances and redeploy it elsewhere on the cluster when necessary. If your application is stateful, this could easily create an issue. You would either end up with corrupted data or your application could simply crash and require manual intervention.&nbsp;</p><p id="""">Therefore, specifically for stateful applications, Kubernetes offers so-called StatefulSets. These are special Kubernetes objects that will create and manage pods for your stateful application. Unlike in a standard Deployment, StatefulSets are aware that your application is stateful and will therefore treat it accordingly.&nbsp;</p><h4 id="""">Stable And Ordered</h4><p id="""">Kubernetes StatefulSets provide two main advantages (for stateful applications) over Deployments: a stable identity of the pods and the ability to follow specific Deployment orders.&nbsp;</p><p id="""">Stable identity means persistent identity in this case. And persistent pod identity means that when a pod gets rescheduled for whatever reason, it will have the same network identifiers and the same storage assigned to it. So, from the perspective of other pods, it will look like the same pod. This is not the case when using Deployments, and it's very important for the proper working of stateful applications.&nbsp;</p><p id="""">We already mentioned that if your application needs to be deployed or updated in a specific order, that's a good indication that it's stateful. In a traditional Deployment, if you'll have multiple pods in one Deployment, they would be deployed in a random order, which in the case of stateful application would probably mean that the application won't start properly. And therefore, this ability to follow a specific order when deploying or updating is built into the StatefulSets.&nbsp;</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1000px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1000px""><div id=""""><img alt=""Graphical user interface, text, applicationDescription automatically generated"" src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/6318c25975360834172f4ca9_Egs53Ce4E7sEkcQsHhXvHF5G4UwMlhYEWTszrunJcCknKc3L5oCZqMrK7UKf9U728uqpsnKcgCZtv2nKJbUydZ--B9Oyer8C-pRCIpWYy8zvOCVZFRrhkgm7pyQrbrag5cVZwbHP0ltpe7TLnTBc1gQK420X4G3mdbOPxIO-eUKs5A69K_8y3oqx.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><h4 id="""">Creating StatefulSet</h4><p id="""">Enough theory. Let's create some StatefulSets. The YAML definition of StatefulSets is very similar to standard Deployments and in a simple example looks like this:&nbsp;</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yaml"">
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: example-statefulset
spec:
  selector:
    matchLabels:
      app: nginx
  replicas: 1
  serviceName: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: registry.k8s.io/nginx-slim:0.8
        ports:
        - containerPort: 80
          name: web
</code>
</pre></div><p id="""">Once you save the above code in a YAML file, you can deploy it, as usual, using <strong id="""">kubectl apply</strong>:&nbsp;</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yaml"">
$ kubectl apply -f statefulset.yaml 
statefulset.apps/example-statefulset created
</code>
</pre></div><p id="""">You can then validate that everything is working with <strong id="""">kubectl get</strong>:&nbsp;</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yaml"">
$ kubectl get statefulsets
NAME                  READY   AGE
example-statefulset   1/1     2m4s

$ kubectl get pods
NAME                    READY   STATUS    RESTARTS   AGE
example-statefulset-0   1/1     Running   0          2m8s
</code>
</pre></div><p id="""">OK, your first StatefulSet is up and running. Congratulations. This was, however, a very simple example with only one pod in your StatefulSet. But you'll most likely use StatefulSets with multiple pods to get all the benefits.&nbsp;</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1430px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1430px""><div id=""""><img alt=""TextDescription automatically generated with medium confidence"" src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/6318c2599b54f8c3bd39b3f7_IURjY9ZFBr7gBd87wiAI7UVYR3tNfwrbszekkfjS4qSoKaqtU6c_ViYSm_HpLCI5LDC490-SQpEFO9c_Lp93_VQ3R_ytqSq4RuGvVjxnuCbVG1XsZUf94WDS0dKTbNtf-KEgtRCAn99JHh7g_b5ypMXSuxAOVL6We7JMnKSZaVG3p7T9ls0AB-AB.jpeg"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><h3 id="""">StatefulSets Specifics</h3><p id="""">Let's spice things up a little to see StatefulSets doing its job. Execute the following command to scale your nginx from one to ten replicas:&nbsp;</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yaml"">
$ kubectl scale statefulsets example-statefulset --replicas=10
statefulset.apps/example-statefulset scaled
</code>
</pre></div><p id="""">Now, if you watch what's happening, you'll see the specific behavior of StatefulSets:&nbsp;</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yaml"">
$ kubectl get pods
NAME                    READY   STATUS              RESTARTS   AGE
example-statefulset-0   1/1     Running             0          11m
example-statefulset-1   0/1     ContainerCreating   0          1s

$ kubectl get pods
NAME                    READY   STATUS              RESTARTS   AGE
example-statefulset-0   1/1     Running             0          11m
example-statefulset-1   1/1     Running             0          2s
example-statefulset-2   0/1     ContainerCreating   0          0s

$ kubectl get pods
NAME                    READY   STATUS              RESTARTS   AGE
example-statefulset-0   1/1     Running             0          11m
example-statefulset-1   1/1     Running             0          4s
example-statefulset-2   1/1     Running             0          2s
example-statefulset-3   0/1     ContainerCreating   0          1s

(...)

$ kubectl get pods
NAME                    READY   STATUS    RESTARTS   AGE
example-statefulset-0   1/1     Running   0          14m
example-statefulset-1   1/1     Running   0          3m19s
example-statefulset-2   1/1     Running   0          3m17s
example-statefulset-3   1/1     Running   0          3m16s
example-statefulset-4   1/1     Running   0          3m14s
example-statefulset-5   1/1     Running   0          3m13s
example-statefulset-6   1/1     Running   0          3m12s
example-statefulset-7   1/1     Running   0          3m10s
example-statefulset-8   1/1     Running   0          3m9s
example-statefulset-9   1/1     Running   0          3m7s
</code>
</pre></div><p id="""">You can see that Kubernetes provisioned all replicas in order, one by one. This is one of the differences between Deployments and StatefulSets. In Deployments, all pods will be deployed in random order, with more than one pod being created at once. In StatefulSets, it happens sequentially, and pods are even numbered and do not get a random hash assigned as part of the name, which is the case in Deployments. Moreover, if at any point one of the replicas fails to start, the whole process will stop. So, for example, Kubernetes will only create <strong id="""">example-statefulset-5</strong> after <strong id="""">example-statefulset-4 </strong>is up and running.&nbsp;</p><h3 id="""">Name Stays the Same</h3><p id="""">Following the same logic, if something happens to any of the pods, it will be recreated with the same name.&nbsp;</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-"">
$ kubectl delete pod example-statefulset-3
pod ""example-statefulset-3"" deleted

$ kubectl get pods
NAME                    READY   STATUS    RESTARTS   AGE
example-statefulset-0   1/1     Running   0          20m
example-statefulset-1   1/1     Running   0          9m41s
example-statefulset-2   1/1     Running   0          9m39s
example-statefulset-4   1/1     Running   0          9m36s
example-statefulset-5   1/1     Running   0          9m35s
example-statefulset-6   1/1     Running   0          9m34s
example-statefulset-7   1/1     Running   0          9m32s
example-statefulset-8   1/1     Running   0          9m31s
example-statefulset-9   1/1     Running   0          9m29s
example-statefulset-3   1/1     Running   0          1s
</code>
</pre></div><p id="""">This, again, differs from Deployments, where you'd get another randomly named pod. This is important for stateful applications because, most likely, each pod will hold its own state. Therefore, it's crucial not to mix them up. Also, other microservices that would connect to these pods will probably need to always connect to the same pod even if it dies and is rescheduled.&nbsp;</p><p id="""">The same applies to networking. You can always connect to a specific pod by its domain name, like <strong id="""">example-statefulset-6.nginx.default.svc.cluster.local,</strong> and you'll have a guarantee that you'll always reach the same pod. That's not the case with Deployments.&nbsp;</p><h3 id="""">Summary</h3><p id="""">Kubernetes StatefulSets are really useful. In theory, using them means doing something that Kubernetes wasn't designed to work with in the first place. But it's really hard to have every single application on your cluster stateless. Especially in big environments with dozens or even hundreds of applications, there will always be some microservice that needs to hold some state. In some cases, it simply doesn't make sense to spend time and money on redesigning a stateless application to be stateful if it won't bring much difference or business value.&nbsp;</p><p id="""">In this post, you learned what <a href=""https://docs.releasehub.com/reference-documentation/application-settings/application-template/schema-definition#stateful-sets-and-deployments"" target=""_blank"">StatefulSets</a> are and how to create them. If you want to learn more about other Kubernetes resources, take a look at <a href=""https://release.com/blog"">our blog</a>.&nbsp;</p>",false,,,,https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63e41ede77c34a2133ced373_100522%20(1).jpg,"a cell phone with the screen saying eat, sleep, code, repeat",nick-busey,5,Wed Oct 05 2022 18:00:00 GMT+0000 (Coordinated Universal Time),,
Kubernetes Volumes: What They Are and How to Use Them,kubernetes-volumes-what-they-are-and-how-to-use-them,62aa5a70cd5ba27d9d0d718a,6320cb3f6c79f456c43c0e2a,Tue Sep 13 2022 18:26:07 GMT+0000 (Coordinated Universal Time),Fri Apr 21 2023 17:46:39 GMT+0000 (Coordinated Universal Time),Fri Apr 21 2023 18:22:03 GMT+0000 (Coordinated Universal Time),Learn how to inject file systems into Kubernetes pods using volumes and talk about the different types of their uses.,"<p id="""">By default, the file system available to a Kubernetes pod is limited to the pod's lifetime. As such, when the pod is deleted, all changes are lost.</p><p id="""">But many applications will need to store data persistently, irrespective of whether a pod is running or not. For example, we need to retain data that was updated in the database or files written. Also, we may want to share a file system across multiple containers, and those may be running on different nodes.</p><p id="""">Let's take a look at Kubernetes <a href=""https://kubernetes.io/docs/concepts/storage/volumes/"" target=""_blank"">volumes</a>, which can address these problems.</p><h3 id="""">The Basics</h3><p id="""">Most data storage that applications use is ultimately file system-based, e.g., even though a database may keep some or all of its data in memory while running, it also keeps it updated in the data files on the file system for persistence.&nbsp;</p><p id="""">Volumes allow us to inject the application with a reference to a file system, which the application can then read from or write to.&nbsp;</p><p id="""">Injecting the file system makes it independent of the container's lifetime. We need to specify an absolute path where the injected file system should be mounted within the container's file system.&nbsp;</p><p id="""">Volumes may be persistent or not. There are many different types of volumes, as we shall see.&nbsp;</p><p id="""">A volume has to first be defined using the <strong id="""">volumes</strong> key, and then used by a container using the <strong id="""">volumeMounts</strong> key.&nbsp;</p><h4 id="""">Example</h4><p id="""">Below is a partial YAML snippet to illustrate how we can define and use volumes in a pod. Depending on the type of volume, its definition and usage could be in separate places.&nbsp;</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yaml"">
apiVersion: v0
kind: Pod
metadata:
  name: my-pod
spec:
  containers:
  - image: some-image-name
    name: my-container
    volumeMounts:
    - mountPath: /tempfiles
      name: temp-files-volume
  volumes:
  - name: temp-files-volume
    emptyDir: {}
</code>
</pre></div><p id="""">Here, we've defined a volume of the <strong id="""">emptyDir </strong>type. We'll see more about this later.&nbsp;</p><p id="""">Since this type can only be used at the level of a single pod, not across, it's defined along with the pod. There could be multiple containers in a pod (though usually not), and they could all use the same volume.&nbsp;</p><p id="""">So, if one container in a pod writes a new file to the volume, it would be visible to the other containers in that pod that use that volume. The name of the volume can be anything.&nbsp;</p><p id="""">The <strong id="""">volumeMounts</strong> entry under the container specifies where to mount that volume within the container's file system. In this case, we want /tempfiles.&nbsp;</p><p id="""">When the application in the container writes to /tempfiles, it'll be writing to the temp-files volume. A container may use many different volumes or none. Note that in order to use volumes, the application in the container has to use the path that we specified in mountPath.&nbsp;</p><p id="""">So, if you want to use a container image with volumes, make sure that the path it uses to read/write files matches the path we specified in <strong id="""">volumeMounts.</strong>&nbsp;</p><p id="""">A volume could—depending on its type—specify other attributes like accessModes, i.e., what kind of access it allows.&nbsp;</p><p id="""">Modes can be <strong id="""">ReadWriteOnce</strong>, <strong id="""">ReadOnlyMany</strong>, <strong id="""">ReadWriteMany</strong>, and<strong id=""""> ReadWriteOncePod</strong>. Note that specifying an access mode may not constrain the actual usage by the container. See <a href=""https://kubernetes.io/docs/concepts/storage/persistent-volumes/#access-modes"" target=""_blank"">access modes</a> for details.&nbsp;</p><p id="""">Now, let's take a look at different types of volumes.&nbsp;</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1067px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1067px""><div id=""""><img alt=""A picture containing textDescription automatically generated"" src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/6320c0c8c892840262c1e6fe_1sxUveou2fqLPVI7akR2zYAqqmDeCDnWDcqtOVcUIdbks0RLuOhDOtz-DWQ0xGeRea6iT0Zw41orhyyLWV2Qha3er0VDd4M0idWufNPeuRAPJzJCykyq6mVk-fxU41v8AfLzeD3L8MQ7x5o_q2Hcz6sIlEN92eXPVhyXKojsxt6LLhvSAoFT1XPxWg.jpeg"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><h3 id="""">Volume Types</h3><h4 id="""">EmptyDir</h4><p id="""">Kubernetes first creates an emptyDir volume when it assigns the <a href=""https://kubernetes.io/docs/concepts/workloads/pods/"" target=""_blank"">pod</a> using that volume to a <a href=""https://kubernetes.io/docs/concepts/architecture/nodes/"" target=""_blank"">node</a>. As the name suggests, it's empty to start with, i.e., it contains no files/directories.&nbsp;</p><p id="""">Containers in the same pod can share the volume so that changes made by any container are visible to others. The emptyDir volume persists as long as the pod using it does—a container crash does not delete a pod.&nbsp;</p><p id="""">Thus, it's an ephemeral or temporary kind of storage for things like cached files/data or intermediate results, etc. Also, we cannot use it to share data across pods.&nbsp;</p><h4 id="""">Persistent</h4><p id=""""><a href=""https://kubernetes.io/docs/concepts/storage/persistent-volumes/"" target=""_blank"">Persistent volumes</a> are defined by an administrator at the Kubernetes cluster level and can be used by multiple nodes in the cluster. They can retain their data even if we delete the pod using them.&nbsp;</p><p id="""">Applications in containers can request to use a persistent volume by specifying a persistent volume <strong id="""">claim</strong>. The claim specifies how much storage of what type it requires and using which access mode.&nbsp;</p><p id="""">The cluster can allocate the storage for a claim in two ways: statically if a claim is satisfied by a provisioned volume, and dynamically—for if no volume is available for a claim, the cluster may try to provision the volume dynamically based on the storage class specified.&nbsp;</p><p id="""">The claim with the allocated storage is valid as long as the pod making the claim exists.&nbsp;</p><p id="""">The<strong id=""""> reclaim policy</strong> of a volume specifies what to do with a volume once the application no longer needs the volume storage—for example, when we delete a pod using the volume.&nbsp;</p><p id="""">Accordingly, we can either retain or delete the data on the volume. Note also that the available access modes will depend on what type of volume is used. Since Kubernetes itself does not provide a file-sharing solution, we need to set that up first.&nbsp;</p><p id="""">For instance, when using NFS, we need to set up the NFS share first, and then we can refer to it when creating a persistent volume. Additionally, we may need to install drivers for supporting that volume on the cluster.&nbsp;</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1000px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1000px""><div id=""""><img alt=""Graphical user interface, text, applicationDescription automatically generated"" src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/6320c0c8e9e5b97fb4370e67_2CQomOue795VD8M6B6-HrNhmHG70i5Cj1A3FnqL6xqtNicX-Qa8dUvOJBZX6YTUK2lM3uXg0XiWU40kXvV4yiZaetuGX0eH7Nk823kfXijt-eY-hr2FDoU52qFAkk586w5e01khvZnL7Q9oir5KqBNoxJPbv4L2sWz3JePeo7XW31OJ8lAG5m7cqRQ.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><h5 id="""">YAML Example</h5><p id="""">Let's look at an example configuration for an NFS volume.&nbsp;</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yaml"">
apiVersion: v1
kind: PersistentVolume
metadata:
name: nfs-vol
spec:
capacity:
storage: 1Mi
accessModes:
- ReadWriteMany
nfs:
server: nfs-server-name
path: ""/""
mountOptions:
- nfsvers=4.1
</code>
</pre></div><p id="""">The name, capacity, and accessModes are common to all types of volumes, whereas the section at the end, ""nfs"" in this case, is specific to the type of volume.&nbsp;</p><p id="""">We can create the volume with <strong id="""">kubectl apply</strong>. To get information about a volume, we would use&nbsp;</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yaml"">
kubectl get pv <volume-name>
</code>
</pre></div><p id="""">Now, create a persistent volume claim, and again with kubectl apply&nbsp;</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yaml"">
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: my-pv-claim
spec:
  storageClassName: manual
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 3Gi
</code>
</pre></div><p id="""">Here, we can request a particular storage class (useful for dynamic provisioning), the access mode, and the amount of storage needed.&nbsp;</p><p id="""">We can query for a claim using&nbsp;</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yaml"">
kubectl get pvc <claim-name>
</code>
</pre></div><p id="""">Finally, we can use the claim in a pod:&nbsp;</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-"">
apiVersion: v1
kind: Pod
metadata:
  name: my-pod
spec:
  volumes:
    - name: my-pv-storage
      persistentVolumeClaim:
        claimName: my-pv-claim
  containers:
    - name: my-pv-container
      image: nginx
      ports:
        - containerPort: 8080
          name: ""tomcat-server""
      volumeMounts:
        - mountPath: ""/usr/data""
          name: my-pv-storage
</code>
</pre></div><p id="""">Here, we link the persistent volume to the claim we created earlier. Then, as usual, we refer to the volume to mount it at the specified path in the container.&nbsp;</p><p id="""">Next, let's go through the supported types of persistent volumes.&nbsp;</p><h5 id="""">HostPath</h5><p id="""">This is probably the easiest way to test persistent volumes.&nbsp;</p><p id="""">HostPath mounts content from the node's file system into the pod. It has specific use cases, like when the container needs to run sys tools or access Docker internals. Containers usually shouldn't make any assumptions about the host node, so good practice discourages such use.&nbsp;</p><p id="""">Also, hostPath exposes the host's file system—and potentially the cluster—to security flaws in the application. We should only use it for testing on a single node, as it doesn't work in a multi-node cluster. You can check out the <strong id="""">local </strong>volume type instead.&nbsp;</p><h5 id="""">Local</h5><p id="""">Using local storage devices mounted on nodes is a better alternative to hostPath for sharing a file system between multiple pods but on the same node.&nbsp;</p><p id="""">The volume definition contains <strong id="""">node affinity,</strong> which points to the particular node name on which the local storage is available. The controller will assign pods using the local storage volume to the node that has the local storage, thus using the node affinity to identify the node name.&nbsp;</p><p id="""">If the node with the local storage becomes unhealthy, the storage will become unavailable, and pods using it will fail too. Thus, local storage is not suitable where fail safety is important.&nbsp;</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1000px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1000px""><div id=""""><img alt=""Graphical user interface, text, applicationDescription automatically generated"" src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/6320c0c8a33c09c2baa0c991_1NFEdOhV9GiZ4tytFMMswNfL6RnP32SrpXp52eUXhHmwuAyVg57LVmwm0FH8OtrgqdLQp8YYA2bvK9GJwNDfDCAPV3B6EZf9zA9id5DhP2SFX6H4K1O2xAYqO6_vn6KjrjcCsliRkRejDWRqbmdABSIkNzzjMpgmRptmF7i8Uxr-KA1-BcaKESxbVw.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><h4 id="""">Projected</h4><p id="""">A projected volume maps several existing volume sources into the same directory. The supported volume types for this are downwardAPI, secret, configMap, and serviceAccountToken.&nbsp;</p><h4 id="""">ISCSI</h4><p id="""">iSCSI—SCSI over IP—is an IP-based standard for transferring data that supports host access by carrying SCSI commands over IP networks. <a href=""https://en.wikipedia.org/wiki/SCSI"" target=""_blank"">SCSI</a> is a set of standards for physically connecting and transferring data between computers and peripheral devices.&nbsp;</p><h4 id="""">CSI</h4><p id="""">The container storage interface defined by Kubernetes is a standard for exposing arbitrary block and file storage systems to containerized workloads. To support using a new type of file system as a volume, we need to write a CSI driver for that file system and install it on the cluster. A list of CSI drivers can be seen <a href=""https://kubernetes-csi.github.io/docs/drivers.html"" target=""_blank"">here</a>, including drivers for file systems on popular cloud providers like AWS and Azure.&nbsp;</p><h4 id="""">Fc</h4><p id="""">Fc, or <a href=""https://www.ibm.com/docs/en/ds8880/8.1.1?topic=attachment-fibre-channel-storage-area-networks"" target=""_blank"">Fibre Channel storage</a>, is a high-speed network that attaches servers and storage devices.&nbsp;</p><h4 id="""">Nfs</h4><p id="""">A <a href=""https://en.wikipedia.org/wiki/Network_File_System"" target=""_blank"">network file system</a> is a distributed file system originally developed by Sun Microsystems that's based on the <a href=""https://en.wikipedia.org/wiki/Open_Network_Computing_Remote_Procedure_Call"">open network computing remote procedure call</a>.&nbsp;</p><h4 id="""">Cephfs</h4><p id="""">A <a href=""https://docs.ceph.com/en/quincy/cephfs/"" target=""_blank"">Ceph file system</a> is a POSIX-compliant, open-source file system built on top of Ceph’s distributed object store, <strong id="""">Rados</strong>. It provides a multi-use, highly available, and performant file store.&nbsp;</p><h4 id="""">RBD</h4><p id="""">A Rados block device is the device on which the Ceph file system is built. Block storage allows us to access storage as blocks of raw data rather than files and directories.&nbsp;</p><h4 id="""">AwsElasticBlockStore (deprecated)</h4><p id="""">We can use this volume type to mount an AWS EBS store. It is now deprecated, so we should use the CSI drivers instead.&nbsp;</p><h4 id="""">AzureDisk (deprecated)</h4><p id="""">This is used to mount an Azure disk. It is now deprecated, so we should use the CSI drivers instead.&nbsp;</p><p id="""">The above list of persistent volume types is not exhaustive, but it covers the commonly used types.&nbsp;</p><h4 id="""">ConfigMap</h4><p id="""">This type of volume exposes key value pairs from a ConfigMap as files on the file system.&nbsp;</p><p id="""">Specifically, the key becomes the file name, and the value becomes the file contents. For example, the log-level=debug key value is represented as a file named log-level with contents = ""debug"". We can specify the path at which we want to mount the volume in the container. But first, we need to create a ConfigMap using <strong id="""">kubectl create.</strong>&nbsp;</p><p id="""">We can create it from properties files or literal values. It's also possible to expose the values from the ConfigMap as environment variables for a pod. See <a href=""https://kubernetes.io/docs/tasks/configure-pod-container/configure-pod-configmap/"" target=""_blank"">more</a> for details.&nbsp;</p><h4 id="""">Downward API</h4><p id="""">The downward API exposes pod and container field values to applications. The downward API volume exposes the key value pairs as files on the file system similar to ConfigMap above.&nbsp;</p><h4 id="""">Secret</h4><p id="""">This is a tempfs-based file system used to store secrets, e.g., for authentication. It's similar to ConfigMap. We need to first create a &nbsp;secret using the Kubernetes API. We can also expose secrets as environment variables.&nbsp;</p><h3 id="""">Conclusion</h3><p id="""">In this post, we've highlighted how to inject file systems into Kubernetes pods using volumes.&nbsp;</p><p id="""">We've also explored the different kinds of volumes and their uses. Using volumes allows us to use various types of storage, persist data independent of the pod, and also share data across pods. </p>",false,,,,https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63e41f36087e0d1713c299b4_101222%20(1).jpg,"A table with a cell phone, pencil, glasses and a plant",erik-landerholm,5,Wed Oct 12 2022 18:00:00 GMT+0000 (Coordinated Universal Time),,
"Sequoia leads $2.7M Seed Round to Launch ReleaseHub, Environments-as-a-Service",launch-press-release,62aa5a70cd5ba27d9d0d718a,62aa5a70cd5ba268ca0d72de,Thu Apr 29 2021 02:45:32 GMT+0000 (Coordinated Universal Time),Thu Dec 01 2022 19:25:30 GMT+0000 (Coordinated Universal Time),,"Sequoia leads $2.7M Seed Round to Launch ReleaseHub, Environments-as-a-Service","<p>SAN FRANCISCO -- ReleaseHub, maker of Environments-as-a-Service, today announced a seed round of $2.7 million led by Sequoia, with participation from Y Combinator, Rogue VC, Liquid Capital, and other angel investors. ReleaseHub is also announcing general availability of its Environments-as-a-Service platform, which allows organizations to quickly and easily deploy, manage, and reproduce production-replica environments. Customers use ReleaseHub to remove environment bottlenecks in software delivery, deliver on-demand sales demo environments, and deliver production environments and SaaS solutions into their customer’s virtual private clouds.</p><p>Environments are one of the greatest bottlenecks for software development, causing developers to sit idle and release dates to slip. Software organizations don’t provision enough environments because they are complex to stand up and costly to maintain. ReleaseHub lets developers easily create reproducible environments that are exact replicas of the production environment.</p><p>These environments run on Kubernetes in the software developer’s cloud account and have access to all of the developer’s cloud native services. ReleaseHub environments can be created on-demand, through a command line interface, with a pull request or via an API to allow for easy integration with existing CI/CD solutions.</p><p>""One of the most pervasive problems in software development is that development and production environments are never the same. Access to multiple pre-production environments that are precise replicas of production is a critical facilitator of collaboration between teams, yet managing numerous environments is so time consuming that companies ration access,” said Bogomil Balkansky, partner at Sequoia. “ReleaseHub removes these pain points with its Environments-as-a-Service platform, which makes the software development process more efficient.”</p><p>ReleaseHub was founded by Tommy McClung, Erik Landerholm and David Giffin, who led the technology team at an ecommerce company where they led the companywide effort to remove environment bottlenecks. There were no commercial solutions at the time so the founders built their own environment bottleneck solution and soon after launched ReleaseHub to commercialize Environments-as-a-Service.</p><p>""As modern cloud-based applications have become more complex with microservices and an ever growing arsenal of cloud services, it has become incredibly difficult for teams to create and maintain numerous environments,"" said McClung, ReleaseHub CEO. “We are transforming environments from an organizational headache into a one-click convenience.”</p><p>Datasaur, an artificial intelligence technology company, started using ReleaseHub to accelerate software development and testing, as well as reduce bottlenecks in pre-production development. The company needed to quickly scale and used ReleaseHub to power its production environments. ReleaseHub provided the power of Kubernetes without having to invest in a lengthy and costly Kubernetes migration.</p><p>“ReleaseHub enables Datasaur to focus on building applications without requiring a complex environment management infrastructure,” said Ivan Lee, Datasaur founder and CEO. “This has improved the efficiency of launching our services for the Datasaur community and added capabilities that have allowed us to handle our rapid growth.”</p>",false,,,,https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/608be8d175f68c36489c2d2d_Release.jpg,Illustration of 3 software development environments created by Release,,2,Thu Apr 29 2021 13:00:00 GMT+0000 (Coordinated Universal Time),,
Lessons learned from maintaining the SOC 2 Type 2 certification over the years,lessons-learned-from-maintaining-the-soc-2-type-2-certification-over-the-years,62aa5a70cd5ba27d9d0d718a,657b1eb90aec104fdf9fcbe9,Thu Dec 14 2023 15:26:49 GMT+0000 (Coordinated Universal Time),Thu Apr 25 2024 20:26:40 GMT+0000 (Coordinated Universal Time),Thu May 02 2024 20:19:24 GMT+0000 (Coordinated Universal Time),Learn why we prioritized security and compliance early in our product development and why we recommend you do the same. ,"<p id="""">Here at Release we like to say “move fast and break fewer things”. As a startup we experiment, iterate, and bring our ideas to the market faster than most mature companies. However, our speed never comes at the expense of security, compliance, and customer trust. Our customers entrust application environment creation to the Release platform. This means they test, stage, and run their applications in production using our Environments as a Service platform. Although all workloads live in the customers’ own cloud account, we facilitate and orchestrate the underlying infrastructure, so making sure our services meet and exceed security standards is non-negotiable for us. </p><p id="""">To demonstrate our commitment to secure practices, we obtained the SOC 2 Type 2 certification back in 2021 and maintain it to this day. SOC 2 is a major undertaking for any company, and many startups wonder if it’s worth the effort early on. For us, it was a team effort that ultimately made us rethink our practices and the culture we were building as a company. Here are some lessons we learned throughout the process that could help you decide if the SOC 2 stamp of approval is appropriate at your stage of growth.</p><h4 id=""""><strong id="""">What is SOC 2 and why it matters? </strong></h4><p id="""">Service Organization Controls (<a href=""https://www.aicpa-cima.com/topic/audit-assurance/audit-and-assurance-greater-than-soc-2"" id="""">SOC</a>) 2 is a compliance report standard defined by the American Institute of Certified Public Accountants (<a href=""https://www.aicpa-cima.com/home"" id="""">AICPA</a>). SOC 2 reports are issued by an independent third-party CPA after a thorough audit that demonstrates how a service organization achieves key compliance controls and objectives. SOC 2 reports focus on non-financial reporting controls that relate to the <strong id="""">security</strong>, <strong id="""">availability</strong>, <strong id="""">processing integrity</strong>, <strong id="""">confidentiality</strong>, and <strong id="""">privacy</strong> of a system. Ultimately these reports help users evaluate the risks associated with the evaluated service.</p><p id="""">You typically hear about SOC 2 Type 1 and Type 2 reports. The key differences between them are the timing and depth of the audit. Type 1 reports on the suitability of design controls at a specific point in time, while Type 2 assesses the operational effectiveness of these controls over a period, usually a minimum of six months. Most organizations prepare for Type 1 assessment, while simultaneously starting to collect the data for the Type 2 assessment to follow shortly after. </p><p id="""">SOC 2 Type 2 compliance involves a rigorous process that includes designing controls to meet Trust Service Criteria, implementing these controls, and then undergoing a thorough audit by an independent CPA to prove all controls are working as intended. </p><p id="""">The five Trust Service Criteria crucial for SOC2 Type 2 certification are:</p><ul id=""""><li id=""""><strong id="""">Security</strong>: Protection of system resources against unauthorized access.</li><li id=""""><strong id="""">Availability</strong>: Availability of the system as agreed upon in the contract.</li><li id=""""><strong id="""">Processing Integrity</strong>: Completeness, validity, accuracy, timeliness, and authorization of system processing.</li><li id=""""><strong id="""">Confidentiality: </strong>Protection of confidential information as committed or agreed.</li><li id=""""><strong id="""">Privacy:</strong> Collection, use, retention, disclosure, and disposal of personal information in conformity with an organization's privacy notice</li></ul><p id="""">When startups take this process seriously, they build resilient internal processes and standards that keep their customers safe, and set them up for success. </p><h4 id=""""><strong id="""">What did we learn from our SOC 2 Type 2 journey? </strong></h4><p id="""">After going through the initial audit, and completing subsequent evaluations here are a few things we learned, that might help other startups: &nbsp;</p><p id=""""><strong id="""">✅ Start early.</strong> The best time to start the SOC 2 preparations is before you have paying customers and even employees. It sounds early, but the sooner you set the foundation for consistent secure practices, the easier it will be to maintain them. The initial assessment identifies existing controls and shows you the gaps in your current setup (believe it, you will have gaps, so better catch them before anyone is affected), and gives you an opportunity to course correct. Many of our early customers required us to have a SOC 2 certification before signing contracts to use our services, so it was a blessing to already have one in hand to get started with actual paying customers!</p><p id=""""><strong id="""">✅ Team effort.</strong> Based on the assessment, specific controls are implemented. These range from physical security measures to IT governance and data encryption practices. This is an all-hands-on-board effort where everyone cleans up their shop. Once you set the foundation, ongoing compliance becomes standard operations for the company. When the time comes to show evidence of implemented controls, you know you did things right. </p><p id=""""><strong id="""">✅ Automate evidence gathering.</strong> Even the most diligent teams can get stressed when asked to provide specific evidence on the spot. Automating your evidence collection, vendor management and security policies makes the process run much smoother come the audit time. Tools like <a href=""https://drata.com/"" id="""">Drata</a> and <a href=""https://www.vanta.com/"" id="""">Vanta</a> centralize and automate control monitoring, reduce the manual toil and give you real-time visibility into your security posture during and between the audits. </p><p id=""""><strong id="""">✅ Learn and improve. </strong>Use the findings from the recurring pen tests to build a more resilient and safer product. The security landscape changes quickly and the guidance for SOC 2 also changes year to year. Make sure to keep track of major developments and use the monitoring features in your compliance tools to keep track of your updated posture. </p><p id=""""><strong id="""">✅ Set the budget</strong>. As you head into the next budgeting cycle, make sure to set aside the funds compliance. Between the tools, the auditor fees, the pen tests and any remediations you will need to make, there will be a cost associated with getting “the stamp”. But make sure to spend your money wisely. Choose a reputable firm and get industry-tested tools. This is not the time to look for a bargain. </p><h4 id=""""><strong id="""">In conclusion, is SOC 2 Type 2 certification worth it? </strong></h4><p id="""">Short answer: Yes. </p><p id="""">Security and compliance is not why any of us build startups (unless you’re Drata or Vanta), but it’s the reason why we stay in business. When our customers trust us, we can keep on innovating. Taking the time and putting the effort into validating the safety, security, and integrity of our products gives our customer peace of mind and allows them to rely on the services we provide. After all, customers are running their application environments in Release and we take that responsibility seriously. </p>",false,,,,https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/657b1d54387e970e54164344_SOC2Type2.jpg,,david-giffin,5,Wed Jan 10 2024 00:00:00 GMT+0000 (Coordinated Universal Time),news,12-things-you-didnt-know-you-could-do-with-release-part-1; a-managers-guide-to-release-cycles
Looking Back: A Big 2023 for Release,looking-back-a-big-2023-for-release,62aa5a70cd5ba27d9d0d718a,6582059701e68522ba1a2d51,Tue Dec 19 2023 21:05:27 GMT+0000 (Coordinated Universal Time),Thu Apr 25 2024 20:26:34 GMT+0000 (Coordinated Universal Time),Thu May 02 2024 20:19:24 GMT+0000 (Coordinated Universal Time),A look back at what happened over the past year and our favorite highlights from 2023.,"<p id="""">When the holidays approach and a new year looms on the horizon, I always find myself looking back at what happened over the past 12 months. Reflecting on what the Release team achieved this year, especially in expanding ephemeral environment automation to more developers and use cases, is truly remarkable. Here are some of the highlights from 2023.</p><p></p>",false,,,"<p id="""">‍<strong id="""">What’s in a Name</strong></p><p id="""">Changing the company’s name <a href=""https://release.com/blog/meet-the-new-release"" id="""">from ReleaseHub to Release</a> was a significant milestone for us. This name more accurately reflects our vision and our mission from the inception to help people release their ideas to the world faster. It aligns well with the expanded use cases, capabilities, and technologies our development team has added to the product.</p><p id=""""><strong id="""">Bigger and Better Data for Every Developer</strong></p><p id="""">After the renaming, the next major release delivered improvements to Release Instant Datasets. Conversations with customers consistently underscored the importance of data to effective development and test processes and the complexity of managing massive data in the SDLC. Release Instant Datasets 2.0 made production-like data for developers even more powerful, with improvements to both the workflow and architecture of the tool. (CTO Erik Landerholm <a href=""https://release.com/blog/new-and-improved-instant-datasets"" id="""">goes into the details of these changes in this blog post</a>). Shortly after, we made <a href=""https://release.com/blog/introducing-standalone-instant-datasets-build-and-test-with-realistic-production-like-data-with-ease"" id="""">Instant Datasets available as a standalone product</a>, enabling everyone to take advantage of data automation whether or not they are using the entire Release platform.&nbsp;</p><p id=""""><strong id="""">Docker Desktop Extension</strong></p><p id="""">Instant Datasets became a crucial component of Release’s support across the software development life cycle. Another major piece of the SDLC puzzle was the launch of <a href=""https://release.com/product/docker-extension"" id="""">Release Share as a Docker Desktop extension</a>. Release Share provides fast simple container sharing to the millions of Docker developers directly from the Docker Desktop application. Shifting environment sharing left to the start of the development process means app delivery teams can create a self-serve version of their app for feedback and testing at every stage of the process.</p><p id=""""><strong id="""">More Integrations for More Choice</strong></p><p id="""">Docker is just one part of a developer’s toolbox we support at Release. Developers ask us all the time “Does Release work with my [insert favorite tool name]?” We’re proud to have a broad set of partnerships and integrations with the tools and platforms favored by cloud-native app teams. Notably, we’ve improved our <a href=""https://release.com/blog/the-value-of-data-obfuscation-for-instant-datasets-tonic-meets-release"" id="""">integration with Tonic.ai</a> for data automation and anonymization,&nbsp; developed <a href=""https://release.com/blog/gitlab-self-managed-now-available-on-release"" id="""">new integrations with GitLab</a> for more repo choices, and expanded our relationships with AWS and GCP. We’re excited about many significant new integrations coming in early 2024.</p><p id=""""><strong id="""">The Year of AI&nbsp;</strong></p><p id="""">And finally, no discussion of the last 12 months would be complete without talking about AI. ChatGPT and OpenAI are having a huge impact on the world, and the Release team <a href=""https://release.com/blog/training-chatgpt-with-custom-libraries-using-extensions"" id="""">has been creating innovative ways</a> to use it to help make Release more powerful, and more importantly, make AI and dataset training more accessible to developers and teams. <a href=""https://release.ai/"" id="""">We launched release.ai as our hub for all things AI for DevOps.</a> Today Release users can use interactive prompts with Release to “talk to your infrastructure” and get insights into their environments.&nbsp; We are also helping companies apply ephemeral environment automation to dataset training and production workloads to make dataset training and optimization more efficient and accessible to more companies.&nbsp;</p><p id="""">Thank you to our team, our partners, and most importantly our Release community for your collaboration, feedback, and suggestions throughout this year. It's been a wild year for all of us, with a lot of highs, lows, and challenges personally and professionally. It's gratifying and humbling to interact with this awesome community, whether on Slack, at industry events, or Zoom discussions. Here’s to a peaceful and prosperous 2024!</p><p></p>",https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/65ce450644c5e63338065bc3_year%20in%20review.png,2023 year in review for Release,matt-carter,3,Tue Dec 19 2023 21:05:00 GMT+0000 (Coordinated Universal Time),product,
Mat Werber: Why I joined Release,mat-werber-why-i-joined-release,62aa5a70cd5ba27d9d0d718a,62aa5a70cd5ba238850d730d,Thu Feb 24 2022 03:19:21 GMT+0000 (Coordinated Universal Time),Mon Apr 03 2023 22:14:08 GMT+0000 (Coordinated Universal Time),,I’m very excited to join Release to build their new Solutions Architecture (SA) team.,"<h4 id=""""><em id="""">Can you introduce yourself and tell us a little bit about your professional background and what you'll be doing at Release?</em></h4><p id="""">My name is Mat Werber and I’ve worn many hats over my eleven-year career, ranging from IT &amp; financial auditor, ERP implementation, business intelligence, and - most recently four years as a cloud solutions architect at AWS and Apple helping companies of virtually every size and industry innovate faster by building their applications in cloud environments.&nbsp;</p><p id="""">I’m very excited to join Release to build their new Solutions Architecture (SA) team. SAs serve as technical advisors, providing our customers like <a href=""https://releasehub.com/casestudy/datasaurai"" id="""">Datasaur.ai</a> and <a href=""https://releasehub.com/casestudy/monad"" id="""">Monad</a> with guidance that allows them to to bring their ideas to the world more easily, powered by the ephemeral and persistent environments-as-a-service offered by Release.&nbsp;</p><h4 id=""""><em id="""">How did your experience at previous roles impact your decision to join Release?</em>&nbsp;</h4><p id="""">It’s really quite amazing when you look at how modern cloud services have and continue to transform the developer experience. As engineers, we no longer have to have to wait months or years to build data centers or establish contracts with colocation facilities, rack and stack hardware, or worry about heating, cooling, power, networking, or physical security. With a few API calls, we can offload all of that work to providers like AWS, GCP, Azure and others.&nbsp;&nbsp;</p><p id="""">At the same time, demand for infrastructure has and continues to skyrocket, driven by greater access to broadband internet, advancements in hardware and software, an explosion of data from new IoT, new streaming music and video platforms, and more. To keep up with this demand, commercial and open source communities continue to create new cloud services, scale-out technologies like Kubernetes and Apache Spark, and design patterns and tooling like microservices, serverless, and CI/CD.</p><p id="""">Time and time again, I saw that my customers adopting these technologies were merely shifting complexity to another team or service within the company rather than eliminating it.&nbsp;</p><p id="""">For example, containers helped us solve“it worked on my machine, so why did it fail in staging?”, platforms like Kubernetes (K8s) solved “how do I run this at scale?”, and services like Amazon EKS solved “how do I easily <em id="""">create</em> and manage a K8s control plane?”. However, even with all of this in place, you still have a lot of work to do before that cluster is running your application in production.</p><p id="""">Now, I am <em id="""">not </em>picking on Kubernetes. It’s a great technology backed by an awesome open source community and used all across the globe. Rather, I only mean to illustrate the ever-growing complexity needed to build an environment. And, that’s just <em id="""">one part</em> of an environment… you also need to think about how you learn, build, and stitch together:&nbsp;&nbsp;</p><ul id=""""><li id="""">Cloud VPC, DNS, IAM, security, scaling, load balancing</li><li id="""">Infrastructure-as-code (Terraform, CloudFormation, Pulumi, etc.)</li><li id="""">Integration and delivery/deployment pipelines</li></ul><p id="""">As a cloud architect, I had a unique vantage point that allowed me to see hundreds of customers deal with these same questions.&nbsp;&nbsp;</p><p id="""">What really caught my attention was that even when my customers built a well-designed environment, much of their work inevitably became tech debt as they scaled. For example, at a certain point, they would outgrow their single dev/test/prod environments and realize that they’ll need to create <em id="""">multiple</em> copies of environments to prevent development from slowing to a crawl. Then, there’s the question of how to coordinate deployments when environments have dependencies with one another, rolling back environments due to drift, refreshing or resetting data in test / QA, added infrastructure cost of new environments, engineering-hours to operate all of this, and more.&nbsp;</p><p id=""""><em id="""">Every customer</em> I’ve met who develops software relies on environments, and whether boldly testing in prod (<em id="""">please don’t!), </em>managing a few environments or hundreds, they are <em id="""">all </em>encountering the same challenges and forced to reinvent and refactor the wheel on an ongoing basis. Sure, engineers are <em id="""">paid </em>to solve complex problems…but society benefits the most when those problems differentiate their business.</p><h4 id=""""><em id="""">What makes you most excited about the mission and upcoming journey at Release?</em></h4><p id="""">I was first drawn to Release for the same reason I became a cloud architect: I’m passionate about helping others make the world a better place, driven by bringing ideas to life through automation and cloud. Release not only shares this vision but <em id="""">delivers</em> on it by handling the heavy lifting of infrastructure, pipelines, and data needed to transform their customers’ commits into reality.&nbsp;</p><p id="""">My excitement only grew after seeing how comprehensive and widely applicable our platform was to the developer community. Just to name a few, we provide our customers with a multi-cloud solution that offers persistent and ephemeral environments, ephemeral databases with clean, near-production fidelity data, support for serverless, microservices and monoliths, custom cloud resources using customers’ infra-as-code framework of choice, and integration with partner services like LaunchDarkly.&nbsp;&nbsp;</p><p id="""">Most importantly, the company culture and every teammate embody our founding principles and are driving toward the same mission: Allowing customers to release their ideas faster. We’re growing rapidly. If you’re interested, see more information on Release <a href=""https://releasehub.com/company"" id="""">here</a>, or reach me directly regarding open roles.&nbsp;<br></p>",false,,,,https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/62174678536d918bba4c98d2_Release-welcomes-Mat-Werber.jpg,Release Welcomes Mat Werber,mat-werber,4,Tue Mar 01 2022 03:19:00 GMT+0000 (Coordinated Universal Time),,
Meet the new Release!,meet-the-new-release,62aa5a70cd5ba27d9d0d718a,63c1bec86eff1171d0fb8428,Fri Jan 13 2023 20:27:52 GMT+0000 (Coordinated Universal Time),Thu Apr 20 2023 20:59:42 GMT+0000 (Coordinated Universal Time),Thu Apr 20 2023 21:44:19 GMT+0000 (Coordinated Universal Time),"New year, new name. Check out the new Release!","<p id="""">It's a new year, and we’re making some changes to our brand and web property. Moving forward, we are now known as simply Release, and our domain name is release.com. This change doesn’t impact the functionality of our current environments-as-a-service offerings, but better reflects how we support the release tasks of app delivery teams across the lifecycle. </p><p id="""">This change may require you to make some minor changes to your configuration files moving forward. All existing releasehub.com pages will redirect to release.com equivalents, which might require reworking your CI/CD configuration files. APIs using ReleaseHub will continue to work after this change.</p><p id="""">Customers who have questions about the change are encouraged to post these to their private Release Slack channel so our team can address them. </p><p id="""">The product roadmap for the year ahead is exciting and is directly evolved from feedback from our users. We’re excited to make this change to our brand and website, and how it sets us up for the years ahead. </p><p id="""">I look forward to seeing the great things you release next!</p>",false,,,,https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63c1bebeca87e7ad127d4ef2_Logo%20-%20Light%20on%20Dark.jpg,,matt-carter,2,Mon Jan 16 2023 18:00:00 GMT+0000 (Coordinated Universal Time),,
ModernCTO Podcast Recap,moderncto-podcast-recap,62aa5a70cd5ba27d9d0d718a,62aa5a70cd5ba286080d7310,Thu Apr 14 2022 07:06:51 GMT+0000 (Coordinated Universal Time),Mon Apr 03 2023 22:15:03 GMT+0000 (Coordinated Universal Time),,"In this episode of the ModernCTO podcast, Joel Beasley talks to Tommy, co-founder and CEO of Release.","<p id="""">In this <a href=""https://moderncto.io/tommy-mcclung/"">episode of the ModernCTO podcast</a>, Joel Beasley talks to Tommy, co-founder and CEO of Release. They discuss how Tommy founded Release to solve the huge problem of quickly spinning up environments, why the best entrepreneurs are always extremely persistent, and why being vulnerable goes a long way as a leader.</p><p id=""""><strong id="""">Tommy</strong>: <em id="""">I know how it is to be an entrepreneur. I know the journey you've been through a bit.&nbsp;</em></p><p id=""""><em id="""">I guess we have some glutton for punishment or something. I don't know what it is. I just think about going to work for someone else, and I just can't make myself. I mean, I did. I've done it.</em></p><p id=""""><em id="""">I was the CTO at True Car before this. After a company I started got acquired, the whole time I was there, I could just not get it out of my head that I wanted to do it again. So, there's something about it. That's exciting and freeing. Scary, but I don't know. That's what makes me tick is just going out there and trying to do something crazy.</em></p><p id=""""><strong id="""">Joel</strong>: <em id="""">Absolutely. And I love what you guys are doing with Release because you know, my background, like I said, I was a software developer for 17 years. I understand environments and all of that. And when I saw what you guys were doing, I think it was like a month or two ago, I was like, legitimately, this is one of the coolest things I've seen in a long time.</em></p><p id=""""><em id="""">So I was hoping you could just explain what Release is, what it does.&nbsp;</em></p><p id=""""><strong id="""">Tommy</strong>: <em id="""">Yeah. So Release is environments as a service. Environments are needed for developers, for everything from developing their code, testing, and QA. Running it for their customers. Obviously your production environment is where the customer sees the end result of your work.</em></p><p id=""""><em id="""">In modern software delivery, there's all sorts of new delivery mechanisms where you might have a customer who comes to you and says, I don't want to have my data co-mingled with all of your other customers in a multi-tenant kind of environment.&nbsp;</em></p><p id=""""><em id="""">So, I would rather have a version of what you're selling me, isolated to myself, a single tenant version of that. Sometimes, our customers host those for their customers, like a direct single tenant environment. Sometimes they deliver those into their cloud accounts. So, they're kind of like an enterprise version of their applications. And so, environments are just kind of like this absolutely necessary fundamental part of the software ecosystem.</em></p><p id=""""><em id="""">Development, all the way through to getting your ideas in front of your customers. They're just really hard. They've always been really hard. Release makes it really easy to spin up and down environments on demand.&nbsp;</em></p><p id=""""><em id="""">So, you know, as the CTO of a public company for a, for a handful of years and got to see, you know, what does it take to run a large engineering organization and make them effective and efficient and environments were a really difficult problem to solve, especially when you get to a larger scale company where the applications get really complicated.</em></p><p id=""""><em id="""">It's not just a rails app and that's all you're running or a JS app. It's got data pipelines, it's got ETL that needs to happen. It's got, you know, lots of different applications, internal and external. Then, you have big teams of engineers trying to collaborate and they all tend to bottleneck around.</em></p><p id=""""><em id="""">Well, I got to use the staging environment or QA is being used by this team. And so I've got to wait. I really, really felt the pain in trying to get an engineering organization efficient and environments were kind of a headache there. So, me and my co-founders decided we were going to start a company to solve that problem because nothing really existed in the market.</em></p><p id="""">Listen to the full episode <a href=""https://moderncto.io/tommy-mcclung/"">here</a>.</p>",false,,,,https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/6257c788ddd14b64bf2c4f40_tmc.png,Release on ModernCTO Podcast,sam-allen,3,Thu Apr 14 2022 16:00:00 GMT+0000 (Coordinated Universal Time),,
New and Improved Instant Datasets 2.0,new-and-improved-instant-datasets,62aa5a70cd5ba27d9d0d718a,64adb94aed61980a7cc52b1f,Tue Jul 11 2023 20:19:22 GMT+0000 (Coordinated Universal Time),Thu Jul 25 2024 15:07:06 GMT+0000 (Coordinated Universal Time),Thu Jul 25 2024 16:27:39 GMT+0000 (Coordinated Universal Time),Our most popular feature gets an upgrade. Now it's even easier to build and test with production-like data.,"<p id="""">I don’t know about you, but for me the most frustrating thing about local development, remote development, basically any kind of development is getting access to production or production-like data. Getting data, modifying data, securing data, and removing data when finished, these are some of the hardest things about dealing with multiple environments. And almost any sufficiently complicated feature development can benefit from real data. Ephemeral environments are great for developing and testing complicated features, solving race conditions, etc, but none of that is possible without something that gives you instant access to production-like data. </p><p id="""">We created Instant Datasets to allow you to get access to production-like data for your ephemeral environments, instantly. &nbsp;If you are an existing Release user, you know that Instant Datasets capability has been part of Release from the very beginning and is one of the most popular features of the platform. As such, we are constantly thinking about ways to make it faster, better, and more useful to all developers. Our most recent set of improvements include an architectural overhaul, addition of a new key functionality – a native data obfuscation integration, and (soon) the ability to use Instant Datasets as a stand-alone product, launching July 25th. </p><h3 id="""">What’s New?</h3><p id="""">Under the hood Instant Datasets 1.0 consisted of a homegrown workflow engine, a few state machines and background job processing. This has served us well, but when we decided to improve our data cleansing options, we knew we needed to use a real workflow engine. </p><p id="""">Before, you needed to clean your production data during the deployment process. This greatly increased the time of deployment and was incumbent on our customer to manage this process. The new architecture allows us to simplify the process and add integrations our customers wanted. </p><p id="""">We had great experiences implementing <a href=""https://github.com/temporalio/temporal"" id="""">Temporal</a> for internal workflows and knew it would also serve us well in this application. Temporal gives us the scalability, durability, performance and extensibility we need to create our world class platform. At the moment, working with Temporal and Ruby/Rails is not a trivial exercise and we faced a number of interesting challenges making it work. But overall it was the right choice for this task. (Keep your eyes open for a deeper dive into how we use Temporal at Release!)</p><p id="""">Now, this new architecture gives us the tools to build a workflow that allows our customers to plug in ANY data obfuscation tool as a custom task. To make it even simpler for our users, we created a native integration with <a href=""http://tonic.ai/"" id="""">Tonic.ai</a> to quickly build an obfuscated dataset and easily use it with your environments. Here is how the Tonic integration works: </p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-center"" data-rt-type=""image"" data-rt-align=""center""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/64b03c329989ad9d70f4f040_3ko9AIWhqiFAhRkPJk-LJQSNGiP5oVQBNeO2S-AC6op6UTz5XRl6l_axgLUyM0zPoqevp32_GHfEj5QFDERUBLhOeo_4TxCxdXMVLCjKu69OXi3IUkjPb7qvYXPEyPRugAzVw4N0xApz5EuJWfUPRDA.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div><figcaption id=""""><em id="""">Tonic and Release instant dataset creation workflows.</em></figcaption></figure><p id="""">All configurations are done in one simple UI in Release. You select your RDS snapshot. Release creates source and destination databases to be processed in Tonic. You provide your Tonic api and workspace ids and Tonic does its magic. Once finished, you get a pool of obfuscated, cleaned, truncated and otherwise modified databases instantly available for all your environments!</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-center"" data-rt-type=""image"" data-rt-align=""center""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/64b03c32186615761dba1351_zVTfSWfChcPUxCyEZZmlIb_7RTGZb5VyTKbJVECqZR1uJsX_dTj9VPzvaLcxrGPa3xZr2r-Q6F0bPd7UiO9zEDJKCPFN2HtOkUso5gDLAdXl-KYWJsQezN_N8bGPaHpXRPWheGPj2PkmlEk3iQdEbCY.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div><figcaption id=""""><em id="""">Select the source snapshot and configure Tonic all in one simple UI </em></figcaption></figure><p id="""">To learn more about the Tonic integration, see the docs <a href=""https://docs.release.com/reference-documentation/instant-dataset-tasks/tonic-cloud"" id="""">here</a>. For more information on Instant Datasets in Release check out our documentation on AWS <a href=""https://docs.release.com/reference-documentation/instant-datasets-aws"" id="""">here</a> and GCP <a href=""https://docs.release.com/reference-documentation/instant-datasets-gcp"" id="""">here</a>. </p><p id=""""><em id="""">*At the time of this post, we have not finished migrating all of our dataset integrations from our Instant Datasets 1.0 architecture to 2.0. For now, the ability to cleanse your data is limited to AWS (RDS and Aurora), but we are actively working on our GCP support and it will be available shortly* &nbsp;</em> </p><h3 id="""">What’s Next?</h3><p id="""">With our new architecture in hand we have a lot of new functionality to add to Instant Datasets in the coming weeks:</p><ul id=""""><li id=""""><strong id="""">Add GCP support on our new architecture.</strong> &nbsp;At the moment we only support our Tonic integration and new architecture when using AWS databases, both RDS and Aurora are supported. We have support for <a href=""https://cloud.google.com/sql"" id="""">CloudSql</a> in our Instant Datasets 1.0 and will have that same functionality plus the Tonic integration very soon.</li><li id=""""><strong id="""">Bring your own obfuscation tool!</strong> Tonic is great, we use it, but if you have other tools or just scripts in a container you would like to run when creating your Instant Dataset we will have support for that in the next couple of weeks also.</li><li id=""""><strong id="""">More Integrations!</strong> Instant Datasets are useful beyond your traditional databases. &nbsp;Any set of data that can be cloned or used to create an example can be used by Release to create an Instant Dataset for your environments. We will be adding support for MongoDB Atlas, Neon Serverless, and more in the near future!</li><li id=""""><strong id="""">Stand-alone Instant Datasets</strong> soon available to all developers, regardless if they already use the Release platform or not. We believe that production-like data is the best kind of data to develop and test with, so we are making Instant Datasets available to everyone, at no cost. <a href=""https://www2.release.com/instant-datasets"" id="""">Sign up to be the first to know when stand-alone Instant Datasets launches on July 25th. </a></li></ul><h3 id="""">What’s in it for you?</h3><p id="""">Testing and developing with production-like data has always been our default at Release. It allows us to prevent rework, makes potential bugs apparent much sooner, and gives us a realistic preview of how our app will behave under actual loads. We want to share those benefits with the wider developer community and continue adding useful functionality to our platform. Take the new and improved Instant Datasets for a spin and <a href=""mailto:hello@release.com"" id="""">let us know</a> what you think (and what we should add next).</p>",false,,,,https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/64b040a25abe0ae6216d447f_ID%20_%20blog-2.jpg,,erik-landerholm,5,Thu Jul 13 2023 18:00:00 GMT+0000 (Coordinated Universal Time),product,12-things-you-didnt-know-you-could-do-with-release-part-1
On-Demand Environments Defined and Explained,on-demand-environments-defined-and-explained,62aa5a70cd5ba27d9d0d718a,63ce9531383079e2ef6ed58d,Mon Jan 23 2023 14:09:53 GMT+0000 (Coordinated Universal Time),Mon Jan 23 2023 14:11:01 GMT+0000 (Coordinated Universal Time),,"We cover on-demand environments, the best way to implement them, and the advantages and disadvantages of on-demand envir","<p id="""">There is no better feeling than having a seamless development process. However, teams that use a shared staging environment to test their features often complain about feature conflicts and code-breaking. In this post, you’ll learn about on-demand environments and how they can help improve your development process.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-center"" data-rt-type=""image"" data-rt-align=""center""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63ce94e24218173ab2e2fca1__rAYB0Pbzji6djiKjHwZ7nBE2P_kNNg4q3fTrrT8LlARVgg_dclb0GBu-N3qeKPLMH_gIOsksBLj1nUvgI_gy5xLznyQ7sRwMpUqyhyGhRwDsLibEz1TCwVHhMPelx4NvcjOlgB6HZSd-tbHBjYFBrEUUSV4QRkavPP6acMcgfCFYCbP6K9rLYMIrJgz.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><h2 id=""""><strong id="""">What is an On-Demand Environment?&nbsp;</strong></h2><p id="""">An on-demand environment is a temporary, isolated environment that developers set up to test features, build features, or fix bugs without disrupting other development processes. On-demand environments create a full-fledged replica of your project, which includes all microservices and dependencies that your app needs to function. You can create as many on-demand environments as you want and they can run simultaneously. In other words, every developer on your team working on specific features can have their own little playground to make and break their code without hindering other developers' work. And if they mess the replica up completely, they can throw it away and create another one.</p><p id="""">Using an on-demand environment automatically gives you an independent<a href=""https://releasehub.com/staging-environments"" id=""""> staging environment</a> for every single feature. This is great because your developers have a place they can verify that their work meets expectations before merging it into production. In contrast, using a shared staging environment to test features might lead to a bottleneck. The on-demand environment approach is a much better way to go about feature testing in your software development process.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-center"" data-rt-type=""image"" data-rt-align=""center""><div id=""""><img alt=""On-demand environment"" src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63ce94e26746b4cf2260f706_eTHWj0GA-5Tdl2KJvLaKYYgTB63wYrnaNWffstlJ-yOXNjlNRqDtZMEQkbtzXmpwkIcP0ptFsubQFjyFOBdqv9B6flko1xmFU15QkeFdCZm08eUDDDL5FdFsSyADW2hPXU-3keNxKVQCiF_L0E32UEZak-FzG3MCdZL4lJJl8I_1ej6bQpBJm5MmN-mg.jpeg"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><h2 id=""""><strong id="""">Implementing On-Demand Environments</strong></h2><p id="""">Typically, we create this environment manually, which includes configuring all your microservices (for example, dependencies, libraries, databases, and third-party software) using a tool like<a href=""https://www.terraform.io/"" id=""""> Terraform</a>, but this approach requires a fair amount of precise reconfiguration of your project setup.</p><ul id=""""><li id=""""><strong id="""">Each Git branch should be prefixed</strong> with the name of the task or feature associated with it. This makes it clear and easy to identify what each feature branch represents. For instance, for <strong id="""">fea-productName-branch-URL, Fea </strong>= feature name, <strong id="""">productName</strong> = product name, and <strong id="""">branch-URL</strong> = Git branch URL.</li><li id=""""><strong id="""">Access and parameterize</strong> all your configurations that might differ between developers. For instance, you can package your configurations in a dataset and store them on the<a href=""https://aws.amazon.com/systems-manager/"" id=""""> AWS Systems Manager,</a> then use Terraform to communicate with them whenever you need them to spin up an environment.</li><li id=""""><strong id="""">Define rules and restrictions</strong> for your environments. You can use Terraform with<a href=""https://learn.microsoft.com/en-us/azure/governance/policy/overview"" id=""""> Azure Policy</a> to create policies for what sort of action can be carried out in each environment. For example, for security, you can restrict your developers from communicating with specific databases or even restrict them from using some services in specific locations.</li><li id=""""><strong id="""">Consider cost. </strong>You may want to create a configuration that automatically deletes an environment after merging and deleting pull requests. Also, you can configure it to stop every instance of services running on environments that do not require them.</li></ul><h2 id=""""><strong id="""">Buy an Environment Instead</strong></h2><p id="""">These days, many organizations use remote automated tools for creating on-demand environments. This lets them enjoy all the privileges of a full-fledged environment without bothering themselves or their DevOps team with the task of creating one manually. These automated tools allow you and your team to concentrate on more important tasks, like developing an application with all the features it needs, but without headaches.</p><p id="""">Automation tools can create an on-demand environment out of the box for you, which would include the full-fledged application, its frontend and backend, and every service or dependency it may require to fully function. These tools make creating an environment on demand quite easy and accessible. It takes care of most of the heavy lifting for you, like managing configurations and dependencies and destroying the environment when necessary. In contrast, if you use the manual process of creating these environments, you have to handle all these manipulations by yourself.</p><h2 id=""""><strong id="""">Advantages of On-Demand Environments</strong></h2><h3 id=""""><strong id="""">Faster App Release Time</strong></h3><p id="""">The traditional way to test every feature in development involved a shared staging environment. This is not idle, because a staging environment merges all the features in the app for testing and that can result in feature conflict. This delays production because a fix needs to happen before you carry on. On-demand environments come in handy because your team gets an isolated environment where they can work on every feature in your app independently, without worrying about it affecting other development work. This ultimately lets you avoid bottlenecks in your development process, giving your app a faster time to market. And this gives you an edge over competitors that don’t use on-demand environments.</p><h3 id=""""><strong id="""">Dedicated Testing for Each Feature</strong></h3><p id="""">A good software development company seizes any chance they get to test that their product meets exceptions. With an on-demand environment, you get another chance to test your product and you get to test every feature independently. This gives you and your team more time to figure out what works and what doesn’t.</p><h3 id=""""><strong id="""">Product Quality</strong></h3><p id="""">Imagine a team using a shared staging environment for testing features in their app. Because of all the problems they might face with testing in a shared staging environment, they might run out of patience and release products that, at the end of the day, don’t really impress. But with on-demand environments, every developer working on a feature gets an isolated environment to build and test their feature before shipping to a staging or production environment. This can immensely improve the quality of your product. Note that you don't have to have a staging environment when you use on-demand environments, but a staging environment lets you test all the features that you worked on individually in the on-demand environment as a unit.</p><h3 id=""""><strong id="""">Time Saving</strong></h3><p id="""">It takes a lot of time to resolve feature conflicts in a staging environment because you have to figure out which code change or feature broke the code. This wastes time unnecessarily. Time is precious and you need to complete other tasks. On-demand environments let other work continue, so you don’t waste time while you fix broken code. With an isolated development area, the development of your main app continues, unaffected.</p><h3 id=""""><strong id="""">Happy Team</strong></h3><p id="""">Avoiding work stoppages because of feature conflicts makes teams happy and we can all agree that when your team is happy, you're bound to have quality work.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-center"" data-rt-type=""image"" data-rt-align=""center""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63ce94e23830797be56ecc14_N6VFbGzw54-spyxaW-30-MGvtS92fX0RU6cjQ70mbLODoVe_bgnRBD26ni6yd_AXPodE1NGEw5ySMupxDZFXa2sl9BCajeQ8njutrK6tndhQARBnJ3GxXt8QHGxof_RnRUwp9pyoRBJlDwEtClRzKsXiMWHwNfdCmYAXKcj3oKjH_4kNO3CgykNJR1wS.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><h2 id=""""><strong id="""">Disadvantages of On-Demand Environments</strong></h2><p id="""">One of the major concerns with on-demand environments is cost. Because all your micro-services actively work in more than one instance of your application, bills start to pile up. You can mitigate this by using an automation tool that manages the lifespan of all your environments, from when you activate them to when you disable them. These tools can automatically destroy any environment you no longer need, saving you money. You can also define policies for the maximum number of environments that you or your team can create.</p><h2 id=""""><strong id="""">Conclusion&nbsp;</strong></h2><p id="""">We covered on-demand environments, the best way to implement them, and the advantages and disadvantages of on-demand environments. Now the only thing left is putting the icing on the cake.</p><p id="""">If you read this post in full, you now know some of the tremendous benefits on-demand environments can bring to your development process.<a href=""https://releasehub.com/ebook/the-complete-guide-to-automated-software-environments"" id=""""> Release</a> is one tool you can use to spin up environments on-demand without dealing with lengthy set ups or convoluted processes. It doesn't matter if you have a complex app with<a href=""https://docs.docker.com/compose/"" id=""""> Docker Compose</a> or you just want to deploy a simple<a href=""https://docs.releasehub.com/reference-documentation/static-service-deployment"" id=""""> static app</a> with package.json, Release can generate a template using these files to build your environment on-demand. If any of this intrigues you and you are yearning for an effortless means of creating environments for your projects, don't hesitate to learn about the magic of<a href=""https://releasehub.com/ebook/the-complete-guide-to-automated-software-environments"" id=""""> Release</a>.</p>",false,,,,https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63ce9568b44e887c87b9e207_env-on-demand.jpeg,,,6,,,
On-Demand Testing Spaces Turbocharge Developer Velocity,on-demand-testing-spaces-turbocharge-developer-velocity,62aa5a70cd5ba27d9d0d718a,65b0372aebeacdff241514f3,Tue Jan 23 2024 22:01:14 GMT+0000 (Coordinated Universal Time),Thu Jul 25 2024 15:01:19 GMT+0000 (Coordinated Universal Time),Thu Jul 25 2024 16:27:39 GMT+0000 (Coordinated Universal Time),Embracing ephemerality in software development with on-demand ephemeral environments.,"<p id="""">We live in an ephemeral world. From social media stories that vanish within 24 hours, to app messages that self-destruct after viewing, to our speed-of-light news cycle where today’s headlines are forgotten tomorrow. For better or for worse, we as a culture are just not that into permanence.</p><p id="""">One area where ephemerality has been making positive inroads is in the world of software development, where it’s solving a particularly intractable problem. I’m referring to a relatively new innovation known as ephemeral environments — virtual spaces that can be created and destroyed on demand to streamline software development and remove the staging or QA environment bottleneck.</p><h2 id=""""><strong id="""">The Bottleneck Problem</strong></h2><p id="""">Delivering software is hard. With applications <a href=""https://thenewstack.io/the-growing-complexity-of-kubernetes-and-whats-being-done-to-fix-it/"" id="""">growing in complexity</a>, getting ideas from one’s fingertips to users is fraught with challenges.</p><p id="""">Consider the gauntlet that a developer’s code must run before it can be released to the world. It must pass through development, staging or QA, and production environments, each of which is critical.</p><p id="""">Staging is the environment where code is “staged” — that is, tested and merged — before being run in front of users so dev teams can be sure it works as designed. The staging environment should mirror production as closely as possible, but without exposing the code to clients, customers and the public.</p><p id="""">At many tech organizations, the staging environment is the source of the biggest and most costly bottleneck in software development.</p><p id="""">There are several reasons for this, including:</p><ul id=""""><li id=""""><strong id="""">Complexity:</strong> These preproduction environments are actually highly complex platforms comprising dozens or even hundreds of services, cloud platforms and supporting technologies. Standing up a new environment is a highly specialized task that requires the <a href=""https://thenewstack.io/how-to-close-the-devops-skills-gap/"" id="""">skills of DevOps</a> engineers.</li><li id=""""><strong id="""">Cost: </strong>In addition to the cost of engaging DevOps talent, for which there is great demand in the industry, staging environments can be expensive to maintain, and they often break. Any shared resource can be a pain point for companies large and small that are trying to rapidly ship new features.</li><li id=""""><strong id="""">Scarcity:</strong> Most organizations have one or very few of these environments, and only one engineer at a time can deploy to an environment. There are <a href=""https://en.wikipedia.org/wiki/Software_engineering_demographics"" id="""">over 20 million developers</a> in the world, and pretty much all of them use these environments.</li></ul><p id="""">The result is that developer teams often waste precious time twiddling their thumbs as they wait for testing environments to become available, or argue over priorities and who can use the environment. Teams can sometimes wait days or weeks to <a href=""https://thenewstack.io/gitops-at-home-automate-code-deploys-with-kubernetes-and-flux/"" id="""">deploy their code</a> due to constraints on single environments. This combination of long queues, idle engineers and delayed releases can cost organizations tens of millions of dollars per year. We estimate managing and maintaining environments costs organizations more than $45 billion a year.</p><h2 id=""""><strong id="""">On-Demand Ephemeral Staging Environments</strong></h2><p id="""">With ephemeral environments, the traditional idea of “staging” is gone — and with it the need for a single testing and integration environment where all code must merge before going to production. Staging happens on demand and at a click, allowing developers to create a limitless supply of environments for any purpose.</p><p id="""">Ephemeral environments offer a production-like replica that allows developers to properly test their code (shift left) and isolate bugs to a single branch, while ensuring a smooth merge to staging and production. They can be created automatically with every code pull request, and URLs to the environment can be shared with stakeholders so they can see progress while code is being developed.</p><h2 id=""""><strong id="""">3 Ways to Get the Most from Ephemeral Environments</strong></h2><p id="""">Just as physical pop-up stores offer a range of benefits to retailers, restaurateurs and others — cost-effectiveness and easy experimentation, for example — developer pop-ups offer several advantages over traditional staging. Here are three.</p><ul><li><strong id="""">Early and frequent feedback: </strong>With developer pop-ups, the environment is updated each time a developer pushes code to their source control system, providing a live reflection of the feature during development. Stakeholders such as product managers, designers and QA are automatically notified when changes are live, and they can preview those changes and give feedback immediately, enabling better code quality in development and minimizing rework.</li><li><strong id="""">Get feedback on your branch:</strong> Ephemeral environments can be created for any code branch, on demand, and loaded with your data. Developers can spin up as many isolated environments as they need, get feedback on a specific branch and work with the certainty that, if there is an issue, it’s not in their environment.</li><li><strong id="""">Greater developer velocity:</strong> In making bottlenecks a thing of the past, ephemeral environments allow for more frequent release cycles, better customer experiences and — the holy grail of software development — better developer velocity.</li></ul><p id=""""><a href=""https://www.mckinsey.com/industries/technology-media-and-telecommunications/our-insights/developer-velocity-how-software-excellence-fuels-business-performance"">According to McKinsey</a>, developer velocity is a key predictor of performance. Companies in the top 25% of McKinsey’s Developer Velocity Index outperform others in the market by four to five times, are more innovative, and score higher on customer satisfaction and other measures.</p><p id="""">Ephemeral environments turbocharge development velocity by eliminating bottlenecks in the process and helping companies produce consistent, reliable and plentiful environments on demand so they can get their best ideas to the world quickly.</p>",false,,,,https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/65ce456cdaf601505655f8ea_on%20demand%20testing.png,New Stack Release Article,tommy-mcclung,4,Tue Jan 23 2024 21:55:00 GMT+0000 (Coordinated Universal Time),platform-engineering,
Overcome the DevOps Talent Gap with Environments as a Service,overcome-devops-talent-gap-with-environments-as-a-service,62aa5a70cd5ba27d9d0d718a,62aa5a70cd5ba2d54b0d72df,Tue Mar 30 2021 23:52:54 GMT+0000 (Coordinated Universal Time),Fri Apr 21 2023 19:47:12 GMT+0000 (Coordinated Universal Time),Fri Apr 21 2023 20:04:43 GMT+0000 (Coordinated Universal Time),Is your company affected by the DevOps talent scarcity and how do you catch up to your competitors?,"<h3 id="""">Overcome the DevOps Talent Gap with Environments as a Service</h3><p id="""">DevOps is becoming the most critical function in software development but there is a shortage of talent to support needs across technology driven organizations. Applications have evolved and the shift in architecture is now to be more closely intertwined with development than ever.&nbsp;</p><p id="""">Developers have new requirements that are invented by the evolution toward containerized applications and microservice-based architectures. Instead of just focusing on their code, they now need to be more mindful of how their code relates to the infrastructure that supports their applications.&nbsp;</p><p id="""">The interconnected web of dependencies that live under the hood are difficult to unravel as new technologies, processes, and systems have been introduced and then retired over the years. When large companies have adopted a plan to move toward modern application architectures, they are subject to the constraints of their existing ecosystems. Startups are unique and often can start developing their applications’ architecture from a blank slate.</p><h3 id="""">The Problem</h3><p id="""">The irony of containerized applications and microservice architectures is that the same problems arise around DevOps for both startups and large enterprises. The problems exist around dependency management, server maintenance, and support for consistency across environments.&nbsp;</p><p id="""">Changes in computing, technology, and processes are going to continuously evolve and adapt at a speed that is faster than most teams’ ability to keep up. We can only do our best to increase time to value and accelerate productivity to make sure we remain inside the window of survival.</p><p id="""">A lack of sufficient DevOps talent should not eliminate the rollout of a strategy to future proof your business. Complementing your existing DevOps team with technologies that add the services and automation in areas that they don’t have bandwidth to reach is how teams can accomplish more with less.&nbsp;</p><p id="""">Technology companies need to support DevOps, automate their workflows, or stand in their place when there is a void of not having a team in place. The reality is that every company is a technology company in the 21st century, and everyone is going to deal with challenges around bandwidth issues in their DevOps organizations. If the talent pool is so small in the industry, there is a high chance that your pool of hires will also be too small for the needs of your business over time.</p><p id="""">DevOps hires are so important because not only are they limited but they are the people that determine what the organization should or should not do to streamline development. We are too dependent on people in a field that should allocate investment where it is measurable and within our control.</p><h3 id="""">The Solution</h3><p id="""">Environments are where the visions and strategies developed by DevOps come to life.&nbsp;</p><p id="""">Preview environments that have all of the tooling, technology relationships, data, APIs, and all other criteria vital to application performance result in faster development lifecycles. The value of environments that replicate production in pre-production phases of development is obvious but standing up those environments is a strain on time and resources. In many cases, creating these important environments is so time consuming and costly these important steps are skipped, much to the detriment of the entire business.</p><p id="""">Release complements DevOps teams or can replace the DevOps overhead of smaller organizations through automated execution of workflows. We make it easy to run your engineering organization within an advanced DevOps framework without the additional headcount or reduction of resources. We accomplish this by offering a platform for Environments as a Service (EaaS) where we configure your applications to enable automated creation of environments with each code commit that live in isolation to provide visibility at every change. In this way, organizations can hire DevOps talent without additional headcount, and even gain productivity at mass quantities.</p><p id="""">On-Demand environments take the workload of provisioning and maintaining servers off of your engineering team so that they can focus on code instead of spinning up servers. Release offers&nbsp; a hands off process to run your microservices and apps on Kubernetes (k8s) to spin up environments within a matter of minutes. DevOps can prioritize more important tasks while engineers can bring software to production with velocity when reproducible environments are accessible to all.</p>",false,,,,https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63e3ffc6b6ed0b7d5f71a3c4_033121%20(1).jpg,Illustration showing the developer environment created by Release,amanda,4,Thu Apr 01 2021 00:00:00 GMT+0000 (Coordinated Universal Time),,
Platform Engineering: What’s Hype and What’s Not?,platform-engineering-whats-hype-and-whats-not,62aa5a70cd5ba27d9d0d718a,65b96bb09de93e382ae5ba9b,Tue Jan 30 2024 21:35:44 GMT+0000 (Coordinated Universal Time),Thu Jul 25 2024 15:00:38 GMT+0000 (Coordinated Universal Time),Thu Jul 25 2024 16:27:39 GMT+0000 (Coordinated Universal Time),Platform engineering is all the rage these days. Separate the facts from the hype with this roundup.,"<p id="""">There’s a sea change happening in the software development world. The merging of developers and operations that gave us DevOps may be entering a new chapter. In this new world, the emerging discipline of platform engineering is quickly gaining popularity.</p><p id="""">Platform engineering is the practice of designing and building self-service capabilities to reduce cognitive load for developers and to facilitate fast-flow software delivery. Many large organizations have struggled to reap the benefits of DevOps, in part because shifting more operational and security concerns “left” and into the domain of software developers has created bottlenecks for dev teams. At the same time, faced with a growing cognitive burden of repetitive, time-consuming tasks that kick them out of the flow state of highly productive coding, many devs want less and less to do with ops and the “you build it, you run it” paradigm.</p><p id="""">Platform engineering is emerging as the solution to many of these challenges. But in all the buzz around it, it’s hard to know what’s real and what’s not. To help you separate the facts from the hype, here’s a round-up of viewpoints on what platform engineering is and is not.</p><h3 id="""">Platform Engineering Is New – Hype</h3><p id="""">There are those who hail platform engineering as <a href=""https://www.youtube.com/watch?v=wXyNHngEN-s"" id="""">the new kid on the block</a>. But there’s nothing new about the building of digital platforms as a means of delivering software at scale. It even pre-dates the birth of the DevOps movement in the mid-2000s. According to <a href=""https://support.puppet.com/hc/en-us/articles/221368047-The-2023-State-of-DevOps-Report-is-here-"" id="""">Puppet’s <em id="""">2023 State of DevOps Report</em></a>, large software companies have been taking a platform approach for decades as a way to enable developer teams to build, ship, and operate applications more quickly and at higher quality.</p><p id="""">What is new, especially in the enterprise space, is the rapidly growing traction of platform engineering as a way for larger companies to improve software delivery at scale. Gartner identified platform engineering (which it interchangeably calls “platform ops”) as one of the <a href=""https://www.gartner.com/en/information-technology/insights/top-technology-trends"" id="""">Top Strategic Technology Trends of 2023</a>. Gartner analysts predict that 80% of software engineering organizations will establish platform teams by 2026, and 75% of those will include developer self-service portals.</p><h3 id="""">Platform Engineering Has Toppled DevOps – Hype</h3><p id="""">Those who claim that DevOps is dead and that platform engineering has supplanted it are engaging in hyperbole. “DevOps is dead, long live Platform Engineering!” <a href=""https://twitter.com/sidpalas/status/1551936840453820417"" id="""">tweeted</a> software engineer and DevOps commentator Sid Palas in 2022. “1. Developers don’t like dealing with infra, 2. Companies need control of their infra as they grow. Platform engineering enables these two facts to coexist.”</p><h3 id="""">Platform Engineering Is the Next Evolution of DevOps and SRE – Fact</h3><p id="""">Rather than dealing a death blow to DevOps, a more accurate take is that platform engineering is the next evolution of DevOps and SRE (site reliability engineering). In particular, it benefits developers struggling with code production bottlenecks as they wait on internal approvals or fulfillment. It also helps devs deliver on their own timeline rather than that of their IT team. And it helps operator types (such as SREs or DevOps engineers) who are feeling the pain of repetitive request fulfillment and operational firefighting — busy work that keeps them from building their vision for the future.</p><h3 id="""">Platform Engineering Should Embrace DevOps Culture – Fact</h3><p id="""">The agile development practices that are at the core of DevOps culture — such as collaboration, communication, and continuous improvement — have not extended to the operations domain. This has hobbled the ability of agile development teams to quickly deliver products. In order not to perpetuate this dynamic, DevOps team culture should evolve to support platform engineering, and platform teams should embrace DevOps team culture.</p><h3 id="""">Platform Engineering Is a Con – Hype</h3><p id="""">There are those, like independent technology consultant Sam Newman, who argue <a href=""https://samnewman.io/blog/2023/02/08/dont-call-it-a-platform/"" id="""">platform engineering is just another vendor-generated label to be slapped on to old practices</a> in a bid to mask the horrendously complex technology ecosystems we’ve accumulated. Newman’s concern is that “single-issue” platform teams risk becoming the very bottleneck they’re supposed to alleviate, and can become so focused on managing the tool that they forget about outcomes. Rather than using names like Platform Team, Newman suggests more outcome-oriented labels such as “Delivery Support” or even better “Delivery Enablement.”</p><h3 id="""">Platform Engineering Is All about Scaling – Fact</h3><p id="""">Platform engineering solves the challenges of scaling and accelerating DevOps adoption by dedicating a team to the delivery of a shared self-service platform for app developers. It works best for enterprises with more mature DevOps practices who need to scale and move fast. It often does not make sense in smaller companies, for a single development team, or when multiple divergent platforms need to be supported, where scale is not a driving factor of success yet.</p><h3 id="""">Platform Engineering Centers on IDPs and Golden Paths – Fact</h3><p id="""">One definition of platform engineering is the practice of creating a reusable set of standardized tools, components, and automated processes, often referred to as an <a href=""https://internaldeveloperplatform.org/what-is-an-internal-developer-platform/"" id="""">internal developer platform</a> (IDP). IDPs, and the teams that build them, provide paths of least resistance that developers can take to complete their day-to-day tasks. These “golden paths” come with recommended tools and best security practices built in, enabling developers to self-serve and self-manage their code.</p><h3 id="""">Platform Engineering Requires a Platform as Product Approach – Fact</h3><p id="""">Gartner and others recommend treating your platform as a product by treating the developers who use it as your customers, so that they in turn can deliver services to your organization’s customers. Like any other product, pushing your platform on to developers without their input is unlikely to produce positive outcomes. So it’s essential to talk to your internal consumers to solve for their needs. Many traditional infrastructure teams don’t do this and often don’t even understand the workloads running on their platforms.</p><h2 id="""">Striking a Balance</h2><p id="""">Successful IDPs achieve a balance between allowing developers to remain in the flow state of highly productive coding while eliminating repetitive tasks through automated, full-stack environments. Developers can deliver apps faster because platform engineers smooth the path for them, enabling them to create their own environment with every check-in. This allows devs to review, share, and test apps without waiting in line or worrying about code conflicts. When done well, platform engineering delivers best-in-class developer experiences; provides choices of leading tools, platforms and clouds across the software development lifecycle; and gives self-service access to full-stack environments to every developer.</p>",false,,,,https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/65ce4541652bf0938081a369_hype.png,Platform Engineering: What’s Hype and What’s Not?,tommy-mcclung,5,Tue Jan 30 2024 21:35:00 GMT+0000 (Coordinated Universal Time),platform-engineering,
Podcast Guest on Screaming in the Cloud,podcast-guest-on-screaming-in-the-cloud,62aa5a70cd5ba27d9d0d718a,62aa5a70cd5ba271c80d7300,Tue Jan 26 2021 22:22:08 GMT+0000 (Coordinated Universal Time),Wed Feb 08 2023 19:43:10 GMT+0000 (Coordinated Universal Time),Wed Feb 08 2023 22:48:51 GMT+0000 (Coordinated Universal Time),"I had the pleasure of reconnecting with Corey Quinn on his excellent podcast, “Screaming in the Cloud”. The far-ranging ","<p>I had the pleasure of reconnecting with Corey Quinn on his excellent podcast, <a href=""https://www.lastweekinaws.com/podcast/"" target=""_blank"">“Screaming in the Cloud”</a>. The far-ranging topics wandered working together at a company, to discussing how far internet technology has developed, to changing jobs in the middle of a pandemic, to what we do at Release. You can view the full episode and transcript <a href=""https://www.lastweekinaws.com/podcast/screaming-in-the-cloud/reconnecting-with-an-old-boss-with-regis-wilson/"" target=""_blank"">here</a>.</p><p>You can listen to the podcast below.</p><div data-rt-embed-type='true'><iframe width=""100%"" height=""180"" frameborder=""no"" scrolling=""no"" seamless src=""https://share.transistor.fm/e/751eba02""></iframe></div>",false,,,,https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63e3fb4bf57d30c1ba485666_012721%20(1).jpg,Release on Corey Quinn podcast “Screaming in the Cloud”,regis-wilson,1,Thu Jan 28 2021 00:00:00 GMT+0000 (Coordinated Universal Time),,
Rails and Kubernetes: A Guide to Using Them Together,rails-and-kubernetes-a-guide-to-using-them-together,62aa5a70cd5ba27d9d0d718a,6332086da782dc2b99af56c6,Mon Sep 26 2022 20:15:41 GMT+0000 (Coordinated Universal Time),Fri Apr 21 2023 17:27:21 GMT+0000 (Coordinated Universal Time),Fri Apr 21 2023 18:22:03 GMT+0000 (Coordinated Universal Time),"Is Kubernetes only for modern, cloud-native, microservices-based applications or can it be used with Rails monoliths?
","<p id="""">You may have heard that Kubernetes is for modern cloud-native, microservices-based applications. Does that mean you can't run it with a monolith Rails application?&nbsp;</p><p id="""">Not quite. Technically, nothing will stop you from just deploying your Rails application on Kubernetes. It'll work just fine. But there are a few things to consider to decide if you actually should.&nbsp;</p><p id="""">Let's take a look.&nbsp;</p><h3 id="""">Rails on Kubernetes—Why?</h3><p id="""">Let's start with the most important question: why would you want to run your Rails application at all?&nbsp;</p><p id="""">As we already mentioned, Rails applications are monolithic, which means most of the application logic is combined into one piece. And Kubernetes was designed to run distributed, a <a href=""https://en.wikipedia.org/wiki/Microservices"" target=""_blank"" id="""">microservices-based</a> application where one application consists of multiple smaller pieces. So, it sounds like it's not a good fit for Rails, right?&nbsp;</p><p id="""">Well, at first, indeed, it does. But let's dive a bit deeper into it.&nbsp;</p><p id="""">What's your alternative? Running a Rails application on a normal server, right?&nbsp;</p><p id="""">And do you have only one server for your Rails application? If it's a very small application or you just started with Rails, then possibly.&nbsp;</p><p id="""">But in most production situations, you'll actually have a few servers working together for one Rails application. This includes one server for the Rails itself, another server handling access to your application—so, for example, NGINX and Puma—a third server for Redis, a fourth server for a database, and a fifth server for background jobs like Delayed Job or Sidekiq.&nbsp;</p><p id="""">On top of that, you'll probably have some load balancer in front of all that and maybe yet another separate server or a dedicated solution for storage.&nbsp;</p><p id="""">And, suddenly, you realize that your monolithic Rails application actually requires a dozen servers and not just one.&nbsp;<strong id="""">‍</strong></p><h4 id="""">The Problem</h4><p id="""">The problem with that approach is that it's not very flexible, and you need to make a lot of compromises. For example, some of your servers may be heavily underutilized most of the time, but you can't scale them down because, from time to time, there is a spike in traffic. And the bigger the application and the more servers you have, the more resources that are probably wasted—or, to put it nicer, not used efficiently.&nbsp;</p><p id="""">Another thing is that it'll take a lot of time and effort to get all these servers maintained—periodic system upgrades, making sure they all have enough free disk space left, etc.&nbsp;</p><p id="""">Adding a new server when needed will probably take a while, too. You'll need to deploy it first, then configure the operating system, then install what's needed for Rails, and finally make sure it connects properly with the rest.&nbsp;</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1000px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1000px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63320348765de64e1d527af2_bPwI1ZxshyYvMkGgonXKrkpU0EGVLUP2NbzNQZDf-bn0K_Ws3k2DP7V8IgzEHgSaviZimJQd7sZ7uyHxBQLn4ae6tMyv7EXuVcIOJD90JHA8L_R0P356Y6MJBcYLCw0ZmJStOLetDfg9us3u-jQx2vediBHOdECzWWNsLBSOqbYkRnOP3YlMQf3fgQ.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><h4 id="""">Enter the Kubernetes World</h4><p id="""">The problems stated in the previous section bring us to Kubernetes. You could simply put all your Rails application pieces to Kubernetes and not worry about individual servers anymore. Kubernetes abstracts infrastructure from you, so you won't need to worry about underlying operating systems, security patching, etc.&nbsp;</p><p id="""">Sure, your Rails application will still be a monolith, so you won't get all the benefits of Kubernetes, but you'll get some. All the extras that your Rails application may need will still be deployed on Kubernetes as separate pieces, so you'll still benefit from easier scaling and node management.</p><p id="""">And, last but not least, once your Rails application is on Kubernetes, you'll have an easier path to start decoupling your monolith in microservices if you wish to do so.&nbsp;</p><h4 id=""""><strong id="""">How to Use Rails on Kubernetes</strong>‍</h4><p id=""""><sup id="""">‍</sup>Now that we know some theory, let's see in practice how to run Rails application on Kubernetes. </p><p id="""">In fact, there's nothing special about running Rails on Kubernetes in comparison with other frameworks. You'll need to package your Rails application into a container and create a YAML manifest file in order to deploy it on Kubernetes.&nbsp;</p><p id="""">Let's start with the container. There's no single specific way of packaging Ruby on Rails applications into a container—a lot will depend on how your application is working and how many best practices you want to implement, but here's a good starting point.&nbsp;</p><h4 id="""">Docker Container</h4><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yaml"">
FROM ruby:3.1

# Create directory for our Rails application and set it as working directory
RUN mkdir /app
WORKDIR /app

# Copy the Gemfile
COPY Gemfile Gemfile.lock ./

# Install nodejs and yarl (if you need anything else you can add it here)
RUN apt-get update && apt-get install -y nodejs yarn

# Install bundler and run bundle install
RUN gem install bundler
RUN bundle install

# Copy the Rails application code
COPY . .

# Precompile the Rails assets.
RUN rake assets:precompile

# Expose your Rails app
EXPOSE 3000

# Run Rails server
CMD [""rails"", ""server"", ""-b"", ""0.0.0.0""]
</code>
</pre></div><p id="""">Save the above as Dockerfile (with no extension) in your Rails project root directory, and then build your image:&nbsp;</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yaml"">
$ docker build -t rails_on_k8s:0.1 .

Sending build context to Docker daemon  13.58MB
Step 1/11 : FROM ruby:3.1
3.1: Pulling from library/ruby
1671565cc8df: Pull complete 
(...)
2e02738a3297: Pull complete 
Digest: sha256:74f02cae856057841964d471f0a54a5957dec7079cfe18076c132ce5c6b6ea37
Status: Downloaded newer image for ruby:3.1
 ---> e739755aa18e
Step 2/11 : RUN apt-get update && apt-get install -y nodejs yarn postgresql-client
 ---> Running in 5095ff174667
Get:1 http://deb.debian.org/debian bullseye InRelease [116 kB]
(...)
Step 11/11 : CMD [""rails"", ""server"", ""-b"", ""0.0.0.0""]
 ---> Running in ac82bad67bb6
Removing intermediate container ac82bad67bb6
 ---> 731093adf23f
Successfully built 731093adf23f
Successfully tagged rails_on_k8s:0.1
</code>
</pre></div><p id="""">OK, that's pretty much it. That's how you package a Rails application into a Docker container.&nbsp;</p><p id="""">Now what?&nbsp;</p><p id="""">We could start writing out Kubernetes manifest, but it's best to first check if our application is working in the container properly. To do that, run that freshly built image by executing <strong id="""">docker run -p 3000:3000 rails_on_k8s:0.1</strong>, and then open your web browser and head to <strong id="""">localhost:3000.</strong>&nbsp;</p><p id="""">If you see your application, everything is good, and we can proceed.&nbsp;</p><h4 id="""">Kubernetes Manifest</h4><p id="""">OK, we're ready to write a Kubernetes deployment manifest for our Rails application.&nbsp;</p><p id="""">It'll be a fairly simple deployment at first. The only thing you need is to upload your Docker image to some container registry. If you don't have your own private repository, you can use a free account on dockerhub.com. You'll need to register there and follow the instructions on tagging your image and pushing it to the registry.&nbsp;</p><p id="""">Once you have your image ready in a container registry, you can create a Kubernetes manifest:&nbsp;</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yaml"">
apiVersion: apps/v1
kind: Deployment
metadata:
  name: rails-on-k8s
spec:
  replicas: 1
  selector:
    matchLabels:
      app: rails-k8s
  template:
    metadata:
      labels:
        app: rails-k8s
    spec:
      containers:
      - name: rails-k8s
        image: davezworka/rails_on_k8s:0.1
        ports:
        - containerPort: 3000
</code>
</pre></div><p id="""">Once created, it's time to deploy it. We'll use the <strong id="""">kubectl apply</strong> command and pass the YAML file with the <strong id="""">-f</strong> parameter followed by the file name:&nbsp;</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yaml"">
$ kubectl apply -f k8s_deployment.yaml 
deployment.apps/rails-on-k8s created
</code>
</pre></div><p id="""">OK, it looks like it worked! Let's check:&nbsp;</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yaml"">
$ kubectl get pods
NAME                            READY   STATUS    RESTARTS   AGE
rails-on-k8s-7b9f7fb574-8l74g   1/1     Running   0          48s
</code>
</pre></div><p id="""">It seems like, indeed, we successfully deployed our Rails application on Kubernetes.&nbsp;</p><p id="""">But here's the first tip for you. The fact that your Rails pod is running doesn't actually mean that the application inside is too. Without proper checks added to our deployment definition, it's possible that our Rails server actually failed to start properly and Kubernetes doesn't know about it.&nbsp;</p><p id="""">To validate if the application in the pod is indeed running, we can first check the logs:&nbsp;</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yaml"">
$ kubectl logs rails-on-k8s-7b9f7fb574-8l74g
=> Booting Puma
=> Rails 7.0.3.1 application starting in development 
=> Run `bin/rails server --help` for more startup options
Puma starting in single mode...
* Puma version: 5.6.5 (ruby 3.1.2-p20) (""Birdie's Version"")
*  Min threads: 5
*  Max threads: 5
*  Environment: development
*          PID: 1
* Listening on http://0.0.0.0:3000
</code>
</pre></div><p id="""">That looks promising. It seems like our Rails server started properly, so, most likely, everything is good. But the ultimate test would be just to try to access the application. To do that, we first need to expose it somehow. In the Kubernetes world, we do that using Kubernetes services.&nbsp;</p><h4 id="""">Kubernetes Services</h4><p id="""">In order to create a service for our Rails deployment, create another YAML file with content similar to this, but keep in mind that you may need to adjust the type of the service based on what kind of Kubernetes cluster you're using:&nbsp;</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yaml"">
apiVersion: v1
kind: Service
metadata:
  name: rails-service
spec:
  type: LoadBalancer
  selector:
    app: rails-k8s
  ports:
    - port: 80
      targetPort: 3000
</code>
</pre></div><p id="""">Apply the above using <strong id="""">kubectl apply</strong>:&nbsp;</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yaml"">
$ kubectl apply -f rails-svc.yaml 
service/rails-service created
</code>
</pre></div><p id="""">Now, after a few seconds, we should see the IP address of the load balancer that exposes our application:&nbsp;</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yaml"">
$ kubectl get svc
NAME               TYPE           CLUSTER-IP   EXTERNAL-IP     PORT(S)        AGE
kubernetes         ClusterIP      10.0.0.1     <none>          443/TCP        8h
rails-service      LoadBalancer   10.0.74.93   20.101.11.115   80:31330/TCP   48s
</code>
</pre></div><p id="""">And if you open that IP address in your web browser, you should see your application, which in our case is showing a simple ""Hello world"" message:&nbsp;</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:374px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""374px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63320348532d6d29fe074392_oir2-3oatR0SBnWF41muoHhVWAr1v9ffJQkD5cZ6A1Jpvb5gn_8lgRDk9Ti7hC5qG0-RGgFZYTA1O-e1PYIYfpdqm1X_rQ1v5eQaKSD4hFJiQyyhe7PylUJoq3Gh8V9mpCfbtJf1fa_bsxRjlP3T5MR_RE2712cnY6RLApI4lkiAo5xpcvRwLikgLA.jpeg"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><p id="""">All works well, then. We have a Rails application running on a Kubernetes cluster.&nbsp;</p><p id="""">But this was just a simple example when we only deployed our Rails application. If that's all you'd do, it probably won't give you many benefits to move to Kubernetes.&nbsp;</p><p id="""">Earlier in this post, we mentioned that it makes sense to move your Rails application when you have lots of other components supporting your application.&nbsp;</p><p id="""">And if you do, the process of moving them to Kubernetes would be very similar. You either create a <a href=""https://release.com/blog/cutting-build-time-in-half-docker-buildx-kubernetes"" id="""">Docker image</a> from scratch for all the components you need or search for ready-to-use images. Companies behind well-known tools like Redis or NGINX are publishing images themselves, so it's a little bit easier to onboard them on Kubernetes.&nbsp;</p><h4 id="""">But What About the Database?</h4><p id="""">This is a fair question. The database is really important for Rails. Databases can run on Kubernetes too. But depending on your specific needs and due to the nature of databases, it may make more sense to use an SaaS database or keep it on traditional servers.&nbsp;</p><p id="""">You can easily keep your Rails application on a Kubernetes cluster and have it connected to the external database. Of course, for performance reasons, however, you should keep it as close as possible to your Kubernetes, which means ideally in the same cloud provider, the same region, and even the same network.&nbsp;</p><h4 id="""">Rails and Kubernetes Summary</h4><p id="""">We've only touched the tip of the iceberg here when it comes to Kubernetes, but you learned how to package your Rails application into a container and deploy it on Kubernetes. From there, the next steps will depend on your application and company specifics.&nbsp;</p><p id="""">Kubernetes itself is a complex system and gives you multiple choices when it comes to deploying applications, networking, and storage. But you now know how to use it for your Rails application and when it wouldn't really make too much sense.&nbsp;</p><p id="""">If you're interested in learning more about Kubernetes, check out our blog <a href=""https://release.com/blog"" id="""">here</a> for more posts about it.&nbsp;</p>",false,,,,https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63e423c3abd1734902e11bc7_121422%20(1).jpg,,erik-landerholm,3,Wed Dec 14 2022 19:00:00 GMT+0000 (Coordinated Universal Time),,
Rainbow Deployment: Why and how to do it,rainbow-deployment-why-and-how-to-do-it,62aa5a70cd5ba27d9d0d718a,633202a59c2baa7f01c13b4a,Mon Sep 26 2022 19:51:01 GMT+0000 (Coordinated Universal Time),Wed Oct 30 2024 19:54:55 GMT+0000 (Coordinated Universal Time),Wed Oct 30 2024 19:54:55 GMT+0000 (Coordinated Universal Time),Zero-downtime deployments with rainbow deployments. Learn how they work and what benefits they can bring.,,true,<p>Empower zero-downtime deployments and enhance application performance using rainbow deployment strategies with Release.</p>,https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=rainbow-deployments,"<p id="""">What makes an application modern? One defining factor of modern applications is whether they use zero-downtime deployments. If you can deploy a new version of your application without your users realizing it, it's a good indicator that your application follows modern practices. </p><p id="""">In modern, cloud-native environments, zero-downtime deployments are relatively easy to achieve, however it's not always as simple as deploying a new version of your application and then quickly switching traffic to it. Some applications may need to finish long-running tasks first. Others will have to somehow deal with not breaking user sessions. </p><p id="""">What this means is that zero-downtime deployments range from basic to advanced.</p><p id="""">In this post, we're interested in the more advanced zero-downtime deployments. We'll talk about what rainbow deployments are, and how you can use them for efficient zero-downtime deployments.&nbsp;</p><h3 id="""">Zero-Downtime Deployments</h3><p id=""""><strong id="""">Zero-downtime deployment</strong> is when you release a new version of your application without any downtime. This usually means that you deploy a new version of the application, and users are switched to that new version without even knowing.&nbsp;</p><p id="""">Zero-downtime deployments are superior to traditional deployments, where you schedule a ""maintenance window"" and show a ""we are down for maintenance"" message to your users for a certain amount of time. In the world of Kubernetes, there are two main ways of completing (near) zero-downtime deployments: the Kubernetes rolling update deployment, and blue/green deployments. Let's quickly go over both so we'll have a good base of knowledge before diving into the rainbow deployments.&nbsp;</p><h4 id="""">Rolling Update</h4><p id=""""><a href=""https://kubernetes.io/docs/tutorials/kubernetes-basics/update/update-intro/"" target=""_blank"" id="""">Kubernetes rolling updates</a> are simple and effective. Whereas the traditional software update process is usually done by shutting down the old version and then deploying the new version which, of course, will introduce some downtime —a Kubernetes rolling update first deploys a new version of the application next to the old version, and switches traffic to the new version as soon as it's marked as up and running. Only then is the old version deleted. Therefore, no downtime.&nbsp;</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1392px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1392px""><div id=""""><img src=""https://cdn.prod.website-files.com/603dd147c5b0a4221d1bd360/6331ff46619813799a141e9c_eYZ6djaRcmPaLELWBrFfGPVfERG3arPw8zySZEuzRRWvcALej0Q0okNu8LoDPLmDOmFPSJL122zrhqc1SCsflelm-d4T6kIxuYctXLx-nP0l5UqzqwpMwYpA28JCSwFk03kFUkqR2s7mlv0lxBXIsLlf3qpaPSrku-jOBSfI-Y3-pb4gAoAWiuGgfg.jpeg"" id="""" width=""auto"" height=""auto"" loading=""auto"" alt=""""></div></figure><p id="""">However, a Kubernetes rolling update has some limitations. Your application needs to be able to handle such a process, you need to think about database access, and it's a very on/off process. Therefore, you don't have any control over when or how gradually traffic is switched to the new version.&nbsp;</p><h4 id="""">Blue-Green Deployments</h4><p id="""">Blue-green deployments are next-level deployments that try to answer the limitations of simple rolling updates. In this model, you always keep two deployments (or two clones of the whole infrastructure). One is called blue and one is called green. At any given time, only one is active and serving traffic, while the other one will be idle. And once you need to release an update, you do that on the idle side, test if everything works, and then switch the traffic.&nbsp;</p><p id="""">This model is better than a simple rolling update because you have control over switching traffic, and you can have the new version running for a few minutes or even hours so that you can do testing to make sure you won't have any surprises once live traffic hits it.&nbsp;</p><p id="""">However, while better than rolling updates, blue/green deployments also have their limitations. The most important is that you're limited to two environments: blue and green. In many cases, that's enough, but there are use cases where two environments would be a limiting factor, for, example, if you have long-running tasks such as database migrations or AI processing.&nbsp;</p><h4 id="""">When Blue-Green is not Enough</h4><p id="""">Imagine a situation where you deploy a new version of your long-running software to your blue environment, you test if it's okay, and you make it your <a href=""https://release.com/blog/setup-test-environment"" id="""">live environment</a>. Then you do the same again for the green environment—you deploy a new version there and switch again from blue to green.&nbsp;</p><p id="""">Now, if you'd like to deploy a new version again, you'd have to do it in the blue environment. But blue could still be working on that long-running task. You can't simply stop a database migration in the middle because you'll end up with a corrupted database. So you'll have to wait until the software on the blue environment is finished before you can make another deployment. And that's where rainbow deployments come into play.&nbsp;</p><h4 id="""">What is a Rainbow Deployment?</h4><p id="""">Rainbow deployment is the next level of deployment methods that solves the limitation of blue-green deployments. In fact, rainbow deployments are very similar to blue/green deployments, but they're not limited to only two (blue and green) environments. You can have as many colorful environments as you like—thus the name.&nbsp;</p><p id="""">At Release we use Kubernetes namespaces along with our deployment system to automate the creation and removal of rainbow deployments for your application. Release will automatically create and manage a new namespace for each deployment.</p><p id="""">The working principle of rainbow deployment is the same as blue/green deployments, but you can operate on more copies of your application than just two. So, let's take our example from before, in which we would have to wait for the blue environment to finish the long-running task before making a third deployment. With rainbow deployments, you can just add another environment, let's call it yellow.&nbsp;</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1355px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1355px""><div id=""""><img src=""https://cdn.prod.website-files.com/603dd147c5b0a4221d1bd360/6331ff46765de663365256f9_m_tG0e-4nr5mVtvqTJ2rAaLpT_5jp2tTgsNjDftu2K__rgeGGqwFS0xrSOL0RnmcvwQVaCpqOwcwTLpaNOl4Vplsls6H3dNIyLLhBF4HXIAisertb7ToSUXyzCtuBB5jH7nRWkHnTCHtrHly9DRmXHa1zC8SokpCce_ahmOmJXkFNompMOAvMyLCgw.jpeg"" id="""" width=""auto"" height=""auto"" loading=""auto"" alt=""""></div></figure><p id="""">Now we have three environments: blue, green, and yellow. Our blue is busy, and green is currently serving traffic. If we want to deploy a new version of our application, we can deploy it to yellow and then switch traffic to it from green. And that's how rainbow deployment works.&nbsp;</p><p id="""">This is a very powerful method of deploying applications because you can avoid downtime as much as possible for as many users as possible. Long-running tasks blocking your deployments provide just one example, but there are more use cases. For example, if your application uses <a href=""https://en.wikipedia.org/wiki/WebSocket"" target=""_blank"" id="""">WebSockets</a>, no matter how fast and downtime-free your deployments are, you'll still have to disconnect users from their WebSockets sessions, so they'll potentially lose some notifications or other data from your app. Rainbow deployments are the solution: You deploy a new version of your application, and you keep the old one until all users have disconnected from WebSockets sessions. Then you kill the old version of the application.&nbsp;</p><h4 id="""">How to do a Rainbow Deployment</h4><p id="""">Now that you know what rainbow deployments are, let's see how you actually implement them. There is no single standard way of achieving rainbow deployments and there aren't any tools you can install to do rainbow deployments for you—it's more of a do-it-yourself approach. But it isn't all bad news: you can leverage the tools you have to enable rainbow deployments with just a few extra lines of logic.&nbsp;</p><p id="""">So, how do you do it,? You use your current CI/CD pipelines. All you need to do is to point whatever network device you're using to a specific ""color"" of the application when you deploy. In the case of Kubernetes, this could mean changing the <strong id="""">Service</strong> or <strong id="""">Ingress</strong> objects to point to a different deployment. </p><p id="""">Below are some very simple and typical Kubernetes deployment and service definitions:&nbsp;</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yaml"">
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
  labels:
    app: nginx
spec:
  replicas: 1
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: your_application:0.1
        ports:
        - containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: nginxservice
spec:
  selector:
    app: nginx
  ports:
    - protocol: TCP
      port: 80
      name: nginxs
      targetPort: 80
</code>
</pre></div><p id="""">We have one deployment and one service that points to that deployment. The service knows which deployment to target based on deployment labels. The service is instructed to search for deployment that has a label <strong id="""">app</strong> with a value of <strong id="""">nginx.</strong> But what if we target the deployment by color too? Well, that would be a rainbow deployment strategy.&nbsp;</p><h4 id="""">Enter Rainbow Magic</h4><p id="""">So, your definition would look something like this:&nbsp;</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yaml"">
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment-[color]
  labels:
    app: nginx
spec:
  replicas: 1
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
        color: [color]
    spec:
      containers:
      - name: nginx
        image: your_application:0.2
        ports:
        - containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: nginxservice
spec:
  selector:
    app: nginx
    environment: [color]
  ports:
    - protocol: TCP
      port: 80
      name: nginxs
      targetPort: 80
</code>
</pre></div><p id="""">And it would be in your CI/CD job to replace <strong id="""">[color]</strong> in the YAML definition every time you want to deploy a new version. So you deploy your application and service for it, then the next time you want to deploy a new version of that application, instead of updating the existing deployment, you create a new deployment and update the existing service to point to that new deployment. And you can repeat that process as many times as you want. Once the old deployments aren't needed anymore, you can delete them. This is the working principle of rainbow deployments.&nbsp;</p><p id="""">It's also worth mentioning that you don't need to use colors to distinguish your deployments—you can use anything. A common example is to use a Git commit hash. Also, this method isn't exclusive to Kubernetes. You can use it in pretty much any infrastructure or environment as long as you have a way to identify a specific deployment and point your network traffic to.&nbsp;</p><h4 id="""">Rainbow Deployment Summary</h4><p id="""">Rainbow deployments solve a lot of problems that come with common deployment methods and they bring true benefits to your users. However, rainbow deployments are not a magic solution that will solve all your application problems. Your infrastructure and your application need to be compatible with this approach. Database handling may be especially tricky (for example, you don't want to have two applications writing to the same record in the same database). But these are typical problems that you need to solve anyway when dealing with distributed systems.&nbsp;</p><p id="""">Once you improve the user experience, you can also think about improving your developer productivity. If you want to learn more, take a look at our<a href=""https://docs.releasehub.com/reference-documentation/workflows-in-release/rainbow-deployments"" target=""_blank"" id=""""> documentation here.</a></p><p>‍</p>",https://cdn.prod.website-files.com/603dd147c5b0a4221d1bd360/63e423341fc65bd7a2a38d19_120722%20(1).jpg,,tommy-mcclung,4,Wed Dec 07 2022 19:00:00 GMT+0000 (Coordinated Universal Time),,
Release.ai Now Supports NVIDIA NIM: Accelerating AI Development and Deployment,release-ai-now-supports-nvidia-nim-accelerating-ai-development-and-deployment,62aa5a70cd5ba27d9d0d718a,66b2f77d499b608d7f098f4d,Wed Aug 07 2024 04:26:37 GMT+0000 (Coordinated Universal Time),Wed Aug 07 2024 15:04:33 GMT+0000 (Coordinated Universal Time),Wed Aug 07 2024 15:04:33 GMT+0000 (Coordinated Universal Time),Start using NVIDIA NIM and NVIDIA NeMo framework with a simplified setup process,"<p id="""">Release.ai is an orchestration and infrastructure management platform for custom AI applications. We handle the complex backend work, allowing developers and data scientists to focus on innovation. Our self-service platform provides powerful building blocks that simplify AI development and deployment, enabling teams to create advanced applications more efficiently.</p><p id="""">We're excited to announce that Release.ai now supports NVIDIA NIM (NVIDIA Inference Microservices), further enhancing our AI development capabilities. As a member of the NVIDIA Inception program, we're bringing powerful, easy-to-deploy AI functionalities to developers while minimizing the complexities of AI infrastructure management.</p><p>‍</p><p id=""""><strong id="""">Simplifying AI Development with NIM</strong></p><p id="""">NVIDIA NIM offers a set of easy-to-use inference microservices that streamline the deployment of foundation models. By integrating NIM into our platform, Release.ai is making it simpler than ever for teams to leverage NVIDIA's AI technologies in their projects.</p><p>‍</p><p id=""""><strong id="""">Out-of-the-box templates for NVIDIA GPUs</strong></p><p id="""">Release.ai now offers pre-configured templates optimized for NVIDIA GPUs and the NIM framework. These templates dramatically reduce setup time and learning curves, allowing developers to skip the tedious configuration process and jump straight into AI development. With our ever-growing library of the latest models and frameworks, you can start using NVIDIA NIM without navigating a complex setup process. With a ready-to-run NIM framework (along with the NeMo framework included in Release.ai), developers can quickly spin up an environment with the essential AI tools, build and deploy with confidence.</p><p id="""">‍</p><p id="""">‍<strong id="""">Effortless NIM Deployment and Management</strong></p><p id="""">With Release.ai, deploying and managing NIM is streamlined:</p><p id="""">1. Select a NIM-ready template</p><p id="""">2. Launch your AI environment with a few clicks</p><p id="""">3. Let Release.ai handle the ongoing management of your NIM infrastructure</p><p id="""">Our platform takes care of provisioning resources based on your team and application needs, and even auto-scales resources as your application demands change.</p><p>‍</p><p id=""""><strong id="""">AI Lifecycle Workflows</strong></p><p id="""">Release.ai goes beyond just spinning up NIM environments. Our platform provides tools to develop, debug, and manage the entire lifecycle of your AI applications. You can create custom workflows to streamline your development process, easily import AI apps into ephemeral environments, and scale efficiently with proper governance in place.</p><p id="""">‍</p><p id="""">‍<strong id="""">Maximizing ROI on AI Infrastructure</strong></p><p id="""">By leveraging Release.ai for your NVIDIA NIM deployments, you can:</p><p id="""">- <strong id="""">Save Money: </strong>Optimize your compute spend with intelligent resource management</p><p id="""">- <strong id="""">Save Time: </strong>Minimize setup effort and focus on building AI applications</p><p id="""">- <strong id="""">Scale Efficiently:</strong> Grow your AI initiatives with automated, governed processes</p><p id="""">- <strong id="""">Keep Data Private:</strong> Maintain full control over your data, models, and infrastructure</p><p id="""">‍</p><p id=""""><strong id="""">The Power of the NVIDIA Inception Program</strong></p><p id="""">As a member of the NVIDIA Inception program, Release.ai is at the forefront of AI innovation. This partnership allows us to work closely with NVIDIA, ensuring we deliver the right capabilities to teams using NVIDIA GPUs and their data science and AI/ML stacks.</p><p id="""">‍</p><p id=""""><strong id="""">What's Next: Community Templates and Beyond</strong></p><p id="""">Stay tuned for our upcoming community templates, which will allow developers to share and use custom NIM configurations. This feature will further accelerate AI development by leveraging collective expertise.</p><p id="""">‍</p><p id=""""><strong id="""">Getting Started</strong></p><p id="""">Whether you're building large language models, working on computer vision projects, or exploring other AI applications, Release.ai with NVIDIA NIM support provides the easiest way for teams to accelerate their AI development journey.</p><p id="""">‍</p><p id="""">Ready to supercharge your AI workflows? Here's how to get started with Release.ai and NVIDIA NIM:</p><p id="""">‍</p><p id="""">1. Sign up for a free Release.ai account at release.ai/sign-up</p><p id="""">2. Choose a NIM-ready template from our library when creating your application</p><p id="""">3. Launch your AI environment and start building</p><p id="""">‍</p><p id="""">For a limited time, we're offering a sandbox with our premium features. Don't miss this opportunity to transform your AI development process. <a href=""http://release.ai/sign-up"">Get started with Release.ai and NVIDIA NIM today!</a> &nbsp;</p>",false,,,,https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/66b388021ce838ea9c7eca55_Release%2BNVIDIA.jpg,,tommy-mcclung,4,Wed Aug 07 2024 17:00:00 GMT+0000 (Coordinated Universal Time),ai; news; nvidia,
Release Delivery helps SaaS companies meet the needs of their enterprise customers,release-delivery-helps-saas-companies-meet-the-needs-of-their-enterprise-customers,62aa5a70cd5ba27d9d0d718a,6421cdf5764e72d8e24cf808,Mon Mar 27 2023 17:10:13 GMT+0000 (Coordinated Universal Time),Thu Apr 25 2024 20:30:52 GMT+0000 (Coordinated Universal Time),Thu May 02 2024 20:19:24 GMT+0000 (Coordinated Universal Time),"Introducing Release Delivery, our latest addition to the Release environments as a service platform. ","<p id="""">Today, I’m excited to announce the general availability of <a href=""https://release.com/product/release-delivery"" id="""">Release Delivery</a>, our latest addition to the Release environments as a service platform.&nbsp;</p><p id="""">Working with numerous cloud-native SaaS providers we saw the common struggle: <strong id="""">scaling the deployment options to meet the hosting demands of the enterprise customers. </strong>These companies would lead with a multi-tenant offering to optimize resource utilization, deliver faster and more predictable updates, and provide overall consistent performance. This approach would create great software, however, many enterprise customers would see the multi-tenant aspect as potentially commingled data, elevated IT risk profile, and longer approval/whitelisting processes. In the end, enterprises will opt for a safer, albeit not as good, single-tenant version from a competitor.&nbsp;</p><p id="""">Building out a single-tenant version of a SaaS product is a significant investment, and it presents a <strong id="""">tough choice for growing software vendors</strong>. Commit resources to build infrastructure instead of innovation, or miss out on the chance to crack large customer deals.</p><p id="""">We spoke to many SaaS companies who face this conundrum. Do I proactively create the ability to deliver my SaaS application as a single-tenant/self-hosted instance? Or do I wait until a large customer asks for it and then scramble to build it? What if I build it and they decide not to buy it? Did I just waste valuable time building something no one will want?&nbsp;</p><p id=""""><strong id="""">Enter Release Delivery:</strong> the fast way for SaaS companies to <strong id="""">deploy into a variety of environments </strong>required by many large organizations without having to build the functionality themselves.</p><h3 id="""">How Release Delivery Works</h3><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1552px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1552px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/64220e35262071ae64f4d1df_Delivery032823.png"" loading=""lazy"" id="""" width=""auto"" height=""auto""></div></figure><p id="""">Release Delivery templatizes your application deployment and allows you to “stamp out copies” custom-tuned to each unique customer environment, turning your SaaS product into an Enterprise-grade SaaS product.&nbsp;</p><p id="""">With Release Delivery we set out to address the key pain points our customers were facing, and here is what we did:</p><div data-rt-embed-type='true'><table>
  <tr>
    <th>Problem</th>
    <th>Solution</th>
  </tr>
  <tr>
    <td>Modifying your application to run in each disparate single-tenant environment often requires numerous choices and unknowns that make the task a chore</td>
    <td>Release Delivery uses an application template that codifies the necessary design choices and patterns. We saw hundreds of customers design their applications to be reproduced at a click of a button, with a confidence that the decisions they made to adapt the application for a single-tenant/self-hosted deployment will work and be easy to manage in production</td>
  </tr>
  <tr>
    <td>Installing single-tenant applications into customer cloud accounts is tedious and difficult</td>
    <td>Release Delivery simplifies the installation process for your customers. Simply send your customer the installation URL and Release will walk them through every step to get your application running in their cloud account.</td>
  </tr>
  <tr>
    <td>Managing and debugging numerous installations of a single-tenant application becomes untenable</td>
    <td>For troubleshooting and performance monitoring, Release Delivery provides logging and console access to running applications in your customers' cloud accounts. And you can always add other APM tools such as DataDog to your deployed applications.</td>
  </tr>
  <tr>
    <td>Performing timely updates on all single-tenant applications in customer cloud accounts is difficult and slow</td>
    <td>Release Delivery provides a simple mechanism to deploy an upgraded version of your application to one or many deployed single-tenant/self-hosted applications at once. You can also share a ""preview"" version of your app with the customers before going live.</td>
  </tr>
  <tr>
    <td>Handling unique customer security requirements for the single-tenant/self-hosted versions of your SaaS app is a challenge</td>
    <td>Security is top of mind with Release Delivery. In addition to the product being highly secure, including the ability to ""cut the cord"" between Release and your deployments (air-gapped), Release provides in-depth documentation on all necessary security and privacy controls you can provide to your customers during your sales process.</td>
  </tr>
</table></div><p id="""">We believe our core Release Environments as a Service product is the best solution for reproducing and maintaining complex applications. So the ability to deploy an application into a single-tenant or self-hosted environment is a natural extension of our core Release product line.&nbsp;</p><h3 id="""">Unlocking new opportunities</h3><p id="""">Release Delivery comes to you after a year-long close collaboration with our select customers, who saw an opportunity to <strong id="""">take their SaaS products to the next level</strong>. Thanks to their generous input and feedback, we navigated the peculiarities of the “on-cloud”/self-hosted versioning, unlocked access to enterprise requirements and helped them grow large account sales. With Release Delivery these SaaS providers now give their customers an amazing installation experience, wherever and however they want it. With only a few clicks their applications are deployed and running. Our customers can easily monitor and manage these deployments, and quickly upgrade the instances to keep <em id="""">their</em> customers on the latest app version, <strong id="""">without a heavy lift.</strong>&nbsp;&nbsp;</p><p id="""">I am extremely excited about making Release Delivery available to all. If you’re a startup or a SaaS company that wants to generate more revenue from an “on-cloud”/self-hosted version of your application, talk to us! To learn more, <a href=""https://release.com/webinar/release-delivery"" id="""">join my webinar</a> on April 20th, where I will show how Release Delivery works and share how it helps companies solve the problems around flexible deployment. Or, you <a href=""https://release.com/vpc-book-a-meeting"" id="""">book a meeting</a> with our team to see a custom demo and get a trial account.</p>",false,,,,https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/64220e35262071ae64f4d1df_Delivery032823.png,,tommy-mcclung,6,Tue Mar 28 2023 16:00:00 GMT+0000 (Coordinated Universal Time),news,
"Release is going to AWS Summit in New York City on July 26th, 2023",release-is-going-to-aws-summit-in-new-york-city-on-july-26th-2023,62aa5a70cd5ba27d9d0d718a,64adbc92a021ff880cf62d01,Tue Jul 11 2023 20:33:22 GMT+0000 (Coordinated Universal Time),Thu Apr 25 2024 20:28:59 GMT+0000 (Coordinated Universal Time),Thu May 02 2024 20:19:24 GMT+0000 (Coordinated Universal Time),Join us at AWS Summit in NYC!,"<p id="""">Join Release for a trip to New York City this summer! We are headed to <a href=""https://aws.amazon.com/events/summits/new-york/?trk=3fc68b46-3c7f-4e12-9881-a329f1ff1877&sc_channel=el"" id="""">AWS Summit NYC</a> on July 26th and are thrilled to see you all there. &nbsp;You can find us at the Expo Hall booth #343, as well as in the keynote and breakout sessions, just look for a purple Release t-shirt. </p><p>This year the one-day summit is packed with existing content at all technical levels on a wide range of topics like AI and machine learning, analytics, database, EC2 compute, storage, DevOps &amp; developer productivity, serverless, and much more. Explore the agenda with all 150+ sessions <a href=""https://aws.amazon.com/events/summits/new-york/agenda/?amer-summit-cards.sort-by=item.additionalFields.startDateTime&amer-summit-cards.sort-order=asc&awsf.amer-summit-session=*all&awsf.amer-summit-level=*all&awsf.amer-summit-areaofinterest=*all&awsf.amer-summit-industry=*all&awsf.amer-summit-roles=*all&awsf.amer-summit-topic=*all"" id="""">here</a>. And if you are in NY a day early, check out the <a href=""https://aws.amazon.com/events/summits/new-york/women-of-the-cloud/?trk=cfaaccc0-e11b-4d9e-8f78-794bc8efe479&sc_channel=el"" id="""">Women of the Cloud event</a> taking place on July 25th, as part of the summit. </p><p>While you’re at the summit, stop by at our booth to not only test your (and our) rubik's cube solving skills (and get your own cube!), but also learn about all the exciting developments here at Release. </p><p>Our stand-alone Instant Datasets is going live on July 25th. Now you can build and test your applications with near-production data available to you instantly. No more waiting to copy gigantic datasets, or using unrealistic seed data that hides production bugs. Instant Datasets generates and maintains a pool of on-demand production-like datasets available to you when you need them. &nbsp;And for the initial cohort of users we are making Instant Datasets absolutely free. So <a href=""https://www2.release.com/instant-datasets"" id="""">sign up for updates</a> and snag your free account!</p><p>ReleaseAI is in preview and we are actively working with the early adopters to make it even easier for you to talk to your infrastructure.<a href=""https://release.ai/waitlist"" id=""""> Join the preview today</a>. </p><p>Last but not least, we’ve made improvements to our core Release platform making it even easier to create ephemeral replicas of &nbsp;your production environment with every pull request, every check-in and for every developer. Check it out for yourself with <a href=""https://release.com/signup"" id="""">a free trial</a>. </p><p>See you in NYC! </p>",false,,,,https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/64b95a5e1c8ab21a4ec8e5bc_AWS%20Summit%20in%20NYC.jpg,photo credit: Lukas Kloeppel,ira-casteel,3,Thu Jul 20 2023 18:00:00 GMT+0000 (Coordinated Universal Time),events,
"Release is going to AWS Summit in Washington, DC on June 7-8, 2023",release-is-going-to-aws-summit-in-washington-dc-on-june-7-8-2023,62aa5a70cd5ba27d9d0d718a,64774ff264e892ebd35e500d,Wed May 31 2023 13:47:30 GMT+0000 (Coordinated Universal Time),Thu Apr 25 2024 20:29:40 GMT+0000 (Coordinated Universal Time),Thu May 02 2024 20:19:24 GMT+0000 (Coordinated Universal Time),"Join Release at the AWS Summit in Washington, DC ","<p id="""">Release is thrilled to participate in this year’s <a href=""https://aws.amazon.com/events/summits/washington-dc/"">AWS Summit in Washington, DC</a> on June 7th and 8th. Come join us for this complimentary event to learn about the latest and greatest in cloud computing and how Release and AWS are helping organizations accelerate their digital transformation and deliver mission-critical applications. &nbsp;</p><p id="""">Release Environments as a Service platform is built on AWS and is deployed directly into your AWS public or GovCloud account. This gives you full control and visibility into your AWS activities, and helps consolidate spend for best cloud discount options. Release is available though the <a href=""https://aws.amazon.com/marketplace/pp/prodview-faxqlvzq65fea"" id="""">AWS Marketplace</a>, providing procurement flexibility and maximization of AWS commitments.</p><p id="""">Find us in Booth #363 of the main Expo Hall to talk about all things environments, such as <a href=""https://release.com/product/release-delivery"" id="""">Release Delivery</a> that allows you to deploy your SaaS apps inside of your customers’ environment, without creating an on-prem version of your product; or<a href=""https://release.com/blog/rainbow-deployment-why-and-how-to-do-it"" id=""""> Rainbow deployments</a>, taking your blue-green approach to the next level. See our latest integrations of the generative AI helping build the environment templates even faster with the <a href=""https://release.com/blog/code-to-cloud-simplified-how-release-uses-ai-to-make-deployment-easier"" id="""">Dockerfile auto-generation project</a>, and making sense of your application documentation with the <a href=""https://release.com/blog/gromit-an-open-source-ai-assistant-for-your-documentation"" id="""">Gromit project</a>. Or simply say hi in one of the <a href=""https://aws.amazon.com/events/summits/washington-dc/agenda/?amer-summit-cards.sort-by=item.additionalFields.startDateTime&amer-summit-cards.sort-order=asc&awsf.amer-summit-session=*all&awsf.amer-summit-level=*all&awsf.amer-summit-areaofinterest=*all&awsf.amer-summit-industry=*all&awsf.amer-summit-roles=*all&awsf.amer-summit-topic=*all"" id="""">numerous breakout sessions</a>, lightning talks or workshops. Just look out for the purple Release t-shirt! </p><p id="""">We are excited to spend a couple days learning and sharing our ideas. See you in DC!</p>",false,,,,https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/64774da1b6f33f509c00b830_AWS%20Summit%20in%20DC.jpg,,ira-casteel,2,Wed May 31 2023 18:37:00 GMT+0000 (Coordinated Universal Time),events,
Release is going to DockerCon 2023,release-is-going-to-dockercon-2023,62aa5a70cd5ba27d9d0d718a,6509b7dbfd7bed2165e35fab,Tue Sep 19 2023 15:01:47 GMT+0000 (Coordinated Universal Time),Thu Jul 25 2024 15:04:43 GMT+0000 (Coordinated Universal Time),Thu Jul 25 2024 16:27:39 GMT+0000 (Coordinated Universal Time),"Join us at DockerCon 2023 for two days of learning, exploring and networking. ","<p id="""">Back-to-school and back-to-conferences! We're excited to announce that Release will be attending <a href=""https://www.dockercon.com/"" id="""">DockerCon 2023</a> held in-person in Los Angeles this year. As one of the notable tech events in the container industry, DockerCon promises exciting innovations, industry insights, and networking opportunities. Here's what we're looking forward to this year:</p><p id=""""><strong id="""">🐳 Exploring the latest Docker Technology:</strong>&nbsp;<br>Docker reshaped the way we think about, build, and deploy software. DockerCon is an excellent opportunity to learn about new tools, techniques, and best practices, and get a firsthand look at the latest Docker developments. We’re curious to see how these developments might resonate with our initiatives and broader industry landscape. </p><p id=""""><strong id="""">👋 Engaging with the Docker Community:&nbsp;<br>‍</strong>There's nothing like being surrounded by fellow Docker enthusiasts who are just as passionate about containers as we are! DockerCon offers a unique platform to meet experts, share experiences, and foster partnerships. We're excited to exchange insights with fellow attendees and potentially collaborate on future endeavors.</p><p id=""""><strong id="""">📝 Diving into Workshops and Breakout Sessions:<br>‍</strong>DockerCon's workshops are where theory meets practice. We're particularly interested in hands-on sessions that will provide our team with practical knowledge and skills that we can immediately apply to our projects. Check out what one of the Docker Captain has to say about the <a href=""https://www.docker.com/blog/dockercon-workshops-what-to-expect/"" id="""">upcoming DockerCon workshops</a> and pick a track that makes sense for you. </p><p id=""""><strong id="""">🤓 Sharing Our Own Insights:<br>‍</strong>While we're excited to absorb a ton of new information, we're also thrilled to share our own experiences with the Docker community. Our team has worked on some groundbreaking projects, and we believe our experiences can be valuable to others on their containerization journey. Come see us at the booth or in one of the sessions, just look for a purple Release t-shirt. </p><p id="""">DockerCon 2023 promises to be more than just a conference; it's an experience, a learning expedition, and an opportunity to engage with the global container community. We at Release are gearing up for an enlightening few days, and we can't wait to share our learnings and experiences with you once we're back.</p><p id="""">P.S. If you're also attending DockerCon 2023, drop us a line! We'd love to catch up and share notes.</p>",false,,,,https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/6509b7d48be9f9d1c10bffe4_Docker%20Crates.jpg,Moby DockerCon2022,ira-casteel,2,Wed Sep 20 2023 17:00:00 GMT+0000 (Coordinated Universal Time),events; docker,
Release is going to KubeCon 2023,release-is-going-to-kubecon-2023,62aa5a70cd5ba27d9d0d718a,652eea9d37aec5bfbe51c4da,Tue Oct 17 2023 20:12:13 GMT+0000 (Coordinated Universal Time),Thu Apr 25 2024 20:27:48 GMT+0000 (Coordinated Universal Time),Thu May 02 2024 20:19:24 GMT+0000 (Coordinated Universal Time),Join us at KubeCon this year to explore all things Kubernetes. ,"<p id="""">The countdown has begun! We’re pleased to share that Release will be attending <a href=""https://events.linuxfoundation.org/kubecon-cloudnativecon-north-america/"" id="""">KubeCon in Chicago</a> on November 6-9. As one of the key events centered on Kubernetes and cloud-native technologies, KubeCon promises insightful sessions, spirited discussions, and invaluable networking opportunities. Here's a sneak peek into what we're anticipating:</p><p id="""">‍<strong id="""">☸️ Kubernetes Innovations:</strong></p><p id="""">Every year, KubeCon presents a myriad of newest trends and best practices in Kubernetes. We’re keen on delving into the latest developments and understanding how they can fit into our ecosystem and benefit our customers. See this year’s <a href=""https://events.linuxfoundation.org/kubecon-cloudnativecon-north-america/program/keynote-speakers/"" id="""">Keynote speakers lineup</a> for some clues on what will be discussed. </p>",true,"<p id="""">Ready to try Release? <br>Use code #LEARN&nbsp;for 30 days free. </p>",https://release.com/signup,"<p id="""">‍<strong id="""">🔊Discussions and Panels:</strong></p><p id="""">KubeCon has always been a melting pot of ideas. The panel discussions and breakout sessions are where some of the best minds come together to discuss the future of cloud-native solutions. We’re all set to participate, contribute, and learn throughout the event. Check out the <a href=""https://events.linuxfoundation.org/kubecon-cloudnativecon-north-america/program/schedule/"" id="""">Program</a> to build a schedule that fits your interests. </p><p id="""">‍<strong id="""">👋 Networking with Cloud Native Enthusiasts:</strong></p><p id="""">KubeCon gathers a diverse group of professionals interested in cloud-native technologies. We're eager to share experiences, discuss challenges, and explore possibilities with peers from across the globe. As always, there are a number of <a href=""https://events.linuxfoundation.org/kubecon-cloudnativecon-north-america/co-located-events/cncf-hosted-co-located-schedule/"" id="""">co-located events</a> to fit any kind of interest. </p><p id="""">‍<strong id="""">🎙️Showcasing Release Contributions:</strong></p><p id="""">Stop by the Booth N30 to chat about our latest developments, see a demo and pick up some Release swag. Whether you have a specific challenge in mind or are just curious about what we do, our team is there to engage and share. </p><p id="""">Don’t forget to <a href=""https://events.linuxfoundation.org/kubecon-cloudnativecon-north-america/register/"" id="""">register</a> and see you at the conference.</p>",https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/652ee8c045850151faa913ce_Kubecon23.jpg,,ira-casteel,2,Wed Oct 18 2023 18:00:00 GMT+0000 (Coordinated Universal Time),events,10-kubernetes-namespace-best-practices-to-start-following; how-to-make-kubernetes-config-files-not-suck
Release is going to OSS North America,release-is-going-to-oss-north-america,62aa5a70cd5ba27d9d0d718a,64593e6c680449176d2d9e2f,Mon May 08 2023 18:24:44 GMT+0000 (Coordinated Universal Time),Thu Apr 25 2024 20:30:04 GMT+0000 (Coordinated Universal Time),Thu May 02 2024 20:19:24 GMT+0000 (Coordinated Universal Time),,"<p id="""">Come see us at <a href=""https://events.linuxfoundation.org/open-source-summit-north-america/"" id="""">Open Source Summit North America</a> in Vancouver, Canada this week! We are excited to be attending the conference, hearing amazing talks, and seeing all the cool things the community is working on. This year we are showing off our own open source project “gromit” and are demoing the improvements we made to our Release Development Platform.</p><p id="""">Here at Release we are strong believers in the power of open source. In our drive to create a development platform that better serves modern development teams, our founding team has been using and contributing to the open source community for years. Founded in 2019 Release was and still is built entirely with open source software!&nbsp;</p><p id="""">This year, we are excited to announce a contribution to the open source community that allows anyone to take their documentation and create an AI powered assistant. Based on the work done at <a href=""https://supabase.com/blog/chatgpt-supabase-docs"" id="""">Supabase</a> to create a ChatGPT user interface for documentation, Release is open sourcing <a href=""https://rubygems.org/gems/gromit"" id="""">a ruby gem “gromit”</a> that provides a vector search backend using Ruby and Redis.</p><p id="""">Our vision at Release is for all developers and teams to use the cloud to its full potential without needing to be experts in everything. We do this by simplifying the creation, access and usage of all environments (development, ephemeral, testing, staging, production, etc.), while maintaining access to advanced tooling.</p><p id="""">If you are looking for ways to increase developer productivity through a truly modern cloud agnostic platform or just want to see a cool demo of “gromit” in action come see us at our booth! We love chatting with developers, learning about their successes and challenges (comparing notes) and making teams as productive as possible.&nbsp; See you there!</p><p id="""">‍</p>",false,,,,https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/64593dc368044995bc2d98c9_Release%20is%20going%20to%20OSS%20North%20America.png,Release is going to OSS North America,david-giffin,2,Mon May 08 2023 18:22:00 GMT+0000 (Coordinated Universal Time),events,11-continuous-deployment-tools-and-how-to-choose-one
Release is going to PlatformCon 2023  ,release-is-going-to-platformcon-2023,62aa5a70cd5ba27d9d0d718a,647ff9ea2d252ed460bab9dc,Wed Jun 07 2023 03:30:50 GMT+0000 (Coordinated Universal Time),Thu Apr 25 2024 20:29:34 GMT+0000 (Coordinated Universal Time),Thu May 02 2024 20:19:24 GMT+0000 (Coordinated Universal Time),Join Release for the exciting two days of learning and discovery at PlatformCon 2023 on June 8th and 9th.,"<p id="""">Join Release for the exciting two days of learning and discovery at<a href=""https://platformcon.com/"" id=""""> PlatformCon 2023</a> on June 8th and 9th. This is the second year of PlatformCon bringing the community of Platform Engineers, DevOps Practitioners and Technology Leaders together to discuss and share insights around the growing field of Platform Engineering. And again it is a free online event, so make sure to sign up and join the discussion. </p><p>This years’ <a href=""https://platformcon.com/talks"" id="""">talks</a> are organized into five categories: Stories, Tech, Culture, Blueprints and Impact, and sessions across the all tracks are quite impressive. We are looking forward to learning about the history and humble beginnings of containers from Micheal Irvin, a DevRel lead at <a href=""https://www.docker.com/"" id="""">Docker</a>, in his talk “<a href=""https://platformcon.com/talks/containers-where-we-came-from-and-where-the-futures-taking-us"" id="""">Containers: Where we came from and where the future's taking us</a>”. Exploring the complexities of today's software and distributed systems with Gregor Hohpe, a Director of Enterprise Strategy at <a href=""https://aws.amazon.com/"" id="""">AWS</a>, in his talk “<a href=""https://platformcon.com/talks/build-abstractions-not-illusions"" id="""">Building Abstractions, not illusions</a>”. And learning valuable practitioner lessons with Reynis Vazquez-Guzman, Sr. Software Engineer at <a href=""https://www.affirm.com/"" id="""">Affirm</a>, in her “​​<a href=""https://platformcon.com/talks/where-we-went-wrong-in-building-a-self-service-platform-on-kubernetes"" id="""">Where we went wrong in building a self-service platform on Kubernetes</a>” talk. </p><p>Our own Nick Busey will be sharing his insights around evolving developer experiences. He will discuss &nbsp;on-demand environments as a way of providing teams with a fulfilling development experience. In particular, how you build systems to maintain the excitement and ease of collaboration, as you build out your product in his talk “<a href=""https://platformcon.com/talks/from-skateboard-to-car-how-to-evolve-your-developer-experience"" id="""">From skateboard to car: How to evolve your developer experience</a>” on June 8th, in the Blueprints track. </p><p>What sessions are you most looking forward to? See you there! </p>",false,,,,https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/647ff8965e57889a95f51462_release%2BPlatfromCon.jpg,,ira-casteel,2,Wed Jun 07 2023 18:00:00 GMT+0000 (Coordinated Universal Time),events,12-things-you-didnt-know-you-could-do-with-release-part-1
Release is going to RailsConf 2023,release-is-going-to-railsconf-2023,62aa5a70cd5ba27d9d0d718a,64399657f987dbe8a015660e,Fri Apr 14 2023 18:07:19 GMT+0000 (Coordinated Universal Time),Thu Apr 25 2024 20:30:26 GMT+0000 (Coordinated Universal Time),Thu May 02 2024 20:19:24 GMT+0000 (Coordinated Universal Time),RailsConf 2023 is almost here! We will be showing off some exciting additions to the Release Development Platform.,"<p id=""""><a href=""https://railsconf.org/"" target=""_blank"">RailsConf 2023</a> is almost here and we at Release are delighted to be attending and showing off some really exciting additions to the Release Development Platform.&nbsp; We love attending RailsConf to hear amazing talks and show people all the cool things we are working on to support developers and teams as they use Rails to create amazing products.&nbsp;&nbsp;</p><p id="""">Founded in 2019 Release was and still is built almost entirely with <a href=""https://rubycentral.org"" target=""_blank"" id="""">Rails</a>!&nbsp; The founding team has 30+ years of Rails experience and Rails has served us perfectly in our drive to create a development platform that serves the modern development organization.</p><p id="""">Our vision at Release is for all developers and teams to use the cloud to its full potential without needing to be experts in everything. We do this by simplifying the creation, access and usage of all environments (development, ephemeral, testing, staging, production, etc.), while maintaining access to advanced tooling.</p><p id="""">This year we will be demoing something we are super excited about!&nbsp; Our newest “Code to Cloud” feature was developed using AI to get your applications running on <a href=""https://docs.release.com/integrations/integrations-overview/aws-integration/aws-howtos"" target=""_blank"" id="""">AWS</a> or <a href=""https://docs.release.com/integrations/integrations-overview/gcp-integration"" target=""_blank"" id="""">GCP</a> quickly!&nbsp; Release now gets your Rails project into the cloud without you needing to be docker, k8s, terraform, buildpacks, helm or even cloud specialist.&nbsp;</p><p id="""">We use advanced AI to interrogate your repositories and generate all the necessary configuration (dockerfiles, k8s manifests, etc.) with very little intervention and iteration on your part.&nbsp; We smooth out the learning curve of moving from something like heroku to AWS while allowing you to retain the simplicity of heroku with the power of AWS!</p><p id="""">If you are looking for ways to increase developer productivity through a truly modern cloud agnostic platform or just want to see a cool demo of getting a Rails repository running in AWS on EKS, come see us at our booth! We love chatting with developers, learning about their successes and challenges (comparing notes) and making teams as productive as possible.&nbsp; See you there!</p>",false,,,,https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/643cb72eaa4cc54b02c8c19f_release%2BRailsConf.jpg,Release is going to RailsConf 2023,erik-landerholm,2,Fri Apr 14 2023 18:00:00 GMT+0000 (Coordinated Universal Time),events,
Release joins NVIDIA Inception Partner Program; Brings AI Environments to NVIDIA GPUs and Stack,release-joins-nvidia-inception-partner-program-brings-ai-environments-to-nvidia-gpus-and-stack,62aa5a70cd5ba27d9d0d718a,65ccf5dbe48d5d1d9a26fad8,Wed Feb 14 2024 17:18:19 GMT+0000 (Coordinated Universal Time),Thu Jul 25 2024 15:00:19 GMT+0000 (Coordinated Universal Time),Thu Jul 25 2024 16:27:39 GMT+0000 (Coordinated Universal Time),We’re excited to announce that Release has joined the NVIDIA Inception Partner Program.,"<p id="""">We’re excited to <a href=""/press-releases/release-joins-nvidia-inception-to-bring-ai-environments-tailored-for-nvidia-stack-and-gpus"">announce that Release has joined the NVIDIA Inception Partner Program.</a> The NVIDIA Inception program is an ecosystem of startups working on AI and data science with NVIDIA GPUs. As developer and data science teams begin to build applications and train data sets with NVIDIA NeMo and GPUs, <a href=""https://www.youtube.com/watch?v=IKajKw7-Ajs"" id="""">environments as a service</a> is a perfect complement to the success and speed of teams wanting to focus on data insights, not complex infrastructure.</p><p id="""">As an Inception Partner, we are working closely with NVIDIA to make sure we are delivering the right capabilities to teams using NVIDIA GPUs and their data science and AI/ML stacks. NVIDIA is the latest partnership for Release, which spans the entire application delivery and deployment lifecycle. We’re committed to integrations with all of the tools our customers use, from repositories and CI/CD tooling to cloud platform vendors and consulting services.&nbsp;</p><p id="""">But why does this matter to you? For starters, it means seamless integration with your preferred cloud platform, ensuring optimal performance without any compatibility headaches. Additionally, our partnerships provide access to valuable resources and support, streamlining deployment and troubleshooting. And let's not forget about staying at the forefront of innovation – with our connections, you'll always be in tune with the latest advancements and trends in technology.&nbsp;</p><p id="""">Along with NVIDIA, Release is working with companies like:</p><ul id=""""><li id="""">AWS</li><li id="""">Google Cloud Platform</li><li id="""">Docker</li><li id="""">Tonic</li><li id="""">Codingscape</li></ul><p id="""">You can get a full list of our partners at release.com/partners</p><p id="""">Is there a company you would like to see as a partner? Let us know! Email <a href=""mailto:partner@release.com"" id="""">partner@release.com</a> and tell us who we should work with.</p>",false,,,,https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/65cd2a930be999e7a7f5f63e_65cd2a3bf27f15280fa26b1f_release%2BNVIDIA.webp,,matt-carter,3,Thu Feb 15 2024 14:00:00 GMT+0000 (Coordinated Universal Time),news; product; ai,
Release now available through the AWS Marketplace,release-now-available-through-the-aws-marketplace,62aa5a70cd5ba27d9d0d718a,637bbef6467da3d3d9ac2b0f,Mon Nov 21 2022 18:09:58 GMT+0000 (Coordinated Universal Time),Fri Apr 21 2023 16:48:24 GMT+0000 (Coordinated Universal Time),Fri Apr 21 2023 16:55:46 GMT+0000 (Coordinated Universal Time),"We have heard from many customers that they prefer to buy software through the AWS Marketplace, and this announcement me","<p id="""">I’m excited to announce that customers can now purchase Release through the <a href=""https://aws.amazon.com/marketplace/pp/prodview-faxqlvzq65fea"" target=""_blank"">AWS Marketplace.</a> We have heard from many customers that they prefer to buy software through the AWS Marketplace, and this announcement means you can buy Release in the way you and your company prefer. </p><p id="""">If you are unfamiliar with the <a href=""https://aws.amazon.com/marketplace"" target=""_blank"">AWS Marketplace</a>, it provides a number of compelling benefits for AWS customers. It streamlines procurement and accelerates the vendor onboarding process by allowing organizations to leverage existing agreements with AWS. It enables quicker self-service transactions through the marketplace portal. And finally, it allows customers to fulfill a portion of their contractual AWS spend commitment, potentially attaining deeper AWS discounts. Specifically, 50% of customer's Release spend counts toward their total AWS spend commitment when transacted via the AWS Marketplace.</p><p id="""">AWS Marketplace is the latest addition to our deep commitment to and partnership with AWS. Release helps AWS customers build, test, and deploy their apps with speed and confidence, and this further accelerates how AWS customers can use Release in their app pipelines. If you or your team are looking for more details on our support for AWS apps, you can find additional details <a href=""https://release.com/whitepaper/aws-releasehub-for-you"">in this white-paper. </a></p>",false,,,,https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63e69b7b3612c397c84ffaf6_63c1bebeca87e7ad127d4ef2_Logo%20-%20Light%20on%20Dark%201.svg,Release logo,tommy-mcclung,1,Tue Nov 22 2022 18:30:00 GMT+0000 (Coordinated Universal Time),,
Release Offers Support for ECS,release-offers-support-for-ecs,62aa5a70cd5ba27d9d0d718a,638650de801e8edcfb1cee9c,Tue Nov 29 2022 18:35:10 GMT+0000 (Coordinated Universal Time),Fri Apr 21 2023 16:48:03 GMT+0000 (Coordinated Universal Time),Fri Apr 21 2023 16:55:46 GMT+0000 (Coordinated Universal Time),"Release now supports the use of IaC (Infrastructure as Code: Terraform, Pulumi, etc) to create resources in ECS","<p id="""">Complex applications on AWS frequently combine not only the core code but also connections to resources and dependencies (such as ECS Tasks and Lambdas). We’re excited to announce support for ECS tasks! Development, testing, and production of complex apps can now be managed through our newest Release update. This latest update includes support for the use of IaC (Infrastructure as Code, for example, Terraform and Pulumi) &nbsp;to create resources in ECS, which you can manage in Release right alongside your EKS resources. In this post, we'll show you some simple set-up steps to add ECS resources to Release, access them through the terminal, and view their logs.</p><p id="""">Release is a Kubernetes-first platform, but we have many customers that have described their environments in detail with an IaC and would like their environments to match their production environments as closely as possible. With our support for ECS you can now view and manage your ECS and EKS resources through Release’s single administration interface. Release automatically tags and configures your ECS resources if you use our Infrastructure Runner, but we also allow you to tag and configure any ECS resources and manage them in Release, regardless of how they were deployed.</p><h3 id="""">AWS ECS Task Support</h3><p id="""">The first supported cloud resources are AWS ECS Tasks, but more resource types will be supported soon. Let us know what cloud resources you’d like to see supported!</p><h3 id="""">Cloud Resources</h3><p id="""">After tagging a resource, it will show up on the Environment page in the Cloud Resources section, below your Instances.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-center"" data-rt-type=""image"" data-rt-align=""center""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63864d9a2838767ed3aef78e_6vp6oHneiORC2ZBesZElbqRuKYCAjwt7bNHv_GYWyG7JnzASkvwaaHt85DmybykHWjtrkfzgdVEUZGCFS4ZFNMoooeABB_Tsk5VkNaxT06kpuAqBgQd_TY6qMJ-csOmCVFXg-xVHCk1YxTbaXxpul5WO6o83sBdsDI4-ZxC34QnDbBxOUP5UUEeVV6ui5A.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><p id=""""><em id=""""><br>Cloud Resources like this ECS Task are listed on the environment page in a section below Instances. ECS Tasks support Terminal and Logs, which you launch by clicking the corresponding button.</em></p><h3 id="""">Interacting with Cloud Resources</h3><p id="""">Currently, once you've tagged them, ECS Tasks support a couple of actions: Terminal and Logs (note the Terminal and Logs buttons in the previous screenshot).</p><p id="""">You can launch a web terminal into the task containers by clicking the <strong id="""">Terminal</strong> button:</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63864d9acd30f07323233730_wnVQjdDtGQT6bvT7_SYDBuIYCbGwCC5V6Pa5a0-9Ilnad-bkWrmp-rgqW3i4MnHb8N7tSW65iSJcw1ea3I5jsCtcOqe28jpJEfGRqq2YT_r0KxOTddppA4BItJume2WED0rLjiTju3ndRAq2XH9-Cgfyp1y_MirnWfG0wtpWqKKtVVfHwA6GbkxbIyLjQg.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><p id="""">And you can view container logs by clicking the <strong id="""">Logs</strong> button:</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63864da831662d6dabceb043_SZlujsaQiITV9ZhKqpVisWC7J74JHW-N4C8IORQNELHcKTSGlkPljRKxOfIfFEqIL2LwGi4hBDh1xcKXaHEq77xvTGkrvO_ACvV_QOsr-ejI2At-ru_TepzYWD9TMmeHdHC1CpAO8ueKOqrNBcwBtS5mBn0enAmRTZMYYd-CO9p6F_L6YXOoUWrSDB6a8w.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><h3 id="""">How to add Cloud Resources</h3><p id="""">To add a cloud resource to your Release environments, you'll need to add a couple of tags/labels to that resource in AWS or GCP.</p><p id="""">Two tags with values are required:</p><div data-rt-embed-type='true'><table class=""blog_table"">
  <tr class=""blog_table_row"">
    <th class=""blog_table_header"">Tag Key</th>
    <th class=""blog_table_header"">Tag Value</th>
    <th class=""blog_table_header"">Location in UI</th>
    <th class=""blog_table_header"">Env variable</th>
  </tr>
  <tr class=""blog_table_row"">
    <td class=""blog_table_cell"">
      <div class=""blog_table_tag-key"">releasehub.com/app</div>
      <div class=""blog_table_tag-key"">name</div>
    </td>
    <td class=""blog_table_cell"">
      Tag Key
    </td>
    <td class=""blog_table_cell"">
      Top of the environment page. In the screenshot below, the app name is
      <div class=""blog_table_highlight"">example-voting-app</div>
    </td>
    <td class=""blog_table_cell"">
      <div class=""blog_table_highlight"">RELEASE_APP_NAME</div>
      (see default Release environment variables)
    </td>
  </tr>
  <tr class=""blog_table_row"">
    <td class=""blog_table_cell"">
      <div class=""blog_table_tag-key"">releasehub.com/env-id</div>
    </td>
    <td class=""blog_table_cell"">
      (the environment handle)
    </td>
    <td class=""blog_table_cell"">
      In the screenshot below, the environment handle is
      <div class=""blog_table_highlight"">ted3bff</div>
    </td>
    <td class=""blog_table_cell"">
      <div class=""blog_table_highlight"">RELEASE_ENV_ID</div>
      (see default Release environment variables)
    </td>
  </tr>
</table></div><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63864d9a5b977d98b65f9a38_80uIbsve4EXO-YVaE7zzy38NyLAjvqYziIa1EfEeLzGbnEtQ-Fu1Uh4YmG2Fv46TI1B3LFLYSsho9-_20ud9wYdBuH4FetglWmbgrugkSJtkZx8mg9LFEsul6p1PKd7U858a6ExFv9tcA46YhAON8Xz65VyO-qi6yCRrQI21GSwVb-4LS6aC-f4xW4ogAQ.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><p id="""">AWS Documentation can be found <a href=""https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs-using-tags.html"" target=""_blank"" id="""">here</a>.</p><h3 id="""">How to Configure the Logs</h3><p id="""">In order to send your ECS task logs to the Release log viewer, you must configure your ECS tasks to use the awslogs log driver:</p><p id="""">You can specify the log driver in the task definition. Inside each entry in containerDefinitions, update (or add) the logConfiguration key to specify the logDriver and options. Options must specify awslogs-group, and awslogs-stream-prefix. The logDriver must be awslogs, but you may choose your own group and stream-prefix.<br><br>That section of your task definition should look something like this when you’re done:</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yaml"">
‍
{
   ""containerDefinitions"": [
       {
           ""logConfiguration"": {
               ""logDriver"": ""awslogs"",
               ""options"": {
                   ""awslogs-group"": ""firelens-container"",
                   ""awslogs-region"": ""us-west-2"",
                   ""awslogs-create-group"": ""true"",
                   ""awslogs-stream-prefix"": ""firelens""
               }
           }
       }
   ]
}
</code>
</pre></div><p id="""">For more details, check out the <a href=""https://docs.aws.amazon.com/AmazonECS/latest/developerguide/using_awslogs.html#specify-log-config"" target=""_blank"" id="""">AWS documentation on using the awslogs log driver</a>.</p><h3 id="""">How to Configure the Terminal</h3><p id="""">The following requirements must be met before being able to terminal into any ECS task:</p><ol id=""""><li id="""">ECS Exec needs to be enabled for the task. (<a href=""https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs-exec.html#ecs-exec-enabling"" target=""_blank"" id="""">more info</a>)</li><li id="""">ECS Exec IAM permissions need to be added for the task. (<a href=""https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs-exec.html#ecs-exec-required-iam-permissions"" target=""_blank"" id="""">more info</a>)</li></ol><p id="""">For more details, check out the <a href=""https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs-exec.html"" target=""_blank"" id="""">AWS documentation on enabling ECS Exec</a> and the best practices around it.</p><p id="""">After configuring the ECS task, use the Cloud Resources refresh button and you should be able to choose the container to terminal into. Note that Release will gray out the terminal button for containers that are not running or tasks that do not have ECS Exec enabled.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1244px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1244px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63864d9a982e95d8d2f8c64b_fbX87liG3mRBfGePVoiKicRzqKbV8rssBdykJmSj7WbrWXEbLbIWTm46g6Gx5RFTXEmZyooo2jXdusPCA41_Bnt7Ly826JKJLU_EC1Sl_G0phAOgOb_AYbMPu3xdKrcMx8t_Xure0F83K95jywPj8hK1J0P4IA_GukKRscqKhIbSeYNILk5-lghtAGA_Jw.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><h3 id="""">Try it Yourself</h3><p id="""">At Release we always try to support the infrastructure our customers have in place while allowing them to take advantage of any new offerings from AWS. If you would like to manage your containers, regardless of which container management service you use, read through our <a href=""https://docs.releasehub.com/reference-documentation/cloud-resources"" target=""_blank"" id="""">documentation</a> and/or schedule a <a href=""https://releasehub.com/book-a-demo"" target=""_blank"" id="""">demo</a>!</p>",false,,,,https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63e3f4159948b73b195df9da_1130_222%20(1).jpg,A group of colorful containers,erik-landerholm,4,Wed Nov 30 2022 18:00:00 GMT+0000 (Coordinated Universal Time),,
ReleaseHub Announces $20 Million Series A Round Led by CRV,releasehub-20-million-series-a-led-by-crv,62aa5a70cd5ba27d9d0d718a,62aa5a70cd5ba26da70d72fd,Mon Oct 04 2021 21:33:00 GMT+0000 (Coordinated Universal Time),Fri Apr 21 2023 19:42:22 GMT+0000 (Coordinated Universal Time),,ReleaseHub has completed a $20 million Series A funding round led by CRV with participation by Sequoia and Y Combinator.,"<p id=""""><a href=""https://release.com/"">Release,</a> which provides environments-as-a-service (EaaS), has announced it has completed a $20 million Series A funding round. Led by <a href=""http://www.crv.com"" id="""">CRV </a>with participation from <a href=""https://www.sequoiacap.com/"" id="""">Sequoia</a>, <a href=""https://www.ycombinator.com/"" id="""">Y Combinator</a>, <a href=""https://bowcapital.com/team/"" id="""">Bow Capital</a>, <a href=""https://www.artisanalv.com/"" id="""">Artisanal Ventures</a>, <a href=""https://hack-vc.com/"" id="""">Hack VC </a>and angel investors Amit Agarwal (Datadog), Bill Clerico (WePay) and Chase Gilbert (Built Technologies), the Series A follows a $2.7 million seed round in April 2020.</p><p id="""">ReleaseHub solves the universal problem of the costs and difficulty of creating, managing and maintaining increasingly complex environments for software developers. Customers of ReleaseHub are using EaaS to create cloud native full-stack environments with data, on-demand. These environments are being used for previewing and QA’ing features in the dev workflow, demoing software with sales demo environments, running production applications and delivering SaaS products into their customer’s virtual private clouds. ReleaseHub’s EaaS delivers an exceptional and simple development experience with environments as its core, with the full capability of the major cloud providers.</p><p id="""">“Making developers and organizations more effective and efficient has been a problem and a challenge everywhere we’ve worked,” said co-founder and CEO Tommy McClung. “In addition, development in the cloud has become more complicated. Using Kubernetes and deep integrations with each cloud provider to facilitate the ability to reproduce environments, ReleaseHub can replace a ton of work that has been a bespoke maintenance nightmare allowing developers to do it instantaneously.”</p><p id="""">Part of the Y Combinator winter 2020 cohort, ReleaseHub was founded by McClung along with Erik Landerholm and David Giffin. They previously led the technology team at an ecommerce company where they spearheaded the companywide effort to remove environment bottlenecks. There were no commercial solutions at the time so the founders built their own environment management solution and soon after launched ReleaseHub to commercialize EaaS.</p><p id="""">""Time and time again we hear from technology companies that managing their environments is becoming increasingly complex as we move to hybrid and microservice based architectures,” said James Green, venture investor at CRV. “Given this complexity, a logical step to making development teams more productive is a platform to automate many of these workflows around environments. We came across ReleaseHub through the developer community and were blown away by their product and progress. The team experienced this pain themselves while on the leadership team at TrueCar and are now solving this issue at many complex organizations including Shogun, Monad Security and BraveCare. We're delighted to partner with them.”</p><p id="""">Reflective of the challenges and opportunities of launching, hiring and scaling during the pandemic, ReleaseHub will remain 100 percent remote and has added Artisanal Ventures as an investor to gain an edge in recruiting as they aggressively hire top talent across executive, product, engineering and go-to-market functions. Eschewing the reputation of tech companies that prioritize work over all else, ReleaseHub is leaning into remote work and purposefully advocating for team members to better weave their life, work and family together making it an appealing workplace.</p><h3 id="""">About CRV</h3><p id=""""><a href=""https://crv.com/"" id="""">CRV</a> is a venture capital firm that invests in early-stage enterprise and consumer startups. Since 1970, the firm has invested in more than 400 startups at their most crucial stages, including DoorDash, Airtable, Patreon, Drift and Iterable. Founders need more than capital to build a great company. It takes a partner who understands the entrepreneurial journey and knows what it takes to win. From founding to IPO and beyond, CRV is there every step of the way. Founders rely on CRV to be trusted, long-term, committed partners, which has helped make CRV into one of the longest-running venture capital firms in the world. Learn more about CRV and the companies shaping the future at <a href=""https://www.crv.com"" id="""">crv.com</a>.</p>",false,,,,https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/61f198feeca69261f71b2904_Release.png,Illustration of 3 software development environments created by Release,tommy-mcclung,3,Tue Oct 05 2021 13:00:00 GMT+0000 (Coordinated Universal Time),,
Release Hires New CMO and Adds Enterprise Class Capabilities ,releasehub-adds-enterprise-class-capabilities-with-remote-development-environments-datadog-integration-and-instant-data-sets,62aa5a70cd5ba27d9d0d718a,6352e34c55fdc56017e33b12,Fri Oct 21 2022 18:22:04 GMT+0000 (Coordinated Universal Time),Fri Jan 13 2023 15:19:18 GMT+0000 (Coordinated Universal Time),,"Release grows marketing team, hires former Docker VP of Marketing as CMO - and announces new features","<p id=""""><strong id="""">San Fransisco, CA </strong>—<strong id=""""> &nbsp;October 24, 2022</strong> — Release, the leading provider of Environments-as-a-Service (EaaS), today announced enterprise class technologies for remote development environments, Datadog integration, and instant data management to improve developer confidence and increase release velocity. The company also announced it hired Matt Carter, former Docker Vice President of Marketing, as CMO.</p><p id="""">‍</p><p id="""">“This is the era of Environments as a Service, which is driving fundamental changes throughout the application development process,” said Tommy McClung, Release CEO. “Our goal is to provide developers with the ability to write and commit code without penalty. With ReleaseHub, developers can spin up identical copies of environments as needed, within minutes.”</p><p id="""">‍</p><p id="""">One of the biggest bottlenecks in software delivery happens when developers are stuck waiting for access to environments. This results in significant delays in testing, debugging, and deploying software. EaaS mimics true production environments but are spun up and down on demand, so developers avoid development paralysis.</p><p id="""">‍</p><p id="""">EaaS signals a leap in release velocity and developer confidence. Similar to how containers let developers isolate software code, EaaS frees developers from fixed environments, letting them move quickly, at low cost, and without disrupting the workflow of their team.</p><p id="""">‍</p><p id="""">“We’re seeing greater EaaS uptake as developer teams recognize traditional environments are a major bottleneck,” said Matt Carter, Release CMO. “I’ve worked at Microsoft, Chef, and Docker, and have seen many technology transitions over the years. As we add more enterprise-class features, we’re seeing similar growth now, with greater adoption among a wider range of organizations.”</p><p id="""">‍</p><p id="""">Additionally, Release announced a number of new features that extend EaaS platform value to application development teams, including:</p><p id="""">‍</p><ul id=""""><li id=""""><strong id="""">Remote Development Environments</strong></li></ul><p id="""">Release is improving release velocity with new technology that lets developers build code locally while running it remotely within a customer’s cloud account. Remote development environments increase release velocity by letting developers use their full stack, while receiving immediate feedback from production or production-like data in their development environment. </p><p id="""">‍</p><ul id=""""><li id=""""><strong id="""">Datadog Integration</strong></li></ul><p id="""">ReleaseHub is introducing native support for Datadog, enabling developers to work from the leading cloud monitoring platform. Datadog observability gives developers Release insights from their Datadog accounts as well as single sign-on, role-based access control, and secrets management within ReleaseHub. These enterprise class features help organizations manage their DevOps pipelines at scale.</p><p id="""">‍</p><ul id=""""><li id=""""><strong id="""">Instant Data Sets</strong></li></ul><p id="""">Release is accelerating the replication of application data with Instant Data Sets, which lets developers create a replica of application data in minutes. Developers can build and test against the actual data their app uses. </p><p id="""">‍</p><p id=""""><strong id="""">About Release</strong></p><p id="""">Release delivers Environments-as-a-Service. It lets developers easily share progress with stakeholders when a full stack environment is created with every pull request and is shareable via custom URLs and directly in Slack. Every environment is a full instance of the app with all its services. ReleaseHub was funded by CRV, Sequoia, Y Combinator, Bow Capital, Artisanal Ventures, Hack VC, and other investors. More information is available at <a href=""http://www.release.com/"">www.release.com</a>. &nbsp;</p><p id="""">‍</p>",false,,,,https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/6356b716fd97a81d8cb67d6c_Image5%20(1).jpg,,,2,Mon Oct 24 2022 19:00:00 GMT+0000 (Coordinated Universal Time),,
Release Appoints Kelsey DeGeorge as Chief Revenue Officer,releasehub-appoints-kelsey-degeorge-as-chief-revenue-officer,62aa5a70cd5ba27d9d0d718a,62aa5a70cd5ba243c70d72fe,Wed Feb 23 2022 23:23:46 GMT+0000 (Coordinated Universal Time),Fri Jan 13 2023 15:19:46 GMT+0000 (Coordinated Universal Time),,Release appoints Kelsey DeGeorge as Chief Revenue Officer. DeGeorge previously managed ISV startup sales at AWS.,"<p id=""""><strong id="""">February 24, 2022</strong> – Release today announced that Kelsey DeGeorge has been appointed as Chief Revenue Officer. DeGeorge joins Release from Amazon Web Services, where she led a sales segment dedicated to B2B ISV companies, supporting them as consumers of AWS technology while forming joint GTM partnerships. DeGeorge will be responsible for scaling the GTM organization by accelerating Release adoption, elevating Release’s visibility through the channel, including cloud partner programs, and connecting with hundreds of thousands of developers on AWS who use conventional environments today. She will report directly to Release CEO and co-founder Tommy McClung.</p><p id="""">“Hiring Kelsey is a major milestone for Release,” said Tommy McClung, Release co-founder and CEO. “We immediately recognized her unmatched knowledge of the software vendor market and her deep insights into how companies can build GTM partnerships with cloud providers, an influential channel strategy in the industry. She is intimately familiar with the challenges ISVs face, and her skillset is the perfect match as we ramp up for the next phase of growth.”</p><p id="""">Release provides Environments-as-a-Service to software developers, easing a significant bottleneck in software production. Developers frequently wait for access to limited environments, costing the software industry $45 billion annually. Having access to Environments-as-a-Service empowers developers to create cloud native full-stack environments on-demand, which can be used for quality analysis, running production applications and delivering software to customers on virtual private clouds or public clouds like AWS.</p><p id="""">“I was introduced to Release during my time at AWS and recognized the opportunity that their novel Environments-as-a-Service platform provides,” said DeGeorge. “They’ve created a unique technology with significant growth potential. It’s a rare opportunity to join such an innovative company at the startup stage, and I’m thrilled to be part of the team.”</p><p id="""">Prior to joining Release and AWS, DeGeorge held gtm and engineering roles at PTC, Schlumberger, and RealID. She has a BS in Mechanical Engineering from University of Colorado, and is involved in the Society of Women Engineers (SWE) and the Lunar University Network for Astrophysics Research (LUNAR).</p>",false,,,,https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/61f198feeca69261f71b2904_Release.png,,tommy-mcclung,2,Thu Feb 24 2022 16:30:00 GMT+0000 (Coordinated Universal Time),,
Release your Ideas with Environments as a Service.  Fueled by a Seed Round from Sequoia.,releasehub-sequoia-funded,62aa5a70cd5ba27d9d0d718a,62aa5a70cd5ba2292e0d72fb,Thu Apr 29 2021 14:13:13 GMT+0000 (Coordinated Universal Time),Fri Apr 21 2023 20:12:56 GMT+0000 (Coordinated Universal Time),Fri Apr 21 2023 20:13:19 GMT+0000 (Coordinated Universal Time),Today we are super excited to announce our $2.7M Seed round led by Sequoia Capital.,"<p id="""">Today we are super excited to announce our $2.7M Seed round led by Sequoia Capital. You can read the details of the round in our <a href=""https://release.com/press-releases"">press release</a>.</p><p id=""""><a href=""https://release.com/blog/the-release-mission"" id="""">Our mission at Release</a> is to help great ideas get to the world quickly. Over the 20 years of our careers in technology we’ve seen the evolution of building software on physical servers that we racked and stacked ourselves to today’s world of building on the cloud. One thing in that timeframe has remained consistent: delivering software isn’t getting any easier.</p><p id="""">Before a software developer’s code can be released to the world it has to be deployed to an environment. Development, staging, and production environments are a critical step to releasing code. These highly complex platforms are built with dozens or hundreds of services, cloud platforms, and supporting technologies. They are so complex that standing up a new environment is a dedicated job for DevOps engineers, some of the most capable and in-demand technical talent in any organization. Environments thus become the biggest and most costly bottleneck in software development.</p><p id="""">There are more than <a href=""https://en.wikipedia.org/wiki/Software_engineering_demographics"" target=""_blank"" id="""">21 million</a> developers worldwide and they all use environments, but only one engineer at a time can deploy to an environment. Developers can wait days or weeks to deploy their code due to constraints on single environments. This leads to long queues, idle engineers, and delayed releases costing organizations tens of millions of dollars per year, something we estimate to be more than $45B per year to manage and maintain environments.</p><p id="""">With each <strong id="""">technical advancement</strong> we’ve seen the possibilities of what we can do with software expand. With these advancements, the complexity of our systems are also advancing. Modern applications are anything but simple. Software today is more complex than ever with all of the tools, clouds, services, and interconnectivity at our disposal.</p><p id="""">This <strong id="""">complexity</strong> means getting your ideas from your fingertips to users has become even harder than it was 20 years ago. While we aren’t fighting with racking and stacking hardware and freezing in a data center, we are now dealing with the ever growing complexity of our applications and our environments.</p><p id="""">There was a moment in the early 2000’s when the complexity was embedded within the operating system. Distributed services across the cloud has become our operating system and it feels like the early 2000’s again. Something is needed that <strong id="""">virtualizes</strong> this complexity much like the VM virtualized the operating system back then.</p><p id=""""><strong id="""">Environments</strong> are the manifestation of your application running within this complex technical ecosystem. All the great ideas delivered with software run in environments and we have big plans on how making environments easy to reproduce will get these ideas to the world faster and easier.</p><p id="""">Release can:</p><ul id=""""><li id="""">Help you <strong id="""">streamline development</strong> and remove bottlenecks in your development process with ephemeral environments, test and QA environments, as well as staging environments that can be created on-demand.</li><li id="""">Deliver an <strong id="""">amazing developer experience</strong> to get your code from your mind to production. We’ve been told this is a Heroku-like experience delivered on Kubernetes in your cloud.&nbsp;</li><li id="""">Deliver your <strong id="""">B2B application</strong> into your customers’ cloud VPC environments</li></ul><p id="""">This is just the beginning of what can be done when your environments can easily be reproduced, created and delivered on-demand.</p><p id="""">We’re proud to have <strong id="""">Sequoia</strong> as our partner in this journey. They’ve consistently been a part of the biggest ideas and greatest companies that have ever been built. We’re especially excited to be working with <strong id="""">Bogomil Balkansky</strong> who spent many years at VMWare thinking about how to deliver simplicity to developers and companies.</p><p id="""">I also want to thank our team for all of their hard work since we founded the company. They have made this possible and their energy, enthusiasm, and dedication has made building this company an absolute joy. I know we’re at the starting line but sometimes even getting to the starting line is a journey and this team is the best group of people I’ve ever had the pleasure of working with. If you’re interested in helping us on our mission, we always post new roles here: <a href=""https://release.com/company"" id="""">https://release.com/company</a>.</p><p id="""">If you’re interested in seeing how Release Environments as a Service (EaaS) can help you and your team get your ideas to the world faster, drop us a line at <a href=""mailto:hello@release.com"">hello@release.com</a> and we’d love to show you what the future of software development looks like.</p>",false,,,,https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/6442ee4148b391f49a57164a_042921%201-%20Sequoia%20(1).jpg,Sequoia logo representing the $2.7M Seed round led by Sequoia Capital,tommy-mcclung,2,Thu Apr 29 2021 14:00:00 GMT+0000 (Coordinated Universal Time),,
Remote Development Environments,remote-development-environments,62aa5a70cd5ba27d9d0d718a,63530a2ee38d38f5af6bf8b1,Fri Oct 21 2022 21:07:58 GMT+0000 (Coordinated Universal Time),Mon Aug 28 2023 18:39:54 GMT+0000 (Coordinated Universal Time),Mon Aug 28 2023 18:39:54 GMT+0000 (Coordinated Universal Time),Use Remote Development Environments to get instant feedback using production like data as early as the development stage,"<h2 id="""">Introducing Remote Development Environments</h2><p id="""">Kubernetes is the best way to manage and orchestrate modern, complex container-based applications in the cloud. &nbsp;</p><p id="""">While this is great for production environments it poses serious challenges for developers and DevOps practitioners. The differences between development and production often result in the utterance of the most dreaded words in application development, “But it works on my machine!” at the most inopportune times, specifically when there is an outage in production. &nbsp;</p><p id="""">There are a few ways to mitigate this issue, but the only “<a href=""https://www.youtube.com/watch?v=rYldJY3t9zk"" target=""_blank"" id="""">win-win-win</a>” solution is a development environment that replicates production. Historically, this was often too difficult, time-consuming, and expensive, but Release has bridged the gap to provide you with a single pane of glass for all categories of environments you might need.</p><p id="""">You may already know that Release can create ephemeral environments on pull requests and run your staging and production environments. But did you know that Release can also install your SaaS-based offerings in your customer’s VPCs, all from the same UI/CLI?&nbsp;</p><p id="""">With this new feature, we have added production-like, cloud-based development environments to our industry-leading offerings. Your developer teams now get all the benefits of a traditional cloud-based development environment:</p><ul id=""""><li id="""">Write code on your machine (thin client, laptop, small supercomputer under your desk), but run it remotely with production mirroring hardware, software and configuration.</li><li id="""">Total control over the configuration of remote development environments: You can build remote development environments to be as much like production as you need.</li><li id="""">Bring your own tools. &nbsp;Release doesn’t force you to use a web-based IDE or change anything about your development, build, or deployment process.</li></ul><p id="""">While other solutions may give you these same benefits, Release has always maintained a focus on merging development, staging, and production, meaning we can offer further benefits that our competitors simply can’t:</p><ul id=""""><li id="""">Instant datasets so that you can develop using production or production-like data. &nbsp;You will no longer need to worry anymore that your seed data and production data have diverged. Test all of your migrations during development on real data before running them in production. &nbsp;Customer-facing bugs that only appear on production can now be debugged and fixed in record time!</li><li id="""">Model the most complicated applications and develop against multiple services at once, even if each service needs its own data store and custom configuration.</li></ul><p id="""">With Release, you have the best of all worlds and can finally feel confident that “works on my machine” will be a thing of the past. </p><p id="""">Let’s take a look at how you can get started with Release remote development environments.</p><h3 id="""">Getting Started</h3><p id="""">Starting remote development with Release consists of two steps:</p><ul id=""""><li id="""">Add configuration to a particular environment or Application Template.</li><li id="""">Use the Release <a href=""https://docs.releasehub.com/cli/getting-started"" target=""_blank"" id="""">CLI</a> to set up the environment in development mode.</li></ul><p id="""">Each step takes less than a minute to set up and you will be happily remote developing in record time!</p><p id="""">Watch a <a href=""https://vimeo.com/manage/videos/762779889/privacy"" target=""_blank"" id="""">video showing the set-up steps</a> in detail with an example or read our <a href=""https://gist.github.com/NickBusey/a2107705c3c524283ae725c0ca44f4b0"" target=""_blank"" id="""">how-to-guide</a> for more details.</p><h3 id="""">Configuration</h3><p id="""">To enable remote development on an ephemeral environment you will need to add a new high-level stanza to your Application Template or Environment Configuration.</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yaml"">
development_environment:
  services:
   - name: api #references service from config
     command: bundle exec rails s -b 0.0.0.0 #optional, can use default command if the same
     sync: #block to tell Release which paths to sync from local to remote
     - local_path: "".""  #path on your local machine to sync to...
       remote_path: ""/app"" #this remote path in the cloud
     port_forwards: #port mapping from local to remote, localhost:3010 now points to remote end-point
     - remote_port: 3000
       local_port: 3010
   - name: sidekiq #develop against multiple services
     command: bundle exec sidekiq
     sync:
     - local_path: "".""
       remote_path: ""/app""
</code>
</pre></div><p id=""""> &nbsp;<em id="""">Example of multi-service remote development</em> </p><p id="""">This example allows you to develop against two different services, in the cloud, while exposing ports for the API<strong id=""""> </strong>service but not the <em id="""">sidekiq</em> service. Once you save this configuration, you can use the CLI to activate development mode for this particular environment. You can view the full documentation <a href=""https://docs.releasehub.com/reference-documentation/application-settings/application-template/schema-definition#development-environments"" target=""_blank"" id="""">here</a>.</p><p id="""">The above syntax works per environment or at the application level and affects all ephemeral environments.</p><h3 id="""">CLI</h3><p id="""">Next, use the CLI to set up a bidirectional sync between your local machine and the remote environment. Release remote development environments use a sync process to mirror changes locally with your development environment. This allows you to work locally should you have connectivity issues, unlike developing directly on a remote container in the cloud.</p><p id="""">From the directory with the source code for the particular environment, run a single command using the Release CLI:</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yaml"">
release envs dev –app backend –environment env_id
</code>
</pre></div><p id=""""><em id="""">‘env_id’ is the ID of the environment</em></p><p id="""">The CLI will:</p><ul id=""""><li id="""">Start a deployment and set the environment to development mode using your configuration.</li><li id="""">Activate the bidirectional sync service.</li><li id="""">Set up the port forwards.</li><li id="""">Launch the command you defined locally via SSH. This allows you to interact with your container as if it was local. You can run commands on the container and send signals (such as ctrl-c) as if it was a local service.</li></ul><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1280px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1280px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/6356ef2da85a5fdef07f5a89_rHHMgdBxK-XC0exAriz_dNbD8cS1s4zXZ1NeFvYxb5DHSMgqhNMxBSRmTdDuDnbrQWz8j4xmpNzslaLI_VpoWotuLzzsH_2yEiro2j2bPAxxG66U2B6B3w57EHdkIBNeP_IcpiAPudl6yVzdCZbpDTYstgaAevt6bTNj5YmzXtrTqs0ca98w-uo7Bw.gif"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><p id="""">That’s it! You are now ready to start making changes locally that will be synced to your remote containers.</p><h3 id="""">Conclusion</h3><p id="""">The addition of Remote Development Environments to our services makes Release the most complete EaaS (Environments as a Service) platform available. Your entire development life cycle can now be implemented using Release environments, from development to production. </p><p id="""">The adoption of new technologies, like k8s, can result in massive changes or regressions in our development process. Release mitigates these trade-offs by offering a complete solution in one platform, including features like instant datasets and developing against multiple services that just aren't possible with other solutions.</p><p id="""">Not quite ready to do it on your own? <a href=""https://calendly.com/d/d35-s6p-4nf/releasehub-full-stack-environment-automation?month=2022-10"" id="""">Set up time </a>with our team to walk you through.</p>",false,,,,https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63e42033f62bddf2c3b8a995_102522%20(1).jpg,,erik-landerholm,4,Tue Oct 25 2022 17:00:00 GMT+0000 (Coordinated Universal Time),,
Rules for putting together the article,rules-for-putting-together-the-article,62aa5a70cd5ba27d9d0d718a,642b09ee8da2d25134124232,Mon Apr 03 2023 17:16:30 GMT+0000 (Coordinated Universal Time),Thu Apr 20 2023 20:42:35 GMT+0000 (Coordinated Universal Time),,,"<h2 id="""">1 - The article should ALWAYS start with a paragraph or a heading H2</h2><p id="""">Otherwise the body of the post will be misaligned with the social block</p><h3 id="""">2 - The H1 should never be used, because this tag is reserved for the blog title</h3><p id="""">It's bad for SEO having two or more H1 in the same page</p><h3 id="""">3 - It's not needed to add bold to the for the headings, because the weight is already set for the headings.</h3><h4 id="""">Otherwise the headings will look like this: <strong id="""">Heading bold</strong></h4><h3 id="""">4 - It's not needed to add extra space between the paragraphs</h3><p id="""">It is not necessary to use space between paragraphs as they already have a defined margin.</p><p id="""">Adding extra spaces will leave your text with excess space, as we can see below.<br>‍</p><p id="""">Can you see? I told you</p><h3 id="""">5 - When working with the block codes it's important to pay attention to spacing</h3><div data-rt-embed-type='true'><pre class=""language-yaml line-numbers""><code class=""language-yaml"">  readiness_probe:
    exec:
      command:
      - curl
      - ""-Lf""
      - http://localhost
    failure_threshold: 5
    period_seconds: 30
    timeout_seconds: 3
</code>
</pre></div><p id=""""><em id="""">If you add a description for the blog, make it italic.</em></p><h3 id="""">6 - When you add an image there is no need to add empty lines above and/or below the image, the image itself is already spacing defined. </h3><p id="""">Here's how you won't need to add a line between the paragraph and the image:</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:776px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""776px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/642b0e17db05cfe83a627581_Content.png"" loading=""lazy"" id="""" width=""auto"" height=""auto""></div><figcaption id="""">Image description</figcaption></figure><p id="""">It will be perfect even if you don't have a description for the image, don't worry.</p><p id="""">But see below that if you have an empty line it will create non-harmonious spacing between the paragraph and the image</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-center"" data-rt-type=""image"" data-rt-align=""center""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/642b0e17db05cfe83a627581_Content.png"" loading=""lazy"" id="""" width=""auto"" height=""auto""></div></figure><p id="""">‍</p><h3 id="""">7 - Take care to always end your article without empty lines at the end, as this will generate empty space at the end of the article.</h3><h3 id="""">8 - The proper way to write inline code block:</h3><p id=""""> &lt;code inline&gt;release instances terminal&lt;/code&gt;</p>",false,,,,,,,5,Fri Apr 03 2020 17:23:00 GMT+0000 (Coordinated Universal Time),,
Secure Secrets Management with Doppler in Release: A Step-by-Step Guide,secure-secrets-management-with-doppler-in-release-a-step-by-step-guide,62aa5a70cd5ba27d9d0d718a,6723f9c97a69968108e5d497,Thu Oct 31 2024 21:42:33 GMT+0000 (Coordinated Universal Time),Thu Oct 31 2024 22:03:54 GMT+0000 (Coordinated Universal Time),Thu Oct 31 2024 22:03:59 GMT+0000 (Coordinated Universal Time),"Learn to securely manage secrets in Kubernetes by integrating Doppler with Release, ensuring streamlined and safe deplo",,true,"<p id="""">See how easy it is to manage secrets with Doppler and Release. </p>",https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=doppler-release,"<p id="""">Managing secrets securely is essential for any modern application infrastructure, and integrating <a href=""https://www.doppler.com/"">Doppler Secrets Manager</a> with Release can streamline this process. This guide covers how to set up and configure Doppler to work seamlessly with your Release environments, providing a reliable and scalable way to manage secrets across various services and jobs.</p><h3 id=""""><strong id="""">What is Doppler Secrets Manager?</strong></h3><p id="""">Doppler Secrets Manager provides a secure, centralized way to manage and inject sensitive information, such as API keys and database credentials, into your applications. Through the integration of Doppler and Release, you can ensure that these secrets are securely synchronized with your Kubernetes clusters in Release environments.</p><h3 id=""""><strong id="""">Prerequisites</strong></h3><p id="""">Before you dive into the setup, make sure you have the following ready:</p><ul id=""""><li id=""""><strong id="""">Access to a Release environment</strong> with configured Kubernetes clusters.</li><li id=""""><strong id="""">Release CLI</strong> installed and configured on your local machine.</li><li id=""""><strong id="""">Doppler account</strong> with generated service tokens that have the necessary permissions.</li></ul><h3 id=""""><strong id="""">Integrating Doppler with Release</strong></h3><p id="""">Let's walk through the steps to securely manage secrets using Doppler in your Release environment.</p><h4 id=""""><strong id="""">Step 1: Set Up Kubeconfig for Your Release Cluster</strong></h4><p id="""">First, you'll need to configure your kubeconfig to access the Release cluster. This will allow kubectl to interact with your Release environment directly.</p><p id="""">Run the following commands to generate and configure kubeconfig:</p><p id=""""><strong id="""">bash</strong></p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-bash"">
release clusters kubeconfig --account Release --cluster release-development ./export KUBECONFIG=./config-release-development.yaml
</code>
</pre></div><p id="""">With this, the kubeconfig for your Release cluster is now set as the current context for kubectl.</p><h4 id=""""><strong id="""">Step 2: Install the Doppler Kubernetes Operator</strong></h4><p id="""">The Doppler Kubernetes Operator is responsible for syncing secrets from Doppler to your Kubernetes environment. To install it, start by adding the Doppler Helm repository and installing the operator:</p><p id=""""><strong id="""">bash</strong></p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-bash"">
helm repo add doppler https://helm.doppler.comhelm install --generate-name doppler/doppler-kubernetes-operator
</code>
</pre></div><p id="""">This setup deploys the Doppler Kubernetes Operator, ready to securely synchronize secrets.</p><h4 id=""""><strong id="""">Step 3: Create a Doppler Token Secret in Kubernetes</strong></h4><p id="""">To allow the Doppler Operator access to your secrets, you’ll need to create a Kubernetes secret with your Doppler service token. Replace YOUR_DOPPLER_SERVICE_TOKEN with the actual token from your Doppler account.</p><p id=""""><strong id="""">bash</strong></p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-bash"">
kubectl create secret generic doppler-token-secret \  --namespace doppler-operator-system \  --from-literal=serviceToken=YOUR_DOPPLER_SERVICE_TOKEN
</code>
</pre></div><p id="""">With this token in place, the Doppler Operator can access and synchronize secrets into your Kubernetes environment.</p><h4 id=""""><strong id="""">Step 4: Configure Your Release Application to Use Doppler Secrets</strong></h4><p id="""">Now, configure your application in Release to use these Doppler-managed secrets. Begin by defining the secrets you need in Doppler, associating each set with a specific Doppler project and configuration. In Release, link these secrets to your services using the secrets_from field within the service configuration. This enables each service to access only the secrets it needs, ensuring secure, targeted access.</p><p id="""">For example, you can define separate secret configurations for a Rails and an AI project:</p><p id=""""><strong id="""">yaml</strong></p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yaml"">
secrets:
  - name: development
    type: doppler
    project: rails
    config: dev
  - name: development-ai
    type: doppler
    project: ai
    config: dev
</code>
</pre></div><p id=""""><code id="""">‍</code>‍</p><p id="""">Next, associate these secrets with the respective services in Release:</p><p id=""""><strong id="""">yaml</strong></p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yaml"">
services:
  - name: rails
    image: github-org/rails
    secrets_from:
      - development
  - name: ai-chatbot
    image: github-org/ai-chatbot
    secrets_from:
      - development-ai
jobs:
  - name: chatbot-setup
    image: github-org/rails
    secrets_from:
      - development
      - development-ai
    steps:
      - run: bundle exec rake chatbot:setup
</code>
</pre></div><p id="""">In this setup:</p><ul id=""""><li id="""">The <strong id="""">Rails service</strong> pulls the development secrets from the Rails project in Doppler.</li><li id="""">The <strong id="""">AI chatbot service</strong> accesses the development-ai secrets from the AI project in Doppler.</li></ul><p id="""">This configuration keeps secrets streamlined and service-specific, enhancing security and simplifying secret management across your environments.</p><h3 id=""""><strong id="""">Troubleshooting Doppler Secrets Synchronization Issues</strong></h3><p id="""">If you encounter issues accessing secrets, you can view the Doppler operator logs to diagnose and resolve synchronization issues:</p><p id=""""><strong id="""">bash</strong></p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-bash"">
kubectl logs -f deployment/doppler-operator-controller-manager -n doppler-operator-system
</code>
</pre></div><p id="""">This command lets you track the Doppler operator's logs for any potential issues. Common errors include incorrect service account permissions, invalid service tokens, or misconfigured Doppler projects and configurations.</p><h2 id="""">Conclusion</h2><p id="""">Integrating Doppler with Release provides a robust, scalable solution for managing secrets in Kubernetes. By following these steps, you can securely manage secrets across multiple services, enhancing security and maintaining control over sensitive data in your Release environments. With Doppler Secrets Manager, Release environments become even more secure and manageable, empowering development teams to focus on building rather than managing configurations.</p><p id="""">‍</p>",https://cdn.prod.website-files.com/603dd147c5b0a4221d1bd360/6723f9095d729bde2322a895_doppler-release-guide.webp,Release Doppler Integration,david-giffin,10,Thu Oct 31 2024 21:15:00 GMT+0000 (Coordinated Universal Time),product,kubernetes-secrets-management-a-practical-guide; how-to-manage-gitops-secrets-a-detailed-guide; beyond-k8s-introduction-to-ephemeral-environments
See you at Google Cloud Next '24!,see-you-at-google-cloud-next-24,62aa5a70cd5ba27d9d0d718a,660af23c95130937e289576c,Mon Apr 01 2024 17:43:24 GMT+0000 (Coordinated Universal Time),Thu Apr 25 2024 20:22:37 GMT+0000 (Coordinated Universal Time),Thu May 02 2024 20:19:24 GMT+0000 (Coordinated Universal Time),Release is heading to Las Vegas for Google Cloud Next '24 (April 9-11).,"<p id="""">Release is heading to Las Vegas for <a href=""https://cloud.withgoogle.com/next"" id=""""><strong id="""">Google Cloud Next '24</strong></a><strong id=""""> </strong>(April 9-11). We’re looking forward to sharing product updates, meeting with the community and seeing how Google will shape the future of cloud technology. From artificial intelligence applications to infrastructure solutions, Google Cloud Next '24 represents a new chapter in digital transformation.</p><p id="""">Stop by the Release booth (#1202), where we'll be showcasing our latest products and advancements, including our <a href=""http://release.ai"" id="""">early access program for Release.ai</a>.</p><h2 id=""""><strong id="""">What to Expect at Google Cloud Next '24</strong></h2><p id="""">With over 400 sessions, Google Cloud Next, attendees can expect valuable insights, interactive sessions, and networking opportunities. We’re looking forward to sessions focused on:</p><ul id=""""><li>AI and ML</li><li>Application Developers</li><li>Infrastructure Architecture and IT Operators</li><li>Data Analysts, Data Scientists, Data Engineers</li><li>Database Professionals</li><li>DevOps, ITOps, Platform Engineers, SREs</li><li>IT Managers and Business Leaders</li><li>Productivity and Collaboration</li><li>Security Professionals</li></ul><h2 id=""""><strong id="""">Who Should Attend Google Cloud Next ‘24</strong></h2><p id="""">Google Cloud Next is tailored to cater to a diverse range of individuals, including:</p><ul id=""""><li>IT professionals and managers seeking to enhance their knowledge and skills.</li><li>Software developers and architects aiming to stay updated with the latest advancements.</li><li>Cloud engineers and consultants dedicated to optimizing cloud infrastructure.</li><li>Business leaders who aspire to harness the power of cloud technologies for both growth and innovation.</li></ul><h2 id=""""><strong id="""">Release at Google Cloud Next '24</strong></h2><p>Google Cloud Next '24 is an important event in the industry. It's a chance to share knowledge, discover new ideas, and connect with others. If you're at the event, head over to <strong id="""">Release (booth #1202)</strong> where you can learn more about our products, and sign up for our <a href=""http://release.ai"" id="""">early access program for Release.ai</a>.</p>",false,,,,https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/660af1ca1fe53113ef5c3709_GCN23_GE_BlogHeader_2436x1200_3.width-1200.format-webp.webp,Google Cloud Next 24,annika-major,3,Mon Apr 01 2024 17:41:00 GMT+0000 (Coordinated Universal Time),events,
How to Setup Test Environments That Are Easy to Maintain,setup-test-environment,62aa5a70cd5ba27d9d0d718a,62aa5a70cd5ba2747e0d7305,Tue Feb 15 2022 22:04:30 GMT+0000 (Coordinated Universal Time),Fri Apr 21 2023 19:12:33 GMT+0000 (Coordinated Universal Time),Fri Apr 21 2023 19:37:52 GMT+0000 (Coordinated Universal Time),"How to set up test environment | Data, Scope, environment lifetime, and what is the focus and goal at each stage.","<p id="""">Setting up test environments is an integral step in ensuring smooth-running and bug-free applications post deployment. But once they're live, test environments also require the same TLC you invest into production servers. With this comes that double-effort dread. Should developers be spending time managing environments instead of the applications?</p><p id="""">This post explains how your engineers can set up test environments that don't demoralize developers with huge effort requirements. We'll review the various test environment options at your disposal, along with a few common challenges you might encounter during the setup phase. We'll then close with actionable solutions to the pain points around test environments.</p><p id="""">Before we discuss strategies, let's investigate why any of this is necessary.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1361px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1361px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/620c226be72faec388566aa5_how%20to%20set%20up%2001.png"" loading=""lazy"" width=""auto"" height=""auto"" id=""""></div></figure><h3 id="""">Why You Need Preproduction Testing Environments</h3><p id="""">Let's cut to the chase. If you don't invest time and effort into test environments, you'd almost certainly spend resources correcting faulty performance issues in production. Not to take anything away from your development skills, but testing itself is one skill crucial to the success of any software project. The best way to execute tests is in test environments—replicas of the production environment, accessible only to engineers and testers.</p><p id="""">Skipping the creation of test environments, or running badly configured ones, can counter the benefits of testing. Whichever stage (and associated environments) your team runs tests at must represent the production environment closely. This cancels out any environment parameter-based bugs later on.</p><h4 id="""">Common Testing Environments</h4><p id="""">Testing saves your team a lot of time (and face). As such, it's worth investing in the most suitable environments for your applications. Not all testing environments are the same. In fact, testing itself takes place at different stages along an application's metamorphosis. Three such milestones, each sparking an environment, commonly appear regardless of your development framework.</p><ul id=""""><li id=""""><strong id="""">Development:</strong> Tests run alongside new feature additions. Since this is where the main copy (branch) of an application resides, setting up is near effortless and cost the least to manage.</li><li id=""""><strong id="""">Testing:</strong> This environment is created after development as a separate environment for engineers to put applications under various tests. This can be a minimum resource allocated container. It's a discovery phase that might itself be replicated to try out different testing frameworks/applications.</li><li id=""""><strong id="""">Staging:</strong> Staging is perfect for post-development demonstrations of the application to a few users, but not the entire user base. Usually an opportunity for the app' investors/owners to check if the product addresses their business case.</li></ul><p id="""">Some QAs will include the production environment as another test opportunity. Yes, continuous tests have become the norm, especially with teams practicing CI/CD principles. Being continuous implies an application is always being worked on. Consequently, changes made due to test results after deployment just mean we're in the development environment, after all.</p><h4 id="""">Challenges in Setting Up Test Environments</h4><p id="""">Even with the knowledge of which environment to create, how well your testing turns out depends on <strong id="""">how</strong> you create and manage the environment. Without proper management of test environments, the following burdens can plague your team:</p><ul id=""""><li id=""""><strong id="""">Brittle smoke tests:</strong> This leads to loss of confidence in a project's feasibility. In practice, you'll only find out that your environment caused test failures, and not your application. However, you'll have already wasted valuable time and possibly drowned team morale.</li><li id=""""><strong id="""">High cost of data synchronization with production:</strong> Running tests on live data (or subsets of it) requires proper planning and infrastructure setup. Unpleasant loss of data, or corruption of live instances, can ruin tests altogether.</li><li id=""""><strong id="""">Inconsistent test data across teams:</strong> All members of a development team should be up to speed with the most up-to-date test data. Inconsistencies that arise due to badly configured test environments can result in teams wasting efforts on otherwise resolved issues.</li></ul><p id="""">All these problem cases cause setup costs to accumulate. Unmanaged, a team might use the budget on unnecessary or duplicate resources.</p><h4 id="""">Traditional Costs of Provisioning and Changing a Test Environment</h4><p id="""">Typically, test environments will have a cost budget close to that of a live environment. If you already have a live version of the application, you'll be wise to use it as a threshold. You can then provide containers and resources that let teams run tests without ""choking"" the application.</p><p id="""">Costs can pile as your team creates new and persisting test environments. This brings the issue of environment lifecycle into focus. If your service provider charges on a resource consumption model, you're best alerting teams of the need to tear environments down after tests. This significantly optimizes your testing routine. Having templates of the various test environments your team requires is another way of making sure they don't end up provisioning too many resources.</p><p id="""">With a versioning platform hosting application code, moving your application from one environment to the next becomes cheaper than manually copying and pasting code. Here you'll save on storage and reduce the range of errors that befall blocks of code as they move from place to place.</p><h3 id="""">How to Run a Tight Ship Around Test Environments</h3><p id="""">You'll have to approach every step of setting up and maintaining test environments with deliberate strategy. Also, it's important not to focus on cost-saving entirely as you'll likely negotiate on the performance and range of possible tests. Balancing these out requires listing out the tests you'll run and the tools for which each environment will require.</p><p id="""">Doing the above, along with manually checking for any unnecessary leaks from redundant environments, will surely take all of your engineers' time and effort. You might need to hire someone to focus on just that, but this would just pile costs on your testing process. A better route would use tools to manage test environments.</p><p id=""""><a href=""https://release.com/"">Release </a>is a good example of tools created to manage and lessen test environment costs. This includes creating on-demand test environments with an Environment as a Service approach for instant deployment and execution of tests. The platform also enables sharable test cases, keeping teams at par with the latest test results across all created environments.</p><p id="""">Perhaps the least-discussed issue around testing is how scalable every environment you create is. As an application grows, so too should the environments in which tests are run. Test environment tools that provide a central configuration spot to scale the provision of test spaces become crucial to successful tests.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1360px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1360px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/620c23127ba62511e78922c6_how%20to%20set%20up%2002.png"" loading=""lazy"" width=""auto"" height=""auto"" id=""""></div></figure><h3 id="""">Last Take: Testing Done Right</h3><p id="""">Knowing what the focus and goal are at each stage of the test environments' creation makes it much easier to manage them. This allows for proper planning and ample resource allocation for every test case you deem necessary to test your applications.</p><p id="""">With a plan in hand, the task to set up test environments is best left to the mandate of the right tools. Regardless of the environments you need, creating them should be an easy process. On-demand<a href=""https://release.com/staging-environments""> staging environments</a> for as many test cases as necessary will resolve most of the pain points we discussed above.</p><p id="""">Tools also immediately make managing them less stressful compared to manually managing them. They also automate and standardize how you set up test environment variables and share any results thereof. Combine such a tool with test automation frameworks and your developers get to focus on the applications, not the environment.</p><p id=""""><em id="""">This post was written by Taurai Mutimutema. </em><a href=""https://twitter.com/rusiqe"" target=""_blank""><em id="""">Taurai </em></a><em id="""">is a systems analyst with a knack for writing, which was probably sparked by the need to document technical processes during code and implementation sessions. He enjoys learning new technology and talks about tech even more than he writes.</em></p>",false,,,,https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63e401112b96ac289678a676_051722%20(1).jpg,How to Setup Test Environments That Are Easy to Maintain,taurai-mutimutema,6,Tue May 17 2022 23:47:00 GMT+0000 (Coordinated Universal Time),,
Solving for Dynamic OAuth 2.0 Callbacks with Environment Handles,solving-for-dynamic-oauth-2-0-callbacks-with-environment-handles,62aa5a70cd5ba27d9d0d718a,62d7feccec7500d565d2e627,Wed Jul 20 2022 13:10:36 GMT+0000 (Coordinated Universal Time),Fri Apr 21 2023 18:39:25 GMT+0000 (Coordinated Universal Time),Fri Apr 21 2023 18:46:03 GMT+0000 (Coordinated Universal Time),"In this blog post, we will discuss the concept of Environment Handles and how they can be used to support dynamic OAuth ","<h3 id="""">Ephemeral Environments and Dynamic Hostnames</h3><p id="""">When creating an ephemeral, on-demand application environment with your environment platform, you may have one or more services, such as a frontend web application or API server, that require internet ingress. </p><p id="""">While these endpoints can be hard-coded into your environment configuration, it’s beneficial to avoid this when possible and instead use a random ID as part of the hostname so that your developers can freely spin up multiple environments from the same definition, if needed.</p><p id="""">For example, cloud environments created with <a href=""https://release.com/"" id="""">Release</a> will by default generate a random environment ID, <strong id="""">${ENV_ID}</strong>, &nbsp;and let you use that value anywhere you want within the hostname of an ephemeral ingress endpoint, such as https://frontend-<strong id="""">${ENV_ID}</strong>.example.com.</p><h4 id="""">OAuth 2.0 and the Authorization Grant Flow</h4><p id=""""><a href=""https://oauth.net/2/"" target=""_blank"" id="""">OAuth 2.0</a> (“OAuth”) is a common standard that allows a user to authorize one application to access their resources hosted within another system. &nbsp;The OAuth standard provides several different methods, referred to as OAuth flows, for which delegated access can be provisioned. While an in-depth discussion of OAuth and the various flows is outside the scope of this blog, we will define the <strong id="""">authorization grant flow</strong> and refer back to it later as follows: </p><p id=""""><em id="""">The authorization grant flow </em>is an OAuth 2.0 flow that allows a user of your <strong id="""">client application</strong> to provide your application with delegated permission to access the user’s resources hosted within a 3rd-party system’s <strong id="""">resource server</strong>. In practice, your client application will redirect the user to the 3rd party’s <strong id="""">authorization server</strong> to authenticate the user and, once authenticated, confirm with the user whether they want to grant your client application one or more permissions (<strong id="""">scopes</strong>) to their resources. If the user approves the access, the authorization server will redirect the user to a predefined <strong id="""">callback URL </strong>(aka redirect URL or sign-in redirect) while providing an <strong id="""">authorization grant</strong> to the client application. The client application will then exchange the grant for an <strong id="""">access token </strong>with the authorization server which ultimately allows the client application to access the user’s resources on the resource server. The authorization and resource servers are often part of the same application and domain, but they don’t have to be. </p><p id="""">As a practical example, if you’ve ever granted permission for an application to interact with your GitHub account or repositories, as I’m doing with GitKraken in the example below, you were almost certainly using OAuth 2.0 under the hood:</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1006px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1006px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/62d7fdc72cd8872f8f3152b1_WKh_WNqJ3RXZCfHlp3VoFj_zljIsZFgOuxDE4Swo6Hn8mPkwFZYYQrd3XGhVIzQlSFcx2IS16_TUDxRbrZKasNm9ZOzxLBp7mWgHOHSZlr2vbhVUTD-kzbk3IxOxwO_sJV5ZENZzdDBDpShmKgPktOQ.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><p id="""">When using OAuth 2.0, a client application must be configured within the 3rd-party Authorization server in advance before the client can actually initiate an OAuth flow. The specific configuration varies based on the third-parties implementation of the protocol, but at a minimum, the authorization code flow requires that (1) the client application redirect URL(s) are defined in advance and (2) when first initiating the flow, the client application includes a pre-approved redirect URL as a query parameter. </p><h4 id="""">The Challenge</h4><p id="""">We now know that: <br></p><ol id=""""><li id="""">The DNS endpoint (and thus callback URL) of an ephemeral client application environment is typically not known prior to creating the environment<br></li><li id="""">The OAuth 2.0 authorization grant flow that you explicitly define the callback URL(s) <em id="""">prior </em>to actually initiating an actual authorization flow</li></ol><p id="""">‍<strong id="""">This poses an interesting challenge:</strong> how can you use OAuth 2.0 flows that require pre-defined, static callback URLs when the DNS endpoint of your client application and callback URL is dynamic?</p><h4 id="""">Environment Handles</h4><p id="""">Instead of substituting random environment IDs into the hostnames of your ephemeral environments, you can instead create a pool of predefined environment IDs, aka <strong id="""">Environment Handles</strong>. These could be &nbsp;a simple set of numbers like 1 through 10, a subset of your favorite superhero names, or anything else. </p><p id="""">With this approach, when your platform creates an environment it would randomly select one of the available handles, assign it to the newly-created environment, and mark it as in-use. The handle would remain unavailable until your environment is terminated and the handle is once again marked as available.</p><p id="""">For example, say you know that you will have at most five concurrent environments running at any time and you therefore create a list of handles, one through five. Continuing with our previous example, this means that the set of possible application URLs would be: </p><ul id=""""><li id="""">https://frontend-<strong id="""">1</strong>.example.com</li><li id="""">https://frontend-<strong id="""">2</strong>.example.com</li><li id="""">https://frontend-<strong id="""">3</strong>.example.com</li><li id="""">https://frontend-<strong id="""">4</strong>.example.com</li><li id="""">https://frontend-<strong id="""">5</strong>.example.com</li></ul><p id="""">Even though you don’t know which environment will get which handle and when, your hostnames are now deterministic because they must be one of the five values above. This ultimately means that you now have a reliable list of callback URLs that you can configure with your 3rd-party OAuth provider and successfully use your ephemeral environments with OAuth 2.0. </p><p id="""">As an example, we use Release to deploy demo application environments that rely on OAuth to sign in and access third-party resources in source control providers like GitHub, and we use a pool of our favorite superhero names as environment handles as shown below:<br></p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/62d7fdc7f42d0c505d02b75a_87QBBFcN1Hx3HgYnxAuMonB-tpCGCVqRJq3VysaKXy1VSe5nIk9-QZG3mAIK_yPbsa5Sq7PXQLBxKV6S0spWOmQrJ-F5GsykhTycY3jrGwR8CSKJblo8b6kui1f-PJCJjVFlMCUW3QA0T0esmXISB5U.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><p id="""">Above, we can see that one of our 23 available handles, <em id="""">shazam</em>, is assigned to an active environment to test some frontend notification modals. When we navigate to the frontend URL assigned to that environment, we can see that the <em id="""">shazam </em>handle is used to form the ephemeral frontend hostname:</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/62d7fdc77de9822f328cad95_goNxf4kL4DRxjds00cHHEvOLuyyHU7ZrJgbf9WW-Km2rfGc3W-judQ27LnNEm2MgoFi8l4WWtKHokRflzxoTVx0Abyha33poqw-y_aIT6UHUbvDKXRnpPg2-RKfReW4wD2Q1ww2EEwzSSiVS9neFIGc.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><p id="""">The <em id="""">https://frontend-shazam-xxxxxx </em>URL above, along with the other superhero variations from our environment handle pool, has been pre-configured as a callback URL within Release’s OAuth client settings in GitHub, BitBucket, and GitLab so that we can test <a href=""https://openid.net/connect/"" target=""_blank"" id="""">OpenID Connect</a> (OIDC), an extension of the OAuth 2.0 protocol, to let users log in with their third-party credentials. </p><h3 id="""">Considerations when using Environment Handles</h3><h4 id="""">Environment Handle Pool Size</h4><p id="""">The size of your environment handle pool, and thus the number of URLs you need to allow-list in your OAuth client app configuration, is determined based on the number of concurrent ephemeral environments you expect you’ll need plus some safety margin. For example, if your team will likely have three environments running at any one time, you might configure five handles just to be safe.</p><h4 id="""">Individual vs. Shared OAuth 2.0 Client per Handle</h4><p id="""">When setting up the third-party OAuth clients for your environment handles, you’ll need to determine whether you use a single client configuration for all of your environment handles or whether you set up a separate client per handle. </p><p id="""">A single client configuration with multiple callback URLs (one for each handle/endpoint) means that you would have a single Client ID and Client Secret to keep track of and results in a simpler configuration. However, whether the OAuth provider supports multiple callback URLs and how many are supported per client configuration may vary, and you may find that you have to create multiple client applications with your OAuth provider. </p><p id="""">If you do configure separate client applications, that means that each handle will have their own distinct set of metadata attributes, such as Client ID or Client Secret. In this case, your environments must be provided and use the metadata values that correspond with the environment handle they are given at runtime.</p><p id="""">Continuing with our previous example, Release Environment Handles also allow you to associate plaintext and secret key-value pairs with each environment handle that will automatically be injected as environment variables to your container services or custom jobs that run as part of your environment: </p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/62d7fdc7294cc017ff3945f7_wWtYaNFhTUhSswIdWjKzsZOVEPp9LXCzczmZZl3JrCKldSafC8cJZbhI5nEVsO6X58_7pDx9HQYJuUJmI2vWx40BpU57mQ3-o6GbTbNMk6RxPYXLUxucayUy2rTvunGNOrSjHohVZE6Lr-xMBqkvijE.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><h4 id="""">Alternative approaches to environment handles</h4><h5 id="""">Wildcard subdomains in OAuth 2.0 callbacks</h5><p id="""">Some 3rd-parties support the use of wildcard subdomains when allow-listing a client application’s callback URL which technically could eliminate the need for a solution like environment handles. However, wildcard subdomains are a security risk (e.g. <a href=""https://bolster.ai/blog/subdomain-hijacking-takeover"" id="""">subdomain hijacking</a>) and aren’t supported by all OAuth 2.0 authorization servers For these reasons, we recommend avoiding them when possible.</p><p id=""""><em id="""">Example - Okta provides a warning if you enable wildcards in your callback URLs:</em></p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1382px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1382px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/62d7fdc75b8d87a0986557cd_4ixhMuejJmB28GHPpORccjDnLScsMs4VW00UkguGss7vamoH1xRCBQi0mUrOVGDxrzP_JwI6ercPRjAeuPm8fnWjUNKpkUTh6R-eccgnxkzaQCIHt0NUEyTAU-Ff5jwcPkdT8DIKingm6_2Li-GF5dE.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><h4 id="""">OAuth 2.0 Callback Proxy</h4><p id="""">Depending on the size of your team, number of concurrent environments, or number of OAuth integrations, you may find that you need hundreds of callback URLs or a constantly-increasing number of URLs.</p><p id="""">In these scenarios, environment handles may not be practical and you might instead need to build an OAuth 2.0 callback proxy. Implementation details may vary, but as an example, one approach we’ve helped customers implement involves modifying their client application’s initial OAuth 2.0 authorization code request to: </p><ol id=""""><li id="""">Use a callback URL that points to a static OAuth2 proxy service with a static hostname (e.g https://<em id="""">oauth2.example.com/callback) instead of </em>the client application’s true callback URL. <br></li><li id="""">Encode the client application’s true callback URL (e.g. https://<em id="""">env-123.example.com/callback</em>) in one of the original request parameters, like <strong id="""">state</strong>. </li></ol><p id="""">After the user authenticates and approves any requested resource scopes, the authorization server would return the authorization grant to your OAuth2 proxy service’s callback URL, which is the <em id="""">only </em>callback URL you need to allow-list in your OAuth client configuration for test purposes. Your OAuth2 proxy service would then decode the <strong id="""">state </strong>parameter to determine your environment’s true callback URL and forward the authorization grant to that destination.</p><p id="""">While outside the scope of this blog, stay tuned, as we plan to share a closer look at an example OAuth 2.0 proxy implementation in the near future.</p><h4 id="""">OAuth 2.0 Dynamic Client Registration Protocol</h4><p id="""">The Internet Engineering Task Force (IETF) standardized the <a href=""https://datatracker.ietf.org/doc/html/rfc7591"" id="""">OAuth 2.0 Dynamic Client Registration Protocol</a> in RFC 7591. As the name suggests, it can support just-in-time registration of a client application with an OAuth 2.0 authorization server. However, because it is not yet widely adopted, we haven’t yet explored its applicability to ephemeral environments and the authorization code flow. If you’ve worked with this protocol and have a perspective to share, please let us know. </p><h3 id="""">Summary</h3><p id="""">In this blog, we discussed the challenges that can arise when ephemeral environments with dynamic hostnames need to leverage a protocol like OAuth 2.0 that requires predefining static callback URLs to those same applications, and how solutions like Environment Handles can work within these constraints. </p><p id="""">If you’re looking for a platform that makes it easy to create ephemeral environments in <em id="""">your </em>AWS or GCP account, <a href=""https://release.com/"">Release</a> can help, and if you’re looking to use deterministic hostnames with your Release environment for OAuth callbacks or anything else, we offer <a href=""https://docs.releasehub.com/reference-documentation/account-settings/environment-handles"" id="""">out-of-the-box support for Environment Handles</a> within your Release environments.</p>",false,,,,https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63e41a019cf814561220a3c0_072622%20(1).jpg,a keyboard with red lights,mat-werber,8,Tue Jul 26 2022 16:10:00 GMT+0000 (Coordinated Universal Time),,
Summer Reading List: Docker edition   ,summer-reading-list-docker-edition,62aa5a70cd5ba27d9d0d718a,64f79ca71a96e3e207d630f3,Tue Sep 05 2023 21:24:55 GMT+0000 (Coordinated Universal Time),Thu Apr 25 2024 20:28:25 GMT+0000 (Coordinated Universal Time),Thu May 02 2024 20:19:24 GMT+0000 (Coordinated Universal Time),What's on your Summer Reading List? Check out our Docker reading suggestions.,"<p id="""">How are you spending the last days of summer? How about taking a few minutes to brush up on your Docker knowledge? As we prepare for the <a href=""https://www.dockercon.com/"" id="""">DockerCon</a> conference later this fall, we wanted to get you up-to-speed on all things Docker. </p><p id="""">This installment of the Summer Reading List highlights five especially popular Docker articles that help you understand and troubleshoot Docker challenges. Read on and let us know if you have other favorite Docker resources we could learn from. </p><h4 id="""">🐳 <a href=""https://release.com/blog/creating-your-first-application-in-release-with-docker-compose"" id="""">Creating your first Application in Release with Docker Compose</a></h4><p id="""">Learn how to migrate applications from Docker Compose to Kubernetes using Release. This step-by-step guide shows how to analyze a Compose file, generate an Application Template, set environment variables and build arguments, and deploy an application. Part two the series will cover deployment specifics and Kubernetes objects.</p><h4 id="""">🐳 <a href=""https://release.com/blog/how-to-set-docker-compose-environment-variables"" id="""">How to set Docker Compose Environment Variables</a></h4><p id="""">This practical guide to managing environment variables in Docker Compose shows you how to define variables in the Compose file, pass variables from the host machine, or store them using a &lt;code inline&gt;.env&lt;/code&gt; file. It also explains the priority of variables, offers best practices for handling sensitive data, and goes over solution for secure and efficient environment management.</p><h4 id="""">🐳 <a href=""https://release.com/blog/6-docker-compose-best-practices-for-dev-and-prod"" id="""">6 Docker Compose Best Practices for Dev and Prod</a></h4><p id="""">Explore Docker Compose best practices for both development and production environments. Learn how to use Docker Compose to manage multi-container Docker applications effectively, and get tips on mounting code as a volume, using override files, employing YAML anchors for sharing settings, and configuring CPU and memory limits. </p><h4 id="""">🐳 <a href=""https://release.com/blog/how-to-edit-a-file-in-a-docker-container"" id="""">How to edit a file in a Docker container</a></h4><p id="""">This hands-on guide explains how to edit files in Docker containers, addressing the lack of preinstalled file editors like Vim or Nano. It offers two methods: editing from the command line and using Visual Studio Code (VS Code) with the Docker extension. The guide emphasizes best practices, including removing unnecessary packages after editing and persisting an editor in the Dockerfile for frequent changes. It also highlights the importance of saving new images after making changes for testing and consistency.</p><h4 id="""">🐳 <a href=""https://release.com/blog/cutting-build-time-in-half-docker-buildx-kubernetes"" id="""">Cutting Build Time In Half with Docker’s Buildx Kubernetes Driver</a></h4><p id="""">Follow along on our journey to improve our build infrastructure by implementing Docker's buildx project. When we faced challenges with slow build times and concurrency bottlenecks in our build process, we turned to buildx, to speed-up both our cached and uncached builds. This blog covers our infrastructure setup and how buildx's features like parallelism, Kubernetes driver, and load balancing helped us optimize our Docker image building process, ultimately benefiting our service delivery and customer experiences.</p><p id="""">This concludes our Docker Summer Reading List. For more engaging reads, check out our <a href=""https://release.com/blog"">blog</a>. And to use Release in your next project, <a href=""https://release.com/signup"">sign up for a free account</a>. </p>",false,,,,https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/64f79d2fe512c23d96286cae_Summer%20Reding%20List%20Docker%20edition.jpg,image credit: cottonbro studio,ira-casteel,3,Wed Sep 06 2023 18:00:00 GMT+0000 (Coordinated Universal Time),docker,
Summer Reading List: Kubernetes edition   ,summer-reading-list-kubernetes-edition,62aa5a70cd5ba27d9d0d718a,64a59aaae620e7e634c1b2b7,Wed Jul 05 2023 16:30:34 GMT+0000 (Coordinated Universal Time),Thu Apr 25 2024 20:29:12 GMT+0000 (Coordinated Universal Time),Thu May 02 2024 20:19:24 GMT+0000 (Coordinated Universal Time),What's on your Summer Reading List? Check out our Kubernetes reading suggestions.,"<p id="""">What are your summer plans? Are you headed to the beach, or the mountains? Are you exploring your backyard or traveling abroad? Or do you have a list of projects you want to tackle during these warm months? Wherever summer takes you, this is a great time to brush up your tech skills or learn something new. That’s why we are putting together a series of summer reading lists to help you along your learning journey. &nbsp;</p><p id="""">This installment of the Summer Reading List highlights five especially popular Kubernetes articles that help you understand and troubleshoot Kubernetes challenges. Read on and let us know if you have a favorite Kubernetes resource we could learn from. </p><h4 id="""">📖 <a href=""https://release.com/blog/why-kubernetes-is-so-hard"" id="""">Why Is Kubernetes So Hard - 4 Reasons Why And What to do About it</a></h4><p id="""">In this piece, we discuss challenges and complexities of using Kubernetes for application orchestration. We highlight the difficulties in setting up and managing Kubernetes infrastructure, deciphering YAML configurations, and troubleshooting issues. We also look at alternative solutions, including outsourcing Kubernetes management or using automation tools to simplify the deployment process.</p><h4 id="""">📖 <a href=""https://release.com/blog/how-to-create-and-configure-your-kubernetes-service-account"" id="""">How to Create and Configure Your Kubernetes Service Account</a></h4><p id="""">This article explains the concept of Kubernetes service accounts and how to use them for non-human access to Kubernetes clusters. While human users authenticate to the Kubernetes API using user accounts, non-human users like CI/CD pipelines or monitoring tools require service accounts. The article covers creating service accounts, assigning them to pods, validating their usage, and adjusting permissions. It emphasizes the importance of properly configuring service accounts for security reasons.</p><h4 id="""">📖 <a href=""https://release.com//blog/10-kubernetes-namespace-best-practices-to-start-following"" id="""">10 Kubernetes Namespace Best Practices to Start Following</a></h4><p id="""">Here we discuss the importance of namespaces in Kubernetes. Namespaces are used to segregate and allocate resources in a Kubernetes cluster. Here we cover naming conventions, attaching labels to namespaces, using RBAC for resource allocation, implementing resource quotas and limit ranges, using NetworkPolicy for secure communication, and managing secrets. Following the best practices outlined in this article can improve efficiency, organization, and cost-effectiveness in Kubernetes workflows.</p><h4 id="""">📖 <a href=""https://release.com/blog/kubernetes-environment-variables"" id="""">How to Make the Most of Kubernetes Environment Variables</a></h4><p id="""">Environment variables in Kubernetes are dynamic key-value variables that provide information about the environment to software. They can be used to configure pods, pass sensitive information, or make applications adaptable. In this article you will learn about the three main methods for adding environment variables: direct specification, using secrets, and utilizing ConfigMaps, and how leveraging environment variables enables greater flexibility and reusability of Docker images in Kubernetes deployments.</p><h4 id="""">📖 <a href=""https://release.com/blog/kubernetes-how-to-debug-crashloopbackoff-in-a-container"" id="""">Kubernetes - How to Debug CrashLoopBackOff in a Container</a></h4><p id="""">The article discusses how to debug the ""CrashLoopBackOff"" error in Kubernetes containers, and explains that the error can be caused by misconfigurations or code issues. The article provides steps to inspect and investigate the container image, override the container Entrypoint, and add debugging tools to diagnose and fix the problem. It also emphasizes the importance of documenting the debugging steps for future referenc</p><p id="""">This concludes our Kubernetes Summer Reading List. For more engaging reads, check out our <a href=""https://release.com/blog"">blog</a>. And to use Release in your next project, <a href=""https://web.release.com/register"">sign up for a free account</a>. </p>",false,,,,https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/64a59a8be620e7e634c18f39_Summer%20Reading_%20Kubernetes.jpg,,ira-casteel,3,Wed Jul 05 2023 18:30:00 GMT+0000 (Coordinated Universal Time),kubernetes,
Syncing Databases: How to Do It and Best Practices,syncing-databases-how-to-do-it-and-best-practices,62aa5a70cd5ba27d9d0d718a,6488eb1f4cd7b1ba52742c9f,Tue Jun 13 2023 22:18:07 GMT+0000 (Coordinated Universal Time),Wed Oct 23 2024 19:46:39 GMT+0000 (Coordinated Universal Time),Wed Oct 23 2024 19:46:55 GMT+0000 (Coordinated Universal Time),"Read about database synchronization, what it's used for, how to sync databases, & explore tools to simplify the process",,true,<p>Effortlessly manage database synchronization with Release's on-demand environments.</p>,https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=casestudy&utm_content=database-sync,"<p id="""">As an engineering leader or developer, you may have encountered the need to sync multiple copies of a database to ensure data consistency across different systems or locations. Whether you're working on a distributed system, a mobile application, or a cloud-based platform, syncing the databases is a crucial task that requires careful planning and execution. </p><p id="""">In this article, we'll look at database synchronization and what to use it for. Next, we'll explore different types of synchronization processes and how to sync databases step by step. We'll also cover some helpful tooling that makes syncing your databases easier and faster. Let's get started! </p><h3 id=""""><strong id="""">What Is Database Synchronization?</strong></h3><p id="""">Database synchronization is the process of keeping multiple copies of a database in sync with one another. </p><p id="""">There are different ways of synchronizing databases, depending on the type of database, the network infrastructure, and the application's requirements. Some of the standard <strong id="""">methods</strong> of database synchronization include the following: </p><h4 id=""""><strong id="""">Two-way synchronization</strong></h4><p id="""">In two-way synchronization, both databases can make changes and synchronize with each other. </p><h4 id=""""><strong id="""">One-way synchronization</strong></h4><p id="""">In one-way synchronization, one database acts as the source, and the other databases are updated to match it. </p><h4 id=""""><strong id="""">Incremental synchronization</strong></h4><p id="""">With incremental synchronization, changes are only made since the last synchronization. </p><h4 id=""""><strong id="""">File-based synchronization</strong></h4><p id="""">With file-based synchronization, data is exported to a file and imported into the other databases. </p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1000px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1000px""><div id=""""><img src=""https://cdn.prod.website-files.com/603dd147c5b0a4221d1bd360/6488eaa85594acd90610c74e_MgQqZtD5sRSsBW5QzJZcPbe5BwIci1m9BN96Lo76TXBaI4yyRPXxmSej_kPb9trlSXBvJVAMtNdgz94ykKaNa840ps2OWwhmc4WqEkKZU70UhT9nkkEeEY6lHtqCjuVNb5I0xEo9B9mIWvme5bcaDSg.png"" id="""" width=""auto"" height=""auto"" loading=""auto"" alt=""""></div></figure><h3 id=""""><strong id="""">Use Cases for Synchronization</strong></h3><p id="""">Database synchronization is used in various situations where multiple copies of a database are in use and when it's necessary to ensure that the data in each copy is consistent. Some common use cases include the following: </p><h4 id=""""><strong id="""">Backup and recovery</strong></h4><p id="""">You can use database synchronization to keep a secondary copy of a database in sync with the primary copy, providing a way to recover from data loss or corruption. </p><h4 id=""""><strong id="""">Mobile and offline applications</strong></h4><p id="""">Applications that work offline or on mobile devices may need to synchronize data with a central database when a connection becomes available. Database synchronization ensures that the data on the mobile device or offline application is consistent with the data on the central server. </p><h4 id=""""><strong id="""">Collaborative platforms</strong></h4><p id="""">Multiple users may work on the same data in a collaborative platform. Here, database synchronization ensures that changes made by one user propagate to all other users, maintaining data consistency. </p><h4 id=""""><strong id="""">Distributed Systems</strong></h4><p id="""">In a distributed system, multiple copies of a database may run on different servers or in different locations. Database synchronization ensures that changes made to one copy of the database propagate to all other copies, maintaining data consistency across the system. </p><h4 id=""""><strong id="""">Cloud-based systems</strong></h4><p id="""">Cloud-based systems often have multiple copies of a database running in different regions to provide high availability and reduce latency. Database synchronization ensures that data is consistent across all copies of the database. </p><h3 id=""""><strong id="""">Types of Database Synchronization</strong></h3><p id="""">There are several different types of database synchronization, each with its own advantages and disadvantages. Some common types include the following: </p><h4 id=""""><strong id="""">Source/Replica replication</strong></h4><p id="""">In this type of replication, one database acts as the source, and the other databases are updated to match it. The source database receives all updates and changes, which then propagate to the replica databases. It's commonly used for read-heavy workloads. </p><h4 id=""""><strong id="""">Multi-master replication</strong></h4><p id="""">In this type of replication, all databases can act as both sources and replicas. Changes made to one database are propagated to all other databases, ensuring that all copies of the data are consistent. This type of replication is helpful for write-heavy workloads. </p><h4 id=""""><strong id="""">File-based synchronization</strong></h4><p id="""">In this type of synchronization, data is exported to a file and then imported into the other databases. It's a simple method that's easy to implement, but it can be slow and may not be suitable for large amounts of data. </p><h4 id=""""><strong id="""">Log-based replication</strong></h4><p id="""">In this type of replication, changes made to a database are recorded in a log and then propagated to the other databases. This allows for fast and efficient replication but can be more complex to set up and maintain. </p><h4 id=""""><strong id="""">Trigger-based replication</strong></h4><p id="""">In this type of replication, triggers are set up on the source database to capture changes, which can then be propagated to the target database. It allows for fine-grained control over which changes are propagated, but it can be resource-intensive and may be unsuitable for high-traffic systems. </p><h4 id=""""><strong id="""">Cloud-based database synchronization</strong></h4><p id="""">We can also use cloud services like <a href=""https://aws.amazon.com/dms/"" id="""">AWS Database Migration Service (DMS)</a> and <a href=""https://azure.microsoft.com/en-us/products/database-migration"" id="""">Azure Database Migration Service</a> to sync databases. This is a good option if you have a cloud-based infrastructure and want to leverage the scalability and reliability offered by these services. </p><p id="""">The best approach for your use case will depend on the type of data, the number of databases, the network infrastructure, and the requirements of the application. </p><h3 id="""">‍<strong id="""">How to Sync Databases</strong></h3><p id="""">The process of syncing databases can vary, depending on the type of databases, the method of synchronization, and the specific requirements of the application. Below is the step-by-step guide on how to sync databases: </p><h4 id=""""><strong id="""">Step 1: Understand your use case</strong></h4><p id="""">Understand the specific requirements of your use case and choose the synchronization method that best fits those needs. </p><h4 id=""""><strong id="""">Step 2: Identify the databases to be synced</strong></h4><p id="""">Determine which databases need to be synced and the type of data they contain. </p><h4 id=""""><strong id="""">Step 3: Choose a synchronization method</strong></h4><p id="""">Decide on the method of synchronization that's most appropriate for your use case. This may be replication, a data syncing tool, a custom script, or a cloud-based service. </p><h4 id=""""><strong id="""">Step 4: Configure the databases</strong></h4><p id="""">Set up the databases for synchronization. This may include configuring replication settings, installing data syncing tools, or writing custom scripts. </p><h4 id=""""><strong id="""">Step 5: Test the synchronization</strong></h4><p id="""">Test the synchronization by making changes to one database and verifying that the changes are propagated to the other databases. This will help you identify any issues or bugs before deploying it in production. </p><h4 id=""""><strong id="""">Step 6: Schedule synchronization</strong></h4><p id="""">Set a schedule for the synchronization to occur regularly. You can do this using a built-in scheduling feature or by writing a custom script. </p><h4 id=""""><strong id="""">Step 7: Monitor and troubleshoot</strong></h4><p id="""">Monitor the synchronization process and troubleshoot any issues that arise. This may include monitoring replication lag, checking for errors, and addressing any conflicts that occur. It may also include monitoring replication lag, checking for errors, and addressing any conflicts that occur. </p><h4 id=""""><strong id="""">Step 8: Maintain and update</strong></h4><p id="""">Regularly maintain and update the synchronization process to ensure that it continues to function correctly. </p><p id="""">‍<strong id="""">Note:</strong> Some steps may vary, depending on the type of database and the method of synchronization. For example, for a cloud-based service like AWS DMS, the process can be simpler. You have to create the replication task and configure the source and target databases. </p><h3 id=""""><strong id="""">Tooling for Database Synchronization</strong></h3><p id="""">There are various tools available for database synchronization, depending on the type of database and the method of synchronization you're using. You can achieve database synchronization through various methods, such as the following: </p><h4 id=""""><strong id="""">Custom scripts</strong></h4><p id="""">You can write custom scripts using programming languages such as Python or Java to sync databases. This involves writing code to compare data in two databases and making changes as needed. </p><h4 id=""""><strong id="""">Replication</strong></h4><p id="""">This involves copying data from one database to another so that changes made to one are reflected in the other. </p><ul id=""""><li id=""""><a href=""https://dev.mysql.com/doc/refman/8.0/en/replication.html"" id="""">MySQL</a> provides built-in replication capabilities, allowing users to replicate data between two or more MySQL servers.</li><li id=""""><a href=""https://learn.microsoft.com/en-us/sql/relational-databases/replication/sql-server-replication?view=sql-server-ver16"" id="""">Microsoft SQL Server</a> also provides built-in replication capabilities, including transactional replication and merge replication.</li><li id=""""><a href=""https://www.postgresql.org/docs/current/runtime-config-replication.html"" id="""">PostgreSQL</a> offers several replication solutions, including streaming replication and <a href=""https://www.postgresql.org/docs/15/logical-replication.html"" id="""">logical replication</a>.</li><li id=""""><a href=""https://www.mongodb.com/docs/manual/replication/"" id="""">MongoDB</a> provides built-in replication features, including replica sets and sharded clusters for horizontal scalability.</li></ul><h4 id=""""><strong id="""">Cloud-based services</strong></h4><p id="""">Below are some of the common cloud-based services that you can use to sync databases. </p><ul id=""""><li id=""""><a href=""https://aws.amazon.com/dms/"" id="""">AWS Database Migration Service (DMS)</a> can migrate, replicate, and sync databases between different platforms and environments, including on-premises environments and cloud-based environments like Amazon Web Services (AWS).</li><li id=""""><a href=""https://azure.microsoft.com/en-us/products/database-migration"" id="""">Azure Database Migration Service</a> is a fully managed service designed to enable seamless migrations from multiple database sources to Azure.</li></ul><h4 id=""""><strong id="""">Data syncing tools</strong></h4><p id="""">Various data syncing tools can automate the process of keeping databases in sync. </p><ul id=""""><li id=""""><a href=""https://www.symmetricds.org/"" id="""">SymmetricDS</a> is open-source data synchronization software that supports multiple relational databases, including MySQL, PostgreSQL, Oracle, and more.</li><li id=""""><a href=""https://www.talend.com/"" id="""">Talend</a>, <a href=""https://www.informatica.com/"" id="""">Informatica</a>, and <a href=""https://boomi.com/"" id="""">Boomi</a> can automate the process of keeping databases in sync.</li><li id=""""><a href=""https://www.oracle.com/in/integration/goldengate/"" id="""">Oracle GoldenGate</a> is a real-time data integration and replication software for heterogeneous environments, including Oracle, SQL Server, DB2, and more.</li></ul><p id="""">These are some of the popular tooling used for database synchronization. However, the best tool depends on the type of database, the method of synchronization, and the specific requirements of the application. </p><h3 id=""""><strong id="""">Best Practices</strong></h3><p id="""">When working with database synchronization, there are several best practices that can help you ensure that your synchronization process is efficient and reliable. </p><h4 id=""""><strong id="""">Keep your databases in sync</strong></h4><p id="""">Regularly check and compare data between the different copies of the database and make updates as needed. </p><h4 id=""""><strong id="""">Use a replication tool</strong></h4><p id="""">Use a replication tool that best fits your use case and the type of data you're working with. </p><h4 id=""""><strong id="""">Backups</strong></h4><p id="""">Regularly back up your databases to ensure that you can recover from data loss or corruption. </p><h4 id=""""><strong id="""">Use cloud-based services</strong></h4><p id="""">You can also use Cloud-based services like AWS DMS and Azure Database Migration Service to sync databases. It's a good option if you have a cloud-based infrastructure and want to leverage the scalability and reliability offered by these services. </p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1000px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1000px""><div id=""""><img src=""https://cdn.prod.website-files.com/603dd147c5b0a4221d1bd360/6488eaa82371874063995e16__d9-PL0Z8bdGU1GDzyOUAEdhx2tRH0r6NB6LPMEyrpsLoF2Zhj3QufXPbDZ4Pcg5bQgIVB1vkHpORpqNv5FvoqL4-4wcPyPQQCc3xmU8QIJEF_yT_aKAWsJIVJ8ySkQohW42CHXB7Jv8arkXNKvmbLs.png"" id="""" width=""auto"" height=""auto"" loading=""auto"" alt=""""></div></figure><h4 id=""""><strong id="""">Security</strong></h4><p id="""">Ensure that your synchronization process is secure by encrypting data in transit and at rest and by implementing access controls. </p><p id="""">By following these best practices, you can ensure that your databases are kept in sync and that your synchronization process is efficient and reliable. Additionally, it's always important to be aware of the particularities of the database you're working with, the replication tool you're using, and the type of data you're syncing in order to apply the best practices in a way that fits your specific needs. </p><h3 id=""""><strong id="""">Conclusion</strong></h3><p id="""">Syncing databases is a crucial task that requires careful planning and execution. By understanding your use case, choosing the right synchronization method, and following best practices such as testing, monitoring, and maintaining, you can ensure that your databases sync effectively and efficiently. Additionally, by using cloud-based services, you can reduce the complexity of the process and ensure that your databases are always in sync. </p><p id="""">If you're looking for a solution to manage your <a href=""https://release.com/blog/remote-development-environments"" id="""">remote development environments</a>, you can use <a href=""https://release.com/book-a-demo"" id="""">Release</a> to create, manage and share development environments, collaborate with your team, and automate your release pipeline, all in one place. <a href=""https://release.com/book-a-demo"" id="""">Book a demo</a> today and see the difference it can make in your remote development process</p><p id="""">‍</p>",https://cdn.prod.website-files.com/603dd147c5b0a4221d1bd360/6488eb18418be989093da0bb_Syncing%20Databases_%20How%20to%20Do%20It%20and%20Best%20Practices.jpg,,regis-wilson,10,Wed Jun 14 2023 18:00:00 GMT+0000 (Coordinated Universal Time),,
Table test,table-test,62aa5a70cd5ba27d9d0d718a,638b8ce15fd2fdca4f945c40,Sat Dec 03 2022 17:52:33 GMT+0000 (Coordinated Universal Time),Mon Dec 05 2022 17:35:55 GMT+0000 (Coordinated Universal Time),,,"<h4 id="""">Step 1: Gather test data</h4><p id="""">The first step, as exactly described above in Stack Overflow, is to run the AWS CLI to output a JSON list of all of your Route53 entries (in our case, over 5,000+!!!). Take a few lines of the first part of the output to play with. You can grab a few entries by limiting the --max-items option in the list-resource-record-sets command. Take those and paste them into the jq play screen on the left, then select “.” as the operator to output everything. You can follow along with this snippet. Here is what it looks like initially:</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:776px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""776px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/638b8cd057859f6e50cc8f90_Content.png"" loading=""lazy"" id="""" width=""auto"" height=""auto""></div></figure><p id="""">This is a good starting point but we need to first start with unwrapping the outer layer of the “ResourceRecords” key to find the list of entries as follows (turn on the “Compact View” to make it easier to understand and see more of what’s happening:</p><div data-rt-embed-type='true'><table class=""blog_table"">

<tr class=""blog_table_row"">
    <th class=""blog_table_header"">Tag Key</th>
    <th class=""blog_table_header"">Tag Value</th>
    <th class=""blog_table_header"">Location in UI</th>
    <th class=""blog_table_header"">Env variable</th>
</tr>


<tr class=""blog_table_row"">
    <td class=""blog_table_cell"">
        <div class=""blog_table_tag-key"">releasehub.com/app-</div>
        <div class=""blog_table_tag-key"">name</div>
    </td>
    <td class=""blog_table_cell"">(the app name)</td>
    <td class=""blog_table_cell"">Top of the environment page. In the screenshot below, the app name is<div class=""blog_table_highlight"">example-voting-app</div></td>
    <td class=""blog_table_cell""><div class=""blog_table_highlight"">RELEASE_APP_NAME</div>(see default ReleaseHub environment variables)</td>
</tr>


<tr class=""blog_table_row"">
    <td class=""blog_table_cell"">
        <div class=""blog_table_tag-key"">releasehub.com/env-id</div>
    </td>
    <td class=""blog_table_cell"">(the environment handle)</td>
    <td class=""blog_table_cell"">In the screenshot below, the environment handle is<div class=""blog_table_highlight"">ted3bff</div></td>
    <td class=""blog_table_cell""><div class=""blog_table_highlight"">RELEASE_ENV_ID</div>(see default ReleaseHub environment variables)</td>
</tr>

</table></div>",false,,,,,,,10,,,
Talk to your infrastructure in plain English: introducing Release AI,talk-to-your-infrastructure-in-plain-english-introducing-release-ai,62aa5a70cd5ba27d9d0d718a,64e8f5b2b58b8db8c0b99798,Fri Aug 25 2023 18:40:50 GMT+0000 (Coordinated Universal Time),Fri Aug 25 2023 18:41:20 GMT+0000 (Coordinated Universal Time),,,"<p id="""">The wait(list) is over. Release AI is now available to everyone, and for a limited time it’s absolutely free. I am thrilled to share this innovative AI tool designed to redefine the way you build and deliver applications so you can experience our context-aware Release AI, directly through your CLI. Join our launch community, and share your feedback that will shape the feature roadmap of Release AI. </p><p id="""">Sign up for your free account <a href=""https://beta.release.com/ai/register"" id="""">here</a> and let us know what you think. &nbsp;</p><p id="""">Release helps teams release their ideas to the world faster. For developers like you, it means spinning up ephemeral environments in seconds and seeing exactly how your app will perform in production, or using production-like data to test your creations under real loads. With Release AI it also means getting all your infrastructure questions answered without relying on the limited resources of your DevOps team. Originally, DevOps was meant to empower developers with operational tasks, but instead it turned into an additional hurdle on the path to delivering product features quickly.</p><p id="""">Release AI lets you tap into the information needed to deploy and release your ideas to the world. Now you don’t need to track down your go-to DevOps engineer and wait for them to run complex queries to answer questions like: </p><ul id=""""><li>What's going on in my AWS account? </li><li>How is it configured? </li><li>How is my Kubernetes cluster setup? </li><li>Are pods crashing? </li><li>What are the IP addresses of my instances? </li><li>What was the bill in my AWS account yesterday?</li></ul><p id="""">What used to take hours or days to figure out, is now available directly in your command line. With Release AI you can talk to your underlying infrastructure in plain English and have it perform complex tasks with your context in mind. You can even schedule tasks, so you can ask these questions on a recurring basis and get the result delivered to you automatically. &nbsp;This removes a bottleneck in your ability to deliver software faster, and gets the DevOps out of the business of being highly paid developer support engineers. </p><p id="""">To get started, <a href=""https://beta.release.com/ai/register"" id="""">sign up for a Release AI account</a> and follow the steps in our <a href=""https://docs.release.com/release-ai/quickstart"" id="""">Quickstart guide</a> to get up and running. <a href=""https://join.slack.com/t/release-ai/shared_invite/zt-20dxgp2o0-ePCXzsag6l8_i4HwOgRItg"" id="""">Join our Slack Channel</a> to stay up to date on new developments and share your insights. Make sure to check our <a href=""https://docs.release.com/release-ai/introduction"" id="""">docs</a> for more details on how Release AI works, to get sample prompts and to see advanced configurations. </p><p id="""">We believe Release AI has the potential to transform the way you work, empowering you to achieve new levels of efficiency, accuracy, and productivity. However, we cannot accomplish this alone. Your expertise and feedback are invaluable in shaping the future of Release AI, and ensuring that it genuinely caters to the needs of professionals like you. Join us for this preview phase, available free for all Release users via the CLI.</p>",false,,,,,,tommy-mcclung,3,,,
Terraform Kubernetes Deployment: A Detailed Walkthrough,terraform-kubernetes-deployment-a-detailed-walkthrough,62aa5a70cd5ba27d9d0d718a,62eee8ab27e6ae4cbc16cec8,Sat Aug 06 2022 22:18:19 GMT+0000 (Coordinated Universal Time),Fri Apr 21 2023 18:29:57 GMT+0000 (Coordinated Universal Time),Fri Apr 21 2023 18:46:03 GMT+0000 (Coordinated Universal Time), Did you know that you can use Terraform for deployment of your Kubernetes clusters? Learn how and why to do it here.,"<p id="""">Terraform and Kubernetes are two of the most popular tools in their categories. Terraform is widely adopted as the tool of choice for infrastructure as code, and Kubernetes is number one when it comes to orchestrating containers. Is it possible to combine both? Sure! You can use Terraform to deploy your Kubernetes clusters. It's actually quite common, and it lets you deploy Kubernetes just like the rest of your infrastructure. In this post, you'll learn how to do it. </p><h3 id="""">Terraform + Kubernetes: How and Why?</h3><p id="""">We have two main questions to answer here. How can you deploy Kubernetes with Terraform, and why would you do that? Let's start with the latter. </p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1000px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1000px""><div id=""""><img alt=""ApplicationDescription automatically generated with medium confidence"" src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/62eee871caeb354d6ce046da_2hDBOCbdoxziKZ5jM5SRbOplmaoJOdQMidCAUbjBw5MUWtS-QNSHAKnepBDfRsUisDJc6jp8TEGUu15o0fwufg_pW1I_o1C1ckeTqnNTLWy3q9qnfXRXUrNbMnR0mZn2YC7MQuxGC24aNcqnkOomreo.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><p id="""">The answer doesn't differ from ""Why would you deploy anything with Terraform?"" From that perspective, there's nothing special about Kubernetes, and you get the same benefit by using Terraform to deploy it as with any other infrastructure. You get automation, infrastructure versioning, reliability, and even the ability to perform infrastructure security scanning. </p><p id="""">As for how, the answer is actually similar. You can deploy Kubernetes with Terraform just like any other infrastructure. Meaning, you first need to find a Kubernetes resource definition for Terraform (we'll show you that shortly), adjust some parameters for your needs, add it to your Terraform code, and you're done. And just like with any other resource, Terraform will be able to track changes to your cluster and update its configuration after you make changes to the code. </p><h3 id="""">Deploying Kubernetes: First Steps</h3><p id="""">Enough theory. Let's see how it works in practice. First, you need to find a Terraform provider for your cloud. If you want to deploy Kubernetes on <a href=""https://www.digitalocean.com/"" id="""">DigitalOcean</a>, you'd need to follow <a href=""https://registry.terraform.io/providers/digitalocean/digitalocean/latest/docs/resources/kubernetes_cluster"" target=""_blank"" id="""">this documentation</a>. For <a href=""https://azure.microsoft.com/en-us/"" target=""_blank"" id="""">Microsoft Azure</a>, you'd need to head <a href=""https://registry.terraform.io/providers/hashicorp/azurerm/latest/docs/resources/kubernetes_cluster"" target=""_blank"" id="""">here</a> for details. And for <a href=""https://cloud.google.com/"" target=""_blank"" id="""">Google Cloud</a>, you need to check <a href=""https://registry.terraform.io/providers/hashicorp/google/latest/docs/resources/container_cluster"" target=""_blank"" id="""">here</a>. These are just a few examples. But no matter which cloud provider you're using, the general approach will be the same. For today's example, we'll use DigitalOcean. </p><p id="""">To start from nothing, in the simplest scenario, you need to create two files named <strong id="""">provider.tf</strong> and <strong id="""">main.tf</strong>. You could do it all in one file, but it's a good practice to separate providers and main resource definitions. In the code below, you can define your DigitalOcean provider for Terraform and pass your DigitalOcean token: </p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yaml"">
terraform {
  required_providers {
    digitalocean = {
      source = ""digitalocean/digitalocean""
      version = ""~> 2.0""
    }
  }
}
variable ""do_token"" {
  default = ""[replace_with_your_token]""
}
# Configure the DigitalOcean Provider
provider ""digitalocean"" {
  token = var.do_token
}
</code>
</pre></div><p id="""">In <strong id="""">main.tf</strong> you can now define your Kubernetes. </p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yaml"">
resource ""digitalocean_kubernetes_cluster"" ""test"" {
  name   = ""test_cluster""
  region = ""nyc1""
  version = ""1.22.11-do.0""
  node_pool {
    name       = ""worker-pool""
    size       = ""s-2vcpu-2gb""
    node_count = 3
  }
}
</code>
</pre></div><p id="""">Now that you have your Terraform files prepared, you need three things. First, you need to initiate the DigitalOcean provider. You can do that with <strong id="""">terraform init</strong>. </p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yaml"">
# terraform init
Initializing the backend...
Initializing provider plugins...
- Finding digitalocean/digitalocean versions matching ""~> 2.0""...
- Installing digitalocean/digitalocean v2.21.0...
- Installed digitalocean/digitalocean v2.21.0 (signed by a HashiCorp partner, key ID F82037E524B9C0E8)
Partner and community providers are signed by their developers.
If you'd like to know more about provider signing, you can read about it here:
https://www.terraform.io/docs/cli/plugins/signing.html
Terraform has created a lock file .terraform.lock.hcl to record the provider
selections it made above. Include this file in your version control repository
so that Terraform can guarantee to make the same selections by default when
you run ""terraform init"" in the future.
Terraform has been successfully initialized!
You may now begin working with Terraform. Try running ""terraform plan"" to see
any changes that are required for your infrastructure. All Terraform commands
should now work.
If you ever set or change modules or backend configuration for Terraform,
rerun this command to reinitialize your working directory. If you forget, other
commands will detect it and remind you to do so if necessary.

</code>
</pre></div><p id="""">Then you can run your <strong id="""">terraform plan</strong>, which will show you planned changes to the infrastructure (which in this case should be creating a new Kubernetes cluster). </p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yaml"">
# terraform plan
Terraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following
symbols:
  + create
Terraform will perform the following actions:
  # digitalocean_kubernetes_cluster.test will be created
  + resource ""digitalocean_kubernetes_cluster"" ""test"" {
      + cluster_subnet = (known after apply)
      + created_at     = (known after apply)
      + endpoint       = (known after apply)
      + ha             = false
      + id             = (known after apply)
      + ipv4_address   = (known after apply)
      + kube_config    = (sensitive value)
      + name           = ""test-cluster""
      + region         = ""nyc1""
      + service_subnet = (known after apply)
      + status         = (known after apply)
      + surge_upgrade  = true
      + updated_at     = (known after apply)
      + urn            = (known after apply)
      + version        = ""1.22.11-do.0""
      + vpc_uuid       = (known after apply)
      + maintenance_policy {
          + day        = (known after apply)
          + duration   = (known after apply)
          + start_time = (known after apply)
        }
      + node_pool {
          + actual_node_count = (known after apply)
          + auto_scale        = false
          + id                = (known after apply)
          + name              = ""worker-pool""
          + node_count        = 3
          + nodes             = (known after apply)
          + size              = ""s-2vcpu-2gb""
        }
    }
Plan: 1 to add, 0 to change, 0 to destroy.
</code>
</pre></div><p id="""">The plan looks good. One resource will be added, and that's your Kubernetes cluster, so you can go ahead and apply the changes with <strong id="""">terraform apply</strong>. </p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yaml"">
# terraform apply
(...)
Plan: 1 to add, 0 to change, 0 to destroy.
Do you want to perform these actions?
  Terraform will perform the actions described above.
  Only 'yes' will be accepted to approve.
  Enter a value: yes
digitalocean_kubernetes_cluster.test: Creating...
digitalocean_kubernetes_cluster.test: Still creating... [10s elapsed]
(...)
digitalocean_kubernetes_cluster.test: Still creating... [7m10s elapsed]
digitalocean_kubernetes_cluster.test: Creation complete after 7m16s [id=49fd0517-a4a5-41e8-997d-1412c081e000]
Apply complete! Resources: 1 added, 0 changed, 0 destroyed.
</code>
</pre></div><p id="""">If you now head to your DigitalOcean portal to validate, you can indeed see it there. </p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1024px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1024px""><div id=""""><img alt=""DigitalOcean portal"" src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/62eee8711cef28941d20377b_SEFDhJ1BHMoQCY1tPpNNWIthoy-CtC8bgReGcSLR3oQPezCt6q7TEBtc5pKbY0PMQy6LVlteW0QacD1KRmhVlVN9clNcY6o5oZdBhy_KKlQe8gdTOWS5_vA7VDlUBujSH3DQTd4Vwdxwf7DBQ_XFjfY.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><p id=""""> And that's it! That's how you deploy Kubernetes with Terraform. </p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1430px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1430px""><div id=""""><img alt=""A picture containing device, control panelDescription automatically generated"" src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/62eee873dd39ff78f6f5f991_lZcyMp5_T4xdgBALD3L2GbSlPJiTwK1WDGziOkevokDC60P3WPcsArpsnn5zkp7rhGXXH7mpJGXq7HGh4ufLSFtMzxquUNHG6Ewcyucr3aHmsqHW_gsiyGPqEaNgqJ2Atprewqd2KCCueRdeAlAC8Kw.jpeg"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><h3 id="""">Deploying Kubernetes: Next Steps</h3><p id="""">Now that you know how it works in general, there are a few things that you need to learn next. First, all you've done is deploy basic, minimal Kubernetes. In more realistic scenarios, you'll probably want to parametrize more options for your Kubernetes. This, however, will highly depend on what you actually need. If you know what you need, you can head to the Terraform documentation and check <a href=""https://registry.terraform.io/providers/digitalocean/digitalocean/latest/docs/resources/kubernetes_cluster#argument-reference"" target=""_blank""><strong id="""">argument reference</strong></a> for your Kubernetes resource. Find what you need and add it to your code. </p><p id="""">For example, if you'd like your Kubernetes cluster to automatically upgrade, you can find the following in the documentation: </p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1024px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1024px""><div id=""""><img alt=""Auto upgrade"" src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/62eee873c33cc7c9f6c34ecb_8YwL3UxXWSQN_vv8mnD_G8-9QMUDDy_fqiiwuHv68FMTLA5dlhJComwm0I7PFEL_kYSkV0Lqlf4B6aHlb5fEk8fMhYu3MdhUDjJElKgpJhKwN4Tkj8cU29ll-Bs1KOQtTjp-0pVLIx_VefBQl7An2Fw.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><p id=""""> To make your freshly deployed cluster automatically upgrade, you just need to add the following to your Kubernetes resource definition in <strong id="""">main.tf</strong> as follows: </p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yaml"">
resource ""digitalocean_kubernetes_cluster"" ""test"" {
  name   = ""test-cluster""
  region = ""nyc1""
  version = ""1.22.11-do.0""
  auto_upgrade = true
  node_pool {
    name       = ""worker-pool""
    size       = ""s-2vcpu-2gb""
    node_count = 3
  }
}
</code>
</pre></div><p id="""">But you're not there yet. You can quickly see in the DigitalOcean portal that the cluster currently does not automatically upgrade. </p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1024px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1024px""><div id=""""><img alt=""auto upgrade in portal"" src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/62eee8735cac0a4fc06ed58c_Bl6WCezYhPOhk1k2_mf-PUO8QxkTcEGhen9KoJmQ-MCj8neu-fwT9iAuKFBf2MmiwaeMwg3H0ACCG-VgU2fEM2LbDXizA29Ikr_bx30F0WPiVUPKOpfpLo0YQfxM3cT-bYceK0ytTxH1DQK-7syDOpo.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><p id=""""> Automatic upgrades are disabled now, so you can run <strong id="""">terraform plan</strong> again to check what Terraform will try to do. </p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yaml"">
# terraform plan
Terraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols:
  ~ update in-place
Terraform will perform the following actions:
  # digitalocean_kubernetes_cluster.test will be updated in-place
  ~ resource ""digitalocean_kubernetes_cluster"" ""test"" {
      ~ auto_upgrade   = false -> true
        id             = ""49fd0517-a4a5-41e8-997d-1412c081e000""
        name           = ""test-cluster""
        tags           = []
        # (13 unchanged attributes hidden)
        # (2 unchanged blocks hidden)
    }
Plan: 0 to add, 1 to change, 0 to destroy
</code>
</pre></div><p id="""">As expected, Terraform will now try to update your cluster in place and add an auto-upgrade option to it. Let's go ahead and apply that change. </p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yaml"">
# terraform apply
(...)
Plan: 0 to add, 1 to change, 0 to destroy.
Do you want to perform these actions?
  Terraform will perform the actions described above.
  Only 'yes' will be accepted to approve.
  Enter a value: yes
digitalocean_kubernetes_cluster.test: Modifying... [id=49fd0517-a4a5-41e8-997d-1412c081e000]
digitalocean_kubernetes_cluster.test: Modifications complete after 2s [id=49fd0517-a4a5-41e8-997d-1412c081e000]
Apply complete! Resources: 0 added, 1 changed, 0 destroyed.

</code>
</pre></div><p id="""">The change was quickly applied to your cluster, and if you double check in the portal again, you can see that, indeed, the auto-upgrade option is now enabled. </p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1024px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1024px""><div id=""""><img alt=""auto-upgrade enabled"" src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/62eee8735cac0acc056ed58d_k3RlpN0A_UUkDrFS9q-3vQwVodIjMqfzGgY14vTYZ_xjJ6DZ0wkJrvt0-aqlLayej7G3F0y1uMBQDq85qg4aAOlt4qjLiDS1LhHmHKwWnooHvlb6fbbOowZlbpVOrScz7PbLDoaoWvQ0cMGi7m8ON_c.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><h3 id="""">Destroying Kubernetes</h3><p id="""">If you no longer want your Kubernetes cluster, you can destroy it just as easily as you deployed it. All you need to do is execute <strong id="""">terraform destroy</strong>. </p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yaml"">
# terraform destroy
digitalocean_kubernetes_cluster.test: Refreshing state... [id=49fd0517-a4a5-41e8-997d-1412c081e000]
Terraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols:
  - destroy
Terraform will perform the following actions:
  # digitalocean_kubernetes_cluster.test will be destroyed
  - resource ""digitalocean_kubernetes_cluster"" ""test"" {
      - auto_upgrade   = true -> null
      - cluster_subnet = ""10.244.0.0/16"" -> null
      - created_at     = ""2022-07-24 06:15:34 +0000 UTC"" -> null
      - endpoint       = ""https://49fd0517-a4a5-41e8-997d-1412c081e000.k8s.ondigitalocean.com"" -> null
      - ha             = false -> null
      - id             = ""49fd0517-a4a5-41e8-997d-1412c081e000"" -> null
      - kube_config    = (sensitive value)
      - name           = ""test-cluster"" -> null
      - region         = ""nyc1"" -> null
      - service_subnet = ""10.245.0.0/16"" -> null
      - status         = ""running"" -> null
      - surge_upgrade  = true -> null
      - tags           = [] -> null
      - updated_at     = ""2022-07-24 06:37:27 +0000 UTC"" -> null
      - urn            = ""do:kubernetes:49fd0517-a4a5-41e8-997d-1412c081e000"" -> null
      - version        = ""1.22.11-do.0"" -> null
      - vpc_uuid       = ""877cc187-97ad-426c-9301-079e3683d351"" -> null
      - maintenance_policy {
          - day        = ""any"" -> null
          - duration   = ""4h0m0s"" -> null
          - start_time = ""10:00"" -> null
        }
      - node_pool {
          - actual_node_count = 3 -> null
          - auto_scale        = false -> null
          - id                = ""8df9b48c-329d-41f5-899e-b7b896e28e15"" -> null
          - labels            = {} -> null
          - max_nodes         = 0 -> null
          - min_nodes         = 0 -> null
          - name              = ""worker-pool"" -> null
          - node_count        = 3 -> null
          - nodes             = [
              - {
                  - created_at = ""2022-07-24 06:15:34 +0000 UTC""
                  - droplet_id = ""309670716""
                  - id         = ""b82aeb19-78d8-4571-91e6-a0c2cffdb1db""
                  - name       = ""worker-pool-c1766""
                  - status     = ""running""
                  - updated_at = ""2022-07-24 06:19:09 +0000 UTC""
                },
              - {
                  - created_at = ""2022-07-24 06:15:34 +0000 UTC""
                  - droplet_id = ""309670715""
                  - id         = ""6b0d1ecf-4e48-427b-99a9-0e153056238d""
                  - name       = ""worker-pool-c176t""
                  - status     = ""running""
                  - updated_at = ""2022-07-24 06:18:27 +0000 UTC""
                },
              - {
                  - created_at = ""2022-07-24 06:15:34 +0000 UTC""
                  - droplet_id = ""309670717""
                  - id         = ""5ea0e536-96aa-4171-8602-dc0ab19e9888""
                  - name       = ""worker-pool-c176l""
                  - status     = ""running""
                  - updated_at = ""2022-07-24 06:18:27 +0000 UTC""
                },
            ] -> null
          - size              = ""s-2vcpu-2gb"" -> null
          - tags              = [] -> null
        }
    }
Plan: 0 to add, 0 to change, 1 to destroy.
Do you really want to destroy all resources?
  Terraform will destroy all your managed infrastructure, as shown above.
  There is no undo. Only 'yes' will be accepted to confirm.
  Enter a value: yes
digitalocean_kubernetes_cluster.test: Destroying... [id=49fd0517-a4a5-41e8-997d-1412c081e000]
digitalocean_kubernetes_cluster.test: Destruction complete after 1s
Destroy complete! Resources: 1 destroyed.
</code>
</pre></div><p id="""">Just like that, the cluster is gone.</p><h3 id="""">Summary</h3><p id="""">And there you have it. That's how you can manage Kubernetes clusters with Terraform. You used DigitalOcean Kubernetes for this purpose, but as mentioned before, the process will be exactly the same for other providers. You'll just need to initiate different providers in <strong id="""">provider.tf</strong> and then adjust the Kubernetes resource definition in <strong id="""">main.tf</strong>. It's best to follow Terraform documentation for that. You'll find examples and argument references for major cloud providers. </p><p id="""">Managing infrastructure with Terraform definitely helps you save time, but did you know that you can also easily spin up an environment on <a href=""https://release.com/"">Release</a> directly from your docker-compose file? <a href=""https://release.com/"">Give it a shot here</a>, and if you want to expand your Terraform knowledge further, take a look at our <a href=""https://release.com/blog/terraforms-for-each-examples"">post about for_each</a>. </p>",false,,,,https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63e41ab977c34a59d6ce9af5_081122%20(1).jpg,,regis-wilson,6,Thu Aug 11 2022 17:10:00 GMT+0000 (Coordinated Universal Time),,
Terraform vs Kubernetes: What's the Difference and Why It Matters,terraform-vs-kubernetes-whats-the-difference-and-why-it-matters,62aa5a70cd5ba27d9d0d718a,63f4ffcd6e31184735e576dc,Tue Feb 21 2023 17:30:53 GMT+0000 (Coordinated Universal Time),Thu Apr 25 2024 20:31:25 GMT+0000 (Coordinated Universal Time),Thu May 02 2024 20:19:24 GMT+0000 (Coordinated Universal Time),We'll look at Terraform and Kubernetes and how you can use them separately or together to automate and scale your cloud ,"<p id="""">Terraform vs. Kubernetes. Which tool is better? Which one do you need?&nbsp;</p><p id="""">Sometimes people confuse these two cloud automation tools, but it turns out they're not the same thing, and they're not in competition with each other. As a matter of fact, they work very well together. <a href=""https://www.terraform.io"" target=""_blank"">Terraform</a> is an <a href=""https://en.wikipedia.org/wiki/Infrastructure_as_code"" target=""_blank"">infrastructure as code</a> tool, while <a href=""https://kubernetes.io"" target=""_blank"">Kubernetes</a> is a <a href=""https://en.wikipedia.org/wiki/Orchestration_(computing"" target=""_blank"">container orchestration</a> system. Terraform can help you build and manage reproducible cloud infrastructure. Kubernetes can help you create more robust cloud applications.&nbsp;</p><p id="""">This post will explain what Terraform and Kubernetes are and how you can use them separately or together to automate and scale your cloud environments.&nbsp;</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1000px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1000px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63f4feb5215e9f6395996079_YgP04PmOYOAbU-wOc9kofsTNioI58Wy9d32Ona9DqHggT3AQBPmSfWFr3seAvzYUAvYRoU5-R7ToOvkRTNLm_p4T8xcOkxNo8t2u8ASOShO3eLWYU0CxojjgtN7qt-n2DObo4zxUg2X4f1VYRgyUWw.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><h3 id="""">Terraform</h3><p id="""">Terraform is <a href=""https://www.hashicorp.com"" target=""_blank"">Hashicorp's</a> infrastructure as code automation tool for provisioning and configuring cloud services. It has a consistent command-line interface (CLI) to more than 300 different cloud APIs. You describe your cloud environments in a declarative configuration syntax and then execute Terraform <a href=""https://www.terraform.io/docs/glossary#plan-verb"" target=""_blank"">plans</a> to build them. Hashicorp offers both open-source and licensed versions of Terraform.&nbsp;</p><p id="""">With Terraform you:&nbsp;</p><ul id=""""><li id="""">Increase your infrastructure automation and eliminate preventable mistakes.</li><li id="""">Use the same workflow to build environments in different public clouds</li><li id="""">Manage your infrastructure like code with configuration files, pull requests, and reviews</li><li id="""">Have a powerful tool for ensuring that your development, staging, and production environment are consistent.</li></ul><p id="""">Terraform uses Hashicorp Configuration Language (HCL) to describe infrastructure resources. The CLI reads these files and executes your plans using the cloud provider's native APIS.&nbsp;</p><p id="""">You can use Terraform to provision a wide variety of different tools via a robust plugin system. Plugins are called ""providers."" Hashicorp supports hundreds of official providers, <a href=""https://registry.terraform.io/browse/providers"" target=""_blank"">maintains an official registry</a>, and has an <a href=""https://www.terraform.io/cloud-docs/api-docs/private-registry/providers"" target=""_blank"">API</a> so you can write your own. They even have a Kubernetes provider that we'll discuss later.&nbsp;</p><h3 id="""">Terraform HCL</h3><p id="""">HCL is a user-friendly language that is easy to write and easy to read. If you understand the underlying infrastructure that you're using Terraform to provision, you can figure out HCL rather quickly. But it's not a restricted or limited programming language. It has variables, data types, and control flow.&nbsp;</p><p id="""">This is how you declare a string variable in HCL:&nbsp;</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-hcl"">
variable ""username"" {
  default = ""webmaster""
}
</code></pre></div><p id="""">This is an example of a list:&nbsp;</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-hcl"">
variable ""servers"" {
    default = [""webserver"", ""databaseserver"", ""backupserver""]
}
</code></pre></div><p id="""">Here's a for loop that traverses the list and outputs each name in upper case:&nbsp;</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-hcl"">
variable ""servers"" {
    default = [""webserver"", ""databaseserver"", ""backupserver""]
}

output ""forloop_terraform"" {
   value = [
      for name in var.servers : upper(name)
   ]
}
</code></pre></div><p id="""">Additionally, HCL has many powerful features, like maps, functions, and code libraries.&nbsp;</p><h3 id="""">Terraform Examples</h3><p id="""">Terraform is cloud-agnostic, but unfortunately, that doesn't mean you can write one HCL file and use it with more than one cloud. Each cloud provider had distinct APIs and very different configuration models.&nbsp;</p><p id="""">Here is code from the <a href=""https://learn.hashicorp.com/terraform?utm_source=terraform_io"" id="""">Terraform tutorial</a> for provisioning an Amazon Web Service (AWS) EC2 instance.&nbsp;</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-hcl"">
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 3.27""
    }
  }

  required_version = "">= 1.1.0""
}

provider ""aws"" {
  profile = ""default""
  region  = ""us-west-2""
}

resource ""aws_instance"" ""app_server"" {
  ami           = ""ami-830c94e3""
  instance_type = ""t2.micro""

  tags = {
    Name = ""ExampleAppServerInstance""
  }
}
</code></pre></div><p id="""">Here is a similar example for GCP:&nbsp;</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-hcl"">
provider ""google"" {
  project = ""terraform-examples-gcloud""
  region  = ""us-east1""
}

resource ""google_compute_instance"" ""example"" {
  name          = ""example""
  machine_type  = ""f1-micro""
  zone          = ""us-east1-b""
  
  boot_disk {
    initialize_params {
      image = ""ubuntu-1604-lts""
    }
  }
  
  network_interface {
    network = ""default""

    access_config {
      // Ephemeral IP
    }
  }
  
  tags = [""terraform-example""]
}
</code></pre></div><p id="""">You can separate your code into modules and separate variable declarations from code. This helps abstract out the difference between different cloud platforms.&nbsp;</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:871px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""871px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63f4feb5f2b9100132dafff0_0OLp5VivDb2vct5nZw-acFEkIpcMBo3nkxkG-ho0KrwpZTN3RoinzqmK71DrXESjI9F8a9YL-T1J1ZOU7vlSFjT6Xc_EyYU-gZ4xR7inZ21fhFFvKm-DjzQi_p31Aa8WhGLiJAjcjeyqdBC3-nUJXA.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><h3 id="""">Kubernetes</h3><p id="""">Kubernetes (<a href=""https://release.com/blog/why-kubernetes-is-so-hard"">K8s</a>) is an open-source container orchestration system. Like Terraform, it's an automation tool, but for containerized applications, not infrastructure. K8s automates deploying, scaling, and managing applications that run in containers. Usually, those containers are <a href=""https://www.docker.com"" id="""">Docker</a>.&nbsp;</p><p id="""">Google created K8s, but it is not limited to Google infrastructure. You can run K8 on-premises, on a cluster you build and manage yourself in the cloud, or on a cloud provider's implementation. GCP offers <a href=""https://cloud.google.com/kubernetes-engine/"" target=""_blank"">GKE</a>, AWS offers its own <a href=""https://cloud.google.com/kubernetes-engine/"" target=""_blank"">managed implementation</a>, and Azure has <a href=""https://azure.microsoft.com/en-us/services/kubernetes-service/#overview"" target=""_blank"">AKS</a>.&nbsp;</p><h3 id="""">K8s Benefits</h3><p id="""">When you run your app with Kubernetes, you get:&nbsp;</p><ul id=""""><li id=""""><strong id="""">Automated deployments:</strong> You tell K8s how you want your application to look, and it figures out how to get there gradually, without disrupting uptime. So, K8s can deploy or roll back your code for you.</li><li id=""""><strong id="""">On-the-fly and automatic scaling:</strong> You can tell K8s how much memory and CPU a container needs. Then it figures out how to fit them into the nodes you allocated to your cluster. K8s ability to make the most of your system resources is one of its most powerful abilities.</li><li id=""""><strong id="""">Storage management:</strong> K8s coordinates mounting and unmounting storage for your containers. It works with block storage, proprietary cloud systems, and more.</li><li id=""""><strong id="""">Dynamic load balancing:</strong> K8s makes containers available via an IP address or DNS name to the outside world. If traffic to a single instance reaches a designated threshold, it will distribute the load to more than one instance.</li><li id=""""><strong id="""">Configuration:</strong> K8s has tools for storing and managing configuration information, including secrets and tokens. These tools are a way to safely store secrets and push changes to applications without building a deploying new container.</li><li id=""""><strong id="""">Automatic recovery and failover:</strong> When containers fail or stop unexpectedly, K8s will restart them for you. You can provide a custom health check to K8s, and it will restart your container when the check fails.</li></ul><p id="""">So with Kubernetes, you get the portability of containers combined with a robust automated infrastructure that runs in the cloud.&nbsp;</p><h3 id="""">Terraform and Kubernetes Together</h3><p id="""">So, you can see how ""Terraform vs. Kubernetes"" isn't an entirely valid comparison. Terraform is an automation tool for building infrastructure. Kubernetes is an automation platform for running applications. They are both automation tools, and both can help you make your cloud applications easier to build, scale, and maintain. But, they solve different problems and are far from mutually exclusive.&nbsp;</p><p id="""">Hashicorp offers an official <a href=""https://registry.terraform.io/providers/hashicorp/kubernetes/latest/docs/guides/getting-started"" target=""_blank"">Kubernetes provider</a>. It's <a href=""https://github.com/hashicorp/terraform-provider-kubernetes"" target=""_blank"">open-source and available on Github</a>. The documentation is complete, and Hashicorp offers sample HCL scripts for <a href=""https://github.com/hashicorp/terraform-provider-kubernetes/tree/main/_examples/eks"" target=""_blank"">EKS</a>, <a href=""https://github.com/hashicorp/terraform-provider-kubernetes/tree/main/_examples/gke"" target=""_blank"">GKE</a>, and <a href=""https://github.com/hashicorp/terraform-provider-kubernetes/tree/main/_examples/aks"" target=""_blank"">AKS </a>with the course code.&nbsp;</p><p id="""">There are several reasons to provision a cloud Kubernetes cluster with Terraform instead of using their proprietary tools. The obvious reason is that Terraform's cloud-agnostic features make it easier to build clusters to other cloud providers, even if you consider the differences between systems. Even if you don't want to move to a different cloud provider, you may find it necessary to run on an alternative public cloud for disaster recovery. Another good reason is to make it easier to create different environments inside the same cloud provider. In both cases, using Terraform to build K8s clusters acts as a form of documentation. Instead of proprietary shell scripts or worse, steps in a web interface, you have HCL scripts that document how you built your k8s cluster.&nbsp;</p><p id="""">You can <a href=""https://learn.hashicorp.com/tutorials/terraform/kubernetes-provider"" target=""_blank"">manage a Kubernetes cluster with Terraform, too.</a>&nbsp;</p><h3 id="""">Cloud Automation Tools</h3><p id="""">We covered what Terraform and Kubernetes are, how they differ, and how you can use them together to make your cloud infrastructure easier to build and manage. Terraform is an automated infrastructure as code tool for provisioning and managing cloud infrastructure. It supports a large number of different platforms and services with a scripting language for describing the services you need and how Terraform is to provision them. Kubernetes is a container orchestration platform. You use it to automate deploying, scaling, recovering, and load-balancing your containerized applications. It runs on every major public cloud platform, as well as on-premises.&nbsp;</p><p id="""">Terraform automates infrastructure. Kubernetes automates containers and applications. Rather than competing, they complement each other. You can use Terraform to deploy your Kubernetes clusters and make it easy to reproduce them in different environments.&nbsp;</p>",false,,,,https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/640b46bd85f0c02f7b8dfc41_022123.jpg,Terraform vs Kubernetes,eric-goebelbecker,7,Tue Feb 21 2023 17:35:00 GMT+0000 (Coordinated Universal Time),kubernetes,
"How to Use Terraform's `for_each`, With Examples",terraforms-for-each-examples,62aa5a70cd5ba27d9d0d718a,62aa5a70cd5ba235660d72f7,Fri Jan 07 2022 00:32:29 GMT+0000 (Coordinated Universal Time),Fri Apr 21 2023 19:39:09 GMT+0000 (Coordinated Universal Time),Fri Apr 21 2023 20:04:43 GMT+0000 (Coordinated Universal Time),"Creating multiple resources with Terraform? Use for_each. In this post, you'll learn what is it as well as how and when ","<p id="""">Terraform is one of the most popular<a href=""http://en.wikipedia.org/wiki/Infrastructure_as_code"" id=""""> infrastructure as code</a> (IaC) tools. With Terraform, you can write code that defines the infrastructure components you want and the configuration for them. You then execute that code, and Terraform will make sure that your infrastructure is set up the way you defined it. This means either creating new components or responding with ""All of these are already created.""</p><p id="""">Sounds easy, right?</p><p id="""">In reality, Terraform code can be quite complicated. That's especially likely when you want to do some advanced stuff, like iterating over a certain number of resources.</p><p id="""">For example, let's say you want to create multiple virtual machines with the same configuration. For cases like that, in Terraform you can use `for_each`<strong id="""">. </strong>In this post, you'll learn what that is as well as how and when to use it.</p><h3 id="""">Infrastructure as Code With Terraform</h3><p id="""">If you're new to<a href=""https://www.terraform.io/"" id=""""> Terraform</a>, before we move on, you need to understand what it actually is.</p><p id="""">Modern environments are quite complex, and you have a few options when you want to add, change or destroy some components of your infrastructure. If you're on the cloud, you can go to your cloud provider web UI and execute any necessary action from there. You can also use CLI or even write some scripts yourself and call your cloud provider API.</p><p id="""">There are, however, some limitations to all these options. Clicking in the web UI doesn't scale. And you don't want to create dozens of different services every time you need a new environment.</p><p id="""">Using CLI or writing your own scripts is a step forward. But why not take two steps forward instead and use a dedicated infrastructure-as- code tool?</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1000px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1000px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/61df16bb0cc96700d8753d3f_jrnCWxDYFBw2f9n8Fzsk_uTfcee09M3UYLZNhI5BS3NlKZgyBNbldhjMNw-vSupV2e_TTpAaYEUOPesM_kZkxNBjuR4U-Iiz6UcsPlQejc55mEGhDbt8vPCviEutFHXwgApAx6kF.png"" width=""auto"" height=""auto"" loading=""auto"" id=""""></div></figure><p id="""">Terraform is one such tool. You write Terraform-specific code defining how you want your infrastructure to look. Then you execute Terraform and everything is taken care of for you. It's a highly efficient and scalable way of creating infrastructure.</p><p id="""">Also, one of the biggest advantages of using Terraform is that it will &nbsp;keep the state of your infrastructure saved. Therefore, it will always try to have your infrastructure in sync. So once you execute Terraform, it will only create, change or destroy resources that aren't in sync with the saved state.</p><h3 id="""">Terraform Meta-Arguments</h3><p id="""">Before we dive into explaining how `for_each` works, let's briefly talk about what it actually is.</p><p id="""">In Terraform, we mainly talk about resource blocks and module blocks. For example, when you want to create a virtual machine, you need to define a resource block with configuration details of that machine. Within the resource and module block, you can also use one of the five so-called meta-arguments. These are special instructions that aren't part of the resource configuration per se, but they instruct Terraform to do some action in relation to that resource. And one of these instructions is `for_each`.</p><p id="""">As I already mentioned, the main purpose of the `for_each` meta-argument is to create multiple instances of a resource. So, as you can imagine, it's quite useful to know.</p><p id="""">It's also worth mentioning that `for_each` has been added to Terraform in version 0.12. But I hope you've already upgraded to Terraform 1.x anyway.</p><h3 id="""">Multiple Resources</h3><p id="""">To understand better what purpose `for_each` serves, let's see how you could achieve the same outcome in Terraform without using `for_each`.</p><p id="""">The outcome we're talking about is deploying multiple copies of the same resource. So, let's take virtual machines, for example.</p><p id="""">Normally, to deploy more than one virtual machine, you'd have to specify multiple resource blocks, like this:</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yaml"">
resource ""google_compute_instance"" ""vm1"" {
name         = ""vm1""
machine_type = ""e2-small""
zone         = ""us-central1-a""
(...)
}

resource ""google_compute_instance"" ""vm2"" {
name         = ""vm2""
machine_type = ""e2-medium""
zone         = ""us-central1-a""
(...)
}

resource ""google_compute_instance"" ""vm3"" {
name         = ""vm3""
machine_type = ""f1-micro""
zone         = ""us-central1-a""
(...)
}
</code>
</pre></div><p>Seems like a lot of duplicated code, doesn't it? That's exactly where `for_each` can help.</p><p id="""">Instead of duplicating all that code for each virtual machine, you can define your resource once and provide a map or a set of strings to iterate over.</p><p id="""">Take a look at the example. This is how achieving the same results as above would look with `for_each`:</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yaml"">
resource ""google_compute_instance"" ""vm"" {
for_each = {
  ""vm1"" = ""e2-small""
  ""vm2"" = ""e2-medium""
  ""vm3"" = ""f1-micro""
}
name = each.key
machine_type = each.value
zone = ""us-central1-a""
(...)
}
</code>
</pre></div><p>As you can see, we defined the configuration parameters that differ per virtual machine as key-value pairs in the `for_each` block and left the parameters that are the same for each VM in the resource block. Then, we accessed the key-value pair by special keywords `each.key<strong id="""">`</strong> and `each.value<strong id="""">`</strong>.</p><p id="""">What if you want to pass more than just two (key and value) parameters? For example, what if you want to also parameterize the zone in the above example? You can simply change the value to a map, as follows:</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yaml"">
resource ""google_compute_instance"" ""vm"" {
for_each = {
  ""vm1"" = { vm_size = ""e2-small"", zone = ""us-central1-a"" }
  ""vm2"" = { vm_size = ""e2-medium"", zone = ""us-central1-b"" }
  ""vm3"" = { vm_size = ""f1-micro"", zone = ""us-central1-c"" }
}
name = each.key
machine_type = each.value.vm_size
zone = each.value.zone
(...)
}
</code>
</pre></div><p>You can pass as many parameters in the value as you want. Then in the actual resource configuration, you can reference them with `each.value.&lt;parameter_key&gt;<strong id="""">`</strong>.</p><p id="""">To keep your code clean and have the ability to reuse values for different resources, you can even extract the actual parameters into a variable:</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-yaml"">
locals {
virtual_machines = {
  ""vm1"" = { vm_size = ""e2-small"", zone = ""us-central1-a"" },
  ""vm2"" = { vm_size = ""e2-medium"", zone = ""us-central1-b"" },
  ""vm3"" = { vm_size = ""f1-micro"", zone = ""us-central1-c"" }
 }
}

resource ""google_compute_instance"" ""vm"" {
for_each = local.virtual_machines
name = each.key
machine_type = each.value.vm_size
zone = each.value.zone
(...)
}
</code>
</pre></div><h3 id="""">`for_each` Versus `count`</h3><p id="""">If you're not new to Terraform, you may have used another meta-argument that seems like the same thing: `count<strong id="""">`. </strong>And while `count<strong id="""">`</strong> also lets you create multiple instances of the same resource, there's a difference between `count<strong id="""">`</strong> and `for_each`. The latter isn't sensitive to changes in the order of resources.</p><p id="""">A common issue with `count<strong id="""">`</strong> is that once you delete any resource other than the last one, Terraform will try to force replace all resources that the index doesn't match.</p><p id="""">You don't have that problem with `for_each`<strong id=""""> </strong>because it uses the key of a map as an index. You can't use both `count<strong id="""">`</strong> and `for_each` on the same resources, but why would you anyway?</p><p id="""">Are there any drawbacks to `for_each`? Yes.</p><h3 id="""">Limitations of `for_each`</h3><p id="""">While `for_each` is pretty straightforward to use, there are some limitations you should be aware of. First of all, the keys in your `for_each` map block must have a known value. Therefore, for example, they can't be generated on the fly by functions (like `bcrypt<strong id="""">`</strong> or `timestamp<strong id="""">`</strong>). They also can't refer to resource-specific attributes that are provided by a cloud provider, like a cloud resource ID. Another limitation is the fact that you can't use sensitive values as arguments for `for_each`. Basically, when using `for_each`, you need to directly specify the values.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1000px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1000px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/61df16bb6640bbe9ac977955_ED9F9RQxWgDMpXC9hib0y449OopYlGqcQNMks3-OOValKAL0kQkWTUrUrzLRTQSyyLZvqbahYH6TWIvnqIow-B4ExbJZcrlgJ9kzTj5ighdIaHM7F79BcankVQvxqkfAzsmI1ny7.png"" width=""auto"" height=""auto"" loading=""auto"" id=""""></div></figure><h3 id="""">Summing up and Learning More</h3><p id="""">`for_each` is probably one of the most commonly used Terraform meta-arguments. Modern environments usually consist of multiple instances of resources for high-availability and scalability reasons. Using `for_each` is relatively easy, but you need a solid understanding of how it works to get the most benefits from it. It also has its own limitations.</p><p id="""">In this article, you learned how `for_each` works and got some tips on how to use it efficiently. Now, you can try to play around with it yourself or look into<a href=""https://www.terraform.io/language/meta-arguments/depends_on"" id=""""> other meta-arguments</a>.</p><h3 id="""">Terraform at Scale</h3><p id=""""><a href=""https://hubs.li/Q011Rfby0"" id="""">Release</a>’s Environment-as-a-Service, is an easy to use, highly scalable service that leverages Terraform to create snapshots of even the most complex environments and automatically manages their creation and teardown as part of your development lifecycle.</p>",false,,,,https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63e3fbe867257e35a8a8f9ef_012822%20(1).jpg,photo of computer code ,dawid-ziolkowski,5,Fri Jan 28 2022 16:22:00 GMT+0000 (Coordinated Universal Time),,
Test Blog,test-blog,62aa5a70cd5ba27d9d0d718a,6508774b161a59e261bbe845,Mon Sep 18 2023 16:14:03 GMT+0000 (Coordinated Universal Time),Wed Oct 11 2023 21:47:02 GMT+0000 (Coordinated Universal Time),,Let’s explore a significant decision in every software endeavor: do we build or do we buy?,"<p id="""">Previously, we wrote about the goals and outcomes of <a href=""https://release.com/blog/what-is-an-internal-developer-platform-and-why-should-i-have-one"" id="""">an Internal Developer Platform (IDP</a>), as well as the <a href=""https://release.com/blog/components-of-a-successful-idp-build-a-product-your-developers-actually-want-to-use"" id="""">components of a successful IDP</a>. At this point, you may be either excited by or overwhelmed with where your platform could go and the efficiencies that it could create.</p><p id="""">As the next step, let’s explore a significant decision in every software endeavor: do we build or do we buy?</p><p id="""">Considering this is rarely a clean “yes or no” question, we’re also going to look at the options that sit between the two. In actuality, the solution you choose will be somewhere on a large spectrum between a complete build from scratch and entirely buying software that you never have to maintain yourself. Where your org falls on that spectrum depends on your current needs and capabilities.</p><p id="""">First, let’s look at some of the options available.</p><blockquote id=""""><em id="""">Use code #IDP to get free Release Ephemeral Environments for 30 days. Sign up </em><a href=""https://release.com/signup"" id=""""><em id="""">here</em></a><em id="""">. </em></blockquote><h4 id="""">Build or Buy, or Else?</h4><p id="""">When we make decisions around build vs. buy, we should first realize that when it comes to an IDP, it’s not all or nothing. This is a spectrum, where you can buy a managed IDP and all related components or you can pick and choose what you build and what you buy. As a reminder, an IDP isn’t one solution, but a platform with various components that provide solutions to different problems. Therefore, you can choose to build or buy the platform and the separate components.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1000px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1000px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/65008cf4824e9f9cb24dbc05_jkl6g0bDWEUvpq4FGnZYb7UdnE4yq5FQK6z5i-SeBeDbuMJD5uJjPA54m_uCEtIiuXHqHhv3fBtTwkDxXIgw6MZlJDP76sz2IBXVnNNJAPyT51VKyoTf2fLH-3yGMx9nLP0vZBRvY4mOagj0SgTViXk.png"" id="""" width=""auto"" height=""auto"" alt="""" loading=""auto""></div></figure><p id="""">‍<strong id="""">🛠️ Fully Build the IDP and Integrations</strong></p><p id="""">The most involved option would be to build everything yourself from scratch. This will be the most expensive option and will require a large team, but in theory, it will give you the most flexibility.</p><p id="""">Spotify did this when they built Backstage. Now, you may think that that’s the way to go. But do realize that Spotify had a large number of people working on this both internally and later externally when the platform was released as open source. Once they went open source, they had both individual contributors and partner companies that provided time and engineering efforts toward the build. Their<a href=""https://backstage.io/blog/2022/03/16/backstage-turns-two/"" id=""""> two-year anniversary called out 5,000 contributors</a> to the project.</p><p id="""">Most companies do not have hundreds, let alone thousands, of developers who can put in time and effort to building an IDP from scratch. In addition to the large number of resources, that level of contribution requires forgoing other projects within your organization that involve shipping features. As importantly, this platform is just that. A platform. Once you have the platform built, you will then need to configure the necessary components and add-ons that your organization needs. Each of these components will result in a build vs. buy decision, so focus on the most important features for your organization first.</p><h5 id="""">‍<strong id="""">🏗️ Build Atop Open-Source Frameworks and Commercial Components</strong></h5><p id="""">If you’re not at the stage where you can build everything (and most organizations aren’t), another option would be to take an open-source framework and build on top of that.</p><p id="""">In this scenario, you’re using something like backstage.io but building out the implementation. </p><p id="""">This is a better option over a complete build as the base framework. However, you’ll still need to build integrations and quickly form opinions on how this should work for your organization.</p><p id="""">Many prefer this to the build option, but it will still take a considerable team to make it happen. And it’s not the ideal choice if you’re just starting to learn about IDPs and how they can help your org.</p><p id="""">For example, companies could choose a managed observability component like Datadog or New Relic. These tools aren’t cheap but will get your organization started with solid monitoring and observability. However, as costs increase, you could find yourself building your observability tools over open-source libraries like Prometheus and Grafana.</p><p id="""">Coming at this from a similar angle, you could handle your environment management in-house using Ansible, Terraform, or Docker Compose. You might begin by building out your environment management using these tools as the solution seems simple. As the complexity of your environment management increases, and as the problems you need to solve become more sophisticated, choosing an ephemeral environment solution like Release can reduce the complexity for your teams.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1000px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1000px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/65008cf48ff0af7d98cbcdae_h5OYdEvOgxY3fAU70Bm96rICWGnTnLjl3pNt73hrRgRH2PSwplHw1EXMpbTMXfH0Iy_J5c4kLt_2zEUvXNLP4Y_5XmfjmdZWacSsktmZw_KIHjB1Z7OHDvltxaFChxNGdyhTfSurfw9gswOJ1IgUoH0.png"" id="""" width=""auto"" height=""auto"" alt="""" loading=""auto""></div></figure><h5 id=""""><strong id="""">🤝Hire Someone to Build It</strong></h5><p id="""">For some software projects, you could consider hiring out the build. For example, you could hire third-party contractors or consultants to build your IDP. Or you could directly hire new employees with the necessary experience and onboard them into your organization to build it out.</p><p id="""">Organizations will take this option if they don’t have the necessary knowledge in-house. Though this can work, it often has poor outcomes. The folks coming into the org don’t know the culture and processes of your teams and will need to learn them, or they’ll build something not based on your internal culture and processes. Frequently, for third-party development teams, they don’t have the level of ownership needed for projects of this size. And hiring new employees for this work will also increase the burden on hiring resources. And if we don’t have existing employees with this expertise, we will oftentimes make expensive hiring decisions.</p><p id="""">To mitigate some of the risks with these options, you could have a small number of experts build alongside your team so that you have more of your own long-term workforce working on the project than you have temporary parties. That would be the only way I’d consider this option.</p><h5 id="""">‍<strong id="""">💰Buy It: Vendor-Managed Options</strong></h5><p id="""">Buying enterprise software isn’t a plug-it-in-and-forget-it endeavor. There will still be work involved in configuration, management, and onboarding teams. This is even more true for IDPs, as they are simply platforms and will need the right components and integrations added on to make a usable product.</p><p id="""">Even though there’s not a complete buy-it option, buying the right components that provide value to your org will reduce the development burden on your teams. You’ll be able to focus on the integrations that solve your organization’s biggest development workflow pain points, which can free you up to decide how you want to incorporate the use of the IDP into your organizational culture.</p><p id="""">Additionally, for an IDP, you have managed solutions available for a number of components. For example, there are organizations that provide managed IDP solutions like <a href=""https://www.cortex.io/"" id="""">Cortex</a> or <a href=""https://www.opslevel.com/"" id="""">OpsLevel</a> for service catalogs and integrations to other tools.</p><p id="""">You can also consider whether it makes sense to buy components and capabilities that extend your IDP. Let’s consider the monitoring and observability components of your IDP. We already mentioned purchasing products like Datadog or New Relic. You can further this by adding paging options like <a href=""https://www.pagerduty.com/"" id="""">PagerDuty</a> or <a href=""https://www.atlassian.com/software/opsgenie"" id="""">Opsgenie</a>. These problems have robust solutions on the market that can fill your organization’s needs.</p><p id="""">When it comes to environment management, Release provides a managed platform as a service (PAAS) solution for managing both standard and ephemeral environments. This provides additional building blocks for your IDP, adding environment management capabilities to further increase your development team efficiency. And yes, you could build it yourself, scripting atop open-source solutions like <a href=""https://www.terraform.io/"" id="""">Terraform</a> or <a href=""https://docs.docker.com/compose/"" id="""">Docker Compose</a>. But again, these types of products are not something that will take a few days and still scale well.</p><p id="""">When considering what components to build or buy, consider how the value compares to the effort. What components can provide big benefits to your engineering teams that will take a lot of time and effort to build? And consider your most pressing needs. Do you need a central place to locate tools and services? How about robust monitoring tools to operate your systems? Or environment management to increase developer velocity? And which do you want to benefit from quickly?</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1000px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1000px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/65008cf4c6f0a6091603b5e8_zlfj-HbCvSHT4dpzP9VtND1UNfW0v1cvMQfD319E9gkLIGhubltz5Jv4PZql8X1oN85tZ1-uYXlxoBiFrvlC4xp_M7cUN3vRapNwm5sC_P2eTBu86Z5fkET9x1Mddu-giE-kPMVBfiDV25Kl6bD7xtM.png"" id="""" width=""auto"" height=""auto"" alt="""" loading=""auto""></div></figure>",true,"<p id="""">Take Release for a spin.<br>First 30 days are on us with code #LEARN</p>",https://release.com/signup,"<h4 id="""">Build vs. Buy Factors to Consider</h4><p id="""">We’ve gone over many alternatives in the build, build on open source, hire out, and buy spectrum. There isn’t a one-size-fits-all solution, and you’ll have to consider what’s best for your organization.</p><p id="""">But there are a number of things to consider when deciding if you will build or buy your IDP.</p><h5 id="""">1. Cost</h5><p id="""">Cost isn’t just the amount of money that you’ll spend on the software licenses or the developer headcount to build the IDP. If you buy software, you’ll still have to spend time on training, evangelizing, and configuring the IDP for your organization. If you build, you’ll need to include engineering time as well as the training, evangelizing, and configuring just like when you buy. Building also includes opportunity costs. What could your engineering staff be building to further differentiate your product in the market? Should you focus their energies on your product or on integrations that all organizations need?</p><h5 id="""">2. Expertise and Capabilities</h5><p id="""">Building software like an IDP requires a team with the right skills and expertise. And don’t think your team of five production engineers can take on this effort. Organizations like Netflix, Facebook, and Spotify can build tools like this in-house because they have hundreds, if not more, of employees working on internal systems and efficiencies. These folks are dedicated to improving the developer experience.</p><p id="""">If you have a small development team focused on application or product development, developing a <a href=""https://engineering.fb.com/category/production-engineering/"" id="""">production engineering</a> or platform enablement team will not work well. It won’t take advantage of their expertise in your domain and will take them away from building the product that is your market differentiator.</p><h5 id="""">3. Scalability</h5><p id="""">With the build option, you do control the ability to scale the needs of the system yourself to your specific loads and use cases. However, consider whether you could start with buying a managed IDP option and then move over to build after you’ve outgrown the capabilities and scale of that option.</p><h5 id="""">4. Custom Integrations</h5><p id="""">If your organization has its own custom-built tools for testing, security, governance, or more, integrations through an IDP may be more difficult. Or you’ll potentially have the same amount of effort between the build and the buy option.</p><h5 id="""">5. Security and Compliance</h5><p id="""">When you build the software, you can control the security and compliance features. But that also means you must be the experts in security and compliance. This loops back to #2. Do you have the expertise, or do you need to offload that need for expertise to another organization where it’s their product differentiator?</p><h5 id="""">6. Competitive Advantage</h5><p id="""">If you’re an org like Spotify, Netflix, Meta, or similar, then squeezing every bit of efficiency out of your development team by ensuring everything is custom-built for their needs makes sense. But if you’re not at that scale, consider starting smaller.</p><h5 id="""">7. Usability and Adoption</h5><p id="""">If you build a custom IDP but don’t consider usability, then you will have poor adoption. This means not only UI concerns of the IDP but also usability of configurations and interactions with the developer workflow.</p><h5 id="""">8. Support</h5><p id="""">Custom-built solutions won’t have an internet full of adopters that use the tool or similar tools for debugging, tutorials, and instructions. Buying software solutions through a vendor-managed option means you don’t have to create a large support network and can offload much of that to the wider developer ecosystem.</p><h5 id="""">9. Course Correction</h5><p id="""">One final factor involves transitioning from one decision to another. Consider this. Would it be easier and more economical to move from a “buy” solution that doesn’t meet your needs or from a “build” solution that fails? If you decide to build but find that this is not maintainable or salable, then the effort put into building will be wasted.</p><p id="""">If you find that a buy solution doesn’t work for your needs, yes, you’ll still have sunk costs, but they should be more manageable. Depending on the components involved and your expertise, your answer can vary.</p><p id="""">However, if you start with a buy solution or use a PaaS or managed solution, you can transition to build when you’re ready. You’ll build up the experience needed and the understanding of what you need from the product. For something like an IDP that consists of many integrated components, you can build piece by piece, taking on more of the build as your organization’s expertise and understanding of the component grows.</p><h4 id="""">Making a Decision</h4><p id="""">Making the right decision depends on many factors that vary across organizations. Consider what’s best for your situation today and focus on solving for the 80% before deciding to move to a build solution. This may consist of a combination of building, building on top of open source, buying, or hiring out.</p><p id="""">In the next installment of this series, we’ll talk about using product thinking to establish your IDP. Stay tuned!</p><p id="""">‍<em id="""">This post was written by Sylvia Fronczak. </em><a href=""https://sylviafronczak.com/"" id=""""><em id="""">Sylvia</em></a><em id=""""> is a software developer who has worked in various industries with various software methodologies. She’s currently focused on design practices that the whole team can own, understand, and evolve over time.</em></p><p>‍</p>",https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/6500a31129b89ac202977d7c_Buid%20vs%20Buy.jpg,Credit: Google DeepMind,sylvia-fronczak,10,Tue Sep 12 2023 21:00:00 GMT+0000 (Coordinated Universal Time),,
Test Environment: A Definition and How-to Guide,test-environment-a-definition-and-how-to-guide,62aa5a70cd5ba27d9d0d718a,63a2b9c43f0f3876cb568e8e,Wed Dec 21 2022 07:46:12 GMT+0000 (Coordinated Universal Time),Thu Apr 20 2023 21:30:39 GMT+0000 (Coordinated Universal Time),Thu Apr 20 2023 21:44:19 GMT+0000 (Coordinated Universal Time),What a test environment is and how to set one up. ,"<p id="""">Developers today are under increasing pressure to produce high-quality software, with 89% of companies now competing primarily on <a href=""https://www.smartkarrot.com/resources/blog/customer-experience-statistics/#:~:text=General%20customer%20experience%20statistics&text=89%25%20of%20businesses%20compete%20primarily,warm%20and%20friendly%20customer%20experience."" target=""_blank"">customer experience</a>. Today, customers expect high-quality software that’s secure, reliable, and easy to use. When you fail to meet those expectations, adoption of your product is likely to plummet.&nbsp;</p><p id="""">To ensure software quality, developers frequently choose to set up test environments where they can safely isolate code and analyze performance. Simply put, test environments help to reduce production errors, lower development costs, and improve customer satisfaction. &nbsp;</p><p id="""">This post offers a primer on test environments covering what a test environment is, how it fits into the software development life cycle, best practices for managing a test environment, and a step-by-step guide for setting one up. &nbsp;</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63a2b81bfa137f9d853d00e0_unnamed.jpg"" loading=""lazy"" id="""" width=""auto"" height=""auto""></div></figure><h3 id="""">What is a Test Environment?</h3><p id="""">A test environment is a dedicated space—usually a server or container—for <a href=""https://www.forbes.com/sites/forbestechcouncil/2022/10/04/how-to-improve-the-quality-of-software-testing/"" target=""_blank"">running software tests</a>. You can add a copy of an application to a test environment to inspect it for bugs and vulnerabilities. Usually, creating a test environment involves setting up a physical or virtual server and configuring it according to the specifications of the application that’s being tested. &nbsp;</p><p id="""">A dedicated test environment helps you ensure accuracy and consistency when code is analyzed. Engineers can alter code and experiment with it in the virtual environment and assess the results in real-time. For example, a developer could use a test environment to check the compatibility between different code versions and address any underlying bugs.&nbsp;</p><p id="""">Some projects may call for a single test environment while others may require multiple test environments for each access instance. Depending on your needs, you can either run tests in a fixed order or simultaneously in multiple environments. </p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1000px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1000px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63a2b850e938101c93e2b355_unnamed.png"" loading=""lazy"" id="""" width=""auto"" height=""auto""></div></figure><h3 id="""">What are the Different Environments Testing is Done in?</h3><p id="""">Many developer teams use dedicated testing environments, but you can also test your software in different kinds of environments. Let's explore what these testing environments are.</p><h4 id="""">Development Environment</h4><p id="""">The development environment is where engineers write code and make changes. Preliminary testing also takes place in the development environment before advancing code to other stages. To illustrate, an engineer might set up a development environment to build an application and create a sample for additional testing and modifications.&nbsp;</p><h4 id="""">Staging Environment </h4><p id="""">After software has been through development and testing, it moves into a staging environment. The staging environment closely resembles live production. The main purpose of the staging environment is to provide a window of insight into how the software will perform after deployment. &nbsp;</p><p id="""">Companies often use staging environments to present final projects before moving them along for final testing and production. Staging environments also provide a chance for stakeholders to see a digital project in action, collaborate, and make adjustments. &nbsp;</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1000px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1000px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63a2b88bf3c0d818f8a3effc_unnamed%20(1).png"" loading=""lazy"" id="""" width=""auto"" height=""auto""></div></figure><h4 id="""">Production Environment </h4><p id="""">The production environment is the final stage of software development, when the software is live and running on a production server. At this point, users can access and interact with the software in real-time. &nbsp;For example, all software that you use—like Slack or Teams—is currently in a live production environment. &nbsp;</p><p id="""">Even though the software is live at this point, companies often continue to test and iterate on it in the production environment. Testing in production can provide better accuracy and allow for more frequent deployments.&nbsp;</p><h3 id="""">Why Should You Use a Software Testing Environment?</h3><p id="""">While testing in production is possible, there are several advantages to setting up a private, dedicated testing environment. &nbsp;</p><h4 id="""">Support Different Tests</h4><p id="""">It’s possible to run several different types of analysis within a testing environment, depending on your needs. For example, you can set up a test environment to run integration tests, unit tests, <a href=""https://release.com/blog/user-acceptance-testing-checklist"">user acceptance tests</a>, performance tests, alpha tests, and beta tests. &nbsp;</p><h4 id="""">Save Money </h4><p id="""">In general, testing software earlier in the development life cycle eliminates rework, enables you to make changes more easily, and significantly reduces development costs. &nbsp;</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1000px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1000px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63a2b8bc3f0f382c76568939_unnamed%20(2).png"" loading=""lazy"" id="""" width=""auto"" height=""auto""></div></figure><h4 id="""">Ensure Product Quality</h4><p id="""">All too often, vulnerabilities are discovered too late in the development process and inadvertently pushed into production. With a testing environment in place, you can address underlying bugs and vulnerabilities more easily, resulting in a better quality product and fewer user complaints.&nbsp;</p><h4 id="""">Improve Testing Accuracy</h4><p id="""">A dedicated test environment ensures greater accuracy and consistency when you analyze code because tests can be conducted in isolation from other activities and resources that may interfere with test results.</p><h3 id="""">What is a Good Test Environment? Best Practices for Managing Test Environments</h3><p id="""">There is no single way to set up and manage test environments—developers tend to use different strategies depending on their workflows and preferences. However, here are a few best practices for managing test environments that can help you keep your test environments safe, effective, and efficient. &nbsp;</p><h4 id="""">Document Your Test Environment </h4><p id="""">Thoroughly document the steps you take to set up test environments. Good documentation lets other team members replicate environments and ensures consistency. &nbsp;</p><h4 id="""">Outline Testing Goals </h4><p id="""">Outline your specific testing goals before moving code into a test environment to save time, prevent rework, and enable testing engineers to move with speed and confidence. &nbsp;</p><h4 id="""">Secure Your Environment</h4><p id="""">Test environments contain a wealth of data and can pose security risks. To <a href=""https://www.cio.com/article/219952/10-things-cios-must-absolutely-know-about-their-software.html"" target=""_blank"">keep risk low</a>, it’s critical to properly secure your test environment and restrict access through effective user authentication protocols.&nbsp;</p><h4 id="""">Don’t Strive for Perfection</h4><p id="""">It isn't always possible to create test environments that align perfectly with production environments. Attempting to create test environments that match production perfectly will only waste time and delay development. Test environments don’t have to be an exact match of production to perform well. &nbsp;</p><h3 id="""">How to Create a Test Environment</h3><p id="""">Creating a test environment is a multistep process, which typically involves the following actions. &nbsp;</p><h4 id="""">1. Determine Your Objectives </h4><p id="""">The first step is to clearly define your objectives and outline your reasons for creating a test environment. Before moving on, make sure that a test environment is necessary for your specific use case.&nbsp;</p><h4 id="""">2. Select an Environment </h4><p id="""">The next step is to pick a physical or virtual environment. At this point, you must determine whether you want to use the cloud or an on-prem server. &nbsp;</p><h4 id="""">3. Optimize Your Network</h4><p id="""">A poorly performing network can negatively impact testing and lead to inaccurate results. It’s worth taking time to make sure that your network is optimized and secure.&nbsp;</p><h4 id="""">4. Plan Your Data Strategy</h4><p id="""">Test data plays a critical role in the software development process. You should have a plan in place for collecting, analyzing, and securing the data that you collect during software tests. &nbsp;</p><h4 id="""">5. Automate Test Environments to Improve Efficiency </h4><p id="""">Most businesses now use automation to manage their test environments. For example, Jenkins is a popular free open-source tool that enables automated testing. </p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1000px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1000px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63a2b8f81084672248a5ab23_unnamed%20(3).png"" loading=""lazy"" id="""" width=""auto"" height=""auto""></div></figure><h3 id="""">Save Time with On-Demand Environments </h3><p id="""">Without a doubt, test environments are a crucial part of the software development process. However, they can take time to <a href=""https://release.com/blog/setup-test-environment"">set up and maintain</a>, which can slow things down and frustrate engineers. &nbsp;</p><p id="""">To save time, Release now offers on-demand environments as a service. With the help of Release, your team can instantly deploy fast, scalable, managed environments that are easy to replicate and access. Improve accuracy and reliability and free your developers to spend more time analyzing and optimizing code. &nbsp;</p><p id="""">For further reading on how to streamline development and build better applications, check out Release’s <a href=""https://release.com/ebook/the-complete-guide-to-automated-software-environments"">complete guide to automated software environments</a>.&nbsp;</p>",false,,,,https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63e3f5c43f98020ad31f3fff_010323%20(1).jpg,A hand holding a magnifying glass over a laptop keyboard,erik-landerholm,6,Tue Jan 03 2023 17:13:00 GMT+0000 (Coordinated Universal Time),,
Test Guild Webinar: The Secret to Continuous Testing,test-guild-webinar-the-secret-to-continuous-testing,62aa5a70cd5ba27d9d0d718a,62aa5a70cd5ba2358c0d730e,Sun Mar 20 2022 21:22:28 GMT+0000 (Coordinated Universal Time),Fri Apr 21 2023 19:09:49 GMT+0000 (Coordinated Universal Time),Fri Apr 21 2023 19:37:52 GMT+0000 (Coordinated Universal Time),Tommy McClung joins Joe Colantonio on the Test Guild webinar to discuss the secret ingredient to continuous testing.,"<p id="""">Tommy McClung joins Joe Colantonio on the <a href=""https://vimeo.com/680498142/7cea1c8bf0"" target=""_blank"" id="""">Test Guild webinar</a> to discuss the secret ingredient to continuous testing.</p><p id="""">Tommy, the co-founder and CEO of Release, has been building scalable infrastructure for at least over 20 years. He’s also a serial entrepreneur. He co-founded CarWoo!, which was acquired by TrueCar.</p><p id="""">If you want to know the secret to continuous testing, including people and processes, and why environments are key to many of the bottlenecks that engineers and developers face, you’re in the right place.</p><p id="""">Tommy started his career at RLX Technologies, the inventor of the blade server, which was later sold to Hewlett Packard. Back then, there were no virtual machines, and the way that developers could improve productivity consisted of squishing as many servers into a rack as you possibly could. Then developers had to have software that could make it all work. At RLX technologies, they had installations of thousands and thousands of blade servers.</p><p id="""">Later on in his career, Tommy went on to start CarWoo!, an automotive company, which was later sold to TrueCar. At TrueCar, he was tasked with building a new product, and encountered many issues with environments. TrueCar had a very large QA team that was very dependent on environments. It was very difficult to build, deploy, and test code. The idea of continuous integration and continuous testing was not even possible because of the ecosystem and how few environments they had. Later, as CTO of TrueCar, Tommy set out to solve this environments issue so that his team could do true continuous deployment, continuous integration, and continuous testing.&nbsp;</p><p id="""">Tommy has been working on this problem for many years. As a developer at heart, he cares about getting great ideas to the world quickly, and environments tend to be a key ingredient in making that happen. This is why he created Release and the concept of ephemeral environments, or “environments as a service”. With Release, developers can create on demand environments with the click of a button or via a pull request.</p><p id=""""><a href=""https://vimeo.com/680498142/7cea1c8bf0"" target=""_blank"" id="""">Watch the webinar </a>below to see how on-demand environments work. Learn more about the variety of <a href=""https://release.com/use-cases"" id="""">use cases for ephemeral environments</a>: staging, production, QA, integration environments, customer facing environments, sandbox environments, and more. Environments are needed throughout the entire product development life cycle.</p><div data-rt-embed-type='true'><div style=""padding:56.25% 0 0 0;position:relative;""><iframe src=""https://player.vimeo.com/video/680498142?h=7cea1c8bf0&amp;badge=0&amp;autopause=0&amp;player_id=0&amp;app_id=58479"" frameborder=""0"" allow=""autoplay; fullscreen; picture-in-picture"" allowfullscreen style=""position:absolute;top:0;left:0;width:100%;height:100%;"" title=""Release Webinar March 1""></iframe></div><script src=""https://player.vimeo.com/api/player.js""></script></div><p>‍</p><p id="""">Want to try Release for yourself? <a href=""https://release.com/"" id="""">Request a demo.</a></p>",false,,,,https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63e3fee690f3c93ac4c274f8_032122%20(1).jpg,The Secret Ingredient to Continuous Testing Webinar,sam-allen,2,Mon Mar 21 2022 21:21:00 GMT+0000 (Coordinated Universal Time),,
Testing Environment Types and What They're Used For,testing-environment-types-and-what-theyre-used-for,62aa5a70cd5ba27d9d0d718a,63bbc919fb087e73d325b255,Mon Jan 09 2023 07:58:17 GMT+0000 (Coordinated Universal Time),Thu Apr 20 2023 21:02:56 GMT+0000 (Coordinated Universal Time),Thu Apr 20 2023 21:44:19 GMT+0000 (Coordinated Universal Time),"Check out this article to learn about different testing environment types, what they're used for, and which one is right","<p id="""">The testing environment is crucial for the success of any software project. It allows developers to test their code against a variety of conditions, including different operating systems, hardware configurations, and user scenarios. This makes it possible to identify and fix bugs before the software is released to the public.&nbsp;</p><p id="""">A well-designed testing environment can also help improve the quality of the code. By forcing developers to test their code under different conditions, they're more likely to write robust and error-free code.&nbsp;</p><p id="""">In this blog, we'll look at different types of testing environments and what they're suitable for. There's no one-size-fits-all solution. We need to understand where and how to invest our time and effort.&nbsp;</p><h3 id="""">What is a Testing Environment?</h3><p id="""">A testing environment is a separate environment used for testing purposes. It's a set of conditions created to test a software application or system. It typically includes test data, tools, software, and the necessary hardware and networking infrastructure.&nbsp;</p><p id="""">A testing environment should be as similar as possible to the production environment in order to provide accurate results. However, it's often not possible or practical to create an exact replica of the production environment. Therefore, it's essential to carefully consider which aspects of the production environment are most important for testing and to create a testing environment that accurately reflects those aspects.&nbsp;</p><p id="""">Creating a <a href=""https://release.com/blog/setup-test-environment"" target=""_blank"">suitable testing environment</a> is critical to the software development process and can significantly impact the final product's quality.&nbsp;</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1000px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1000px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63bbc8abfb087ec91625a869_8d1R8AYQtpy407q7Rr08SOlSkx_eqvhC47xy0zsT9O0Gr4QMOHcU4_egugrRpnblQJvIdpwxMNkwnYXlNKhbZbzDZ1CWbwAcS9puCiRGuuZBY43_HBcY2tyEZCkmoGOD271AUpJ2l8FEHIY8NfrlrWvMX6nNHCNLFmYMdK5p9rvYIuw7eHzt-hxW8RGJ.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><h3 id="""">Understanding Testing Environment Types</h3><p id="""">There are many different testing environments, each with its advantages and disadvantages. Some of the most common types of testing environments include:&nbsp;</p><h4 id="""">Security Testing Environment</h4><p id="""">A security testing environment is a controlled environment where security testing can be conducted. This environment includes the necessary tools, resources, and personnel to safely and effectively conduct security testing.&nbsp;</p><p id="""">A security testing environment should be designed to mimic the production environment as closely as possible to accurately assess the system's security. To this end, the security testing environment should include the same hardware, software, data, and network configurations as the production environment.&nbsp;</p><p id="""">This environment should be isolated from the production environment to prevent any potential damage to the system under test. This isolation can be accomplished through the use of physical or logical separation.&nbsp;</p><p id="""">A security testing environment should also be well-documented so that all stakeholders can understand the purpose and scope of the testing being conducted. This documentation should include a list of all tools and resources used and a detailed testing plan.&nbsp;</p><h4 id="""">Performance Testing Environment</h4><p id="""">A performance testing environment is a testing setup used to assess a software application's performance. This testing is typically used to evaluate the response time of an application under various load conditions.&nbsp;</p><p id="""">This environment typically includes a test client, a test server, and a network. The test client is used to generate load on the test server. The test server is used to host the application under test. The network is used to connect the test client and the test server.&nbsp;</p><p id="""">An effective performance testing environment can help developers optimize their code for better performance and reliability. It also helps identify potential improvement areas in the development process itself.&nbsp;</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1000px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1000px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63bbc8aba5f15f1560873465_0uAJRO9QEENLj8-CcIKefqXtfGDTbL4O3h40rkyWE21wa6qLyvHbzegUTgVGUZQi3bbnKsS9womy56miC_aENvp77OJWrJRqaY_3CAjyqp124KHOaO2jpVPF75mHv1_L3xPwwp4kzqc2g5h4Hr4S5UR4iKCPrtNMZufJRyzW0ENmSOzW4fBOhFItkgQa.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><h4 id="""">System Integration Testing</h4><p id="""">System Integration Testing (SIT) is software testing that verifies the interactions between various system components. It's typically performed after all individual unit tests have been completed to ensure that the multiple components of the system work together as intended. SIT can be used to test interfaces between different software applications and hardware and software components.&nbsp;</p><p id="""">SIT is an essential step in the software development process, as it can help identify errors and potential problems that may not have been apparent during unit testing. By testing the interactions between different system components, SIT can help ensure that the system is functioning correctly.</p><p id="""">SIT can be a time-consuming and complex process, particularly for large and complex systems. However, SIT is essential to ensure the system's quality before it is deployed.&nbsp;</p><h4 id="""">User Acceptance Testing</h4><p id="""">User acceptance testing (<a href=""https://en.wikipedia.org/wiki/Acceptance_testing#User_acceptance_testing:~:text=product%20provider/developer).-,User%20acceptance%20testing,-%5Bedit%5D"" target=""_blank"">UAT</a>) verifies that a software application meets the business requirements of the end user. It's the last stage of testing before the software is released to the end user. It's also known as beta testing or end-user testing.&nbsp;</p><p id="""">The goal of UAT is to ensure that the software application is fit for purpose and meets the user's expectations. UAT aims to identify any issues that may impact the user's ability to use the software application effectively. UAT is typically conducted by the end user, but it can also be performed by a third-party testing company.&nbsp;</p><p id="""">UAT is a vital part of the software development process and should be given the same attention as other types of testing, such as functional and system testing. It should be conducted in a controlled environment, such as a test lab, to ensure that any issues that are identified can be effectively managed.&nbsp;</p><p id="""">By conducting <a href=""https://release.com/blog/user-acceptance-testing-best-practices"">UAT</a>, organizations can ensure that the software application meets the end user's needs and that any issues are identified and resolved before the software is released to the end user. As such, it shouldn't be overlooked.&nbsp;</p><h4 id="""">QA Environment</h4><p id="""">A quality assurance environment (QA environment) is a set of hardware and software tools used to test the functionality of a computer system or application. QA environments simulate real-world conditions so that developers can identify and fix bugs before a product is released to customers.&nbsp;</p><p id="""">There are many different types of QA environments, and the tools used in each environment vary depending on the project's specific needs. For example, a QA environment for a web-based application might include a web server, a database server, and a test client. A QA environment for a desktop application might consist of a test client and a set of test data.&nbsp;</p><p id="""">QA environments can be complex and expensive to set up and maintain. However, they're essential for ensuring the quality of a product before it is released to customers.&nbsp;</p><h3 id="""">Best Practices for Setting Up a Testing Environment</h3><p id="""">There are various best practices to consider when setting up a testing environment. Let's discuss some of them.&nbsp;</p><ul id=""""><li id="""">Always create a dedicated testing environment. This ensures that testing can be carried out independently from other activities and that other factors do not affect the results.</li><li id="""">Ensure that the testing environment is as similar as possible to the production environment. This includes factors such as using the same operating system and software versions and having the same hardware configurations.</li><li id="""">Set up a well-defined process for setting up and maintaining the testing environment. This should include clear instructions for how to install and configure the environment, as well as how to keep it running smoothly.</li><li id="""">Establish a robust system for managing and tracking changes to the testing environment. This helps ensure that changes are made safely and that the environment is always in a known state.</li></ul><h3 id="""">Testing Environment Types Conclusion</h3><p id="""">In the agile world, it's essential to have different types of testing environments in place. This allows for more comprehensive software testing and can help identify potential issues early on.&nbsp;</p><p id="""">It was our pleasure to share with you what testing environment types are and some information on how they can be used. We hope this information will help you as you continue on your path to creating better test cases and meeting your testing goals. Check out <a href=""https://release.com/ebook/the-complete-guide-to-automated-software-environments"">our latest guide</a> if you'd like to know more about automating software environments.&nbsp;</p>",false,,,,https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63e69b7b3612c397c84ffaf6_63c1bebeca87e7ad127d4ef2_Logo%20-%20Light%20on%20Dark%201.svg,,,6,Mon Jan 30 2023 22:43:00 GMT+0000 (Coordinated Universal Time),,
The Release Mission,the-release-mission,62aa5a70cd5ba27d9d0d718a,62aa5a70cd5ba2a69d0d7255,Tue Jan 26 2021 22:22:08 GMT+0000 (Coordinated Universal Time),Mon Apr 03 2023 21:26:04 GMT+0000 (Coordinated Universal Time),Mon Apr 03 2023 21:28:47 GMT+0000 (Coordinated Universal Time),"Hey everyone, we’re Tommy, David and Erik, co-founders of Release. We wanted to take a minute and kick off our blog with","<p id="""">Hey everyone, we’re Tommy, David and Erik, co-founders of Release. We wanted to take a minute and kick off our blog with a post about our mission at Release. We’re a mission driven organization working on one of the most important unsolved problems in all of technology.</p><blockquote id=""""><em id="""">Release exists so great ideas can freely and easily make their way into the world.</em></blockquote><h3 id="""">Getting great ideas into the world is still too hard</h3><p id="""">We’ve been building software and products for the greater part of 20 years. We’ve been involved in every type of company from the smallest startups (just the three of us) to being a part of the growth of a unicorn (David at Etsy) to becoming the technology leadership team at a public company (TC). There is one thing that has remained a constant throughout our entire careers in technology. Great ideas are trapped behind systems that are too difficult to manage and great ideas remain stuck in the minds of brilliant people with no way to unleash them on the world.</p><p id="""">Every organization has some velocity in which they are delivering ideas. It’s a function of the number of people in the organization times how easy a change is to make and release. What tends to happen is these two numbers are inversely correlated. The larger the number of people, the harder it is to release ideas and changes. Why is this?</p><p id="""">In the beginning, when you’re a startup, the number of people is small but the ability to change the system is incredibly high. Primarily because no one is paying attention and you have nothing to lose. You can release software, it can break and no one knows. So you just go about releasing as fast as possible. Things break. No one cares. You either have no users or very few early adopters and they expect things to break. You’re optimizing for velocity over all else and it is an amazing time of delivery. (and happiness for everyone).</p><p id="""">As you start to add customers and employees you can feel product velocity slowing. You become more careful, you have more to lose. You start putting checks and balances in place to ensure changes have high quality and meet your users needs. You begin protecting what you’ve built and for good reason. You’ve got customers paying you, employees who’s lives depend on these customers and the products you’re delivering. You now have something to lose. The bigger you get, the more you have to lose.</p><h3 id="""">Managing complexity as you grow gets harder</h3><p id="""">At the core of these product velocity/release issues is how difficult technology is to manage as things scale. As you add engineers, you add capacity to change and introduce new products but you’ve also added a ton of ways things can now break and your systems get more and more complex. Every change you introduce into your technology ecosystem introduces a risk or a reward.</p><p id="""">Being careful becomes what you do. You start testing more diligently, you may start automating things. All in an effort to keep velocity high and minimize your risk. As you become more diligent, you add more people to either add new things or protect against things breaking. (Automation, QA, etc…). With more people, more risk, more testing, more automation… the cycle continues.</p><p id="""">I’m sure if you could chart output per employee at every company. You’d find that in every organization, the output per employee is inversely correlated to the number of people in the organization. The more employees you have, the more complex everything gets and the less each individual employee produces.</p><p id="""">As product velocity slows, other issues emerge. There becomes a time when the output per employee drops to a level that only so many things can get done as more change is introduced into the system. The systems can’t handle the change… so now ideas start to get trapped. You have to meet and make decisions on what ideas will you attempt. It’s easy for ideas to be ignored and not attempted on their merits any longer. They are attempted based on who has the most seniority to make a decision in an organization. Politics emerge, ideas are trapped.</p><p id="""">At the core of this problem are the systems the organization uses to get their ideas into the world. The better the systems are to handle change, the more ideas they can try. The less ideas remain trapped and the more the organization can grow.</p><h3 id="""">You’re at a disadvantage</h3><p id="""">You are at a disadvantage to the great technology companies. They understand this issue and have overcome it, otherwise they wouldn’t be where they are. They have solved slowing innovation by investing heavily in their internal systems and platforms.</p><p id="""">I just did a quick search on Facebook’s job hiring site as of the date of this writing… There are currently 338 open Infrastructure positions at Facebook. 338. The same search for the number of software engineering roles open at Facebook turned up 237. Over 100 more infrastructure roles than software engineers. That was even more unexpected than I though.</p><p id="""">Here’s the same data on Google. 778 Software Engineering Roles. 1072 Infrastructure roles, 200 DevOps roles.</p><p id="""">If one company’s platform to allow change (with low risk) is higher than another’s, they have an inherent strategic advantage. They can adapt faster, they can attempt new business lines faster, they can outmaneuver competitors because they are fast. They’ve found a way to increase productivity per employee and scale at the same time.</p><p id="""">But it comes at a massive cost which is prohibitively high for most organizations.</p><p id="""">There are two things that are alarming for companies that are not Facebook or Google (or any other mega company). The first is you can’t compete with them for talent. They will always be able to pay DevOps and infrastructure engineers more than you. So the best in the world aren’t going to be working for you. The second is, if you aren’t investing in your infrastructure, you’re not moving as fast as you could… and ideas in your organization are trapped. A good measure to know if you’re in this situation is to think about the politics in your organization. If you feel like you have a lot of politics, you probably have trapped ideas.</p><h3 id="""">Why we started Release and our Mission</h3><p id="""">In every company we’ve been involved in from the smallest startups to the public companies, we’ve had to combat slowing innovation. We’ve had to invest heavily in building platforms to enable rapid delivery. Nothing existed in the world that just got the job done. We had to stitch together solutions, hire teams, spend millions of dollars to try and make our engineering orgs move fast. And even then, we had to keep building on it, keep spending to maintain and evolve our platforms.</p><p id="""">And guess what? It wasn’t our core competency as a business, but we knew if we didn’t invest heavily we wouldn’t be able to innovate We knew, for a fact, that great ideas were trapped and we were in an endless cat and mouse game. One could argue that building these internal systems enables customer happiness, but I would argue that largely these are solved problems and a distraction for most organizations.</p><p id="""">Release exists so ideas can freely make their way to the world. When ideas can freely be released, the best ideas will emerge, politics will lessen and power structures in organizations will become less important. To achieve this mission we’re building the technological capability that supports rapid change that is accessible to everyone. We’re starting with environment management because we believe it’s one of the hardest unsolved problems, but our driving force is enabling companies to deliver ideas fast.</p>",false,,,,https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63e400456c2eb57382a325ee_043020%20(1).jpg,The man in the moon symbolizing the Release mission,tommy-mcclung,6,Fri May 01 2020 00:00:00 GMT+0000 (Coordinated Universal Time),,
The Value of Data Obfuscation for Instant Datasets: Tonic Meets Release,the-value-of-data-obfuscation-for-instant-datasets-tonic-meets-release,62aa5a70cd5ba27d9d0d718a,64ecc494312aa56a7fba01cd,Mon Aug 28 2023 16:00:20 GMT+0000 (Coordinated Universal Time),Thu Jul 25 2024 15:05:24 GMT+0000 (Coordinated Universal Time),Thu Jul 25 2024 16:27:39 GMT+0000 (Coordinated Universal Time),"In this post, Tonic CEO Andrew Colombo shares his views on the value of data obfuscation for software testing and more","<p id="""">Developers and DevOps teams know and appreciate the importance of data privacy. We don’t have to tell you twice that your company’s reputation and ability to hold on to your customers hinges on their trust in your company to keep their data safe. </p><p>‍<a href=""https://www.tonic.ai/"" id="""">Tonic</a>’s test data platform is the industry-leading one-stop-shop for safe, realistic data for lower environments. <a href=""https://release.com/"" id="""">Release</a> and <a href=""https://www.tonic.ai/"" id="""">Tonic</a> share the same goal: getting production-like data into development and staging environments as efficiently as possible to streamline the development cycle and enhance software quality. With the launch of Release’s <a href=""https://release.com/product/instant-datasets"" id="""">Instant Datasets</a> and their native integration with Tonic, we’re excited to make obfuscated test data easily accessible in your ephemeral environments as well. A match made in developer productivity heaven.</p><p>In this blog post I’ll discuss the value of data obfuscation for software testing and why the marriage between data obfuscation and reliable, isolated, and replicable development and testing environments is a love story for the ages.</p><p>Already excited to engage more in the conversation? Sign-up for the <a href=""https://release.com/webinar/release-tonic-best-practices"" id="""">September webinar</a> with Release’s CTO and co-founder Erik Landerholm and myself, Tonic’s CTO and co-founder Andrew Colombi, to learn best practices for getting developers the data they need! </p><h3 id="""">Understanding Data Obfuscation</h3><p id="""">Data obfuscation techniques are especially useful for staging and testing as they can strike the perfect balance between data privacy and data utility. Obfuscation techniques such as masking, encryption, generalization, randomization, and tokenization effectively hide sensitive information while maintaining the integrity of your production data to allow for the most accurate and secure testing experience. </p><p>There are several different reasons why you should be thinking about obfuscating your test data:</p><ol id=""""><li><strong id="""">General privacy protection - </strong>Your users, customers, and patients entrust you with their personally identifiable information, financial details, and medical records. Your reputation as a reliable, trust-worthy company hinges on keeping that information safe.</li><li><strong id="""">Regulatory compliance - </strong>Your company not only has to answer to your customers, but also to the powers that be.<strong id=""""> </strong>Data protection policies such as GDPR, HIPAA, and CCPA are all examples of ways organizations are held accountable to maintaining the privacy of those individuals they collect data from. </li><li><strong id="""">Data sharing -</strong> Of course your team is talented, but fresh perspectives and specialized skill sets can improve any brand or product. If data needs to be shared with those outside your organization such as researchers or partners, data obfuscation can be used to do so without compromising your sensitive data. </li><li><strong id="""">Testing and development - </strong>Develop, test, test, test, release - that should be your workflow to push out the best software<strong id=""""> </strong>possible. But along the way, data used for software development and testing can be at risk of exposure. Obfuscating your production databases is one way of allowing developers to access the data they need to ship the highest quality products while still respecting the privacy of your customers. </li></ol><p>No matter what type of sensitive data you have, data obfuscation can be done using a variety of techniques: </p><ul id=""""><li><strong id="""">Tokenization -</strong> replacing sensitive data with tokens, or random strings of characters, while the original data is securely stored. </li><li><strong id="""">Encryption - </strong>transforming data into a format that is unreadable by an attacker using complex algorithms and the maintenance of a decryption key so that the data can be transformed back.</li><li><strong id="""">Masking -</strong> a set of techniques to hide a certain subset of sensitive data. </li><li><strong id="""">Generalization</strong> <strong id="""">-</strong> reducing the granularity and specificity of data by aggregating it or putting it into a more broad format. </li><li><strong id="""">Randomization - </strong>perturbing data or adding random noise to it in order to make it challenging to decipher individuals’ information. </li><li><strong id="""">Synthetic data generation -</strong> creating new data based on the patterns in a real dataset or based on rules defined by a user. </li></ul><p>The good news is that we at Tonic have built our platform to seamlessly offer all of the above obfuscation methods and streamline your success in implementing them in your testing environments. And with the Release Instant Datasets, it’s that much easier to access the exact dataset you need. </p><h3 id="""">Data Obfuscation for Software Testing </h3><p id="""">Testing applications on production data is a dangerous game. Test data obfuscation techniques create a dataset that is safe for use in testing of all kinds.</p><p id="""">With Tonic you can customize which, how, and where you obfuscate your production data as well as who has access to what. This allows for flexible workflows making sure that everyone gets the data they need to deliver at the highest caliber. </p><p id="""">Combining obfuscated data with ephemeral environments takes software testing to the next level. Ephemeral environments allow engineers the ability to work in reliable and isolated testing environments, accelerate their feedback cycles for quicker bug fixes, and boost collaboration to ultimately produce higher-quality software and better user experiences. We’re big fans of the solutions <a href=""https://release.com/"" id="""">Release</a> provides.</p><p>Cruise on over to the <a href=""https://www.tonic.ai/blog/test-your-apps-with-high-fidelity-production-like-data-with-release-and-tonic"" id="""">Tonic.ai blog</a> and check out Release’s CTO and cofounder Erik Landerholm’s guest blog post about the integration and sign up for our collaborative <a href=""https://release.com/webinar/release-tonic-best-practices"" id="""">September webinar</a> to learn more about how to use production-like, secure data for your application development and testing processes.</p>",false,,,,https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/64ecc86b07170c3f4e445b6f_release%2BTonic.jpg,,andrew-colombi,5,Tue Aug 29 2023 16:00:00 GMT+0000 (Coordinated Universal Time),news; product,introducing-standalone-instant-datasets-build-and-test-with-realistic-production-like-data-with-ease
Three ways you're doing modern web development wrong,three-ways-youre-doing-modern-web-development-wrong,62aa5a70cd5ba27d9d0d718a,639ceb9f406f2fe45180f79e,Fri Dec 16 2022 22:05:19 GMT+0000 (Coordinated Universal Time),Thu Apr 20 2023 21:35:58 GMT+0000 (Coordinated Universal Time),Thu Apr 20 2023 21:44:19 GMT+0000 (Coordinated Universal Time),Ensure that your team is using modern development practices with these 3 best practices,"<p id="""">Modern web development has come a long way in the past 20 years, but many companies seem like they're still stuck in the late 90s with the way they do development.</p><p id="""">Ensuring that your team is using modern development practices is beneficial in many ways, including easier hiring and onboarding, faster feature development, reduced bugs, and increased developer velocity. Not to mention keeping your developers happy and “in the zone”. Make things easier on your developers, and they will be able to complete work faster, at a higher quality, and be happier while doing it. These problems may not be obvious at first, but as you scale, they quickly become more and more painful. Here are three common mistakes you are probably making right now.</p><h3 id="""">Mistake #1: Working In A Fixed Number Of Environments</h3><p id="""">Your developer has finally written code, and now wants QA to review and test it. QA should not be testing on their local machines, they should be testing in something that more closely resembles a production environment. This leaves only cloud or on-premises environments as viable options for testing.</p><p id="""">If you haven't done the undifferentiated heavy lifting of setting up an in-house auto-scaling environment solution, then you are likely stuck with a fixed number of environments for developers and QA teams to share.</p><p id="""">This can slow the entire team down, and the added overhead of coordinating who is using what environments can be painful. (""Is anyone using dev3?"" Sound familiar?)</p><h4 id="""">Improvement #1: Get Ephemeral</h4><p id="""">Use an EaaS platform like Release. Creating an internal EaaS system is likely not your business’s core competency, and isn’t going to differentiate you in your industry. Just like you probably shouldn’t try and roll your own email delivery system, you are better off focusing on providing your customers value through features and bug fixes, rather than reinventing the wheel on building something that has already been built in a more fully featured manner than what is likely to come from an internal tool.</p><h3 id="""">Mistake #2: Not Using Production-Like Data</h3><p id="""">Seeding your dev environment with a predefined set of records is a very common way to do things, and also a very common cause of uncaught bugs making their way into production.</p><p id="""">There is simply no way that developers will be able to maintain a set of database seed files that will accurately represent the way your system is used in the real world.</p><p id="""">Developer-defined seed files tend to stay on the happy path, while the sad path, where data are incomplete, incorrect, or conflicting, often remains untested.</p><h4 id="""">Improvement #2: Fresh Data Every Time</h4><p id="""">Use a system that provides your developers with access to a fresh copy of production-like data to use every time they start work on a new branch. With Release’s Instant Datasets feature, you can get this going on day one.</p><p id="""">If you have sensitive data in your production database such as PII, you can use a tool like tonic.ai to fuzz or obfuscate sensitive data before using Release’s Instant Dataset feature, allowing every environment to have access to real, fresh, legally compliant data.</p><h3 id="""">Mistake #3: Running Everything Locally</h3><p id="""">While it is good to have the ability to run locally when needed (example: working without internet access), it tends to slow down actual development in modern stacks. </p><p id="""">Running 10 different services when you're only working on or trying debug a single one of them, is an unnecessary drain on the battery, extra load on the developer’s machine, and can lead to what I refer to as ""hovercraft mode"", when the laptop fans spin so fast it sounds like your laptop is about to take off.</p><p id="""">In addition to all of that, onboarding tends to be much slower due to having to get every new hire’s development machine setup exactly right with all the dependencies required across multiple different tech stacks.</p><h4 id="""">Improvement #3: Look To The Cloud</h4><p id="""">Instead of relying on local dev for everything, you could utilize a cloud based development model. Release Remote Development Environments allow developers to work in the exact type of environment their code will eventually be executed in, which can reduce bugs that come from environment and configuration problems. Beyond that a hybrid approach can offer the best of both worlds and bridge the gap between fully committing to either method. For example, you can run only the frontend locally, while the rest of the stack runs in a shared cloud environment.</p><h3 id="""">Time to Modernize</h3><p id="""">To avoid these issues and improve your team's development practices, consider investing in modern development tools and processes, such as <a href=""https://release.com/blog/remote-development-environments"">Remote Development Environments</a> and using <a href=""https://docs.releasehub.com/reference-documentation/instant-datasets-aws"" target=""_blank"">Production Data for testing</a>. These changes may require some initial effort and investment, but they will pay off in the long run by making your team more efficient, effective, and happy.</p>",false,,,,https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63e3fbe867257e35a8a8f9ef_012822%20(1).jpg,a computer screen with a keyboard,nick-busey,3,Mon Dec 19 2022 18:00:00 GMT+0000 (Coordinated Universal Time),,
Training ChatGPT with Custom Libraries Using Extensions,training-chatgpt-with-custom-libraries-using-extensions,62aa5a70cd5ba27d9d0d718a,6446f29358a3ca1088201a4d,Mon Apr 24 2023 21:20:19 GMT+0000 (Coordinated Universal Time),Thu Jul 25 2024 15:11:58 GMT+0000 (Coordinated Universal Time),Thu Jul 25 2024 16:27:39 GMT+0000 (Coordinated Universal Time),How we are now leveraging embeddings and vector databases to generate prompts for ChatGPT.,"<p id="""">I'm excited to share with you some of the fascinating work we've been doing here at Release. Our team has been exploring the power of embeddings, vector databases, and language models to create innovative product features. In this post, I'll explain our journey as we explored OpenAI and ChatGPT and how we are now leveraging <strong id="""">embeddings and vector databases</strong> to <strong id="""">generate prompts for ChatGPT</strong>.</p><p id="""">We began to look into various ways to leverage AI in our product. We were hoping that we could have ChatGPT generate Release Application Templates, the blueprints that we use to describe an application in Release. We quickly realized that ChatGPT is only trained on data before September 2021 and it was questionable if it knew anything about Release.&nbsp;</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:2970px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""2970px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/6446f3df869a87b02ee9fac2_Training%20ChatGPT%20with%20Custom%20Libraries%20Using%20Extensions%20image1.png"" loading=""lazy"" alt=""Image of a solicitiation in ChatGPT"" id="""" width=""auto"" height=""auto""></div></figure><p id="""">Release supports using both Docker and Docker Compose so you would be able to use these files in Release to generate an Application Template.&nbsp; But it was clear that ChatGPT needed to be trained using the Release documentation or a large corpus Release Application Templates&nbsp; if it was going to generate one from scratch.</p><h3 id="""">Exploring ChatGPT Plugins</h3><p id="""">ChatGPT Plugins seemed like the best way to give ChatGPT outside knowledge from its training set. We signed up for the ChatGPT Plugin waitlist and eventually got access to <a href=""https://openai.com/blog/chatgpt-plugins"">ChatGPT Plugins</a>. The <a href=""https://github.com/openai/chatgpt-retrieval-plugin"">ChatGPT Retrieval Plugin</a> seemed like a place to start experimenting with ChatGPT Plugins and get an understanding of how they work.</p><p id="""">After adding a few files to the <a href=""https://github.com/openai/chatgpt-retrieval-plugin/compare/main...davidgiffin:chatgpt-retrieval-plugin:main?expand=1"">chatgpt-retrival-plugin </a>we had it running Release. Then we started working on loading the data into the plugin, converting all of our docs into JSON and uploading them into the retrieval plugin using the `/upsert` endpoint. Once the plugin was configured ChatGPT we were able to ask ChatGPT to “How do I create an application template in Release”</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1650px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1650px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/6446f8f358a3caede0209707_Training%20ChatGPT%20with%20Custom%20Libraries%20Using%20Extensions%20-%20image.png"" loading=""lazy"" id="""" width=""auto"" height=""auto"" alt=""Image of a solicitiation in ChatGPT""></div></figure><p id="""">&nbsp;The retrieval plugin works well for asking a question that can be answered using the documentation it has access to. However it was unclear when plug-ins are going to be generally available for all users to access. We plan to develop a ChatGPT Plugin that everyone can use once that happens.&nbsp;</p><h3 id="""">Using Embeddings and Prompt Generation</h3><p id="""">As our team continued to explore the AI space we came across an article from the <a href=""https://supabase.com/blog/chatgpt-supabase-docs"">Supabase Blog</a>. The article explained a different approach to “train” ChatGPT. Instead of ChatGPT having access to our documentation directly you could feed snippets of the docs to ChatGPT in the prompt. Here is the prompt template that takes the users question and the relevant snippets from the docs to answer a users question:</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-none"">
`
      You are a very enthusiastic Release representative who loves
      to help people! Given the following sections from the Release
      documentation, answer the question using only that information,
      outputted in markdown format. If you are unsure and the answer
      is not explicitly written in the documentation, say
      ""Sorry, I don't know how to help with that.""
      
      Context sections:
      ${contextText}
x

      """"""
      Answer as markdown (including related code snippets if available):
`
</code>
</pre></div><p id="""">The folks who helped build the Supabase AI functionality also created an open source standalone project <a href=""https://github.com/supabase-community/nextjs-openai-doc-search"">next.js OpenAI Search Starter.</a> We have been using this project as a starting point for our AI based documentation search.</p><h3 id="""">What are Embeddings?</h3><p id="""">Both the ChatGPT Retrieval Plugin and Supabase’s AI Documentation Search rely on generating, storing and searching embeddings. So what is an embedding? <br></p><p id="""">Embeddings are a way to represent text, images, or other types of data in a numerical format that can be easily processed by machine learning algorithms. In the context of natural language processing (NLP), word embeddings are vector representations of words, where each word is mapped to a fixed-size vector in a high-dimensional space. These vectors capture the semantic and syntactic relationships between words, allowing us to perform mathematical operations on them. The following diagram shows the relationship between various sentences:</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1432px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1432px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/6446f497230b7e011da55fec_Training%20ChatGPT%20with%20Custom%20Libraries%20Using%20Extensions%20image3.png"" loading=""lazy"" id="""" width=""auto"" height=""auto""></div></figure><p id=""""><a href=""https://deepai.org/publication/in-search-for-linear-relations-in-sentence-embedding-spaces"">image of sentence embeddings - from DeepAI</a></p><p id="""">Embeddings can be used to find words that are semantically similar to a given word. By calculating the cosine similarity between the vectors of two words, we can determine how similar their meanings are. This is a powerful tool for tasks such as text classification, sentiment analysis, and language translation.&nbsp;</p><h3 id="""">What are Vector Databases?</h3><p id="""">Vector databases, also known as vector search engines, are specialized databases designed to store and search for high-dimensional vectors efficiently. They enable fast similarity search and nearest neighbor search, which are essential operations when working with embeddings.</p><p id="""">Supabase’s AI Documentation Search uses <a href=""https://github.com/pgvector/pgvector"">pgvector</a> to store and retrieve embeddings. But many other vector databases exist today:</p><p id=""""><a href=""https://github.com/openai/openai-cookbook/tree/main/examples/vector_databases/pinecone"" target=""_blank"" id="""">Pinecone</a>, a fully managed vector database</p><p id="""">‍<a href=""https://github.com/openai/openai-cookbook/tree/main/examples/vector_databases/weaviate"" target=""_blank"" id="""">Weaviate</a>, an open-source vector search engine</p><p id=""""><a href=""https://github.com/openai/openai-cookbook/tree/main/examples/vector_databases/redis"" target=""_blank"" id="""">Redis</a>, a vector database</p><p id=""""><a href=""https://github.com/openai/openai-cookbook/tree/main/examples/vector_databases/qdrant"" target=""_blank"" id="""">Qdrant</a>, a vector search engine</p><p id=""""><a href=""https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/Using_vector_databases_for_embeddings_search.ipynb"" target=""_blank"" id="""">Milvus</a>, a vector database built for scalable similarity search</p><p id=""""><a href=""https://github.com/chroma-core/chroma"" target=""_blank"" id="""">Chroma</a>, an open-source embeddings store</p><p id=""""><a href=""https://typesense.org/docs/0.24.0/api/vector-search.html"" target=""_blank"" id="""">Typesense</a>, fast open source vector search</p><p id="""">All of these databases support three basic things: storing embeddings as vectors, the ability to search the embedding/vectors and finally sorting the results based on similarity. When using OpenAI’s `text-embedding-ada-002` model to generate embeddings OpenAI recommends using <a href=""https://typesense.org/docs/0.24.0/api/vector-search.html"" target=""_blank"" id="""">cosine similarity</a> which is built into most of the vector databases listed above.&nbsp;</p><h3 id="""">How to Generate, Store and Search Embeddings</h3><p id=""""><a href=""https://platform.openai.com/docs/api-reference/embeddings"">OpenAI provides an API endpoint </a>to generate embeddings from any text string.&nbsp;</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-ruby"">
```ruby
        # OpenAI recommends replacing newlines with spaces
        # for best results (specific to embeddings)
        input = section.gsub(/\n/m, ' ')
        response = openai.embeddings(parameters: { input: input, model: ""text-embedding-ada-002""})

        token_count = response['usage']['total_tokens'] # number of tokens used
        embedding = response['data'].first['embedding'] # array of 1536 floats
```
</code>
</pre></div><p id="""">Storing this data in Redis <a href=""https://redis.io/docs/stack/"">redis-stack-server</a> and making it searchable requires an index. To create an index using redis-stack-server you need to issue the following command:</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-ruby"">
```
FT.CREATE index ON JSON PREFIX 1 item: SCHEMA $.id AS id TEXT $.content AS content TEXT $.token_count AS token_count NUMERIC $.embedding AS embedding VECTOR FLAT 6 DIM 1536 DISTANCE_METRIC COSINE TYPE FLOAT64
```
</code>
</pre></div><p id="""">Now we can store items into Redis and havethem indexed with the following command:</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-JSON"">
```
JSON.SET item:1002020 $ '{""id"":""963a2117895ec9a29f242f906fd188c6"", ""content"":""# App Imports: …”, ""embedding"":[0.008565563,0.012807296…]}’
```
</code>
</pre></div><p id="""">Note that if you don’t provide all 1536 dimensions of the vector your data will not be indexed by Redis and it will give you no error response.</p><p id="""">Searching Redis for results and sorting them can be done with the following command:</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-JSON"">
```
FT.SEARCH index @embedding:[VECTOR_RANGE $r $BLOB]=>{$YIELD_DISTANCE_AS: my_scores} PARAMS 4 BLOB \x00\x00\x00 r 5 LIMIT 0 10 SORTBY my_scores DIALECT 2
```
</code>
</pre></div><p id="""">Note that BLOB provided is in binary format and needs to have all 1536 dimensions of vector data as well. We use the OpenAI Embeddings API to generate the embedding vector and convert it to a binary in Ruby using `embedding.pack(""E*"")`.</p><h3 id="""">Release ChatGPT Powered Documentation Search</h3><p id="""">We have replaced the backend <a href=""https://github.com/supabase-community/nextjs-openai-doc-search"">next.js OpenAI Search Starter </a>with Ruby and Redis. We will be releasing our project as an open source Gem that will allow anyone to quickly add AI based document searching to their site.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1776px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1776px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/6446f4c557569061d0ca57f5_Training%20ChatGPT%20with%20Custom%20Libraries%20Using%20Extensions%20image4.png"" loading=""lazy"" id="""" width=""auto"" height=""auto""></div></figure><p id="""">We have a <a href=""https://frontend-vapey-prod.releaseapp.gethandsup.com/"">working example</a> of the Release AI Powered Documentation Search using slightly modified version of the <a href=""https://github.com/supabase-community/nextjs-openai-doc-search)"">next.js OpenAI Search Starter.</a> We’ve added support for scrolling, better rendering of markdown (which the Supabase version had) and the ability to plugin your search API backend.</p><h3 id="""">Conclusion</h3><p id="""">By combining the power of embeddings, vector databases, and language models like ChatGPT, we've been able to create product features that provide valuable insights and enhance user experiences. Whether it's answering customer queries, generating personalized content, or providing recommendations, our approach has opened up new possibilities for innovation.</p><p id="""">We're excited about the potential of this technology, and we're looking forward to exploring new ways to leverage it in the future. As we continue to develop and refine our product offerings, we're committed to staying at the forefront of AI and NLP research. Our goal is to create tools and solutions that empower businesses and individuals to harness the power of language models in meaningful and impactful ways.</p><p id="""">Thank you for taking the time to read our blog post. We hope you found it informative and that it sparked your curiosity about the exciting possibilities that embeddings, vector databases, and language models like ChatGPT have to offer. If you have any questions or would like to learn more about our work at Release, <a href=""https://release.com/book-a-demo"" id="""">please feel free to reach out to us or book a demo.</a> We'd love to hear from you!</p>",false,,,,https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/6446f7a47dc8e56afc023f1b_Training%20ChatGPT%20with%20Custom%20Libraries%20Using%20Extensions.png,A huge book shelf full of books,david-giffin,6,Tue Apr 25 2023 21:19:00 GMT+0000 (Coordinated Universal Time),product; ai; docker,rainbow-deployment-why-and-how-to-do-it
TripActions Selects Release to Give Full Stack Environments to Every Developer,tripactions-selects-releasehub-to-give-full-stack-environments-to-every-developer,62aa5a70cd5ba27d9d0d718a,633589a886ec0a82f1bbc6a6,Thu Sep 29 2022 12:03:52 GMT+0000 (Coordinated Universal Time),Fri Jan 13 2023 15:19:25 GMT+0000 (Coordinated Universal Time),,"Release’s environments can be spun up or down on-demand, increasing product release velocity and eliminating develope","<p id=""""><strong id="""">SAN FRANCISCO, September 29, 2022</strong> — Release, the leading provider of on-demand software environments, today announced that TripActions, an industry-leading all-in-one travel, corporate card, and expense management solution, has selected ReleaseHub to provide its developers with ephemeral, full-stack staging environments that can be spun up and down on demand. </p><p id="""">‍</p><p id="""">Switching to Release’s on-demand staging environments will allow TripActions to stand up and tear down identical production-like environments instantly, eliminating common delays that developers across all industries experience with conventional shared environments. With ReleaseHub, organizations reduce the cost of wasted developer time, and increase operational efficiency and productivity. </p><p id="""">‍</p><p id="""">“Developers around the world understand how frustrating and pervasive staging environment delays are,” said Tommy McClung, co-founder and CEO of Release. “TripActions is in a unique position because they’re an industry leader at the cutting edge of innovation, with thousands of global customers, so they have unparalleled insight into developer pain points. Delays frequently leave the most talented, highly-paid professionals stuck waiting in limbo. Our ephemeral environments eliminate the limbo time entirely.”</p><p id="""">‍</p><p id="""">With Release, organizations — including TripActions — can also focus on attracting and retaining the highest quality professionals by removing staging delays, a continual source of frustration for developers across all industries. </p><p id="""">‍</p><p id="""">“Release will enable us to increase our release cadence and reduce developer down-time,” said Chris Willmore, director of productivity engineering at TripActions. “We have hundreds of developers and if they are using conventional shared staging environments, many end up sitting around, waiting for access. Release eliminates the waiting time, improves developer flow, saves wasted resources, and helps us deliver software faster.”</p><p id="""">‍</p><p id="""">Release delivers Environments-as-a-Service using environments-as-code. Unlike other solutions, ReleaseHub abstracts away much of the work required by developers. The process of creating environments-as-code is automated and customizable. The result allows developers to manage environments simply via code in their development workflow. ReleaseHub supports running custom Infrastructure-as-a-Code including Terraform, Helm and other Infrastructure-as-a-Service providers. </p><p id="""">‍</p><p id=""""><strong id="""">About Release</strong></p><p id="""">Release delivers Environments-as-a-Service. It lets developers easily share progress with stakeholders when a full stack environment is created with every pull request and is shareable via custom URLs and directly in Slack. Every environment is a full instance of the app with all its services. ReleaseHub was funded by CRV, Sequoia, Y Combinator, Bow Capital, Artisanal Ventures, Hack VC, and other investors. More information is available at <a href=""http://www.release.com/"">www.release.com</a>. &nbsp;</p><p id="""">‍</p><p id=""""><strong id="""">About TripActions</strong></p><p id="""">TripActions is the all-in-one travel, corporate card, and expense management solution, providing customers around the globe with unprecedented visibility and control over spend. Trusted by travel managers and finance teams alike, TripActions and TripActions Liquid leverage real-time data to help companies keep traveling employees safe, reduce spend, and drive productivity. &nbsp;Learn more at www.tripactions.com</p><p id="""">‍</p>",false,,,,https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63358b28c275f9034ae9fd8e_nikhil-mitra-gssCjrDR5Y4-unsplash.jpg,,,2,Thu Sep 29 2022 16:00:00 GMT+0000 (Coordinated Universal Time),,
Use Product Thinking to Establish Your IDP,use-product-thinking-to-establish-your-idp,62aa5a70cd5ba27d9d0d718a,651340a74b100ceb02a0c384,Tue Sep 26 2023 20:35:51 GMT+0000 (Coordinated Universal Time),Thu Jul 25 2024 15:04:14 GMT+0000 (Coordinated Universal Time),Thu Jul 25 2024 16:27:39 GMT+0000 (Coordinated Universal Time),"Learn how to build a success platform by combining IDP's goals, components and problems to solve, with product thinking.","<p id="""">In previous posts, we talked about the <a href=""https://release.com/blog/what-is-an-internal-developer-platform-and-why-should-i-have-one"" id="""">benefits of Internal Developer Platforms</a> (IDPs) and the <a href=""https://release.com/blog/components-of-a-successful-idp-build-a-product-your-developers-actually-want-to-use"" id="""">components that form a great IDP</a>. But getting all the benefits out of an IDP isn’t just about gathering a bunch of tools together in one UI and assuming everyone will use it.</p><p id="""">Instead, we need to treat the entire platform, with all its components and integrations as a product—a product that solves problems and improves your developers’ workflows.</p><p id="""">Within each organization, an IDP might solve different problems in different ways. And in order for you to get the benefits out of the IDP for your organization, you’ll need to take a product-thinking approach and fulfill the needs of your customers—the developers.</p>",true,<p>Ready to try Release?<br>Use code #IDP to get 30 days free.</p>,https://release.com/signup,"<p id="""">In this post, we’ll talk about how you can combine the goals of an IDP, the problems it solves through various components, and product-thinking principles to build a successful platform. If your organization is new to product thinking, or if you haven’t used those skills when developing tooling for your software developers, this will give you a starting point with frameworks as well as common elements and metrics that you can build on when kicking off your IDP journey.</p><p id="""">First, let’s look at product thinking and a few approaches and frameworks that provide your organization with the tools it needs to build a successful product.</p><h4 id="""">The Product-Thinking Approach</h4><p id="""">If you’re not familiar with product thinking, it may seem like a nebulous idea that requires the most experienced product managers with fancy frameworks and high consulting fees. However, product thinking, at its core, involves identifying and understanding the problems your customers experience and then prioritizing solutions that provide the most value.</p><p id="""">Organizations use various frameworks to drive product thinking. These frameworks identify activities that support your discovery and understanding of the problems and prioritization of the work to be done. </p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1000px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1000px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/651456742db10d1a948d5aa0_RCIaQvfCsEzA_Yxms2aatzEEC_Ea4VHhaM6laJD5WtMlbVHpABHKtplLkdeVO0UC4n-D6KeWpPrjG8yIjJXLM6ngyOuaF5c67FZtESE5e3sUt1xakQ4cnTAycpsByo8dJKbQicefiKzIfneOzFAXoZE.png"" id="""" width=""auto"" height=""auto"" alt="""" loading=""auto""></div></figure><p id="""">Let’s take a look at some frameworks that organizations use to drive product thinking. You can use one or combination of these approaches, as they complement each other well.</p><h5 id="""">🚀 Lean Startup: Build, Measure, and Learn</h5><p id="""">The <a href=""https://theleanstartup.com/"" id="""">Lean Startup</a> methodology broke down the process of building products into three iterative steps: build, measure, and learn. This methodology combines UX, product, and engineering to build products by solving problems using minimum viable products (MVPs) and customer validation. The focus is on learning more about your customer and their needs from each iteration of the build-measure-learn cycle. </p><p id="""">What would that look like for an IDP?</p><p id="""">In the <strong id="""">build</strong> phase, you can begin by integrating one component of an IDP or automating one part of a workflow that your developers must do. For example, your org may frequently build new microservices or stand-alone products. Each of your teams requires an easy way to set up new environments and CI/CD pipelines for these microservices. Your IDP could first automate a basic CI/CD pipeline so that teams don’t have to do that themselves.</p><p id="""">In the <strong id="""">measure</strong> phase, you’ll see how your developers are using your IDP MVP through usage metrics and feedback. Looking at the MVP example that creates a pipeline, how often do teams use it? How often do they edit the pipelines later to fit their use cases? And what do they change?</p><p id="""">Next, we enter the <strong id="""">learn</strong> phase. For our IDP example, we will assess our findings. Then perhaps we’ll learn that the pipeline is good enough for now and that provisioning infrastructure needs more automation or features. On the other hand, we could learn that the pipeline isn’t flexible enough or doesn’t provide easy rollback capabilities. Either way, we now know what we can build next and restart the cycle.</p><h5 id="""">💎 Double Diamond Model: Discover, Define, Develop, and Deliver</h5><p id="""">The <a href=""https://www.designcouncil.org.uk/our-resources/archive/articles/double-diamond-universally-accepted-depiction-design-process"" id="""">double diamond mode</a> also takes an iterative approach to building a product. We repeatedly work through its four phases—discover, define, develop, and deliver—over the life of the product.</p><p id="""">During <strong id="""">discovery</strong>, you’ll explore and understand the problem space. For IDPs, this could include interviewing engineers, watching them work, and gathering available metrics on where the most pain exists in the development workflow.</p><p id="""">Once we’ve pulled our research from discovery, you’ll move to the <strong id="""">define</strong> phase. You’ll shift to framing the problem to a smaller scope and determining which problem you will solve first.</p><p id="""">In the <strong id="""">development</strong> phase, the development team will build solutions. This may include building components, or more likely, it includes configuring prebuilt platform as a service (PaaS) solutions to work with your infrastructure and organization.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1000px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1000px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/6514567115006ea9efec8abf_yvLZwRBhm-rc1Q0oaWXwU65g49HY0IJUMwSMvtiBVO_lgZW3192vp8GY43iVNgv6uFgWm50e-_bhgMj83gGHK87Tbhu44PGspWIDLrRGB7nR474bQAzRTbGTqlnFEE0C5Ia6kspyIIJ8CLFStgdr72E.png"" id="""" width=""auto"" height=""auto"" alt="""" loading=""auto""></div></figure><p id="""">And finally, you’ll <strong id="""">deliver</strong> the initial product to market: your development teams.</p><p id="""">Then you’ll start at the beginning again, refining and validating your choices from the previous phases.</p><h5 id="""">🔁 Iterative Development Using Agile Methodologies</h5><p id="""">Our next approach also iterates on the product but focuses on the engineering side. With Agile, we focus on people from cross-functional groups working together to build working software through customer collaboration. Although product thinking is still used in each iteration, it’s more about how the product is actually built. </p><p id="""">Our first iteration may be an MVP like the ones we see in Lean Startup, or it could be a more refined product spec that has already been proved out. Once our first iteration of the product is complete, we’ll release it to the developer community that uses the IDP and ensure that expectations have been met. We’ll work with the developers and other stakeholders to ensure that we build the right product and deliver value to the customers.</p><p id="""">At this point, you’ve been introduced or had a refresher on some common product thinking frameworks. So what’s next? We’ll want to consider how we can incorporate them into our product and what <a href=""https://release.com/blog/components-of-a-successful-idp-build-a-product-your-developers-actually-want-to-use"" id="""">components</a> we’ll need to harness in order to build a successful IDP.</p><h4 id="""">Product Thinking Best Practices</h4><p id="""">Regardless of the methodology or framework you use, you need to follow certain best practices to understand the problems your developers experience and prioritize the solutions that provide the most value:</p><ol id=""""><li id="""">Interdisciplinary team</li><li id="""">User-centered approach</li><li id="""">Focus on value</li><li id="""">Iterative development</li><li id="""">Measurable outcomes</li></ol><h5 id="""">1. Interdisciplinary team</h5><p id="""">Most product-driven approaches include an interdisciplinary or cross-functional team. To build the right IDP, you need the right people looking at the problem from different perspectives.</p><p id="""">Your cross-functional team should, at minimum, include these main roles: </p><ul id=""""><li id=""""><strong id="""">Product:</strong> to assess and prioritize the potential problems and solutions.</li><li id=""""><strong id="""">UX:</strong> to work with the developers to understand workflows and pain points. UX uses their tools to improve efficiency, flow, and usability of the IDP.</li><li id=""""><strong id="""">Engineering: </strong>to serve two purposes. First, as the customer of the IDP, their experience and feedback will provide you with insights into making their workflow better. Second, they’ll use their engineering experience to build out the IDP. This could mean building a component or integration or just configuring and automating various integrations.</li></ul><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1000px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1000px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/651456714a0b00f581851a47_yTd3PvbLRt-3aiqA0aEDsMEedSFau2UXcR9T8e0L_5wSw6mC9tIlkc3Tvk0uQsNrrsTDVBB7evHqL_QNj01p4y-gklWemGMGrUFitpEjKkIgG2Sb7BHLOrLdec58KdIWJl3APJNCbQ1p7pkIAx9_QS8.png"" id="""" width=""auto"" height=""auto"" alt="""" loading=""auto""></div></figure><p id="""">In addition, consider adding subject matter experts (SMEs) from data, security, and compliance. When aligned with the mission of improving developer workflows, they’ll advise on what automation can fulfill different regulatory or security requirements, removing manual processes from the developers’ workflow.</p><h5 id=""""><strong id="""">2. User-Centered Approach</strong></h5><p id="""">For IDPs, your development teams are your main users. Understanding their environment and how they work will provide critical data for solving the right problems in the right way.</p><p id="""">To understand the developers’ workflow, you’ll conduct interviews, get feedback, and collect data on their current processes. You also have to understand the developers’ experience with their current tools and how long different processes take. Where is their toil, and what steps take more time? What tools are better suited for a traditional UI, and what works best as a CLI, API, or other integration?</p><p id="""">One important note: Just because the developers offer feedback doesn’t mean that you must act on it. Product thinking isn’t about taking orders from customers; it's about understanding problems. When you understand problems, you’ll need to dig in and find where the problems come from and which are more important to solve.</p><h5 id=""""><strong id="""">3. Focus on Value</strong></h5><p id="""">To start your IDP, your interdisciplinary team creates clear objectives and goals that focus on the value. Consider what problems or types of problems you are trying to solve and how you’ll measure success.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1000px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1000px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/6514567267f6cfc4ee3521f6_nU2p77hgLwM9HrU5ycjySAlgelNsNsZPIaCGFqFpK42tYqe0Jytb4O3m23DZwbf9Vrc0gcWbBCVnWl2ro3T9n5qmNwVMG3aUUKpk7KbwvsYsPYD-kPcjmaY2evfI3ILeljCnYFUbpjxSKglQ3xhJLjM.png"" id="""" width=""auto"" height=""auto"" alt="""" loading=""auto""></div></figure><p id="""">Again, you don’t have to solve all the use cases for a particular problem. Using the objectives for your product, focus on the features that will give you the biggest bang for your buck. Solve for 80% of the scenarios that your developers encounter and leave the other 20% for custom solutions that teams can work through on their own. And realize that the ideal solution may not be worth the cost when compared to the current or a slightly improved workflow.</p><h5 id=""""><strong id="""">4. Iterative Development</strong></h5><p id="""">Most product-driven frameworks and methodologies include iterative development. This is especially true of the IDP, as there are so many options and so much that can be integrated in different ways. Your main focus will be on starting small and shipping new bits of functionality frequently.</p><h5 id=""""><strong id="""">5. Measurable Outcomes</strong></h5><p id="""">Measuring the success of IDPs or any developer tooling can be difficult. Consider these metrics to ensure that you’re providing value, and automate their collection from the start.</p><ul id=""""><li id=""""><strong id="""">Adoption rate:</strong> For all features and integrations, measure how many developers are using the platform and how frequently they use it.</li><li id=""""><strong id="""">Time saved: </strong>Track how much time developers save by using the features of your platform when compared to their previous workflow.</li><li id=""""><strong id="""">Feature usage: </strong>Determine which features are used the most and which ones the least.</li><li id=""""><strong id="""">Satisfaction scores:</strong> Periodically collect satisfaction scores from developers to gauge their experiences with the platform.</li></ul><p id="""">You may be tempted to track metrics like release or feature velocity or the number of bugs a development team produces after onboarding to an IDP, but these have very little to do with the IDP. Focus on what’s directly affected by your IDP.</p><h4 id="""">To Sum Up </h4><p id="""">When building an IDP, use product thinking to ensure you’re providing the right value and spending your resources well.</p><p id="""">Go back to our post on <a href=""https://release.com/blog/build-vs-buy-where-to-focus-your-energy-with-idps"" id="""">build vs. buy</a> and consider what components can be brought in off the shelf with little development effort. That will provide you with fast feedback and the ability to understand the problem before sinking a lot of time into building it yourself.</p><p id="""">For optimal success, include an interdisciplinary team, focus on the user and the value you can provide, measure your success, and iterate, iterate, iterate.</p><p><em id="""">This post was written by Sylvia Fronczak. </em><a href=""https://sylviafronczak.com/"" id=""""><em id="""">Sylvia</em></a><em id=""""> is a software developer who has worked in various industries with various software methodologies. She’s currently focused on design practices that the whole team can own, understand, and evolve over time.</em></p>",https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/65145ee014d44276123cf803_PE%234%20Product%20Thinking.jpg,,sylvia-fronczak,10,Wed Sep 27 2023 18:00:00 GMT+0000 (Coordinated Universal Time),platform-engineering,what-is-an-internal-developer-platform-and-why-should-i-have-one
User Acceptance Testing Best Practices,user-acceptance-testing-best-practices,62aa5a70cd5ba27d9d0d718a,62aa5a70cd5ba2c5c50d72f4,Tue Jan 04 2022 23:05:44 GMT+0000 (Coordinated Universal Time),Fri Apr 21 2023 19:41:41 GMT+0000 (Coordinated Universal Time),Fri Apr 21 2023 20:04:43 GMT+0000 (Coordinated Universal Time),"In this blog post you'll learn what is user acceptance testing, and user acceptance testing best practices.","<p id="""">Software testing plays a critical role in the end-to-end software product development process as it ensures as well as upholds the overall product quality.</p><p id="""">Among the different phases of this software testing include integration testing, unit testing, system testing, acceptance testing, and the last phase of software testing, known as the <a href=""https://releasehub.com/user-acceptance-testing-with-ephemeral-environments"" id="""">UAT or User Acceptance Testing</a>, which plays a significant role in the entire software testing process.</p><p id="""">This blog series aims to explore more about user acceptance testing, including what it is, UAT best practices, stages of UAT, disadvantages, challenges, and more.</p><p id="""">This is a 4-part series on User Acceptance Testing (UAT)</p><ul id=""""><li id=""""><strong id="""">Part 1</strong>: <strong id="""">What is User Acceptance Testing and its Best Practices</strong></li><li id="""">Part 2: <a href=""https://releasehub.com/blog/how-do-you-prepare-for-user-acceptance-testing"" id="""">How to Prepare for User Acceptance Testing?</a></li><li id="""">Part 3: <a href=""https://releasehub.com//blog/what-are-the-challenges-faced-during-uat-testing"" id="""">User Acceptance Testing Challenges &amp; UAT Environment Examples</a></li><li id="""">Part 4: <a href=""https://releasehub.com/blog/user-acceptance-testing-checklist"" id="""">UAT Checklist</a></li></ul><h3 id="""">What is User Acceptance Testing?</h3><p id="""">User acceptance testing or UAT is a specific type of testing which is mainly performed by real users in the last stage of testing before the application or software is released to the live production environment.</p><p id="""">The primary reason why you need user acceptance testing is to validate if all business requirements of the software are met or not. This must be necessarily done before releasing the software to the market. This type of testing sometimes is also known as beta testing, application testing, or, more often, end-user testing.</p><p id="""">The key purpose of UAT is to validate end-to-end business flow. It is similar to black-box testing, where two or more end-users will be involved. Put simply, UAT is essential because:</p><ul id=""""><li id="""">It is a critical part of the software testing procedure and may help find defects that were not detected earlier.</li><li id="""">It helps users confirm if the developed solution meets the requirements and fulfills the needs of target users.</li><li id="""">The needs and requirements of the customers are unique, unpredictable. Implementing UAT helps to effectively fill this gap and identify scenarios that were not properly defined or requirements that need revisit.</li></ul><h3 id="""">User Acceptance Testing Best Practices</h3><p id="""">In this section, we are going to discuss some of the user acceptance testing best practices to ensure that your application is thoroughly tested, the software meets user demands in real-world scenarios, bugs get resolved much before launch, and all your clients are successful and happy.</p><p id="""">Among these best practices are:</p><h4 id="""">‍<strong id="""">Identify your target audience<br></strong></h4><p id="""">When it comes to UAT, it is crucial to identify your target audience and understand their unique problems and needs. This allows you to ensure that there is no wastage of time / resources on something that won't work for users.</p><h4 id=""""><strong id="""">Prepare a realistic test environment and data</strong>‍</h4><p id="""">Another important requirement for UAT is that the environment and test data must resemble the production environment as closely as possible. In an ideal scenario, the <a href=""https://releasehub.com/user-acceptance-testing-with-ephemeral-environments"" id="""">UAT environment</a> should be completely separate from the QA environment. However, if this is not possible, it is best to have a complete refresh before UAT, where QA professionals should check the refreshed environment to ensure it's working as expected. Spinning up a demo environment can be cumbersome and expensive. With Release's <a href=""https://releasehub.com/ephemeral-environments"" id="""">Ephemeral Environments</a> it’s possible to view a developer’s changes at any time or any place. You don’t need to worry about having the proper environment variables in place or running any code locally.<br></p><h4 id=""""><strong id="""">Create proper test criteria<br></strong></h4><p id="""">The test lead or a QA manager should work with QA professionals to be able to ensure that the test coverage is complete. Here checklists can offer an effective alternative to test cases and scripts. Also, the acceptance criteria should form the basis of testing user stories.<br></p><h4 id=""""><strong id="""">Be very specific<br></strong></h4><p id="""">Test cases in UAT need to be as detailed, thorough, and as specific as possible. It is best to be clear about and specify what buttons to click, what data to enter, what accounts to use, and what results the user should see. In addition to this, it should also cover how new functionality fits in well with existing pieces.<br></p><h4 id=""""><strong id="""">Create robust bug communication standards<br></strong></h4><p id="""">Bugs are inevitable in any application, and hence it is important to focus on how they are communicated to quickly resolve the issues. Make sure to be as specific as possible when creating a bug. Being vague can lead to confusion among developers, increases triaging times and further pushes back the timeline for&nbsp; the bug.</p><h4 id=""""><strong id="""">Create user stories based on specific business requirements and scenarios<br></strong></h4><p id="""">User acceptance testing can be very useful if you target the right set of users. And to be able to target the expected users, it is required to create it in sync with the business requirements. Also, before proceeding with the user acceptance tests, it's very important to clearly define the event that you would consider a success and what would define that the product passed/failed the acceptance test? <br></p><p id="""">Now that you are familiar with user acceptance testing best practices, next we'll look at <a href=""https://releasehub.com/blog/how-do-you-prepare-for-user-acceptance-testing"" id="""">How to Prepare for User Acceptance testing</a>.</p>",false,,,,https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63e428ff3f62f303ad33a2d2_1222_221%20(1).jpg,a coffee cup and a book,tommy-mcclung,5,Thu Dec 23 2021 01:25:00 GMT+0000 (Coordinated Universal Time),,
User Acceptance Testing Checklist,user-acceptance-testing-checklist,62aa5a70cd5ba27d9d0d718a,62aa5a70cd5ba27ad20d72f6,Wed Jan 05 2022 03:07:39 GMT+0000 (Coordinated Universal Time),Fri Apr 21 2023 19:40:20 GMT+0000 (Coordinated Universal Time),Fri Apr 21 2023 20:04:43 GMT+0000 (Coordinated Universal Time),User Acceptance Testing Checklist: Follow this checklist to get best results with your UAT testing.,"<p id="""">This is a 4-part series on User Acceptance Testing (UAT)</p><ul id=""""><li id="""">Part 1: <a href=""https://release.com/blog/user-acceptance-testing-best-practices"" id="""">What is User Acceptance Testing and its Best Practices</a></li><li id="""">Part 2: <a href=""https://release.com/blog/how-do-you-prepare-for-user-acceptance-testing"" id="""">How to Prepare for User Acceptance Testing?</a></li><li id="""">Part 3: <a href=""https://release.com/blog/what-are-the-challenges-faced-during-uat-testing"" id="""">User Acceptance Testing Challenges &amp; UAT Environment Examples</a></li><li id=""""><strong id="""">Part 4: UAT Checklist</strong></li></ul><h3 id="""">UAT Checklist</h3><p id="""">To be able to perform seamless user acceptance testing, it is important to ensure that the mentioned test stages are thoroughly covered as part of the UAT for best results.</p><h4 id="""">Initiating the user acceptance testing project</h4><p id="""">This is the first stage that helps to prevent any future issues. If there are several things to be kept in mind and you don’t know what to do initially, here are the stages/checklist of preparation:</p><ul id=""""><li id="""">Make a list and contact your future stakeholders</li><li id="""">Discuss with end-users the objectives, aims, and key deliverables of the project</li><li id="""">Pick a single point of contact for testing in the team</li><li id="""">Ascertain all the documents and the UAT resources</li><li id="""">Create the project template and gear up UAT training for the team</li></ul><h4 id="""">Planning the User Acceptance Testing and define the methodology</h4><p id="""">At this point, make a strategy that holds all the information collected at the previous stage to help you plan the execution and formulate the final results. At this step, make sure to-</p><ul id=""""><li id="""">Recognize to the overall UAT methodology for evaluating the right UAT solution</li><li id="""">Explicitly define the specifications of business and clarify them with the team</li><li id="""">Assess various existing documentation to serve as a reference for test basis</li><li id="""">Ensure that various business requirements are included and documented.</li></ul><h4 id="""">User acceptance testing design</h4><p id="""">This stage clearly states the test points and verifies that the earlier stages are delivered successfully. At this step, make sure to-</p><ul id=""""><li id="""">Set clear expectations at the beginning of UAT</li><li id="""">List test conditions and approaches to kick off UAT</li><li id="""">Define the criteria as well as the test cases based on the existing one</li><li id="""">List testing scenarios and prepare test cases</li><li id="""">Ensure that the test cases consist of all the business requirements</li></ul><h4 id="""">User acceptance testing execution</h4><p id="""">While comprehensive preparation is the key, things can still go wrong during UAT kick-off. Therefore, the project has to be monitored, led, and tracked on all stages to achieve the UAT timeline. At this stage, make sure-</p><ul id=""""><li id="""">Users give enough time to execute the UAT test scripts&nbsp;</li><li id="""">Your team executes the testing as per the defined test plan and strategy</li><li id="""">All defects are reported accurately and promptly&nbsp;</li><li id="""">Conduct meetings daily to communicate status and address concerns as they come up</li><li id="""">Schedule dedicated time for defect resolution and re-testing of functionality</li></ul><h4 id="""">UAT exit</h4><p id="""">This is the final phase and helps produce a transparent and detailed analysis. A powerful sign off on the UAT execution is critical to go live, and to ensure this, make sure to-</p><ul id=""""><li id="""">Generate a clear test exit report offering details of executed tests, bug raised, and existing status of all defects</li><li id="""">Evaluate and accordingly take a call to officially close the UAT phase.</li></ul><h4 id="""">User Acceptance Testing in Agile Environment</h4><p id="""">The agile environment is typically more dynamic, and in an agile world, business users will be fully involved throughout the project sprints, and the project would be accordingly enhanced based on the feedback loops from them.</p><p id="""">In Agile teams, the entire responsibility of maximizing the product's value is with the product owner. The product owner here represents all stakeholders, including customers/users, and is the only authorized entity mentioned in the definition of user acceptance testing. </p><p id="""">The product owner must work in close collaboration with stakeholders to understand their specific expectations and help the scrum team give feedback about the product.</p><p id="""">The feedback that is received during sprint demo and UAT is collated and added back to the product backlog, which is reviewed and prioritized constantly. Overall, in an agile world, the business users are typically more close to the project, and unlike the traditional waterfall projects, they evaluate the same for its use on a more frequent basis.</p><p id="""">If you are a Product Manager or Designer who would like to get a sneak peak at new features being built and need a trusted demo environment to perform User Acceptance Testing (UAT) without a hassle, you should take a look at 👉🏽 <a href=""https://release.com/user-acceptance-testing-with-ephemeral-environments""><em id="""">User Acceptance testing (UAT)</em></a> with Release <a href=""https://docs.releaseapp.io/release-overview#ephemeral-environments"" target=""_blank"" id=""""><em id="""">Ephemeral Environments</em></a>, With Release’s Ephemeral Environments, it’s possible to view a developer’s changes at any time or any place. You don’t need to worry about having the proper environment variables in place or running any code locally.</p><h3 id="""">Conclusion</h3><p id="""">For user acceptance testing to be effective, it is important to view it as validation instead of verification. If approached right, user acceptance testing helps reduce the likelihood of issues that typically happen in web development projects, which reduces the amount of work and effort required in development and maintenance.<br></p>",false,,,,https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63e4251cfb5eaeaf5d92a887_122421%20(1).jpg,a woman sitting at a table writing on a book,tommy-mcclung,5,Sat Dec 25 2021 03:09:00 GMT+0000 (Coordinated Universal Time),,
Using Docker Environment Variables in Compose,using-docker-environment-variables-in-compose,62aa5a70cd5ba27d9d0d718a,63d78067b9892d8fc398aca0,Mon Jan 30 2023 08:31:35 GMT+0000 (Coordinated Universal Time),Thu Apr 20 2023 20:59:31 GMT+0000 (Coordinated Universal Time),,"This post will explain what Docker Compose variables are, their use, and the risks involved with environment variables.","<p id="""">As a developer, you probably use Docker to run your application efficiently. Containerizing helps you avoid the ""but it works on my computer"" problem. You might even use Docker Compose to manage different services that run on different containers.</p><p id="""">Some services managed by Docker Compose, like backend services, may have sensitive information that should be kept secret. This is where you need to use environment variables to specify your configuration. But what are Docker environment variables in Compose?</p><p id="""">This post will explain what Docker Compose variables are, how you can use them in Compose, and the risks associated with putting secrets in an environment variable.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1000px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1000px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63d77e780468790a42a74fa3_aeworqiaCvN8P3hxBHOiX-9mw_86DGTyNTx_iVVgHTBU60jy8lyNTIez8tk3hPzIxCP9-_q6XdfcfCSzp7tYIhhBwcDN35OTzsF9lnXJoIJzMG-kmfUdIAMDwX0qBBVRUaEIGqC0haTZeIMJaGZ2XpoWgw5q7aynwlkJRVoKA2ok0hG-NkIpODSZJ8jb.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><h3 id="""">What is a Docker Environment Variable?</h3><p id="""">Environment variables are used in programs to store values that the program checks at runtime. This means that the value is not stored in code but is instead stored in a separate file.</p><p id="""">A Docker environment variable is a variable that's passed to a Docker container when it's created. You can use environment variables to configure your application. Also, you can use them to store sensitive information like keys and passwords.</p><p id="""">You can set environment variables in several ways, such as using the <strong id="""">ENV</strong> instruction in a Dockerfile, using the <strong id="""">-e</strong> flag when running the <strong id="""">docker run</strong> command, or using environment files.</p><p id="""">When a container is created, the environment variables are passed to it. You can access them within the container. For example, you can access environment variables in a Linux-based container using the <strong id="""">$</strong> notation like <strong id="""">$APP_ENV</strong> or echo $APP_ENV.</p><p id="""">Using Docker environment variables keeps your application configuration flexible.</p><h3 id="""">What is Docker Compose?</h3><p id=""""><a href=""https://docs.docker.com/compose/"" target=""_blank"" id="""">Docker Compose</a> is a tool that spins up instances of your Dockerfile where your Dockerfile is the blueprint of your application. It helps you manage and configure your app's specific requirements. It also gives you the flexibility to define different services your app needs. For example, you may have different Dockerfiles for different services, such as the frontend and the backend.</p><p id="""">By using Docker Compose, you can use one file to configure the relationship between the two services. This single file gives you the ability to use only a single command to build your entire application.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1000px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1000px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63d77e789c1d375c4c50a434_6IWMhSLNcNxbz6IqVzCZd8kaKDha0qJev0mZ8K-mGXfxqqZAC60h6HuGNNfF5S6HXrcixUr5I586jJcNOVAitvWLJ-3kKEXx7NXd1bYQjFIsw6kaCG0hOM3LMrM4lQYeCaLMm1CV6e5AriYWQhK7n8uKkjMmtziC-b-7HiK0bzcFmfGrV6XT6MSlkeK9.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><h3 id="""">Can I use Environment Variables in a Docker Compose File?</h3><p id="""">Yes, you can use environment variables in a Docker Compose file.</p><p id="""">Docker and Compose work together to provide a way to manage and run containers. When using Compose, you define your application's services, networks, and volumes in a single <strong id="""">docker-compose.yml</strong> file.</p><p id="""">Use environment variables to set specific options in the Compose file, such as image name, command, ports, volumes, and links. You can set these values in different ways, such as by using the <strong id="""">environment</strong> key in the compose file, by using the <strong id="""">-e</strong> flag when running the Docker run command, or by using environment files.</p><p id="""">When you run the <strong id="""">docker-compose up</strong> command, Compose reads the <strong id="""">docker-compose.yml</strong> file and creates the specified services, networks, and volumes. As part of this process, Compose also sets the environment variables for each service as specified in the Compose file.</p><p id="""">Using environment variables in a Compose file can make your application more flexible and configurable. You can use different environment variables to set different values for different stages of your application, such as development, staging, and production.</p><h3 id="""">How to use Docker Environment Variables in Compose</h3><p id="""">You can set and pass Docker environment variables in several ways in Compose. Some of these ways include the following:</p><p id=""""><strong id="""">Environment Key:</strong> You can configure a container by setting environment variables in the Compose file. If you want to use your app in production mode, you can set the value of the APP_ENV variable like so:</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
	<code class=""language-yml"">
services:
  web:
    environment:
      - APP_ENV=production
 </code>
</pre></div><p id=""""><strong id="""">-e Flag:</strong> You can also set environment variables when running a container by using the -e flag. For example, you can set the variable APP_ENV with a value of production when running a container like this:</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
	<code class=""language-yml"">
docker compose run -e APP_ENV=production myimage
 </code>
</pre></div><p id=""""><strong id="""">Environment Files:</strong> You can also use environment files to set environment variables. This can be useful when you have multiple environment variables that you want to set or when you want to keep your environment variables separate from your Compose file.</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
	<code class=""language-yml"">
services:
  web:
    env_file:
      - Docker/web/web.env
 </code>
</pre></div><p id="""">To use environment files, you can pass the --env-file flag when running the Compose command: docker-compose --env-file /path/toenv.env up. This will override the default path.</p><p id=""""><strong id="""">.env:</strong> The .env file is a simple text file containing key-value pairs, with one pair per line. The .env file should be in the same directory as the <strong id="""">docker-compose.yml</strong> file. You don't need to pass any flag when running the Compose command. Compose will automatically pick the .env file. If you defined a version to your web app in your .env file, this is how you'll use it in Compose:</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
	<code class=""language-yml"">
services:
  web:
    image: ""webapp:${VERSION}""
 </code>
</pre></div><p id="""">Always remember that environment variables passed to a container are only visible to the processes running in that container. If you need to share environment variables between containers, you can use a tool such as Docker Compose's environment key or a third-party tool like a key-value store.</p><h3 id="""">How to Substitute Environment Variables</h3><p id="""">Using environment variables in Compose allows substituting values at runtime rather than hard coding them in the Compose file. This makes it easy to switch between different<a href=""https://release.com/blog/environments-as-a-service-eaas-top-3-benefits""> environments</a>, such as development, staging, and production, without modifying the Compose file.</p><p id="""">One way to manage this is by using multiple environment files, each with its own values. For example, you can have a <strong id="""">development.env</strong> file with development-specific values and a <strong id="""">production.env</strong> file with production-specific values.</p><p id="""">When running the Compose command, you can specify which environment file to use with the <strong id="""">-f</strong> flag. For example, docker-compose -f docker-compose.yml -f development.env will start the containers with the values specified in the <strong id="""">development.env</strong> file.</p><p id="""">This approach allows you to keep your environment-specific values separate from your Compose file, making it easy to switch between environments and maintain different configurations for different application stages.</p><h3 id="""">The Security Risks of Putting Secrets in Environment Variables</h3><p id="""">There are risks associated with putting secrets such as passwords and API keys in environment variables. Here are a few examples:</p><ul id=""""><li id="""">Anyone with access to the host system can access environment variables. If attackers gain access to the host system, they might access any secrets stored in environment variables.</li><li id="""">Any process running on the host system can access environment variables. Thus, an attacker can run a malicious process to access the sensitive information stored in the environment variable and gain control of your application.</li><li id="""">If an environment variable contains a secret, it might be logged or displayed in plain text, allowing anyone with access to the logs or display to see the secrets.</li><li id="""">Suppose a developer pushes code to a public repository with a file containing secret environment variables. In that case, the secret might be exposed to anyone with access to the repository.</li><li id="""">To mitigate these risks, it's important to be careful when using environment variables to store secrets and to use other secure methods, such as encrypted secrets stores or secret management tools, whenever possible.</li></ul><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1000px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1000px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63d77e78a7491d8f908c38b9_5YMKIxSHncZTS726VP_SDzgl7cgDX00F-FZTFn89Zp4iT2NmZeyC6nqkr8uQjTabd_oCUDN-zUF-EGIQpcChm6k09OYNA2pVJ5jD6KxCY2UMgviwr2LVB624msOqzyBjI2UTtw_VXeDFwIcAurI19kTJxl4kzjDJ7eXidbOicLFCZBlwL2auDjfCyvJP.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><h3 id="""">Conclusion</h3><p id="""">Docker environment variables are useful for configuring and managing containerized applications with Compose. You can use them to pass information to the containers at runtime and to override the default values defined in the Compose file. Be careful when handling sensitive information as environment variables are stored in plain text and visible to any process inside the container. Having looked at how you can use environment variables in Compose, you should also look at the<a href=""https://release.com/blog/environments-as-a-service-eaas-top-3-benefits"" target=""_blank""> benefits of having environments as a service.</a></p><p id=""""><em id="""">This post was written by Mercy Kibet. </em><a href=""https://hashnode.com/@eiMJay"" target=""_blank""><em id="""">Mercy</em></a><em id=""""> is a full-stack developer with a knack for learning and writing about new and intriguing tech stacks.</em></p>",false,,,,,,mercy-kibet,6,,,
Using Environment Variables in Angular: A Guide,using-environment-variables-in-angular-a-guide,62aa5a70cd5ba27d9d0d718a,63ce94589f6192f3624fc45c,Mon Jan 23 2023 14:06:16 GMT+0000 (Coordinated Universal Time),Mon Jan 23 2023 14:11:12 GMT+0000 (Coordinated Universal Time),,What are environments? Why do you need them? And how do you correctly use environment variables in an Angular app?,"<figure id="""" class=""w-richtext-figure-type-image w-richtext-align-center"" data-rt-type=""image"" data-rt-align=""center""><div id=""""><img alt=""angular environment"" src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63ce911f5f00e3ec5625d388_DaRwcr1PtVzmdc3xpUKCsKr1bOhqAZigtxhUc6q0BrSw_azRdWXCMcVkba5JJpA13f-xlew99gYporTf31CIYA3n8prtxSWY7mfP1oIZw5r4sMjwIgBkLJPsynlUJZMeONGsTQ-iAsOZWoFVHryEsrn5cj1dfor9Am2DRztqKbTMA1rP8tjap5vp1n5i.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><p id="""">Angular is one of the most popular and preferred frontend frameworks today, especially among large engineering teams. It's backed by a robust open-source community that makes it an ideal choice for building scalable web applications.</p><p id="""">However, being a frontend framework, your Angular application at some point needs to communicate with an external API or a third-party service application.</p><p id="""">For verifying authentic communication between your Angular application and the server, you'll need to use API keys and URLs that may change based on the environment of your Angular application.</p><p id="""">In this post, I'll walk you through how to use environment variables in an Angular application.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-center"" data-rt-type=""image"" data-rt-align=""center""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63ce911f57a94514b90d6817_GHxxYSGBN0A6IUw0Vb6vo0p366o87PNLgr0GIyE4oXTl1Hmbej_HI7Mpw0I9jOh8syzmKTQb02-Cbu4Ee6_BbaSEAH2aVODobRL_PGd2F-cah7W1vv5d-yAFeVJSLM4aAZctTY2NjIjnlsSoZJCfzts82_EDUi3rMCFkbyNbtK5AK3KXKV496vOmxmPf.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><h2 id=""""><strong id="""">What are Environments in Angular?</strong></h2><p id="""">Before we get into what environment variables are, let's quickly understand what an environment means for Angular. Angular by default provides a way to detect and modify some files in your application based on where the application is running.</p><p id="""">Broadly speaking, Angular considers two environments for any application. One is the development environment, which means you're running Angular locally on your own system. This is when you're actually writing code for your Angular application.</p><p id="""">The other environment is production. This is where your Angular application is running for the end users on a domain or a server. This environment comes into the picture when you create a JavaScript bundle for your Angular application and deploy it to production.</p><h2 id=""""><strong id="""">How Does Angular Know Which Environment?</strong></h2><p id="""">Now that we understand what environments are in Angular, let's see how Angular maintains and keeps track of them.</p><p id="""">Create a fresh Angular app by running the following:</p><div data-rt-embed-type='true'>ng new angular-environment-variables</div><p id="""">To keep track of different environments, Angular maintains an environment directory in the src directory:</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-center"" data-rt-type=""image"" data-rt-align=""center""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63ce911fc83b9e2da7a065eb_HCW2m8hIRXiUH8Q_iEa5V8X4FSwxScf8pHnT_wB-HivcYaisCbtX6HTiYZHkRZZPuZshvbOhifKIOBd3NvwE42SIUYESD0lx93l3e-0d1vkGg6cemodVCms_K0diQlsO3a2fRz_KEVAXRLDdy_SOkxJjvLT3BSw8K-MsP0D3EBF2EOB_wIlunTv0IpdS.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><p id="""">Notice that Angular maintains two environment files called <strong id="""">environment.ts</strong> and <strong id="""">environment.prod.ts</strong>.</p><p id="""">Now, let's go to the <strong id="""">angular.json</strong> file in the root directory. If you look closely at the <strong id="""">configurations</strong> section, you'll notice it has some configurations declared for <strong id="""">production</strong> and <strong id="""">development</strong>.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-center"" data-rt-type=""image"" data-rt-align=""center""><div id=""""><img alt=""angular environment"" src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63ce911f8b7b738344c20754_XiZv7ERdDaiRG3Vuxj7jaPridzINp35kwOrLbbrFt_V0m9g2gxpJ7TSvpOO9g7SmWIjHZoz5jyWXHuLP8Sn2Bzy4RSUxM3y3gUy7z5yXDjpUU5FB_6LstjpW9MKRVe_ZDHl2vqMjkQah0dYmPGSBrhgZSwlDreEL9X6lhjS_q-hKgMjdV_zboQiQKD2H.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><p id="""">If you take a look at the <strong id="""">fileReplacements</strong> array in this file, you'll see it instructs Angular to replace the <strong id="""">environment.ts</strong> file with the <strong id="""">environment.prod.ts</strong> file for production. So, when you run the <strong id="""">ng build</strong> command and deploy your Angular application in production, Angular automatically switches these two files so that the production bundle uses the production environment.</p><h2 id=""""><strong id="""">Detecting Environment in Angular</strong></h2><p id="""">Angular makes it really easy for you to detect the environment you're running it in. There are also other ways you can do this, including by checking the URL of your application's domain. But Angular provides an out-of-the-box solution for this.</p><p id="""">Let's explore this a bit.</p><p id="""">In the <strong id="""">src/app/app.component.ts</strong> file, import the <strong id="""">isDevMode</strong> from <strong id="""">@angular/core</strong>:</p><div data-rt-embed-type='true'>import { Component, isDevMode } from '@angular/core';</div><p id="""">Then, create a variable called <strong id="""">environment</strong> inside the AppComponent:</p><div data-rt-embed-type='true'>export class AppComponent {
  title = 'angular-environment-variables';
  environment = '';
  
}</div><p id="""">Now we can use the <strong id="""">isDevMode</strong> to populate the <strong id="""">environment</strong> variable above:</p><div data-rt-embed-type='true'>export class AppComponent {
  title = 'angular-environment-variables';
  environment = '';
  ngOnInit() {
    if (isDevMode()) {
      this.environment='Development'
    } else {
      this.environment='Production'
    }
  }
}</div><p id="""">Let's now render the <strong id="""">environment</strong> variable in the component's HTML:</p><div data-rt-embed-type='true'><h1>This is Angular app running in {{environment}} Environment</h1></div><p id="""">If you visit your Angular development server, it should tell you that your Angular app is running in a development environment:</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-center"" data-rt-type=""image"" data-rt-align=""center""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63ce911f57a9453e700d681d_MR0KAYPYGWOoGVHVZ24T0Pq-WwF2bem2_BRys7QDBd0dDWW3PVeZJ6VDSK31m2ZIMJ51TYB9FCLnL9KS0KcCdxwyGjc64_xi8n2k1SLgZv-R3jxdrlYLLQuBRk-kCcRqhlPLDR_h9O2lPFME2kS2QvAL-HHr3LXfGJsGz8f8p7r5BZQ7B5EgvIi2nsee.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><p id="""">Great!</p><p id="""">Now let's look at how we can modify or update our environment files to use environment variables in our Angular application.</p><h2 id=""""><strong id="""">Using Environment Variables in Angular</strong></h2><p id="""">Now that you understand how Angular maintains and keeps track of the environment, let's play around with the environment directory that we have in the root directory.</p><p id="""">First, let's look at the default contents of each. The <strong id="""">/src/environments/environment.ts</strong> file contains the following code by default:</p><div data-rt-embed-type='true'>// This file can be replaced during build by using the `fileReplacements` array.
// `ng build` replaces `environment.ts` with `environment.prod.ts`.
// The list of file replacements can be found in `angular.json`.

export const environment = {
  production: false
};

/*
 * For easier debugging in development mode, you can import the following file
 * to ignore zone related error stack frames such as `zone.run`, `zoneDelegate.invokeTask`.
 *
 * This import should be commented out in production mode because it will have a negative impact
 * on performance if an error is thrown.
 */
// import 'zone.js/plugins/zone-error';  // Included with Angular CLI.</div><p id="""">There's a variable or object called <strong id="""">environment</strong>. It has a key called <strong id="""">production</strong>, which is set to <strong id="""">false</strong>. Hence, we can say this is the environment file for the development environment.</p><p id="""">Similarly, if we look at the <strong id="""">/src/environmnets/environment.prod.ts</strong> file, we see this:</p><div data-rt-embed-type='true'>export const environment = {
  production: true
};</div><p id="""">The <strong id="""">production: true</strong> in the code above indicates this file is meant for the <strong id="""">production</strong> environment.</p><p id="""">Now, let's say you wish to add API URLs for the development and production environment. Here's how the <strong id="""">/src/environments/environment.ts </strong>file could look:</p><div data-rt-embed-type='true'>export const environment = {
  production: false,
  apiUrl:'local api url'
};</div><p id="""">And your <strong id="""">/src/environments/environments.prod.ts</strong> file could look like this:</p><div data-rt-embed-type='true'>export const environment = {
  production: true,
  apiUrl:'production api url'
};</div><p id="""">Now, let's go and use these environment variables in our app component. Update the <strong id="""">app.component.ts</strong> file with the following:</p><div data-rt-embed-type='true'>import { Component, isDevMode } from '@angular/core';
import { environment } from '../environments/environment';


@Component({
  selector: 'app-root',
  templateUrl: './app.component.html',
  styleUrls: ['./app.component.css']
})
export class AppComponent {
  title = 'angular-environment-variables';
  environment = '';
  apiUrl=environment.apiUrl;
  ngOnInit() {
    if (isDevMode()) {
      this.environment=`Development`
    } else {
      this.environment=`Production`
    }
  }
}</div><p id="""">Then, we can update the template:</p><div data-rt-embed-type='true'><h1>This is Angular app running in {{environment}} Environment</h1>

<h3>API URL: {{apiUrl}}</h3></div><p id="""">And we should now see the <strong id="""">apiUrl</strong> referring to the development environment on the page:</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-center"" data-rt-type=""image"" data-rt-align=""center""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63ce911fb44e88788cb99df4_ebl34zT12qZ1EB3HEPkzrYEhkSpE8Kc7__1LlidIpW_KpSgG_zile0lGLo8eR_GzMqByHsTjclYEUQNs4l9uTolCrGSHfTVojz_Ej1CFI-cjS5vVX6T1JvOahvezjsGZwQ40gPSyx2JBW2-keAmn0CM_5WiufssO9a1mzOWKld8-vvfOgZQOKzQmXYSC.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><p id="""">But if you close the Angular development server and run it in production mode using this command</p><div data-rt-embed-type='true'>ng serve --configuration=production</div><p id="""">that should render the template for the production environment as shown below:</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-center"" data-rt-type=""image"" data-rt-align=""center""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63ce912170ce9859f1728e97_g5neh-yWIra5WQO50hGPiQyGN8Cyve6iV2Zdjw0EHT9mB64J_kQjrR_gaUJBD9Zs5wkehQwMv5QQD6YUZWVPNZIniV86NArZQWF5WwYo1sqUZVIgKpkSS_gtkTnzG5OzVBCcrFZ0URyTfcORU6TOxlUDTSCnEExlz8iLjTch_t0pnFMWDn-mRbqKOTCv.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><p id="""">Great!</p><p id="""">But what if we had another environment called staging or QA, where we occasionally test our applications?</p><h2 id=""""><strong id="""">Using Staging Environment Variables</strong></h2><p id="""">We can add as many custom environments in Angular as we want. All we need to do is define the relevant environment configuration in the <strong id="""">angular.json</strong> file and then create that environment file in the <strong id="""">/src/environments</strong> directory.</p><p id="""">Let's say we were to add a new staging environment.</p><p id="""">First, under the <strong id="""">build</strong> configurations, we'll add the following <strong id="""">fileReplacements</strong> array for our staging environment:</p><div data-rt-embed-type='true'>{
...
""staging"":{
              ""fileReplacements"": [
                {
                  ""replace"": ""src/environments/environment.ts"",
                  ""with"": ""src/environments/environment.stage.ts""
                }
              ]
     ...
}</div><p id="""">Then, under the <strong id="""">serve</strong> configurations, we'll add the <strong id="""">browserTarget</strong> configuration for our staging environment:</p><div data-rt-embed-type='true'>{ 
...
""staging"":{
              ""browserTarget"": ""angular-environment-variables:build:staging""
            }
  ...
}</div><p id="""">Almost there!</p><p id="""">Now, we'll create a new file called <strong id="""">environment.stage.ts</strong> inside the <strong id="""">/src/environments</strong> directory with the following contents:</p><div data-rt-embed-type='true'>export const environment = {
    production: false,
    apiUrl:'staging api url'
  };</div><p id="""">Awesome!</p><p id="""">Then, all we need to do is run the following command:</p><div data-rt-embed-type='true'>ng serve --configuration=staging</div><p id="""">And you should see your Angular app running in the newly defined staging environment. The <strong id="""">apiUrl</strong> will resolve to the value you specified in the <strong id="""">environment.stage.ts</strong> file:</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-center"" data-rt-type=""image"" data-rt-align=""center""><div id=""""><img alt=""angular environment"" src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63ce911fe75933f798733de4_70KtD9RcGEOr9Pa9V0yp_JdYxHi3PVyrPD10izuxFzkKjkdyvb6F59X451jvXAHH6QEdl2dEKAB0CNpfbdTEVXE_ScultvZC0sG3_yLcx-izznNjpHrk2YXKXK9ZH8UKkgx0KtHnsPZNpR5myBAU8b4ERnWEWPu9K9fUSZdCTc8D5eE9JaGrCj4SneLX.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><h2 id=""""><strong id="""">Security Considerations</strong></h2><p id="""">The concept of environments we've explored in Angular helps us dynamically inject some variables based on the run-time environment of our application.</p><p id="""">However, this is quite different from the environment variables you use in a backend server or a system. Typically, environment variables are defined in the system or on a server where they can't be accessed by anyone else.</p><p id="""">In this case, our environment variables are exported for other components and files to work with. This exposes these environment variables to anyone who's using the frontend application.</p><p id="""">For this reason, you shouldn't store any sensitive credentials in your Angular app's environment variables. For instance, if you have an API key that, if exposed, could cause an attacker to use APIs on your behalf, you shouldn't store them here.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-center"" data-rt-type=""image"" data-rt-align=""center""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63ce91209f619266a54f89c8_i068YVGpKOCMzh75KXx5jhlhwUlnv52y5dRqRVYPcTv6Rodu3geTFITHuhtLSPB2wc8fZr2hd0u7Tqbv8esWVhMR7kI6hMlVXuOGmK_38aMANi-cSXKZdpXVNrC7PXUuQb2Rptcf7tPopIS6NXQ0YeFAbXxrbVSJGNTKVK8wkoazoqmhyoi-mBC8XDQ8.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><h2 id=""""><strong id="""">Conclusion</strong></h2><p id="""">Angular's default support for environments makes it really convenient for developers and testers to build and test the application in different environments.</p><p id="""">You can pretty much create your own custom environment and use it any way you like, as we did here for the staging environment.</p><p id="""">Finally, remember to keep security considerations in mind when using environment variables in any frontend application. If you don't want to manage environments on your own, you can also use an automated environment management service from Release. Learn more about it<a href=""https://releasehub.com/ebook/the-complete-guide-to-automated-software-environments"" id=""""> here</a>.</p><p id="""">We've explored what environments are, why you need them, and how to correctly use environment variables in an Angular application. Hopefully, this has given you the starting point you need to dive deeper into environments and explore further use cases for environment variables in your applications.</p>",false,,,,https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63ce945114bd057609e9f947_env-header.png,,,5,,,
"Webhook Authentication Learnings for GitHub, GitLab, and Bitbucket",webhook-authentication-learnings,62aa5a70cd5ba27d9d0d718a,62aa5a70cd5ba2bc1b0d72e0,Wed Jun 02 2021 17:13:56 GMT+0000 (Coordinated Universal Time),Fri Apr 21 2023 19:46:29 GMT+0000 (Coordinated Universal Time),Fri Apr 21 2023 20:04:43 GMT+0000 (Coordinated Universal Time),I was recently tasked with implementing GitLab support for Release and to complete that task I needed to implement authe,"<p id="""">I was recently tasked with implementing GitLab support for Release and to complete that task I needed to implement authentication for GitLab and a way to handle their webhooks. I completed the authentication first and left the webhook implementation for another pull request as I wanted to refactor the way we were handling all webhooks.</p><p id="""">As I started the webhook work, the state of how Release handled webhooks was that GitHub was using the <a href=""https://github.com/ssaunier/github_webhook"" target=""_blank"" id="""">github_webhook</a> gem and Bitbucket was using some custom built code that lived in a Controller Concern. With the need to add a third client I wanted to align everything into a few classes that allow us to easily onboard more providers if the need ever arose.</p><p id="""">As I finished up the work I thought it would be useful to share some things I found about the differences between the three providers and share some of the code that I wrote in case anyone else is trying to do a similar implementation. First I'll talk about a few things I came across and later, in the Technical Details section, I'll go over a few classes we're using in our Ruby on Rails project for handling the webhooks.</p><p id="""">I enjoyed doing this work because it spanned a lot different aspects of our codebase, including designing a refactor that would allow for processing webhooks from three different sources, reading documentation and understanding three different APIs, and writing tests to ensure that as we move forward we shouldn't need to every worry about breaking our webhook processing.</p><h3 id="""">Authentication Methods</h3><p id="""">Further down in the Technical Details section, I'll show some code for the <strong id=""""><em id="""">Authenticator</em></strong> class so you'll see the implementation around this topic, but I wanted to touch on how each of the three providers handles authenticating the webhooks. We'll start with GitHub.</p><ul id=""""><li id=""""><strong id="""">GitHub</strong> - With a GitHub App, when you create the App, you can supply a secret key which will be used as the basis for authenticating the webhooks for all repositories. Their approach is to combine the payload of the webhook and the secret to generate a hash. The generated hash will be passed as a header when the request is sent to you. The documentation gives an example in Ruby on how to generate your own version of the hash. Then, if the comparison of the two hashes matches, you know the authenticity of the request is valid. I would rate this as the most secure way of authenticating the webhooks because if the request were intercepted and decoded, the secret used to generate the hash is not present anywhere in the request. You can read GitHub's <a href=""https://docs.github.com/en/developers/webhooks-and-events/webhooks/securing-your-webhooks"" target=""_blank"" id="""">Securing your webhooks documentation</a> for yourself if you want to learn more.</li><li id=""""><strong id="""">GitLab</strong> - GitLab follows a similar approach to GitHub, except that a different webhook object must be created on GitLab for each Repository (as opposed to the singular GitHub App). Each webhook installation can take in a secret as it is being created and that secret is sent with the webhook request in the headers. There is no generating of a hash like GitHub, the secret is simply added to the request. Due to the secret being sent in the header, we decided to generate a different secret for every Repository as we save the Repository in the database. This means that if a request were to be intercepted and decoded, at most a single Repository would be compromised. You can read Gitlab's <a href=""https://docs.gitlab.com/ee/user/project/integrations/webhooks.html"" target=""_blank"" id="""">Webhook documentation</a> for yourself if you want to learn more.</li><li id=""""><strong id="""">Bitbucket Cloud</strong> - Bitbucket is set up in a similar fashion to GitLab where we have to create a webhook object for each Repository. However, unlike Gitlab, Bitbucket Cloud does not offer a way to add a secret. I came across a <a href=""https://jira.atlassian.com/browse/BCLOUD-14683"" target=""_blank"" id="""">JIRA Ticket</a> that was created in 2017 outlining this omission of a way to secure the requests but it is currently still open. It is unfortunate that Bitbucket doesn't offer a way to authenticate the webhook requests as it would allow someone to potentially send requests with bad information. They do offer an alternative solution in their documentation about whitelisting specific IPs that the requests could come from, but my opinion is that they should implement adding the secret and at least follow in GitLab's approach to send the secret in the headers. You can read Bitbucket Cloud's <a href=""https://support.atlassian.com/bitbucket-cloud/docs/manage-webhooks/"" target=""_blank"" id="""">Manage webhooks documentation</a> for yourself if you want to learn more.</li></ul><h3 id="""">The Action Is Separated From The Event</h3><p id="""">One aspect I really like with Github and GitLab is that they differentiate their webhook request through an event and an action. The event is sent as a header in the request and an example would be <em id="""">pull_request</em> (on Github) or <em id="""">merge_request</em> (on GitLab). There are many different things that can happen with a Pull Request though: it might be one of opened, closed, merged, reopened, and so on. Those different actions that could happen on the Pull Request are sent over in the payload as the key <em id="""">action</em> with the value of the aforementioned states. From a coding perspective this event and action pattern allowed me to create a method, say <em id="""">process_pull_request</em> and inside of that method, handle the many different actions that could occur in another method, say <em id="""">pull_request_opened</em>. I found that designing the code this way allowed for a good abstraction and thorough unit testing of all the different action methods.</p><p id="""">The outlier is Bitbucket which added the event and the action together in the header. For example, when a Pull Request is created, the header contains <em id="""">pullrequest:created</em>, when closed <em id="""">pullrequest:rejected</em>, and when merged <em id="""">pullrequest:fulfilled</em>. When using Release for ephemeral environments, closing and merging a Pull Request are considered the same type of action: we will destroy that ephemeral environment. But since the header contains two different values, I had to implement two different methods: <em id="""">process_pullrequest_rejected</em> and <em id="""">process_pullrequest_fulfilled</em> which simply call another method. While it is a pretty minor inconvenience, I like the code pattern of the action and event separated compared to having them combined.</p><h3 id="""">Technical Details</h3><p id="""">First and foremost I want to acknowledge the great work on the <a href=""https://github.com/ssaunier/github_webhook"" target=""_blank"" id="""">github_webhook</a> gem as I used a good amount of what they had done to create the foundation for the <strong id=""""><em id="""">Authenticator</em></strong> and <strong id=""""><em id="""">Processor</em></strong> classes. What follows is the Ruby code I wrote to manage the webhooks from the three providers that Release currently supports.</p><h3 id="""">The Authenticator</h3><p id="""">First up, we'll look at the <strong id=""""><em id="""">Authenticator</em></strong> class. Its purpose is to authenticate the webhooks that we are receiving to ensure that they're valid. You'll see that there is an optional parameter for the Repository and it is optional because as I mentioned in the Authentication Methods above, for GitHub we have a single secret, while for GitLab and Bitbucket a secret is generated for each Repository.</p><p id="""">Aside from the initialization method, the class has a single public method, <em id="""">authenticate_request!</em> which does as it is named. It will raise an error if the authenticity of the request cannot be validated otherwise the call will return. The <em id="""">expected_signature</em> method follows the different providers implementations with GitHub needing to create a hash to compare, GitLab needing only the secret, and Bitbucket currently using a random string due to not offering an authentication method.</p><div data-rt-embed-type='true'><pre class=""language-ruby line-numbers"">
<code class=""language-ruby"">
module Webhooks
  class Authenticator
    class SignatureError < StandardError; end

    def initialize(request:, vcs_type:, repository: nil)
      @request = request
      @vcs_type = vcs_type
      @repository = repository
    end

    def authenticate_request!
      secret = client_secret(@vcs_type, @repository)
      request_signature = signature_header(@vcs_type, @request)
      expected_signature = expected_signature(@vcs_type, secret, @request)

      unless ActiveSupport::SecurityUtils.secure_compare(request_signature, expected_signature)
        raise SignatureError
      end
    end

    private

    def request_body(request)
      @request_body ||= (
        request.body.rewind
        request.body.read
      )
    end
  
    def signature_header(vcs_type, request)
      @signature_header ||= (
        case vcs_type
        when :github
          @request.headers['X-Hub-Signature-256']
        when :gitlab
          @request.headers['X-Gitlab-Token']
        when :bitbucket
          'bitbucket_cloud'
        end
      )
    end

    def expected_signature(vcs_type, secret, request)
      digest = OpenSSL::Digest.new('sha256')

      case vcs_type
      when :github
        ""sha256=#{OpenSSL::HMAC.hexdigest(digest, secret, request_body(request))}""
      when :gitlab
        secret
      when :bitbucket
        'bitbucket_cloud'
      end
    end
  
    def client_secret(vcs_type, repository)
      case vcs_type
      when :github
        Clients::Github.webhook_secret
      when :gitlab, :bitbucket
        repository.webhook_secret
      end
    end
    
  end
end
</code>
</pre></div><h3 id="""">The Processor</h3><p id="""">If the request is authenticated, then we need to process the payload that comes with the request and the <strong id=""""><em id="""">Processor</em></strong> class does just that. It will look through the payload and try to find the associated Repository in our database, if that Repository cannot be found, then an error occurs. To determine what event occurred, we look through the different headers in the request and parse the value into Ruby method declaration form by replacing any non-word character with an underscore. Based on the provider who sent the request, a service object is initialized and then we attempt to call a <em id="""">process_</em> method. Some webhooks we receive are for things Release doesn't deal with, for example GitHub's <em id="""">issues</em> webhooks, so we safely <em id="""">try</em> the method as there may not be an implemented <em id="""">process_</em> method.</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-ruby"">
module Webhooks
  class Processor
    def initialize(request, vcs_type)
      @request = request
      @payload = json_body(request)
      @vcs_type = vcs_type
      @repository = repository_from_payload(@vcs_type, @payload)
      
      @webhook_service = webhook_service(@vcs_type, @payload, @repository)
    end

    def repository
      @repository
    end

    def process_webhook
      process_method = ""process_#{event_method(@vcs_type, @request)}""
      @webhook_service.try(process_method)
    end

    private

    def json_body(request)
      payload = request.body.read
      ActiveSupport::HashWithIndifferentAccess.new(JSON.load(payload))
    end

    def repository_from_payload(vcs_type, payload)
      provider_repository_id = provider_repository_id(vcs_type, payload)
      if provider_repository_id
        Repository.find_by!(type: ""Repositories::#{vcs_type.capitalize}"", provider_repository_id: provider_repository_id)
      end
    rescue ActiveRecord::RecordNotFound => error
      # Re-Raise the error with info from the payload so we know what the repository is
      repository_info = payload.dig('repository', 'full_name')
      new_error = ActiveRecord::RecordNotFound.new(error.message + ""Repository Info: #{repository_info}"")
      new_error.set_backtrace(error.backtrace)
      raise new_error
    end

    def provider_repository_id(vcs_type, payload)
      case vcs_type
      when :github
        payload.dig('repository', 'id')
      when :gitlab
        payload.dig('project', 'id')
      when :bitbucket
        payload.dig('repository', 'uuid')
      else
        nil
      end
    end

    def event_method(vcs_type, request)
      @event_method ||= 
        (
          case vcs_type
          when :github
            request.headers['X-GitHub-Event']
          when :gitlab
            request.headers['X-Gitlab-Event']
          when :bitbucket
            request.headers['X-Event-Key']
          else
            nil
          end
        )&.downcase&.gsub(/\W/, '_')&.to_sym
    end

    def webhook_service(vcs_type, payload, repository)
      service_class = ""Webhooks::#{vcs_type.to_s.capitalize}""
      service_class.constantize.new(payload, repository)
    end
  end
end
</code>
</pre></div><h3 id="""">GitHub Webhook Service</h3><p id="""">The last method in <strong id=""""><em id="""">Processor</em></strong>, <em id="""">webhook_service</em> returns a service class that goes through our internal business logic of what we want to do with the webhook. I'm going to provide a small snippet of the GitHub service when we receive a Pull Request webhook. If you recall, I mentioned this method in the ""The Action Is Separated From The Event"" section and how I liked this pattern of structuring the code. If someone else were to look at this code, I would hope they would find it easy to understand that anything to do with GitHub Pull Request webhooks happens inside of the <em id="""">process_pull_request</em> method and the <em id="""">case</em> statement handles all the different actions that can take place.</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-ruby"">
module Webhooks
  class Github
    def initialize(payload, repository)
      @payload = payload
      @action = @payload.dig('action')

      @repository = repository
    end

    def process_pull_request
      if @action.nil?
        error_message = ""ERROR: Pull Request no action received, payload : #{@payload}, do nothing""
        Rails.logger.error(error_message)
      else            
        message = ""Pull Request with action : *#{@action}*. Received Repository : #{@repository.name}.""
        Rails.logger.info(message)
  
        case @action
        when 'opened', 'reopened'
          pull_request_opened
        when 'closed'
          pull_request_closed
        when 'labeled'
          pull_request_labeled
        else
          message = ""Pull Request with action : *#{@action}*. Nothing to do for now.""
          Rails.logger.info(message)
        end
      end
    end
  end
end
</code>
</pre></div><h3 id="""">The Controller</h3><p id="""">The final piece to tie everything together is the controller. <strong id=""""><em id="""">WebhooksController</em></strong> is the base class and each subclass implements only the <em id="""">vcs_type</em> method. Our previous approach had custom code for each of the <em id="""">Webhooks::GithubController</em> and <em id="""">Webhooks::BitbucketController</em>. This meant that each required a ton of specific tests to ensure that we were processing all the different webhooks correctly. My refactored approach moved all that logic out of the controller and aimed for the smallest footprint possible to make testing as simple as possible.</p><p id="""">There is only one route in the controller, which is a <em id="""">POST</em> to <em id="""">create</em>. I decided that since the Repository may be optional in the <strong id=""""><em id="""">Authenticator</em></strong> that I will store it in the <strong id=""""><em id="""">Processor</em></strong> and pass it into the <strong id=""""><em id="""">Authenticator</em></strong>. Otherwise you can see that the public methods for each of the classes are called. If an error is raised by either, due to an unauthenticated webhook or possibly a webhook for a Repository we don't have in our database, we'll capture the error and log as much information as possible so that we can look into what went wrong.</p><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-ruby"">
class Webhooks::GithubController < WebhooksController
  vcs_type(:github)
end
</code>
</pre></div><div data-rt-embed-type='true'><pre class=""line-numbers"">
<code class=""language-ruby"">
class WebhooksController < ActionController::Base
  skip_before_action :verify_authenticity_token

  def self.vcs_type(vcs_name)
    define_method :vcs_type do
      vcs_name
    end
  end

  rescue_from StandardError do |error|
    payload = @webhook_service&.payload

    backtrace_cleaner = ActiveSupport::BacktraceCleaner.new
    cleaned_backtrace = backtrace_cleaner.clean(error.backtrace)

    error_message = ""Error in #{self}! Message : #{error.message}\nPayload : #{payload}\nBacktrace : #{cleaned_backtrace.join(""\n"")}""
    Rails.logger.error(error_message)
    head :bad_request
  end

  def create
    processor = Webhooks::Processor.new(request, vcs_type)
    repository = processor.repository
    
    authenticator = Webhooks::Authenticator.new(request: request, vcs_type: vcs_type, repository: repository)
    authenticator.authenticate_request!

    processor.process_webhook

    head :ok
  end
end
</code>
</pre></div><h3 id="""">Conclusion</h3><p id="""">That's a wrap on my stint in refactoring our webhook code to work with GitHub, Bitbucket, and GitLab. If we ever have to add another provider I think it will be quite straightforward and I hope you enjoyed taking a peek inside some development work at Release. If you're interested in having an ephemeral environment created whenever we receive a Pull Request webhook from your Repository, head on over to the <a href=""https://release.com"">homepage</a> and sign up!</p><p id="""">Photo by <a href=""https://unsplash.com/@brookanderson?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText"" target=""_blank"" id="""">Brook Anderson</a> on <a href=""https://unsplash.com/?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText"" target=""_blank"" id="""">Unsplash</a></p>",false,,,,https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63e4046e3ad3feb3d840d66e_060221%20(1).jpg,Rope passing on rock climbing pitons conveying the idea of Webhook Authentication,jeremy-kreutzbender,9,Wed Jun 02 2021 17:13:00 GMT+0000 (Coordinated Universal Time),,
What are the Challenges Faced During UAT Testing?,what-are-the-challenges-faced-during-uat-testing,62aa5a70cd5ba27d9d0d718a,62aa5a70cd5ba23a810d72f5,Wed Jan 05 2022 02:31:00 GMT+0000 (Coordinated Universal Time),Fri Apr 21 2023 19:40:55 GMT+0000 (Coordinated Universal Time),Fri Apr 21 2023 20:04:43 GMT+0000 (Coordinated Universal Time),In this blog post you'll learn the challenges in UAT testing and examples of user acceptance testing environment.,"<p id="""">This is a 4-part series on User Acceptance Testing (UAT)</p><ul id=""""><li id="""">Part 1: <a href=""https://release.com/blog/user-acceptance-testing-best-practices"" id="""">What is User Acceptance Testing and its Best Practices</a></li><li id="""">Part 2: <a href=""https://release.com/blog/how-do-you-prepare-for-user-acceptance-testing"" id="""">How to Prepare for User Acceptance Testing?</a></li><li id=""""><strong id="""">Part 3: User Acceptance Testing Challenges &amp; UAT Environment Examples</strong></li><li id="""">Part 4: <a href=""https://release.com/blog/user-acceptance-testing-checklist"" id="""">UAT Checklist</a></li></ul><h3 id="""">User Acceptance Testing Challenges</h3><p id="""">User acceptance testing often uncovers various challenges and clarifies requirements for the software. In some cases, the users might also find issues specific to the aspects of their platform that were not tested in other environments.</p><p id="""">Here are some of these challenges that you might face during UAT:</p><h4 id="""">Mimicking the right testing environment<br></h4><p id="""">UAT is often conducted in the same environment used by the design team, potentially bypassing most of the real-world issues that will likely arise for the end-user. We highly recommend <a href=""https://releasehub.com/user-acceptance-testing-with-ephemeral-environments"" id="""">replicating your production environment</a> and making sure you perform UAT on a replica that is as close to production as possible including the right data, services, infrastructure to test both the functionality, user experience and performance. Given the complexity of today’s software architecture, too often organizations settle for an environment that is limited thus many issues slip to production.</p><p id="""">Many times UAT requires testing different types of users and different states hence in some cases multiple environments might be required to properly run UAT.&nbsp;</p><h4 id="""">Determining time frames<br></h4><p id="""">This is another challenge that you might face when you're defining the UAT project plan at the beginning of a project. It is recommended to always include criteria for the standard time frame that the organization expects. While most places accept two weeks, it should be ideally defined for each UAT project.</p><h4 id="""">Reviewing your test plans<br></h4><p id="""">UAT test plans can have errors similar to any other type of software project documentation. To navigate this effectively, UAT plans can be reviewed by the UAT team, QA team, project manager, facilitator, or anyone else with knowledge of testing and the project.</p><h4 id="""">Ambiguous requirements<br></h4><p id="""">Ambiguous requirements will typically bubble up during UAT as the tester needs to decide whether a certain experience meets the requirements or not. If requirements are not well defined, it would be up to the tester’s own judgement to check the box or not on certain requirements.&nbsp;</p><p id="""">If the requirements are not well defined log it as a defect. The end-user/customer then expects these errors to be fixed in the current release without considering the time for any change requests and impact on the release plan.&nbsp;</p><h4 id="""">Asking functional test team to perform user acceptance testing<br></h4><p id="""">Asking the functional test team to perform UAT just to offload the responsibility to the test team for reasons such as lack of resources can. The purpose of UAT testing gets compromised in such cases, and you also run the risk of the end-users quickly spotting the issues that are not considered real-world scenarios by the functional testers.</p><h3 id="""">User Acceptance Testing Environment Examples</h3><p id="""">Depending on what exactly you are evaluating, there can be various user acceptance test scripts that may need a variety of UAT templates or UAT environments. UAT environment template is primarily a data and information collection tool that helps testers gather feedback so they can improve their end product.</p><p id="""">Below are some of the UAT environment examples:</p><h4 id="""">Single-purpose UAT environment<br></h4><p id="""">When a developer wishes to test a particular aspect of their product or software, the best option is a single-purpose UAT environment as it clearly outlines the test and its description along with different parameters.</p><h4 id="""">Priority-based UAT environment<br></h4><p id="""">When there is a range of aspects to be evaluated and assessed, it is best to use an environment that offers the option to prioritize different testing criteria. This allows developers to address various critical issues, followed by focusing on small bugs and fixes.<br></p><h4 id="""">Multi-purpose UAT environment<br></h4><p id="""">In case a tester or developer is looking to evaluate a range of different applications, it is always best to rely on a flexible user acceptance testing environment that also enables them to accumulate data regarding their product.</p><h4 id="""">Customer-focused UAT scenario<br></h4><p id="""">Testers and developers use this UAT environment when they wish to involve customers in the testing process as it enables them to engage the target audience and also collect relevant data when it comes to addressing various customer-oriented issues.<br></p><h3 id="""">Disadvantages of Acceptance Testing</h3><p id="""">While there are several benefits of UAT, there are some disadvantages too. For instance, as per the testing plan, the customer has to write their requirements in their own words and by themselves. However, there are two main problems here-</p><ul id=""""><li id="""">Customers are not willing to do this, and it defeats the entire purpose of acceptance testing.</li><li id="""">In case the test cases are written by someone else, the customer does not understand them. The tester then has to perform the inspections by themselves only.</li></ul><p id="""">If the process of UAT is done in this manner, it completely defeats the very purpose of the acceptance testing.<br></p>",false,,,,https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63e42487abd173476ce125ee_122021%20(1).jpg,a woman jumping in the air with a beautiful view as background,tommy-mcclung,5,Tue Dec 21 2021 02:35:00 GMT+0000 (Coordinated Universal Time),,
What Is a Production Environment and Why Is It Unique?,what-is-a-production-environment-and-why-is-it-unique,62aa5a70cd5ba27d9d0d718a,6410b9c43cbc210c588608c4,Tue Mar 14 2023 18:15:32 GMT+0000 (Coordinated Universal Time),Thu Apr 20 2023 23:01:45 GMT+0000 (Coordinated Universal Time),Fri Apr 21 2023 12:25:49 GMT+0000 (Coordinated Universal Time),What is a production environment? What makes it special? Learn to make your production environment better!,"<p id="""">Is the code ready for production? Can we test that in production? How will this configuration change affect production? These questions, and ones like them, are issues that developers and engineers hear and ask nearly every day. But what is a production environment? What makes it different from development, user-acceptance testing, or <a href=""https://release.com/staging-environments"" id="""">staging</a>? What does the name ""production"" imply, and what do you need to keep in mind when using it?&nbsp;</p><p id="""">Let's take a close look at production environments. We'll talk about what an environment is, what makes an environment ""production,"" what can go wrong in a production environment, and how to improve it.&nbsp;</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:870px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""870px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/6410b7f4d72122a86dd66168_fOkJxUa2JJQyQhpBl27rBtay3JQ_b1X1fpqnbFlCZAsmC_PQRZJpAkHG6bJLQcjw6rOh_0_Zkt5eLa2tO-33Gh1bi8Jt9MdM_H9mZoPuIqQe47GTHWv2ZRcfxIsa-AwRlVD-BwMRGid2UcZBK5NstQ.jpeg"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><h3 id="""">Environments</h3><p id="""">Before we delve into what makes a production environment ""production,"" let's discuss what environments are and how to think about them.&nbsp;</p><h3 id="""">What Is an Environment?</h3><p id="""">An environment, or more formally a <a href=""https://en.wikipedia.org/wiki/Deployment_environment"" target=""_blank"" id="""">deployment environment</a>, is a set of computer systems and services that host and support a software application. The size and scope of applications vary, and environments have to scale to support them. So, it’s useful to define the term application before talking about environments.&nbsp;</p><p id="""">When we're talking about an application deployed to an environment, we're usually talking about an application composed of several, if not dozens of programs that work together to serve a business need. For example, the application might be an online store, a trading platform, or software as a service (SaaS) that provides capabilities to client businesses.&nbsp;</p><p id="""">The requirements for these applications directly affect the size and scope of the environments that support them. A development environment could be a single system running in a few containers. A production environment might span 100s of virtual machines. Two different environments can support the same application in two very different contexts. A developer can run a containerized application on their laptop for testing purposes, while the production environment requires ten or more virtual machines.&nbsp;</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1000px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1000px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/6410b7f421a63e588079f2e0_7SuCuyDe8-naDaPD2XPql51PXn6_M3E8Lbo9DdaTqt_kt_I6XXhwjTo9rS1ESkhkA7cTHURjCtVaEfGNyFipTPnHHhSIwo5x38l5y62t6yT8i2DYik8Vd5CpamJujV4iHqNmyYNomIQqQOQsofT0lg.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><p id="""">You build environments to satisfy the <a href=""https://en.wikipedia.org/wiki/Release_management"" target=""_blank"" id="""">release management cycle</a>. You do your coding, troubleshooting, and initial testing in a development environment. Then you conduct testing in User Acceptance Testing (UAT) environments. You might test your final configuration and perform more intense testing in a staging environment. Finally, you serve your customers from the production environment.&nbsp;</p><h3 id="""">What Are Environments Composed of?</h3><p id="""">When we talk about environments, especially production, we tend to focus on the systems that run the core software stack. A typical scenario might include a database server (or cluster), <a href=""https://docs.release.com/guides-and-examples/advanced-guides/microservices-architecture"" target=""_blank"" id="""">microservices</a>, customer-facing web server, and network gear. These may be managed resources, cloud native services, or ephemeral systems in a cloud environment, but they tend to be ""what we talk about when we talk about environments.""</p><p id="""">But environments are more than the systems and services they run on. An application has code, but it has data in the form of configurations, user information, and other resources, too. In most cases, that data differs between environments. That data is part of the environment. When the developer runs the environment on their laptop, they create a development environment. Their system configuration is very different from what's required to support the production environment and they are usually not using live user information.&nbsp;</p><p id="""">In today’s complex software architecture environments also include cloud native services, and security policies governing access requirements to various systems and services.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1552px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1552px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/6410b96ad1f0aa9373231b33_122429.jpg"" loading=""lazy"" id="""" width=""auto"" height=""auto""></div></figure><h3 id="""">What Is a Production Environment?</h3><p id="""">Simply put, your production environment is where your application meets your primary users. This applies to external (public, customers consuming your services) or internal (enterprise, users within the organizations for whom the application is built). Let's look at what that means and how it affects how you treat this environment.&nbsp;</p><h3 id="""">What Makes Production Unique?</h3><h4 id="""">Revenue and Customer Satisfaction</h4><p id="""">For almost all businesses, the fact that production supports their users means that it represents their primary source of revenue. In many cases, clients are paying for access to the application, (or internal users are using it to do business directly). Other clients may pay for ""offline"" services, but production is how they interface with the company. In all cases, if the production environment isn't functioning correctly, revenue is in danger.&nbsp;</p><p id="""">If clients can't access your application, they can't give (or make) you money. Every minute of downtime means lost revenue. When a major internet service provider reports downtime, the headlines usually contain ""lost"" revenue numbers.&nbsp;</p><p id="""">But a production outage has a long-term effect, too. While your application is down, there's an excellent chance your customers will take their business to someone else. If they have a good experience with your competitor, they may not come back.&nbsp;</p><h4 id="""">Security</h4><p id="""">If users use your production information, it's available to the public, which means it's in constant danger from attack. Production environments have extraordinary security requirements.&nbsp;</p><p id="""">Attackers can cause problems in many ways. One is extortion via ransomware. If they can compromise even one system, they can install code that spreads throughout your production environment and forces you to either pay them and hope they follow through or start from scratch and build a new environment.&nbsp;</p><p id="""">Another attacker may break in to steal user information. This attack exposes you to legal and financial liability. It can have a fatal impact on your reputation, too.&nbsp;</p><h3 id="""">What Are the Risks to Production?</h3><p id="""">We've talked about what can go wrong if your production environment is broken or compromised. What are the common causes of these issues?&nbsp;</p><h3 id="""">Insufficient or Incorrect Testing</h3><p id="""">In many organizations, testing is like the weather; everyone talks about testing, but no one does anything about it. A test plan missed an issue because it didn't exist, or it wasn't executed, or the test plan failed to account for the issue.&nbsp;</p><p id="""">The first two scenarios are probably more common than many of us would like to admit.&nbsp;</p><p id="""">The third scenario points back to the definition of an environment. User acceptance testing doesn't work if your <a href=""https://release.com/blog/what-are-the-challenges-faced-during-uat-testing"" target=""_blank"" id="""">environment doesn't look like production.</a> If, for example, your production environment has more than 30 virtual machines, but your development environment only has four, testing in development doesn't cover all of your cases. It's too expensive to reproduce your production environment for every developer or development team. But you still need to find a cost-effective way to <a href=""https://release.com/usecase/on-demand-staging-environments"" target=""_blank"" id="""">build a staging environment</a> that looks like production.&nbsp;</p><h3 id="""">Security and Access Control</h3><p id="""">Security is an ongoing concern for everyone. We already covered the risks that a breach represents above. Proper access control goes a long way toward preventing these issues. Access control can also act as a check against incorrect configuration changes and premature releases.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1000px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1000px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/6410b7f4a0c8032bf809ca33_IQWslaZYHj3U54PAq_3INkgNNI42-iTyf_xwcHn3IX9vd5EMtc7OrtpRo-SiPGSILEqGBVpv9x5ZUlJtBOXRTbGTFgwNCfj9nz74x7RcykQQZCuDYdD874i9XOmqFdJn2Mt3f8M72x_Jp9O8PHRK6Q.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><p id="""">Another way to tighten security is via automation. Automation is an effective tool for enforcing the ""<a href=""https://release.com/usecase/on-demand-staging-environments"" target=""_blank"" id="""">principle of least privilege</a>"" because you can delegate deployment and configuration tasks to entities with limited privileges.&nbsp;</p><p id="""">To protect yourself from data loss, recognize that any system storing user information is part of the production environment. If you don't want to extend production into that system, don't let the data go there. If the system is storing data that you can't afford to lose, confine it to the strictest level of access control possible.&nbsp;</p><h3 id="""">How Can Release Help?</h3><p id="""">Developing and testing for today’s architectures can get complex. Things work well in development and staging, then they break down in pre-production and you have to rollback. Sound familiar?</p><p id="""">Creating an up-to-date production replica will significantly reduce your rollback rate. When you test with the right dataset, security policies, and services you remove the differences between your environments, and testing is more effective.&nbsp;</p><p id="""">Better user acceptance testing makes your production environment better, and the <a href=""https://release.com/blog/how-do-you-prepare-for-user-acceptance-testing"" target=""_blank"" id="""">best way to accomplish that</a> is with an environment that looks like production. What if you could create a replica of your production environment, perform your testing, and then <a href=""https://release.com/ephemeral-environments"" target=""_blank"" id="""">tear it down until you need it next time</a>? What if your user acceptance testing could help you find shortcomings in your production environment and not just in your code?&nbsp;</p><p id="""">Do you need a large dataset to test or code against? Don't risk your production data; use an <a href=""https://docs.release.com/reference-documentation/instant-datasets-aws"" target=""_blank"" id="""">instant dataset</a> and throw it away when you're finished.&nbsp;</p><h3 id="""">Multi-Cloud Deployments of Private Applications</h3><p id="""">Software companies who, for security reasons, deploy their application on customers' private clouds often find it challenging to ensure that application works on every cloud vendor.</p><p id="""">Release makes it easy to deploy your private application in AWS or GCP without refactoring. We create an abstraction of the cloud native services, making it easier to support and maintain your application without limiting yourself to one cloud vendor or another.</p><h3 id="""">Build a Better Production Environment</h3><p id="""">This post talked about the essential parts of an environment and how you need to examine those parts when comparing one domain to another. Then we discussed production and what makes it unique when compared to other environments. Finally, we talked about how you take positive steps to improve the reliability of your production environment.&nbsp;</p><p id="""">Release has the tools you need to build better environments. Discover <a href=""https://release.com/use-cases"" target=""_blank"" id="""">ways</a> to improve production development speed and flexibility, and equip your testing and engineering teams with better tools today, <a href=""https://release.com/get-started"" id="""">start for free</a>!</p>",false,,,,https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/64113fba23a773eb4e6723ce_122429.jpg,What Is a Production Environment and Why Is It Unique?,eric-goebelbecker,7,Tue Mar 14 2023 18:15:00 GMT+0000 (Coordinated Universal Time),,
What Is a QA Environment and How Do You Manage It?,what-is-a-qa-environment-and-how-do-you-manage-it,62aa5a70cd5ba27d9d0d718a,63a2e03e2f54a5b66a100c56,Wed Dec 21 2022 10:30:22 GMT+0000 (Coordinated Universal Time),Thu Apr 20 2023 21:11:24 GMT+0000 (Coordinated Universal Time),Thu Apr 20 2023 21:44:19 GMT+0000 (Coordinated Universal Time),This article will discuss the importance of a robust QA environment and give some practical tips on how to manage it.,"<p id="""">""QA"" is an umbrella term that covers many different areas of a company's operations. It can describe various tasks, such as the testing and quality assurance process, test methods, and processes. These terms are often used interchangeably but should not be interpreted literally. The term ""<a href=""https://en.wikipedia.org/wiki/Quality_assurance"" target=""_blank"" id="""">QA</a>"" describes how software was tested during its production process before it was released.&nbsp;</p><p id="""">Software testing is critical to the software development life cycle. It helps ensure that the final product meets its requirements before being released into production. Software testers perform various tasks, such as verifying that the application functions correctly, identifying bugs, and making sure the application complies with relevant regulations. They also test the user interface and usability of the application. &nbsp;</p><p id="""">Managing quality assurance is a crucial aspect of software development. Unfortunately, it's often overlooked due to time constraints. And many companies use manual testing or simply don't test at all. A QA environment is a type of test environment specifically designed to support software quality. In other words, it's a test environment with a set of requirements containing elements specifically intended to support software verification. &nbsp;</p><p id="""">Today, organizations are using QA environments more and more frequently as part of the development process. This article will discuss the importance of a robust QA environment and give some practical tips on how to manage it. Read on to learn more!&nbsp;</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63a2dff7022b484eba1c221c_j4_fzmZYLt9PT9VCoV-ZX5Tg-4T2ehV9hShJN7AaLa_Mj3fAdYmIl85mLe6vEAd0NrB1OhMQJlh747Zeti460muD8peJXJo1vEic-6XGZMLnmwtd0gWaZOm8149JowLGTcXXNJDwsOq7PhqaPZ6lhRZcPCabq7hC7nI0b1XvYrMqpLVG54OFWS_F36Bp.jpeg"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><h3 id="""">What Is a QA Environment, and What Is Its Purpose?</h3><p id="""">A QA environment is a testing environment used to validate a software application's quality before it is deployed to production, and to optimize software development processes so that the software works. This environment is typically isolated from other environments, such as development and staging, to ensure that any issues that are found in the QA environment do not impact the live production system.&nbsp;</p><p id="""">QA environments can monitor software development processes periodically or continuously. Continuous monitoring means your software developer continuously monitors and records data about how their project is going, how much time they spend on each process, and how many bugs they find during tests and feature additions.&nbsp;</p><p id="""">Periodic testing is where you have one set of developers doing one type of testing for one period of time. This way, you have more consistent quality throughout your entire team. You can also ensure that all updates and additions are properly accounted for. Additionally, you don't have to constantly monitor and record data about each process in your organization.&nbsp;</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1000px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1000px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63a2dff7a129843b5b4df18b_OrHxUK-45BBhZoq-53FyyzWrMOFdhTw4DTaUZeT5Irzy5NMdoM4Iv0Q5nblKIlS1n0jdOkrjTvRnzOmv20Ei9rzV73oE3WwgyWuSIU6EaklxROQQqPvpVD8COxKXGDdvma7azlSjV6ZlFWWzglpQ952qv2oZTBSR4AVPzGXN9Zkl-ME_ZVcnyzd6mEVr.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><p id="""">Instead, a developer manually checks in with their manager when they're done with their process check-in. They then perform another check-in after every feature addition or process update. This is because the outlying cases are so widespread that it would be too difficult for them to catch up if they were constantly monitoring everything.&nbsp;</p><h3 id="""">Why Do We Need a QA Environment?</h3><p id="""">QA environments are often referred to as ""<a href=""https://dzone.com/articles/5-principles-of-production-readiness"" target=""_blank"">production ready</a>."" This means they meet specific minimum requirements before they can be rolled out to production for general consumption. The goal of QA is to ensure that bugs don't slip through during development. It helps ensure that the final product meets its specifications and performs flawlessly once released into production.&nbsp;</p><p id="""">A realistic test environment—the production equivalent of an actual system deployment—is what you need to manage a realistic QA environment. There may be no specific reason why your QA environment should meet these minimum requirements, but it might be more effective if you have one.&nbsp;</p><h3 id="""">How Do You Create a QA Environment?</h3><p id="""">Here is what you need to know about creating and running a quality-aware QA environment. To set up a QA environment, you'll need to determine the kind of infrastructure to test and which testing technique to use.&nbsp;</p><p id="""">You'll also need to choose the testing tools and methodologies. Once the infrastructure is set up and configured, you'll need to <a href=""https://release.com/blog/setup-test-environment"">create a test environment</a>. This will be the environment in which your application will be tested.&nbsp;</p><p id="""">Once the environment is set up, you'll need to create and run tests. The tests will verify that your application works as expected and that the infrastructure underneath can run your application smoothly.&nbsp;</p><p id="""">When the tests are complete, you'll need to analyze the results and make changes to your application as necessary. &nbsp;</p><h3 id="""">Understanding the QA Environment Using an Example</h3><p id="""">Let's take an example of a SaaS product. The application side is for existing users, and there's a set of landing pages as a part of the marketing campaign. The testing that needs to be done and the scope of QA could be, say, the performance of the application and the static pages.&nbsp;</p><p id="""">Many tools analyze the performance of web applications and landing pages that organizations widely use. Lighthouse is one such open-source, automated tool for improving the quality and performance of web applications. It runs audits for performance, accessibility, progressive web apps, SEO, and much more. You can run Lighthouse using Chrome DevTools, from the command line, or as a standalone Node.js module.&nbsp;</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1000px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1000px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63a2dff7fa137f83533f9fb6_pLFRebIjvhEB75TmRIGKaWbrgmKXcQT6lPIThyPdLIR1hPjP9z7TBD4BEZqmXhZxGuUVz2Fbira4VAvq8ltXnBqDHccSHBfJvz1yIFz9WS6xxyLt2DlBhwFP8r5PgG9CI_mQkQTSpDttx7OnUAPFpmKOnzqQlIrceXpliBo_oAzSTG-F-jDMefW9PCWw.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><p id="""">With Lighthouse, you can test static HTML pages against dozens of performance and modern development best practices. It produces a report that includes a score for each category, actionable advice, and a list of resources for further reading. This is helpful for those who want to dive deeper into making their landing pages top-class in terms of performance.&nbsp;</p><p id="""">On the other hand, Selenium is a tool that developers can use to automate web browsers. You can use it to simulate user actions on your web application, such as filling out a form or clicking a button. Selenium can also automatically take screenshots of webpages or record a user's browsing session.&nbsp;</p><p id="""">With Selenium, you can also test dynamic web applications whose state changes based on user actions or depending on the user's life cycle to ensure that the web application is working as expected.&nbsp;</p><h3 id="""">What Are QA and UAT?</h3><p id="""">People often confuse QA environments with user acceptance testing (UAT) environments since they both involve testing. But they have different objectives.&nbsp;</p><p id=""""><a href=""https://release.com/blog/user-acceptance-testing-best-practices"">User acceptance testing</a> is a process of designing a test that helps make sure the functionality of a software application is supported by the users. In contrast, QA testing is a process of validating software application functionality against specific requirements (to ensure the accuracy and completeness of the software). Software engineers often use it to validate a software application's functionality mathematically.&nbsp;</p><p id="""">Simply put, the difference is that QA aims for error-free software, whereas UAT ensures that users get the product they want. Quality assurance teams work hard to make the user acceptance testing process as smooth and customer-friendly as possible.&nbsp;</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1000px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1000px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63a2dff8022b488b481c221d_cRgHfI8JUG9RUeUIR6IEht7z2hLVFTz3jGrXZILHYFU41h6DO_jFFjo_zkmRWLxb-5fA5vS79CKuuLh-qYPEDxu8vCEA7o9dxClMOzV1EPDoNUzed9SggFhaKM9m-VUAYKc0nGY_x3npkVLkU5VA0rYWHld9-eh7vU1YDQjMMQEwAwcBPvrpYbc_Y6r7.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><h3 id="""">Summary</h3><p id="""">QA environments are essential in making sure your application runs smoothly. They are also helpful for testing new features or bug fixes before deployment. &nbsp;Testing typically involves:&nbsp;</p><ul id=""""><li id="""">Reviewing code for errors</li><li id="""">Checking functionality</li><li id="""">Running tests against sample data</li><li id="""">Conducting usability studies</li></ul><p id="""">Quality assurance takes place throughout the entire life cycle of the project—from planning and research to design, coding, testing, deployment, support, maintenance, and even after retirement. Therefore, you need to manage your QA environment efficiently to get the maximum return on investment. This means managing your time well, having a good plan, and keeping track of tasks. It's important to remember that every piece of software has bugs. But you want to ensure these bugs don't impact your application's performance or functioning.&nbsp;</p><p id="""">We hope you liked the post. If you want to learn more about automated software environments, check out our <a href=""https://release.com/ebook/the-complete-guide-to-automated-software-environments"">ultimate guide</a>. &nbsp;</p><p id="""">The bottom line is that a QA environment is an integral part of the software development process, and it's essential for ensuring the quality of your application. By following the tips in this article, you can create a QA environment that is efficient and effective.&nbsp;</p>",false,,,,https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63e3f6cd8c2e282c7fe07a78_011023%20(1).jpg,,regis-wilson,5,Tue Jan 10 2023 17:00:00 GMT+0000 (Coordinated Universal Time),,
What Is an Internal Developer Platform and Why Should I Have One?,what-is-an-internal-developer-platform-and-why-should-i-have-one,62aa5a70cd5ba27d9d0d718a,64cac16d94eacf0153a1e67d,Wed Aug 02 2023 20:49:49 GMT+0000 (Coordinated Universal Time),Thu Jul 25 2024 15:06:16 GMT+0000 (Coordinated Universal Time),Thu Jul 25 2024 16:27:39 GMT+0000 (Coordinated Universal Time),"IDPs automate self-service for developers, streamlining software practices, infrastructure, environments, and operations","<p id="""">It seems like everyone is talking about internal developer platforms (IDPs). But what is an IDP, really, and how can it help your team?</p><p id="""">Let’s explore what an IDP is from a few angles. We’ll look at the definition, how the need for the IDPs developed, and the goals and outcomes of IDPs. With this information, you can assess whether IDPs can help your teams and organization and briefly look at next steps.</p><p id="""">Now let’s get started with a definition.</p><h3 id="""">A Simple Definition</h3><p id="""">An internal developer platform provides automated self-service solutions for developers to simplify and standardize software practices, infrastructure, environments, and operations.</p><p id="""">That’s it.</p><p id="""">Does it not seem like it’s specific enough?</p><p id="""">Well, that open-ended definition can be interpreted and built in many different ways into many forms. Is that OK? Yes, absolutely. That’s deliberate.</p><p id="""">Let’s note what this definition does not specify.</p><p id="""">First, perhaps you noticed that the definition does not specify any particular tool. An IDP consists of multiple tools and technologies that are integrated together. These tools vary from company to company and stack to stack. It really depends on the use case and should be built in accordance with the needs of the particular organization that wants the IDP.</p><p id="""">Additionally, this definition doesn’t mention <a href=""https://release.com/blog/11-continuous-deployment-tools-and-how-to-choose-one"" id="""">continuous integration and continuous deployment (CI/CD) tools</a>, microservice catalogs, or environment provisioning. Though those capabilities are often provided through the IDP, they are not an IDP on their own.</p><p id="""">And finally, the definition doesn’t mention a single pane of glass. In recent years, there’s been a lot of focus on bringing all the tools together in one place. Unfortunately, a central location isn’t enough. The IDP should be integrated throughout the development workflow in various ways based on need. There may be common UIs in an IDP, but there could also instead be CLIs, APIs, and other integration points between tools. It’s more about understanding the development process and adding the right automation in the right place at the right time.</p><p id="""">Now that we’ve covered the definition, let's talk about how the need for IDPs developed.</p><h3 id="""">How the Need for IDPs Developed</h3><p id="""">In the distant past, many developers handled both infrastructure and software. The environments were simpler and didn’t require a lot of specialization.</p><p id="""">Of course, the landscape changed. It was less about one C++ app running on a server somewhere and more about a collage of technologies working together. Tasks like deploying and operating the infrastructure moved to specialized operations or release management teams.</p><p id="""">Unfortunately, and typically due to a lack of automation and increased bureaucracy, the development teams felt held back, unable to get their changes into <a href=""https://release.com/blog/what-is-a-production-environment-and-why-is-it-unique"" id="""">the production environment</a> quickly enough. And operations folks guarded their environment against changes in production to reduce incidents and outages. This took many forms, some more automated than others. As an example of less automation and more bureaucracy, some organizations required multiple requests, forms, or even Excel docs that specified what changes and what files were supposed to move to what environment. Even when there was more automation, the tools tended to be focused on the needs of administrators and not developers. There was a lot of friction and pain, and developers were not able to get their changes to production quickly.</p><p id="""">From that pain, DevOps was born. In order to liberate developers and allow them to deploy more frequently, they were also given the keys to the CI/CD pipeline and entrusted to handle infrastructure and operations for their applications as well. This made sense in theory, but in practice, various interpretations of DevOps resulted in every single team in an organization implementing their own CI/CD pipeline, every team applying the same patches to their pipeline, and learning all the same lessons of proper CI/CD and operations as each other. What started as an exception became the norm. And once again, development teams across organizations spent countless hours on work that had very little to do with building new products and features.</p><p id="""">There has to be a better way. How can we take the learnings from DevOps, but also make the development life cycle simpler to manage from the development team’s perspective? That’s where IDPs can help. </p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1000px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1000px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/64caae88ae3e041d13afbddf_QMUMSB1bshyoPa97KKcjW2WnQJiPptkYwa1IUh2NIDr8lBjjFtm4Pjg089jdfE3wUkitt21l35Zey4Pxv6nYM3J0zYDzmCGPZqRM7rkHwjTar0NgRkxOAQOUw89jT11yi-B-9DkBhLwjDSurA0GTTJY.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><h3 id="""">Goals/Outcome of an IDP</h3><p id="""">When we begin to look at what an IDP can do for an organization, we typically try to meet one or more of the following goals:</p><ol id=""""><li id="""">Reduce toil</li><li id="""">Reduce context switching</li><li id="""">Allow for technical specialization</li><li id="""">Drive standardization</li><li id="""">Increase efficiency</li><li id="""">Improve developer experience</li><li id="""">Improve time to market</li></ol><p id="""">These goals all tie together well and overlap frequently.</p><p id="""">First, <strong id="""">reducing toil</strong> involves reducing repetitive work that can be automated away. This could involve often-used tactics like automating CI/CD and tests. Reducing toil can also include automating dependency upgrades, audits, and access requests.</p><p id="""">Second, an IDP should reduce <strong id="""">context switching</strong>. In part, when we reduce toil, we automatically reduce some amount of context switching. Additionally, with the appropriate automations and integrations, developers should not have to jump context between application development, environment management, infrastructure concerns, and whatever else is necessary.</p><p id="""">A proper IDP will automate and standardize many of the tools and technologies required by the development team to get their features out to production, which then also allows them to <strong id="""">specialize</strong> in their product domain and in software engineering skills.</p><p id="""">Additionally, IDPs can drive <strong id="""">standardization</strong> by providing out-of-the-box integration with static code analysis and enforced coding standards. And the teams themselves do not configure this each time for their new service or product. Instead, it’s baked in from the start and just works.</p><p id="""">When you <strong id="""">increase efficiency</strong>, you’re making it easier for the team to focus on the job they do best—write software for their domain. They spend less time on work that doesn’t involve their craft or domain, making them more efficient overall. This, as well as previous points, also improves the <strong id="""">developer experience.</strong> The development team has more time to focus on their craft and domain and worries less about the peripheral needs of their software stack.</p><p id="""">And finally, all of these combined result in improving <strong id="""">time to market</strong>. When you reduce toil, reduce context switching, and increase efficiency, your development team can focus on building the product for your customer and not spend time on efforts that dilute their focus. They are able to prioritize the most important work with increased specialization for your target audience.</p><h3 id="""">Common Problems IDPs Can Solve</h3><p id="""">We’ve covered the definition, history, and goals of IDPs, but sometimes it’s still difficult to see how this applies to your organization. You need more to understand how an IDP can help you.</p><p id="""">Let’s look at common problems that IDPs can solve.</p><p id="""">First, let’s consider the attention sprawl and security risks example mentioned earlier. If teams have to manage their own infrastructure and CI/CD pipeline, they’re spending time on tasks that do not deliver working software to customers. More so, they are spending time doing tasks that they’re not always knowledgeable in. This can result in security vulnerabilities, like leaving certain ports and directories open or not utilizing proper deployment strategies and configuration management.</p><p id="""">Adding on to the security aspect, when teams manage their own infrastructure and CI/CD, they each must learn how to integrate your new security scanning software into their pipeline. At best, this will lead to repetitive work for each team that should have been centralized. At worst, teams may not configure the scans correctly, leaving you more vulnerable to security exploits.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1000px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1000px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/64caae884861c30eca11f290_SqrPeg2Yoc2LPaREciwCXyYNWNdOtd6cb2sD0y3TgKZ4YHTVAANdrR5qlNpt9yz_bGnPxjQrTiFaqSNuNAYVMc4oxBoOzBTUclV_Elr-aqfcfopxABKag9AP5UnfRK_Sm1qgvksrOrjGr1NbXNu6o3Y.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><p id="""">As another example, perhaps your teams struggle with environment setup. This can manifest as inconsistent environment setup or, more commonly, as a limitation in the number of environments available. Many teams are tied to a <a href=""https://release.com/blog/testing-environment-types-and-what-theyre-used-for"" id="""">small number of environments</a>—staging, QA, UAT, and production, for example. It’s been historically difficult to replicate <a href=""https://release.com/blog/a-simple-guide-to-software-environments"" id="""">everything that goes into an environment</a>, and so organizations often stick to these static environments. This can cause <a href=""https://release.com/blog/increase-developer-velocity-by-removing-environment-bottlenecks"" id="""">bottlenecks</a> for testing and deployments with multiple devs working in the same environment or waiting on others to complete their testing. With static environments like this, you may also end up with a need to do large data refreshes or resets periodically to clean up odd testing scenarios. With proper IDP integration, we could utilize tools like Release to <a href=""https://release.com/blog/improving-developer-productivity-with-ephemeral-environments"" id="""">provide ephemeral environments instead</a>.</p><p id="""">For our final example, let’s talk about waiting on tickets. Perhaps your organization utilizes ticket queues and request systems. If your teams are often waiting on requests for infrastructure changes, blocking their ability to build or ship code, then you’ve found an opportunity for automation in your IDP.</p><p id="""">This is a short and general list. Wherever there’s friction in the development process, there’s opportunity to improve the process through an IDP.</p><h3 id="""">Next Steps</h3><p id="""">At this point, you may think that an IDP will easily solve everyone’s problems. However, that’s not always the case. You need to carefully design your IDP to fit your specific needs, so that it can reduce toil, reduce context switching, improve the developer experience, and improve time to market. And if you don’t focus on the right aspects of your IDP, you could waste time and make the problems you’re trying to solve even worse. </p><p id="""">The investment in an IDP will require investigation, prioritization, and a product-driven perspective. Your IT will require a dedicated team, support, and governance. To begin, take a look at <strong id="""">how</strong> your development teams work. What tools do they use? How often do they spend on tasks outside of their domain or primary technologies? Once you’ve assessed your starting point, you can begin to build the components of an IDP that will benefit your particular organization the most. </p><p id="""">Stay tuned to learn more about the components of a successful IDP in the next chapter on this IDP series. &nbsp;</p><p id="""">‍<em id="""">This post was written by Sylvia Fronczak. </em><a href=""https://sylviafronczak.com/"" id=""""><em id="""">Sylvia</em></a><em id=""""> is a software developer that has worked in various industries with various software methodologies. She’s currently focused on design practices that the whole team can own, understand, and evolve over time.</em></p>",false,,,,https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/64cac0a3f230c461efc2cb22_IDP_1.jpg,,sylvia-fronczak,10,Wed Aug 02 2023 22:00:00 GMT+0000 (Coordinated Universal Time),platform-engineering,
What is GitOps and how to get started with GitOps,what-is-gitops,62aa5a70cd5ba27d9d0d718a,62aa5a70cd5ba2288c0d72f9,Wed Jan 26 2022 02:47:33 GMT+0000 (Coordinated Universal Time),Fri Apr 21 2023 19:27:58 GMT+0000 (Coordinated Universal Time),Fri Apr 21 2023 19:37:52 GMT+0000 (Coordinated Universal Time),We recently joined a panel on Gitops. This blog will summarize the key nuggets and share tips on where to start,"<p id=""""><strong id="""">We recently joined Kong, Stackhawk, Fairwinds and Devops.com to discuss GitOps. You can click </strong><a href=""https://www.youtube.com/watch?v=m1VN-RjTHHo&feature=youtu.be""><strong id="""">here</strong></a><strong id=""""> to watch the panel discussion. If you prefer reading, we summarized some of the nuggets for you.</strong><br></p><p id=""""><strong id="""">Does GitOps replace DevOps or is it just an opinionated instance using more declarative tools?</strong><br></p><p id="""">In short,&nbsp;</p><ol id=""""><li id="""">GitOps is a theory, a way to operate: the philosophy that DevOps people use in response to the challenges of operating distributed applications.</li><li id="""">GitOps = XaC (X=infrastructure, configuration or any dependencies) + DevOps<br></li></ol><p id="""">More in length, GitOps is a tool in the DevOps toolbox. If you want to stretch it a bit, you could say that GitOps is a philosophical bent of DevOps practitioners. The mantra for several years has been “X as code” where “X” could be infrastructure, configuration, policy, and so forth. The meaning of GitOps is usually understood as “check in X and Y happens”.<br></p><p id="""">Unfortunately, as is usually the case with such broad strokes, there are a few places where this falls down. For example, the most oft-cited example of Infrastructure as Code is usually declarative configuration files that are not code. These “Infrastructure as Code” imposters (to use a negative term) are actually static text files that are copy-pasted and littered about the repository as if they were code. Often, these configuration files are whole directories and subtrees full of hardcoded boilerplate that need to be copy-paste-edited.<br></p><p id="""">Even worse, these configuration files often get stale and freeze-dried into branches and can’t be merged due to overlapping changes or schema adaptations that lag behind current versions or upstream fixes.<br></p><p id="""">The actual practice of GitOps should not be reduced to an overly simple saying like, “X as code,” but rather should be restated as “X in version control with good templating and deployment strategies”. What is the difference? Well, consider the usual case of configuration files sitting in a repository with names like “env.production”, “env.staging”, “env.qa”, and so on. Is this a good GitOps practice?<br></p><p id="""">What happens when we need a new environment like “env.qa2”, or even worse, “env.alice_dev”, “env.bob_dev”, “env.caden_dev”, etc.? Instead, you would prefer there be one, maybe two configurations with names like “production” and “testing” where “production” is instantiated many times as staging, qa1, qa2, and so on; and “testing” is instantiated N number of times by anyone and any environment that needs to be created. The “diff” between production and testing configurations should approach zero as time advances to infinity.<br></p><p id="""">This is why GitOps should not be the only philosophy or tool in the DevOps practitioner’s toolbox. It should be balanced as well by the <a href=""https://12factor.net/"" id="""">twelve factors</a> of good application development, including “one codebase, many deploys,” “store config in the environment,” and “keep dev, stage, and prod as similar as possible”.</p><p id=""""><strong id="""">Is GitOps it just about managing infrastructure as code or is that just one pain point?</strong><br></p><p id="""">GitOps is not just about infrastructure. It’s about all dependencies, internal and external. However, infrastructure is the main pain point that people address first.<br></p><ul id=""""><li id="""">More of a means to an end. IaaC gives you the option to automate your infrastructure. DevOps teams can then do whatever automation they need.&nbsp;</li><li id="""">Leverage the GitFlow to apply to infrastructure. Rigor of dev process to your infrastructure methodology.</li><li id="""">IaaC is the biggest and most. Don’t have data as code, security policy as code. As infra gets checked in, you’ll end up with needs outside of infra.</li></ul><p id="""">‍<strong id="""">Is GitOps a response to continuous delivery obstacles?</strong><br></p><p id="""">GitOps is an extension of CD. Automating your release process requires many ingredients in place. One such example is automated testing. The same way continuous testing is an extension of CI, GitOps an extension of CD. Automating version control, code reviews, testing and many other aspects of CI/CD relay on the premise that you have the right environments available at the right time in the release cycle.<br></p><ul id=""""><li id="""">It is a response in part, and a necessary part. Because infra is needed, when you make a code change that needs an S3 bucket, you can define it in IaaC alongside your code. Making sure all of that works adds another layer of complexity.</li><li id="""">It is part of the process to get to CD. Let’s get our config into Git.&nbsp;</li><li id="""">You don’t have to do GitOps to get to CD.&nbsp;</li><li id="""">It’s a philosophy… Other tools can be used: Ansible, Puppet, Chef.</li><li id="""">Build additional tooling is really required (Release, ah hem)</li></ul><p id="""">It would almost be impossible to accomplish continuous delivery without something like GitOps.<br></p><p id="""">By the way, CD is probably the most challenging part of GitOps and we’re still figuring it out. Testing code we’ve been doing for years. Testing infrastructure is more challenging.</p><p id=""""><strong id="""">How to get started with GitOps? Where do you learn and find the right communities?</strong><br></p><p id="""">The two most common tools are <a href=""https://argoproj.github.io/cd/"" id="""">Argo</a> and Flask. Each supports different applications. If you want to get your hands dirty, try them out and see which one better fits your needs. If you don’t have a Kubernetes cluster, <a href=""https://release.com"" id="""">Release</a> is the place to go to get started.<br>‍<br>For the full panel discussion click <a href=""https://www.youtube.com/watch?v=m1VN-RjTHHo&feature=youtu.be"">here</a>.</p><h3 id="""">Additional Resources</h3><ul id=""""><li id=""""><a href=""https://release.com/blog/increase-developer-velocity-by-removing-environment-bottlenecks"" id="""">Increase Developer Velocity by Removing Environment Bottlenecks</a></li><li id=""""><a href=""https://release.com/ephemeral-environments"" id="""">What is an Ephemeral Environment?</a></li><li id=""""><a href=""https://docs.releasehub.com/reference-guide/gitops"" id="""">Using Release GitOps</a></li></ul>",false,,,,https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63e3fad767257e72e4a8e8c5_012522%20(1).jpg,GitOps Panel,regis-wilson,7,Wed Jan 26 2022 02:57:00 GMT+0000 (Coordinated Universal Time),,
Why I Joined Release,why-i-joined-release,62aa5a70cd5ba27d9d0d718a,62aa5a70cd5ba2698d0d72ff,Tue Jan 04 2022 20:52:58 GMT+0000 (Coordinated Universal Time),Mon Apr 03 2023 22:01:45 GMT+0000 (Coordinated Universal Time),Mon Apr 03 2023 22:06:20 GMT+0000 (Coordinated Universal Time),Why the mission of solving development environment availability & private application deployment led me to join Release,"<p id="""">I’m excited to start 2022 by sharing that I've joined the sales team at <a href=""https://releasehub.com/"" id="""">Release</a>. We provide Environments-as-a-Service (EaaS). High performing organizations require development environments that are a close replica of their production environment. It’s the secret sauce to high quality frequent releases. Readily available <a href=""https://releasehub.com/ephemeral-environments"" id="""">ephemeral environments</a> enable developers to remove bottlenecks and internal friction, eliminate unnecessary complexity, and improve velocity. It’s a way for them to focus on ideas, code, and collaboration by eliminating the operational and political burden of environment management. <br></p><p id="""">But Release is more than just ephemeral environments. Permanent environments play a key role in testing and continuous deployment, especially with the emergence of multi-cloud. &nbsp;They’re also critical in the deployment of private applications inside your customer cloud, be it GCP, AWS, Azure, Govcloud, on-prem, etc. &nbsp; &nbsp;<br></p><p id="""">I found out about the company through Y Combinator’s <a href=""http://workatastartup.com/"" id=""""><em id="""">Work at a Startup</em></a> and struck up a conversation with CEO Tommy McClung. He began sharing the team’s <a href=""https://releasehub.com/blog/the-release-mission"" id="""">mission</a>; to enable anyone to bring their best ideas to the world more effectively. He was steadfast in his belief that too many ideas never see the light of day due to resource constraints, lack of seniority, and the hassle of creating and managing the necessary environments. In previous roles I’d see just how challenging infrastructure and DevOps management can be, especially as companies scale and their architecture gets complex. It became clear to me this is a massive universal problem and Release has the team and tech to achieve its mission and make customers wildly successful. <br>If you’re a developer and would like to learn more please reach out. Release is backed by investors like Y Combinator, Sequoia, and CRV and <a href=""https://releasehub.com/blog/releasehub-20-million-series-a-led-by-crv"" id="""">raised a $20M Series A</a> in Oct 2021. PS <a href=""https://releasehub.com/company#hire"" id="""">we’re hiring!</a></p>",false,,,,https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63e3f6365b968fc8095dae34_010422%20(1).jpg,Jon-Burns-Release,jon-burns,3,Wed Jan 05 2022 00:54:00 GMT+0000 (Coordinated Universal Time),,
Why I Joined Release: Bryce Fehmel,why-i-joined-release-bryce-fehmel,62aa5a70cd5ba27d9d0d718a,6307d26b91bb8b1c89c825b4,Thu Aug 25 2022 19:50:03 GMT+0000 (Coordinated Universal Time),Mon Apr 03 2023 19:12:48 GMT+0000 (Coordinated Universal Time),Mon Apr 03 2023 19:16:03 GMT+0000 (Coordinated Universal Time),Bryce Fehmel shares why he is excited to join the team at Release,"<p id="""">My name is Bryce Fehmel, a Sales Development Representative with Release. Contrary to many of my colleagues, I have spent the entirety of my career playing professional baseball. I grew up dreaming of playing Major League Baseball, but realized how unlikely it would be after multiple injuries. I began to take an interest in the analytics side of baseball, and how those analytics have an impact on each game and years to come for the organization. </p><p id="""">After my collegiate career and being drafted in the 21st round in 2019, I began my professional career in Arizona where I had enough success to be promoted to what was known as short season (Single A), in Salem, Oregon. During Spring Training of 2020, I found out I had torn a ligament in my elbow which would require Tommy John surgery, a procedure that would take a year to get back on the mound. Throughout the rehab process, I recognized that the ligament in my elbow still had not healed. After months of rehab with physical therapists, strength coaches and many more appointments with specialty doctors, I was told that the same ligament had torn once again. I quickly realized the chances of my dreams of playing in the Major Leagues had become significantly more slim than when I was first drafted. </p><p id="""">After recovery from the second surgery, I considered my possible options due to the amount of time I spent away from playing and the hours of rehab that would continue to be required.. There were long and difficult conversations about my potential future in baseball and what could be next. After attending Spring Training in 2022 and finally pitching in a game for the first time in 2+ years, it was clear that my injury would cause a third consecutive missed season. In that moment, I decided to retire gracefully, with my head held high leaving the professional baseball world with nothing but love and excitement for anyone else who has the opportunity to chase their dreams. </p><p id="""">That is where Release and CEO of the company, Tommy McClung come into the picture. Getting to know Tommy, a former Oregon State alumni himself, and his children over the years through their baseball experience, I gained an immense amount of curiosity around the tech industry and different positions where I could bring my expertise. As I step into a new journey with Release, I am eager to learn and grow in the software industry to make an impact in a variety of different ways. I look forward to the opportunity of helping others reach their goals and to bring their ideas to life faster and more freely. Most importantly, I am excited to work with my teammates to carry out the company mission of allowing the best ideas to emerge through efficiency and collaboration. </p><p id="""">Release is a complex software, in its own way, similar to the game of baseball. To be able to use Release and fully automate development more efficiently with code testing and ephemeral environments, there are many pieces that tie together. Similarly, baseball takes a group of people to come together, (even further than the players on the field) to create a winning team that remains memorable for years to come. As winning teams are memorable, so are features that end users benefit from to drive revenue and company growth. With Release, it is the easiest way to develop, manage and deploy full-stack complex environments on-demand.</p>",false,,,,https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63e41c5f8552d1fb2561634d_082922%20(1).jpg,,bryce,2,Mon Aug 29 2022 18:00:00 GMT+0000 (Coordinated Universal Time),,
Why I Joined Release: Matt Carter,why-i-joined-release-matt-carter,62aa5a70cd5ba27d9d0d718a,6355556e4d0d4158bba3098d,Sun Oct 23 2022 14:53:34 GMT+0000 (Coordinated Universal Time),Fri Apr 21 2023 16:48:34 GMT+0000 (Coordinated Universal Time),Fri Apr 21 2023 16:55:46 GMT+0000 (Coordinated Universal Time),Read Matt's story on why he joined Release as Chief Marketing Officer and learn about his experience.,"<p id="""">Earlier this month, I joined the team at Release to build out their marketing team as part of the company’s next chapter of growth. After spending time with the Release leadership team, Tommy, Erik, David, and Kelsey, I could see my values and vision were clearly aligned with a group who understood the needs of developers and app dev teams, saw the creation of the Environments as a Service (EaaS) category will help teams go faster with better results, and most importantly wanted to build a company where people could do their best work and thrive.</p><p id="""">Developer obsession</p><p id="""">For most of my career, I’ve been fortunate to build products and communities that help developers get their jobs done better and faster. Microsoft’s Visual Studio, Chef, and Docker all revolutionized how millions of developers could use technology as a force multiplier to not only innovate faster but get greater joy from their craft. The work Tommy, Erik, and David have done to date at Release comes directly from their experiences building startups and large company IT systems. Release came directly from their frustrations and aspirations–the chance to take a product with this vision in mind and help it grow is truly exciting, and matters in a meaningful way to devs. The Release developer experience creates the “flow” developers love that is the result of this deep understanding and real-life experience of the founders.</p><p id="""">Great developer experiences are just the beginning. As I looked more closely at what was possible through the Release vision, the power of EaaS became apparent. Apps are getting more complicated every day. Multiple dependencies, microservices, massive datasets, and different deployment options make rich testing more complicated. With EaaS, automated immediate environments are available to dev and ops teams throughout the software development lifecycle. This week’s announcement of Remote Development Environments let developers use the power of automated environments to get instant feedback from the earliest stages of coding. And as apps get ready to move to testing and review, Release automatically creates ephemeral or permanent environments with every code pull request, complete with dependencies, namespaces, and data sources in your cloud environment. This eliminates the need for teams to build bespoke solutions and for devs to wait in line for their turn to deploy. This gives every team the chance to release as quickly as desired. More features delivered faster equals better customer experiences and improved innovation velocity. </p><p id="""">Finally, the people at Release and the commitment to deliver tools to solve tangible problems for dev teams made this decision easy. From the founders and investors to each individual on the team, every individual is committed to shaping a company that delivers great tools and delightful experiences. I’ve been lucky to be part of teams building awesome companies in the past, and I see in Release a chance to help build another great organization. The opportunity, products, and people Tommy Erik, and David have brought together create the chance to make a big difference to app development teams. I’m excited to be along for the ride!</p>",false,,,,https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63e420aea2e78b2f0b997d24_102622%20(1).jpg,,matt-carter,2,Wed Oct 26 2022 17:00:00 GMT+0000 (Coordinated Universal Time),,
Why I Joined Release,why-i-joined-release-matt-riley,62aa5a70cd5ba27d9d0d718a,62aa5a70cd5ba2a3440d730f,Wed Mar 23 2022 14:24:34 GMT+0000 (Coordinated Universal Time),Fri Apr 21 2023 19:08:16 GMT+0000 (Coordinated Universal Time),Fri Apr 21 2023 19:37:52 GMT+0000 (Coordinated Universal Time),"Matt Riley joins the Release team and shares his enthusiasm about ""Environments-as-a-Service""","<p id="""">When learning about Release, I immediately recognized the magnitude of the problem EaaS solves. My 9-year professional career in technical program management, business development, and account management has been centered around helping customers answer ""how can we streamline processes"" and ""how can we create better experiences for customers."" Release’s EaaS solution answered both of these and ignited my passion to connect this tool with DevOps leaders to bring their ideas to the world faster.</p><p id="""">The goal of DevOps is to accelerate the process of application production. This involves increasing innovation, collaboration, improving quality, and being more responsive to business needs through rapid delivery and continuity between software developers and operations. Ultimately, this allows businesses to be more consistent with enhancing their competitive position and customer demand. Automation plays a critical role in accomplishing these goals by saving time and increasing velocity. Today, DevOps teams are automating infrastructure, code testing, and workflows. However, conventional environments for development, testing, staging, and production are difficult to manage. Engineering resources are distracted from the core product offering due to efforts required to create, maintain, and administrate environments. These efforts are undifferentiated, including configuring VPCs and networking, load balancing, creating Kubernetes manifests, and building end-to-end pipelines to turn your code into a running application in your environment. Valuable engineering resources should be focused on revenue-generating code, which Release is here to solve.</p><p id="""">Release is an out-of-the-box automation tool to spin up even the most complicated environments to enable software development uniformity and velocity. EaaS can be viewed as an automated template or package of all infrastructure as code and configuration settings to minimize prep work so software engineers can focus on their code. Outside of the common shared environment types (staging, QA, etc.), Release offers ephemeral environments. These are short-lived production replications where you can input a feature to receive instant results and share with stakeholders without disrupting other environments. No more waiting in line for a shared environment, either.</p><p id="""">Environments provide customers:&nbsp;</p><ul id=""""><li id=""""><strong id="""">Faster time-to-market: </strong>Improve product velocity with the ability to parallelize development.&nbsp;</li><li id=""""><strong id="""">Quality: </strong>Enables less rework by shifting testing closer to the developer, allowing bugs to be caught earlier before they hit production.</li><li id=""""><strong id="""">Innovation: </strong>Eliminates internal friction and bottlenecks to easily implement team members' ideas.</li><li id=""""><strong id="""">Collaboration: </strong>Enables real-time visualization and sharing of code changes.&nbsp;</li><li id=""""><strong id="""">Flexibility: </strong>Automatically generate ephemeral, development, testing, staging, and production environments on-demand. Spin them down just as quickly and only pay for what you use.&nbsp;</li><li id=""""><strong id="""">Scalability:</strong> Scale faster by using EaaS to automatically configure Infrastructure as Code, VPC, DNS, load balancing, integration pipelines, and more.&nbsp;</li></ul><p id="""">If you’re in Development or DevOps and would like to learn more, please reach out directly (<a href=""mailto:matthew@release.com"">matthew@release.com</a>).</p>",false,,,,https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63e3ff43eae14a427685d2f9_032322%20(1).jpg,,matt-riley,2,Wed Mar 23 2022 14:00:00 GMT+0000 (Coordinated Universal Time),,
Why I'm excited about environment-as-a-service and Release,why-im-excited-about-environment-as-a-service-and-release,62aa5a70cd5ba27d9d0d718a,62aa5a70cd5ba257ea0d7303,Thu Feb 03 2022 18:52:52 GMT+0000 (Coordinated Universal Time),Fri Apr 21 2023 20:11:23 GMT+0000 (Coordinated Universal Time),Fri Apr 21 2023 20:13:19 GMT+0000 (Coordinated Universal Time),Jimmy recently joined Release from AWS. This post summarizes why he is excited about environment-as-a-service.,"<p id="""">I recently concluded an incredible four-year journey at Amazon Web Services. I partnered with hundreds of customers across virtually every industry, from early seed startups to the 100+ year old enterprise companies, helping them leverage AWS to bring their ideas to reality. While differences in company size, funding, and industry brought their own unique requirements, it was their shared and seemingly universal challenges that really caught my attention:<br></p><ol id=""""><li id="""">Increase revenue with new features and faster time to market </li><li id="""">Reduce cost (whether it be operational, fixed, or variable) </li><li id="""">Develop a work environment that attracts the best technical talent</li><li id="""">Automate and simplify business processes to become agile and effective</li></ol><p id="""">After 2 years of working with some of the most innovative and disruptive startups, I decided it was time to embark on the startup journey myself. I was extremely thoughtful in my selection. I knew I wanted to get in early and have a hand in building out the sales organization while wearing many other hats. I also knew that I wanted to work for a company that helps customers solve all the above business challenges / goals. Outside of that, I considered the experience of founders / leadership, credibility of investors, total addressable market, unique or disruptive offering, and the company’s culture. <a href=""https://releasehub.com/"" id="""">Release</a> checked every single box for me. <br></p><p id=""""><a href=""https://releasehub.com/"" id="""">Release </a>is a YC, Sequoia and CRV-backed environment-as-a-service startup. It creates, runs, and manages infrastructure environments of any complexity on demand. Release <a href=""https://releasehub.com/blog/releasehub-20-million-series-a-led-by-crv"" id="""">raised a $20M Series A</a> in October 2021 and they are solving a unique challenge that every customer with cloud infrastructure faces in some capacity. </p><h4 id="""">Why Do Environments Matter?</h4><p id="""">Every member of product development is dependent on <a href=""https://releasehub.com/ephemeral-environments"" id="""">environments</a>. With applications becoming increasingly complex, creating and managing environments at scale becomes difficult. I saw this with customers at AWS. The cloud helped them with their infrastructure, but their IT teams needed something to operationalize their cloud resources to be effective and fast. I worked with legacy enterprise customers who tried to build an internal solution to meet this need but frequently hit walls in the process and eventually gave up. There wasn’t anything in the market to my knowledge that completely solved this. When developers (or their teams) have to manage the undifferentiated heavy lifting of building and maintaining an environment in order to test code, it results in slower time to market, lower software quality, and exorbitant operational costs. Alternative solutions are either ill-matched (CI/CD utilities, container tools, IDEs) or require building home-grown solutions that require dedicated employees, and might take years to build. </p><h4 id="""">What is Environment-as-a-service?</h4><p id="""">Release makes it easy to create ephemeral or permanent environments around any code commit for performance testing, <a href=""https://releasehub.com/staging-environments"" id="""">QA</a>, migrations, <a href=""https://releasehub.com/usecase/sales-demo-environments"" id="""">sales demos</a>, <a href=""https://releasehub.com/usecase/running-your-production-environments"" id="""">VPC deployments</a> and more. Customers can integrate with their source control system of choice and Release will create temporary or permanent branch-based environments with every code push or pull request. You can automate creation of complex environments, replicate copies of that environment in minutes and continually update environments whenever a change is made. We do all this in an EKS cluster built and managed by us which allows you to benefit from Kubernetes even if your team is unfamiliar with the orchestration system. </p><h4 id="""">How EaaS Impacts Release Cycles?</h4><p id="""">During the due diligence process I spoke to multiple Release customers, but one customer’s testimony told me all I needed to know about Release. <em id="""">“ It deployed the environment super quick and it literally just works. I don’t even know how to come up with a previous metric compared to what I just witnessed.”.</em> Release is giving customers faster time to market in an increasingly competitive environment, &nbsp;and cost reduction both from removing the undifferentiated heavy lifting and from making the best use of existing dev employees. Customers get best practice DevOps out of the box, up to date instant data sets for realistic experiences, isolated environments, and they are creating an exciting work environment where developers can do what they do best.. Make cool applications. </p><p id="""">If you are looking to join an incredible organization with experienced founders, top-tier investors, and a solution that is disrupting the cloud industry, please feel free to reach out to me directly (<a href=""mailto:jimmy@release.com"" id="""">jimmy@release.com</a>) or apply to any of the various open roles we have <a href=""https://releasehub.com/company#hire"" id="""">Here</a>.<br></p>",false,,,,https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/6442edc0462a978c0a896ec6_jimmy%20welcome%20(1).jpg,Jimmy Bried headshot,jimmy-bried,4,Mon Feb 07 2022 14:00:00 GMT+0000 (Coordinated Universal Time),,
Why Is Kubernetes So Hard - 4 Reasons Why And What to do About it,why-kubernetes-is-so-hard,62aa5a70cd5ba27d9d0d718a,62aa5a70cd5ba2444d0d72c4,Wed Mar 03 2021 03:03:48 GMT+0000 (Coordinated Universal Time),Fri Apr 21 2023 19:50:13 GMT+0000 (Coordinated Universal Time),Fri Apr 21 2023 20:04:43 GMT+0000 (Coordinated Universal Time),Kubernetes (k8s) has been all the rage for the last few years because application orchestration has become a de facto ,"<h3 id="""">Overview</h3><ul id=""""><li id=""""><a href=""#intro"" id="""">Introduction to Kubernetes</a></li><li id=""""><a href=""#k8sishard"" id="""">Kubernetes Infrastructure Is Hard</a></li><li id=""""><a href=""#yaml"" id="""">YAML is not a Markup Language</a></li><li id=""""><a href=""#copypaste"" id="""">You Can't Just Copy-Paste Code in Kubernetes</a></li><li id=""""><a href=""#debug"" id="""">Kubernetes Debugging</a></li><li id=""""><a href=""#solution"" id="""">Release Hub Solution</a></li></ul><p id=""""><a href=""#solution"">‍</a></p><div data-rt-embed-type='true'><a id=""intro""></a></div><h3 id="""">Introduction to Kubernetes</h3><p id="""">Kubernetes (k8s) has been all the rage for the last few years because application orchestration has become a de facto table-stakes requirement for production workloads running containers. “Containerising” applications is <em id="""">relatively</em> straightforward, and most DevOps engineers worth their salt can create a few Dockerfiles and build images in a pipeline that are ready to run. But where do you “run” your Docker containers? And which versions do you deploy? And how do all the containers talk to each other? This is where orchestration comes into play and where a few options are proposed by large vendors. There are two main options available at the time of this writing: Elastic Container Service (ECS) from Amazon Web Services (AWS) and Kubernetes which is offered by all the Infrastructure As A Service (IaaS) providers, including even AWS.</p><p id="""">The hope is that orchestration will allow companies to deliver their containerised applications to test and integration environments quickly and painlessly. The ideal scenario is that it “just works”, that is, you would snap your fingers and wait a few minutes before you see your application running in front of you. Ideally, you’d only need to specify the minimum information necessary to run your application: name, framework, dependencies, and so forth, preferably read out from existing configuration files you already have available. That’s the hope anyway.</p><p id="""">Obviously, from the title, we are focusing on Kubernetes, mainly because it is available everywhere and mainly because the only <em id="""">other</em> option is ECS which only proves the point of our thesis that Kubernetes is hard to use because AWS came up with their own solution that is <em id="""">supposed to be</em> easier. But why is it so hard to use? How long would it reasonably take to get your application(s) running in K8s?? And why isn’t it easier?</p><div data-rt-embed-type='true'><a id=""k8sishard""></a></div><h3 id="""">Kubernetes Infrastructure Is Hard</h3><p id="""">We always start with the infrastructure even though most companies wouldn’t build their own Kubernetes clusters themselves. If you were ever going to use a managed infrastructure service, K8s would be at the top of the list. I’m sure there are a few people who start up <a href=""https://minikube.sigs.k8s.io/docs/start/"" target=""_blank"" id="""">minikube</a> on their laptops and say to themselves, “Wow, this is easy! I can do this myself!!” This reminds me of people who start up an <a href=""https://www.elastic.co/guide/en/elastic-stack-get-started/current/get-started-elastic-stack.html"" target=""_blank"" id="""">Elasticsearch</a> container on their laptop and say, “Wow, we should implement this for our website!!” Fast forward to a production launch six months or a year later and the simple “We can do this ourselves” mantra turns into “I wish we didn’t have to do this anymore.”</p><p id="""">If you were truly going to build your own Kubernetes cluster, you’d need to build all the control plane servers and services on your bare-metal or Virtual Machines (VMs) from an IaaS of choice, and then tie them all together with some fancy networking configuration to separate control-plane traffic from container traffic. You’d need to configure and run all of the control plane software and get them all talking to each other, running stably and monitored properly. Perversely, you’d be orchestrating the containers that orchestrate the application, but without a lot of orchestration! The fancy mirage that’s presented when you run minikube or Docker Desktop on Windows hides all the inception of running a container orchestration system using containers.</p><p id="""">We haven’t even gotten to the complications of setting up ingresses (which are just nginx instances, usually) and load balancers that sit on top of or next to the control plane stack. A lot of the time, you’ll feel like you are creating a whole infrastructure just for your infrastructure to run (which isn’t unusual, but definitely doesn’t feel better than trying to orchestrate things yourself). We also haven’t gotten into the Role Based Authentication Controls and network policies that need to be set to support more than a single application or stack running in one cluster. The number of configuration points and server-side setups start to mount quickly and we haven’t even started orchestrating applications yet, which is the whole point of the orchestration system we’re supposed to be setting up.</p><p id="""">And let’s suppose that you really do wade out into this deep North Atlantic Ocean of huge waves and death-inducing freezing waters, and build yourself a production-worthy ship that can orchestrate your containers into an actual application. You look back up at your calendar and it’s been six months or a year since you started, and you’re just now deploying a control plane that says, “Hello World!” You think you’re successful and you’re about to celebrate when you check the releases section on the website and now you have a new version of k8s to deploy!!!</p><p id="""">I hear what you’re saying, “We’re large company and we have lots of DevOps engineers who are top decile of engineering talent in the whole world. We can handle all the heavy lifting. You’re just a whining, jealous baby.” I see you, <a href=""https://www.youtube.com/watch?v=HDm9iNkLyPI"" target=""_blank"" id="""">Datadog</a> and <a href=""https://www.youtube.com/watch?v=wqXVKneP0Hg"" target=""_blank"" id="""">Ticketmaster</a>. (By the way, your accusations of jealousy might be correct. At the end of my good friend Justin Dean’s keynote speech where he shows the slides with all the team members, my picture should have been up there — but I had left the team two years earlier.) For everyone else, we all just decide to not spend six months or a year trying to build our control plane and start up our IaaS provider’s managed service and cross our fingers and pray.</p><div data-rt-embed-type='true'><a id=""yaml""></a></div><h3 id="""">K8s YAML Ain’t Markup Language</h3><p id="""">If you’ve skipped ahead and just started up a managed k8s cluster, you’re still in for a long and tedious journey wading into a deep sea of confusing YAML. YAML is to text what James Joyce’s <a href=""https://www.finwake.com/1024chapter1/fw01.htm"" target=""_blank"" id=""""><em id="""">Finnegan’s Wake</em></a> is to English. If you close one eye, use only your left pinky and right thumb to follow some brail, put your feet into ballet’s fifth position, and then recite World War II codes under your breath, then you will easily see that YAML is quite a breeze to comprehend. Once you get the hang of it, it’s like riding a bicycle over a frozen lake on centimeter-thin ice with rabid wolves chasing you. It’s as easy as trying to crash the Ancient Aliens cocktail party held in Fort Knox on gold smuggling days.</p><p id="""">Look, it’s not actually that hard, right? Let’s say a guy walks up to you on the street. He’s a k8s expert and he’s going to show you how easy the “hello world” web service deployment is. The conversation goes like this:</p><div data-rt-embed-type='true'><pre>
<code class=""language- line-numbers"">
Him: “kind: Deployment”

You: “Oh, I see. Yes, I like it.”

Him: “apiVersion: apps/v1beta1.”

You: “Uh, okay. Isn’t v1beta1 out of date? You can use v1 as of k8s 1.9.
It’s actually removed in 1.16, but I wonder how many people have never
updated.”

Him: “Start over.”

You: “Wat.”

Him: “kind: Deployment”

You: “Stop with the Kinds everywhere!”

Him: “apiVersion: apps/v1”

You: “This again.”

Him: “spec:”

You: “Huh??”

Him: “selector:”

You: “No.”

Him: “matchLabels:”

You: “Wat.”

Him: “app: nginx”

You: “That’s nearly the first thing I’ve understood about this so far.”

Him: “spec:”

You: “Again?”

Him: “containers:”

You: “Okay, now we’re getting somewhere.”

Him: “image: nginx:1.14.2”

You: “Hmm.”

Him: “ports:”

You: “Aiiiiieeee.”

Him: “containerPort: 80”

You: “I’m going home. I quit. There must be a devops job I can get where
I work on [Gatsby blogs](https://www.gatsbyjs.com/ ""Gatsby nodejs frontend"")
all day.”
</code>
</pre></div><p id="""">And that’s just trying to read and understand the file. Try reading two k8s yaml examples and then generate one yourself from scratch. Even better, every day try a <a href=""http://codekata.com/"" target=""_blank"" id="""">code kata</a> practice of writing working and deployable configurations to Kubernetes.</p><p id="""">I DARE YOU.</p><div data-rt-embed-type='true'><a id=""copypaste""></a></div><h3 id="""">Copy Paste Ain’t Code</h3><p id="""">I’m still chuckling over the previous section. I have to chuckle because this is the daily pain of my day-to-day existence and facing that pain directly is like standing in front of a bus being driven by Keanu Reeves on the freeway. The only thing that keeps my nose going back to the grindstone is the realization that working with NodeJs would be worse. The problem is that the Kubernetes docs are pretty good. You copy-paste some hello world examples and the outputs look like they work. You start to get pretty good at using kubectl. You can see vague shapes and outlines in YAML. You’re starting to gain confidence that you might be able to do something useful.</p><p id="""">“Let’s try to move our application into Kubernetes!” you yell into the air as you emerge dripping wet from your bathtub wrapped only in a towel, like <a href=""https://www.scientificamerican.com/article/fact-or-fiction-archimede/"" target=""_blank"" id="""">Archimedes</a> sprinting through the streets of Syracuse. “We’ll just copy-paste some sections from here and here and put them there and there, and we’ll have our app running in no time,” you breathlessly explain to your coworkers. “Does it work?!” they excitedly ask. “Not yet. I mean, no. I need to indent the section and remove one piece that is not used in this spec. Then I need to decide if we use a deployment or a daemonset, but it’s almost there. I swear!”</p><p id="""">First of all, put on some clothes. I’m all for taking a bath while thinking about kubernetes YAML files, but you need to get dressed afterward. Also, if you drop your Macbook Air into the bath with you, the results can be electrifying. I know. Second, here’s a riddle for you: how many YAML files do you think you need to run and deploy your application? Good thing that some people have ten fingers and ten toes because that’s probably how many you’ll need. And they’re all related but not really. You can copy-paste sections around if you’re adventurous and gullible, but you have no idea if the sections are compatible. There are only four required fields, all of which are gibberish, and everything goes under spec: (including spec:). Most of the sections are duplicated but only slightly. They vary microscopically in ways that matter macroscopically.</p><p id="""">Copying and pasting is a wonderful art, and I’ve personally worked my entire adult career that way. I gleefully admit my whole output in life is like a <a href=""https://tvtropes.org/pmwiki/pmwiki.php/Main/CutAndPasteNote"" target=""_blank"" id="""">ransom note</a> cut from stack overflow and documentation examples. But piecing together this fragile web of text to do what really should be quite simple and obvious is tedious, error-prone, and too trial-and-error-y. It would be much better to express what you want and be able to actually emit workable, executable code that produces a result you want: namely your application running.</p><p id="""">All this complaining about YAML is quite amusing, but really it’s the symptom of the cause: Kubernetes is so difficult to use because the interface has to be completely rigid. K8s configurations are not living, majestic trees, they are a bunch of dead chopped wood. They are worse than chopped wood, they are whole petrified forests, vast piles of rocks with the imprint of thousands of years of growth rings imprinted on them and preserved for millions of years.</p><p id="""">No, they are worse than petrified wood forests! Kubernetes manifests are the punch cards of the twenty-first century. Each YAML is a collection of holes poked into chopped up wooden cards that we can’t read and understand, that we shove blindly into the kubectl apply -f command and hope that we put them in the correct order and didn’t make a single-hole mistake anywhere in the stack. Then, just like the machines of yesteryear, we try to gain insight into what’s happening by looking at the blinking lights and obscure output of ticker tape, hoping to glean insight.</p><p id="""">Just like trying to reproduce Mozart or Beethoven on a <a href=""https://en.wikipedia.org/wiki/Player_piano#Music_rolls"" target=""_blank"" id="""">pianola</a> is tedious, laborious, error prone, and ultimately unfulfilling, similarly k8s manifests are frozen forever in time, impossible to write expressively, and playing the same tune <em id="""">ad infinitum</em>. The reason people still use v1beta1 even though v1 has been available for <em id="""">two years</em> is because nobody has generated new k8s configurations since then.</p><div data-rt-embed-type='true'><a id=""debug""></a></div><h3 id="""">Doctor, Heal Thyself; or Debugging Yourself Is Hard</h3><p id="""">The great thing about k8s is that when something goes wrong, nobody knows. I can’t count the number of times I’ve deployed something, worked on something else for a few hours, came back and realised that the deployment had just silently failed and nothing ever notified me. The error message was available somewhere: was it in the deployment logs or the pod logs? Is the ingress or ingress deployment running? Where in the ten or dozens of Kinds files did the log entry appear? And the root cause was often some unrelated issue: an errant and invisible whitespace, not using double quotation marks when I should have, not using single quotation marks when I should have, or getting the brunt end of the indent from a copy-paste issue from three weeks ago.</p><p id="""">There are, of course, tools and techniques and monitoring tools that help out; it’s like Elon Musk’s <a href=""https://youtu.be/aBr2kKAHN6M?t=837"" target=""_blank"" id="""">Mars orbiter</a> MVP: “Does it work?” “Absolutely!! A thousand times, yes!” “What does it do?” “Almost anything you want!” You have to know what to look for and where to look for it, then you have to know how to figure out what to do about it, then you have to figure out where in the ten or dozens of files which line or lines to fix, and then you have to know how to fix it.</p><p id="""">The other great thing about k8s is that you own the whole thing. Listen: friendo, pal, buddy, you chose this existence. You copy pasted the “code”. The documentation examples work. I can run “Hello World!” on my laptop so it’s clearly all on you. You’re the one who ran through the office dripping wet in a towel shouting “Kubernetes!” If the Hippocratic oath is “Do no harm” then maybe the Devops oath is “Do no more harm than that which will get you fired.”</p><p id="""">And the last great thing about k8s is there are tons of people and companies who claim to know what is going on and what to do, and they’ll gladly take your money to show you whether that’s true or not. Type Kubernetes into the search engines and see all the ads that pop up. This article is part of the problem, and also the solution, so stay with me.</p><div data-rt-embed-type='true'><a id=""solution""></a></div><h3 id="""">The Solution, Finally</h3><p id="""">There are several ways to make Kubernetes easier to use:</p><ol id=""""><li id="""">Don’t use k8s: run, screaming for your lives</li><li id="""">Train all your people to figure it out (come back to me when you’re done; I still might be alive. Probably not.)</li><li id="""">Hire more people for your team to figure it out (I’m available, hit me up. Ha ha, just kidding.)</li><li id="""">Hire someone else to do it for you</li><li id="""">Wait longer for results, do more with less, eventually settle on something that isn’t horrible</li><li id="""">Find a solution that deploys your applications to environments for you and get on with your actual business of, well, whatever business it is you actually do. Automation tools and services can help you get your application running without investing in the activities described above. Someone has to do it, but it better not be you.</li></ol><p id="""">At <a href=""https://releasehub.com/"" target=""_blank"" id="""">Release</a> we work tirelessly to bring your application to life in an orchestrated, human interface. We write software to deal with all the complexity, difficulty, and strain so that no one else has to (unless they want to!) We create the engine that drives the Kubernetes vehicle, and we deliver solutions that our customers can use to get on with their business of doing business.</p><h3 id="""">Additional Resources</h3><ul id=""""><li id=""""><a href=""https://releasehub.com/blog/kubernetes-pods-advanced-concepts-explained"" id="""">Kubernetes Pods Advanced Concepts Explained</a></li><li id=""""><a href=""https://releasehub.com/blog/how-to-make-kubernetes-config-files-not-suck"" id="""">How Do You Make Kubernetes Config Files Not Suck?</a></li><li id=""""><a href=""https://releasehub.com/blog/kubernetes-how-to-debug-crashloopbackoff-in-a-container"" id="""">Kubernetes - How to Debug CrashLoopBackOff in a Container</a></li><li id=""""><a href=""https://releasehub.com/blog/cutting-build-time-in-half-docker-buildx-kubernetes"" id="""">Cutting Build Time In Half with Docker’s Buildx Kubernetes Driver</a></li><li id=""""><a href=""https://releasehub.com/blog/kubernetes-health-checks-2-ways-to-improve-stability"" id="""">Kubernetes Health Checks - 2 Ways to Improve Stability in Your Production Applications</a></li></ul>",false,,,,https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63e3fde2863124d72ae798c3_030121%20(1).jpg,Two wrestling athletes fighting representing why Kubernetes infrastructure is hard,regis-wilson,9,Tue Mar 02 2021 00:00:00 GMT+0000 (Coordinated Universal Time),,
"You don't need to know what you’re doing, you need an agile DevOps methodology",you-need-agile-devops-methodology,62aa5a70cd5ba27d9d0d718a,62aa5a70cd5ba2531e0d72c6,Tue Jan 26 2021 22:22:08 GMT+0000 (Coordinated Universal Time),Fri Apr 21 2023 19:58:04 GMT+0000 (Coordinated Universal Time),Fri Apr 21 2023 20:04:43 GMT+0000 (Coordinated Universal Time),"Do you ever feel like you have no idea what you’re doing? Like you’re just kind of going along with things, doing your b","<p id="""">Do you ever feel like you have no idea what you’re doing? Like you’re just kind of going along with things, doing your <em id="""">best</em> but not really sure if you’re doing it <em id="""">right</em>?</p><p id="""">Here’s a little secret I’ve learned over the years: <em id="""">Nobody knows what they’re doing.</em></p><p id="""">Denis, with his steadfast approach and surgeon-like precision? Hannah, with her pen and notebook, who writes down every detail to later review? Kendall, who is quick on the trigger to any bizarre question that upper management tosses her way and always leaves them with good laughs and big smiles?</p><p id="""">All of them? <em id="""">They have no idea what they’re doing.</em> They don’t <strong id="""">have</strong> to know what they’re doing. They’ve unlocked the biggest secret that formal education has desperately tried to unteach us: Failing is <em id="""">fine</em>! Failing is <strong id="""">good</strong>! Failing is <strong id=""""><em id="""">the fastest way to success</em></strong>!</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:650px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""650px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/603dd147c5b0a445431bd468_slimemold.gif"" loading=""lazy"" width=""auto"" height=""auto"" id=""""></div><figcaption id=""""><em id="""">Brainless, single-cell slime hunting for food</em></figcaption></figure><p id="""">In the world of software development, we’ve already accepted this as scientific fact. We embrace this and weave it into the foundation of our methodologies and systems. We know, with absolute certainty, that nothing is certain. The winner will be the little boat whose chart is scribbled on the back of a napkin and can pivot on a dime, not the monolithic Titanic who, despite the captain’s best efforts, is going to collide with that iceberg.</p><p id="""">However, even though we recognize the advantages and the <em id="""">need</em> to be <strong id="""">agile</strong> in this industry, that does not mean that we’ve mastered all the ways to optimize this. While different Software Development methodologies can have their place for different problem spaces (just as different programming languages are better suited for some problems over others), one particular approach to <strong id=""""><em id="""">failing fast</em></strong> has gained a lot of traction over the past decade. The <strong id="""">DevOps</strong> methodology was forged from the fires of Agile, and today DevOps has been crowned the champion of how to build great software, quickly and wicked fast. Modern technology companies that thrive to compete place DevOps on the forefront of their mind.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:365px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""365px""><div id=""""><img src=""https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/603dd147c5b0a429da1bd469_ComplexGargantuanFluke-size_restricted.gif"" loading=""lazy"" width=""auto"" height=""auto"" id=""""></div><figcaption id=""""><em id="""">Twining motion of vines trying to find something to climb</em></figcaption></figure><h3 id="""">What is DevOps?</h3><p id="""">If you’re not already familiar with DevOps, the term can be a little confusing. DevOps began as a cultural movement within companies. Rather than Developer teams taking their code and throwing it over the fence for the Operations team to deploy and monitor (while the Security team haughtily throws their arms in the air over any concern—be it major or minor—introduced by the other teams), DevOps works to tear down these artificial walls.</p><p id="""">The way this works in practice is through tight feedback loops and blurring the edges of responsibility. “DevOps” has turned from an idea into a career where you build a racetrack for product development. This enables the idea-to-deployment cycle to hasten. No longer do we need to take months to plan, build, test, release, deploy, evaluate. No longer do we need to make sure every release is perfect “because there’s no going back”. Now we can do what we do best: <em id="""">Make mistakes.</em></p><p id="""">You’ve heard of (or worked with) companies that deploy their code tens to thousands of times a day. This is incredibly powerful. Ideas always look a little different in practice and sometimes they turn out to be bad ideas. But sometimes those silly ideas that would have you laughed out of a boardroom turn out to be the ones worth their weight in gold. With the ability to experiment and quickly reset, we can fractal our way to the perfect solution for any problem.</p><h3 id="""">Why do I need Environments?</h3><p id="""">If your company is big enough, has the capital, and understands the need, you may be lucky enough to have a team of DevOps Engineers who work to help make sure everyone has the environments they need, and the tools to build and deploy code.You have your build and deploy pipelines. Your code only takes one push, merge, and a couple button clicks to make its way into production. You’re. Living. The. Life.</p><p id="""">Okay, sure, there are rough edges. The Developers and QA might be a little agitated that they have “bad data” in their databases. This bad data causes weird shadow bugs that wouldn’t exist “for a real user”. You have Product Managers and UX/UI Engineers digging around your QA and Staging Environments to make sure the feature matches the requirements, but they run into these shadow bugs and in a panic, hold a meeting to discuss “why the application is broken”.</p><p id=""""><em id="""">C’est la vie.</em> Your environments are starting to drift apart, but it’s okay. QA finds a bug, but they can’t be sure when it was introduced. The Developer isn’t sure either, there are a few features that others were working on that have all been merged into here. One major feature is ready, but the others are blocked. Hours tick into the evening as the team scrambles to fix the concerns, introducing more in their haste, until finally QA calls to cancel today’s deployment. Disappointed and mentally taxed, everyone finally goes home frustrated.</p><p id="""">This happens regularly. It’s a clog in the system, but it is manageable. However, what you don’t know yet is that your competitor has committed to allocating resources to addressing this problem directly. It took them twelve months (a bit off from their original six-month estimate), but now the tooling is built out and QA can create a build from any branch, on the fly. Now the data is fresh, every time, freeing QA to get their hands dirty in this sandbox. The Product team is excited to be able to look at new features side by side before they’re released, and features can even be put on hold or tinkered with in isolation.</p><p id="""">Meanwhile, your company is falling behind. The only way you’d be able to keep up is to spend the money <em id="""">and the time</em> to build this for yourselves. It’s a big investment, a big time commitment, and your company is worried. What if the project fails? There’s a lot at stake here.</p><p id="""">Environments are the key to rapid prototyping and quick feature releases, while maintaining a solid, battle-tested product. But environments are also expensive. The upfront and maintenance cost put them outside the scope for many companies and by the time a strong need rears its head, the architecture has evolved into a technical labyrinth.</p><p id="""">At Release, we understand this problem in depth. We have customers who use Release to give themselves a competitive advantage after they realized environments were holding them back. A new concept in DevOps has emerged called <a href=""https://releasehub.com/ephemeral-environments"" target=""_blank"" id="""">Ephemeral Environments</a> which eliminates the bottleneck of shared staging environments. An Ephemeral Environment is automatically created when a developer does a pull request and has just their changes on their branch. This environment spins up for UAT testing and when the branch is merged, it disappears. Developers never wait for access to environments as they appear as part of their development workflow.</p><p id="""">We’re using Ephemeral Environments which has put us on fast-track to shipping deliverables that we can stand behind. The unfortunate truth is that building this infrastructure is necessary to have a fighting chance against the heavyweights. But the problem is that the cost and time required can be astronomical. The <a href=""https://releasehub.com"" target=""_blank"" id="""">Release</a> platform specifically aims to solve this problem directly by providing Environments-as-a-Service. This way, you get all the advantages of environments, without the costs or headache of Doing It Yourself, freeing you up to focus on the business and the product needs.</p><h3 id="""">In short…</h3><p id="""">Remember: Nobody knows what they’re doing. And that’s okay. Nobody’s ever known what they’re doing; we’re all just stumbling around. But if we stumble with purpose, we can fall into something that works. If we fail fast, we get to success faster. Our methodologies in software development reflect this, but in practice, building supportive infrastructure is costly. If we can get to a place where we optimtimize failing fast by deploying early and often, we can more quickly find what our product needs to be. We can find success <em id="""">without ever having to know</em> what we’re doing.</p><h3 id="""">Ephemeral Environments</h3><p id="""">Curious about Ephemeral Environments and what they can do for you? Check out <a href=""https://release.com/ephemeral-environments"" target=""_blank"">this article</a> on what Ephemeral Environments are and what they can do for you.<br></p>",false,,,,https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/63e3f68ec632fd5efdce12ae_010521%20(1).jpg,White light on dark background,vicky-koblinski,5,Wed Jan 06 2021 00:00:00 GMT+0000 (Coordinated Universal Time),,
You’re Invited! Join the Release.ai Technical Preview Program,youre-invited-join-the-release-ai-technical-preview-program,62aa5a70cd5ba27d9d0d718a,65f228192514b586e2d938f2,Wed Mar 13 2024 22:26:33 GMT+0000 (Coordinated Universal Time),Thu Apr 25 2024 20:25:53 GMT+0000 (Coordinated Universal Time),,"We're excited to launch the Release.ai Technical Preview Program, aimed at simplifying infrastructure management.","<p id="""">Today we are excited to launch the Release.ai Technical Preview Program, aimed at simplifying infrastructure management for AI workloads. We’ve spoken with teams doing incredible work in AI, from building new foundational models to integrating custom trained models into products, and one thing is clear, the infrastructure is lagging behind. All across the industry we see amazing innovations pop up every week (as I’m writing this <a href=""https://www.bloomberg.com/news/articles/2024-03-12/cognition-ai-is-a-peter-thiel-backed-coding-assistant"" id="""">Cognition AI is blowing up the internet with their Devin assistant</a>), and underneath the surface to make all this happen are infrastructure teams toiling away at accumulating GPUs, managing finicky clusters, investigating the latest timeouts, and making their finance folks bug-eyed at the bills being generated.</p><p id=""""><a href=""http://release.com"" id="""">Release</a> is a leader in abstracting away infrastructure complexity for conventional SDLC processes, so that engineering teams can focus on shipping high quality products, faster. Now we are bringing that expertise to support AI engineering teams. To begin, we partnered with NVIDIA to simplify the setup and management of the NeMo framework infrastructure. Now you can use NeMo and Release to build and deploy models in a portable and cloud agnostic way on top of Kubernetes clusters we manage for you.</p><p id="""">While Release.ai simplifies AI infrastructure, you don’t have to choose between simplicity and control - everything on Release.ai is orchestrated in your cloud account on hardware that you own, simplifying toil while maintaining control. Currently we support training, fine-tuning, and inference workflows with more workflows and supported frameworks to come in the near future.&nbsp;</p><p id="""">We opened access to our beta program for teams who build, fine tune and train AI models. Come build a new AI Infrastructure platform with us!</p><p id=""""><a href=""https://release.ai/"" id="""">Sign up for Release.ai today!</a></p>",false,,,,https://uploads-ssl.webflow.com/603dd147c5b0a4221d1bd360/65f227b42b16b3bf24a30e71_pexels-google-deepmind-17485657.jpg,,michael-poon,3,Thu Mar 14 2024 16:00:00 GMT+0000 (Coordinated Universal Time),news,
