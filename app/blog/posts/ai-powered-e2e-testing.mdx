---
title: "AI-Powered E2E Testing: How Cursor and Release Make Quality at Speed Possible"
summary: >-
  As AI accelerates software development, robust end-to-end testing becomes crucial. 
  Learn how to leverage Cursor and Release to build, maintain, and run E2E tests in 
  ephemeral environments, ensuring quality keeps pace with AI-powered development velocity.
excerpt: >-
  Discover how AI tools like Cursor make E2E testing more accessible while Release's 
  ephemeral environments provide the perfect infrastructure for automated testing pipelines.
categories:
  - ai
  - testing
  - platform-engineering
imageAlt: >-
  A developer using AI to create and run E2E tests in ephemeral environments
publishDate: "2024-05-07T08:00:00.000Z"
author: jstotz
readingTime: 10
mainImage: /blog-images/ai-e2e-testing.png
showCTA: true
ctaCopy: Try Release to run your E2E tests in ephemeral environments and accelerate your testing workflow
ctaLink: >-
  https://release.com/signup?utm_source=blog&utm_medium=cta&utm_campaign=blog-cta&utm_content=ai-powered-e2e-testing
relatedPosts:
  - improve-developer-velocity-with-ephemeral-environments
  - introducing-release-ai
  - kubernetes-health-checks-2-ways-to-improve-stability
ogImage: /blog-images/ai-e2e-testing.png
tags:
  - ai
  - testing
  - platform-engineering
ctaButton: Try Release for Free
---

## AI Is Accelerating Development: Why E2E Testing Matters More Than Ever

Software development is in the midst of a revolution. AI-powered tools like Cursor, GitHub Copilot, and Claude are dramatically increasing developer productivity, enabling engineers to write code faster than ever before. At Release, we've seen firsthand how AI assistants can transform development workflows, reducing time spent on repetitive tasks and accelerating implementation of new features.

But this increased velocity comes with a challenge: how do we maintain quality when development is moving faster than ever?

As the pace of development accelerates, robust testing becomes not just important but essential. End-to-end (E2E) tests that verify your application works correctly from the user's perspective are particularly valuable because they validate the entire system, not just isolated components.

The good news? The same AI tools accelerating development can also help create and maintain E2E tests. In this post, we'll:

1. Explore how AI assistants like Cursor can help build E2E test suites
2. Walk through creating Puppeteer tests for a sample application
3. Show how to run these tests in ephemeral environments using Release
4. Demonstrate GitHub Actions integration for automated testing

Let's dive in.

## Why E2E Testing Matters in an AI-Accelerated World

Before we get into the technical details, let's consider why E2E testing is especially important in an environment where AI is accelerating development:

1. **Feature Velocity**: AI tools can help developers implement features much faster, but without proper testing, bugs can slip through.

2. **System Verification**: While AI can generate code that looks correct, only E2E tests can verify that all components work together as expected.

3. **Regression Prevention**: As codebases grow more complex, E2E tests provide confidence that new features don't break existing functionality.

4. **Documentation**: E2E tests serve as living documentation of how your application is supposed to work.

5. **User Experience Validation**: E2E tests simulate real user interactions, validating that the application works correctly from the user's perspective.

## Setting Up the Example Application

For this tutorial, we'll use [Release Example Voting App](https://github.com/awesome-release/release-example-voting-app), a microservice application with a React frontend, a Python API, and a Node.js worker. This app allows users to vote between two options and see the results in real time.

![Screenshot of the Voting App interface]
[IMAGE: Screenshot showing the voting app interface with two voting options]

The application architecture looks like this:

- **Frontend**: React application serving the voting interface and results page
- **API**: Python Flask application providing REST endpoints
- **Worker**: Node.js service processing votes
- **Database**: Redis for temporary storage and Postgres for persistent storage

## Creating E2E Tests with Cursor in Agent Mode

Now let's use Cursor in Agent mode to create an E2E test suite using Puppeteer. If you're not familiar with Cursor, it's an AI-powered code editor that can help you write, understand, and refactor code.

### Step 1: Set Up the Project

First, let's create a new directory for our tests and initialize a new project. In Cursor's Agent mode, we'd use a prompt like this:

> **Cursor Prompt:** Create a new directory for E2E tests with Puppeteer and Jest. Initialize a Node.js project and install the necessary dependencies.

![Cursor Agent creating a new project]
[IMAGE: Screenshot of Cursor Agent suggesting commands to create a test directory and initialize a project]

Based on this prompt, Cursor would suggest the following commands:

```bash
mkdir e2e-tests
cd e2e-tests
npm init -y
npm install puppeteer jest jest-puppeteer @types/puppeteer @types/jest
```

### Step 2: Configure Jest and Puppeteer

Next, we'll create a Jest configuration file to work with Puppeteer. Here's the prompt we'd use in Cursor:

> **Cursor Prompt:** Create a Jest configuration file that works with Puppeteer for end-to-end testing. Also create a jest-puppeteer configuration file that allows us to run tests against an external server.

![Cursor configuring Jest]
[IMAGE: Screenshot of Cursor Agent creating a Jest configuration file]

Cursor would then generate these configuration files:

Create a `jest.config.js` file:

```javascript
module.exports = {
  preset: 'jest-puppeteer',
  testMatch: ['**/*.test.js'],
  testTimeout: 30000,
};
```

And a `jest-puppeteer.config.js` file:

```javascript
module.exports = {
  launch: {
    headless: process.env.HEADLESS !== 'false',
    slowMo: process.env.SLOWMO ? parseInt(process.env.SLOWMO) : 0,
    args: ['--no-sandbox', '--disable-setuid-sandbox'],
  },
  server: {
    command: 'echo "Using external server"',
    port: 3000,
    launchTimeout: 10000,
    usedPortAction: 'ignore',
  },
};
```

### Step 3: Write the First E2E Test

Now, let's use Cursor to write our first E2E test. We'll start by testing the voting functionality with this prompt:

> **Cursor Prompt:** Write a Puppeteer test file called voting.test.js that tests the basic functionality of our voting app. The app has a main page with two voting options, a thank you page after voting, and a results page. Use environment variables for the app URL with a localhost fallback.

![Cursor creating the first test]
[IMAGE: Screenshot showing Cursor generating the first E2E test]

Create a file named `voting.test.js`:

```javascript
describe('Voting App', () => {
  beforeAll(async () => {
    // Set the target URL from environment variable or use a default
    const baseUrl = process.env.APP_URL || 'http://localhost:3000';
    await page.goto(baseUrl, { waitUntil: 'networkidle0' });
  });

  it('should load the voting page', async () => {
    // Verify the title is correct
    await expect(page.title()).resolves.toMatch('Voting App');
    
    // Verify voting options are displayed
    const options = await page.$$('.voting-option');
    expect(options.length).toBe(2);
  });

  it('should allow a user to vote', async () => {
    // Click the first voting option
    await page.click('.voting-option:nth-child(1)');
    
    // Wait for vote to be processed
    await page.waitForNavigation({ waitUntil: 'networkidle0' });
    
    // Verify we're on the thank you page
    const thankYouMessage = await page.$eval('.thank-you-message', el => el.textContent);
    expect(thankYouMessage).toContain('Thank you for voting!');
  });

  it('should show results page with data', async () => {
    // Navigate to results page
    await page.goto(`${process.env.APP_URL || 'http://localhost:3000'}/results`, { waitUntil: 'networkidle0' });
    
    // Verify results are displayed
    const resultsContainer = await page.$('.results-container');
    expect(resultsContainer).not.toBeNull();
    
    // Verify vote counts are displayed
    const voteCounts = await page.$$('.vote-count');
    expect(voteCounts.length).toBe(2);
  });
});
```

### Step 4: Add More Test Coverage

Let's extend our test suite to cover more functionality. Here's the prompt for Cursor:

> **Cursor Prompt:** Create a new test file called results.test.js that focuses on testing the results page. Include tests for real-time updates when votes are cast and verify the page works correctly on mobile devices.

![Cursor extending test coverage]
[IMAGE: Screenshot showing Cursor generating additional tests]

Create a file named `results.test.js`:

```javascript
describe('Results Page', () => {
  beforeAll(async () => {
    const baseUrl = process.env.APP_URL || 'http://localhost:3000';
    await page.goto(`${baseUrl}/results`, { waitUntil: 'networkidle0' });
  });

  it('should display the results in real-time', async () => {
    // Verify the results page title
    await expect(page.title()).resolves.toMatch('Voting Results');
    
    // Get initial vote counts
    const initialCounts = await page.$$eval('.vote-count', els => 
      els.map(el => parseInt(el.textContent.trim(), 10))
    );
    
    // Open a new page to cast a vote
    const newPage = await browser.newPage();
    await newPage.goto(process.env.APP_URL || 'http://localhost:3000', { waitUntil: 'networkidle0' });
    
    // Vote for the first option
    await newPage.click('.voting-option:nth-child(1)');
    await newPage.waitForNavigation({ waitUntil: 'networkidle0' });
    await newPage.close();
    
    // Wait for results to update (allow time for the worker to process)
    await page.waitForTimeout(2000);
    
    // Get updated vote counts
    const updatedCounts = await page.$$eval('.vote-count', els => 
      els.map(el => parseInt(el.textContent.trim(), 10))
    );
    
    // Verify vote count for first option increased
    expect(updatedCounts[0]).toBe(initialCounts[0] + 1);
  });

  it('should properly render on mobile screens', async () => {
    // Emulate a mobile device
    await page.setViewport({ width: 375, height: 667 });
    
    // Verify the results are still visible and properly formatted
    const resultsContainer = await page.$('.results-container');
    const isVisible = await resultsContainer.isIntersectingViewport();
    expect(isVisible).toBe(true);
    
    // Return to desktop viewport
    await page.setViewport({ width: 1280, height: 800 });
  });
});
```

### Step 5: Create Utilities for Test Setup

To make our tests more maintainable, let's create a utilities file with helper functions. Here's our prompt:

> **Cursor Prompt:** Create a utility file with helpful functions for our E2E tests. Include functions for waiting for elements, handling network idle states, casting votes, and setting up a browser instance.

![Cursor creating test utilities]
[IMAGE: Screenshot showing Cursor generating utility functions]

Create a file named `test-utils.js`:

```javascript
const puppeteer = require('puppeteer');

/**
 * Waits for an element to be visible
 * @param {Page} page - Puppeteer page object
 * @param {string} selector - CSS selector for the element
 * @param {number} timeout - Maximum wait time in milliseconds
 */
async function waitForElement(page, selector, timeout = 5000) {
  await page.waitForSelector(selector, { visible: true, timeout });
}

/**
 * Waits for network to be idle
 * @param {Page} page - Puppeteer page object 
 */
async function waitForNetworkIdle(page) {
  await page.waitForNavigation({ waitUntil: 'networkidle0' });
}

/**
 * Casts a vote for a specific option
 * @param {Page} page - Puppeteer page object
 * @param {number} optionIndex - Index of option to vote for (0-based)
 */
async function castVote(page, optionIndex) {
  await page.click(`.voting-option:nth-child(${optionIndex + 1})`);
  await waitForNetworkIdle(page);
}

/**
 * Sets up a fresh browser instance
 * @returns {Promise<Browser>} Puppeteer browser instance
 */
async function setupBrowser() {
  return await puppeteer.launch({
    headless: process.env.HEADLESS !== 'false',
    args: ['--no-sandbox', '--disable-setuid-sandbox'],
  });
}

module.exports = {
  waitForElement,
  waitForNetworkIdle,
  castVote,
  setupBrowser,
};
```

## Running E2E Tests in Ephemeral Environments with Release

Now that we have our E2E tests, let's integrate them with Release to run them in ephemeral environments. This approach has several advantages:

1. Tests run against isolated, clean environments
2. Multiple test runs can execute in parallel
3. Environments are automatically cleaned up after tests
4. Tests run against realistic, production-like setups

### Setting Up Release for E2E Testing

To run our tests in Release ephemeral environments, we need to:

1. Configure the Release YAML file for our application
2. Create a GitHub Actions workflow to trigger tests 
3. Set up the necessary environment variables and secrets

### Step 1: Configure Release.yaml

First, let's use Cursor to create a Release.yaml file for our application:

> **Cursor Prompt:** Create a release.yaml file for our microservice voting application that includes a frontend, API, worker, Redis, and Postgres. Also add a testing section that configures an E2E testing framework with Puppeteer.

```yaml
apiVersion: release.com/v1
kind: Application
metadata:
  name: voting-app
spec:
  services:
    frontend:
      build:
        context: ./frontend
      ports:
        - port: 3000
      env:
        - name: API_URL
          value: http://api:5000
      resources:
        limits:
          memory: 256Mi
          cpu: 200m
    
    api:
      build:
        context: ./api
      ports:
        - port: 5000
      env:
        - name: REDIS_HOST
          value: redis
        - name: POSTGRES_HOST
          value: postgres
      resources:
        limits:
          memory: 256Mi
          cpu: 200m
    
    worker:
      build:
        context: ./worker
      env:
        - name: REDIS_HOST
          value: redis
        - name: POSTGRES_HOST
          value: postgres
      resources:
        limits:
          memory: 256Mi
          cpu: 200m
    
    redis:
      image: redis:alpine
      ports:
        - port: 6379
    
    postgres:
      image: postgres:14-alpine
      env:
        - name: POSTGRES_PASSWORD
          value: postgres
        - name: POSTGRES_USER
          value: postgres
        - name: POSTGRES_DB
          value: postgres
      ports:
        - port: 5432

  # Configure testing
  testing:
    enabled: true
    frameworks:
      - name: e2e
        command: "cd /app/e2e-tests && npm test"
        image: node:16-alpine
        env:
          - name: APP_URL
            value: http://frontend:3000
          - name: HEADLESS
            value: "true"
        resources:
          limits:
            memory: 512Mi
            cpu: 500m
```

### Step 2: Create GitHub Actions Workflow

Now, let's use Cursor to create a GitHub Actions workflow file for running E2E tests:

> **Cursor Prompt:** Create a GitHub Actions workflow file that sets up the Release CLI, creates an ephemeral environment for our voting app, runs E2E tests in that environment, and then cleans up afterward.

![Cursor creating GitHub Actions workflow]
[IMAGE: Screenshot showing Cursor generating a GitHub Actions workflow file]

Create a file named `.github/workflows/e2e-tests.yml`:

```yaml
name: E2E Tests

on:
  pull_request:
    branches: [main]
  push:
    branches: [main]

jobs:
  e2e-tests:
    runs-on: ubuntu-latest
    
    steps:
      - name: Check out repository
        uses: actions/checkout@v3
      
      - name: Set up Release CLI
        run: |
          curl -sL https://cli.release.com/install.sh | bash
          echo "$HOME/.release/bin" >> $GITHUB_PATH
      
      - name: Login to Release
        run: release login --api-key ${{ secrets.RELEASE_API_KEY }}
      
      - name: Create ephemeral environment
        id: create-env
        run: |
          ENV_ID=$(release env create --name=e2e-test-${{ github.run_id }} --wait)
          echo "ENV_ID=$ENV_ID" >> $GITHUB_ENV
          
          # Get the environment URL for testing
          ENV_URL=$(release env get $ENV_ID --output=json | jq -r '.frontendUrl')
          echo "ENV_URL=$ENV_URL" >> $GITHUB_ENV
      
      - name: Run E2E tests
        run: |
          release env test $ENV_ID --framework=e2e --wait
      
      - name: Cleanup environment
        if: always()
        run: |
          release env delete $ENV_ID --force
```

### Step 3: Configure Secrets and Variables

In your GitHub repository settings, add the following secrets:

- `RELEASE_API_KEY`: Your Release API key for authentication

## Leveraging Release's Ephemeral Environments for Testing

Running E2E tests in ephemeral environments provides several key benefits:

### 1. Isolation and Consistency

Each test run gets a fresh, isolated environment that closely resembles production. This eliminates test flakiness caused by shared state or previous test runs.

### 2. Parallelization

Multiple test environments can run simultaneously, speeding up your CI/CD pipeline.

### 3. Realistic Testing

Tests run against the full application stack, including databases and microservices, providing confidence that your application works end-to-end.

### 4. Resource Efficiency

Environments are automatically provisioned before tests and destroyed afterward, preventing unnecessary resource usage.

## Best Practices for AI-Assisted E2E Testing

Based on our experience using Cursor and Release for E2E testing, here are some best practices:

### 1. Start with Clear Test Objectives

Before asking AI to generate tests, clearly define what you want to test. Specific prompts like "Write a test to verify user login functionality" work better than vague requests.

### 2. Review and Refine AI-Generated Tests

AI tools like Cursor are powerful, but they're not perfect. Always review generated tests for:
- Edge cases the AI might have missed
- Application-specific assumptions that might be incorrect
- Brittle selectors that could break easily

### 3. Create Modular, Reusable Components

Structure your tests with reusable helper functions and page objects. This makes tests more maintainable and easier for AI to understand and modify.

### 4. Use Explicit Waits

AI-generated tests often use static timeouts. Replace these with explicit waits for elements or network idle states to make tests more robust.

### 5. Monitor and Analyze Test Results

Set up monitoring for your E2E tests to identify patterns in failures. This feedback loop helps you continuously improve both your application and your tests.

## Conclusion: Quality at AI Speed

As AI accelerates software development, E2E testing becomes more critical than ever. By combining AI-powered coding tools like Cursor with ephemeral environment platforms like Release, teams can maintain high quality standards even as development velocity increases.

The same AI capabilities that help you write application code faster can also help create comprehensive test suites, maintaining the balance between speed and quality.

Ready to accelerate your testing workflow with ephemeral environments? [Try Release today](https://release.com/signup) and see how it can transform your E2E testing strategy.

## Resources

- [Release Example Voting App](https://github.com/awesome-release/release-example-voting-app)
- [Release Documentation: E2E Testing](https://docs.release.com/reference-documentation/e2e-testing)
- [Puppeteer Documentation](https://pptr.dev/)
- [Jest Documentation](https://jestjs.io/) 